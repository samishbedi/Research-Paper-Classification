Journal of Machine Learning Research 16 (2015) 455-490

Submitted 3/14; Revised 8/14; Published 3/15

Iterative and Active Graph Clustering Using Trace Norm
Minimization Without Cluster Size Constraints∗

Nir Ailon
Department of Computer Science
Technion IIT Haifa, Israel

Yudong Chen
Department of Electrical Engineering and Computer Sciences
University of California, Berkeley
Berkeley, CA 94720, USA

nailon@cs.technion.ac.il

yudong.chen@eecs.berkeley.edu

Huan Xu
Department of Mechanical Engineering
National University of Singapore
Singapore 117575

Editor: Tong Zhang

mpexuh@nus.edu.sg

Abstract

This paper investigates graph clustering under the planted partition model in the presence
of smal l clusters. Traditional results dictate that for an algorithm to provably correctly
√
recover the underlying clusters, all clusters must be suﬃciently large—in particular, the
cluster sizes need to be ˜Ω(
n), where n is the number of nodes of the graph. We show
that this is not really a restriction: by a reﬁned analysis of a convex-optimization-based
recovery approach, we prove that small clusters, under certain mild assumptions, do not
hinder recovery of large ones. Based on this result, we further devise an iterative algorithm
to provably recover almost al l clusters via a “peeling strategy”: we recover large clusters
ﬁrst, leading to a reduced problem, and repeat this procedure. These results are extended
to the partial observation setting, in which only a (chosen) part of the graph is observed.
The peeling strategy gives rise to an active learning algorithm, in which edges adjacent
to smaller clusters are queried more often after large clusters are learned (and removed).
We expect that the idea of iterative peeling—that is, sequentially identifying a subset of
the clusters and reducing the problem to a smaller one—is useful more broadly beyond the
speciﬁc implementations (based on convex optimization) used in this paper.
Keywords:
graph clustering, community detection, active clustering, convex optimiza-
tion, planted partition model, stochastic block model

1. Introduction

This paper considers the following classic graph clustering problem: given an undirected
unweighted graph, partition the nodes into disjoint clusters so that the density of edges
within each cluster is higher than those across clusters. Graph clustering arises naturally in
many applications across science and engineering; prominent examples include community

∗. This work extends and improves a preliminary conference version Ailon et al. (2013).

c(cid:13)2015 Nir Ailon and Yudong Chen and Huan Xu.

Ailon, Chen and Xu

detection in social networks (Mishra et al., 2007; Zhao et al., 2011), submarket identiﬁcation
in E-commerce and sponsored search (Yahoo!-Inc, 2009), and co-authorship analysis in
document database (Ester et al., 1995), among others. From a purely binary classiﬁcation
theoretical point of view, the edges of the graph are (noisy) labels of “similarity” or “aﬃnity”
between pairs of ob jects, and the concept class consists of clusterings of the ob jects (encoded
graphically by identifying clusters with cliques).
Many theoretical results in graph clustering consider the Planted Partition Model (Con-
don and Karp, 2001), in which the edges are generated randomly based on an unknown
set of underlying clusters; see Section 1.1 for more details. While numerous diﬀerent meth-
ods have been proposed, their performance guarantees under the planted partition model
generally have the following form: under certain conditions of the density of edges (within
√
clusters and across clusters), the method succeeds to recover the correct clusters exactly if
al l clusters are larger than a threshold size, typically ˜Ω(
n);1 see e.g., McSherry (2001);
Bollob´as and Scott (2004); Ames and Vavasis (2011); Chen et al. (2012); Chaudhuri et al.
(2012); Anandkumar et al. (2014).
In this paper, we aim to relax this cluster size constraint of graph clustering under the
planted partition model. Identifying extremely small clusters is inherently hard as they are
easily confused with “fake” clusters generated by noisy edges,2 and is not the focus of this
paper. Instead, in this paper we investigate a question that has not been addressed before:
Can we still recover large clusters in the presence of small clusters? Intuitively, this should
be doable. To illustrate, consider an extreme example where the given graph G consists of
two subgraphs G1 and G2 with disjoint node sets. Suppose G1 , if presented alone, can be
correctly clustered using some existing methods, G2 is a very small clique, and there are
relatively few edges connecting G1 and G2 . The graph G certainly violates the minimum
cluster size requirement of previous results, but why should G2 spoil our ability to correctly
cluster G1?
Our main result conﬁrms this intuition. We show that the cluster size barrier arising in
previous work is not really a restriction, but rather an artifact of the attempt to solve the
problem in a single shot and recover large and small clusters simultaneously. Using a more
√
careful analysis, we prove that a mixed trace-norm and (cid:96)1 -norm based convex formulation
can recover clusters of size ˜Ω(
n) even in the presence of smaller clusters. That is, small
clusters do not interfere with recovery of the large clusters.
The main implication of this result is that one can apply an iterative “peeling” strategy
to recover smaller and smaller clusters. The intuition is simple: suppose the number of
clusters is limited, then either all clusters are large, or the sizes of the clusters vary signif-
icantly. The ﬁrst case is obviously easy. But the second is also tractable, for a diﬀerent
reason: using the aforementioned convex formulation, the larger clusters can be correctly
identiﬁed; if we remove all nodes from these larger clusters, the remaining subgraph contains
signiﬁcantly fewer nodes than the original graph, which leads to a much lower threshold on
the size of the cluster for correct recovery, making it possible for correctly identify some

1. The notations ˜Ω(·) and ˜O(·) ignore logarithmic factors.
2. Indeed, even in a more lenient setup where one clique (i.e., a perfect cluster) of size K is embedded in an
√
Erdos-Renyi graph of n nodes and 0.5 probability of forming an edge, the best known polynomial-time
n) in order to recover the hidden clique, and it has been a long standing open
method requires K = Ω(
problem to relax this requirement.

456

Iterative and Active Clustering Without Size Constraints

smaller clusters. By repeating this procedure, indeed, we can recover the cluster structure
for almost all nodes with no lower bound on the minimal cluster size. Below we summarize
our main contributions and techniques:

1. We provide a reﬁned analysis (Theorem 2) of the mixed trace-norm and (cid:96)1 -norm
convex relaxation approach for exact cluster recovery proposed in Chen et al. (2014a,
partition setting, if each cluster is either large (more precisely, of size at least σ ≈ √
2012), focusing on the case where small clusters exist. We show that in the planted
n)
or small (of size at most σ/C for some global constant C > 1), then with high
probability, this convex relaxation approach correctly identiﬁes all large clusters while
“ignoring” the small ones.
In fact, it is possible to arbitrarily increase the tuning
parameter σ in quest of an interval (σ/C, σ) that is disjoint from the set of cluster
sizes. The analysis is done by identifying a certain feasible solution to the convex
program and proving its almost sure optimality. This solution easily identiﬁes the
√
large clusters. Previous analysis is performed only in the case where all clusters are
of size greater than
n.

2. We provide a converse (Theorem 5) of the result just described. More precisely, we
show that if for some value of the tuning parameter σ , an optimal solution to the
convex relaxation program is an exact representation of a collection of large clusters
(a partial clustering), then these clusters are actual ground truth clusters, even if the
particular interval corresponding to σ isn’t really free of cluster sizes. This allows
the practitioner to be certain that the optimal solution is useful. Moreover, this
has important algorithmic implications for an iterative recovery procedure which we
describe below.

3. The last two points imply that if some interval of the form (σ/C, σ) is free of cluster
sizes, then an exhaustive search of this interval will constructively ﬁnd large clusters,
though not necessarily for that particular interval (Theorem 6). Removing the re-
covered large clusters leads to a reduced problem with a smaller graph. Repeating
this procedure gives rise to an iterative algorithm (Algorithm 2), using a “peeling
strategy”, to recover smaller and smaller clusters that are otherwise impossible to re-
cover. Using this iterative algorithm, we prove that as long as the number of clusters
is bounded by O(log n), regardless of the cluster sizes, we can correctly recover the
cluster structure for an overwhelming fraction of nodes (Theorem 7). To the best of
our knowledge, this is the ﬁrst result of provably correct graph clustering assuming
only an upper bound on the number of clusters, but otherwise no assumption on the
cluster sizes.

4. We extend the result to the partial observation setting, where only a fraction of
similarity labels (i.e., edge/no edge) are queried. As expected, large clusters can be
identiﬁed using small observation rates, and a higher rate is needed to ﬁnd smaller
clusters. Hence, the observation rate serves as the tuning parameter. This gives rise
to an active learning algorithm (Algorithm 4) based on adaptively increasing the rate
of sampling in order to hit an interval free of cluster sizes, and spending more queries
on smaller subgraphs after we identify large clusters and peel them oﬀ. Performance

457

Ailon, Chen and Xu

guarantees are given for this algorithm (Corollary 8–Theorem 11). This active learning
scheme requires signiﬁcantly fewer samples than uniform sampling .

Beside these technical contributions, this paper suggests a new strategy that is poten-
tially useful for general low-rank matrix recovery and other high-dimensional statistical
problems, where the data are typically assumed to have certain low-dimensional structures.
Many methods have been developed to exploit this a priori structural information so that
consistent estimation is possible even when the dimensionality of the problem is larger than
the number of samples. Our result shows that one may combine these methods with a
“peeling strategy” to further push the envelope of learning structured data: by iteratively
recovering the easier structural components and reducing the problem complexity, it may be
possible to learn complicated structures that are otherwise diﬃcult to recover using existing
one-shot approaches.

1.1 Related Work

The literature of graph clustering is too vast for a detailed survey here; we concentrate on
the most related work, and in particular those provide provable guarantees on exact cluster
recovery.

1.1.1 Planted Partition Model

Also known as the stochastic block model (Holland et al., 1983; Condon and Karp, 2001),
this classical model assumes that n nodes are partitioned into subsets, referred to as the
“true clusters”, and a graph is randomly generated as follows: for each pair of nodes, de-
pending on whether or not they belong to the same subset, an edge connecting them is
generated with a probability p or q respectively. The goal is to correctly recover the clus-
ters given the random graph. The planted partition model has a large body of literature.
Earlier work focused on the setting where the minimal cluster size is Θ(n) (Boppana, 1987;
Condon and Karp, 2001; Carson and Impagliazzo, 2001; Bollob´as and Scott, 2004). Sub-
sequently, a number of methods have been proposed methods to handle sublinear cluster
sizes, including randomized algorithms (Shamir and Tsur, 2007), spectral clustering (Mc-
Sherry, 2001; Chaudhuri et al., 2012; Rohe et al., 2011; Kumar and Kannan, 2010), convex
optimization based approaches (Jalali et al., 2011; Chen et al., 2014a, 2012; Ames and
Vavasis, 2011; Oymak and Hassibi, 2011) and tensor decomposition methods (Anandkumar
et al., 2014). See Chen et al. (2014b) for a survey of existing theoretical guarantees for
the planted partition model. While the methodology diﬀers, all the work above requires,
√
sometimes implicitly, a constraint on the minimum size of the true clusters; in particular,
the size must be Ω(
n). Our analysis is carried under the planted partition model, and our
approach requires no constraint on the cluster sizes. We also mention the work of Zhao et al.
(2011) for community detection in social networks, which works under a type of planted
partition model. Like ours, their algorithm extracts clusters in an iterative manner and
is also amenable to outliers. However, their theoretical guarantees are only shown to hold
when n → ∞ and the cluster sizes grow linearly with n.

458

Iterative and Active Clustering Without Size Constraints

1.1.2 Low-rank and Sparse Matrix Decomposition via Trace Norm

Motivated by robustifying principal component analysis (PCA), several authors (Chan-
drasekaran et al., 2011; Cand`es et al., 2011) show that it is possible to recover a low-rank
matrix from sparse errors of arbitrary magnitude, where the key ingredient is using the trace
norm (also known as the nuclear norm) as a convex surrogate of the rank. Similar results
are obtained when the low rank matrix is corrupted by other types of noise (Xu et al., 2012).
Of particular relevance to this paper is the work by Jalali et al. (2011), Oymak and Hassibi
√
(2011) and Chen et al. (2012, 2014a), where they apply this approach to graph clustering,
and speciﬁcally to the planted partition model. These works require the ˜Ω(
n) bound on
the minimal cluster size. Our approach uses the trace norm relaxation, combined with a
more reﬁned analysis and an iterative/active peeling strategy.

1.1.3 Active Learning/Active Clustering

Another line of work that motivates this paper is the study of active learning (a setting in
which labeled instances are chosen by the learner, rather than by nature), and in particular
active learning algorithms for clustering. The most related work is Ailon et al. (2014), who
investigated active learning for the correlation clustering problem (Bansal et al., 2004),
where the goal is to ﬁnd a set of clusters whose Hamming distance from the graph is
minimized. Ailon et al. (2014) obtain a (1 + ε)-approximate solution with respect to the
optimum, while (actively) querying no more than O(n poly(log n, k , ε−1 )) edges, where k
is the number of clusters. Their result imposed no restriction on cluster sizes and hence
inspired this work, but diﬀers in at least two ma jor ways. First, Ailon et al. (2014) did
not consider exact cluster recovery as we do. Second, their guarantees fall in the Empirical
Risk Minimization (ERM) framework, with no running time guarantees. Our work uses a
convex relaxation algorithm, and is hence computationally eﬃcient. The problem of active
learning has also been investigated in other setups including clustering based on distance
matrix (Voevodski et al., 2012; Shamir and Tishby, 2011), hierarchical clustering (Eriksson
et al., 2011; Krishnamurthy et al., 2012) and low-rank matrix/tensor recovery (Krishna-
murthy and Singh, 2013). These setups diﬀer signiﬁcantly from ours..

Remark 1 (A note on a preliminary version of this paper) The authors published
a weaker version of the results in this paper in a preliminary conference paper (Ailon et al.,
2013). An exact comparison is stated after each theorem in the text.

2. Notation and Setup

In this paper the following notations are used. We use X (i, j ) to denote the (i, j )-the
entry of a matrix X . For a matrix X ∈ Rn×n and a subset S ⊆ [n] of size m, the matrix
X [S ] ∈ Rm×m is the principal minor of X corresponding to the set of indexes S . For a
matrix M , s(M ) denotes the support of M , namely, the set of index pairs (i, j ) such that
(cid:40)
M (i, j ) (cid:54)= 0. For any subset Φ of [n] × [n], PΦM is the matrix that satisﬁes
(i, j ) ∈ Φ
M (i, j ),
otherwise.
0,

(PΦM )(i, j ) =

459

Ailon, Chen and Xu

We now describe the problem setup. Throughout the paper, V denotes a ground set
of elements, which we identify with the set [n] = {1, . . . , n}. We assume a ground truth
clustering of V given by a pairwise disjoint covering V1 , . . . , Vk , where k is the number of
clusters. We say i ∼ j if i, j ∈ Va for some a ∈ [k ], otherwise i (cid:54)∼ j . We let na := |Va | be
the size of the a-th cluster for each a ∈ [k ]. For each i ∈ [n], (cid:104)i(cid:105) is index of the cluster that
contains i, the unique index satisfying i ∈ V(cid:104)i(cid:105) .
The ground truth clustering matrix, denoted as K ∗ , is deﬁned as the n × n matrix so
that K ∗ (i, j ) = 1 if i ∼ j , otherwise 0. This is a block diagonal matrix, each block consisting
of 1’s only, and its rank is k . The input is a symmetric n × n matrix A, which is a noisy
version of K ∗ . It is generated according to the planted partition model with parameters p
and q as follows.

We think of A as the adjacency matrix of an undirected random graph, where
the edge (i, j ) is in the graph for i > j with probability pij if i ∼ j , otherwise
with probability qij , independent of other choices, where we only assume the
edge probabilities satisfy (min pij ) =: p > q := (max qij ).

We use the convention that the diagonal entries of A are all 1. The matrix B ∗ := A − K ∗
can be viewed as the noise matrix. Given A, the task is to ﬁnd the ground truth clusters.
We remark that the setup above is more ﬂexible than the standard planted partition
model: we allow the clusters to have diﬀerent sizes, and the edges probabilities (pij and qij )
need not be uniform across node pairs (i, j ). One consequence is that the node degrees may
not be uniform or correlated with the sizes of the associated clusters. Non-uniformity makes
some simple heuristics, such as degree counting and single linkage clustering, vulnerable.
For example, we cannot distinguish between large and small clusters simply by looking at
the node degrees, since nodes in a small cluster may also have high expected degrees. The
√
single linkage clustering approach also fails in the presence of non-uniformity. We illustrate
this with an example. Suppose there are
n clusters of equal size, p = 1 and q = 0.1. We
use the number of common neighbors as the distance function in single linkage clustering.
If all qij are equal to q , then it is easy to see that single linkage clustering will succeed, since
with high probability node pairs in the same cluster will have more common neighbors than
those in diﬀerent clusters. Yet, this is not true for non-uniform qij ’s. Consider three nodes
1, 2 and 3, where nodes 1 and 2 are in the same cluster, and node 3 belongs to a diﬀerent
√
cluster. Suppose for all i > 3, q1i = 0, q2i = q3i = 0.1. The expected number of common
√
√
√
neighbors between nodes 1 and 2 is
n, whereas the expected number of common neighbors
n + 0.01(n − 2
between nodes 2 and 3 is 0.2
n for large n and
n), which is larger than
hence single linkage clustering fails. In contrast, the proposed convex-optimization based
method can handle such non-uniform settings, as we show in what follows.

3. Main Results
the (entry-wise) (cid:96)1 norm of a matrix M is (cid:107)M (cid:107)1 := (cid:80)
We remind the reader that the trace norm of a matrix is the sum of its singular values, and
i,j |M (i, j )|. Consider the following
convex program, combining the trace norm of a matrix variable K with the (cid:96)1 norm of

460

Iterative and Active Clustering Without Size Constraints

(cid:13)(cid:13)Ps(A)c B(cid:13)(cid:13)1
(cid:13)(cid:13)Ps(A)B(cid:13)(cid:13)1
another matrix variable B using two parameters c1 , c2 that will be determined later:
(cid:107)K (cid:107)∗ + c1
min
K,B∈Rn×n
s.t. K + B = A,
0 ≤ Kij ≤ 1, ∀(i, j ).

(CP)

+ c2

Here the trace norm term in the ob jective promotes low-rank solutions and thus encourages
the matrix K to have the zero-one block-diagonal structure of a clustering matrix. The
matrix Ps(A)B = Ps(A) (A − K ) is non-zero only on the pairs (i, j ) between which there is
an edge in the graph (Aij = 1) but the candidate solution has Kij = 0, and thus Ps(A)B
corresponds to the “cross-cluster disagreements” between A and K . Similarly, the matrix
Ps(A)c B corresponds to the “in-cluster disagreements”. Hence, the last two terms in the
ob jective is the weighted sum of these two types of disagreements. The formulation (CP) can
therefore be considered as a convex relaxation of the so-called weighted correlation clustering
approach (Bansal et al., 2004), whose ob jective is to ﬁnd a clustering that minimizes the
weighted disagreements. See Oymak and Hassibi (2011); Mathieu and Schudy (2010); Chen
et al. (2014a) for related formulations.
Important to subsequent development is the following new theoretical guarantee for the
formulation (CP). We show that (CP) identiﬁes the large clusters whose sizes are above a
threshold (chosen by the user) even when small clusters are present. The proof is given in
Section 5.1.

max

,

and set

(cid:96)(cid:93) := b3

Theorem 2 There exist universal constants b3 > 1 > b4 > 0 such that the fol lowing is
(cid:41)
(cid:40)
true. For any (user-speciﬁed) parameters κ ≥ 1 and t ∈ [ 1
κ(cid:112)p(1 − q)n
(cid:112)p(1 − q) log4 n
κ(cid:112)p(1 − q)n
4 p + 3
4 q , 3
4 p + 1
4 q ], deﬁne
√
p − q
p − q
κ(p − q)
1,
(cid:96)(cid:91) := b4
n
(cid:114) 1 − t
(cid:114) t
√
√
1
1
1 − t
,
c2 :=
.
(2)
c1 :=
t
n
100κ
n
100κ
If (i) n ≥ (cid:96)(cid:93) and n ≥ 700, and (ii) for each a ∈ [k ], either na ≥ (cid:96)(cid:93) or na ≤ (cid:96)(cid:91) , then with
probability at least 1 − n−3 , the optimal solution to (CP) with c1 , c2 given above is unique
(cid:40)
and equal to ( ˆK , ˆB ) = (P(cid:93)K ∗ , A − ˆK ), where for a matrix M , P(cid:93)M is the matrix deﬁned by
M (i, j ), max{n(cid:104)i(cid:105) , n(cid:104)j (cid:105)} ≥ (cid:96)(cid:93)
0,
otherwise.

(P(cid:93)M )(i, j ) =

,

(1)

The theorem improves on a weaker version in Ailon et al. 2013, where the ratio (cid:96)(cid:93)/(cid:96)(cid:91) was
√
larger by a factor of log2 n than here. The theorem says that the solution to (CP) identiﬁes
√
clusters of size larger than (cid:96)(cid:93) = Ω(κ
n) and ignores other clusters smaller than (cid:96)(cid:91) . Setting
κ = 1 we recover the usual
n scaling in previous theoretical results. The main novelty
here is the treatment of small clusters, whereas in previous work only large clusters were
allowed, and there was no guarantee for recovery when small clusters are present.

461

Ailon, Chen and Xu

Black represents 1, white represents 0. Here
σmin (K ) is the side length of the smallest
black square.

Figure 1: Illustration of a partial clustering matrix K .

Note that by the theorem’s premise, ˆK is the matrix obtained from K ∗ after zeroing out
p − q ≥ (cid:112)p(1 − q) log4 n/
blocks corresponding to clusters of size at most (cid:96)(cid:91) . Also note that under the assumption
√
κ(cid:112)p(1 − q)n
we get the following simpler expression for (cid:96)(cid:93) in the theorem, replacing its deﬁnition in (1):
p − q

(cid:96)(cid:93) = b3

n ,

(3)

(4)

.

In this case, (cid:96)(cid:93) and (cid:96)(cid:91) diﬀer by only a multiplicative absolute constant b3/b4 . We will make
the assumption (3) in what follows for simplicity, although it is not generally necessary.

Remark 3 The requirement of having a multiplicative constant gap b3/b4 between the sizes
(cid:96)(cid:93) and (cid:96)(cid:91) of the large and smal l clusters, is not an artifact of our analysis; cf. the discussion
at the end of Section 4.

For the convenience of subsequent discussion, we use the following deﬁnition.
Deﬁnition 4 (Partial Clustering Matrix) An n × n matrix K is said to be a partial
clustering matrix if there exists a col lection of pairwise disjoint sets U1 , . . . , Ur ⊆ V (cal led
the induced clusters) such that K (i, j ) = 1 if and only if i, j ∈ Ua for some a ∈ [r], otherwise
0. If K is a partial clustering matrix then σmin (K ) is deﬁned as mina∈[r ] |Ua |.

The deﬁnition is depicted in Figure 1. The key message in Theorem 2 is that by choosing κ
properly such that no cluster size falls in the interval ((cid:96)(cid:91) , (cid:96)(cid:93) ), the unique optimal solution
( ˆK , ˆB ) to the convex program (CP) is such that ˆK is a partial clustering corresponding to
large ground truth clusters.
But how can we choose a proper κ? Moreover, given that we chose a κ (say, by exhaustive
search), how can we certify that it was indeed chosen properly? In order to develop an
algorithm, we would need a type of converse of Theorem 2: There exists an event with high
probability (in the random process generating the input graph), such that conditioned on
this event, for all values of κ, if an optimal solution to the corresponding (CP) is a partial
clustering matrix with the structure illustrated in Figure 1, then the blocks of ˆK correspond
to ground truth clusters.

Theorem 5 There exist absolute constants C1 , C2 > 0 such that with probability at least
1 − n−3 , the fol lowing holds. For al l κ ≥ 1 and t ∈ [ 3
4 q + 1
4 p, 1
4 q + 3
4 p], if (K, B ) is

462

Iterative and Active Clustering Without Size Constraints

(cid:40)
(cid:41)
C2κ(cid:112)p(1 − q)n log n
an optimal solution to (CP) with c1 , c2 as deﬁned in Theorem 2, and additional ly K is a
partial clustering corresponding to U1 , . . . , Ur ⊆ V , with
σmin (K ) ≥ max
C1k log n
p − q
(p − q)2 ,
,
(5)
then U1 , . . . , Ur are actual ground truth clusters, namely, there exists an injection φ : [r] (cid:55)→
[k ] such that Ua = Vφ(a) for al l a ∈ [r].

Algorithm 1 RecoverBigFullObs(V , A, p, q)
require: ground set V , graph A ∈ RV ×V , probabilities p, q
n ← |V |
t ← 1
4 p + 1
4 q , 3
4 p + 3
4 q (or anything in [ 1
4 p + 3
4 q ])
(cid:96)(cid:93) ← n, g ← b3
(cid:27)
(cid:26)
// (If have prior bound k0 on the number of clusters, take (cid:96)(cid:93) ← n/k0 )
b4
√
p(1−q)n log n
while (cid:96)(cid:93) ≥ max
C2
C1 k log n
do
(p−q)2 ,
p−q
solve for κ using (1), set c1 , c2 as in (2)
(K, B ) ← optimal solution to (CP) with c1 , c2
if K is a partial clustering matrix with σmin (K ) ≥ (cid:96)(cid:93) then
return induced clusters {U1 , . . . , Ur } of K
end if
(cid:96)(cid:93) ← (cid:96)(cid:93)/g
end while
return ∅

The proof is given in Section 5.2. The combination of Theorems 2 and 5 implies the
following, which we state in rough terms for simplicity. Let g := b3/b4 . Assume that
√
we iteratively solve (CP) for κ taking values in some decreasing geometric progression of
common ratio g (starting at roughly κ =
n), and halt if the optimal solution is a partial
clustering with clusters of size at least (cid:96)(cid:93) = (cid:96)(cid:93) (κ) (see Algorithm 1). Then these clusters are
(extremely likely to be) ground truth clusters. Moreover, if for some κ in the sequence, (i)
the interval ((cid:96)(cid:91) = (cid:96)(cid:91) (κ), (cid:96)(cid:93) = (cid:96)(cid:93) (κ)) intersects no cluster size, and (ii) there is at least one
cluster at least of size (cid:96)(cid:93) , then such a halt will (be extremely likely to) occur.
The next question is, when are (i) and (ii) guaranteed? If the number of clusters k is a
priori bounded by some k0 , then there is at least one cluster of size at least n/k0 (alluding
to (ii)), and by the pigeonhole principle, any set of k0 + 1 pairwise disjoint intervals of the
form (α, gα) contains at least one interval that intersects no clusters size (alluding to (i)).
For simplicity, we make an exact quantiﬁcation of this principle for the case in which p, q
are assumed to be ﬁxed and independent of n.3 As the following theorem shows, it turns
out that in this regime, k0 can be assumed to be asymptotically logarithmic in n to ensure
recovery of at least one cluster.4 In what follows, notation such as C (p, q), C3 (p, q) denotes
positive functions that depend on p, q only.
3. In fact, we need only ﬁx (p − q), but we wish to keep this exposition simple.
4. In comparison, Ailon et al. (2014) require k0 to be constant for their guarantees, as do the Correlation
Clustering PTAS in Giotis and Guruswami (2006).

463

Ailon, Chen and Xu

Algorithm 2 RecoverFullObs(V , A, p, q)
require: ground set V , matrix A ∈ RV ×V , probabilities p, q
{U1 , . . . , Ur } ← RecoverBigFullObs(V , A, p, q)
V (cid:48) ← [n] \ (U1 ∪ · · · ∪ Ur )
if r = 0 then
return ∅
else
return RecoverFullObs(V (cid:48) , A[V (cid:48) ], p, q) ∪ {U1 , . . . , Ur }
end if

Theorem 6 There exist C3 (p, q), C4 (p, q), C5 > 0 such that the fol lowing holds. Assume
that n > C4 (p, q), and that we are guaranteed that k ≤ k0 , where k0 = C3 (p, q) log n. Then
with probability at least 1 − 2n−3 , Algorithm 1 wil l recover at least one cluster in at most
C5k0 iterations.

The theorem improves on a counterpart in the preliminary paper (Ailon et al., 2013), where
k0 was smaller by a factor of log log n than here.
(cid:17)
(cid:16)
(cid:17)
(cid:16)
(cid:17)
(cid:16)
Proof Consider the set of intervals
n/(g2k0 ), n/(gk0 )

n/(gk0+1k0 ), n/(gk0 k0 )

, . . . ,

n/(gk0 ), n/k0

,

.

By the pigeonhole principle, one of these intervals must not intersect the set of cluster
sizes. Assume this interval is (n/(g i0+1k0 ), n/(g i0 k0 )), for some 0 ≤ i0 ≤ k0 . By setting
√
√
C3 (p, q) small enough so that n/k0 is at least Ω(
n log n), and C4 (p, q) large enough so that
n/gk0+1k0 is at least Ω(
n log n), one easily checks that both the requirements of Theo-
rems 2 and 5 are fulﬁlled.

Theorem 6 ensures that by trying at most a logarithmic number of values of κ, we
can recover at least one large cluster, assuming the number of clusters is logarithmic in n.
After recovering and removing such a cluster, we are left with an input of size n(cid:48) < n,
0 < k0 on the number of clusters. As long as k (cid:48)
together with an updated upper bound k (cid:48)
0
is logarithmic in n(cid:48) , we can continue identifying another large cluster (with respect to the
smaller problem) using the same procedure. Clearly, as long as the input size is of size
at most exp{C3 (p, q)k0}, we can iteratively continue this process. The following has been
proved:

Theorem 7 Assume an upper bound k0 on the number k of clusters, and also that n, k0
satisfy the requirements of Theorem 6. Then with probability at least 1 − 2n−2 , Algorithm 2
recovers clusters covering al l but at most max {exp{C3 (p, q)k0}, C4 (p, q)} elements, without
any restriction on the minimal cluster size.

The theorem improves on a counterpart in the preliminary paper (Ailon et al., 2013). The
consequence is, for example, that if k0 ≤
1
2C3 (p,q) log n, then the algorithm recovers with
high probability clusters covering all but at most O(n1/2 ) elements, without any restriction
on the minimal cluster size.

464

Iterative and Active Clustering Without Size Constraints

3.1 Partial Observations and Active Sampling

We now consider the case where the input matrix A is not given to us in entirety, but rather
that we have oracle access to A(i, j ) for (i, j ) of our choice. Unobserved values are formally
marked as A(i, j ) = ∗.
Consider a more particular setting in which the edge probabilities are p(cid:48) and q (cid:48) , and the
probability of sampling an observation is ρ. More precisely: For i ∼ j we have A(i, j ) = 1
with probability ρp(cid:48) , 0 with probability ρ(1 − p(cid:48) ) and ∗ with remaining probability, indepen-
dently of other pairs. For i (cid:54)∼ j we have A(i, j ) = 1 with probability ρq (cid:48) , 0 with probability
ρ(1 − q (cid:48) ) and ∗ with remaining probability, independently of other pairs. Clearly, by pre-
tending that the values ∗ in A are 0, we emulate the full observation case of the planted
partition model with parameters p = ρp(cid:48) , q = ρq (cid:48) .
Of particular interest is the case in which p(cid:48) , q (cid:48) are held ﬁxed and ρ tends to zero as n
grows. In this regime, by varying ρ and ﬁxing κ = 1, Theorem 2 implies the following:
Corollary 8 There exist constants b3 (p(cid:48) , q (cid:48) ) > b4 (p(cid:48) , q (cid:48) ) > 0 and b5 (p(cid:48) , q (cid:48) ) > 0 such that the
(cid:26)
(cid:27)
fol lowing is true. For any sampling probability parameter 0 < ρ ≤ 1, deﬁne
√
√
log4 n√
(cid:96)(cid:93) = b3 (p(cid:48) , q (cid:48) )
(cid:96)(cid:91) = b4 (p(cid:48) , q (cid:48) )
n√
n√
,
ρ
ρn
ρ
If for each a ∈ [k ], either na ≥ (cid:96)(cid:93) or na ≤ (cid:96)(cid:91) , then, with probability at least 1 − n−3 , the
(cid:115)
program (CP) (after setting ∗ in A to 0) with
(cid:115)

1 − b5 (p(cid:48) , q (cid:48) )ρ
b5 (p(cid:48) , q (cid:48) )ρ
b5 (p(cid:48) , q (cid:48) )
√
1
1 − b5 (p(cid:48) , q (cid:48) )ρ
100
n
has a unique optimal solution equal to ( ˆK , ˆB ) = (P(cid:93)K ∗ , A − ˆK ), where P(cid:93) is as deﬁned in
Theorem 2.

c2 = c2 (p(cid:48) , q (cid:48) ) =

c1 = c1 (p(cid:48) , q (cid:48) ) =

max

√
1
100

n

,

1,

.

(6)

Note that we have slightly abused notation by reusing previously deﬁned global constants
(e.g., b1 ) with global functions of p(cid:48) , q (cid:48) (e.g., b1 (p(cid:48) , q (cid:48) )). Notice now that the sampling
probability ρ can be used as a tuning parameter for controlling the sizes of the clusters we
try to recover, instead of κ. In what follows, we will always assume the following bound on
the observation rate:

ρ ≥ log8 n
n
so that the deﬁnition of (cid:96)(cid:93) in (6) can be replaced by the simpler:
√
(cid:96)(cid:93) = b3 (p(cid:48) , q (cid:48) )
n√
ρ

,

.

(7)

(8)

This assumption is made for simplicity of the exposition, and a more elaborate (though
tedious) derivation can be done without it.
We now present an analogue of the converse result in Theorem 5 for the partial obser-
vation setting. Our main focus is to understand the asymptotics as ρ → 0.

465

Ailon, Chen and Xu

Theorem 9 There exist constants C1 (p(cid:48) , q (cid:48) ), C2 (p(cid:48) , q (cid:48) ) > 0 such that the fol lowing holds
with probability at least 1 − n−3 . For al l observation rate parameters ρ ≤ 1, if (K, B ) is
an optimal solution to (CP) with c1 , c2 as deﬁned in Corol lary 8, and additional ly K is a
(cid:27)
(cid:26) C1 (p(cid:48) , q (cid:48) )k log n
partial clustering corresponding to U1 , . . . , Ur ⊆ V , and also
√
C2 (p(cid:48) , q (cid:48) )
σmin (K ) ≥ max
√
,
(9)
,
ρ
ρ
then U1 , . . . , Ur are actual ground truth clusters, namely, there exists an injection φ : [r] (cid:55)→
[k ] such that Ua = Vφ(a) for each a ∈ [r].

n log n

The proof is similar to that of Theorem 5. The necessary changes are outlined in
Section 5.3. Using the same reasoning as before, we derive the following:
Theorem 10 Let g = (b3 (p(cid:48) , q (cid:48) )/b4 (p(cid:48) , q (cid:48) ))2 (with b3 (p(cid:48) , q (cid:48) ), b4 (p(cid:48) , q (cid:48) ) deﬁned in Corol lary 8).
There exist constants C3 (p(cid:48) , q (cid:48) ) and C4 (p(cid:48) , q (cid:48) ) such that the fol lowing holds. Assume n ≥
C3 (p(cid:48) , q (cid:48) ) and the number of clusters k is bounded by some known number k0 ≤ C4 (p(cid:48) , q (cid:48) ) log n.
Let ρ0 = b3 (p(cid:48) ,q (cid:48) )2 k2
. Then there exists ρ in the set {ρ0 , ρ0g , . . . , ρ0gk0 } for which, if A
0 log n
is obtained with sampling rate ρ (zeroing ∗’s), then with probability at least 1 − 2n−3 , any
n
optimal solution (K, B ) to (CP) with c1 (p(cid:48) , q (cid:48) ), c2 (p(cid:48) , q (cid:48) ) from Corol lary 8 satisﬁes that K is
a partial clustering with the property in (9).

Note that the upper bound on k0 ensures that ρgk0 is a probability. The theorem improves
on a counterpart in the preliminary paper (Ailon et al., 2013), where k0 was smaller by a
factor of log log n compared to here. The theorem is proven, again, using a simple pigeonhole
principle, noting that one of the intervals ((cid:96)(cid:91) (ρ), (cid:96)(cid:93) (ρ)) must be disjoint from the set of cluster
sizes, and there is at least one cluster of size at least n/k0 . The value of ρ0 is chosen so
that n/k0 is larger than the RHS of (9). This theorem motivates the iterative procedure
in Algorithm 3: we start with a low sampling rate ρ, which is then increased geometrically
until the program (CP) returns a partial clustering.
Theorem 10 together with Corollary 8 and Theorem 9 ensures the following. On one
end of the spectrum, if k0 is a constant (and n is large enough), then with high probability
(cid:32)
(cid:19)2k0 (cid:33)
(cid:18) b3 (p(cid:48) , q (cid:48) )
Algorithm 3 recovers at least one large cluster (of size at least n/k0 ) after querying no more
than
nk2
O
0 (log n)
(10)
b4 (p(cid:48) , q (cid:48) )
values of A(i, j ). On the other end of the spectrum, if k0 ≤ δ log n and n is large enough
(exponential in 1/δ), then Algorithm 3 recovers at least one large cluster after querying no
more than n1+O(δ) values of A(i, j ). Iteratively recovering and removing large clusters leads
to Algorithm 4 with the following guarantees.

Theorem 11 Assume an upper bound k0 on the number of clusters k . As long as n is
larger than some function of k0 , p(cid:48) , q (cid:48) , Algorithm 4 wil l recover, with probability at least
1 − n−2 , at least one cluster of size at least n/k0 , regard less of the size of other (smal l)
clusters. Moreover, if k0 is a constant, then clusters covering al l but a constant number
of elements wil l be recovered with probability at least 1 − 2n−2 , and the total number of
observation queries is given by (10), hence almost linear.

466

Iterative and Active Clustering Without Size Constraints

Algorithm 3 RecoverBigPartialObs(V , k0 ) (Assume p(cid:48) , q (cid:48) known, ﬁxed)
require: ground set V , oracle access to A ∈ RV ×V , upper bound k0 on number of clusters
n ← |V |
ρ0 ← b3 (p(cid:48) ,q (cid:48) )2 k2
0 log n
g ← b3 (p(cid:48) , q (cid:48) )2/b4 (p(cid:48) , q (cid:48) )2
n
for s ∈ {0, . . . , k0} do
ρ ← ρ0gs
obtain matrix A ∈ {0, 1, ∗}V ×V by sampling oracle at rate ρ, then zero ∗ values in A
// (can reuse observations from previous iterations)
c1 (p(cid:48) , q (cid:48) ), c2 (p(cid:48) , q (cid:48) ) ← as in Corollary 8
(K, B ) ← an optimal solution to (CP)
if K is a partial clustering matrix satisfying (9) then
return induced clusters {U1 , . . . , Ur }
end if
end for
return ∅

Algorithm 4 RecoverPartialObs(V , k0 ) (Assume p(cid:48) , q (cid:48) known, ﬁxed)
require: ground set V , oracle access to A ∈ RV ×V , upper bound k0 on number of clusters
{U1 , . . . , Ur } ← RecoverBigPartialObs(V , k0 )
V (cid:48) ← [n] \ (U1 ∪ · · · ∪ Ur )
if r = 0 then
return ∅
else
return RecoverFullObs(V (cid:48) , k0 − r) ∪ {U1 , . . . , Ur }
end if

The theorem improves on a counterpart in the preliminary paper (Ailon et al., 2013), where
the recovery covers all but a super-constant (in n) number of elements. Unlike previous
√
convex relaxation based approaches for this problem, which require all cluster sizes to be
n to succeed, there is no constraint on the cluster sizes for our
of size at least roughly
algorithm.

Also note that our algorithm is an active learning one, because more observations fall
√
in smaller clusters which survive deeper in the recursion of Algorithm 4. This feature can
lead to a signiﬁcant saving in the number of queries. When small clusters of size ˜Θ(
n) are
present, previous one-shot algorithms for graph clustering with partial observations (e.g.,
Jalali et al., 2011; Oymak and Hassibi, 2011; Chen et al., 2014a) only guarantee recovery
using O(n2 ) queries, which is much larger than the almost linear requirement ˜O(n) of our
active algorithm.

467

Ailon, Chen and Xu

4. Experiments

We test our main Algorithms 2 and 4 (with subroutines Algorithms 1 and 3) on synthetic
data. In all experiment reports below, we use a variant of the Alternating Direction Method
of Multipliers (ADMM) to solve the semideﬁnite program (CP); see Lin et al. (2011); Chen
et al. (2012). The main cost of ADMM is the computation of the Singular Value Decom-
position (SVD) of an n × n matrix in each round. Note that one can take advantage of the
sparsity of the observations to speed up the SVD (cf. Lin et al. 2011). As is discussed in
previous work, and also observed empirically by us, ADMM converges linearly, so the num-
ber of SVD needed is usually small. See the references above for further discussion of the
optimization issues. The overall computation time also depends on the number of recursive
calls in Algorithm 2 and 4, as well as the number of iterations used in Algorithm 1 and 3
in search for suitable values for κ and ρ (using a multiplicative update rule). These two
numbers are at most O(max(k , log n)) (k is the number of clusters) under the conditions of
the theorems, and in our experiments they are both quite small.

In the experiments we consider simpliﬁed versions of the algorithms: we did not make
an eﬀort to compute the constants (cid:96)(cid:93)/(cid:96)(cid:91) deﬁning the algorithms, creating a diﬃculty in
exact implementation. Instead, for Algorithm 1, we start with κ = 1 and increase κ by
a multiplicative factor of 1.1 in each iteration until a partial clustering matrix is found.
Similarly, in Algorithm 3, the sampling rate ρ has an initial value of 0 and is increased by
an additive factor of 0.025. Still, it is obvious that our experiments support our theoretical
ﬁndings. A more practical “user’s guide” for this method with actual constants is sub ject
to future work.
Whenever we say that “clusters {Vi1 , Vi2 , . . . } were recovered”, we mean that a corre-
sponding instantiation of (CP) resulted in an optimal solution (K, B ) for which K was a
partial clustering matrix induced by {Vi1 , Vi2 , . . . }.

4.1 Experiment 1 (Full Observation)

Consider n = 1100 nodes partitioned into 4 clusters V1 , . . . , V4 , of sizes 800, 200, 80, 20,
respectively. The graph is generated according to the planted partition model with p = 0.5
and q = 0.2, and we assume the full observation setting. We apply the simpliﬁed version
of Algorithm 2 described previously, which terminates in 4 iterations using 44 seconds.
The recovered clusters at each iteration are detailed in Table 1. The table also shows the
values of κ adaptively chosen by the algorithm at each iteration (which happens to equal 1
throughout). We note that the ﬁrst iteration of the algorithm is similar to existing convex
optimization based approaches to graph clustering (Jalali et al., 2011; Oymak and Hassibi,
2011; Chen et al., 2012); the experiment shows that these approaches by itself fail to recover
all the clusters in one shot, thus necessitating the iterative procedure proposed in this paper.

4.2 Experiment 2 (Partial Observation, Fixed Sample Rate)

We have n = 1100 with clusters V1 , . . . , V4 of sizes 800, 200, 50, 50. The observed graph
is generated with p(cid:48) = 0.7, q (cid:48) = 0.1, and observation rate ρ = 0.3. We repeatedly solve
(CP) with c1 , c2 given in Corollary 8. At each iteration, we see that at least one large

468

Iterative and Active Clustering Without Size Constraints

Iteration κ # nodes left Clusters recovered
1100
1
1
V1
300
1
2
V2
3
1
100
V3
20
1
4
V4

Table 1: Results for experiment 1: n = 1100, {|Va |} = {800, 200, 80, 20}, p = 0.5, q = 0.2,
ﬁxed ρ = 1.

Iteration κ # nodes left Clusters recovered
1100
1
1
V1
300
1
2
V2
3
1
100
V3 , V4

Table 2: Results for experiment 2: n = 1100, {|Va |} = {800, 200, 50, 50}, p(cid:48) = 0.7, q (cid:48) = 0.1,
ﬁxed ρ = 0.3.

cluster (compared to the input size at that iteration) is recovered exactly and removed.
The experiment terminates in 3 iterations using 18 seconds. Results are shown in Table 2.

4.3 Experiment 3 (Partial Observation, Adaptive Sampling Rate)

We use the simpliﬁed version of Algorithm 4 described previously. We have n = 1100
with clusters V1 , . . . , V4 of sizes 800, 200, 50, 50. The graph is generated with p(cid:48) = 0.8 and
q (cid:48) = 0.2, and then adaptively sampled by the algorithm. The algorithm terminates in 3
iterations using 148 seconds. Table 3 shows the recovery result and the sampling rates used
in each iteration. From the table we can see that the expected total number of observed
entries used by the algorithm is
11002 · 0.125 + 3002 · 0.25 + 1002 · 0.55 = 179250,

which is 14.8% of all possible node pairs (the actual number of observations is very close to
this expected value). In comparison, we perform another experiment using a non-adaptive
sampling rate, for which we need ρ = 97.5% in order to recover all the clusters in one shot.
Therefore, our adaptive algorithm achieves a signiﬁcant saving in the number of queries.

4.4 Experiment 3A

We repeat the above experiment with a larger instance: n = 4500 with clusters V1 , . . . , V6
of sizes 3200, 800, 200, 200, 50, 50, and p(cid:48) = 0.8, q (cid:48) = 0.2. The algorithm terminates in 182
√
seconds, with results shown in Table 4. Note that we recover the smallest clusters, whose
sizes are below
n. The expected total number of observations used by the algorithm is
3388000, which is 16.7% of all possible node pairs. Using a non-adaptive sampling rate

469

Ailon, Chen and Xu

Iteration
1
2
3

ρ
0.125
0.25
0.55

# nodes left Clusters recovered
1100
V1
300
V2
100
V3 , V4

Table 3: Results for experiment 3: n = 1100, {|Va |} = {800, 200, 50, 50}, p(cid:48) = 0.8, q (cid:48) = 0.2.

Iteration
1
2
3
4

ρ
0.15
0.175
0.2
0.475

# nodes left Clusters recovered
4500
V1
1300
V2
V3 , V4
500
100
V5 , V6

Table 4: Results for experiment 3A: n = 4500, {|Va |} = {3200, 800, 200, 200, 50, 50}, p(cid:48) =
0.8, q (cid:48) = 0.2.

ρ = 35.0% only recovers the 4 largest clusters, and we are unable to recover all 6 clusters
in one shot even with ρ = 1 .

4.5 Experiment 4 (Mid-Size Clusters)

Our current theoretical results do not say anything about the mid-size clusters—those with
sizes between (cid:96)(cid:91) and (cid:96)(cid:93) . It is interesting to investigate the behavior of (CP) in the presence
of mid-size clusters. We generate an instance with n = 750 nodes partitioned into four
clusters of sizes {500, 150, 70, 30}, edge probabilities p = 0.8, q = 0.2 and a sampling rate
ρ = 0.12. We then solve (CP) with a ﬁxed κ = 1. The low-rank part K of the solution
is shown in Figure 2. The large cluster of size 500 is completely recovered in K , while the
two small clusters of sizes 70 and 30 are entirely ignored. The medium cluster of size 150,
however, exhibits a pattern we ﬁnd diﬃcult to characterize. This shows that the constant
gap between (cid:96)(cid:93) and (cid:96)(cid:91) in our theorems is a real phenomenon and not an artifact of our proof
techniques. Nevertheless, the mid-size cluster appears clean, and might allow recovery using
a simple combinatorial procedure. If this is true in general, it might not be necessary to
search for a gap free of cluster sizes. In particular, perhaps for any κ, (CP) identiﬁes all large
clusters above (cid:96)(cid:93) after a simple mid-size cleanup procedure, and ignores all other clusters.
Understanding this phenomenon and its algorithmic implications is of much interest.

5. Proofs

We use the following notation and conventions throughout the proofs. With high probability
or w.h.p. means with probability at least 1 − n−6 . The expressions a ∨ b and a ∧ b mean
max{a, b} and min{a, b}, respectively. For a real n × n matrix M , we use the unadorned
norm (cid:107)M (cid:107) to denote its spectral norm. The notation (cid:107)M (cid:107)F refers to the Frobenius norm,

470

Iterative and Active Clustering Without Size Constraints

(cid:107)M (cid:107)1 is (cid:80)
Figure 2: The solution to (CP) with mid-size clusters.
product (cid:104)X, Y (cid:105) := (cid:80)n
i,j |M (i, j )|, and (cid:107)M (cid:107)∞ is maxij |M (i, j )|. We shall use the standard inner
i,j=1 X (i, j )Y (i, j ).
We will also study operators on the space of matrices, and denote them using a calli-
graphic font, e.g., P . The norm (cid:107)P (cid:107) of an operator is deﬁned as
(cid:107)P (cid:107) :=
(cid:107)PM (cid:107)F .
sup
M ∈Rn×n :(cid:107)M (cid:107)F =1
For a ﬁxed real n × n matrix M , we deﬁne the matrix linear subspace T (M ) as follows:
T (M ) := {Y M + M X : X, Y ∈ Rn×n} .

In words, this subspace is the set of matrices spanned by matrices each row of which is in
the row space of M , and matrices each column of which is in the column space of M . We
let T (M )⊥ denote the orthogonal subspace to T (M ) with respect to (cid:104)·, ·(cid:105), which is given by
T (M )⊥ := {X ∈ Rn×n : (cid:104)X, Y (cid:105) = 0, ∀Y ∈ T (M )} .
It is a well known fact that the pro jection PT (X ) onto T (X ) w.r.t. (cid:104)·, ·(cid:105) is given by
PT (X )M := PC (X )M + PR(X )M − PC (X )PR(X )M ,
where PC (X ) is pro jection (of each column of a matrix) onto the column space of X , and
PR(X ) is pro jection onto the row space of X . The pro jection onto T (M )⊥ is PT (X )⊥M =
M − PT (X )M .
Finally, we recall that s(M ) is the support of M , Ps(M )X is the matrix obtained from
X by setting its entries outside s(M ) to zero, and Ps(M )c X := X − Ps(M )X .

5.1 Proof of Theorem 2

The proof builds on the analysis in Chen et al. (2012). We need some additional notation:
1. We let V(cid:91) ⊆ V denote the set of of elements i such that n(cid:104)i(cid:105) ≤ (cid:96)(cid:91) . (We remind the
reader that n(cid:104)i(cid:105) = |V(cid:104)i(cid:105) |.)
(cid:40)
2. We remind the reader that the pro jection P(cid:93) is deﬁned as follows:
M (i, j ), max{n(cid:104)i(cid:105) , n(cid:104)j (cid:105)} ≥ (cid:96)(cid:93)
otherwise.
0,

(P(cid:93)M )(i, j ) =

471

100200300400500600700100200300400500600700Ailon, Chen and Xu

(P(cid:91)M )(i, j ) =

(cid:40)
3. The pro jection P(cid:91) is deﬁned as follows:
M (i, j ), max{n(cid:104)i(cid:105) , n(cid:104)j (cid:105)} ≤ (cid:96)(cid:91)
0,
otherwise.
In words, P(cid:91) pro jects onto the set of matrices supported on V(cid:91) × V(cid:91) . Note that by the
theorem assumption, P(cid:93) + P(cid:91) = I d (equivalently, P(cid:93) pro jects onto the set of matrices
supported on (V × V ) \ (V(cid:91) × V(cid:91) )).
4. We use U ΣU (cid:62) to denote the rank-k (cid:48) Singular Value Decomposition (SVD) of the
symmetric matrix ˆK , where k (cid:48) = rank( ˆK ) and equals the number of clusters with size
at least (cid:96)(cid:93) .
(cid:9) ,
D := (cid:8)∆ ∈ Rn×n |∆ij ≤ 0, ∀i ∼ j, (i, j ) /∈ V(cid:91) × V(cid:91) ; 0 ≤ ∆ij , ∀i (cid:54)∼ j, (i, j ) /∈ V(cid:91) × V(cid:91)
5. Deﬁne the set
which strictly contains all feasible deviation from ˆK .

6. For simplicity we write T := T ( ˆK ).

We will make use of the following facts:
1. I d = Ps( ˆB ) + Ps( ˆB )c = Ps(A) + Ps(A)c .
2. P(cid:93) , P(cid:91) , Ps( ˆB ) , Ps( ˆB )c , Ps(A) , and Ps(A)c commute with each other.
5.1.1 Approximate Dual Certificate Condition

We begin by giving a deterministic suﬃcient condition for ( ˆK , ˆB ) to be the unique optimal
solution to the program (CP).

Proposition 12 ( ˆK , ˆB ) is the unique optimal solution to (CP) if there exists a matrix
Q ∈ Rn×n and a number 0 <  < 1 satisfying:
1. (cid:107)Q(cid:107) < 1;
2. (cid:107)PT (Q)(cid:107)∞ ≤ 
2 min {c1 , c2};
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )P(cid:93)∆
(cid:68)
(cid:69)
3. ∀∆ ∈ D:
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )P(cid:93)∆
(cid:69)
(cid:68)
U U (cid:62) + Q, Ps(A)Ps( ˆB )P(cid:93)∆
,
= (1 + )c1
U U (cid:62) + Q, Ps(A)c Ps( ˆB )P(cid:93)∆
;
= (1 + )c2
(b)
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )c P(cid:93)∆
(cid:69) ≥ −(1 − )c1
(cid:68)
4. ∀∆ ∈ D:
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )c P(cid:93)∆
(cid:69) ≥ −(1 − )c2
(cid:68)
U U (cid:62) + Q, Ps(A)Ps( ˆB )c P(cid:93)∆
,
U U (cid:62) + Q, Ps(A)c Ps( ˆB )c P(cid:93)∆

(a)

;

(a)

(b)

472

Iterative and Active Clustering Without Size Constraints

(cid:13)(cid:13)(cid:13)Ps( ˆB )c P(cid:91) (U U (cid:62) + Q)
(cid:13)(cid:13)(cid:13)∞
5. Ps( ˆB )P(cid:91) (U U (cid:62) + Q) = c1P(cid:91)
ˆB ;
≤ c2 .
6.
Proof Consider any feasible solution ( ˆK + ∆, ˆB − ∆) to (CP); we know ∆ ∈ D due to
the inequality constraints in (CP). We will show that this solution will have strictly higher
ob jective value than ( ˆK , ˆB ) if ∆ (cid:54)= 0.
For this ∆, let G∆ be a matrix in T ⊥ ∩ Range(P(cid:91) ) satisfying (cid:107)G∆(cid:107) = 1 and (cid:104)G∆ , ∆(cid:105) =
(cid:107)PT ⊥ P(cid:91)∆(cid:107)∗ ; such a matrix always exists because RangeP(cid:91) ⊆ T ⊥ . Suppose (cid:107)Q(cid:107) = b.
Clearly, PT ⊥ Q + (1 − b)G∆ ∈ T ⊥ and, due to Property 1 in the proposition, we have
b < 1 and (cid:107)PT ⊥ Q + (1 − b)G∆(cid:107) ≤ (cid:107)Q(cid:107) + (1 − b) (cid:107)G∆(cid:107) = b + (1 − b) = 1. Therefore,
U U (cid:62) + PT ⊥ Q + (1 − b)G∆ is a subgradient of f (K ) = (cid:107)K (cid:107)∗ at K = ˆK . On the other hand,
Ps(A) ( ˆB + F∆ ) is a subgradient of g1 (B ) = (cid:13)(cid:13)Ps(A)B(cid:13)(cid:13)1
deﬁne the matrix F∆ = −Ps( ˆB )c sgn(∆). We have F∆ ∈ s( ˆB )c and (cid:107)F∆(cid:107)∞ ≤ 1. Therefore,
is a subgradient of g2 (B ) = (cid:13)(cid:13)Ps(A)c B(cid:13)(cid:13)1
at B = ˆB , and Ps(A)c ( ˆB + F∆ )
at B = ˆB . Using these three subgradients, the
diﬀerence in the ob jective value can be bounded as follows:
(cid:44) (cid:13)(cid:13)(cid:13) ˆK + ∆
(cid:13)(cid:13)(cid:13)Ps(A)
(cid:13)(cid:13)(cid:13)∗
− (cid:13)(cid:13)(cid:13) ˆK
(cid:13)(cid:13)(cid:13)Ps(A)c ( ˆB − ∆)
(cid:13)(cid:13)(cid:13)Ps(A) ( ˆB − ∆)
(cid:13)(cid:13)(cid:13)∗ + c1
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)1
d(∆)
(cid:13)(cid:13)(cid:13)Ps(A)c ˆB
(cid:13)(cid:13)(cid:13)1
− c1
ˆB
≥ (cid:68)
(cid:69)
(cid:69)
(cid:68)Ps(A) ( ˆB + F∆ ), −∆
(cid:68)Ps(A)c ( ˆB + F∆ ), −∆
− c2
(cid:68)
(cid:69)
(cid:68)Ps(A)
(cid:69)
(cid:68)Ps(A)c ˆB , −∆
(cid:69)
U U (cid:62) + PT ⊥ Q + (1 − b)G∆ , ∆
+ c1
+ c2
(cid:10)Ps(A)c F∆ , −∆(cid:11)
(cid:10)Ps(A)F∆ , −∆(cid:11) + c2
U U (cid:62) + PT ⊥ Q, ∆
=(1 − b) (cid:107)PT ⊥ P(cid:91)∆(cid:107)∗ +
ˆB , −∆
+ c1
+ c2
(cid:68)
(cid:69)
(cid:69)
(cid:68)P(cid:91)Ps(A)c ˆB , −∆
(cid:68)P(cid:91)Ps(A)
(cid:69)
+ c1
(cid:68)P(cid:93)Ps(A)
(cid:69)
(cid:68)P(cid:93)Ps(A)c ˆB , −∆
(cid:69)
(cid:10)Ps(A)c F∆ , −∆(cid:11) .
(cid:10)Ps(A)F∆ , −∆(cid:11) + c2
=(1 − b) (cid:107)PT ⊥ P(cid:91)∆(cid:107)∗ +
U U (cid:62) + PT ⊥ Q, ∆
ˆB , −∆
+ c2
+ c1
ˆB , −∆
+ c1
+ c2
+ c1
(cid:68)P(cid:91)Ps(A)c ˆB , −∆
(cid:69)
(cid:68)P(cid:91)Ps(A)
(cid:68)P(cid:91)
(cid:69)
(cid:69)
The last six terms of the last RHS satisfy:
(cid:13)(cid:13)(cid:13)1
(cid:69) ≥ − (cid:13)(cid:13)(cid:13)P(cid:93)Ps(A)c Ps( ˆB )∆
(cid:69) ≥ − (cid:13)(cid:13)(cid:13)P(cid:93)Ps(A)Ps( ˆB )∆
(cid:13)(cid:13)(cid:13)1
(cid:68)P(cid:93)Ps(A)
(cid:68)P(cid:93)Ps(A)c ˆB , ∆
ˆB , −∆
ˆB , −∆
, because P(cid:91)
ˆB ∈ s(A).
= c1
1. c1
+ c2
(cid:13)(cid:13)(cid:13)∞
(cid:13)(cid:13)(cid:13) ˆB
ˆB , −∆
2.
and
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )c ∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )c ∆
(cid:13)(cid:13)(cid:13)1
because ˆB ∈ s( ˆB ) and
≤ 1.
3. (cid:10)Ps(A)F∆ , −∆(cid:11) =
and (cid:10)Ps(A)c F∆ , −∆(cid:11) =
, due to
the deﬁnition of F .
(cid:13)(cid:13)(cid:13)P(cid:93)Ps(A)Ps( ˆB )∆
(cid:13)(cid:13)(cid:13)1
(cid:69) − c1
(cid:68)P(cid:91)
(cid:69)
(cid:68)
It follows that
(cid:13)(cid:13)(cid:13)P(cid:93)Ps(A)c Ps( ˆB )∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)Psc ( ˆB )∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)Ps(A)c Psc ∆(cid:13)(cid:13)1
ˆB , −∆
U U (cid:62) + PT ⊥ Q, ∆
d(∆) ≥(1 − b) (cid:107)PT ⊥ P(cid:91)∆(cid:107)∗ +
+ c1
− c2
(11)
.
+ c2
473

(cid:69)

+ c2

+ c1

,

Ailon, Chen and Xu

.

=

+

(cid:69)

(cid:68)
(cid:69)
(cid:68)
(cid:69)
(cid:68)
(cid:69) − (cid:104)PT Q, ∆(cid:105) .
Consider the second term in the last RHS, which equals
U U (cid:62) + PT ⊥ Q, ∆
U U (cid:62) + Q, P(cid:93)∆
U U (cid:62) + Q, P(cid:91)∆
(cid:68)
(cid:69)
We bound these three terms separately. For the ﬁrst term, we have
(cid:16)Ps(A)Ps( ˆB )P(cid:93) + Ps(A)c Ps( ˆB )P(cid:93) + Ps(A)Ps( ˆB )c P(cid:93) + Ps(A)c Psc P(cid:93)
(cid:68)
(cid:69)
(cid:17)
U U (cid:62) + Q, P(cid:93)∆
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )c P(cid:93)∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )P(cid:93)∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )P(cid:93)∆
(cid:13)(cid:13)(cid:13)1
U U (cid:62) + Q,
=
∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )c P(cid:93)∆
− (1 − )c1
≥(1 + )c1
+ (1 + )c2
− (1 − )c2
(Using Properties 3 and 4.)
(cid:69)
(cid:68)
For the second term, we have
(cid:69)
(cid:68)Ps( ˆB )P(cid:91) (U U (cid:62) + Q), ∆
(cid:68)Ps( ˆB )c P(cid:91) (U U (cid:62) + Q), ∆
U U (cid:62) + Q, P(cid:91)∆
(cid:13)(cid:13)(cid:13)Ps( ˆB )c P(cid:91)∆
(cid:13)(cid:13)(cid:13)1
(cid:68)P(cid:91)
(cid:69) − c2
+
=
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )c P(cid:91)∆
(cid:68)P(cid:91)
(cid:69) − c2
≥c1
ˆB , ∆
(using Properties 5 and 6)
(Because Ps(A)c Ps( ˆB )c P(cid:91) = Ps( ˆB )c P(cid:91) .)
Finally, for the third term, Due to the block diagonal structure of the elements of T , we
have PT = P(cid:93)PT and therefore
(cid:104)−PT Q, ∆(cid:105) = − (cid:104)PT Q, P(cid:93)∆(cid:105) ≥ − (cid:107)PT Q(cid:107)∞ (cid:107)P(cid:93)∆(cid:107)1 ≥ − 
min {c1 , c2} (cid:107)P(cid:93)∆(cid:107)1 .
2
Combining the above three bounds with Eq. (11), we obtain
(cid:13)(cid:13)(cid:13)P(cid:93)Ps(A)c Ps( ˆB )∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)P(cid:93)Ps(A)Ps( ˆB )∆
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )c P(cid:93)∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)1
d(∆)
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )c P(cid:91)∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )c P(cid:93)∆
≥(1− b) (cid:107)PT ⊥ P(cid:91)∆(cid:107)∗ + c1
+ c2
+ c1
(cid:13)(cid:13)P(cid:93)Ps(A)c ∆(cid:13)(cid:13)1
(cid:13)(cid:13)P(cid:93)Ps(A)∆(cid:13)(cid:13)1
− 
min {c1 , c2} (cid:107)P(cid:93)∆(cid:107)1
+ c1
+ c2
2
− 
min {c1 , c2} (cid:107)P(cid:93)∆(cid:107)1
=(1 − b) (cid:107)PT ⊥ P(cid:91)∆(cid:107)∗ + c1
+ c2
2
(note that Ps(A)Ps( ˆB )c P(cid:91)∆=0)
≥(1 − b) (cid:107)P(cid:91)∆(cid:107)∗ +
min {c1 , c2} (cid:107)P(cid:93)∆(cid:107)1 ,

2
which is strictly greater than zero for ∆ (cid:54)= 0.

ˆB , ∆

=c1

.

5.1.2 Constructing Q
To prove the theorem, it suﬃces to show that with probability at least 1 − n−3 , there
exists a matrix Q with the properties required by Proposition 12. We do this by explicitly

474

Iterative and Active Clustering Without Size Constraints

(cid:115)

(cid:40)

√
κ

n

,

(cid:41)

,

 :=

max

P(cid:93)Q2 (i, j ) =

P(cid:93)Q1 (i, j ) =

constructing Q. Suppose we take
100(cid:112)t(1 − t)
log4 n
(cid:96)(cid:93)
(cid:96)(cid:93)
and use the weights c1 and c2 given in Theorem 2. We specify P(cid:93)Q and P(cid:91)Q separately.

The matrix P(cid:93)Q is given by P(cid:93)Q = P(cid:93)Q1 + P(cid:93)Q2 + P(cid:93)Q3 , where for (i, j ) /∈ V(cid:91) × V(cid:91) ,
− 1
i ∼ j, (i, j ) ∈ s( ˆB )
n(cid:104)i(cid:105) ,
n(cid:104)i(cid:105) · 1−pij
i ∼ j, (i, j ) ∈ s( ˆB )c
1
,
−(1 + )c2 ,
pij
i (cid:54)∼ j
0,
i ∼ j, (i, j ) ∈ s( ˆB )
1−pij
i ∼ j, (i, j ) ∈ s( ˆB )c
(1 + )c2
(1 + )c1 ,
pij
i (cid:54)∼ j
0,
i (cid:54)∼ j, (i, j ) ∈ s( ˆB )
i (cid:54)∼ j, (i, j ) ∈ s( ˆB )c
−(1 + )c1
qij
1−qij
i ∼ j.
0,
Note that these matrices have zero-mean entries. (Recall that s( ˆB ) = s(A − ˆK ) is a random
set since the graph A is random.)

P(cid:91)Q is given as follows. For (i, j ) ∈ V(cid:91) × V(cid:91) ,
c1 ,
−c2 ,
c1 ,
c2W (i, j ),
(cid:40)
where W is a symmetric matrix whose upper-triangle entries are independent and obey
t−q
2t(1−q) ,
+1, with probability
−1, with remaining probability.

i ∼ j, (i, j ) ∈ s(A)
i ∼ j, (i, j ) ∈ s(A)c
i (cid:54)∼ j, (i, j ) ∈ s(A)
i (cid:54)∼ j, (i, j ) ∈ s(A)c ,

P(cid:93)Q3 (i, j ) =

P(cid:91)Q(i, j ) =

,

,

W (i, j ) =

Note that we introduced additional randomness in W .

5.1.3 Validating Q
4 p ≤ t ≤ p and 1
4 (1 − q) ≤ 1 − t ≤ 1 − q .
Under the choice of t in Theorem 2, we have 1
Also under the assumption (1) in the theorem and since p − q ≤ p(1 − q), (cid:96)(cid:93) ≤ n, we have
≥ b3 log4 n
∨ b3 log4 n
p(1 − q) ≥ b2
3 κ2n
. Using these inequalities, it is easy to check that  < 1
(cid:96)2
(cid:96)(cid:93)
n
2
(cid:93)
provided that the constant b3 is suﬃciently large. We will make use of these facts frequently
in the proof.
We now verify that the Q constructed above satisfy the six properties in Proposition 12
with probability at least 1 − n−3 .

475

Ailon, Chen and Xu

√

1
100κ

n

≤ max

≤ 1
32

,

(cid:107)E[P(cid:91)Q∼ ](cid:107) ≤ (cid:96)(cid:91)

Property 1:
Suppose the matrix Q∼ is obtained from Q by setting all Q(i, j ) with i (cid:54)∼ j to zero,
and Q(cid:54)∼ = Q − Q∼ . Note that (cid:107)Q(cid:107) ≤ (cid:107)P(cid:93)Q∼(cid:107) + (cid:107)P(cid:93)Q(cid:54)∼(cid:107) + (cid:107)P(cid:91)Q∼(cid:107) + (cid:107)P(cid:91)Q(cid:54)∼(cid:107). Below we
show that with high probability, the ﬁrst term is upper-bounded by 7
32 and the other threes
4 , which establishes that (cid:107)Q(cid:107) ≤ 31
terms are upper-bounded by 1
32 .
(a) P(cid:91)Q∼ is a block diagonal matrix support on V(cid:91) × V(cid:91) , where the size of each block is at
most (cid:96)(cid:91) . Note that P(cid:91)Q∼ = E[P(cid:91)Q∼ ] + (P(cid:91)Q∼ − E[P(cid:91)Q∼ ]). Here E[P(cid:91)Q∼ ] is a deterministic
p−t√
√
1
. We thus have
matrix with all non-zero entries equal to
t(1−t)
n
100κ
p − t(cid:112)t(1 − t)
where the last inequality holds under the deﬁnition of (cid:96)(cid:91) in Theorem 2. On the other
hand, the matrix P(cid:91)Q∼ − E[P(cid:91)Q∼ ] is a random matrix whose entries are independent,
bounded almost surely by B := max{c1 , c2} and have zero mean with variance bounded by
1002 κ2n · p(1−p)
t(1−t) . If (cid:96)(cid:91) ≤ n2/3 , we apply part 1 of Lemma 17 to obtain
(cid:115)
(cid:41)
(cid:40)
1
p(1 − p)
√
(cid:107)P(cid:91)Q∼ − E[P(cid:91)Q∼ ](cid:107) ≤ 10 max
(cid:96)(cid:91) log n, (c1 ∨ c2 ) log n
1
(cid:115)
(cid:40)
(cid:32)(cid:114) 1 − t
(cid:114) t
t(1 − t)
100κ
n
p(1 − p)
√
≤ 3
1
1
log n
t(1 − t)
1 − t
n1/3
t
10κ
16
10κ
n
(cid:16)
(cid:17)
where the last inequality follows from t(1 − t) ≥ p(1−q)
n . If (cid:96)(cid:91) ≥ n2/3 ≥ 76, then the
(cid:38) log4 n
16
∨ (1−t)2 log4 n
p(1 − p) ∨ t2 log4 n
1
variance of the entries is bounded by σ2 :=
,
1002 κ2nt(1−t)
(cid:96)(cid:91)
(cid:96)(cid:91)
and σ (cid:38) B log2 n√
(cid:107)P(cid:91)Q∼ − E[P(cid:91)Q∼ ](cid:107) ≤ 10σ(cid:112)(cid:96)(cid:91) ≤ 3
. Hence we can apply part 2 of Lemma 17 to get
(cid:96)(cid:91)
, w.h.p.,
16
16 p(1 − q) (cid:38) log4 n
where in the last inequality we use n ≥ (cid:96)(cid:91) and t(1 − t) ≥ 1
n . We conclude
that (cid:107)P(cid:91)Q∼(cid:107) ≤ (cid:107)E[P(cid:91)Q∼ ](cid:107) + (cid:107)P(cid:91)Q∼ − E[P(cid:91)Q∼ ](cid:107) ≤ 1
32 + 3
16 = 7
32 w.h.p.
(b) P(cid:91)Q(cid:54)∼ is a random matrix supported on V(cid:91) × V(cid:91) , whose entries are independent, zero
1002 κ2n · t2+q−2tq
mean, bounded almost surely by B (cid:48) := max{c1 , c2}, and have variance
1
. If
(1−t)t
(cid:115)
(cid:41)
(cid:40)
(cid:96)(cid:91) ≤ n2/3 , we apply part 1 of Lemma 17 to obtain
t2 + q − 2tq
√
(cid:107)P(cid:91)Q(cid:54)∼(cid:107) ≤ 10 max
(cid:96)(cid:91) log n, (c1 ∨ c2 ) log n
1
(cid:115)
(cid:32)(cid:114) 1 − t
(cid:40)
(cid:114) t
t(1 − t)
n
100κ
t2 + q − 2tq
√
∨
≤ 1
1
log n
1
t(1 − t)
1 − t
n1/3
t
10κ
4
10κ
n
1002 κ2n ·(cid:16) t2+q−2tq
where the last inequality follows from t(1−t) ≥ p(1−q)
n . If (cid:96)(cid:91) ≥ n2/3 ≥ 76, one veriﬁes
(cid:38) log4 n
16
∨ (1−t) log4 n
(1−t)t ∨ t log4 n
that the variance of the entries is bounded by (σ (cid:48) )2 :=
1
(1−t)(cid:96)(cid:91)
t(cid:96)(cid:91)

(cid:33)
log n

log n

,

(cid:33)

(cid:41)

≤ max

,

∨

,

476

(cid:41)

,

(cid:17)

,

Iterative and Active Clustering Without Size Constraints

and σ (cid:48) (cid:38) B (cid:48) log2 n√
(cid:96)(cid:91)

, w.h.p.,

. Hence we can apply part 2 of Lemma 17 to obtain
(cid:107)P(cid:91)Q(cid:54)∼(cid:107) ≤ 10σ (cid:48)(cid:112)(cid:96)(cid:91) ≤ 1
4
16 p(1 − q) (cid:38) log4 n
where in the last inequality we use n ≥ (cid:96)(cid:91) and t(1 − t) ≥ 1
n .
(c) Note that P(cid:93)Q∼ = P(cid:93)Q1 + P(cid:93)Q2 . By construction these two matrices are both
block-diagonal, have independent zero-mean entries which are bounded almost surely by
(cid:96)(cid:93) p and B∼,2 := 2c2
p respectively, and and have variance bounded by σ2∼1 := 1
B∼,1 := 1
and
p(cid:96)2
(cid:93)
σ2∼2 := 4(1−t)
2 respectively. One veriﬁes that σ∼,i (cid:38) B∼,i log2 n
√
c2
for i = 1, 2. We can then
√
p
n
apply part 2 of Lemma 17 to obtain (cid:107)P(cid:93)Q∼(cid:107) ≤ 10(σ∼,1 + σ∼,2 )
n ≤ 1
4 w.h.p.
(d) Note that P(cid:93)Q (cid:54)∼ = P(cid:93)Q3 is a random matrix with independent zero-mean entries
which are bounded almost surely by B(cid:54)∼ := 2c1
1−q and have variance bounded by σ2(cid:54)∼ :=
1 . One veriﬁes that σ(cid:54)∼ ≥ B(cid:54)∼ log2 n√
4t
1−q c2
. We can then apply part 2 of Lemma 17 to obtain
√
n
n ≤ 1
(cid:107)P(cid:93)Q(cid:54)∼(cid:107) ≤ 4σ (cid:54)∼
4 w.h.p.
Property 2:
(cid:13)(cid:13)(cid:13)U U (cid:62) (P(cid:93)Q) + (P(cid:93)Q)U U (cid:62) + U U (cid:62) (P(cid:93)Q)U U (cid:62)(cid:13)(cid:13)(cid:13)∞
Due to the structure of T , we have
(cid:107)PT Q(cid:107)∞ = (cid:107)PT P(cid:93)Q(cid:107)∞ =
(cid:13)(cid:13)(cid:13)∞
(cid:13)(cid:13)(cid:13)U U (cid:62)P(cid:93)Q
(cid:13)(cid:13)(cid:13)U U (cid:62)P(cid:93)Qm
(cid:13)(cid:13)(cid:13)∞ .
3(cid:88)
≤ 3
≤ 3
Now observe that (U U (cid:62)P(cid:93)Qm )(i, j ) = (cid:80)
m=1
n(cid:104)i(cid:105) P(cid:93)Qm (l, j ) is the sum of independent
1
l∈V(cid:104)i(cid:105)
zero-mean random variables with bounded magnitude and variance. Using the Bernstein
inequality in Lemma 19, we obtain that for each (i, j ) and with probability at least 1 − n−8 ,
(cid:115)
(cid:18)(cid:114) 1 − p
(cid:19)
(cid:12)(cid:12)(cid:12) ≤ 10
(cid:12)(cid:12)(cid:12)(U U (cid:62)P(cid:93)Q1 )(i, j )
· (cid:113)
≤ 1
log n
n(cid:104)i(cid:105) log n +
n(cid:104)i(cid:105) (cid:96)(cid:93)
p
p
24κ
(cid:113) log2 n
(cid:13)(cid:13)∞ ≤ 1
By union bound we conclude that (cid:13)(cid:13)U U (cid:62)P(cid:93)Q1
. For i ∈ V(cid:91) , clearly (U U (cid:62)P(cid:93)Q1 )(i, j ) = 0.
where in the last inequality we use p (cid:38) κ2n
(cid:13)(cid:13)U U (cid:62)P(cid:93)Q2
(cid:13)(cid:13)∞ in a similar fashion: for each (i, j ) and with probability
(cid:13)(cid:13)∞ and (cid:13)(cid:13)U U (cid:62)P(cid:93)Q3
(cid:96)2
(cid:93)
w.h.p. We can bound
n(cid:96)(cid:93)
24κ
at least 1 − n−8 :
(cid:18)(cid:114) 1 − p
(cid:19)
(cid:12)(cid:12)(cid:12)(U U (cid:62)P(cid:93)Q2 )(i, j )
(cid:12)(cid:12)(cid:12) ≤ 10
· (cid:113)
(cid:115)
(cid:32)(cid:115)
(cid:33)
n(cid:104)i(cid:105) log n +
p
(1 − p) log n
≤ 15
p(cid:96)(cid:93)
100κ

(cid:115)
(1 + )c2
n(cid:104)i(cid:105)

t
(1 − t)n

log2 n
n(cid:96)(cid:93)

,

log2 n
n(cid:96)(cid:93)

+

log n
(cid:96)(cid:93)p

≤ 1
6κ

, w.h.p.,

log n
p

·

477

Ailon, Chen and Xu

where the last inequality follows from p(1 − t) (cid:38) log n
(cid:18)(cid:114) q
(cid:12)(cid:12)(cid:12)(U U (cid:62)P(cid:93)Q3 )(i, j )
(cid:12)(cid:12)(cid:12) ≤ 10
· (cid:113)
(cid:96)(cid:93)
(cid:32)(cid:115)
(1 + )c1
(cid:114) 1 − t
n(cid:104)i(cid:105) log n +
1 − q
n(cid:104)i(cid:105)
≤ 15
q log n
(1 − q)(cid:96)(cid:93)
100κ
tn

, and

+

·

log n
(cid:96)(cid:93) (1 − q)

(cid:33)
log n
1 − q

(cid:19)

≤ 1
6κ

(cid:115)

log2 n
n(cid:96)(cid:93)

,

=

·

1
κt

log2 n
n(cid:96)(cid:93)

,

c1  ≥ 1
100κ

U U (cid:62) + Q, Ps(A)Ps( ˆB )P(cid:93)∆

log4 n
t(1 − t)(cid:96)(cid:93)
(cid:115)

log4 n
≥ 3
κ
n(cid:96)(cid:93)
(cid:115)

where the last inequality follows from t(1 − q) (cid:38) log n
. On the other hand, under the
(cid:96)(cid:93)
(cid:115)
(cid:115)
(cid:115)
(cid:114) 1 − t
deﬁnition of c1 , c2 and , we have
· 100
tn
(cid:115)
and similarly
log4 n
log2 n
· 100
≥ 3
c2  ≥ 1
t
t(1 − t)(cid:96)(cid:93)
(1 − t)n
It follows that (cid:107)PT Q(cid:107)∞ ≤ 3 ·(cid:0) 1
(cid:1) · 
100κ
n(cid:96)(cid:93)
κ
3 (c1 ∧c2 ) ≤ 
2 (c1 ∧c2 ) w.h.p., proving Property 2).
6 + 1
24 + 1
6
Properties 3(a) and 3(b):
(cid:69)
(cid:68)Ps(A)Ps( ˆB )P(cid:93)Q3 , Ps(A)Ps( ˆB )P(cid:93)∆
(cid:68)
(cid:69)
For 3(a), by construction of Q we have
= (1 + )c1 · (cid:88)
=
P(cid:93)∆(i, j )
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )P(cid:93)∆
(i,j )∈s( ˆB )∩s(A)
= (1 + )c1
where the last equality follows from ∆ ∈ D. Similarly, since
Ps(A)c Ps( ˆB )P(cid:93)Q1 = Ps(A)c Ps( ˆB )P(cid:93) (−U U (cid:62) ),
(cid:68)Ps(A)c Ps( ˆB )P(cid:93)Q2 , Ps(A)c Ps( ˆB )P(cid:93)∆
(cid:69)
(cid:69)
= −(1 + )c2 · (cid:88)
=
P(cid:93)∆(i, j )
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )P(cid:93)∆
(i,j )∈s( ˆB )∩s(A)c
= (1 + )c2
where the last equality again follows from ∆ ∈ D; this proves Property 3(b).
Properties 4(a) and 4(b):

(cid:68)
U U (cid:62) + Q, Ps(A)c Ps( ˆB )P(cid:93)∆

we have

,

.

,

478

Iterative and Active Clustering Without Size Constraints

=

=

(12)

·

≤ c1 .

+

+ (1 + )c2

,

·

1
p(cid:96)(cid:93)

=

100κ
(cid:96)(cid:93)

p − t ≥ p − q
4

(cid:68)
(cid:69)
For 4(a), we have
(cid:68)Ps(A)Ps( ˆB )c P(cid:93)
(cid:16)
(cid:69)
(cid:17)
U U (cid:62) + Q, Ps(A)Ps( ˆB )c P(cid:93)∆
(cid:18) 1
(cid:19)
(cid:88)
, Ps(A)Ps( ˆB )c P(cid:93)∆
U U (cid:62) + P(cid:93)Q1 + P(cid:93)Q2
1 − pij
1 − pij
P(cid:93)∆(i, j )
1
(cid:18) 1
(cid:19) (cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )c P(cid:93)∆
+ (1 + )c2
(cid:13)(cid:13)(cid:13)1
n(cid:104)i(cid:105)
n(cid:104)i(cid:105)
pij
pij
(i,j )∈s( ˆB )c∩s(A)
1 − p
≥ −
p(cid:96)(cid:93)
p
where the last inequality follows from ∆ ∈ D, pij ≥ p and n(cid:104)i(cid:105) ≥ (cid:96)(cid:93) , ∀i ∈ V(cid:93) . Consider the
(cid:115)
(cid:114) 1 − t
(cid:114) n
(cid:114) n
two terms in the parenthesis in (12). For the ﬁrst term, we have
t(1 − t)
t(1 − t)
tn

t(1 − t)
≤ 100κ
1
1002κ2p2n
100κ
(cid:96)(cid:93)
(cid:115)
(cid:41)
(cid:40)
κ(cid:112)b3p(1 − q)n
For the second term in (12), we have the following:
b3p(1 − q) log4 n
(cid:115)
(cid:40)
(cid:112)t(1 − q)
,
(cid:96)(cid:93)
(cid:96)(cid:93)
(cid:112)t(1 − t)
(cid:112)p(1 − t)
√
· max
n
κ
(cid:115)
(cid:40)
(cid:41)
,
(cid:96)(cid:93)
(cid:112)t(1 − t)
√
log4 n
≥8p(1 − t) · 100 max
= 8p(1 − t).
κ
n
t(1 − t)(cid:96)(cid:93)
(cid:113) 1−t
(cid:113) t
1−p
p ≤ (1 − )
A little algebra shows that this implies (1 + )
, or equivalently
1−t
t
1−p
p ≤ (1 − 2)c1 . Substituting back to (12), we conclude that
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )c P(cid:93)∆
(cid:13)(cid:13)(cid:13)1
(cid:69) ≥ − (c1 + (1 − 2)c1 )
(cid:68)
(cid:68)Ps(A)c Ps( ˆB )c P(cid:93)Q3 , Ps(A)c Ps( ˆB )c P(cid:93)∆
(cid:69)
(cid:88)
−(1 + )
P(cid:93)∆(i, j )
c1 qij
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )c P(cid:93)∆
1 − qij
(i,j )∈s(A)c∩s( ˆB )c
≥ −(1 + )
c1 q
1 − q

proving Property 4(a).
(cid:68)
For 4(b), we have
U U (cid:62) + Q, Ps(A)c Ps( ˆB )c P(cid:93)∆

U U (cid:62) + Q, Ps(A)Ps( ˆB )c P(cid:93)∆

≥ 1
max
4
√

log4 n
t(1 − t)(cid:96)(cid:93)

,

(13)

=

b3
4

· p(1 − t) ·

(cid:69)

=

=

,

(cid:96)(cid:93)

(cid:41)

,

(1 + )c2

479

Ailon, Chen and Xu

max

≥ 1
4

where the last inequality follows from qij ≤ q . Consider the factor before the norm in (13).
(cid:115)
(cid:40)
(cid:41)
κ(cid:112)b3p(1 − q)n
Similarly as before, we have
b3p(1 − q) log4 n
t − q ≥ p − q
(cid:115)
(cid:41)
(cid:40)
,
4
(cid:96)(cid:93)
(cid:96)(cid:93)
(cid:112)t(1 − t)
√
log4 n
= 2t(1 − q).
n
κ
t(1 − t)(cid:96)(cid:93)
(cid:113) 1−t
(cid:113) t
1−q ≤ (1 − )
q
A little algebra shows that this implies (1 + )
1−t , or equivalently
t
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )c P(cid:93)∆
(cid:68)
(cid:69) ≥ −(1 − )c2
1−q ≤ (1 − )c2 . Substituting back to (13), we conclude that
q
(1 + )c1
U U (cid:62) + Q, Ps(A)c Ps( ˆB )c P(cid:93)∆

≥ 2t(1 − q) · 100 max

(cid:96)(cid:93)

,

,

proving Property 4(b).
Properties 5 and 6:
Note that P(cid:91)U U (cid:62) = 0 and Ps( ˆB )P(cid:91) = Ps(A)P(cid:91) . These two properties hold by construc-
tion of Q.
We note that Properties (3)-(6) hold deterministically.
Combining the above results and applying the union bound, we conclude that with
probability at least 1 − n−3 , there exists a matrix Q (which is the one constructed and
veriﬁed above) that satisﬁes the properties in Proposition 12, where the probability is with
respect to the randomness in the graph A and the matrix W . Since W is independent of
A, integrating out the randomness in W proves the theorem.

5.2 Proof of Theorem 5

To ease notation, throughout the proof, C denotes a general universal positive constant
that can take diﬀerent values at diﬀerent locations. We let Ω := s(B ∗ ) denote the noise
locations.
Fix κ ≥ 1 and t in the allowed range, let (K, B ) be an optimal solution to (CP), and
assume K is a partial clustering induced by U1 , . . . , Ur for some integer r, and also assume
σmin (K ) = mini∈[r ] |Ui | satisﬁes (5). Let M = σmin (K ). We need a few helpful facts. Note
that from the deﬁnition of t, c1 , c2 ,
(p − q) ≤ c2
= t ≤ p − 1
1
4
c1 + c2
4
We say that a pair of sets Y ⊆ V , Z ⊆ V is cluster separated if there is no pair (y , z ) ∈
Y × Z satisfying y ∼ z .

(p − q) .

(14)

q +

Assumption 13 For al l pairs of cluster-separated sets Y , Z of size at least m := C log n
(p−q)2
each,

| ˆdY ,Z − q | <

(p − q) ,

1
4

(15)

where ˆdY ,Z :=

|(Y ×Z )∩Ω|
|Y |·|Z |

.

480

Iterative and Active Clustering Without Size Constraints

(17)

This is proven by a Hoeﬀding tail bound and a union bound to hold with probability at
least 1 − n−4 . To see why, ﬁx the sizes mY , mZ of |Y |, |Z |, assume mY ≤ mZ w.l.o.g. For
each such choice, there are at most exp{C (mY +mZ ) log n} ≤ exp{2CmZ log n} possibilities
for the choice of sets Y , Z . For each such choice, the probability that (15) does not hold is
exp{−CmY mZ (p − q)2}
(16)
using Hoeﬀding inequality. Hence, as long as mY ≥ m as deﬁned above, using union bound
(over all possibilities of mY , mZ and of Y , Z ) we obtain (15) uniformly. If we also assume
that
M ≥ 3m ,
the implication of Assumption 13 is that it cannot be the case that some Ui contains a
i of size in the range [m, |Ui | − m] such that U (cid:48)
i = Vg ∩ Ui for some g . Otherwise,
subset U (cid:48)
if such a set existed, then we would ﬁnd a strictly better solution to (CP), call it (K (cid:48) , B (cid:48) ),
which is deﬁned so that K (cid:48) is obtained from K by splitting the block corresponding to Ui
i and the other to Ui \ U (cid:48)
into two blocks, one corresponding to U (cid:48)
i . The diﬀerence ∆ between
i and Z := U \ U (cid:48)
the cost of (K, B ) and (K (cid:48) , B (cid:48) ) is (renaming Y := U (cid:48)
i )
∆ = c1 |(Y × Z ) ∩ Ω| − c2 |(Y × Z ) ∩ Ωc | = (c1 + c2 ) ˆdY ,Z |Y | |Z | − c2 |Y | |Z | .
But the sign of ∆ is exactly the sign of ˆdY ,Z − c2
which is strictly negative by (15) and
c1+c2
(14). (We also used the fact that the trace norm part of the utility function is equal for
both solutions: (cid:107)K (cid:48)(cid:107)∗ = (cid:107)K (cid:107)∗ ).
The conclusion is that for each i, the sets (Ui ∩ V1 ), . . . , (Ui ∩ Vk ) must all be of size at
most m, except maybe for at most one set of size at least |Ui | − m. But note that by the
theorem’s assumption,
M > km = (kC log n)/(p − q)2 ,
(18)
so we conclude that not all the sets (Ui ∩ V1 ), . . . , (Ui ∩ Vk ) can be of size at most m. Hence
exactly one of these sets must have size at least |Ui | − m. From this we conclude that there
is a function φ : [r] (cid:55)→ [k ] such that for all i ∈ [r],
|Ui ∩ Vφ(i) | ≥ |Ui | − m .
We now claim that this function is an injection. We will need the following assumption:
Assumption 14 For any 4 pairwise disjoint subsets (Y , Y (cid:48) , Z, Z (cid:48) ) such that (Y ∪ Y (cid:48) ) ⊆ Vi
for some i, (Z ∪ Z (cid:48) ) ⊆ [n] \ Vi , max{|Z |, |Z (cid:48) |} ≤ m, min{|Y |, |Y (cid:48) |} ≥ M − m:
|Y | · |Y (cid:48) | ˆdY ,Y (cid:48) − |Y | · |Z | ˆdY ,Z − |Y (cid:48) | · |Z (cid:48) | ˆdY (cid:48) ,Z (cid:48) >
(|Y | · |Y (cid:48) | − |Y | · |Z | − |Y (cid:48) | · |Z (cid:48) |)
c2
(19)
c1 + c2
The assumption holds with probability at least 1 − n−4 by using Hoeﬀding inequality,
union bounding over all possible sets Y , Y (cid:48) , Z, Z (cid:48) as above. Indeed, notice that for ﬁxed
mY , mY (cid:48) , mZ , mZ (cid:48) (with, say, mY ≥ mY (cid:48) ), and for each tuple Y , Y (cid:48) , Z, Z (cid:48) such that |Y | =
mY , |Y (cid:48) | = mY (cid:48) , |Z | = mZ , |Z (cid:48) | = mZ (cid:48) , the probability that (19) is violated is at most
exp{−C (p − q)2 (mY mY (cid:48) + mY mZ + mY (cid:48) mZ (cid:48) )} .

(20)

481

Ailon, Chen and Xu

Using (17), this is at most

exp{−C (p − q)2 (mY mY (cid:48) )} .
Now notice that the number of possibilities to choose such a 4 tuple of sets is bounded
above by exp{CmY log n}. Assuming

(21)

M ≥ C log n
(p − q)2 ,
and applying a union bound over all possible combinations Y , Y (cid:48) , Z, Z (cid:48) of sizes mY , mY (cid:48) , mZ , mZ (cid:48)
respectively, of which there are at most exp{CmY log n}, we conclude that (19) is violated
for some combination with probability at most
exp{−C (p − q)2mY mY (cid:48) /2}
which is at most exp{−C log n} if

(23)

(22)

M ≥ C log n
(p − q)2 .

(24)

Apply a union bound now over the possible combinations of the tuple (mY , mY (cid:48) , mZ , mZ (cid:48) ),
of which there are at most exp{C log n} to conclude that (19) holds uniformly for all pos-
sibilities of Y , Y (cid:48) , Z, Z (cid:48) with probability at least 1 − n−4 .
Now assume by contradiction that φ is not an injection, so φ(i) = φ(i(cid:48) ) =: j for some
distinct i, i(cid:48) ∈ [r]. Set Y = Ui ∩ Vj , Y (cid:48) = Ui(cid:48) ∩ Vj , Z = Ui \ Y , Z (cid:48) = Ui(cid:48) \ Y (cid:48) . Note that
max{|Z |, |Z (cid:48) |} ≤ m and min{|Y |, |Y (cid:48) |} ≥ M − m by the derivations to this point. Consider
the solution (K (cid:48) , B (cid:48) ) where K (cid:48) is obtained from K by replacing the two blocks corresponding
to Ui , Ui(cid:48) with four blocks: Y , Y (cid:48) , Z, Z (cid:48) . Inequality (19) guarantees that the cost of (K (cid:48) , B (cid:48) )
is strictly lower than that of (K, B ), contradicting optimality of the latter. (Note that we
used the fact that the corresponding contributions (cid:107)K (cid:107)∗ and (cid:107)K (cid:48)(cid:107)∗ to the trace-norm part
of the utility function are equal.)
We can now also conclude that r ≤ k . Fix i ∈ [r]. We show that not too many elements
of Vφ(i) can be contained in V \ {U1 ∪ · · · ∪ Ur }. We need the following assumption.
Assumption 15 For al l pairwise disjoint sets Y , X, Z ⊆ V such that |Y | ≥ M − m, |X | ≥
(cid:19)
(cid:18)|X |
m, (Y ∪ X ) ⊆ Vj for some j ∈ [k ], |Z | ≤ m, Z ∩ Vj = ∅:
(cid:19)
(cid:18)|X |
ˆdx,x − |Y | · |Z | ˆdY ,Z >
2
|X |
(|X | · |Y | +
c2
c1 + c2
2
c1 + c2
The assumption holds with probability at least 1 − n−4 . To see why, ﬁrst notice that
8 (p − q)|X | · |Y | by (5), as long as C2 is large enough. This implies that
|X |/(c1 + c2 ) ≤ 1
(cid:19)
(cid:18)
(cid:18)(cid:18)|X |
(cid:19)
(cid:19)
the RHS of (25) is upper bounded by
(p − q)
p − 1
8
2

|X | · |Y | ˆdX,Y +

− |Y | · |Z |) +

|X | · |Y | +

− |Y | · |Z |

.

(25)

c2
c1 + c

482

(26)

Iterative and Active Clustering Without Size Constraints

Proving that the LHS of (25) (denoted f (X, Y , Z )) is larger than (26) (denoted g(X, Y , Z ))
uniformly w.h.p. can now be easily done as follows. By ﬁxing mY = |Y |, mX = |X |, the
number of combinations for Y , X, Z is at most exp{C (mY + mX ) log n} for some global
C > 0. On the other hand, the probability that f (X, Y , Z ) ≤ g(X, Y , Z ) for any such option
is at most
exp{−C (p − q)2mY mX } .
Hence, by union bounding, the probability that some tuple Y , X, Z of sizes mY , mX , mZ
respectively satisﬁes f (X, Y , Z ) ≤ g(X, Y , Z ) is at most
exp{−C (p − q)2mY /2} ,
which is at most exp{−C log n} assuming
M ≥ C (log n)/(p − q)2 .

(27)

(28)

(29)

Another union bound over the possible choices of mY , mX , mZ proves that (25) holds uni-
formly with probability at least 1 − n−4 .
Now assume, by way of contradiction, that for some i ∈ [r], the set X := Vφ(i) ∩ ([n] \
{U1 ∪ · · · ∪ Ur }) is of size greater than m. Set Y := Vφ(i) ∩ Ui and Z = Ui \ Vφ(i) . Deﬁne
the solution (K (cid:48) , B (cid:48) ) where K (cid:48) is obtained from K by replacing the block corresponding to
Ui = Y ∪ Z in K with two blocks: Y ∪ X and Z . Assumption 15 tells us that the cost of
|X |
(K (cid:48) , B (cid:48) ) is strictly lower than that of (K, B ). Note that the expression
in the RHS of
(25) accounts for the trace norm diﬀerence (cid:107)K (cid:48)(cid:107)∗ − (cid:107)K (cid:107)∗ = |X |.
c1+c2
We are prepared to perform the ﬁnal “cleanup” step. At this point we know that for
each i ∈ [r], the set Ti = Ui ∩ Vφ(i) satisﬁes
ti := |Ti | ≥ max{|Ui | − m, |Vφ(i) | − rm} .
(30)
(To see why ti ≥ |Vφ(i) | − rm, note that at most m elements of Vφ(i) may be contained in
Ui(cid:48) for i(cid:48) (cid:54)= i, and another at most m elements in V \ (U1 ∪ · · · ∪ Ur ).) We are now going to
conclude from this that Ui = Vφ(i) for all i. To that end, let (K (cid:48) , B (cid:48) ) be the feasible solution
to (CP) deﬁned so that K (cid:48) is a partial clustering induced by Vφ(1) , . . . , Vφ(r) . We would like
to argue that if K (cid:54)= K (cid:48) then the cost of (K (cid:48) , B (cid:48) ) is strictly smaller than that of (K, B ).
Fix the value of the collection
(cid:0)mij := |Vφ(i) ∩ Uj |)(cid:1)
Y := ((r, φ(1), . . . , φ(r),
i := |Vφ(i) ∩ (V \ (U1 ∪ · · · ∪ Ur ))|(cid:1)
(cid:0)m(cid:48)
,
i,j∈[r ],i (cid:54)=j
).
i∈[r]
Let β (Y ) denote the number of i (cid:54)= j such that mij > 0 plus the number of i ∈ [r] such
possibilities for K giving rise to Y is exp{C ((cid:80)
i (cid:54)=j mij +(cid:80)
i > 0. We can assume β (Y ) > 0, otherwise Ui = Vφ(i) for all i ∈ [r]. The number of
that m(cid:48)
i mi ) log n}. Fix such a possibility,
and let
Dij = Vφ(i) ∩ Uj , D (cid:48)
i = Vφ(i) ∩ (V \ (U1 ∪ · · · ∪ Ur )) .

483

Ailon, Chen and Xu

δ =c1

m(cid:48)
i ,

+ c1

− c2

|(Dij × Ui ) ∩ Ω| + c1

The diﬀerence δ(K, K (cid:48) ) between the (CP) costs of solutions K and K (cid:48) is given by the
(cid:88)
(cid:88)
(cid:88)
(cid:88)
following expression:
|(Dij1 × Dij2 ) ∩ Ω|
(cid:88)
(cid:88)
(cid:88)
j (cid:54)=i
i
i
j1<j2
j1 ,j2 (cid:54)=i
|((Vφ(i) \ D (cid:48)
i ) ∩ Ω| + c2
i ) × D (cid:48)
|(Dij × Uj ) ∩ Ωc |
(cid:88)
(cid:88)
(cid:88)
(cid:88)
j (cid:54)=i
i
i
|(Dij1 × Dij2 ) ∩ Ωc |
|(Dij × Ui ) ∩ Ωc | − c2
|(Dij × Uj ) ∩ Ω| − (cid:88)
(cid:88)
(cid:88)
(cid:88)
j (cid:54)=i
i
i
j1<j2
j1 ,j2 (cid:54)=i
i ) × D (cid:48)
|((Vφ(i) \ D (cid:48)
i ) ∩ Ωc | − c1
− c2
where the expression (cid:80) m(cid:48)
j (cid:54)=i
i
i
i comes from the trace norm contribution. If the quantity δ(K, K (cid:48) )
(cid:80)
(i) (cid:80)
(cid:80)
j (cid:54)=i |(Dij × Ui ) ∩ Ω| + (cid:80)
is non-positive, then at least one of the following must be true:
(cid:80)
(cid:80)
(cid:80)
(cid:80)
|(Dij1 × Dij2 ) ∩ Ω|
j1<j2
i
i
j1 ,j2 (cid:54)=i
|(Dij1 × Dij2 )|
j (cid:54)=i |(Dij × Ui )| + c2
(cid:80)
(ii) (cid:80)
< c2
j1<j2
i
i
j1 ,j2 (cid:54)=i
c1+c2
c1+c2
(iii) (cid:80)
(cid:80)
(cid:80)
(cid:80)
i )| + 1
i |((Vφ(i) \ D (cid:48)
i ) × D (cid:48)
i ) ∩ Ω| < c1
i |((Vφ(i) \ D (cid:48)
i ) × D (cid:48)
c1+c2
c1+c2
j (cid:54)=i |(Dij × Uj )|.
j (cid:54)=i |(Dij × Uj ) ∩ Ω| > c2
i
i
c1+c2
−C (p − q)2 (cid:88)

Inequality (i) occurs with probability at most
(cid:88)
M
mij
j (cid:54)=i
i
(cid:41)
(cid:40)
−C (p − q)2 (cid:88)
using Hoeﬀding bound; we also used (30). Inequality (ii) occurs with probability at most
4 (p − q) (cid:80)
(cid:80)
i
using Hoeﬀding inequalities. (We also used the fact that the rightmost expression of (ii),
i |((Vφ(i) \ D (cid:48)
i ) × D (cid:48)
i )| due to the theorem
i m(cid:48)
i , is bounded above by 1
1
c1+c2
assumptions.) Inequality (iii) occurs occurs with probability at most (31), using Hoeﬀding
gross estimation, at most exp{((cid:80)
j (cid:54)=i mij + (cid:80)
bounds again.
Now notice that the number of choices of K (cid:48) giving rise to our ﬁxed Y and β (Y ) is, by a
i ) log n}. The assumptions of the theorem
i m(cid:48)
ensure that, using a union bound over all such possibilities K , and then over all options
for β (Y ) and Y , with probability at least 1 − n−4 the diﬀerence δ(K, K (cid:48) ) is positive. This
means that the (CP) cost of K (cid:48) is simultaneously strictly lower than that of K for all K we
have enumerated over.
Taking the theorem’s C1 , C2 large enough to satisfy the requirements above concludes
the proof.

(cid:80)
i m(cid:48)
i .

(31)

(32)

exp

exp

M m(cid:48)
i

484

| ˆdY ,Z − q | <

(ρp(cid:48) − ρq (cid:48) ) ,

1
4

(33)

Iterative and Active Clustering Without Size Constraints

5.3 Proof of Theorem 9

The proof of Theorem 5 in the previous section made repeated use of Hoeﬀding tail inequal-
ities, for uniformly bounding the size of the intersection of the noise support Ω with various
submatrices (with high probability). This is tight for p, q which are bounded away from 0
and 1. However, if p = ρp(cid:48) , q = ρq (cid:48) , the noise probabilities p(cid:48) , q (cid:48) are ﬁxed and ρ tends to 0,
a sharper bound is obtained using Bernstein tail bound (Lemma 18 in see Appendix A.2).
Using Bernstein inequality instead of Hoeﬀding inequality, gives the required result. To see
how this is done, the counterpart of Assumption 13 above is as follows:

Assumption 16 For al l pairs of cluster-separated sets Y , Z of size at least m := C log n
ρ
each,

|(Y ×Z )∩Ω|
where ˆdY ,Z :=
.
|Y |·|Z |
Note: In this section, C (and hence also m) depends on p(cid:48) , q (cid:48) only, which are assumed ﬁxed.
Deﬁning henceforth m as in Assumption 9, Assumption 14 holds with probability at least
1 − n−4 . This can be seen by replacing the Hoeﬀding bound in (20) with a Chernoﬀ bound:
exp{−C (p(cid:48) , q (cid:48) )ρ(mY mY (cid:48) + mY mZ + mY (cid:48) mZ (cid:48) )} .

(34)

The rest of the proof is obtained by a similar step by step technical alteration of the
proof in Section 5.2.

6. Discussion

An immediate future research is to better understand the “mid-size crisis”. Our current
results say nothing about clusters that are neither large nor small, falling in the interval
((cid:96)(cid:91) , (cid:96)(cid:93) ). Our numerical experiments conﬁrm that the mid-size phenomenon is real: they
are neither completely recovered nor entirely ignored by the optimal ˆK . The part of ˆK
restricted to these clusters does not seem to have an obvious pattern. Proving whether we
can still eﬃciently recover large clusters in the presence of mid-size clusters is an interesting
open problem.
Our study was mainly theoretical, focusing on the planted partition model. As such,
our experiments focused on conﬁrming the theoretical ﬁndings with data generated exactly
according to the distribution we could provide provable guarantees for. It would be inter-
esting to apply the presented methodology to real applications, particularly large data sets
merged from web application and social networks.
Another interesting direction is extending the “peeling strategy” to other settings. Our
algorithms use the convex program (CP) as a subroutine, taking advantage of the fact that
the recovery of large clusters via (CP) is not hindered by the presence of small clusters,
and that (CP) has a tunable parameter that controls the sizes of the clusters that are
considered large. It is possible that other clustering routines also have these properties and
thus can be used as a subroutine in our iterative and active algorithms. More generally,
our problem concerns the inference of an unknown structure, and our high-level strategy
is to sequentially infer and remove the “easy” (or low-resolution) part of the problem and

485

Ailon, Chen and Xu

zoom into the “hard” (or high-resolution) part. It is interesting to explore this strategy in
a broader context, and to understand for what problems and under what conditions this
strategy may work.

Acknowledgments

The authors are grateful to the anonymous reviewers for their thorough reviews of this
work and valuable suggestions on improving the manuscript. N. Ailon acknowledges the
support of a Marie Curie International Reintegration Grant PIRG07-GA-2010-268403, and
a grant from Technion-Cornell Innovation Institute (TCII). Y. Chen was supported by NSF
grant CIF-31712-23800 and ONR MURI grant N00014-11-1-0688. The work of H. Xu was
partially supported by the Ministry of Education of Singapore through AcRF Tier Two
grant R265-000-443-112.

Appendix A. Technical Lemmas

In this section we state several lemmas needed in the proofs of our main results.

.

A.1 The Spectral Norm of Random Matrices
Lemma 17 Suppose A ∈ RN ×N is a symmetric matrix, where Aij , 1 ≤ i ≤ j ≤ m are
independent random variables, each of which has mean 0 and variance at most σ2 and is
bounded in absolute value by B a.s.
(cid:110)
(cid:111)
σ(cid:112)N log n, B log n
1. If n ≥ N , then with probability at least 1 − n−6 , the ﬁrst singular value A satisﬁes
λ1 (A) ≤ 10 max
2. If further n ≥ N ≥ 76, N ≥ n2/3 and σ ≥ c1
B log2 n√
N
then with probability at least 1 − n−6 , we have
√
λ1 (A) ≤ 10σ
N .
are zero-mean random matrices independent of each other, and A = (cid:80)
Proof We ﬁrst prove part 1 of the lemma. Let ei be the i-th standard basis in RN . Deﬁne
for i ∈ [N ]. Then the Zij ’s
for 1 ≤ i < j ≤ N , and Zii = Aiieie(cid:62)
j + Aj iej e(cid:62)
Zij = Aij eie(cid:62)
i
i
1≤i≤j≤N Zij . We
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ N σ2 .
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) =
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
have (cid:107)Zij (cid:107) ≤ B almost surely. We also have
(cid:88)
(cid:88)
(cid:88)
(cid:88)
eie(cid:62)
ii )eie(cid:62)
E(Zij Z (cid:62)
E(A2
E(A2
i +
ij )
ij )
i
Similarly, we have (cid:107) (cid:80)
1≤i≤N
1≤i≤N
1≤i≤j≤N
j :j (cid:54)=i
ity (Theorem 1.6 in Tropp 2012) with t = 10 max (cid:8)σ
N log n, B log n(cid:9) yields the desired
ij Zij )(cid:107) ≤ N σ2 . Applying the Matrix Bernstein Inequal-
E(Z (cid:62)
√
1≤i≤j≤N
bound.

for some absolute constant c1 > 0,

486

Iterative and Active Clustering Without Size Constraints

(cid:20) 0 A
(cid:21)
We turn to part 2 of the lemma. Let A(cid:48) be an independent copy of A, and deﬁne
A(cid:48)
0
Note that ¯A is an 2N × 2N random matrix with i.i.d. entries. If σ ≥ c1
B log2 n√
for some
N
√
suﬃciently large absolute constant c1 > 0, then by Theorem 3.1 in Achlioptas and Mcsherry
(2007) we know that with probability at least 1 − n−6 , λ1 ( ¯A) ≤ 10σ
N . The lemma follows
from noting that λ1 (A) ≤ λ1 ( ¯A).

¯A :=

.

Pr

Yi

.

Yi − E

A.2 Standard Bernstein Inequality for the Sum of Independent Variables
Lemma 18 ( Bernstein inequality) Let Y1 , . . . , YN be independent random variables, each
of which has variance bounded by σ2 and is bounded in absolute value by B a.s.. Then we
(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) N(cid:88)
(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > t
(cid:34) N(cid:88)
(cid:35)
(cid:27)
(cid:26)
have that
i=1
i=1
The following lemma is an immediate consequence of Lemma 18.
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) N(cid:88)
(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 10
Lemma 19 Let Y1 , . . . , YN be independent random variables, each of which has variance
(cid:34) N(cid:88)
(cid:17)
(cid:16)
σ(cid:112)N log n + B log n
bounded by σ2 and is bounded in absolute value by B a.s. Then we have
i=1
i=1
with probability at least 1 − 2n−8 .

t2/2
N σ2 + B t/3

≤ 2 exp

Yi − E

Yi

References

Dimitris Achlioptas and Frank Mcsherry. Fast computation of low-rank matrix approxima-
tions. Journal of the ACM, 54(2):9, 2007.

Nir Ailon, Yudong Chen, and Huan Xu. Breaking the small cluster barrier of graph clus-
tering. In Proceedings of International Conference on Machine Learning (ICML), pages
995–1003, 2013.

Nir Ailon, Ron Begleiter, and Esther Ezra. Active learning using smooth relative regret
approximations with applications. Journal of Machine Learning Research, 15:885–920,
2014.

Brendan P. W. Ames and Stephen A. Vavasis. Nuclear norm minimization for the planted
clique and biclique problems. Mathematical Programming, 129(1):69–89, 2011.

Anima Anandkumar, Rong Ge, Daniel Hsu, and Sham M. Kakade. A tensor spectral
approach to learning mixed membership community models. Journal of Machine Learning
Research, 15:2239–2312, June 2014.

487

Ailon, Chen and Xu

Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine Learning,
56:89–113, 2004.

B´ela Bollob´as and Alex D. Scott. Max cut for random graphs with a planted partition.
Combinatorics, Probability and Computing, 13(4-5):451–474, 2004.

Ravi B. Boppana. Eigenvalues and graph bisection: an average-case analysis. In Proceedings
of Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 280–
285, 1987.

Emmanuel J. Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component
analysis? Journal of the ACM, 58:1–37, 2011.

Ted Carson and Russell Impagliazzo. Hill-climbing ﬁnds random planted bisections.
In
Proceedings of the 12th Annual Symposium on Discrete Algorithms, pages 903–909, 2001.

Venkat Chandrasekaran, Sujay Sanghavi, Pablo Parrilo, and Alan Willsky. Rank-sparsity
incoherence for matrix decomposition. SIAM Journal on Optimization, 21(2):572–596,
2011.

Kamalika Chaudhuri, Fan Chung, and Alexander Tsiatas. Spectral clustering of graphs
with general degrees in the extended planted partition model. In Proceedings of the 25th
Annual Conference on Learning Theory (COLT), pages 35.1–35.23, 2012.

Yudong Chen, Sujay Sanghavi, and Huan Xu. Clustering sparse graphs. In Advances in
Neural Information Processing Systems 25, pages 2204–2212. Curran Associates, Inc.,
2012.

Yudong Chen, Ali Jalali, Sujay Sanghavi, and Huan Xu. Clustering partially observed
graphs via convex optimization. Journal of Machine Learning Research, 15:2213–2238,
June 2014a.

Yudong Chen, Sujay Sanghavi, and Huan Xu. Improved graph clustering. IEEE Transac-
tions on Information Theory, 60(10):6440–6455, 2014b.

Anne Condon and Richard M. Karp. Algorithms for graph partitioning on the planted
partition model. Random Structures and Algorithms, 18(2):116–140, 2001.

Brian Eriksson, Gautam Dasarathy, Aarti Singh, and Robert Nowak. Active clustering: ro-
bust and eﬃcient hierarchical clustering using adaptively selected similarities. In Proceed-
ings of International Conference on Artiﬁcial Intel ligence and Statistics, pages 260–268,
2011.

Martin Ester, Hans-Peter Kriegel, and Xiaowei Xu. A database interface for clustering in
large spatial databases.
In Proceedings of 1st International Conference on Know ledge
Discovery and Data Mining (KDD), 1995.

Ioannis Giotis and Venkatesan Guruswami. Correlation clustering with a ﬁxed number of
clusters. Theory of Computing, 2(1):249–266, 2006.

488

Iterative and Active Clustering Without Size Constraints

Paul W. Holland, Kathryn B. Laskey, and Samuel Leinhardt. Stochastic blockmodels: some
ﬁrst steps. Social networks, 5(2):109–137, 1983.

Ali Jalali, Yudong Chen, Sujay Sanghavi, and Huan Xu. Clustering partially observed
graphs via convex optimization. In Proceedigns of the 28th International Conference on
Machine Learning, pages 1001–1008, 2011.

Akshay Krishnamurthy and Aarti Singh. Low-rank matrix and tensor completion via adap-
tive sampling. In Advances in Neural Information Processing Systems 26, pages 836–844,
2013.

Akshay Krishnamurthy, Sivaraman Balakrishnan, Min Xu, and Aarti Singh. Eﬃcient active
algorithms for hierarchical clustering. In Proceedings of the 29th International Conference
on Machine Learning (ICML), pages 887–894, 2012.

Amit Kumar and Ravindran Kannan. Clustering with spectral norm and the k-means
algorithm. In Proceedings of 51st Annual IEEE Symposium on Foundations of Computer
Science (FOCS), pages 299–308, 2010.

Zhouchen Lin, Risheng Liu, and Zhixun Su. Linearized alternating direction method with
adaptive penalty for low-rank representation. In Advances in Neural Information Pro-
cessing Systems 24, pages 612–620, 2011.

Claire Mathieu and Warren Schudy. Correlation clustering with noisy input. In Proceedings
of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, pages 712–
728. SIAM, 2010.

Frank McSherry. Spectral partitioning of random graphs. In Proceedings of 42nd IEEE
Symposium on Foundations of Computer Science, pages 529–537, 2001.

Nina Mishra, Robert Schreiber, Isabelle Stanton, and Robert E. Tarjan. Clustering social
networks. In Algorithms and Models for the Web-Graph, pages 56–67. Springer, 2007.

Samet Oymak and Babak Hassibi. Finding dense clusters via low rank + sparse decompo-
sition. arXiv:1104.5186v1, 2011.

Karl Rohe, Sourav Chatterjee, and Bin Yu. Spectral clustering and the high-dimensional
stochastic block model. Annals of Statistics, 39:1878–1915, 2011.

Ohad Shamir and Naftali Tishby. Spectral clustering on a budget. In Proceedings of the 14th
International Conference on Artiﬁcial Intel ligence and Statistics, pages 661–669, 2011.

Ron Shamir and Dekel Tsur. Improved algorithms for the random cluster graph model.
Random Structure and Algorithm, 31(4):418–449, 2007.

Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of
Computational Mathematics, 12(4):389–434, 2012.

Konstantin Voevodski, Maria-Florina Balcan, Heiko R¨oglin, Shang-Hua Teng, and Yu Xia.
Active clustering of biological sequences. Journal of Machine Learning Research, 13:
203–225, 2012.

489

Ailon, Chen and Xu

Huan Xu, Constantine Caramanis, and Sujay Sanghavi. Robust PCA via outlier pursuit.
IEEE Transactions on Information Theory, 58(5):3047–3064, 2012.

Yahoo!-Inc. Graph partitioning. http://research.yahoo.com/project/2368, 2009.

Yunpeng Zhao, Elizaveta Levina, and Ji Zhu. Community extraction for social networks.
Proceedings of the National Academy of Sciences, 108(18):7321–7326, 2011.

490

Journal of Machine Learning Research 16 (2015) 77-102

Submitted 7/14; Published 1/15

Statistical Topological Data Analysis using Persistence
Landscapes

Peter Bubenik
Department of Mathematics
Cleveland State University
Cleveland, OH 44115-2214, USA

Editor: David Dunson

peter.bubenik@gmail.com

Abstract

We deﬁne a new topological summary for data that we call the persistence landscape. Since
this summary lies in a vector space, it is easy to combine with tools from statistics and
machine learning, in contrast to the standard topological summaries. Viewed as a random
variable with values in a Banach space, this summary obeys a strong law of large numbers
and a central limit theorem. We show how a number of standard statistical tests can be
used for statistical inference using this summary. We also prove that this summary is
stable and that it can be used to provide lower bounds for the bottleneck and Wasserstein
distances.

Keywords:
topological data analysis, statistical topology, persistent homology, topolog-
ical summary, persistence landscape

1. Introduction

Topological data analysis (TDA) consists of a growing set of methods that provide insight
to the “shape” of data (see the surveys Ghrist, 2008; Carlsson, 2009). These tools may
be of particular use in understanding global features of high dimensional data that are not
readily accessible using other techniques. The use of TDA has been limited by the diﬃculty
of combining the main tool of the sub ject, the barcode or persistence diagram with statistics
and machine learning. Here we present an alternative approach, using a new summary that
we call the persistence landscape. The main technical advantage of this descriptor is that
it is a function and so we can use the vector space structure of its underlying function
space. In fact, this function space is a separable Banach space and we apply the theory of
random variables with values in such spaces. Furthermore, since the persistence landscapes
are sequences of piecewise-linear functions, calculations with them are much faster than
the corresponding calculations with barcodes or persistence diagrams, removing a second
serious obstruction to the wider use of topological methods in data analysis.

Notable successes of TDA include the discovery of a subgroup of breast cancers by
Nicolau et al. (2011), an understanding of the topology of the space of natural images by
Carlsson et al. (2008) and the topology of orthodontic data by Heo et al. (2012), and the
detection of genes with a periodic proﬁle by Dequ´eant et al. (2008). De Silva and Ghrist
(2007b,a) used topology to prove coverage in sensor networks.

c(cid:13)2015 Peter Bubenik.

Bubenik

In the standard paradigm for TDA, one starts with data that one encodes as a ﬁnite
set of points in Rn or more generally in some metric space. Then one applies some ge-
ometric construction to which one applies tools from algebraic topology. The end result
is a topological summary of the data. The standard topological descriptors are the bar-
code and the persistence diagram (Edelsbrunner et al., 2002; Zomorodian and Carlsson,
2005; Cohen-Steiner et al., 2007), which give a multiscale representation of the homology
(Hatcher, 2002) of the geometric construction. Roughly, homology in degree 0 describes
the connectedness of the data; homology in degree 1 detects holes or tunnels; homology
in degree 2 captures voids; and so on. Of particular interest are the homological features
that persist as the resolution changes. We will give precise deﬁnitions and an illustrative
example of this method, called persistent homology or topological persistence, in Section 2.
Now let us take a statistical view of this paradigm. We consider the data to be sampled
from some underlying abstract probability space. Composing the constructions above, we
consider our topological summary to be a random variable with values in some summary
space S . In detail, the probability space (Ω, F , P ) consists of a sample space Ω, a σ -algebra
F of events, and a probability measure P . Composing our constructions gives a function
X : (Ω, F , P ) → (S , A, P∗ ), where S is the summary space, which we assume has some
metric, A is the corresponding Borel σ -algebra, and P∗ is the probability measure on S
obtained by pushing forward P along X . We assume that X is measurable and thus X is
a random variable with values in S .
Here is a list of what we would like to be able to do with our topological summary. Let
X1 , . . . , Xn be a sample of independent random variables with the same distribution as X .
We would like to have a good notion of the mean µ of X and the mean X n of the sample;
know that X n converges to µ; and be able to calculate X n (ω), for ω ∈ Ω, eﬃciently. We
would like to have information the diﬀerence X n − µ, and be able to calculate approximate
conﬁdence intervals related to µ. Given two such samples for random variables X and
Y with values in our summary space, we would like to be able to test the hypothesis
that µX = µY . In order to answer these questions we also need an eﬃcient algorithm for
calculating distances between elements of our summary space. In this article, we construct a
topological summary that we call the persistence landscape which meets these requirements.
Our basic idea is to convert the barcode into a function in a somewhat additive manner.
The are many possible variations of this construction that may result in more suitable
summary statistics for certain applications. Hopefully, the theory presented here will also
be helpful in those situations.
We remark that while the persistence landscape has a corresponding barcode and persis-
tence diagram, the mean persistence landscape does not. This is analogous to the situation
in which an integer-valued random variable having a Poisson distribution has a summary
statistic, the rate parameter, that is not an integer.
We also remark that the reader may restrict our Banach space results results to the
perhaps more familiar Hilbert space setting. However we will need this generality to prove
stability of the persistence landscape for, say, functions on the n-dimensional sphere where
n > 2.
There has been progress towards combining the persistence diagram and statistics (Mi-
leyko et al., 2011; Turner et al., 2014; Munch et al., 2013; Chazal et al., 2013; Fasy et al.,
2014). Blumberg et al. (2014) give a related statistical approach to TDA. Kovacev-Nikolic

78

Persistence Landscapes

et al. (2014) use the persistence landscape deﬁned here to study the maltose binding complex
and Chazal et al. (2014) apply the bootstrap to the persistence landscape. The persistence
landscape is related to the well group deﬁned by Edelsbrunner et al. (2011).
In Section 2 we provide the necessary background and deﬁne the persistence landscape
and give some of its properties. In Section 3 we introduce the statistical theory of persistence
landscapes, which we apply to a few examples in Section 4. In Section 5 we prove that the
persistence landscape is stable and that it provides lower bounds for the previously deﬁned
bottleneck and Wasserstein distances.

2. Topological Summaries

The two standard topological summaries of data are the barcode and the persistence diagram.
We will deﬁne a new closely-related summary, the persistence landscape, and then compare
it to these two previous summaries. All of these summaries are derived from the persistence
module, which we now deﬁne.

2.1 Persistence Modules

The main algebraic ob ject of study in topological data analysis is the persistence module.
A persistence module M consists of a vector space Ma for all a ∈ R and linear maps
M (a ≤ b) : Ma → Mb for all a ≤ b such that M (a ≤ a) is the identity map and for all
a ≤ b ≤ c, M (b ≤ c) ◦ M (a ≤ b) = M (a ≤ c).
There are many ways of constructing a persistence module. One example starts with a
set of points X = {x1 , . . . , xn} in the plane M = R2 as shown in the top left of Figure 1.
resulting union, Xr = (cid:83)n
To help understand this conﬁguration, we “thicken” each point, by replacing each point,
x, with Bx (r) = {y ∈ M | d(x, y) ≤ r}, a disk of ﬁxed radius, r, centered at x. The
i=1 Br (xi ), is shown in Figure 1 for various values of r. For each
r, we can calculate H (Xr ), the homology of the resulting union of disks. To be precise,
H (−) denotes Hk (−, F), the singular homology functor in degree k with coeﬃcients in a
ﬁeld F. So H (Xr ) is a vector space that is the quotient of the k-cycles modulo those that
are boundaries. As r increases, the union of disks grows, and the resulting inclusions induce
maps between the corresponding homology groups. More precisely, if r ≤ s, the inclusion
r : Xr (cid:44)→ Xs induces a map H (ιs
r ) : H (Xr ) → H (Xs ). The images of these maps are the
ιs
persistent homology groups. The collection of vector spaces H (Xr ) and linear maps H (ιs
r )
is a persistence module. Note that this construction works for any set of points in Rn or
more generally in a metric space.
The union of balls Xr has a nice combinatorial description. The ˇCech complex, ˇCr (X ),
of the set of balls {Bxi (r)} is the simplicial complex whose vertices are the points {xi} and
whose k-simplices correspond to k + 1 balls with nonempty intersection (see Figure 1). This
is also called the nerve. It is a basic result that if the ambient space is Rn , Xr is homotopy
equivalent to its ˇCech complex (Borsuk, 1948). So to obtain the singular homology of the
union of balls, one can calculate the simplicial homology of the corresponding ˇCech complex.
The ˇCech complexes { ˇCr (X )} together with the inclusions ˇCr (X ) ⊆ ˇCs (X ) for r ≤ s form a
ﬁltered simplicial complex. Applying simplicial homology we obtain a persistence module.
There exist eﬃcient algorithms for calculating the persistent homology of ﬁltered simplicial
complexes (Edelsbrunner et al., 2002; Milosavljevi´c et al., 2011; Chen and Kerber, 2013).

79

Bubenik

Figure 1: A growing union of balls and the 1-skeleton of the corresponding ˇCech complex.
As the radius grows, features—such as connected components and holes—appear
and disappear. Here, the complexes illustrate the births and deaths of three holes,
homology classes in degree one. The corresponding birth-death pairs are plotted
as part of the top left of Figure 2.

The ˇCech complex is often computationally expensive, so many variants have been used
in computational topology. A larger, but simpler complex called the Rips complex has as
vertices the points xi and has k-simplices corresponding to k + 1 balls with all pairwise
intersections nonempty. Other possibilities include the witness complexes of de Silva and
Carlsson (2004), graph induced complexes by Dey et al. (2013) and complexes built using
kernel density estimators and triangulations of the ambient space (Bubenik et al., 2010).
Some of these are used in the examples in Section 4.
Given any real-valued function f : S → R on a topological space S , we can deﬁne the
associated persistence module, M (f ), where M (f )(a) = H (f −1 ((∞, a])) and M (f )(a ≤ b)
is induced by inclusion. Taking f to be the the minimum distance to a ﬁnite set of points,
X , we obtain the ﬁrst example.

2.2 Persistence Landscapes

In this section we deﬁne a number of functions derived from a persistence module. Examples
of each of these are given in Figure 2.
Let M be a persistence module. For a ≤ b, the corresponding Betti number of M , is
given by the dimension of the image of the corresponding linear map. That is,
β a,b = dim(im(M (a ≤ b))).

(1)

Lemma 1 If a ≤ b ≤ c ≤ d then β b,c ≥ β a,d .

Proof Since M (a ≤ d) = M (c ≤ d) ◦ M (b ≤ c) ◦ M (a ≤ b), this follows from (1).
Our simplest function, which we call the rank function is the function λ : R2 → R given
(cid:40)
β b,d
0

if b ≤ d
otherwise.

by

λ(b, d) =

80

Persistence Landscapes

Now let us change coordinates so that the resulting function is supported on the upper
half plane. Let
d − b
b + d
m =
,
and h =
.
2
2
(cid:40)
The rescaled rank function is the function λ : R2 → R given by
if h ≥ 0
βm−h,m+h
0
otherwise.

λ(m, h) =

(2)

Much of our theory will apply to these simple functions. However, the following version,
which we will call the persistence landscape, will have some advantages.
First let us observe that for a ﬁxed t ∈ R, β t−•,t+• is a decreasing function. That is,

Lemma 2 For 0 ≤ h1 ≤ h2 ,

β t−h1 ,t+h1 ≥ β t−h2 ,t+h2 .
Proof Since t − h2 ≤ t − h1 ≤ t + h1 ≤ t + h2 , by Lemma 1, β t−h2 ,t+h2 ≤ β t−h1 ,t+h1 .

Deﬁnition 3 The persistence landscape is a function λ : N × R → R, where R denotes
the extended real numbers, [−∞, ∞]. Alternatively, it may be thought of as a sequence of
functions λk : R → R, where λk (t) = λ(k , t). Deﬁne
λk (t) = sup(m ≥ 0 | β t−m,t+m ≥ k).

The persistence landscape has the following properties.
1. λk (t) ≥ 0,
Lemma 4
2. λk (t) ≥ λk+1 (t), and

3. λk is 1-Lipschitz.

The ﬁrst two properties follow directly from the deﬁnition. We prove the third in the
appendix.
To help visualize the graph of λ : N × R → R, we can extend it to a function λ : R2 → R
(cid:40)
by setting
λ((cid:100)x(cid:101), t),
if x > 0,
if x ≤ 0.
0,
We remark that the non-persistent Betti numbers, {dim(M (t))}, of a persistence module
M can be read oﬀ from the diagonal of the rank function, the m-axis of the rescaled rank
function, and from the support of the persistence landscape.

λ(x, t) =

(3)

81

Bubenik

Figure 2: Persistence landscapes for the homology in degree 1 of the example in Figure 1.
For the rank function (top left) and rescaled rank function (top right) the values
of the functions on the corresponding region are given. The top left graph also
contains the three points of the corresponding persistence diagram. Below the
top right graph is the corresponding barcode. We also have the corresponding
persistence landscape (bottom left) and its 3d-version (bottom right). Notice
that λ1 gives a measure of the dominant homological feature at each point of the
ﬁltration.

82

24681012246810120birthdeath11223002468101220λ1λ2λ32468101220112230Persistence Landscapes

Figure 3: Means of persistence diagrams and persistence landscapes. Top left: the rescaled
persistence diagrams {(6, 6), (10, 6)} and {(8, 4), (8, 8)} have two (Fr´echet) means:
{(7, 5), (9, 7)} and {(7, 7), (9, 5)}.
In contrast their corresponding persistence
landscapes (top right and bottom left) have a unique mean (bottom right).

2.3 Barcodes and Persistence Diagrams

All of the information in a (tame) persistence module is completely contained in a multiset
of intervals called a barcode (Zomorodian and Carlsson, 2005; Crawley-Boevey, 2012; Chazal
et al., 2012). Mapping each interval to its endpoints we obtain the persistence diagram.
There exist maps in both directions between these topological summaries and our func-
tions. For an example of corresponding persistence diagrams, barcodes and persistence
landscapes, see Figure 2. Informally, the persistence diagram consists of the “upper-left
corners” in our rank function. In the other direction, λ(b, d) counts the number of points
in the persistence diagram in the upper left quadrant of (b, d). Informally, the barcode con-
sists of the “bases of the triangles” in the rescaled rank function, and the other direction is
obtained by “stacking isosceles triangles” whose bases are the intervals in the barcode. We
invite the reader to make the mappings precise. For example, given a persistence diagram
{(bi , di )}n
i=1 ,

λk (t) = kth largest value of min(t − bi , di − t)+ ,

where c+ denotes max(c, 0). The fact that barcodes are a complete invariant of persistence
modules is central to these equivalences.
The geometry of the space of persistence diagrams makes it hard to work with. For
example, sets of persistence diagrams need not have a unique (Fr´echet) mean (Mileyko
et al., 2011).
In contrast, the space of persistence landscapes is very nice. So a set of
persistence landscapes has a unique mean (4). See Figure 3.
Compared to the persistence diagram, the barcode has extra information on whether or
not the endpoints of the intervals are included. This ﬁner information is seen in the rank

83

24681012141624681002468101214162468100λ1λ22468101214162468100λ1λ22468101214162468100λ1λ2Bubenik

function and rescaled rank function, but not in the persistence landscape. However when
we pass to the corresponding Lp space in Section 2.4, this information disappears.
2.4 Norms for Persistence Landscapes
everywhere, for 1 ≤ p < ∞, (cid:107)f (cid:107)p = (cid:2)(cid:82) |f |pdµ(cid:3) 1
Recall that for a measure space (S , A, µ), and a function f : S → R deﬁned µ-almost
p , and (cid:107)f (cid:107)∞ = ess sup f = inf {a | µ{s ∈
S | f (s) > a} = 0}. For 1 ≤ p ≤ ∞, Lp (S ) = {f : S → R | (cid:107)f (cid:107)p < ∞} and deﬁne
Lp (S ) = Lp (S )/ ∼, where f ∼ g if (cid:107)f − g(cid:107)p = 0.
On R and R2 we will use the Lebesgue measure. On N × R, we use the product of the
∞(cid:88)
counting measure on N and the Lebesgue measure on R. For 1 ≤ p < ∞ and λ : N × R → R,
(cid:107)λk (cid:107)p
(cid:107)λ(cid:107)p
p =
p ,
k=1
where λk (t) = λ(k , t). By Lemma 4(2), (cid:107)λ(cid:107)∞ = (cid:107)λ1(cid:107)∞ . If we extend f to λ : R2 → R, as
in (3), we have (cid:107)λ(cid:107)p = (cid:107)λ(cid:107)p , for 1 ≤ p ≤ ∞.
If λ is any of our functions corresponding to a barcode that is a ﬁnite collection of ﬁnite
intervals, then λ ∈ Lp (S ) for 1 ≤ p ≤ ∞, where S equals N × R or R2 .
Let λbd and λmh denote the rank function and the rescaled rank function corresponding
to a persistence landscape λ, and let D be the corresponding persistence diagram. Let
pers2 (D) denote the sum of the squares of the lengths of the intervals in the corresponding
barcode, and let pers∞ (D) be the length of the longest interval.
1. (cid:107)λ(cid:107)1 = (cid:107)λmh(cid:107)1 = 1
2 (cid:107)λbd(cid:107)1 = 1
Proposition 5
4 pers2 (D), and
2. (cid:107)λ(cid:107)∞ = (cid:107)λ1(cid:107)∞ = 1
2 pers∞ (D).
Proof
1. To see that (cid:107)λ(cid:107)1 = (cid:107)λmh(cid:107) we remark that both are the volume of the same solid. The
change of coordinates implies that (cid:107)λmh(cid:107)1 = 1
2 (cid:107)λbd(cid:107)1 . If D = {(bi , di )}, then each
i (2hi )2 = 4 (cid:80)
i . Finally, pers2 (D) = (cid:80)
(cid:80)
i to the volume (cid:107)λmh(cid:107)1 , where hi = di−bi
. So (cid:107)λmh(cid:107)1 =
point (bi , di ) contributes h2
2
i h2
i h2
i .
2. Lemma 4(2) implies that (cid:107)λ(cid:107)∞ = (cid:107)λ1(cid:107)∞ . If D = {(bi , di )}, then (cid:107)λ(cid:107)∞ = supi

.

di−bi
2

We remark that the quantities in 1 and 2 also equal W2 (D , ∅)2 and W∞ (D , ∅) respectively
(see Section 5 for the corresponding deﬁnitions).

3. Statistics with Landscapes

Now let us take a probabilistic viewpoint. First, we assume that our persistence landscapes
lie in Lp (S ) for some 1 ≤ p < ∞, where S equals N × R or R2 . In this case, Lp (S ) is a
separable Banach space. When p = 2 we have a Hilbert space; however, we will not use this
structure. In some examples, the persistence landscapes will only be stable for some p > 2
(see Theorem 16).

84

Persistence Landscapes

3.1 Landscapes as Banach Space Valued Random Variables
Let X be a random variable on some underlying probability space (Ω, F , P ), with corre-
sponding persistence landscape Λ, a Borel random variable with values in the separable
Banach space Lp (S ). That is, for ω ∈ Ω, X (ω) is the data and Λ(ω) = λ(X (ω)) =: λ is the
corresponding topological summary statistic.
Now let X1 , . . . , Xn be independent and identically distributed copies of X , and let
Λ1 , . . . , Λn be the corresponding persistence landscapes. Using the vector space structure
of Lp (S ), the mean landscape Λ
n
n
n
n(cid:88)
is given by the pointwise mean. That is, Λ
(ω) = λ
,
where
i=1

λi (k , t).

1
n

n
λ

(k , t) =

(4)

Let us interpret the mean landscape. If B1 , . . . , Bn are the barcodes corresponding to the
persistence landscapes λ1 , . . . , λn , then for k ∈ N and t ∈ R, λ
n
(k , t) is the average value
of the largest radius interval centered at t that is contained in k intervals in the barcodes
B1 , . . . , Bn .
For those used to working with persistence diagrams, it is tempting to try to ﬁnd
a persistence diagram whose persistence landscape is closest to a given mean landscape.
While this is an interesting mathematical question, we would like to suggest that the more
important practical issue is using the mean landscape to understand the data.
We would like to be able to say that the mean landscape converges to the expected
persistence landscape. To say this precisely we need some notions from probability in
Banach spaces.

3.2 Probability in Banach Spaces

Here we present some results from probability in Banach spaces. For a more detailed
exposition we refer the reader to Ledoux and Talagrand (2011).
Let B be a real separable Banach space with norm (cid:107)·(cid:107). Let (Ω, F , P ) be a probability
space, and let V : (Ω, F , P ) → B be a Borel random variable with values in B . The
V−→ B (cid:107)·(cid:107)−−→ R is a real-valued random variable. Let B∗ denote the
composite (cid:107)V (cid:107) : Ω
topological dual space of continuous linear real-valued functions on B . For f ∈ B∗ , the
V−→ B f−→ R is a real-valued random variable.
given by E (Y ) = (cid:82) Y dP = (cid:82)
composite f (V ) : Ω
For a real-valued random variable Y : (Ω, F , P ) → R, the mean or expected value, is
Ω Y (ω) dP (ω). We call an element E (V ) ∈ B the Pettis
integral of V if E (f (V )) = f (E (V )) for all f ∈ B∗ .
Proposition 6 If E (cid:107)V (cid:107) < ∞, then V has a Pettis integral and (cid:107)E (V )(cid:107) ≤ E (cid:107)V (cid:107).
Now let (Vn )n∈N be a sequence of independent copies of V . For each n ≥ 1, let Sn =
V1 + · · · + Vn . For a sequence (Yn ) of B-valued random variables, we say that (Yn ) converges
almost surely to a B-valued random variable Y , if P (limn→∞ Yn = Y ) = 1.
n Sn ) → E (V ) almost surely if and only
Theorem 7 (Strong Law of Large Numbers) ( 1
if E (cid:107)V (cid:107) < ∞.

85

Bubenik

For a sequence (Yn ) of B-valued random variables, we say that (Yn ) converges weakly
to a B-valued random variable Y , if limn→∞ E (ϕ(Yn )) = E (ϕ(Y )) for all bounded contin-
uous functions ϕ : B → R. A random variable G with values in B is said to be Gaus-
sian if for each f ∈ B∗ , f (G) is a real valued Gaussian random variable with mean zero.
The covariance structure of a B-valued random variable, V , is given by the expectations
E [(f (V ) − E (f (V )))(g(V ) − E (g(V )))], where f , g ∈ B ∗ . A Gaussian random variable is
determined by its covariance structure. From Hoﬀmann-Jørgensen and Pisier (1976) we
have the following.
Theorem 8 (Central Limit Theorem) Assume that B has type 2. (For example B =
Lp (S ), with 2 ≤ p < ∞.) If E (V ) = 0 and E ((cid:107)V (cid:107)2 ) < ∞ then 1√
n Sn converges weakly to a
Gaussian random variable G(V ) with the same covariance structure as V .

3.3 Convergence of Persistence Landscapes

Now we will apply the results of the previous section to persistence landscapes.
Theorem 7 directly implies the following.

Theorem 9 (Strong Law of Large Numbers for persistence landscapes)
n → E (Λ) almost surely if and only if E (cid:107)Λ(cid:107) < ∞.
Λ
Theorem 10 (Central Limit Theorem for peristence landscapes) Assume p ≥ 2.
√
If E (cid:107)Λ(cid:107) < ∞ and E ((cid:107)Λ(cid:107)2 ) < ∞ then
n − E (Λ)] converges weakly to a Gaussian
n[Λ
random variable with the same covariance structure as Λ.
Proof Apply Theorem 8 to V = λ(X ) − E (λ(X )).

Next we apply a functional to the persistence landscapes to obtain a real-valued random
variable that satisﬁes the usual central limit theorem.
Corollary 11 Assume p ≥ 2, E (cid:107)Λ(cid:107) < ∞ and E ((cid:107)Λ(cid:107)2 ) < ∞. For any f ∈ Lq (S ) with
(cid:90)
1
p + 1
q = 1, let
S

f Λ = (cid:107)f Λ(cid:107)1 .

Y =

(5)

Then

√

n[Y n − E (Y )]
d−→ N (0, Var(Y )).
where d denotes convergence in distribution and N (µ, σ2 ) is the normal distribution with
mean µ and variance σ2 .
Proof Since V = Λ − E (Λ) satisﬁes the central limit theorem in Lp (S ), for any g ∈ Lp (S )∗ ,
law with mean 0 and variance E (g(V )2 ). If we take g(h) = (cid:82)
the real random variable g(V ) satisﬁes the central limit theorem in R with limiting Gaussian
S f h, where f ∈ Lq (S ), with
q = 1, then g(V ) = Y − E (Y ) and E (g(V )2 ) = Var(Y ).
p + 1
1

(6)

86

Persistence Landscapes

3.4 Conﬁdence Intervals

The results of Section 3.3 allow us to obtain approximate conﬁdence intervals for the ex-
pected values of functionals on persistence landscapes.
Assume that λ(X ) satisﬁes the conditions of Corollary 11 and that Y is a corresponding
real random variable as deﬁned in (5). By Corollary 11 and Slutsky’s theorem we may
use the normal distribution to obtain the approximate (1 − α) conﬁdence interval for E (Y )
using
∗ Sn√
Y n ± z
(cid:80)n
,
n
i=1 (Yi − Y n )2 , and z ∗ is the upper α
2 critical value for the normal

n = 1
where S 2
n−1
distribution.

3.5 Statistical Inference using Landscapes I

Here we apply the results of Section 3.3 to hypothesis testing using persistence landscapes.
Let X1 , . . . , Xn be an iid copies of the random variable X and let X (cid:48)
1 , . . . , X (cid:48)
n(cid:48) be an iid
copies of the random variable X (cid:48) . Assume that the corresponding persistence landscapes
Λ, Λ(cid:48) lie in Lp (S ), where p ≥ 2. Let f ∈ Lq (S ), where 1
q = 1. Let Y and Y (cid:48) be deﬁned
(cid:80)n
p + 1
as in (5). Let µ = E (Y ) and µ(cid:48) = E (Y (cid:48) ). We will test the null hypothesis that µ = µ(cid:48) .
(cid:80)n
First we recall that the sample mean Y = 1
i=1 Yi is an unbiased estimator of µ and the
i=1 (Yi − Y )2 is an unbiased estimator of Var(Y ) and similarly
n
Y = 1
sample variance s2
n−1
Y (cid:48) . By Corollary 11, Y and Y (cid:48) are asymptotically normal.
for Y (cid:48) and s2
We use the two-sample z-test. Let
(cid:113) S 2
Y − Y (cid:48)
S 2
Y (cid:48)
Y
n +
n(cid:48)
where the denominator is the standard error for the diﬀerence. From this standard score a
p-value may be obtained from the normal distribution.

z =

,

3.6 Choosing a Functional
To apply the above results, one needs to choose a functional, f ∈ Lq (S ). This choice will
need to be made with an understanding of the data at hand. Here we present a couple of
options.
(cid:40)
If each λ = Λ(ω) is supported by {1, . . . , K } × [−B , B ], take
if t ∈ [−B , B ] and k ≤ K
1
0
otherwise.

f (k , t) =

(7)

Then (cid:107)f Λ(cid:107)1 = (cid:107)Λ(cid:107)1 .
If the parameter values for which the persistence landscape is nonzero are bounded
by ±B , then we have a nice choice of functional for the persistence landscape that is
the ﬁrst K dominant homological features. That is, using f in (7), (cid:107)f λ(cid:107)1 = (cid:80)K
unavailable for the (rescaled) rank function. We can choose a functional that is sensitive of
k=1(cid:107)Λk (cid:107)1 .
87

Bubenik

(cid:107)f Λ(cid:107)1 = (cid:80)∞
Under this weaker assumption we can also take fk (t) = 1
kr χ[−B ,B ] , where r > 1. Then
kr (cid:107)Λk (t)(cid:107)1 .
1
k=1
The condition that λ is supported by N× [−B , B ] can often be enforced by using reduced
homology or by applying extended persistence (Cohen-Steiner et al., 2009; Bubenik and
Scott, 2014) or by simply truncating the intervals in the corresponding barcode at some
ﬁxed values. We remark that certain experimental data may have bounds on the number
of intervals. For example, in the protein data considered using the ideas presented here in
Kovacev-Nikolic et al. (2014), the simplicial complexes have a ﬁxed number of vertices.

3.7 Statistical Inference using Landscapes II

The functionals suggested in Section 3.6 in the hypothesis test given in Section 3.5 may not
have enough power to discriminate between two groups with diﬀerent persistence in some
T 2 test. For example, consider Y = ((cid:82) (Λ1 − Λ(cid:48)
1 ), . . . , (cid:82) (ΛK − Λ(cid:48)
examples.
To increase the power, one can apply a vector of functionals and then apply Hotelling’s
K )), where K (cid:28) n1 + n2 − 2.
This alternative will not be suﬃcient if the persistence landscapes are translates of each
other, (see Figure 7). An additional approach is to compute the distance between the mean
landscapes of the two groups and obtain a p-value using a permutation test. This is done
in the Section 4.3. This test has been applied to persistence diagrams and barcodes (Chung
et al., 2009; Robinson and Turner, 2013).

4. Examples

The persistent homologies in this section were calculated using javaPlex (Tausz et al.,
2011) and Perseus by Nanda (2013). Another publicly available alternative is Dionysus
by Morozov (2012).
In Section 4.2 we use Matlab code courtesy of Eliran Subag that
implements an algorithm from Wood and Chan (1994).

4.1 Linked Annuli

We start with a simple example to illustrate the techniques. Following Munch et al. (2013),
we sample 200 points from the uniform distribution on the union of two annuli. We then
calculate the corresponding persistence landscape in degree one using the Vietoris-Rips
complex. We repeat this 100 times and calculate the mean persistence landscape. See
Figure 4.
Note that in the degree one barcode of this example, it is very likely that there will be
one large interval, one smaller interval born at around the same time, and all other intervals
are smaller and die around the time the larger two intervals are born.

4.2 Gaussian Random Fields

The topology of Gaussian random ﬁelds is of interest in statistics. The Euler characteristic of
superlevel sets of a Gaussian random ﬁeld may be calculated using the Gaussian Kinematic
Formula of Adler and Taylor (2007). The persistent homology of Gaussian random ﬁelds

88

Persistence Landscapes

Figure 4: 200 points were sampled from a pair of linked annuli. Here we show the points
and a corresponding union of balls and 1-skeleton of the ˇCech complex. This was
repeated 100 times. Next we show two of the degree one persistence landscapes
and the mean degree one persistence landscape.

89

010203040506070809010001020304001020304050607080901000102030400102030405060708090100010203040Bubenik

Figure 5: Mean landscapes of Gaussian random ﬁelds. The graph of a Gaussian random
ﬁeld on [0, 1]2 (top left) and its corresponding mean landscapes (middle row) in
degrees 0 and 1. The 0-isosurface of a Gaussian random ﬁeld on [0, 1]3 (top right)
and the corresponding mean landscapes in degrees 0, 1 and 2 (bottom row).

has been considered by Adler et al. (2010) and its expected Euler characteristic has been
obtained by Bobrowski and Borman (2012).
Here we consider a stationary Gaussian random ﬁeld on [0, 1]2 with autocovariance
function γ (x, y) = e−400(x2+y2 ) . See Figure 5. We sample this ﬁeld on a 100 by 100 grid,
and calculate the persistence landscape of the sublevel set. For homology in degree 0, we
truncate the inﬁnite interval at the maximum value of the ﬁeld. We calculate the mean
persistence landscapes in degrees 0 and 1 from 100 samples (see Figure 5, where we have
rescaled the ﬁltration by a factor of 100).
In the Gaussian random ﬁeld literature, it is more common to consider superlevel sets.
However, by symmetry, the expected persistence landscape in this case is the same except
for a change in the sign of the ﬁltration.
We repeat this calculation for a similar Gaussian random ﬁeld on [0, 1]3 , this time using
reduced homology. See Figure 5. This time we sample on a 25 × 25 × 25 grid.

90

Persistence Landscapes

4.3 Torus and Sphere

Here we combine persistence landscapes and statistical inference to discriminate between
iid samples of 1000 points from a torus and a sphere in R3 with the same surface area, using
the uniform surface area measure as described by Diaconis et al. (2012) (see Figure 6). To
be precise, we use the torus given by (r − 2)2 + z 2 = 1 in cylindrical coordinates, and the
sphere given by r2 = 2π in spherical coordinates.
For these points, we construct a ﬁltered simplicial complex as follows. First we trian-
gulate the underlying space using the Coxeter–Freudenthal–Kuhn triangulation, starting
with a cubical grid with sides of length 1
2 . Next we smooth our data using a triangular
kernel with bandwidth 0.9. We evaluate this kernel density estimator at the vertices of our
simplicial complex. Finally, we ﬁlter our simplicial complex as follows. For ﬁltration level
−r, we include a simplex in our triangulation if and only if the kernel density estimator has
values greater than or equal to r at all of its vertices. Three stages in the ﬁltration for one
of the samples are shown in (see Figure 6). We then calculate the persistence landscape of
this ﬁltered simplicial complex for 100 samples and plot the mean landscapes (see Figure 6).
We observe that the large peaks correspond to the Betti numbers of the torus and sphere.
Since the support of the persistence landscapes is bounded, we can use the integral of
the landscapes to obtain a real valued random variable that satisﬁes (6). We use a two-
sample z-test to test the null hypothesis that these random variables have equal mean. For
the landscapes in dimensions 0 and 2 we cannot reject the null hypothesis. In dimension 1
we do reject the null hypothesis with a p-value of 3 × 10−6 .
We can also choose a functional that only integrates the persistence landscape λ(k , t) for
certain ranges of k . In dimension 1, with k = 1 or k = 2 there is a statistically signiﬁcant
diﬀerence (p-values of 10−8 and 3 × 10−6 ), but not for k > 2. In dimension 2, there is not
a signiﬁcant diﬀerence for k = 1, but there is a signiﬁcant diﬀerence for k > 1 (p-value
< 10−4 ).
Now we increase the diﬃculty by adding a fair amount of Gaussian noise to the point
samples (see Figure 7) and using only 10 samples for each surface. This time we calculate
the L2 distances between the mean landscapes. We use the permutation test with 10,000
repetitions to determine if this distance is statistically signiﬁcant. There is a signiﬁcant
diﬀerence in dimension 0, with a p value of 0.0111. This is surprising, since the mean
landscapes look very similar. However, on closer inspection, they are shifted slightly (see
Figure 7). Note that we are detecting a geometric diﬀerence, not a topological one. This
shows that this statistic is quite powerful. There is also a signiﬁcant diﬀerence in dimensions
1 and 2, with p values of 0.0000 and 0.0000, respectively.

5. Landscape Distance and Stability

In this section we deﬁne the landscape distance and use it to show that the persistence
landscape is a stable summary statistic. We also show that the landscape distance gives
lower bounds for the bottleneck and Wasserstein distances. We defer the proofs of the
results of this section to the appendix.
Let M and M (cid:48) be persistence modules as deﬁned in Section 2.1 and let λ and λ(cid:48) be their
corresponding persistence landscapes as deﬁned in Section 2.2. For 1 ≤ p ≤ ∞, deﬁne the

91

Bubenik

Figure 6: We sample 1000 points for a torus and sphere, 100 times each, construct the
corresponding ﬁltered simplicial complexes and calculate persistent homology. In
columns 1, 2 and 3, we have the mean persistence landscape in dimension 0, 1
and 2 of the torus in row 3 and the sphere in row 4.

92

Persistence Landscapes

Figure 7: We again sample 1000 points sampled from a torus (top left) and sphere (top
middle), this time with Gaussian noise. We show the torus from the perspective
that makes it easiest to see the hole in the middle. We calculate persistent
homology from 10 samples. In columns 1, 2 and 3, we have the mean persistence
landscape in dimension 0, 1 and 2, respectively, with the torus in row 2 and the
sphere in row 3. The top right is a graph of the diﬀerence between the mean
landscapes in dimension 0.

93

Bubenik

p-landscape distance between M and M (cid:48) by
(cid:48)(cid:107)p .
) = (cid:107)λ − λ
(cid:48)
Λp (M , M
Similarly, if λ and λ(cid:48) are the persistence landscapes corresponding to persistence diagrams
D and D (cid:48) (Section 2.3), then we deﬁne

(cid:48)

) = (cid:107)λ − λ(cid:107)p .
Λp (D , D
Given a real valued function f : X → R on a topological space X , let M (f ) denote be
the corresponding persistence module deﬁned at the end of Section 2.1.
Theorem 12 (∞-Landscape Stability Theorem) Let f , g : X → R. Then
Λ∞ (M (f ), M (g)) ≤ (cid:107)f − g(cid:107)∞ .

Thus the persistence landscape is stable with respect to the supremum norm. We remark
that there are no assumptions on f and g , not even the q-tame condition of Chazal et al.
of x. If D = {xj }, let Persk (D) = (cid:80)
(2012).
Let D be a persistence diagram. For x = (b, d) ∈ D , let (cid:96) = d − b denote the persistence
j (cid:96)k
j denote the degree-k total persistence of D .
Now let us consider a persistence diagram to be an equivalence class of multisets of pairs
(b, d) with b ≤ d, where D ∼ D (cid:113) {(t, t)} for any t ∈ R. That is, to any persistence diagram,
we can freely adjoin points on the diagonal. This is reasonable, since points on the diagonal
have zero persistence. Each persistence diagram has a unique representative ˆD without any
points on the diagonal. We set |D| = | ˆD |. We also remark that Persk (D) is well deﬁned.
By allowing ourselves to add as many points on the diagonal as necessary, there exists
∼=−→ D (cid:48) can be
bijections between any two persistence diagrams. Any bijection ϕ : D
represented by ϕ : xj (cid:55)→ x(cid:48)
j , where j ∈ J with |J | = |D | + |D (cid:48) |. For a given ϕ, let
j ) and εj = (cid:107)xj − x(cid:48)
j (cid:107)∞ = max(|bj − b(cid:48)
j |, |dj − d(cid:48)
j |).
xj = (bj , dj ), x(cid:48)
j = (b(cid:48)
j , d(cid:48)
The bottleneck distance (Cohen-Steiner et al., 2007) between persistence diagrams D
and D (cid:48) is given by

W∞ (D , D

(cid:48)

εj ,

) = inf
sup
∼=−→D (cid:48)
j
ϕ:D
where the inﬁmum is taken over all bijections from D to D (cid:48) . It follows that for the empty
persistence diagram ∅, W∞ (D , ∅) = 1
2 supj (cid:96)j .
The ∞-landscape distance is bounded by the bottleneck distance.
Theorem 13 For persistence diagrams D and D (cid:48) ,
) ≤ W∞ (D , D
(cid:48)
Λ∞ (D , D
).
For p ≥ 1, the p-Wasserstein distance (Cohen-Steiner et al., 2010) between D and D (cid:48)
 1
(cid:88)
is given by
p
j

) = inf
∼=−→D (cid:48)
ϕ:D

Wp (D , D

εp
j

.

(cid:48)

(cid:48)

94

Persistence Landscapes

(cid:96)j εp
j

.

W p (D , D

(cid:48)

) = inf
∼=−→D (cid:48)
ϕ:D

We remark that the Wasserstein distance gives equal weighting to the εj while the land-
scape distance gives a stronger weighting to εj if xj has larger persistence. The landscape
distance is most closely related to a weighted version of the Wasserstein distance that we
 1
(cid:88)
now deﬁne. The persistence weighted p-Wasserstein distance between D and D (cid:48) is given by
p
j
Note that it is asymmetric.
For the remainder of the section we assume that D and D (cid:48) are ﬁnite. The following
result bounds the p-landscape distance. Recall that (cid:96)j is the persistence of xj ∈ D and
when ϕ : xj (cid:55)→ x(cid:48)
j , εj = (cid:107)xj − x(cid:48)
j (cid:107)∞
 n(cid:88)
 .
Theorem 14 If n = |D | + |D | then
n(cid:88)
)p ≤ min
2
∼=−→D (cid:48)
p + 1
ϕ:D
j=1
j=1
(cid:19)
(cid:18)
(cid:104)
(cid:105)−1
From this we can obtain a lower bound on the p-Wasserstein distance.
1, 1
.
2

Corollary 15 Wp (D , D (cid:48) )p ≥ min

W∞ (D , ∅) + 1
p+1

Λp (D , D (cid:48) )p

(cid:48)

Λp (D , D

(cid:96)j εp
j +

εp+1
j

For our ﬁnal stability theorem, we use ideas from Cohen-Steiner et al. (2010). Let
f : X → R be a function on a topological space. We say that f is tame if for all but ﬁnitely
many a ∈ R, the associated persistence module M (f ) is constant and ﬁnite dimensional
on some open interval containing a. For such an f , let D(f ) denote the corresponding
If X is a metric space we say that f is Lipschitz if there is some
persistence diagram.
constant c such that |f (x) − f (y)| ≤ c d(x, y) for all x, y ∈ X . We let Lip(f ) denote
the inﬁmum of all such c. We say that a metric space X implies bounded degree-k total
persistence if there is a constant CX,k such that Persk (D(f )) ≤ CX,k for all tame Lipschitz
functions f : X → R such that Lip(f ) ≤ 1. For example, as observed by Cohen-Steiner
et al. (2010), if X is the n-dimensional sphere, then X = S n has bounded k-persistence for
k = n + δ for any δ > 0, but does not have bounded k-persistence for k < n.

Theorem 16 (p-Landscape stability theorem) Let X be a triangulable, compact met-
ric space that implies bounded degree-k total persistence for some real number k ≥ 1, and
let f and g be two tame Lipschitz functions. Then
Λp (D(f ), D(g))p ≤ C (cid:107)f − g(cid:107)p−k∞ ,
for al l p ≥ k , where C = CX,k (cid:107)f (cid:107)∞ (Lip(f )k + Lip(g)k ) + CX,k+1

p+1 (Lip(f )k+1 + Lip(g)k+1 ).
1

Thus the persistence diagram is stable with respect to the p-landscape distance if p > k ,
where X has bounded degree-k total persistence. This is the same condition as for the
stability of the p-Wasserstein distance in Cohen-Steiner et al. (2010). Equivalently, the

95

Bubenik

persistence landscape is stable with respect to the p-norm if p > k , where X has bounded
degree-k total persistence.

Acknowledgments

The author would like to thank Robert Adler, Frederic Chazal, Herbert Edelsbrunner,
Giseon Heo, Sayan Mukherjee and Stephen Rush for helpful discussions. Thanks to Junyong
Park for suggesting Hotelling’s T 2 test. Also thanks to the anonymous referees who made a
number of helpful comments to improve the exposition. In addition, the author gratefully
acknowledges the support of the Air Force Oﬃce of Scientiﬁc Research (AFOSR grant
FA9550-13-1-0115).

Appendix A. Proofs
Proof [Proof of Lemma 4(3)] We will prove that λk is 1-Lipschitz. That is, |λk (t) − λk (s)| ≤
|t − s|, for all s, t ∈ R.
Let s, t ∈ R. Without loss of generality, assume that λk (t) ≥ λk (s) ≥ 0. If λk (t) ≤ |t− s|,
then λk (t) − λk (s) ≤ λk (t) ≤ |t − s| and we are done. So assume that λk (t) > |t − s|.
Let 0 < h < λk (t) − |t − s|. Then t − λk (t) < s − h < s + h < t + λk (t). Thus, by
Lemma 1 and Deﬁnition 3, β s−h,s+h ≥ k . It follows that λk (s) ≥ λk (t) − |t − s|. Thus
λk (t) − λk (s) ≤ |t − s|.

Theorems 12 and 13 follow from the next result which is of independent interest. Follow-
ing Chazal et al. (2009), we say that two persistence modules M and M (cid:48) are ε-interleaved
if for all a ∈ R there exist linear maps ϕa : Ma → M (cid:48)
a → Ma+ε such that for
a+ε and ψ : M (cid:48)
all a ∈ R, ψa+ε ◦ ϕa = M (a ≤ a + 2ε) and ϕa+ε ◦ ψa = M (cid:48) (a ≤ a + 2ε) and for all a ≤ b
M (cid:48) (a + ε ≤ b + ε) ◦ ϕa = ϕb ◦ M (a ≤ b) and M (a + ε ≤ b + ε) ◦ ψa = ψb ◦ M (cid:48) (a ≤ b). For
persistence modules M and M (cid:48) deﬁne the interleaving distance between M and M (cid:48) by
dI (M , M ) = inf (ε | M and M
(cid:48)
Theorem 17 Λ∞ (M , M (cid:48) ) ≤ dI (M , M (cid:48) ).
Proof Assume that M and M (cid:48) are ε-interleaved. Then for t ∈ R and m ≥ ε, the map
M (t − m ≤ t + m) factors through the map M (cid:48) (t − m + ε ≤ t + m − ε). So by Lemma 1,
β t−m+ε,t+m−ε (M (cid:48) ) ≥ β t−m,t+m (M ). Thus by Deﬁnition 3, λ(cid:48) (k , t) ≥ λ(k , t) − ε for all k ≥ 1.
It follows that (cid:107)λ − λ(cid:48)(cid:107)∞ ≤ ε.

are ε-interleaved).

Proof [Proof of Theorem 12] Combining Theorem 17 with the stability theorem of Bubenik
and Scott (2014), we have Λ∞ (M (f ), M (g)) ≤ dI (M (f ), M (g)) ≤ (cid:107)f − g(cid:107)∞ .

Proof [Proof of Theorem 13] For a persistence diagram D , consider the persistence mod-
ule given by the corresponding sum of interval modules (Chazal et al., 2012), M (D) =
⊕(a,b)∈ ˆD
I(a, b). Combining Theorem 17 with Theorem 4.9 of Chazal et al. (2012) we have

96

Persistence Landscapes

(cid:48)

(k , t)|p

k (t)|p dt
(cid:48)

|λk (t) − λ

|λk (t) − λ

Λ∞ (M (D), M (D (cid:48) )) ≤ dI (M (D), M (D (cid:48) )) ≤ W∞ (D , D (cid:48) ).
∼=−→ D (cid:48) with ϕ(xj ) = x(cid:48)
j . Let λ = λ(D) and

Proof [Proof of Theorem 14] Let ϕ : D
(cid:90)
λ(cid:48) = λ(D (cid:48) ). So Λp (D , D (cid:48) )p = (cid:107)λ − λ(cid:48)(cid:107)p
p .
(cid:90)
|λ(k , t) − λ
(cid:48)(cid:107)p
(cid:107)λ − λ
n(cid:88)
p =
(cid:90) n(cid:88)
k=1
k=1
Fix t. Let uj (t) = λ({xj })(1, t) and vj (t) = λ({x(cid:48)
j })(1, t). For each t, let u(1) (t) ≤ · · · ≤
u(n) (t) denote an ordering of u1 (t), . . . , un (t) and deﬁne v(k) (t) for 1 ≤ k ≤ n similarly.
Then u(k) (t) = λk (t) and v(k) (t) = λ(cid:48)
k (t) (see Figure 2). We obtain the result from the
(cid:90) n(cid:88)
following where the two inequalities are proven in Lemmata 18 and 19.
|u(k) (t) − v(k) (t)|p dt
(cid:90) n(cid:88)
k=1
|uk (t) − vk (t)|p dt
(cid:90)
n(cid:88)
k=1
=
≤ n(cid:88)
j=1
j=1

|uj (t) − vj (t)|p dt
n(cid:88)
j=1

2
p + 1

(cid:107)λ − λ

(cid:48)(cid:107)p
p =

k (t)|p dt
(cid:48)

(cid:96)j εp
j +

εp+1
j

.

=

=

≤

|uj − vj |p .

Lemma 18 Let u1 , . . . , un ∈ R and v1 , . . . , vn ∈ R. Order them u(1) ≤ · · · ≤ u(n) and
n(cid:88)
|u(j ) − v(j ) |p ≤ n(cid:88)
v(1) ≤ · · · ≤ v(n) . Then
j=1
j=1
Proof Assume u1 < · · · < un , v1 < · · · < vn , and p ≥ 1. Let u and v denote (u1 , . . . , un )
given by fn (σ) = (cid:80)n
and (v1 , . . . , vn ). Let Σn denote the symmetric group on n letters and let fn : Σn → R be
j=1 |uj − vσ(j ) |p . We will prove by induction that if fn (σ) is minimal
then σ is the identity, which we denote by 1.
For n = 1 this is trivial. For n = 2 assume without loss of generality that u1 = 0, u2 = 1
and 0 ≤ v1 < v2 . Let 1 and τ denote the elements of Σ2 . Then f (1) = vp
1 + |1 − v2 |p and
1 − |1 − v1 |p < vp
2 + |1 − v1 |p . Notice that f (1) < f (τ ) if and only if vp
2 − |1 − v2 |p .
f (τ ) = vp
The result follows from checking that g(x) = xp − |1 − x|p is an increasing function for x ≥ 0.

97

Bubenik

Now assume that the statement is true for some n ≥ 2. Assume that fn+1 (σ∗ ) is minimal.
denotes omission. Since fn+1 (σ∗ ) is minimal for u and v , it follows that (cid:80)n
Fix 1 ≤ i ≤ n + 1. Let u(cid:48) = (u1 , . . . , ˆui , . . . , un+1 ) and v (cid:48) = (v1 , . . . , ˆvσ∗ (i) , . . . , vn+1 ), where ˆ·
j=1,j (cid:54)=i |uj − vσ∗ (j ) |
is minimal for u(cid:48) and v (cid:48) . By the induction hypothesis, for 1 ≤ j < k ≤ n + 1 and j, k (cid:54)= i,
Hence (cid:80)n
j=1 |u(j ) − v(j ) |p ≤ (cid:80)n
σ∗ (j ) < σ∗ (k). Therefore σ∗ = 1. Thus, by induction, the statement is true for all n.
j=1 |uj − vj |p if u(1) < · · · < u(n) and v(1) < · · · < v(n) . The
statement in the lemma follows by continuity.

Lemma 19 Let x = (b, d) and x(cid:48) = (b(cid:48) , d(cid:48) ) where b ≤ d and b(cid:48) ≤ d(cid:48) . Let (cid:96) = d − b and
ε = (cid:107)x − x(cid:48)(cid:107)∞ . Then (cid:107)λ({x}) − λ({x(cid:48)})(cid:107)p
p ≤ (cid:96)εp + 2
p+1 εp+1 .
Proof Let λ = λ({x}) and λ(cid:48) = λ({x(cid:48)}). First λk = λ(cid:48)
k = 0 for k > 1; so (cid:107)λ − λ(cid:48)(cid:107)p =
1(cid:107)p . Second λ1 (t) = (h − |t − m|)+ , where h = d−b
(cid:107)λ1 − λ(cid:48)
2 , m = b+d
2 , and y+ = max(y , 0),
and similarly for λ(cid:48)
0 εp dt + 2 (cid:82) ε
p = 2 (cid:82) h
1 (see Figure 2).
1(cid:107)p
Fix x and ε. As x(cid:48) moves along the square (cid:107)x − x(cid:48)(cid:107)∞ = ε, (cid:107)λ1 − λ(cid:48)
p has a maximum
if x(cid:48) = (a − ε, b + ε). In this case (cid:107)λ1 − λ(cid:48)
1(cid:107)p
0 tp dt = (cid:96)εp + 2
p+1 εp+1 .
responding {εj }. Assume that Wp (D , D (cid:48) ) ≤ 1. Then Wp (D , D (cid:48) )p = (cid:80)n
∼=−→ D (cid:48) be a minimizer for Wp (D , D (cid:48) ), with cor-
Proof [Proof of Corollary 15] Let ϕ : D
j ≤ 1. So for
j=1 εp
1 ≤ j ≤ n, εj ≤ 1. Combining this with Theorem 14, we have that
(cid:18)
(cid:19)
)p ≤ n(cid:88)
(cid:96)j +
Λp (D , D
j=1
2 (cid:96)j , (cid:96)j ≤ 2 W∞ (D , ∅). Hence
Since W∞ (D , ∅) = max 1
(cid:18)
W∞ (D , ∅) +
)p ≤ 2
(cid:48)
1
(cid:104)
p + 1

2
p + 1
(cid:19)

)p .
(cid:105)−1

Wp (D , D

Λp (D , D

εp
j .

(8)

(9)

(cid:48)

(cid:48)

Therefore Wp (D , D (cid:48) )p ≥ 1 or Wp (D , D (cid:48) )p ≥ 1
2
statement of the corollary follows.

W∞ (D , ∅) + 1
p+1

Λp (D , D (cid:48) )p . The

Theorem 16 follows from the following corollary to Theorem 14 which is of independent
interest.
Corollary 20 Let p ≥ k ≥ 1. Then
(cid:20)

(cid:48)

)p ≤ W∞ (D , D

(cid:48)

)p−k

W∞ (D , ∅)(Persk (D) + Persk (D

(cid:48)

))+

Λp (D , D

(cid:21)
))

1
p + 1

(Persk+1 (D) + Persk+1 (D

(cid:48)

98

Persistence Landscapes

(cid:96)(cid:48)
Proof Let ϕ be a minimizer for W∞ (D , D (cid:48) ) with corresponding {εj }. If εj >
(cid:96)j
j
2 then
2 +
) and similarly for x(cid:48)
bj +dj
bj +dj
modify ϕ to pair xj = (bj , dj ) with ¯xj = (
,
j . Note that
2
2
(cid:96)(cid:48)
2 and (cid:107)x(cid:48)
j − ¯x(cid:48)
(cid:107)xj − ¯xj (cid:107)∞ =
j (cid:107)∞ =
2 , so ϕ is still a minimizer for W∞ (D , D (cid:48) ).
(cid:96)j
j
Recall that for all j , (cid:96)j ≤ 2 W∞ (D , ∅). Since ϕ is a minimizer for W∞ (D , D (cid:48) ), for all j ,
εj ≤ W∞ (D , D (cid:48) ). So applying our choice of ϕ to Theorem 14 we have,
2 W∞ (D , ∅)
 .
n(cid:88)
n(cid:88)
)p ≤ W∞ (D , D
)p−k
(cid:48)
Λp (D , D
(cid:16)
(cid:17)q ≤ 1
j ≤ (cid:16) (cid:96)j
j )q (cid:17)
j=1
j=1
the convexity of α(x) = xq for q ≥ 1. Thus (cid:80)n
(cid:96)(cid:48)
for q ≥ 1, where the right hand side follows by
((cid:96)j )q + ((cid:96)(cid:48)
Now εq
j
2 +
2
2
2 (Persq (D) + Persq (D (cid:48) )) for q ≥ 1.
j ≤ 1
j=1 εq
The result follows.

2
p + 1

(cid:48)

εk
j +

εk+1
j

Proof [Proof of Theorem 16] Theorem 16 follows from Corollary 20 by the following two ob-
servations. First, by the stability theorem of Cohen-Steiner et al. (2007), W∞ (D(f ), D(g)) ≤
(cid:107)f − g(cid:107)∞ and W∞ (D(f ), ∅) ≤ (cid:107)f (cid:107)∞ . Second, if Persq (D(f )) ≤ CX,q for all tame Lips-
chitz functions f : X → R with Lip(f ) ≤ 1, then for general tame Lipschitz functions,
Persq (D(f )) ≤ CX,q Lip(f )q .

References

Robert J. Adler and Jonathan E. Taylor. Random Fields and Geometry. Springer Mono-
graphs in Mathematics. Springer, New York, 2007. ISBN 978-0-387-48112-8.

Robert J. Adler, Omer Bobrowski, Matthew S. Borman, Eliran Subag, and Shmuel Wein-
berger. Persistent homology for random ﬁelds and complexes. In Borrowing Strength:
Theory Powering Applications—a Festschrift for Lawrence D. Brown, volume 6 of Inst.
Math. Stat. Col lect., pages 124–143. Inst. Math. Statist., Beachwood, OH, 2010.

Andrew J. Blumberg, Itamar Gal, Michael A. Mandell, and Matthew Pancia. Robust
statistics, hypothesis testing, and conﬁdence intervals for persistent homology on metric
measure spaces. Found. Comput. Math., 14(4):745–789, 2014.
ISSN 1615-3375. doi:
10.1007/s10208-014-9201-4. URL http://dx.doi.org/10.1007/s10208-014-9201-4.

Omer Bobrowski and Matthew Strom Borman. Euler integration of Gaussian random ﬁelds
and persistent homology. J. Topol. Anal., 4(1):49–70, 2012. ISSN 1793-5253.

Karol Borsuk. On the imbedding of systems of compacta in simplicial complexes. Fund.
Math., 35:217–234, 1948. ISSN 0016-2736.

Peter Bubenik and Jonathan A. Scott. Categoriﬁcation of persistent homology. Discrete
Comput. Geom., 51(3):600–627, 2014. ISSN 0179-5376.

99

Bubenik

Peter Bubenik, Gunnar Carlsson, Peter T. Kim, and Zhi-Ming Luo. Statistical topology
In Algebraic Methods in
via Morse theory persistence and nonparametric estimation.
Statistics and Probability II, volume 516 of Contemp. Math., pages 75–92. Amer. Math.
Soc., Providence, RI, 2010.

Gunnar Carlsson. Topology and data. Bul l. Amer. Math. Soc. (N.S.), 46(2):255–308, 2009.
ISSN 0273-0979.

Gunnar Carlsson, Tigran Ishkhanov, Vin de Silva, and Afra Zomorodian. On the local
behavior of spaces of natural images. Int. J. Comput. Vision, 76(1):1–12, 2008. ISSN
0920-5691.

Fr´ed´eric Chazal, David Cohen-Steiner, Marc Glisse, Leonidas J. Guibas, and Steve Y.
Oudot. Proximity of persistence modules and their diagrams. In Proceedings of the 25th
Annual Symposium on Computational Geometry, SCG ’09, pages 237–246, New York,
NY, USA, 2009. ACM. ISBN 978-1-60558-501-7.

Frederic Chazal, Vin de Silva, Marc Glisse, and Steve Oudot. The structure and stability
of persistence modules. arXiv:1207.3674 [math.AT], 2012.

Fr´ed´eric Chazal, Marc Glisse, Catherine Labru`ere, and Bertrand Michel. Optimal rates of
convergence for persistence diagrams in topological data analysis. 2013. arXiv:1305.6239
[math.ST].

Fr´ed´eric Chazal, Brittany Terese Fasy, Fabrizio Lecci, Alessandro Rinaldo, and Larry
Wasserman. Stochastic convergence of persistence landscapes and silhouettes. Symposium
on Computational Geometry (SoCG), 2014.

Chao Chen and Michael Kerber. An output-sensitive algorithm for persistent homology.
Comput. Geom., 46(4):435–447, 2013. ISSN 0925-7721.

Moo K. Chung, Peter Bubenik, and Peter T. Kim. Persistence diagrams in cortical surface
data. In Information Processing in Medical Imaging (IPMI) 2009, volume 5636 of Lecture
Notes in Computer Science, pages 386–397, 2009.

David Cohen-Steiner, Herbert Edelsbrunner, and John Harer. Stability of persistence dia-
grams. Discrete Comput. Geom., 37(1):103–120, 2007. ISSN 0179-5376.

David Cohen-Steiner, Herbert Edelsbrunner, and John Harer. Extending persistence using
Poincar´e and Lefschetz duality. Found. Comput. Math., 9(1):79–103, 2009. ISSN 1615-
3375.

David Cohen-Steiner, Herbert Edelsbrunner, John Harer, and Yuriy Mileyko. Lipschitz
functions have Lp -stable persistence. Found. Comput. Math., 10(2):127–139, 2010. ISSN
1615-3375.

William Crawley-Boevey. Decomposition of pointwise ﬁnite-dimensional persistence mod-
ules. arXiv:1210.0819 [math.RT], 2012.

100

Persistence Landscapes

Vin de Silva and Gunnar Carlsson. Topological estimation using witness complexes. Euro-
graphics Symposium on Point-Based Graphics, 2004.

Vin De Silva and Robert Ghrist. Coverage in sensor networks via persistent homology.
Algebr. Geom. Topol., 7:339–358, 2007a.

Vin De Silva and Robert Ghrist. Homological sensor networks. Notic. Amer. Math. Soc.,
54(1):10–17, 2007b.

Mary-Lee Dequ´eant, Sebastian Ahnert, Herbert Edelsbrunner, Thomas M. A. Fink, Earl F.
Glynn, Gaye Hattem, Andrzej Kudlicki, Yuriy Mileyko, Jason Morton, Arcady R. Mushe-
gian, Lior Pachter, Maga Rowicka, Anne Shiu, Bernd Sturmfels, and Olivier Pourqui´e.
Comparison of pattern detection methods in microarray time series of the segmentation
clock. PLoS ONE, 3(8):e2856, 08 2008.

Tamal Krishna Dey, Fengtao Fan, and Yusu Wang. Graph induced complex on point data. In
Proceedings of the Twenty-ninth Annual Symposium on Computational Geometry, SoCG
’13, pages 107–116, New York, NY, USA, 2013. ACM. ISBN 978-1-4503-2031-3.

Persi Diaconis, Susan Holmes, and Mehrdad Shahshahani. Sampling from a manifold.
arXiv:1206.6913 [math.ST], 2012.

Herbert Edelsbrunner, David Letscher, and Afra Zomorodian. Topological persistence and
simpliﬁcation. Discrete Comput. Geom., 28(4):511–533, 2002. ISSN 0179-5376. Discrete
and computational geometry and graph drawing (Columbia, SC, 2001).

Herbert Edelsbrunner, Dmitriy Morozov, and Amit Patel. Quantifying transversality by
measuring the robustness of intersections. Found. Comput. Math., 11(3):345–361, 2011.
ISSN 1615-3375.

Brittany Terese Fasy, Fabrizio Lecci, Alessandro Rinaldo, Larry Wasserman, Sivaraman
Balakrishnan, and Aarti Singh. Conﬁdence sets for persistence diagrams. Ann. Statist.,
42(6):2301–2339, 2014. ISSN 0090-5364. doi: 10.1214/14-AOS1252. URL http://dx.
doi.org/10.1214/14-AOS1252.

Robert Ghrist. Barcodes: the persistent topology of data. Bul l. Amer. Math. Soc. (N.S.),
45(1):61–75, 2008. ISSN 0273-0979.

Allen Hatcher. Algebraic Topology. Cambridge University Press, Cambridge, 2002. ISBN
0-521-79160-X; 0-521-79540-0.

Giseon Heo, Jennifer Gamble, and Peter T. Kim. Topological analysis of variance and the
maxillary complex. J. Amer. Statist. Assoc., 107(498):477–492, 2012. ISSN 0162-1459.

J. Hoﬀmann-Jørgensen and G. Pisier. The law of large numbers and the central limit
theorem in Banach spaces. Ann. Probability, 4(4):587–599, 1976.

Violeta Kovacev-Nikolic, Giseon Heo, Dragan Nikoli´c, and Peter Bubenik. Using cycles in
high dimensional data to analyze protein binding. 2014. arXiv:1412.1394 [stat.ME].

101

Bubenik

Michel Ledoux and Michel Talagrand. Probability in Banach Spaces. Classics in Mathemat-
ics. Springer-Verlag, Berlin, 2011. ISBN 978-3-642-20211-7. Isoperimetry and processes,
Reprint of the 1991 edition.

Yuriy Mileyko, Sayan Mukherjee, and John Harer. Probability measures on the space of
persistence diagrams. Inverse Problems, 27(12):124007, 22, 2011. ISSN 0266-5611.

Nikola Milosavljevi´c, Dmitriy Morozov, and Primoˇz ˇSkraba. Zigzag persistent homology in
matrix multiplication time. In Computational Geometry (SCG’11), pages 216–225. ACM,
New York, 2011.

Dimitriy Morozov. Dionysus: a C++ library with various algorithms for computing per-
sistent homology. Software available at http://www.mrzv.org/software/dionysus/,
2012.

Elizabeth Munch, Paul Bendich, Katharine Turner, Sayan Mukherjee, Jonathan Mat-
tingly, and John Harer. Probabilistic fr´echet means and statistics on vineyards. 2013.
arXiv:1307.6530 [math.PR].

Vidit Nanda. Perseus: the persistent homology software. Software available at http:
//www.math.rutgers.edu/~vidit/perseus/index.html, 2013.

Monica Nicolau, Arnold J. Levine, and Gunnar Carlsson. Topology based data analysis
identiﬁes a subgroup of breast cancers with a unique mutational proﬁle and excellent
survival. Proc. Nat. Acad. Sci., 108(17):7265–7270, 2011.

Andrew Robinson and Katharine Turner. Hypothesis testing for topological data analysis.
2013. arXiv:1310.7467 [stat.AP].

Andrew Tausz, Mikael Vejdemo-Johansson, and Henry Adams. Javaplex: a research soft-
ware package for persistent (co)homology. Software available at http://code.google.
com/javaplex, 2011.

Katharine Turner, Yuriy Mileyko, Sayan Mukherjee, and John Harer. Fr´echet means for
distributions of persistence diagrams. Discrete Comput. Geom., 52(1):44–70, 2014.

Andrew T. A. Wood and Grace Chan. Simulation of stationary Gaussian processes in [0, 1]d .
J. Comput. Graph. Statist., 3(4):409–432, 1994. ISSN 1061-8600.

Afra Zomorodian and Gunnar Carlsson. Computing persistent homology. Discrete Comput.
Geom., 33(2):249–274, 2005. ISSN 0179-5376.

102

Journal of Machine Learning Research 16 (2015) 491-494

Submitted 11/11; Revised 7/14; Published 3/15

A Classiﬁcation Module for Genetic Programming
Algorithms in JCLEC

Alberto Cano
Jos´e Mar´ıa Luna
Amelia Zafra
Sebasti´an Ventura
Department of Computer Science and Numerical Analysis
Rabanales Campus, University of C´ordoba, 14071, C´ordoba, Spain

acano@uco.es
jmluna@uco.es
azafra@uco.es
sventura@uco.es

Editor: Mikio Braun

Abstract
JCLEC-Classiﬁcation is a usable and extensible open source library for genetic program-
ming classiﬁcation algorithms. It houses implementations of rule-based methods for clas-
siﬁcation based on genetic programming, supporting multiple model representations and
providing to users the tools to implement any classiﬁer easily. The software is written in
Java and it is available from http://jclec.sourceforge.net/classification under the
GPL license.
Keywords:

classiﬁcation, evolutionary algorithms, genetic programming, JCLEC

1. Introduction

In the last decade, the increasing interest in storing information has led to its automatic
processing, discovering knowledge that is potentially useful. Data mining involves the use
of data analysis tools to discover this knowledge previously unknown, valid patterns, and
close relationships in databases. One of the most used data mining tasks is classiﬁcation,
which learns from a set of training examples to produce predictions about future examples.
The classiﬁcation models are being applied to enormous databases in areas such as
bioinformatics, marketing, banks or web mining. Existing classiﬁcation libraries provide
algorithms following many diﬀerent methodologies. However, it is diﬃcult to ﬁnd a library
that contains GP (genetic programming) algorithms, an important evolutionary computa-
tion paradigm. The conceptual diﬃculty of GP makes it diﬃcult to implement algorithms
following this paradigm despite its algorithms perform well as it is proved by many re-
searchers (Espejo et al., 2010).
GP is an eﬃcient and ﬂexible heuristic technique that uses complex representations
such as trees. This technique provides comprehensible models, which are useful in diﬀerent
application domains. For instance, it is applied to supervised learning tasks like regression,
classiﬁcation and unsupervised learning tasks like clustering and association. In classiﬁca-
tion tasks, the application of GP is an important issue since it may oﬀer results that are
comprehensible to humans. Additionally, it oﬀers interesting advantages such as ﬂexibility,
and the possibility of using diﬀerent kinds of representations, e.g., decision trees, rule-based
systems, discriminant functions, etc. An extension of GP is grammar-guided genetic pro-

c(cid:13)2015 Alberto Cano, Jos´e Mar´ıa Luna, Amelia Zafra and Sebasti´an Ventura.

Cano, Luna, Zafra and Ventura

gramming (G3P), which makes the knowledge extracted more expressive and ﬂexible by
means of a context-free grammar (McKay et al., 2010).
This paper presents an open source software for researchers and end-users to develop
classiﬁcation algorithms based on GP and G3P models. It is an intuitive and usable tool
which extends the JCLEC evolutionary computation library (Ventura et al., 2007). The
software presented includes some GP and G3P proposals described in literature, and pro-
vides the necessary classes and methods to develop any kind of evolutionary algorithms for
solving classiﬁcation problems easily.
This paper is organized as follows. Firstly, Section 2 provides a description of the
module, its structure and the way to use it. Finally, the documentation and the requirements
of this module are outlined in Section 3.

2. Description of the Module

The classiﬁcation module is presented in this section, describing the library structure and
its main characteristics.

2.1 Structure of the Module

The net.sf.jclec.problem.classiﬁcation.base package roots the hierarchical structure of the
classiﬁcation module, and provides the abstract classes with the properties and methods
that any classiﬁcation algorithm must contain, e.g., ClassiﬁcationAlgorithm, Classiﬁcation-
Reporter, Rule and RuleBase. A new algorithm included in the module should inherit from
these classes regardless the classiﬁcation model.
In this context, we focus on rule-based
classiﬁers which comprise one or more classiﬁcation rules, each of them being a knowledge
representation model consisting of an antecedent and a consequent. The antecedent of each
classiﬁcation rule is made up of a series of conditions to be met by an instance to consider
that it belongs to the class speciﬁed by the consequent.
Based on whether an algorithm uses a GP or G3P encoding, JCLEC-Classiﬁcation
makes a diﬀerentiation between expression-tree and syntax-tree respectively. In such a way,
each GP classiﬁcation individual is represented by means of the ExprTreeRuleIndividual
class, which represents an individual, comprising all the features required to do it: the
genotype, the phenotype and the ﬁtness function value. The nodes and functions in GP
trees are deﬁned by the ExprTreeSpecies class. Similarly to GP individuals, the Syntax-
TreeRuleIndividual class speciﬁes all the features required to represent a G3P individual,
while the SyntaxTreeSpecies allows us to deﬁne the terminal and nonterminal symbols of the
grammar used to generate individuals. Furthermore, the module allows to encode multiple
syntax and expression trees for Pittsburgh style encodings or multi expression programming
by means of the MultiExprTree and MultiSyntaxTree classes.
In order to represent the phenotype of a rule-base individual, crisp and fuzzy rules are
generated by using the CrispRule and FuzzyRule classes, respectively. These classes provide
the antecedent of the rule in an expression-tree shape and the consequent assigned to this
antecedent. In addition, methods to classify a whole data set or a particular instance are
provided in these classes. These methods compute whether the antecedent of a rule satisﬁes
an instance, returning the consequent of the rule, otherwise the instance is not covered by
the antecedent and therefore no predictions can be made. Besides those packages that repre-

492

A Classification Module for JCLEC

sent the main characteristics of any individual, the net.sf.jclec.problem.classiﬁcation.listener
package to make reports for the train and test classiﬁcation processes is provided. This
package contains the RuleBaseReporter class with methods to make reports specifying the
classiﬁer features such as the rule base, the number of rules, the average number of condi-
tions, the percentage of correct predictions, the percentage of correct predictions per class,
the geometric mean, the kappa rate and the confusion matrix.
Finally, it is noteworthy that several utility classes, which make it easy to load data from
KEEL1 and ARFF2 formatted ﬁles, are provided by a dataset package. Three diﬀerent
attribute types may be represented by this package, integer, continuous and categorical,
and a number of characteristics from the data set are given, comprising type of attributes,
number of classes, number of instances, etc.
The module houses three G3P classiﬁcation algorithms (De Falco et al., 2001; Bo jarczuk
et al., 2004; Tan et al., 2002), which can guide developers to write new algorithms.

2.2 Usage of the Module

Including new classiﬁcation algorithms in this module is very simple. We focus on the algo-
rithm described by Bo jarczuk et al. (2004). This algorithm, which is provided in the module
(see the net.sf.jclec.problem.classiﬁcation.algorithm.bojarczuk package), is constructed with
only three additional classes. One of them, the BojarczukAlgorithm class is inherited from
the ClassiﬁcationAlgorithm class and provides the own features of this algorithm.
Another class required to be implemented is the evaluator, which computes the ﬁtness of
the individuals. This class, named BojarczukEvaluator in this algorithm, inherits from the
JCLEC core AbstractParal lelEvaluator class or from the AbstractEvaluator class, depending
on whether the individuals are evaluated in a sequential or parallel way.
Finally, a class to deﬁne the grammar to be followed in the individual generation stage
is implemented. This class, named BojarczukSyntaxTreeSpecies in this example, inherits
from the class SyntaxTreeSpecies since G3P individuals are deﬁned in this algorithm.
Only deﬁning these three classes, the complete classiﬁcation algorithm is represented.
Due to the core of this module is JCLEC, before an algorithm is ready to run, it is necessary
to carry out a set-up process by using a conﬁguration ﬁle as shown in Figure 1. This
conﬁguration ﬁle and the steps required to execute the algorithm are described in the
JCLEC website. In this ﬁle we specify those parameters required such as the algorithm
to be run, the parent selector, the genetic operators, the evaluator, etc. All the required
parameters are provided by JCLEC, existing a numerous variety of them as it is described
in the JCLEC speciﬁcation (Ventura et al., 2007).

3. Documentation and Requirements

The JCLEC-Classiﬁcation online documentation3 describes the software packages, presents
a user oriented usage example, as well as developer information to include new algorithms,
API reference and running tests. JCLEC requires Java 1.7, Apache commons logging 1.1,

1. KEEL website at http://www.keel.es
2. ARFF format description at http://www.cs.waikato.ac.nz/ml/weka/arff.html
3. JCLEC documentation at http://jclec.sourceforge.net/data/JCLEC- classification.pdf

493

Cano, Luna, Zafra and Ventura

<e x p e r i m e n t>
< p r o c e s s a l g o r i t h m −t y p e = ‘ ‘ n e t . s f . j c l e c . p r o b l em . c l a s s i f i c a t i o n . b o j a r c z u k . B o j a r c z u k A l g o r i t h m ’ ’>
<r a n d−g e n− f a c t o r y s e e d = ‘ ‘ 1 2 3 4 5 6 7 8 9 ’ ’
t y p e = ‘ ‘ n e t . s f . j c l e c . u t i l . random . R a n e c u F a c t o r y ’ ’ />
<p o p u l a t i o n − s i z e>1 0 0</ p o p u l a t i o n − s i z e>
<max−o f −g e n e r a t i o n s>1 0 0</max−o f −g e n e r a t i o n s>
<max−d e r i v − s i z e>2 0</max−d e r i v − s i z e>
< d a t a s e t
t y p e = ‘ ‘ n e t . s f . j c l e c . p r o b l em . u t i l . d a t a s e t . A r f f D a t a S e t ’ ’>
< t r a i n −d a t a>d a t a / i r i s / i r i s −10−1 t r a . a r f f</ t r a i n −d a t a>
< t e s t −d a t a>d a t a / i r i s / i r i s −10−1 t s t . a r f f</ t e s t −d a t a>
< a t t r i b u t e −c l a s s −name>C l a s s</ a t t r i b u t e −c l a s s −name>
</ d a t a s e t>
< r e c o m b i n a t i o n −p r o b> 0 . 8</ r e c o m b i n a t i o n −p r o b>
<c o p y−p r o b> 0 . 0 1</ c o p y−p r o b>
< l i s t e n e r
t y p e = ‘ ‘ n e t . s f . j c l e c . p r o b l em . c l a s s i f i c a t i o n . l i s t e n e r . R u l e B a s e R e p o r t e r ’ ’>
< r e p o r t −d i r −name> r e p o r t s / r e p o r t F r e i t a s</ r e p o r t −d i r −name>
< g l o b a l −r e p o r t −name>s u m m a r y F r e i t a s</ g l o b a l −r e p o r t −name>
< r e p o r t −f r e q u e n c y>1 0</ r e p o r t −f r e q u e n c y>
</ l i s t e n e r>
</ p r o c e s s>
</ e x p e r i m e n t>

Figure 1: Sample conﬁguration ﬁle

Apache commons collections 3.2, Apache commons conﬁguration 1.5, Apache commons lang
2.4, and JUnit 3.8 (for running tests).

Acknowledgments

This research was supported by the Spanish Ministry of Science and Technology pro ject
TIN-2011-22408, the Ministry of Education FPU grants AP2010-0041 and AP2010-0042,
and FEDER funds.

References

C. C. Bo jarczuk, H. S. Lopes, A. A. Freitas, and E. L. Michalkiewicz. A constrained-syntax
genetic programming system for discovering classiﬁcation rules: application to medical
data sets. Artiﬁcial Intel ligence in Medicine, 30(1):27–48, 2004.

I. De Falco, A. Della Cioppa, and E. Tarantino. Discovering interesting classiﬁcation rules
with genetic programming. Applied Soft Computing, 1(4):257–269, 2001.

P. G. Espejo, S. Ventura, and F. Herrera. A survey on the application of genetic program-
ming to classiﬁcation. IEEE Transactions on Systems, Man, and Cybernetics, Part C,
40(2):121–144, 2010.

R. McKay, N. Hoai, P. Whigham, Y. Shan, and M. O’Neill. Grammar-based genetic pro-
gramming: a survey. Genetic Programming and Evolvable Machines, 11:365–396, 2010.

K. C. Tan, A. Tay, T. H. Lee, and C. M. Heng. Mining multiple comprehensible classiﬁcation
rules using genetic programming.
In Proceedings of the Evolutionary Computation on
2002. CEC ’02, volume 2, pages 1302–1307, 2002.

S. Ventura, C. Romero, A. Zafra, J.A. Delgado, and C. Herv´as. JCLEC: a Java framework
for evolutionary computation. Soft Computing, 12:381–392, 2007.

494

Journal of Machine Learning Research 16 (2015) 1-46

Submitted 1/14; Revised 7/14; Published 1/15

Statistical Decision Making for Optimal Budget Allocation
in Crowd Labeling

Xi Chen
Stern School of Business
New York University
New York, New York, 10012, USA

Qihang Lin
Tippie Col lege of Business
University of Iowa
Iowa City, Iowa, 52242, USA

Dengyong Zhou
Microsoft Research
Redmond, Washington, 98052, USA

Editor: Yuan (Alan) Qi

xichen@nyu.edu

qihang-lin@uiowa.edu

dengyong.zhou@microsoft.com

Abstract

It has become increasingly popular to obtain machine learning labels through commercial
crowdsourcing services. The crowdsourcing workers or annotators are paid for each label
they provide, but the task requester usually has only a limited amount of the budget. Since
the data instances have diﬀerent levels of labeling diﬃculty and the workers have diﬀerent
reliability for the labeling task, it is desirable to wisely allocate the budget among all the
instances and workers such that the overall labeling quality is maximized. In this paper,
we formulate the budget allocation problem as a Bayesian Markov decision process (MDP),
which simultaneously conducts learning and decision making. The optimal allocation pol-
icy can be obtained by using the dynamic programming (DP) recurrence. However, DP
quickly becomes computationally intractable when the size of the problem increases. To
solve this challenge, we propose a computationally eﬃcient approximate policy which is
called optimistic knowledge gradient. Our method applies to both pull crowdsourcing mar-
ketplaces with homogeneous workers and push marketplaces with heterogeneous workers.
It can also incorporate the contextual information of instances when they are available.
The experiments on both simulated and real data show that our policy achieves a higher
labeling quality than other existing policies at the same budget level.
Keywords:
crowdsourcing, budget allocation, Markov decision process, dynamic pro-
gramming, optimistic knowledge gradient

1. Introduction

In many real applications, data are usually collected without any innate label. For example,
a digital camera will not automatically tag a picture as a portrait or a landscape. A
traditional approach for data labeling is to hire a small group of experts to provide labels
for the entire set of data. However, for large-scale data, such an approach becomes ineﬃcient
and very costly. Thanks to the advent of many online crowdsourcing services, e.g., Amazon

c(cid:13)2015 Xi Chen and Qihang Lin and Dengyong Zhou.

Chen, Lin and Zhou

Mechanical Turk, a much more eﬃcient way is to post unlabeled data to a crowdsourcing
marketplace, where a big crowd of low-paid workers can be hired instantaneously to perform
labeling tasks.
Despite its high eﬃciency and immediate availability, crowd labeling raises many new
challenges. Since labeling tasks are tedious and workers are usually non-experts, labels
generated by the crowd suﬀer from low quality. As a remedy, most crowdsourcing services
resort to labeling redundancy to reduce the labeling noise, which is achieved by collecting
multiple labels from diﬀerent workers for each data instance. In particular, a crowd labeling
process can be described as a two phase procedure:

1. In the ﬁrst phase, unlabeled data instances are assigned to a crowd of workers and
multiple raw labels are collected for each data instance.

2. In the second phase, for each data instance, one aggregates the collected raw labels
to infer its true label.

In principle, more raw labels will lead to a higher chance of recovering the true label.
However, each raw label comes with a cost: the requester has to pay workers pre-speciﬁed
monetary reward for each label they provide, usually, regardless of the label’s correctness.
For example, a worker typically earns 10 cents by categorizing a website as porn or not.
In practice, the requester has only a limited amount of budget which essentially restricts
the total number of raw labels that he/she can collect. This raises a challenging question
central in crowd labeling: What is the best way to al locate the budget among data instances
and workers so that the overal l accuracy of aggregated labels is maximized ?
The most important factors that decide how to allocate the budget are the intrinsic char-
acteristics of data instances and workers: labeling diﬃculty/ambiguity for each data instance
and reliability/quality of each worker. In particular, an instance is less ambiguous if its label
can be decided based on the common knowledge and a vast ma jority of reliable workers will
provide the same label for it. In principle, we should avoid spending too much budget on
those easy instances since excessive raw labels will not bring much additional information.
In contrast, for an ambiguous instance which falls near the boundary of categories, even
those reliable workers will still disagree with each other and generate inconsistent labels.
For those ambiguous instances, we are facing a challenging decision problem on how much
budget that we should spend on them. On one hand, it is worth to collect more labels to
boost the accuracy of the aggregate label. On the other hand, since our goal is to maximize
the overal l labeling accuracy, when the budget is limited, we should simply put those few
highly ambiguous instances aside to save budget for labeling less diﬃcult instances.
In
addition to the ambiguity of data instances, the other important factor is the reliability
of each worker and, undoubtedly, it is desirable to assign more instances to those reliable
workers. Despite their importance in deciding how to allocate the budget, both the data
ambiguity and workers’ reliability are unknown parameters at the beginning and need to
be updated based on the stream of collected raw labels in an online fashion. This further
suggests that the budget allocation policy should be dynamic and simultaneously conduct
parameter estimation and decision making.
To search for an optimal budget allocation policy, we model the data ambiguity and
workers’ reliability using two sets of random variables drawn from known prior distribu-
tions. Then, we formulate the problem into a ﬁnite-horizon Bayesian Markov Decision

2

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

Process (MDP) (Puterman, 2005), whose state variables are the posterior distributions of
these variables, which are updated by each new label. Here, the Bayesian setting is nec-
essary. We will show that an optimal policy only exists in the Bayesian setting. Using
the MDP formulation, the optimal budget allocation policy for any ﬁnite budget level can
be readily obtained via the dynamic programming (DP). However, DP is computationally
intractable for large-scale problems since the size of the state space grows exponentially in
budget level. The existing widely-used approximate policies, such as approximate Gittins
index rule (Gittins, 1989) or knowledge gradient (KG) (Gupta and Miescke, 1996; Frazier
et al., 2008), either has a high computational cost or poor performance in our problem. In
this paper, we propose a new policy, called optimistic know ledge gradient (Opt-KG). In par-
ticular, the Opt-KG policy dynamically chooses the next instance-worker pair based on the
optimistic outcome of the marginal improvement on the accuracy, which is a function of state
variables. We further propose a more general Opt-KG policy using the conditional value-
at-risk measure (Rockafellar and Uryasev, 2002). The Opt-KG is computationally eﬃcient,
achieves superior empirical performance and has some asymptotic theoretical guarantees.
To better present the main idea of our MDP formulation and the Opt-KG policy, we
start from the binary labeling task (i.e., providing the category, either positive or negative,
for each instance). We ﬁrst consider the pul l marketplace (e.g., Amazon Mechanical Turk
or Galaxy Zoo) , where the labeling requester can only post instances to the general worker
pool with either anonymous or transient workers, but cannot assign to an identiﬁed worker.
In a pull marketplace, workers are typically treated as homogeneous and one models the
entire worker pool instead of each individual worker. We further assume that workers are
fully reliable (or noiseless) such that the chance that they make an error only depend on
instances’ own ambiguity. At a ﬁrst glance, such an assumption may seem oversimpliﬁed.
In fact, it turns out that the budget-optimal crowd labeling under such an assumption has
been highly non-trivial. We formulate this problem into a Bayesian MDP and propose the
computational eﬃcient Opt-KG policy. We further prove that the Opt-KG policy in such a
setting is asymptotically consistent, that is, when the budget goes to inﬁnity, the accuracy
converges to 100% almost surely.
Then, we extend the MDP formulation to deal with push marketplaces with heteroge-
neous workers. In a push marketplace (e.g., data annotation team in Microsoft Bing group),
once an instance is allocated to an identiﬁed worker, the worker is required to ﬁnish the
instance in a short period of time. Based on the previous model for fully reliable workers,
we further introduce another set of parameters to characterize workers’ reliability. Then
our decision process simultaneously selects the next instance to label and the next worker
for labeling the instance according to the optimistic knowledge gradient policy.
In fact,
the proposed MDP framework is so ﬂexible that we can further extend it to incorporate
contextual information of instances whenever they are available (e.g., as in web search and
advertising applications discussed in Li et al., 2010) and to handle multi-class labeling.
In summary, the main contribution of the paper consists of the three folds: (1) we
formulate the budget allocation in crowd labeling into a MDP and characterize the optimal
policy using DP; (2) computationally, we propose an eﬃcient approximate policy, optimistic
knowledge gradient; (3) the proposed MDP framework can be used as a general framework
to address various budget allocation problems in crowdsourcing (e.g., rating and ranking
tasks).

3

Chen, Lin and Zhou

The rest of this paper is organized as follows. In Section 2, we ﬁrst present the modeling
of budget allocation process for binary labeling tasks with fully reliable workers and motivate
our Bayesian modeling. In Section 3, we present the Bayesian MDP and the optimal policy
via DP. In Section 4, we propose a computationally eﬃcient approximate policy, Opt-KG.
In Section 5, we extend our MDP to model heterogeneous workers with diﬀerent reliability.
In Section 6, we present other important extensions, including incorporating contextual
information and multi-class labeling. In Section 7, we discuss the related works. In Section
8, we present numerical results on both simulated and real data sets, followed by conclusions
in Section 9.

2. Binary Labeling with Homogeneous Noiseless Workers

We ﬁrst consider the budget allocation problem in a pull marketplace with homogeneous
noiseless workers for binary labeling tasks. We note that such a simpliﬁcation is important
for investigating this problem, since the incorporation of workers’ reliability and extensions
to multiple categories become rather straightforward once this problem is correctly modeled
(see Section 5 and 6).
Suppose that there are K instances and each one is associated with a latent true label
Zi ∈ {−1, 1} for 1 ≤ i ≤ K . Our goal is to infer the set of positive instances, denoted by
H ∗ = {i : Zi = 1}. Here, we assume that the homogeneous worker pool is ful ly reliable
or noiseless. We note that it does not mean that each worker knows the true label Zi .
Instead, it means that fully reliable workers will do their best to make judgments but their
labels may be still incorrect due to the instance’s ambiguity. Further, we model the labeling
diﬃculty/ambiguity of each instance by a latent soft-label θi , which can be interpreted as the
percentage of workers in the homogeneous noiseless crowd who will label the i-th instance
as positive. In other words, if we randomly choose a worker from a large crowd of fully
reliable workers, we will receive a positive label for the i-th instance with probability θi and
a negative label with probability 1 − θi . In general, we assume the crowd is large enough
so that the value of θi can be any value in [0, 1]. To see how θi characterizes the labeling
diﬃculty of the i-th instance, we consider a concrete example where a worker is asked to
label a person as adult (positive) or not (negative) based on the photo of that person. If
the person is more than 25 years old, most likely, the corresponding θi will be close to 1,
generating positive labels consistently. On the other hand, if the person is younger than
15, she may be labeled as negative by almost all the reliable workers since θi is close to 0.
In both of this cases, we regard the instance (person) easy to label since Zi can be inferred
with a high accuracy based on only a few raw labels. On the contrary, for a person is one
or two years below or above 18, the θi is near 0.5 and the numbers of positive and negative
labels become relatively comparable so that the corresponding labeling task is very diﬃcult.
Given the deﬁnition of soft labels, we further make the following assumption:

Assumption 1 We assume that the soft-label θi is consistent with the true label in the
sense that Zi = 1 if and only if θi ≥ 0.5, i.e., the majority of the crowd are correct, and
hence H ∗ = {i : θi ≥ 0.5}.

Given the total budget, denoted by T , we suppose that each label costs one unit of
budget. As discussed in the introduction, the crowd labeling has two phases. The ﬁrst

4

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

Instance
Instance 1 (θ1 )
Instance 2 (θ2 )
Instance 3 (θ3 )

1st round label

2nd round label

1

1

1

1
−1

Table 1: Toy example with 3 instances and 5 collected labels. Instance 1 has two positive
labels, instance 2 has one positive and one negative label, and instance 3 has only
one positive label. The question is that, given only one more labeling chance,
which instance should be chosen to label?

phase is the budget al location phase, which is a dynamic decision process with T stages.
In each stage 0 ≤ t ≤ T − 1, an instance it ∈ A = {1, . . . , K } is selected based on the
historical labeling results. Once it is selected, it will be labeled by a random worker from
the homogeneous noiseless worker pool. According to the deﬁnition of θit , the label received,
denoted by yit ∈ {−1, 1}, will follow the Bernoulli distribution with the parameter θit :
Pr (yit = −1) = 1 − θit .

Pr (yit = 1) = θit

and

(1)

We note that, at this moment, all workers are assumed to be homogeneous and noiseless
so that yit only depends on θit but not on which worker provides the label. Therefore, it is
suﬃce for the decision maker (e.g., requester or crowdsourcing service) to select the instance
in each stage instead of an instance-worker pair.
The second phase is the label aggregation phase. When the budget is exhausted, the
decision maker needs to infer true labels {Zi}n
i=1 by aggregating all the collected labels.
According to Assumption 1, it is equivalent to infer the set of positive instances whose
θi ≥ 0.5. Let HT be the estimated positive set. The ﬁnal overall accuracy is measured by
|HT ∩ H ∗ | + |(HT )c ∩ (H ∗ )c |, the size of the mutual overlap between H ∗ and HT .
Our goal is to determine the optimal al location policy, (i0 , . . . , iT −1 ), so that overall
accuracy is maximized. Here, a natural question to ask is whether the optimal allocation
policy exists and what assumptions do we need for the existence of the optimal policy.
To answer this question, we provide a concrete example, which motivates our Bayesian
modeling.

2.1 Why We Need a Bayesian Modeling

Let us check a toy example with 3 instances and 5 collected labels (see Table 1). We assume
that the workers are homogeneous noiseless and the label aggregation is performed by the
ma jority vote rule. Now if we only have the budget to get one more label, which instance
should be chosen to label? It is obvious that we should not put the remaining budget on
the ﬁrst instance since we are relatively more conﬁdent on what its true label should be.
Thus, the problem becomes how to choose between the second and third instances. In what
follows, we shall show that there is no optimal policy under the frequentist setting. To be
more explicit, the optimal policy leads to the expected accuracy which is at least as good
as that of all other policies for any values of {θi}n
i=1 .

5

Chen, Lin and Zhou

Current Accuracy

y = 1

θ1 > 0.5
θ1 < 0.5
θ2 > 0.5
θ2 < 0.5
θ3 > 0.5
θ3 < 0.5

1

0

0.5

0.5

1

0

1

0

1

0

1

0

y = −1 Expected Accuracy
1
1

0

0

1

0.5

0.5

0
θ2
1 − θ2
θ3 + 0.5(1 − θ3 )
0.5(1 − θ3 )

Improvement

0

0
θ2 − 0.5 > 0
0.5 − θ2 > 0
0.5(θ3 − 1) < 0
0.5(1 − θ3 ) > 0

Table 2: Expected improvements in accuracy for collecting an extra label, i.e., the expected
accuracy of obtaining one more label minus the current expected accuracy. The 3rd
and 4th columns contain the accuracies with the next label being 1 and −1. The
5th is the expected accuracy which is computed by taking θ times the 3rd column
plus (1 − θ) times the 4th. The last column contains the expected improvements
which is computed by taking the diﬀerence between the 5th and 2nd columns.

Figure 1: Decision Boundary.

Let us compute the expected improvement in accuracy in terms of the frequentist risk
in Table 2. We assume that θi (cid:54)= 0.5 and if the number of 1 and −1 labels are the same
for an instance, the accuracy is 0.5 based on a random guess. From Table 2, we should not
label the ﬁrst instance since the improvement is always 0. This coincides with our intuition.
When max(θ2 − 0.5, 0.5 − θ2 ) > 0.5(1 − θ3 ) or θ3 > 0.5, which corresponds to the blue region
in Figure 1, we should choose to label the second instance. Otherwise, we should ask the
label for the third one. Since the true value of θ2 and θ3 are unknown, a optimal policy
does not exist under the frequentist paradigm. Further, it will be diﬃcult to estimate θ2
and θ3 accurately when the budget is very limited.
In contrast, in a Bayesian setting with prior distribution on each θi , the optimal policy
is deﬁned as the policy which leads to the highest expected accuracy under the given prior

6

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

instead of for any possible values of {θi}n
i=1 . Therefore, we can optimally determine the next
instance to label by taking another expectation over the distribution of θi . In this paper, we
adopt the Bayesian modeling to formulate the budget allocation problem in crowd labeling.

3. Bayesian MDP and Optimal Policy

In this section, we ﬁrst introduce the Bayesian MDP for modeling the dynamic budget allo-
cation process and then provide the optimal allocation policy using dynamic programming.

3.1 Bayesian Modeling

We assume that each θi is drawn from a known Beta prior Beta(a0
i , b0
i ). Beta is a rich
family of distributions in the sense that it exhibits a fairly wide variety of shapes on the
domain of θi , i.e., the unit interval [0, 1]. For presentation simplicity, instead of considering
a full Bayesian model with hyper-priors on a0
i and b0
i , we ﬁx a0
i and b0
i at the beginning.
In practice, if the budget is suﬃcient, one can ﬁrst label each instance equally many times
to pre-estimate {a0
i }K
i , b0
i=1 before the dynamic labeling procedure is invoked. Otherwise,
when there is no prior knowledge, we can simply assume a0
i = b0
i = 1 so that the prior is
a uniform distribution. According to our simulated experimental results in Section 8.1.2,
uniform prior works reasonably well unless the data is highly skewed in terms of class
distribution. Other commonly used uninformative priors such as Jeﬀreys prior or reference
prior (Beta(1/2, 1/2)) or Haldane prior (Beta(0, 0)) can also be adopted (see Robert, 2007
for more on uninformative priors). Choices of prior distributions are discussed in more
details in Section 4.2.
i , bt
At each stage t with Beta(at
i ) as the current posterior distribution for θi , we make a de-
cision by choosing an instance it ∈ A = {1, . . . , K } and acquire its label yit ∼ Bernoulli(θit ).
Here A denotes the action set. By the fact that Beta is the conjugate prior of the Bernoulli,
(cid:40)
the posterior of θit in the stage t + 1 will be updated as:
+ 1, bt
Beta(at
yit = 1;
)
if
it
it
yit = −1.
, bt
Beta(at
+ 1)
if
it
it
i=1 into a K × 2 matrix S t , called a state matrix, and let S t
We put {at
i }K
i , bt
i = (at
i , bt
i ) be the
(cid:40)
i-th row of S t . The update of the state matrix can be written in a more compact form:
S t + (eit , 0)
if yit = 1;
if yit = −1,
S t + (0, eit )
where eit is a K × 1 vector with 1 at the it -th entry and 0 at all other entries. As we
can see, {S t} is a Markovian process because S t+1 is completely determined by the current
state S t , the action it and the obtained label yit . It is easy to calculate the state transition
probability Pr(yit |S t , it ), which is the posterior probability that we are in the next state
S t+1 if we choose it to be label in the current state S t :

Beta(at+1
it

S t+1 =

, bt+1
it

) =

(2)

Pr(yit = 1|S t , it ) = E(θit |S t ) =

at
it
+ bt
it

at
it

and Pr(yit = −1|S t , it ) =

bt
it
+ bt
it

.

at
it

(3)

7

Chen, Lin and Zhou

Given this labeling process, the budget allocation policy is deﬁned as a sequence of
decisions: π = (i0 , . . . , iT −1 ). Here, we require decisions depend only upon the previous
information. To make this more formal, we deﬁne a ﬁltration {Ft}T
t=0 , where Ft is the
information collected until the stage t − 1. More precisely, Ft is the the σ -algebra generated
by the sample path (i0 , yi0 , . . . , it−1 , yit−1 ). We require the action it is determined based on
the historical labeling results up to the stage t − 1, i.e., it is Ft -measurable.

3.2 Inference About True Labels

As described in Section 2, the budget allocation process has two phases: the dynamic budget
allocation phase and the label aggregation phase. Since the goal of the dynamic budget
allocation in the ﬁrst phase is to maximize the accuracy of aggregated labels in the second
phase, we ﬁrst present how to infer the true label via label aggregation in the second phase.
When the decision process terminates at the stage T , we need to determine a positive set
HT to maximize the conditional expected accuracy conditioning on FT , which corresponds
to minimizing the posterior risk:
(cid:0)1(i ∈ H ) · 1(i ∈ H ∗ ) + 1(i (cid:54)∈ H ) · 1(i (cid:54)∈ H ∗ )(cid:1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)FT
(cid:32) K(cid:88)
(cid:33)
i=1
where 1(A) is the indicator function, which takes the value 1 if the event A is true and 0
otherwise. The term inside expectation in (4) is the binary labeling accuracy which can
also be written as |H ∩ H ∗ | + |H c ∩ (H ∗ )c |.
We ﬁrst observe that, for 0 ≤ t ≤ T , the conditional distribution θi |Ft is exactly the
posterior distribution Beta(at
i , bt
i ), which depends on the historical sampling results only
i , bt
i = (at
through S t
i ). Hence, we deﬁne
= Pr(θ ≥ 0.5|θ ∼ Beta(a, b)),
.
I (a, b)
= Pr(i ∈ H ∗ |Ft ) = Pr(θi ≥ 0.5|Ft ) = Pr(θi ≥ 0.5|S t
.
P t
i , bt
i ) = I (at
i ).
i

HT = arg max
H⊂{1,...,K }

(4)

(5)

(6)

E

,

As shown in Xie and Frazier (2013), the optimal positive set HT can be determined by the
Bayes decision rule as follows.
Proposition 2 HT = {i : Pr(i ∈ H ∗ |FT ) ≥ 0.5} = {i : P T
i ≥ 0.5} solves (4).
The proof of Proposition 2 is given in the appendix for completeness.
With Proposition 2 in place, we plug the optimal positive set HT into the right hand
(cid:0)1(i ∈ HT ) · 1(i ∈ H ∗ ) + 1(i (cid:54)∈ HT ) · 1(i (cid:54)∈ H ∗ )(cid:1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)FT
(cid:33)
(cid:32) K(cid:88)
side of (4) and the conditional expected accuracy given FT can be simpliﬁed as:
K(cid:88)
i=1
i=1
= max(x, 1 − x). We also note that P T
.
where h(x)
i provides not only the estimated label
for the i-th instance but also how conﬁdent the estimated label is correct. According to the
next corollary with the proof in the appendix, we show that the optimal HT is constructed
based on a reﬁned majority vote rule which incorporates the prior information.

h(P T
i ),

(7)

E

=

8

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

Corollary 3 I (a, b) > 0.5 if and only if a > b and I (a, b) = 0.5 if and only if a = b.
Therefore, HT = {i : aT
i ≥ bT
i } solves (4).
i as pseudo-counts of 1s and −1s at the initial stage, the parameters
By viewing a0
i and b0
i ≥ bT
i are the total counts of 1s and −1s. The estimated positive set HT = {i : aT
i }
aT
i and bT
consists of instances with more (or equal) counts of 1s than that of −1s. When a0
i = b0
i ,
HT is constructed exactly according to the vanilla majority vote rule.
To ﬁnd the optimal allocation policy which maximizes the expected accuracy, we need
(cid:0)1(i ∈ HT ) · 1(i ∈ H ∗ ) + 1(i (cid:54)∈ HT ) · 1(i (cid:54)∈ H ∗ )(cid:1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)FT
(cid:34)
(cid:32) K(cid:88)
(cid:33)(cid:35)
to solve the following optimization problem:
E
(cid:32) K(cid:88)
(cid:33)
i=1
h(P T
,
i )
i=1
where Eπ represents the expectation taken over the sample paths (i0 , yi0 , . . . , iT −1 , yiT −1 )
generated by a policy π . The second equality is due to Proposition 2 and V (S 0 ) is called
value function at the initial state S 0 . The optimal policy π∗ is any policy π that attains
the supremum in (8).

.
= sup
π

= sup
π

V (S 0 )

Eπ

Eπ

(8)

3.3 Markov Decision Process

The optimization problem in (8) is essentially a Bayesian multi-armed bandit (MAB) prob-
lem, where each instance corresponds to an arm and the decision is which instance/arm
to be sampled next. However, it is diﬀerent from the classical MAB problem (Auer et al.,
2002; Bubeck and Cesa-Bianchi, 2012), which assumes that each sample of an arm yields
independent and identically distributed (i.i.d.) reward according to some unknown distri-
bution associated with that arm. Given the total budget T , the goal is to determine a
sequential allocation policy so that the collected rewards can be maximized. We contrast
this problem with our problem: instead of collecting intermediate independent rewards on
the ﬂy, our ob jective in (8) merely involves the ﬁnal “reward”, i.e., overall labeling accuracy,
which is only available at the ﬁnal stage when the budget runs out. Although there is no
intermediate reward in our problem, we can still decompose the ﬁnal expected accuracy into
sum of stage-wise rewards using the technique from Xie and Frazier (2013), which further
leads to our MDP formulation. Since these stage-wise rewards are artiﬁcially created, they
are no longer i.i.d.
for each instance. We also note that the problem in Xie and Frazier
(2013) is an inﬁnite-horizon one which optimizes the stopping time while our problem is
ﬁnite-horizon since the decision process must be stopped at the stage T .
(cid:33)
(cid:32) K(cid:88)
) − K(cid:88)
i )(cid:12)(cid:12)S t , it
Proposition 4 Deﬁne the stage-wise expected reward as:
= E (cid:0)h(P t+1
h(P t
it
i=1
i=1

) − h(P t
it )|S t , it

R(S t , it ) = E

h(P t+1
i

(cid:1) ,

(9)

then the value function (8) becomes:

9

Eπ

,

(10)

(cid:33)

R(S t , it )

Chen, Lin and Zhou
(cid:32)T −1(cid:88)
V (S 0 ) = G0 (S 0 ) + sup
where G0 (S 0 ) = (cid:80)K
π
t=0
i ) and the optimal policy π∗ is any policy π that attains the
i=1 h(P 0
supremum.
in (9) has a straightforward interpretation. According to (8), the term (cid:80)K
The proof of Proposition 4 is presented in the appendix. In fact, the stage-wise reward
i=1 h(P t
i ) is the
expected accuracy at the t-th stage. The stage-wise reward R(S t , it ) takes the form of the
diﬀerence between the expected accuracy at the (t + 1)-stage and the t-th stage, i.e., the
expected gain in accuracy for collecting another label for the it -th instance. The second
equality in (9) holds simply because: only the it -th instance receives the new label and the
changes while all other P t
corresponding P t
i remain the same. Since the expected reward
(cid:1) ,
(cid:1) = R (cid:0)at
R(S t , it ) = R (cid:0)S t
it
, bt
= (at
(9) only depends on S t
), we write
it
it
it
it , bt
it
it
and use them interchangeably. The function R(a, b) with two parameters a and b has an
analytical representation as follows. For any state (a, b) of a single instance, the reward of
getting a label 1 and a label −1 are:
R1 (a, b) = h(I (a + 1, b)) − h(I (a, b)),
R2 (a, b) = h(I (a, b + 1)) − h(I (a, b)).

(13)

(12)

(11)

The expected reward takes the following form:

R(a, b) = p1R1 + p2R2 ,

(14)

where p1 = a
a+b and p2 = b
a+b are the transition probabilities in (3).
With Proposition 4, the maximization problem (8) is formulated as a T -stage Markov
Decision Process (MDP) as in (10), which is associated with a tuple:
{T , {S t}, A, Pr(yit |S t , it ), R(S t , it )}.
Here, the state space at the stage t, S t , is all possible states that can be reached at t. Once
(cid:41)
(cid:40)
we collect a label yit , one element in S t (either at
or bt
) will add one. Therefore, we have
K(cid:88)
it
it
i − b0
i − a0
i ≥ b0
i ≥ a0
i }K
{at
i ) + (bt
(at
i , bt
i=1 : at
i , bt
i ) = t
i ,
i=1
The action space is the set of instances that could be labeled next: A = {1, . . . , K }. The
transition probability Pr(yit |S t , it ) is deﬁned in (3) and the expected reward at each stage
R(S t , it ) is deﬁned in (9).

S t =

(15)

.

Remark 5 We can also view Proposition 4 as a consequence of applying the reward shap-
ing technique (Ng et al., 1999) to the original problem (8). In fact, we can add an artiﬁcial

10

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

Instance i S T −1
i
1
(3,1)

2

3

(2,2)

(2,1)

p1
3
4
1
2
2
3

p2 R1 (S T −1
i
1
0.0625
4
1
2
1
3

0.1875

0.1250

) R2 (S T −1
)
i
−0.1875
0.1875
−0.2500

R(S T −1 , i) = R(S T −1
)
i
4 · (0.0625) + 1
4 · (−0.1875) = 0
3
2 · (0.1875) + 1
2 · (0.1875) = 0.1875
1
3 · (0.1250) + 1
3 · (−0.2500) = 0
2

Table 3: Calculation of the expected reward for the toy example in Table 1 according to
(12), (13) and (14).

absorbing state, named Sobs , to the original state space (15) and assume that, when the bud-
zero transition reward until the state enters Sobs where the transition reward is (cid:80)K
get al location process ﬁnishes, the state must transit one more time to reach Sobs regard less
of which action is taken. Hence, the original problem (8) becomes a MDP that generates a
space as Φ(S t ) = (cid:80)K
i=1 h(P T
i ).
Then, we deﬁne a potential-based shaping function (Ng et al., 1999) over this extended state
i ) for S t ∈ S t and Φ(Sobs ) = 0. After this, (4) can be viewed
i=1 h(P t
as a new MDP whose transition reward equals that of (8) plus the shaping-reward function
Φ(S (cid:48) ) − Φ(S ) when the state transits from S to S (cid:48) . According to Theorem 1 in Ng et al.
(1999), (4) and (8) have the same optimal policy. This provides an alternative justiﬁcation
for Proposition 4.

3.4 Optimal Policy via DP

With the MDP in place, we can apply the dynamic programming (DP) algorithm (a.k.a.
backward induction) (Puterman, 2005) to compute the optimal policy:
1. Set VT −1 (S T −1 ) = maxi∈{1,...,K } R(S T −1 , i) for al l possible states S T −1 ∈ S T −1 . The
optimal decision i∗
T −1 (S T −1 ) is the decision i that achieves the maximum when the
state is S T −1 .
2. Iterate for t = T − 2, . . . , 0, compute the Vt (S t ) for all possible S t ∈ S t using the
Bellman equation:
(cid:0)S t + (0, ei )(cid:1)(cid:17)
(cid:16)
(cid:0)S t + (ei , 0)(cid:1) + Pr(yi = −1|S t , i)Vt+1
Vt (S t )
R(S t , i) + Pr(yi = 1|S t , i)Vt+1
= max
i
and i∗
t (S t ) is the i that achieves the maximum.
0 , . . . , i∗
The optimal policy π∗ = (i∗
T ). For an illustration purpose, we use DP to calculate
the optimal instance to be labeled next in the toy example in Section 2.1 under the uniform
prior B (1, 1) for all θi . Since we assume that there is only one labeling chance remaining,
which corresponds to the last stage of DP, we should choose the instance i∗
T −1 (S T −1 ) =
arg maxi∈{1,...,K } R(S T −1 , i). According to the calculation in Table 3, there is a unique
optimal instance for labeling, which is the second instance.
Although DP ﬁnds the optimal policy, its computation is intractable since the size of the
state space |S t | grows exponentially in t according to (15). Therefore, we need to develop
a computationally eﬃcient approximate policy, which is the goal of the next section.

,

11

Chen, Lin and Zhou

4. Approximate Policies

Since DP is computationally intractable, approximate policies are needed for large-scale
applications. The simplest policy is the uniform sampling (a.k.a, pure exploration), i.e., we
choose the next instance uniformly and independently at random: it ∼ Uniform(1, . . . , K ).
However, this policy does not explore any structure of the problem.
With the decomposed reward function, our problem is essentially a ﬁnite-horizon Bayesian
MAB problem. Gittins (1989) showed that Gittins index policy is optimal for inﬁnite-
horizon MAB with the discounted reward. It has been applied to the inﬁnite-horizon ver-
sion of problem (10) in Xie and Frazier (2013). Since our problem is ﬁnite-horizon, Gittins
index is no longer optimal while it can still provide us a good heuristic index rule. However,
the computational cost of Gittins index is very high: the state-of-art-method proposed by
Nino-Mora (2011) requires O(T 6 ) time and space complexity.
A computationally more attractive policy is the knowledge gradient (KG) (Gupta and
Miescke, 1996; Frazier et al., 2008). It is essentially a single-step look-ahead policy, which
(cid:18)
(cid:19)
greedily selects the next instance with the largest expected reward:
i , bt
R(at
i )

i , bt
R1 (at
i ) +

(16)

i , bt
R2 (at
i )

.

it = arg max
i∈{1,...,K }

.
=

at
i
i + bt
at
i

bt
i
i + bt
at
i

As we can see, this policy corresponds to the last stage in DP and hence KG policy is
optimal if only one labeling chance is remaining.
When there is a tie, if we select the smallest index i, the policy is referred to deterministic
KG while if we randomly break the tie, the policy is referred to randomized KG. Although
KG has been successfully applied to many MDP problems (Powell, 2007), it will fail in our
problem as shown in the next proposition with the proof in the appendix.
i are positive integers and letting E = {i : a0
i },
Proposition 6 Assuming that a0
i = b0
i and b0
then the deterministic KG policy wil l acquire one label for each instance in E and then
consistently obtain the label for the ﬁrst instance even if the budget T goes to inﬁnity.

According to Proposition 6, the deterministic KG is not a consistent policy, where the
consistent policy refers to the policy that will provide correct labels for all instances (i.e.,
HT = H ∗ ) almost surely when T goes to inﬁnity. We note that randomized KG policy can
address this problem. However, from the proof of Proposition 6, randomized KG behaves
similarly to the uniform sampling policy in many cases and its empirical performance is
undesirable according to Section 8. In the next subsection, we will propose a new approx-
imate allocation policy based on KG which is a consistent policy with superior empirical
performance.

4.1 Optimistic Knowledge Gradient

The stage-wise reward can be viewed as a random variable with a two point distribution,
i.e., with the probability p1 = a
a+b of being R1 (a, b) and the probability p2 = b
a+b of being
R2 (a, b). The KG policy selects the instance with the largest expected reward. However, it
is not consistent.

12

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

Algorithm 1 Optimistic Knowledge Gradient
Input: Parameters of prior distributions for instances {a0
i }K
i , b0
i=1 and the budget T .
for t = 0, . . . , T − 1 do
(cid:0)R+ (at
i ))(cid:1) .
Select the next instance it to label according to:
.
i , bt
i ), R2 (at
i , bt
= max(R1 (at
i , bt
i )

(17)

it = arg max
i∈{1,...,K }
Acquire the label yit ∈ {−1, 1}.
if yit = 1 then
at+1
+ 1, bt+1
= at
it
it
it
else
at+1
it
end if
end for
i ≥ bT
Output: The positive set HT = {i : aT
i }.

, bt+1
it

= at
it

= bt
it

i for all i (cid:54)= it .
; at+1
i , bt+1
i = at
i = bt
i for all i (cid:54)= it .
+ 1; at+1
i , bt+1
i = at
i = bt

= bt
it

Figure 2: Illustration of R+ (a, b).

In this section, we introduce a new index policy called “optimistic knowledge gradient”
(Opt-KG) policy. The Opt-KG policy assumes that decision makers are optimistic in the
sense that they select the next instance based on the optimistic outcome of the reward. As
i , bt
a simplest version of the Opt-KG policy, for any state (at
i ), the optimistic outcome of the
i ) is deﬁned as maximum over the reward of obtaining the label 1, R1 (at
reward R+ (at
i , bt
i , bt
i ),
and the reward of obtaining the label −1, R2 (at
i , bt
i ). Then the optimistic decision maker
selects the next instance i with the largest R+ (at
i , bt
i ) as in (17) in Algorithm 1. The overall
decision process using the Opt-KG policy is highlighted in Algorithm 1.
In the next theorem, we prove that Opt-KG policy is consistent.

i and b0
Theorem 7 Assuming that a0
i are positive integers, the Opt-KG is a consistent
policy, i.e, as T goes to inﬁnity, the accuracy wil l be 100% (i.e., HT = H ∗ ) almost surely.

13

ab  12345678910109876543210.05 0.10.15 0.20.25Chen, Lin and Zhou

Figure 3: Illustration of Conditional Value-at-Risk.

The key of proving the consistency is to show that when T goes to inﬁnity, each instance
will be labeled inﬁnitely many times. We prove this by showing that for any pair of positive
integers (a, b), R+ (a, b) = max(R1 (a, b), R2 (a, b)) > 0 and R+ (a, b) → 0 when a + b → ∞.
As an illustration, the values of R+ (a, b) are plotted in Figure 2. Then, by strong law
of large number, we obtain the consistency of the Opt-KG as stated in Theorem 7. The
details are presented in the appendix. We have to note that asymptotic consistency is the
minimum guarantee for a good policy. However, it does not necessarily guarantee the good
empirical performance for the ﬁnite budget level. We will use experimental results to show
the superior performance of the proposed policy.
The proposed Opt-KG policy is a general framework for budget allocation in crowd
labeling. We can extend the allocation policy based on the maximum over the two possible
rewards (Algorithm 1) to a more general policy using the conditional value-at-risk (CVaR)
(Rockafellar and Uryasev, 2002). We note that here, instead of adopting the CVaR as a
risk measure, we apply it to the reward distribution. In particular, for a random variable
X with the support X (e.g., the random reward with the two point distribution), let α-
quantile function be denoted as Qα (X ) = inf {x ∈ X : α ≤ FX (x)}, where FX (·) is the CDF
of X . The value-at-risk VaRα (X ) is the smallest value such that the probability that X is
less than (or equal to) it is greater than (or equal to) 1 − α: VaRα (X ) = Q1−α (X ). The
conditional value-at-risk (CVaRα (X )) is deﬁned as the expected reward exceeding (or equal
to) VaRα (X ). An illustration of CVaR is shown in Figure 3.
For our problem, according to Rockafellar and Uryasev (2002), CVaRα (X ) can be ex-
pressed as a simple linear program:

{q1≥0,q2≥0} q1R1 + q2R2 ,
CVaRα (X ) = max
q1 ≤ 1
p1 , q2 ≤ 1
α
α

s.t.

p2 , q1 + q2 = 1.

As we can see, when α = 1, CVaRα (X ) = p1R1 + p2R2 , which is the expected reward;
when α → 0, CVaRα (X ) = max(R1 , R2 ), which is used as the selection criterion in (17) in
Algorithm 1. In fact, a more general Opt-KG policy could be selecting the next instance
with the largest CVaRα (X ) with a tuning parameter α ∈ [0, 1]. We can extend Theorem 7
to prove that the policy based on CVaRα (X ) is consistent for any α < 1. According to our

14

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

own experience, α → 0 usually has a better performance in our problem especially when
the budget is very limited. Therefore, for the sake of presentation simplicity, we introduce
the Opt-KG using max(R1 , R2 ) (i.e., α → 0 in CVaRα (X )) as the selection criterion.
Finally, we highlight that the Opt-KG policy is computationally very eﬃcient. For K
instances with T units of the budget, the overall time and space complexity are O(K T ) and
O(K ) respectively. It is much more eﬃcient that the Gittins index policy which requires
O(T 6 ) time and space complexity.

4.2 Discussions

It is interesting to see the connection between the idea of making the decision based on
the optimistic outcome of the reward and the UCB (upper conﬁdence bounds) policy (Auer
et al., 2002) for the classical multi-armed bandit problem as described in Section 3.3. In
particular, the UCB policy selects the next arm with the maximum upper conﬁdence index,
which is deﬁned as the current average reward plus the one-sided conﬁdence interval. As
we can see, the upper conﬁdence index can be viewed as an “optimistic” estimate of the
reward. However, we note that since we are in a Bayesian setting and our stage-wise rewards
are artiﬁcially created and thus not i.i.d. for each arm, the UCB policy (Auer et al., 2002)
cannot be directly applied to our problem.
In fact, our Opt-KG follows a more general principle of “optimism in the face uncer-
tainty” (Szita and L˝orincz, 2008). Essentially, the non-consistency of KG is due to its nature
of pure exploitation while a consistent policy should typically utilizes exploration. One of
the common techniques to handle the exploration-exploitation dilemma is to take an action
based on an optimistic estimation of the rewards (see Szita and L˝orincz, 2008; Even-Dar
and Mansour, 2001), which is the role R+ (a, b) plays in Opt-KG.
(cid:0)R− (at
i ))(cid:1) . However, as shown in the next
For our problem, it is also straightforward to design the “pessimistic knowledge gradient”
policy which selects the next instance it based on the pessimistic outcome of the reward,
.
i , bt
= min(R1 (at
i , bt
i ), R2 (at
i , bt
i.e., it = arg maxi
i )
proposition with the proof in the appendix, the pessimistic KG policy is inconsistent under
the uniform prior.

Proposition 8 When starting from the uniform prior (i.e., a0
i = b0
i = 1) for al l θi , the
pessimistic KG policy wil l acquire one label for each instance and then consistently acquire
the label for the ﬁrst instance even if the budget T goes to inﬁnity.

Finally, we discuss some other possible choices of prior distributions. For presentation
simplicity, we only consider the Beta prior for each θi with the ﬁxed parameters a0
i and b0
i .
In practice, more complicated priors can be easily incorporated into our framework. For
example, instead of using only one Beta prior, one can adopt a mixture of Beta distributions
as the prior and the posterior will also follow a mixture of Beta distributions, which allows
an easy inference about the posterior. As we show in the experiments (see Section 8.1.2),
the uniform prior does not work well when the data is highly skewed in terms of class
distribution. To address this problem, one possible choice is to adopt the prior p(θ) =
w1Beta(c, 1) + w2Beta(1, 1) + w3Beta(1, c) where w1 , w2 and w3 are the weights and c is
a constant larger than 1 (e.g., c = 5).
In such a prior, B (c, 1) corresponds to the data
with more positive labels while B (1, c) to the data with more negative labels. In addition

15

Chen, Lin and Zhou

to the mixture Beta prior, one can adopt the hierarchical Bayesian approach which puts
hyper-priors on the parameters in the Beta priors. The inference can be performed using
empirical Bayes approach (Gelman et al., 2013; Robert, 2007).
In particular, one can
periodically re-calculate the MAP estimate of the hyper-parameters based on the available
data and update the model, but otherwise proceed with the given hyper-parameters. For
common choices of hyper-priors of Beta, please refer to Section 5.3 in Gelman et al. (2013).
These approaches can also be applied to model the workers’ reliability as we introduced in
the next Section. For example, one can use a mixture of Beta distributions as the prior
for the workers’ reliability, where Beta(c, 1) corresponds to reliable workers, Beta(1, 1) to
random workers and Beta(1, c) to malicious or poorly informed workers.

5. Incorporate Reliability of Heterogeneous Workers

In push crowdsourcing marketplaces, it is important to model workers’ reliability so that
the decision maker could assign more instances to reliable workers. Assuming that there
are M workers in a push marketplace, we can capture the reliability of the j -th worker by
introducing an extra parameter ρj ∈ [0, 1] as in (Dawid and Skene, 1979; Raykar et al.,
2010; Karger et al., 2013b), which is deﬁned as the probability of getting the same label
as the one from a random fully reliable worker. Recall that the soft-label θi is the i-th
instance’s probability of being labeled as positive by a fully reliable worker and let zij be
the label provided by the j -th worker for the i-th instance. We model the distribution of
zij for given θi and ρj using the one-coin model (Dawid and Skene, 1979; Karger et al.,
2013b):
Pr(zij = 1|θi , ρj ) = Pr(zij = 1|yi = 1, ρj ) Pr(yi = 1|θi ) + Pr(zij = 1|yi = −1, ρj ) Pr(yi = −1|θi )
= ρj θi + (1 − ρj )(1 − θi );
(18)
Pr(zij = −1|θi , ρj ) = Pr(zij = −1|yi = −1, ρj ) Pr(yi = −1|θi ) + Pr(zij = −1|yi = 1, ρj ) Pr(yi = 1|θi )
= ρj (1 − θi ) + (1 − ρj )θi ,
(19)

where yi denotes the label provided a random fully reliable worker for the i-th instance.
We also note that it is straightforward to extend the current one-coin model to a more
complex two-coin model (Dawid and Skene, 1979; Raykar et al., 2010) by introducing a
pair of parameters (ρj 1 , ρj 2 ) to model the j -th worker’s reliability. In particular, ρj 1 and ρj 2
are the probabilities of getting the positive and negative labels when a fully reliable worker
provides the same label.
Here we make the following implicit assumption:

Assumption 9 We assume that diﬀerent workers make independent judgments and, for
each single worker, the labels provided by him/her to diﬀerent instances are also independent.

As the parameter ρj increases from 0 to 1, the j -th worker’s reliability also increases in
the sense that Pr(zij = 1|θi , ρj ) gets more and more close to θi , which is the probability
of getting a positive label from a random fully reliable worker. Diﬀerent types of workers
can be easily characterized by ρj . When all ρj = 1, it recovers the previous model with
fully reliable workers since Pr(zij = 1|θi , ρj ) = θi , i.e, each worker provides the label only
according to the underlying soft-label of the instance. When ρj = 0.5, we have Pr(zij =
1|θi , ρj ) = Pr(zij = −1|θi , ρj ) = 0.5, which indicates that the j -th worker is a spammer,
who randomly submits positive or negative labels. When ρj = 0, it indicates that the

16

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

p(θi , ρj |zij = z ) =

j -th worker is poorly informed or misunderstands the instruction such that he/she always
assigns wrong labels.
i=1 and workers’ reliability {ρj }M
We assume that instances’ soft-label {θi}K
j=1 are drawn
from known Beta prior distributions: θi ∼ Beta(a0
i ) and ρj ∼ Beta(c0
j , d0
i , b0
j ). At each
stage, we need to make the decision on both the next instance i to be labeled and the
next worker j to label the instance i (we omit t in i, j here for notational simplicity). In
other words, the action space A = {(i, j ) : (i, j ) ∈ {1, . . . , K } × {1, . . . , M }}. Once the
decision is made, the distribution of the outcome zij is given by (18) and (19). Given the
prior distributions and likelihood functions in (18) and (19), the Bayesian Markov Decision
process can be formally deﬁned as in Section 3. Similar to the homogeneous worker setting,
the optimal inferred positive set HT takes the form of HT = {i : P T
i ≥ 0.5} as in Proposition
i = Pr(i ∈ H ∗ |Ft ) = Pr (θi ≥ 0.5|Ft ). The value function V (S 0 ) still takes the
2 with P t
form of (8), which can be further decomposed into the sum of stage-wise rewards in (9)
using Proposition 4. Unfortunately, in the heterogeneous worker setting, the posterior
distributions of θi and ρj are highly correlated with a sophisticated joint distribution, which
makes the computation of stage-wise rewards in (9) much more challenging. In particular,
given the prior θi ∼ Beta(a0
i ) and ρj ∼ Beta(c0
i , b0
j , d0
j ), the posterior distribution of θi and
ρj given the label zij = z ∈ {−1, 1} takes the following form:
Pr(zij = z |θi , ρj )Beta(a0
j , d0
i )Beta(c0
i , b0
j )
Pr(zij = z )
where Pr(zij = z |θi , ρj ) is the likelihood function deﬁned in (18) and (19) and
Pr(zij = 1) = E(Pr(zij = 1|θi , ρj )) = E(θi )E(ρj ) + (1 − E(θi ))(1 − E(ρj ))
d0
c0
a0
b0
j
j
i
i
j + d0
c0
i + b0
a0
j + d0
c0
i + b0
a0
i
j
i
j
As we can see, the posterior distribution p(θi , ρj |zij = z ) no longer takes the form of the
product of the distributions of θi and ρj and the marginal posterior of θi is no longer a
Beta distribution. As a result, P t
i does not have a simple representation as in (5), which
makes the computation of the reward function much more diﬃcult as the number of stages
increases. Therefore, to apply our Opt-KG policy to large-scale applications, we need to
use some approximate posterior inference techniques.
When applying Opt-KG, we need to perform 2 ·K ·M ·T inferences of the posterior distri-
bution in total. Each approximate inference should be computed very eﬃciently, hopefully
in a closed-form. For large-scale problems, most traditional approximate inference tech-
niques such as Markov Chain Monte Carlo (MCMC) or variational Bayesian methods (e.g.,
Beal, 2003; Paisley et al., 2012) may lead to higher computational cost since each inference
is an iterative procedure. To address the computational challenge, we apply the variational
approximation with the moment matching technique so that each inference of the approxi-
mate posterior can be computed in a closed-form. In fact, any highly eﬃcient approximate
inference can be utilized to compute the reward function. Since the main focus of the paper
is on the MDP model and Opt-KG policy, we omit the discussion for other possible approx-
imate inference techniques. In particular, we ﬁrst adopt the variational approximation by

=

,

(20)

+

.

17

Chen, Lin and Zhou

Algorithm 2 Optimistic Knowledge Gradient for Heterogeneous Workers
Input: Parameters of prior distributions for instances {a0
i }K
i , b0
i=1 and for workers
{c0
j }M
j , d0
j=1 . The total budget T .
for t = 0, . . . , T − 1 do
(cid:0)R+ (at
j ))(cid:1) .
1. Select the next instance it to label and the next worker jt to label it according to:
.
j , dt
i , ct
i , bt
j ), R2 (at
j , dt
i , ct
i , bt
= max(R1 (at
j , dt
i , ct
i , bt
j )
arg max
(i,j )∈{1,...,K }×{1,...,M }
(21)
2. Acquire the label zit jt ∈ {−1, 1} of the i-th instance from the j -th worker.
3. Update the posterior by setting:

(it , jt ) =

= ˜bt
ct+1
bt+1
at+1
= ˜ct
= ˜at
jt (zit jt )
it (zit jt )
it (zit jt )
jt
it
it
and all parameters for i (cid:54)= it and j (cid:54)= jt remain the same.
end for
i ≥ bT
Output: The positive set HT = {i : aT
i }.

dt+1
jt

= ˜dt
jt (zit jt ),

assuming the conditional independence of θi and ρj :
p(θi , ρj |zij = z ) ≈ p(θi |zij = z )p(ρj |zij = z ).
We further approximate p(θi |zij = z ) and p(ρj |zij = z ) by two Beta distributions:
p(θi |zij = z ) ≈ Beta(˜ai (z ), ˜bi (z )),
p(ρj |zij = z ) ≈ Beta(˜cj (z ), ˜dj (z )),
where the parameters ˜ai (z ), ˜bi (z ), ˜cj (z ), ˜dj (z ) are computed using moment matching with
the analytical form presented in the appendix. After this approximation, the new posterior
distributions of θi and ρj still have the same structure as their prior distribution, i.e., the
product of two Beta distributions, which allows a repeatable use of this approximation every
time when a new label is collected. Moreover, due to the Beta distribution approximation
of p(θi |zij = z ), the reward function takes a similar form as in the previous setting.
In
particular, assuming at a certain stage, θi has the posterior distribution Beta(ai , bi ) and
ρj has the posterior distribution Beta(cj , dj ). The reward of getting positive and negative
labels for the i-th instance from the j -th worker are presented in (22) and (23):
R1 (ai , bi , cj , dj ) = h(I (˜ai (z = 1), ˜bi (z = 1))) − h(I (ai , bi )),
R2 (ai , bi , cj , dj ) = h(I (˜ai (z = −1), ˜bi (z = −1))) − h(I (ai , bi )).
With the reward in place, we present Opt-KG for budget allocation in the heterogeneous
worker setting in Algorithm 2. We also note that due to the variational approximation of
the posterior, establishing the consistency results of Opt-KG becomes very challenging in
the heterogeneous worker setting.

(22)

(23)

6. Extensions

Our MDP formulation is a general framework to address many complex settings of dy-
namic budget allocation problems in crowd labeling. In this section, we brieﬂy discuss two

18

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

important extensions, where for both extensions, Opt-KG can be directly applied as an
approximate policy. We note that for the sake of presentation simplicity, we only present
these extensions in the noiseless homogeneous worker setting. Further extensions to the
heterogeneous setting are rather straightforward using the technique from Section 5.

6.1 Utilizing Contextual Information

When the contextual information is available for instances, we could easily extend our model
to incorporate such an important information. In particular, let the contextual information
for the i-th instance be represented by a p-dimensional feature vector xi ∈ Rp . We could
utilize the feature information by assuming a logistic model for θi :
exp{(cid:104)w, xi (cid:105)}
1 + exp{(cid:104)w, xi (cid:105)} ,

.
=

θi

where w is assumed to be drawn from a Gaussian prior N (µ0 , Σ0 ). At the t-th stage with
the current state (µt , Σt ), the decision maker determines the instance it and acquire its label
yit ∈ {−1, 1}. Then we update the posterior µt+1 and Σt+1 using the Laplace method as in
Bayesian logistic regression (Bishop, 2007). Variational methods can be applied to further
accelerate the posterior update (Jaakkola and Jordan, 2000). The details are provided in
the appendix.

6.2 Multi-Class Categorization

Our MDP formulation can also be extended to deal with multi-class categorization problems,
where each instance is a multiple choice question with several possible options (i.e., classes).
More formally, in a multi-class setting with C diﬀerent classes, we assume that the i-th
(cid:80)C
instance is associated with a probability vector θ i = (θi1 , . . . θiC ), where θic is the probability
that the i-th instance will be labeled as the class c by a random fully reliable worker and
i=1 θic = 1. We assume that θ i has a Dirichlet prior θ i ∼ Dir(α0
i ) and the initial state
S 0 is a K × C matrix with α0
which follows the categorical distribution: p(yit ) = (cid:81)C
i as its i-th row. At each stage t with the current state
S t , we determine the next instance it to be labeled and collect its label yit ∈ {1, . . . , C },
I (yit =c)
c=1 θ
. Since the Dirichlet is
it c
the conjugate prior of the categorical distribution, the next state induced by the posterior
i for all i (cid:54)= it . Here δ c is a row vector with
distribution is: S t+1
and S t+1
i = S t
= S t
+ δyit
it
it
one at the c-th entry and zeros at all other entries. The transition probability is:
it c(cid:80)C
αt
c=1 αt
it c
(cid:54)= c}. By a
c = {i : θic ≥ θic(cid:48) , ∀c(cid:48)
We denote the true set of instances in class c by H ∗
similar argument as in Proposition 2, at the ﬁnal stage T , the estimated set of instances
belonging to class c is
c = {i : P T
ic ≥ P T
ic(cid:48) , ∀c(cid:48) (cid:54)= c},
H T
c |Ft ) = Pr(θic ≥ θic(cid:48) ,
ic = Pr(i ∈ H ∗
∀ c(cid:48)
(cid:54)= c|S t ). We note that if the i-th
where P t
instance belongs to more than one H T
c , we only assign it to the one with the smallest

Pr(yit = c|S t , it ) = E(θit c |S t ) =

.

19

Chen, Lin and Zhou

index c so that {H T
c=1 forms a partition of {1, . . . , K }. Let Pt
c }C
i = (P t
i1 , . . . , P t
iC ) and
R(S t , it ) = E (cid:0)h(Pt+1
(cid:1) .
i ) = max1≤c≤C P t
h(Pt
ic . The expected reward takes the form of:
it )|S t , it
) − h(Pt
it
With the reward function in place, we can formulate the problem into a MDP and use
DP to obtain the optimal policy and Opt-KG to compute an approximate policy. The
only computational challenge is how to calculate P t
ic eﬃciently so that the reward can be
evaluated. We present an eﬃcient method in the appendix. We can further use Dirichlet
distribution to model workers reliability as in Liu and Wang (2012). Using multi-class
Bayesian logistic regression, we can also incorporate contextual information into the multi-
class setting in a straightforward manner.

7. Related Works

Categorical crowd labeling is one of the most popular tasks in crowdsourcing since it requires
less eﬀort of the workers to provide categorical labels than other tasks such as language
translations. Most work in categorical crowd labeling are solving a static problem, i.e.,
inferring true labels and workers’ reliability based on a static labeled data set (Dawid and
Skene, 1979; Raykar et al., 2010; Liu and Wang, 2012; Welinder et al., 2010; Whitehill
et al., 2009; Zhou et al., 2012; Liu et al., 2012; Gao and Zhou, 2013). The ﬁrst work that
incorporates diversity of worker reliability is by Dawid and Skene (1979), which uses EM
to perform the point estimation on both worker reliability and true class labels. Based on
that, Raykar et al. (2010) extended (Dawid and Skene, 1979) by introducing Beta prior
for workers’ reliability and features of instances in the binary setting; and Liu and Wang
(2012) further introduced Dirichlet prior for modeling workers’ reliability in the multi-class
setting. Our work utilizes the modeling techniques in these two static models as basic
building blocks but extends to dynamic budget allocation settings.
In recent years, there are several works that have been devoted into online learning or
budget allocation in crowdsourcing (Karger et al., 2013a,b; Bachrach et al., 2012; Ho et al.,
2013; Ertekin et al., 2012; Yan et al., 2011; Kamar et al., 2012; Ipeirotis et al., 2013). The
method proposed in Karger et al. (2013b) is based on the one-coin model. In particular, it
assigns instances to workers according to a random regular bipartite graph. Although the
error rate is proved to achieve the minimax rate, its analysis is asymptotic and method is
not optimal when the budget is limited. Karger et al. (2013a) further extended the work
by Karger et al. (2013b) to the multi-class setting. The new labeling uncertainty method in
Ipeirotis et al. (2013) is one of the state-of-the-art methods for repeated labeling. However,
it does not model each worker’s reliability and incorporate it into the allocation process.
Ho et al. (2013) proposed an online primal dual method for adaptive task assignment and
investigated the sample complexity to guarantee that the probability of making an error for
each instance is less that a threshold. However, it requires gold samples to estimate workers’
reliability. Kamar et al. (2012) used MDP to address a diﬀerent decision problem in crowd
labeling, where the decision maker collects labels for each instance one after another and
only decides whether to hire an additional worker or not. Basically, it is an optimal stopping
problem since there is no pre-ﬁxed amount of budget and one needs to balance the accuracy
v.s. the amount of budget. Since the accuracy and the amount of budget are in diﬀerent

20

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

metrics, such a balance could be very sub jective. Furthermore, the MDP framework in
Kamar et al. (2012) cannot distinguish diﬀerent workers. To the best of our knowledge,
there is no existing method that characterizes the optimal allocation policy for ﬁnite T . In
this work, with the MDP formulation and DP algorithm, we characterize the optimal policy
for budget allocation in crowd labeling under any budget level.

We also note that the budget allocation in crowd labeling is fundamentally diﬀerent
from noisy active learning (Settles, 2009; Nowak, 2009). Active learning usually does not
model the variability of labeling diﬃculties among instances and assumes a single (noisy)
oracle; while in crowd labeling, we need to model both instances’ labeling diﬃculty and
diﬀerent workers’ reliability. Secondly, active learning requires the feature information of
instances for the decision, which could be unavailable in crowd labeling. Finally, the goal
of the active learning is to label as few instances as possible to learn a good classiﬁer. In
contrast, for budget allocation in crowd labeling, the goal is to infer the true labels for as
many instances as possible.

In fact, our MDP formulation is essentially a ﬁnite-horizon Bayesian multi-armed bandit
(MAB) problem. While the inﬁnite-horizon Bayesian MAB has been well-studied and the
optimal policy can be computed via Gittins index (Gittins, 1989), for ﬁnite-horizon Bayesian
MAB, the Gittins index rule is only an approximate policy with high computational cost.
The proposed Opt-KG and a more general conditional value-at-risk based KG could be gen-
eral policies for Bayesian MAB. Recently, a Bayesian UCB policy was proposed to address
a diﬀerent Bayesian MAB problem (Kaufmann et al., 2012). However, it is not clear how
to directly apply the policy to our problem since we are not updating the posterior of the
mean of rewards as in Kaufmann et al. (2012). We note that our problem is also related
to optimal stopping problem. The main diﬀerence is that the optimal stopping problem is
inﬁnite-horizon while our problem is ﬁnite-horizon and the decision process must stop when
the budget is exhausted.

8. Experiments

In this section, we conduct empirical study to show some interesting properties of the
proposed Opt-KG policy and compare its performance to other methods. We observe
that several commonly used priors such as the uniform prior (Beta(1, 1)), Jeﬀery prior
(Beta(1/2, 1/2)) and Haldane prior (Beta(0, 0)) for instances’ soft-label {θi}K
i=1 lead to very
similar performance. Therefore, we adopt the uniform prior (Beta(1, 1)) unless otherwise
|HT ∩H ∗ |+|(HT )c∩(H ∗ )c |
speciﬁed. In addition, for the ease of comparison, the accuracy is deﬁned as
K
which is normalized between [0, 1].

,

8.1 Simulation Study

In this section, we conduct simulated study. For each simulated experiment, we randomly
generate 20 diﬀerent sets of data and report the averaged accuracy. The deviations for
diﬀerent methods are similar and quite small and thus omitted for the purpose of better
visualization and space-saving.

21

Chen, Lin and Zhou

(a) T = 5K = 105

(b) T = 15K = 315

(c) T = 50K = 1050

Figure 4: Labeling counts for instances with diﬀerent levels of ambiguity.

(a) T = 5K = 105

(b) T = 15K = 315

(c) T = 50K = 1050

Figure 5: Labeling counts for workers with diﬀerent levels of reliability.

8.1.1 Study on Labeling Frequency

We ﬁrst investigate that, in the homogeneous noiseless worker setting (i.e., workers are
fully reliable), how the total budget is allocated among instances with diﬀerent levels of
ambiguity.
In particular, we assume there are K = 21 instances with soft-labels θ =
(θ1 , θ2 , θ3 , . . . , θK ) = (0, 0.05, 0.1, . . . , 1). We vary the total budget T = 5K, 15K, 50K and
report the number of times that each instance is labeled on average over 20 independent
runs. The results are presented in Figure 4.
It can be seen from Figure 4 that, more
ambiguous instances with θ close to 0.5 in general receive more labels than those simple
instances with θ close to 0 or 1. A more interesting observation is that when the budget
level is low (e.g., T = 5K in Figure 4(a)), the policy spends less budget on those very
ambiguous instances (e.g., θ = 0.45 or 0.5 ), but more budget on exploring less ambiguous
instances (e.g., θ = 0.35, 0.4 or 0.6). When the budget goes higher (e.g., T = 15K in Figure
4(b)), those very ambiguous instances receive more labels but the most ambiguous instance
(θ = 0.5) not necessarily receives the most labels. In fact, the instances with θ = 0.45 and
θ = 0.55 receive more labels than that of the most ambiguous instance. When the total
budget is suﬃciently large (e.g., T = 50K in Figure 4(c)), the most ambiguous instance
receives the most labels since all the other instances have received enough labels to infer
their true labels.

22

00.20.40.60.8124681012Soft label (θ)Labeling frequency00.20.40.60.810102030405060Soft label (θ)Labeling frequency00.20.40.60.81050100150200250300Soft label (θ)Labeling frequency00.20.40.60.810.511.522.533.5Worker reliability (ρ)Labeling frequency00.20.40.60.81246810Worker reliability (ρ)Labeling frequency00.20.40.60.8115161718192021Worker reliability (ρ)Labeling frequencyStatistical Decision Making for Optimal Budget Allocation in Crowd Labeling

(a) Beta(0.5, 0.5)

(b) Beta(2, 2)

(c) Beta(2, 1)

(d) Beta(4, 1)

Figure 6: Density plot for diﬀerent Beta distributions for generating each θi . Here, (a)
represents there are more easier instances; (b) more ambiguous instances; (c) &
(d) imbalanced class distributions with more positive instances.

Next, we investigate that, in the heterogeneous worker setting, how many instances each
worker is assigned. We simulate K = 21 instances’ soft-labels as before and further simu-
late workers’ reliability ρ = (ρ1 , ρ2 , . . . , ρM ) = (0.1, 0.15, . . . , 0.5, 0.505, 0.515, . . . , 0.995) for
M = 59 workers. Such a simulation ensures that there are more reliable workers, which is
in line with actual situation. We vary the total budget T = 5K, 15K, 50K and report the
number of instances that each worker is assigned on average over 20 independent runs in
Figure 5. As one can see, when the budget level goes up, there is clear trend that more
reliable workers receive more instances.

8.1.2 Prior for Instances

We investigate how robust Opt-KG is when using the uniform prior for each θi . We ﬁrst
simulate K = 50 instances with each θi ∼ Beta(0.5, 0.5), θi ∼ Beta(2, 2), θi ∼ Beta(2, 1) or
θi ∼ Beta(4, 1). The density functions of these four diﬀerent Beta distributions are plotted
in Figure 6. For each generating distribution of θi , we compare Opt-KG using the uniform
prior (Beta(1, 1)) (in red line) to Opt-KG with the true generating distribution as the prior
(in blue line). The comparison in accuracy with diﬀerent levels of budget (T = 2K, . . . , 20K )
is shown in Figure 7. As we can see, the performance of Opt-KG using two diﬀerent priors
are quite similar for most generating distributions except for θi ∼ Beta(4, 1) (i.e., the highly
imbalanced class distribution). When θi ∼ Beta(4, 1), the Opt-KG with uniform prior needs
at least T = 16K units of budget to match the performance of Opt-KG with true generating
distribution as the prior. This result indicates that for balanced class distributions, the
uniform prior is a good choice and robust to the underlying distribution of θi . For highly
imbalanced class distributions, if a uniform prior is adopted, one needs more budget to
recover from the inaccurate prior belief.

8.1.3 Prior on Workers

We investigate how sensitive the prior for the workers’ reliability ρj is. In particular, we sim-
ulate K = 50 instances with each θi ∼ Beta(1, 1) and M = 100 workers with ρj ∼ Beta(3, 1),
ρj ∼ Beta(8, 1) or ρj ∼ Beta(5, 2). We ensure that there are more reliable workers than
spammers or poorly informed workers, which is in line with the actual situation. We use the

23

00.20.40.60.81012345x00.20.40.60.8100.511.5x00.20.40.60.8100.511.52x00.20.40.60.8101234xChen, Lin and Zhou

(a) θi ∼ Beta(0.5, 0.5)

(b) θi ∼ Beta(2, 2)

(c) θi ∼ Beta(2, 1)

(d) θi ∼ Beta(4, 1)

Figure 7: Comparison between Opt-KG using the uniform distribution and true generating
distribution as the prior.

(a) Beta(3, 1)

(b) Beta(8, 1)

(c) Beta(5, 2)

(d) Beta(4, 1)

Figure 8: Density plot for diﬀerent Beta distributions for generating ρj . The plot in (d) is
the one that we use as the prior.

prior Beta(4, 1), which indicates that we have the prior belief that most workers preform
reasonably well and the averaged accuracy is 4/5 = 80%. In Figure 8, we show diﬀerent
density functions for generating ρj and the prior that we use (in Figure 8 (d)). For each

24

020040060080010000.860.880.90.920.940.960.98BudgetAccuracy  Uniform Prior: Beta(1,1)True Prior: Beta(0.5,0.5)020040060080010000.70.750.80.850.90.95BudgetAccuracy  Uniform Prior: Beta(1,1)True Prior: Beta(2,2)020040060080010000.750.80.850.90.95BudgetAccuracy  Uniform Prior: Beta(1,1)True Prior: Beta(2,1)020040060080010000.850.90.951BudgetAccuracy  Uniform Prior: Beta(1,1)True Prior: Beta(4,1)00.20.40.60.8100.511.522.53x00.20.40.60.8102468x00.20.40.60.8100.511.522.5x00.20.40.60.8101234xStatistical Decision Making for Optimal Budget Allocation in Crowd Labeling

(a) θi ∼ Beta(3, 1)

(b) θi ∼ Beta(8, 1)

(c) θi ∼ Beta(5, 2)

Figure 9: Comparison between Opt-KG using Beta(4, 1) and true generating distribution
prior as the prior.

generating distribution of θi , we compare the Opt-KG policy using the prior (Beta(4, 1))
(in red line) to the Opt-KG with the true generating distribution as the prior (in blue line).
The comparison in accuracy with diﬀerent levels of budget (T = 2K, . . . , 20K ) is shown in
Figure 9. From Figure 9, we observe that the performance of Opt-KG using two diﬀerent
priors are quite similar in all diﬀerent settings. Hence, we will use Beta(4, 1) as the prior
when the true prior of workers is unavailable.

8.1.4 Performance Comparison Under the Homogeneous Noiseless Worker
Setting

We compare the performance of Opt-KG under the homogeneous noiseless worker setting
to several other competitors, including

1. Uniform: Uniform sampling.

2. KG(Random): Randomized knowledge gradient (Frazier et al., 2008).

3. Gittins-Inf: A Gittins-indexed based policy proposed by Xie and Frazier (2013) for
solving an inﬁnite-horizon Bayesian MAB problem where the reward is discounted by
δ . Although it solves a diﬀerent problem, we apply it as a heuristic by choosing the
discount factor δ such that T = 1/(1 − δ).

4. NLU: The “new labeling uncertainty” method proposed by Ipeirotis et al. (2013).

We note that we do not compare to the ﬁnite-horizon Gittins index rule (Nino-Mora, 2011)
since its computation is very expensive. On some small-scale problems, we observe that the
ﬁnite-horizon Gittins index rule (Nino-Mora, 2011) has the similar performance as Gittins-Inf
in Xie and Frazier (2013).
We simulate K = 50 instances with each θi ∼ Beta(1, 1), θi ∼ Beta(0.5, 0.5), θi ∼
Beta(2, 2), θi ∼ Beta(2, 1) or θi ∼ Beta(4, 1) (see Figure 6). For each of the ﬁve settings,
we vary the total budget T = 2K, 3K, . . . , 20K and report the mean of accuracy for 20
independently generated sets of {θi}K
i=1 . For the last four settings, we report the comparison
among diﬀerent methods when either using the uniform prior (“uni prior” for short) or the

25

020040060080010000.650.70.750.80.850.90.95BudgetAccuracy  Prior: Beta(4,1)True Prior: Beta(3,1)020040060080010000.70.750.80.850.90.95BudgetAccuracy  Prior: Beta(4,1)True Prior: Beta(8,1)020040060080010000.650.70.750.80.850.90.95BudgetAccuracy  Prior: Beta(4,1)True Prior: Beta(5,2)Chen, Lin and Zhou

true generating distribution as the prior. From Figure 10, the proposed Opt-KG outperforms
all the other competitors in most settings regardless the choice of the prior. For θi ∼
Beta(0.5, 0.5), NLU matches the performance of Opt-KG; and for θi ∼ Beta(2, 2), Gittins-inf
matches the performance of Opt-KG. We also observe that the performance of randomized
KG only slightly improves that of uniform sampling.

8.1.5 Performance Comparison Under the Heterogeneous Worker Setting

We compare the proposed Opt-KG under the heterogeneous worker setting to several other
competitors:

1. Uniform: Uniform sampling.

2. KG(Random): Randomized knowledge gradient (Frazier et al., 2008).

3. KOS: The randomized budget allocation algorithm in Karger et al. (2013b).

We note that several competitors for the homogeneous worker setting (e.g., Gittins-inf and
NLU) cannot be directly applied to the heterogeneous worker setting since they fail to model
each worker’s reliability.
We simulate K = 50 instances with each θi ∼ Beta(1, 1) and M = 100 workers with
ρj ∼ Beta(4, 1), ρj ∼ Beta(3, 1), ρj ∼ Beta(8, 1) or ρj ∼ Beta(5, 2) (see Figure 8). For each
of the four settings, we vary the total budget T = 2K, 3K, . . . , 20K and report the mean
of accuracy for 20 independently generated sets of parameters. For the last three settings,
we report the comparison among diﬀerent methods when either using Beta(4, 1) prior or
the true generating distribution for ρj as the prior. From Figure 11, the proposed Opt-KG
outperforms all the other competitors regardless the choice of the prior.

8.2 Real Data

We compare diﬀerent policies on a standard real data set for recognizing textual entailment
(RTE) (Section 4.3 in Snow et al., 2008). There are 800 instances and each instance is a
sentence pair. Each sentence pair is presented to 10 diﬀerent workers to acquire binary
choices of whether the second hypothesis sentence can be inferred from the ﬁrst one. There
are in total 164 diﬀerent workers. We ﬁrst consider the homogeneous noiseless setting
without incorporating the diversity of workers and use the uniform prior (Beta(1, 1)) for
each θi .
In such a setting, once we decide to label an instance, we randomly choose a
worker (who provides the label in the full data set) to acquire the label. Due to this
randomness, we run each policy 20 times and report the mean of the accuracy in Figure
12(a). As we can see, Opt-KG, Gittins-inf and NLU all perform quite well. We also note
that although Gittins-inf performs slightly better than our method on this data, it requires
solving a linear system with O(T 2 ) variables at each stage, which could be too expensive for
large-scale applications. While our Opt-KG policy has a time complexity linear in K T and
space complexity linear in K , which is much more eﬃcient when a quick online decision is
required. In particular, we present the comparison between Opt-KG and Gittins-inf on the
averaged CPU time under diﬀerent budget levels in Table 4. As one can see, Gittins-inf is
computationally more expensive than Opt-KG.

26

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

(a) θi ∼ Beta(1, 1) (True Prior) (b) θi ∼ Beta(0.5, 0.5) (True Prior) (c) θi ∼ Beta(0.5, 0.5) (Uni Prior)

(d) θi ∼ Beta(2, 2) (True Prior)

(e) θi ∼ Beta(2, 2) (Uni Prior)

(f ) θi ∼ Beta(2, 1) (True Prior)

(g) θi ∼ Beta(2, 1) (Uni Prior)

(h) θi ∼ Beta(4, 1) (True Prior)

(i) θi ∼ Beta(4, 1) (Uni Prior)

Figure 10: Performance comparison under the homogeneous noiseless worker setting.

When the worker reliability is incorporated, we compare diﬀerent policies in Figure
12(b). We put a Beta(4, 1) prior distribution for each ρj which indicates that we have the

27

020040060080010000.750.80.850.90.95BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020040060080010000.80.850.90.951BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020040060080010000.80.850.90.951BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020040060080010000.650.70.750.80.850.90.95BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020040060080010000.650.70.750.80.850.90.95BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020040060080010000.750.80.850.90.95BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020040060080010000.750.80.850.90.95BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020040060080010000.850.90.951BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020040060080010000.80.850.90.951BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KGChen, Lin and Zhou

(a) ρj ∼ Beta(4, 1) (True Prior)

(b) ρj ∼ Beta(3, 1) (True Prior) (c) ρj ∼ Beta(3, 1)
Prior)

(Beta(4, 1)

(d) ρj ∼ Beta(8, 1) (True Prior)

(e) ρj ∼ Beta(8, 1) (Beta(4, 1) Prior)

(f ) ρj ∼ Beta(5, 2) (True Prior)

(g) ρj ∼ Beta(5, 2) (Beta(4, 1) Prior)

Figure 11: Performance comparison under the heterogeneous worker setting.

Budget T

2K = 1, 600

4K = 3, 200

6K = 4, 800

10K = 8, 000

Opt-KG

Gittins-inf

1.09

25.87

2.19

35.70

3.29

45.59

5.48

130.68

Table 4: Comparison in CPU time (seconds)

prior belief that most workers perform reasonably well. Other priors in Figure 8 lead to
similar results and thus omitted here. As one can see, the accuracy of Opt-KG is much
higher than that of other policies when T is small. It achieves the highest accuracy of 92.05%
only using 40% of the total budget (i.e., on average, each instance is labeled 4 times). One
may also observe that when T > 4K = 3, 200, the performance of Opt-KG does not improve

28

020040060080010000.40.50.60.70.80.9BudgetAccuracy  UniformKG (Random)KOSOpt−KG020040060080010000.40.50.60.70.80.9BudgetAccuracy  UniformKG (Random)KOSOpt−KG020040060080010000.40.50.60.70.80.9BudgetAccuracy  UniformKG (Random)KOSOpt−KG020040060080010000.40.50.60.70.80.91BudgetAccuracy  UniformKG (Random)KOSOpt−KG020040060080010000.40.50.60.70.80.91BudgetAccuracy  UniformKG (Random)KOSOpt−KG020040060080010000.40.50.60.70.80.9BudgetAccuracy  UniformKG (Random)KOSOpt−KG020040060080010000.40.50.60.70.80.9BudgetAccuracy  UniformKG (Random)KOSOpt−KGStatistical Decision Making for Optimal Budget Allocation in Crowd Labeling

(a) RTE: Homogeneous Noiseless Worker

(b) RTE: Heterogeneous Worker

Figure 12: Performance comparison on the real data set.

and in fact, slightly downgrades a little bit. This is mainly due to the restrictiveness of
the experimental setting. In particular, since the experiment is conducted on a ﬁxed data
set with partially observed labels, the Opt-KG cannot freely choose instance-worker pairs
especially when the budget goes up (i.e., the action set is greatly restricted). According
to our experience, such a phenomenon will not happen on experiments when labels can be
obtained from any instance-worker pair. Comparing Figure 12(b) to 12(a), we also observe
that Opt-KG under the heterogeneous worker setting performs much better than Opt-KG
under the homogeneous worker setting, which indicates that it is beneﬁcial to incorporate
workers’ reliability.

9. Conclusions and Future Work

In this paper, we propose to address the problem of budget allocation in crowd labeling.
We model the problem using the Bayesian Markov decision process and characterize the
optimal policy using the dynamic programming. We further propose a computationally
more attractive approximate policy: optimistic knowledge gradient. Our MDP formulation
is a general framework, which can be applied to binary or multi-class, contextual or non-
contextual crowd labeling problems in either pull or push crowdsourcing marketplaces.

There are several possible future directions for this work. First, it is of great interest
to show the consistency of Opt-KG in heterogeneous worker setting and further provide
the theoretical results on the performance of Opt-KG under ﬁnite budget. Second, in
this work, we assume that both instances and workers are equally priced. Although this
assumption is standard in many crowd labeling applications, a dynamic pricing strategy as
the allocation process proceeds will better motivate those more reliable workers to label more
challenge instances. A recent work in Wang et al. (2013) provides some quality-based pricing
algorithms for crowd workers and it will be interesting to incorporate their strategies into
our dynamic allocation framework. Third, we assume that the labels provided by the same
worker to diﬀerent instances are independent. It is more interesting to consider that the
workers’ reliability will be improved during the labeling process when some useful feedback

29

020004000600080000.650.70.750.80.850.90.95BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020004000600080000.650.70.750.80.850.90.95BudgetAccuracy  UniformKG (Random)KOSOpt−KGChen, Lin and Zhou

can be provided. Further, since the proposed Opt-KG is a fairly general approximate policy
for MDP, it is also interesting to apply it to other statistical decision problems.

Acknowledgments

We would like to thank Qiang Liu for sharing the code for KOS method; Jing Xie and Peter
Frazier for sharing their code for computing inﬁnite-horizon Gittins index; John Platt,
Chris Burges and Kevin Murphy for helpful discussions; and anonymous reviewers and the
associate editor for their constructive comments on improving the quality of the paper.

Appendix A. Proof of Results

.

(24)

=

(25)

HT = arg max
H

E

(1(i ∈ H )1(i ∈ H ∗ ) + 1(i (cid:54)∈ H )1(i (cid:54)∈ H ∗ ))

In this section, we provide the proofs of the main results of our paper.
A.1 Proof of Proposition 2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)FT
(cid:32) K(cid:88)
(cid:33)
The ﬁnal positive set HT is chosen to maximize the expected accuracy conditioned on FT :
i=1
According to the deﬁnition (6) of P T
i , we can re-write (24) using the linearity of the
expectation:
K(cid:88)
(1(i ∈ H ) Pr(i ∈ H ∗ |FT ) + 1(i (cid:54)∈ H ) Pr(i (cid:54)∈ H ∗ |FT ))
K(cid:88)
(cid:0)1(i ∈ H )P T
i )(cid:1) .
i=1
i + 1(i (cid:54)∈ H )(1 − P T
i=1
To maximize (25) over H , it easy to see that we should set i ∈ H if and only if P T
i ≥ 0.5.
Therefore, we have the positive set
i ≥ 0.5}.
HT = {i : P T
(cid:90) 1
0.5
where B (a, b) is the beta function.
It is easy to see that I (a, b) > 0.5 ⇐⇒ I (a, b) > 1 − I (a, b). We re-write 1 − I (a, b) as
(cid:90) 0.5
(cid:90) 1
follows
1 − I (a, b) =
0.5
0

Recall that
I (a, b) = Pr(θ ≥ 0.5|θ ∼ Beta(a, b)) =

ta−1 (1 − t)b−1dt =

ta−1 (1 − t)b−1dt,

(26)

A.2 Proof of Corollary 3

1
B (a, b)

1
B (a, b)

tb−1 (1 − t)a−1dt,

1
B (a, b)

30

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

I (a, b) − (1 − I (a, b)) =

(cid:90) 1
where the second equality is obtained by setting t := 1 − t. Then we have:
(cid:33)
(cid:32)(cid:18) t
(ta−1 (1 − t)b−1 − tb−1 (1 − t)a−1 )dt
(cid:19)a−b − 1
1
(cid:90) 1
B (a, b)
0.5
ta−1 (1 − t)b−1
1
1 − t
dt.
=
(cid:17)a−b
(cid:16) t
B (a, b)
0.5
(cid:16) t
(cid:16) t
(cid:17)a−b ≡ 1 and I (a, b) = 0.5. When a < b,
(cid:17)a−b
> 1 and hence I (a, b) − (1 − I (a, b)) > 0, i.e,
t
1−t > 1. When a > b,
1−t
1−t
1−t

Since t > 0.5,

< 1 and

I (a, b) > 0.5. When a = b,
I (a, b) < 0.5.

A.3 Proof of Proposition 4

(27)

V (S 0 ) = G0 (S 0 ) + sup
π

We use the proof technique in Xie and Frazier (2013) to prove Proposition 4. According to
(cid:32) K(cid:88)
(cid:33)
(8), the value function takes the following form,
Eπ
V (S 0 ) = sup
h(P T
i )
.
To decompose the ﬁnal accuracy (cid:80)K
π
i=1
i ). Then, (cid:80)K
) − (cid:80)K
i ) and Gt+1 = (cid:80)K
we deﬁne G0 = (cid:80)K
can be decomposed as: (cid:80)K
i ) ≡ G0 + (cid:80)T −1
i=1 h(P T
i ) into the incremental reward at each stage,
i=1 h(P t+1
i=1 h(P T
i=1 h(P t
i=1 h(P 0
i )
i
i=1 h(P T
t=0 Gt+1 . The value function can now be
re-written as follows:
T −1(cid:88)
T −1(cid:88)
t=0
T −1(cid:88)
t=0
t=0

Eπ (E(Gt+1 |Ft ))
Eπ (cid:0)E(Gt+1 |S t , it )(cid:1) .
Here, the ﬁrst inequality is true because G0 is determinant and independent of π ; the
second inequality is due to the tower property of conditional expectation and the third one
i , depends on Ft only through S t
holds because Gt+1 , which is a function of P t+1
and P t
i
and it . We deﬁne incremental expected reward gained by labeling the it -th instance at the
state S t as follows:
(cid:33)
(cid:32) K(cid:88)
) − K(cid:88)
R(S t , it ) = E(Gt+1 |S t , it ) = E
(cid:1) .
= E (cid:0)h(P t+1
h(P t+1
i
i=1
i=1
) − h(P t
it )|S t , it
it

= G0 (S 0 ) + sup
π

= G0 (S 0 ) + sup
π

Eπ (Gt+1 )

i )|S t , it
h(P t

(28)

31

Chen, Lin and Zhou

The last equation is due to the fact that only P t
will be changed if the it -th instance is
it
labeled next. With the expected reward function in place, the value function in (8) can be
(cid:33)
(cid:32)T −1(cid:88)
(cid:12)(cid:12)(cid:12)S 0
re-formulated as:
R(S t , it )
t=0

V (S 0 ) = G0 (s) + sup
π

(29)

Eπ

.

A.4 Proof of Proposition 6

To prove the failure of deterministic KG, we ﬁrst show a key property for the expected
reward function:

R(a, b) =

a
a + b

(h(I (a + 1, b)) − h(I (a, b))) +

(h(I (a, b + 1)) − h(I (a, b))) .
(30)
aB (a,a) and if a (cid:54)= b,
Lemma 10 When a, b are positive integers, if a = b, R(a, b) = 0.52a
R(a, b) = 0.

b
a + b

To prove lemma 10, we ﬁrst present several basic properties for B (a, b) and I (a, b), which
will be used in all the following theorems and proofs.

1. Properties for B (a, b):

B (a + 1, b) =

B (a, b) = B (b, a),
a
a + b
b
a + b

B (a, b + 1) =

B (a, b),

B (a, b).

2. Properties for B (a, b):

I (a, b) = 1 − I (b, a),

I (a + 1, b) = I (a, b) +

0.5a+b
aB (a, b)
I (a, b + 1) = I (a, b) − 0.5a+b
bB (a, b)

,

.

(31)

(32)

(33)

(34)

(35)

(36)

The properties for I (a, b) are derived from the basic property of regularized incomplete
beta function. 1

Proof [Proof of Lemma 10]
When a = b, by Corollary 3, we have I (a + 1, b) > 0.5, I (a, b) = 0.5 and I (a, b + 1) < 0.5.
Therefore, the expected reward (30) takes the following form:
R(a, b) = 0.5(I (a + 1, a) − I (a, a)) + 0.5((1 − I (a, a + 1)) − I (a, a))
0.52a
= I (a + 1, a) − I (a, a) =
aB (a, a)

.

1. http://dlmf.nist.gov/8.17

32

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

When a > b, since a, b are integers, we have a ≥ b+1 and hence I (a+1, b) > 0.5, I (a, b) >
0.5, I (a, b + 1) ≥ 0.5 according to Corollary 3. The expected reward (30) now becomes:
(cid:90) 1
b
a
I (a + 1, b) +
a + b
a + b
(cid:90) 1
t · ta−1 (1 − t)b−1dt
1
a
B (a + 1, b)
a + b
0.5
(cid:90) 1
b
1
a + b
B (a, b + 1)
0.5
(t + (1 − t)) · ta−1 (1 − t)b−1dt − I (a, b)
1
=
B (a, b)
0.5
=I (a, b) − I (a, b) = 0.

ta−1 (1 − t)(1 − t)b−1dt − I (a, b)

I (a, b + 1) − I (a, b)

R(a, b) =

=

+

a
1
1
B (a,b+1) = 1
B (a+1,b) = b
Here we use (32) and (33) to show that
B (a,b) .
a+b
a+b
When a ≤ b − 1, we can prove R(a, b) = 0 in a similar way.

With Lemma 10 in place, the proof for Proposition 6 is straightforward. Recall that the
deterministic KG policy chooses the next instance according to

it = arg max
i

R(S t , i) = arg max
i

i , bt
R(at
i ),

and breaks the tie by selecting the one with the smallest index. Since R(a, b) > 0 if and only
i ) > 0 for those instances i ∈ E = {i : a0
i }.
if a = b, at the initial stage t = 0, R(a0
i , b0
i = b0
The policy will ﬁrst select i0 ∈ E with the largest R(a0
i , b0
i ). After obtaining the label yi0 ,
(cid:54)= b1
either a0
or b0
will add one and hence a1
and R(a1
, b1
) = 0. The policy will
i0
i0
i0
i0
i0
i0
select another instance i1 ∈ E with the “current” largest expected reward and the expected
reward for i1 after obtaining the label yi1 will then become zero. As a consequence, the
|E |
|E |
KG policy will label each instance in E for the ﬁrst |E | stages and R(a
, b
i ) = 0 for all
i ∈ {1, . . . , K }. Then the deterministic policy will break the tie selecting the ﬁrst instance
i
1 (cid:54)= bt
to label. From now on, for any t ≥ |E |, if at
1 , then the expected reward R(at
1 , bt
1 ) = 0.
Since the expected reward for other instances are all zero, the policy will still label the ﬁrst
instance. On the other hand, if at
1 = bt
1 , and the ﬁrst instance is the only one with the
positive expected reward and the policy will label it. Thus Proposition 6 is proved.
Remark 11 For randomized KG, after getting one label for each instance in E for the ﬁrst
|E | stages, the expected reward for each instance has become zero. Then randomized KG wil l
uniformly select one instance to label. At any stage t ≥ |E |, if there exists one instance i (at
most one instance) with at
i = bt
i , the KG policy wil l provide the next label for i; otherwise,
it wil l randomly select an instance to label.

A.5 Proof of Theorem 7

To prove the consistency of the Opt-KG policy, we ﬁrst show the exact values for R+
α (a, b) =
max(R1 (a, b), R2 (a, b)).

33

Chen, Lin and Zhou

1. When a ≥ b + 1:

Therefore,

2. When a = b:

0.5a+b
R1 (a, b) = I (a + 1, b) − I (a, b) =
aB (a, b)
R2 (a, b) = I (a, b + 1) − I (a, b) = − 0.5a+b
bB (a, b)

> 0;

< 0.

R+ (a, b) = R1 (a, b) =

0.5a+b
aB (a, b)

> 0.

R1 (a, b) = I (a + 1, a) − I (a, a) =

0.52a
aB (a, a)
0.52a
R2 (a, b) = 1 − I (a, a + 1) − I (a, a) =
aB (a, a)

;

.

Therefore, we have R1 = R2 and

R+ (a, b) = R1 (a, b) = R2 (a, b) =

0.52a
aB (a, a)

> 0.

3. When b − 1 ≥ a:

R1 (a, b) = I (a, b) − I (a + 1, b) = − 0.5a+b
aB (a, b)
0.5a+b
bB (a, b)

R2 (a, b) = I (a, b) − I (a, b + 1) =

> 0.

< 0;

Therefore

R+ (a, b) = R2 (a, b) =

0.5a+b
bB (a, b)

> 0.

We note that the values of R+ (a, b) for diﬀerent a, b are plotted in Figure 2 in main text.
As we can see R+ (a, b) > 0, for any positive integers (a, b), we ﬁrst prove in the following
lemma that

a+b→∞ R+ (a, b) = 0.
lim

(37)

Lemma 12 Properties for R+ (a, b):

1. R(a, b) is symmetric, i.e., R+ (a, b) = R+ (b, a).

2. lima→∞ R+ (a, a) = 0.
3. For any ﬁxed a ≥ 1, R+ (a + k , a − k) = R+ (a − k , a + k) is monotonical ly decreasing
in k for k = 0, . . . , a − 1.

34

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

4. When a ≥ b, for any ﬁxed b, R+ (a, b) is monotonical ly decreasing in a. By the sym-
metry of R+ (a, b), when b ≥ a, for any ﬁxed a, R+ (a, b) is monotonical ly decreasing
in b.

By the above four properties, we have lim(a+b)→∞ R+ (a, b) = 0.

Proof [Proof of Lemma 12]
We ﬁrst prove these four properties.
• Property 1: By the fact that B (a, b) = B (b, a), the symmetry of R+ (a, b) is straight-
forward.
• Property 2: For a > 1,
R+ (a−1,a−1) = 2a−1
R+ (a,a)
2a < 1 and hence R+ (a, a) is monotonically
a(cid:89)
a(cid:89)
decreasing in a. Moreover,
) ≤ R+ (1, 1)e− (cid:80)a
2i − 1
(1 − 1
R+ (a, a) = R+ (1, 1)
Since lima→∞ (cid:80)a
i=2
2i
2i
i=2
i=2
2i = ∞ and R+ (a, a) ≥ 0, lima→∞ R+ (a, a) = 0.
1
i=2
• Property 3: For any k ≥ 0,
R+ (a + (k + 1), a − (k + 1))
R+ (a + k , a − k)

(a + k)B (a + k , a − k)
(a + k + 1)B (a + (k + 1), a − (k + 1))
a − (k + 1)
a + (k + 1)
• Property 4: When a ≥ b, for any ﬁxed b:

= R+ (1, 1)

< 1.

1
2i .

=

=

R+ (a + 1, b)
R+ (a, b)

=

aB (a, b)
2(a + 1)B (a + 1, b)

=

a(a + b)
2a(a + 1)

< 1.

According to the third property, when a + b is an even number, we have R+ (a, b) <
2 ). According to the fourth property, when a + b is an odd number and a ≥ b + 1,
R+ ( a+b
2 , a+b
we have R+ (a, b) < R+ (a − 1, b) < R+ ( a+b−1
, a+b−1
); while when a + b is an odd number
(cid:19)
(cid:18)
2
2
and a ≤ b − 1, we have R+ (a, b) < R+ (a, b − 1) < R+ ( a+b−1
, a+b−1
). Therefore,
2
2
(cid:99)
(cid:99), (cid:98) a + b
2

R+ (a, b) < R+

(cid:98) a + b
2

.

According to the second property such that lima→∞ R+ (a, a) = 0, we obtain (37).

Using Lemma 12, we ﬁrst show that, in any sample path, the Opt-KG will label each
instance inﬁnitely many times as T goes to inﬁnity. Let ηi (T ) be a random variable rep-
resenting the number of times that the i-th instance has been labeled until the stage T
using Opt-KG. Given a sample path ω , let I (ω) = {i : limT →∞ ηi (T )(ω) < ∞} be the
set of instances that has been labeled only ﬁnite number of times as T goes to inﬁnity in

35

Chen, Lin and Zhou

contradiction. Assuming that I (ω) is not empty, then after a certain stage (cid:98)T , instances in
this sample path. We need to prove that I (ω) is an empty set for any ω . We prove it by
Therefore, there will exist ¯T > (cid:98)T such that:
I (ω) will never be labeled. By Lemma 12, for any j ∈ I c , limT →∞ R+ (aT
j (ω), bT
j (ω)) = 0.
i (ω), b (cid:98)T
i∈I R+ (a (cid:98)T
¯T
¯T
¯T
¯T
i∈I R+ (a
R+ (a
i (ω), b
i (ω)) = max
j (ω), b
max
j (ω)) < max
i (ω)).
j∈I c
Then according to the Opt-KG policy, the next instance to be labeled must be in I (ω),
which leads to the contradiction. Therefore, I (ω) will be an empty set for any ω .
Let Y s
i be the random variable which takes the value 1 if the s-th label of the i-th instance
is 1 and the value −1 if the s-th label is 0. It is easy to see that E(Y s
i |θi ) = Pr(Y s
i = 1|θi ) =
θi . Hence, Y s
i , s = 1, 2, . . . are independent and identically distributed random variables.
By the fact that limT →∞ ηT (i) = ∞ in all sample paths and using the strong law of large
(cid:80)ηi (T )
number, we conclude that, conditioning on θi , i = 1, . . . , K , the conditional probability of
i − bT
aT
s=1 Y s
i |θi ) = 2θi − 1
= E(Y s
i
i
lim
= lim
T →∞
T →∞
ηi (T )
ηi (T )
i } and
for all i = 1, . . . , K , is one. According to Proposition 2, we have HT = {i : aT
i ≥ bT
H ∗ = {i : θi ≥ 0.5}. The accuracy is Acc(T ) = 1
K (|HT ∩ H ∗ | + |H c
T ∩ (H ∗ )c |) . We have:
(cid:18)
(cid:19)
(cid:18)
(cid:19)
T →∞(|HT ∩ H ∗ | + |H c
T →∞ Acc(T ) = 1|{θi}K
T ∩ (H ∗ )c |) = K |{θi}K
Pr( lim
lim
i=1 ) = Pr
i=1
i − bT
aT
= 2θi − 1, ∀i = 1, . . . , K |{θi}K
i
lim
T →∞
i=1
ηi (T )
whenever θi (cid:54)= 0.5 for all i. The last inequality is due to the fact that, as long as θi is
i −bT
ηi (T ) = 2θi − 1, ∀i =
not 0.5 in any i, any sample path that gives the event limT →∞ aT
i
i ) = sgn(2θi − 1)(+∞), which further implies
i − bT
1, . . . , K also gives the event limT →∞ (aT
limT →∞ (|HT ∩ H ∗ | + |H c
T ∩ (H ∗ )c |) = K .
(cid:18)
(cid:19)
(cid:20)
(cid:18)
(cid:19)(cid:21)
Finally, we have:
(cid:20)
(cid:18)
T →∞ Acc(T ) = 1|{θi}K
= E{θi }K
lim
T →∞ Acc(T ) = 1
lim
i=1
i=1
T →∞ Acc(T ) = 1|{θi}K
= E{θi :θi (cid:54)=0.5}K
lim
Pr
i=1
i=1
= E{θi :θi (cid:54)=0.5}K
[1] = 1,
i=1
where the second equality is because {θi : ∃i, θi = 0.5} is a zero measure set.

(cid:19)(cid:21)

≥ Pr

= 1,

Pr

Pr

A.6 Proof of Proposition 8
Recall that our random reward is a two-point distribution with the probability p1 = a
a+b
of being R1 (a, b) = h(I (a + 1, b)) − h(I (a, b)) and p2 = b
a+b of being R2 (a, b) = h(I (a, b +
1)) − h(I (a, b)). The pessimistic KG selects the next instance which maximizes R− (a, b) =

36

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

min(R1 (a, b), R2 (a, b)). To show that the policy is inconsistent, we ﬁrst compute the exact
values for R− (a, b) for positive integers (a, b).
Utilizing Corollary 3 and the basic properties of I (a, b) in (34), (35), (36), we have:
1. When a ≥ b + 1:

0.5a+b
R1 (a, b) = I (a + 1, b) − I (a, b) =
aB (a, b)
R2 (a, b) = I (a, b + 1) − I (a, b) = − 0.5a+b
bB (a, b)

> 0;

< 0.

Therefore,

2. When a = b:

R− (a, b) = R2 (a, b) = − 0.5a+b
bB (a, b)

< 0.

R1 (a, b) = I (a + 1, a) − I (a, a) =

0.52a
aB (a, a)
0.52a
R2 (a, b) = 1 − I (a, a + 1) − I (a, a) =
aB (a, a)

;

.

Therefore, we have x1 = x2 and

R− (a, b) = R1 (a, b) = R2 (a, b) =

0.52a
aB (a, a)

> 0.

3. When b − 1 ≥ a:

R1 (a, b) = I (a, b) − I (a + 1, b) = − 0.5a+b
aB (a, b)
0.5a+b
bB (a, b)

R2 (a, b) = I (a, b) − I (a, b + 1) =

> 0.

< 0;

Therefore

< 0.

R− (a, b) = R1 (a, b) = − 0.5a+b
aB (a, b)
We summarize the properties of R− (a, b) in the next Lemma.
Lemma 13 Properties for R− (a, b):
1. R− (a, b) > 0 if and only if a = b.
2. R− (a, b) is symmetric, i.e., R− (a, b) = R− (b, a)
3. When a = b + 1, then R− (a, b) = R− (b + 1, b) is monotonical ly increasing in b. By
the symmetry of R− (a, b), when b = a + 1, R− (a, b) = R− (a, a + 1) is monotonical ly
increasing in a.

37

Chen, Lin and Zhou

Figure 13: Illustration of R− (a, b).

4. When a ≥ b + 1, for any ﬁxed b, R− (a, b) is monotonical ly increasing in a. By the
symmetry of R− (a, b), when b ≥ a + 1, for any ﬁxed a, R− (a, b) is monotonical ly
increasing in b.

For better visualization, we plot values of R− (a, b) for diﬀerent a, b in Figure 13. All the
properties in Lemma 13 can be seen clearly from Figure 13. The proof of these properties
are based on simple algebra and thus omitted here.
From Lemma 13, we can conclude that for any positive integers a, b with a + b (cid:54)= 3:
R− (1, 2) = R− (2, 1) < R− (a, b).

(38)

Recall that the pessimistic KG selects:

it = arg max
i∈{1,...,K }

R− (at
i , bt
i ).

i = 1 for all i ∈ {1 . . . , K }, the corre-
When starting from the uniform prior with a0
i = b0
sponding R− (a0
i ) = R− (1, 1) > 0. After obtaining a label for any instance i, the Beta
i , b0
parameters for θi will become either (2, 1) or (1, 2) with R− (1, 2) = R− (2, 1) < 0. There-
fore, for the ﬁrst K stages, the pessimistic KG policy will acquire the label for each instance
once. For any instance i, we have either aK
i = 2, bK
i = 1 or aK
i = 1, bK
i = 2 at the stage K .
Then the pessimistic KG policy will select the ﬁrst instance to label. According to (38),
for any t ≥ K , R− (at
1 ) > R− (1, 2) = R− (2, 1). Therefore, the pessimistic KG policy will
1 , bt
consistently acquire the label for the ﬁrst instance. Since the tie will only appear at the
stage K , the randomized pessimistic KG will also consistently select a single instance to
label after K stages.

Appendix B. Incorporate Reliability of Heterogeneous Workers

As we discussed in Section 5 in main text, we approximate the posterior so that at any
stage for all i, j , θi and ρj will follow Beta distributions. In particular, assuming at the

38

ab  1234567891010987654321−0.25 −0.2−0.15 −0.1−0.05    0 0.05  0.1 0.15  0.2 0.25Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

current state θi ∼ Beta(ai , bi ) and ρj ∼ Beta(cj , dj ), the posterior distribution conditioned
on zij takes the following form:
Pr(zij = 1|θi , ρj )Beta(ai , bi )Beta(cj , dj )
p(θi , ρj |zij = 1) =
,
Pr(zij = 1)
Pr(zij = −1|θi , ρj )Beta(ai , bi )Beta(cj , dj )
p(θi , ρj |zij = −1) =
Pr(zij = −1)
,
where the likelihood Pr(zij = z |θi , ρj ) for z = 1, −1 is deﬁned in (18) and (19), i.e.,
Pr(zij = 1|θi , ρj ) = θiρj + (1 − θi )(1 − ρj ),
Pr(zij = −1|θi , ρj ) = (1 − θi )ρj + θi (1 − ρj ).

Also,

Pr(zij = 1) = E(Pr(zij = 1|θi , ρj )) = E(θi )E(ρj ) + (1 − E(θi ))(1 − E(ρj ))
dj
cj
bi
ai
ai + bi
cj + dj
ai + bi
cj + dj

=

+

,

Pr(zij = −1) = E(Pr(zij = −1|θi , ρj )) = (1 − E(θi ))E(ρj ) + E(θi )(1 − E(ρj ))
dj
cj
ai
bi
+
.
=
ai + bi
cj + dj
ai + bi
cj + dj
The posterior distributions p(θi , pj |zij = z ) no longer takes the form of the product
of Beta distributions on θi and pj . Therefore, we use variational approximation by ﬁrst
assuming the conditional independence of θi and ρj :
p(θi , ρj |zij = z ) ≈ p(θi |zij = z )p(ρj |zij = z ).

p(ρj |zij = 1) =

p(θi |zij = −1) =

In fact, the exact form of marginal distributions can be calculated as follows:
θiE(ρj ) + (1 − θi )(1 − E(ρj ))
p(θi |zij = 1) =
Pr(zij = 1)
E(θi )ρj + (1 − E(θi ))(1 − ρj )
Pr(zij = 1)
(1 − θi )E(ρj ) + θi (1 − E(ρj ))
Pr(zij = −1)
(1 − E(θi ))ρj + E(θi )(1 − ρj )
Pr(zij = −1)
To approximate the marginal distribution as Beta distribution, we use the moment matching
technique. In particular, we approximate p (θi |zij = z ) ≈ Beta(˜ai (z ), ˜bi (z )) such that
(cid:101)Ez (θi )
˜ai (z )
(cid:101)Ez (θ2
˜ai (z ) + ˜bi (z )
˜ai (z )(˜ai (z ) + 1)
i )
(˜ai (z ) + ˜bi (z ))(˜ai (z ) + ˜bi (z ) + 1)

.
= Ep(θi |zij =z ) (θ2
i ) =

.
= Ep(θi |zij =z ) (θi ) =

p(ρj |zij = −1) =

(39)

(40)

,

Beta(ai , bi ),

Beta(cj , dj ),

Beta(ai , bi ),

Beta(cj , dj ).

,

39

Chen, Lin and Zhou

,

,

(43)

(41)

(42)

(44)

.
= Ep(ρj |zij =z ) (ρ2
j ) =

˜ai (z )(˜ai (z )+1)
˜ai (z )
where
are the ﬁrst and second order moment of
and
(˜ai (z )+˜bi (z ))(˜ai (z )+˜bi (z )+1)
˜ai (z )+˜bi (z )
(cid:101)Ez (θi ) − (cid:101)Ez (θ2
Beta(˜ai (z ), ˜bi (z )). To make (39) and (40) hold, we have:
˜ai (z ) = (cid:101)Ez (θi )
i ) − (cid:16)(cid:101)Ez (θi )
(cid:17)2 ,
(cid:101)Ez (θ2
i )
(cid:101)Ez (θi ) − (cid:101)Ez (θ2
˜bi (z ) = (1 − (cid:101)Ez (θi ))
i ) − (cid:16)(cid:101)Ez (θi )
(cid:17)2 .
(cid:101)Ez (θ2
i )
Similarly, we approximate p (ρj |zij = z ) ≈ Beta(˜cj (z ), ˜dj (z )), such that
(cid:101)Ez (ρj )
˜cj (z )
.
= Ep(ρj |zij =z ) (ρj ) =
(cid:101)Ez (ρ2
˜cj (z ) + ˜dj (z )
˜cj (z )(˜cj (z ) + 1)
j )
(˜cj (z ) + ˜dj (z ))(˜cj (z ) + ˜dj (z ) + 1)
˜cj (z )(˜cj (z )+1)
˜cj (z )
are the ﬁrst and second order moment of
and
where
(˜cj (z )+ ˜dj (z ))(˜cj (z )+ ˜dj (z )+1)
˜cj (z )+ ˜dj (z )
(cid:101)Ez (ρj ) − (cid:101)Ez (ρ2
Beta(˜cj (z ), ˜dj (z )). To make (39) and (40) hold, we have:
˜cj (z ) = (cid:101)Ez (ρj )
j ) − (cid:16) (cid:101)Ez (ρj )
(cid:17)2 ,
(cid:101)Ez (ρ2
j )
(cid:101)Ez (ρj ) − (cid:101)Ez (ρ2
˜dj (z ) = (1 − (cid:101)Ez (ρj ))
j ) − (cid:16)(cid:101)Ez (ρj )
(cid:17)2 .
(cid:101)Ez (ρ2
j )
(46)
i ), (cid:101)Ez (ρj ) and (cid:101)Ez (ρ2
Furthermore, we can compute the exact values for (cid:101)Ez (θi ), (cid:101)Ez (θ2
j ) as
follows.(cid:101)E1 (θi ) =
i )E(ρj ) + (E(θi ) − E(θ2
i ))(1 − E(ρj ))
E(θ2
ai ((ai + 1)cj + bidj )
(cid:101)E1 (θ2
(ai + bi + 1)(ai cj + bidj )
p(zij = 1)
i ) − E(θ3
i ))(1 − E(ρj ))
i )E(ρj ) + (E(θ2
E(θ3
ai (ai + 1)((ai + 2)cj + bidj )
i ) =
(cid:101)E−1 (θi ) =
(ai + bi + 1)(ai + bi + 2)(ai cj + bidj )
p(zij = 1)
(E(θi ) − E(θ2
i )(1 − E(ρj ))
i ))E(ρj ) + E(θ2
ai (bi cj + (ai + 1)dj )
p(zij = −1)
(cid:101)E−1 (θ2
(ai + bi + 1)(bi cj + aidj )
i ) − E(θ3
i )(1 − E(ρj ))
(E(θ2
i ))E(ρj ) + E(θ3
ai (ai + 1)(bi cj + (ai + 2)dj )
p(zij = −1)
i ) =
(cid:101)E1 (ρj ) =
(ai + bi + 1)(ai + bi + 2)(bi cj + aidj )
j ) + (1 − E(θi ))(E(ρj ) − E(ρ2
E(θi )E(ρ2
j ))
cj (ai (cj + 1) + bidj )
(cid:101)E1 (ρ2
(cj + dj + 1)(ai cj + bidj )
p(zij = 1)
j ) − E(ρ3
j ) + (1 − E(θi ))(E(ρ2
E(θi )E(ρ3
j ))
cj (cj + 1)(ai (cj + 2) + bidj )
j ) =
(cid:101)E−1 (ρj ) =
(cj + dj + 1)(cj + dj + 2)(ai cj + bidj )
p(zij = 1)
(1 − E(θi ))E(ρ2
j ) + E(θi )(E(ρj ) − E(ρ2
j ))
cj (bi (cj + 1) + aidj )
p(zij = −1)
(cid:101)E−1 (ρ2
(cj + dj + 1)(bi cj + aidj )
j ) − E(ρ3
(1 − E(θi ))E(ρ3
j ) + E(θi )(E(ρ2
j ))
cj (cj + 1)(bi (cj + 2) + aidj )
p(zij = −1)
j ) =
(cj + dj + 1)(cj + dj + 2)(bi cj + aidj )

(45)

,

.

,

,

=

=

=

=

,

,

,

,

=

=

=

=

40

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

Assuming that at a certain stage, θi follows a Beta posterior Beta(ai , bi ) and ρj follows
a Beta posterior Beta(cj , dj ), the reward of getting positive and negative labels for the i-th
instance from the j -th worker are:
R1 (ai , bi , cj , dj ) = h(I (˜ai (z = 1), ˜bi (z = 1))) − h(I (ai , bi )),
R2 (ai , bi , cj , dj ) = h(I (˜ai (z = −1), ˜bi (z = −1))) − h(I (ai , bi )),
(48)
cj and dj through (cid:101)Ez (θi ) and (cid:101)Ez (θ2
where ˜ai (z = ±1) and ˜bi (z = ±1) are deﬁned in (41) and (42), which further depend on
i ). With the reward in place, we can directly apply the
Opt-KG policy in the heterogeneous worker setting.

(47)

Appendix C. Extensions

In this section, we provide detailed Opt-KG algorithms for extensions in Section 6.

C.1 Utilizing Contextual Information
When each instance is associated with a p-dimensional feature vector xi ∈ Rp , we incorpo-
rate the feature information in our budget allocation problem by assuming:
θi = σ((cid:104)w, xi (cid:105))

1
1 + exp{−(cid:104)w, xi (cid:105)} ,
1
where σ(x) =
1+exp{−x} is the sigmoid function and w is assumed to be drawn from a
Gaussian prior N (µ0 , Σ0 ). At the t-th stage with the state S t = (µt , Σt ) and w ∼ (µt , Σt ),
the decision maker chooses the it -th instance to be labeled and observes the label yit ∈
{−1, 1}. The posterior distribution p(w|yit , S t ) ∝ p(yit |w)p(w|S t ) has the following log-
likelihood:

(49)

.
=

ln p(w|yit , S t ) = ln p(yit |w) + ln p(w|S t ) + const
=1(yit = 1) ln σ((cid:104)w, xit (cid:105)) + 1(yit = −1) ln (1 − σ((cid:104)w, xit (cid:105)))
(w − µt )(cid:48)Ωt (w − µt ) + const,
− 1
2
where Ωt = (Σt )−1 is the precision matrix. To approximate p(w|yit , µt , Σt ) by a Gaussian
distribution N (µt+1 , Σt+1 ), we use the Laplace method (see Chapter 4.4 in Bishop, 2007).
In particular, the mean of the posterior Gaussian is the MAP (maximum a posteriori)
estimator of w:
ln p(w|yit , S t ),

µt+1 = arg max
w
which can be computed by any numerical optimization method (e.g., Newton’s method).
Ωt+1 = −∇2 ln p(w|yit , S t )(cid:12)(cid:12)w=µt+1
The precision matrix takes the following form,
t+1xit+1 )(1 − σ(µ(cid:48)
t+1xit+1 ))xit+1 x(cid:48)
= Ωt + σ(µ(cid:48)
it+1 .
By Sherman-Morrison formula, the covariance matrix can be computed as,
t+1xit )(1 − σ(µt+1xit ))
σ(µ(cid:48)
Σt+1 = (Ωt+1 )−1 = Σt −
Σtxit+1 x(cid:48)
t+1xit )(1 − σ(µ(cid:48)
it Σt .
t+1xit ))x(cid:48)
1 + σ(µ(cid:48)
it

Σtxit

(50)

41

Chen, Lin and Zhou

We also calculate the transition probability of yit = 1 and yit = −1 using the technique
(cid:90)
(cid:90)
from Bayesian logistic regression (see Chapter 4.5 in Bishop, 2007):

Pr(yit = 1|S t , it ) =

p(yit = 1|w)p(w|S t )dw =

σ(w(cid:48)xi )p(w|S t )dw ≈ σ(µiκ(s2
i )),

i /8)−1/2 and µi = (cid:104)µt , xi (cid:105) and s2
i = x(cid:48)
where κ(s2
i ) = (1 + πs2
iΣtxi .
To calculate the reward function, in addition to the transition probability, we also need
to compute:
(cid:19)
(cid:18)
(cid:12)(cid:12)(cid:12)wt ∼ N (µt , Σt )
i = Pr(θi ≥ 0.5|Ft )
P t
txi} ≥ 0.5
1
1 + exp{−w(cid:48)
= Pr
(cid:19)
(cid:18)(cid:90)
(cid:90) ∞
txi ≥ 0|wt ∼ N (µt , Σt ))
= Pr(w(cid:48)
δ(c − (cid:104)w, xi (cid:105))N (w|µt , Σt )dw
w
0
(cid:90)
where δ(·) is the Dirac delta function. Let
w

δ(c − (cid:104)w, xi (cid:105))N (w|µt , Σt )dw.

p(c) =

dc,

=

P t
i =

Therefore, we have:

p(c)dc = 1 − Φ

Since the marginal of a Gaussian distribution is still a Gaussian, p(c) is a univariate-Gaussian
distribution with the mean and variance:
µi = E(c) = (cid:104)E(w), xi (cid:105) = (cid:104)µt , xi (cid:105),
i = Var(c) = (xi )(cid:48)Cov(w, w)xi = (xi )(cid:48)Σtxi .
s2
(cid:19)
(cid:18)
(cid:90) ∞
− µi
si
0
where Φ(·) is the CDF of the standard Gaussian distribution.
With P t
i and transition probability in place, the expected reward in value function takes
(cid:32) K(cid:88)
(cid:33)
(cid:12)(cid:12)(cid:12)S t , it
) − K(cid:88)
the following form :
R(S t , it ) = E
h(P t+1
h(P t
(52)
.
i )
i
and hence (52) cannot be written as E (cid:0)h(P t+1
(cid:1) in (28). In this problem,
i=1
i=1
We note that since w will aﬀect all P t
i , the summation from 1 to K in (52) can not be omitted
)|S t , it
) − h(P t
it
it
KG or Opt-KG need to solve O(2T K ) optimization problems to compute the mean of the
posterior as in (50), which could be computationally quite expensive. One possibility to
address this problem is to use the variational Bayesian logistic regression (Jaakkola and
Jordan, 2000), which could lead to a faster optimization procedure.

(51)

,

42

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

C.2 Multi-Class Categorization

c }C
{H T
c=1 =

Given the model and notations introduced in Section 6.2, at the ﬁnal stage T when all
budget is used up, we construct the set H T
for each class c to maximize the conditional
c
expected classiﬁcation accuracy:
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)FT
(cid:33)
(cid:32) K(cid:88)
C(cid:88)
I (i ∈ Hc )I (i ∈ H ∗
E
c )
K(cid:88)
C(cid:88)
c=1
i=1
c |FT ) .
I (i ∈ Hc ) Pr (i ∈ H ∗
arg max
Hc⊆{1,...,C },Hc∩H˜c=∅
c=1
i=1
c = {i : θic ≥ θic(cid:48) , ∀c(cid:48) (cid:54)= c} is the true set of instances in the class c. The set H T
Here, H ∗
c
consists of instances that belong to class c. Therefore, {H T
c }C
c=1 should form a partition of
all instances {1, . . . , K }. Let

arg max
Hc⊆{1,...,C },Hc∩H˜c=∅

(53)

=

ic = Pr(i ∈ H ∗
c |FT ) = Pr(θic ≥ θi˜c , ∀ ˜c (cid:54)= c|FT ).
P T

(54)

V (S 0 )

Eπ

= sup
π

.
= sup
π

To maximize the right hand side of (53), we have
c = {i : P T
ic ≥ P T
i˜c , ∀˜c (cid:54)= c}.
H T
(55)
(cid:0)maxc∈{1...,C } P T
(cid:1) .
c. The maximum conditional expected accuracy takes the form: (cid:80)K
If there is i belongs to more than one H T
c , we only assign it to the one with the smallest index
(cid:33)
(cid:32) K(cid:88)
(cid:33)(cid:33)
(cid:32) K(cid:88)
(cid:32)
ic
i=1
(cid:12)(cid:12)(cid:12)FT
C(cid:88)
Then the value function can be deﬁned as:
c )I (i ∈ H ∗
I (i ∈ H T
E
c )
c=1
i=1
i=1
.
= maxc∈{1...,C } P T
iC ) and h(PT
i = (P T
where PT
i1 , . . . , P T
i )
ic . Following Proposition 4, let
ic = Pr(i ∈ H ∗
c |Ft ) and Pt
R(S t , it ) = E (cid:0)h(Pt+1
(cid:1) .
P t
i = (P t
i1 , . . . , P t
iC ), we deﬁne incremental reward function at each
stage:
) − h(Pt
it )|S t , it
it
(cid:32)T −1(cid:88)
(cid:12)(cid:12)(cid:12)S 0
The value function can be re-written as:
R(S t , it )
where G0 (S 0 ) = (cid:80)K
t=0
i ). Since the reward function only depends on S t
i=1 h(P0
it
C(cid:88)
we can deﬁne the reward function in a more explicit way by deﬁning:
αc(cid:80)C
˜c=1 α˜c
c=1

h(I (α + δ c )) − h(I (α)).

V (S 0 ) = G0 (S 0 ) + sup
π

∈ RC
+ ,

h(PT
i )

,

(cid:33)

,

R(α) =

Eπ

= αt
it

Eπ

43

Chen, Lin and Zhou

Here δ c be a row vector of length C with one at the c-th entry and zeros at all other entries;
and I (α) = (I1 (α), . . . , IC (α)) where
Ic (α) = Pr(θc ≥ θ˜c , ∀˜c (cid:54)= c|θ ∼ Dir(α)).

(56)

Therefore, we have R(S t , it ) = R(αt
).
it
To evaluate the reward R(α), the ma jor bottleneck is how to compute Ic (α) eﬃciently.
Directly taking the C -dimensional integration on the region {θc ≥ θ˜c , ∀˜c (cid:54)= c} ∩ ∆C will be
computationally very expensive, where ∆C denotes the C -dimensional simplex. Therefore,
we propose a method to convert the computation of Ic (α) into a one-dimensional integration.
It is known that to generate θ ∼ Dir(α), it is equivalent to generate {Xc}C
c=1 with Xc ∼
Gamma(αc , 1) and let θc ≡ Xc(cid:80)C
. Then θ = (θ1 , . . . , θC ) will follow Dir(α). Therefore,
c=1 Xc
we have:

Ic (α) = Pr(Xc ≥ X˜c , ∀˜c (cid:54)= c|Xc ∼ Gamma(αc , 1)).
(cid:90)
(cid:90)
(cid:90)
C(cid:89)
It is easy to see that
(cid:90)
(cid:89)
0≤xC ≤xc
0≤x1≤xc
xc≥0
c=1
xc≥0
˜c(cid:54)=c

FGamma (xc ; α˜c , 1)dxc ,

Ic (α) =

=

fGamma (xc ; αc , 1)

· · ·

· · ·

fGamma (xc ; αc , 1)dx1 . . . dxC

(57)

(58)

where fGamma (x; αc , 1) is the density function of Gamma distribution with the parameter
(αc , 1) and FGamma (xc ; α˜c , 1) is the CDF of Gamma distribution at xc with the parameter
(α˜c , 1). In many softwares, FGamma (xc ; α˜c , 1) can be calculated very eﬃciently without an
explicit integration. Therefore, we can evaluate Ic (α) by performing only a one-dimensional
numerical integration as in (58). We could also use Monte-Carlo approximation to further
accelerate the computation in (58).

References

P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine Learning, 47:235–256, 2002.

Y. Bachrach, T. Minka, J. Guiver, and T. Graepel. How to grade a test without knowing the
answers - a Bayesian graphical model for adaptive crowdsourcing and aptitude testing.
In ICML, 2012.

M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, Gatsby
Computational Neuroscience Unit, University College London, 2003.

C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2007.

S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed
bandit problems. Foundations and Trends in Machine Learning, 5(1):1–122, 2012.

44

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using
the EM algorithm. Journal of the Royal Statistical Society Series C, 28:20–28, 1979.

S. Ertekin, H. Hirsh, and C. Rudin. Wisely using a budget for crowdsourcing. Technical
report, MIT, 2012.

Eyal Even-Dar and Yishay Mansour. Convergence of optimistic and incremental Q-learning.
In NIPS, 2001.

P. Frazier, W. B. Powell, and S. Dayanik. A knowledge-gradient policy for sequential
information collection. SIAM J. Control Optim., 47(5):2410–2439, 2008.

C. Gao and D. Zhou. Minimax optimal convergence rates for estimating ground truth from
crowdsourced labels. arXiv:1310.5764, 2013.

Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B.
Rubin. Bayesian Data Analysis. Chapman and Hall, 3rd edition, 2013.

J. C. Gittins. Multi-armed Bandit Al location Indices. John Wiley & Sons, 1989.

S. S. Gupta and K. J. Miescke. Bayesian look ahead one stage sampling allocations for
selection the largest normal mean. J. of Stat. Planning and Inference, 54(2):229–244,
1996.

C. Ho, S. Jabbari, and J. W. Vaughan. Adaptive task assignment for crowdsourced classi-
ﬁcation. In ICML, 2013.

P. G. Ipeirotis, F. Provost, V. S. Sheng, and J. Wang. Repeated labeling using multiple
noisy label. Data Mining and Know ledge Discovery, 2013.

T. Jaakkola and M. I. Jordan. Bayesian parameter estimation via variational methods.
Statistics and Computing, 10:25–37, 2000.

E. Kamar, S. Hacker, and E. Horvitz. Combing human and machine intelligence in large-
scale crowdsourcing. In AAMAS, 2012.

D. Karger, S. Oh, and D. Shah. Eﬃcient crowdsourcing for multi-class labeling. In ACM
Sigmetrics, 2013a.

D. R. Karger, S. Oh, and D. Shah. Budget-optimal task allocation for reliable crowdsourcing
systems. Operations Research, 62(1):1–24, 2013b.

E. Kaufmann, O. Cappe, and A. Garivier. On Bayesian upper conﬁdence bounds for bandit
problems. In AISTATS, 2012.

L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to person-
alized news article recommendation. In Proceedings of International World Wide Web
Conference, 2010.

C. Liu and Y. M. Wang. Truelabel + confusions: A spectrum of probabilistic models in
analyzing multiple ratings. In ICML, 2012.

45

Chen, Lin and Zhou

Q. Liu, J. Peng, and A. Ihler. Variational inference for crowdsourcing. In NIPS, 2012.

Andrew Y. Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward trans-
formations: Theory and application to reward shaping. In ICML, 1999.

J. Nino-Mora. Computing a classic index for ﬁnite-horizon bandits. INFORMS Journal on
Computing, 23(2):254–267, 2011.

R. D. Nowak. Noisy generalized binary search. In NIPS, 2009.

J. Paisley, D. Blei, and M. Jordan. Variational bayesian inference with stochastic search.
In ICML, 2012.

W. B. Powell. Approximate Dynamic Programming: solving the curses of dimensionality.
John Wiley & Sons, 2007.

M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.
Wiley, 2005.

V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning
from crowds. Journal of Machine Learning Research, 11:1297–1322, 2010.

C. P. Robert. The Bayesian Choice: From Decision-Theoretic Foundations to Computa-
tional Implementation. Springer, 2007.

R. T. Rockafellar and S. Uryasev. Conditional value-at-risk for general loss distributions.
J. of Banking and Finance, 26:1443–1471, 2002.

B. Settles. Active learning literature survey. Technical report, University of Wisconsin–
Madison, 2009.

R. Snow, B. O. Connor, D. Jurafsky, and A. Y. Ng. Cheap and fast - but is it good?
evaluating non-expert annotations for natural language tasks. In EMNLP, 2008.

Istv´an Szita and Andr´as L˝orincz. The many faces of optimism: a unifying approach. In
ICML, 2008.

J. Wang, P. G. Ipeirotis, and F. Provost. Quality-based pricing for crowdsourced workers.
Technical report, New York University, 2013.

P. Welinder, S. Branson, S. Belongie, and P. Perona. The multidimensional wisdom of
crowds. In NIPS, 2010.

J. Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and J. R. Movellan. Whose vote should count
more: Optimal integration of labels from labelers of unknown expertise. In NIPS, 2009.

J. Xie and P. I. Frazier. Sequential bayes-optimal policies for multiple comparisons with a
known standard. Operations Research, 61(5):1174–1189, 2013.

Y. Yan, R. Rosales, G. Fung, and J. Dy. Active learning from crowds. In ICML, 2011.

D. Zhou, S. Basu, Y. Mao, and J. Platt. Learning from the wisdom of crowds by minimax
conditional entropy. In NIPS, 2012.

46

Journal of Machine Learning Research 15 (2014) 141-145

Submitted 4/13; Revised 10/13; Published 1/14

EnsembleSVM: A Library for Ensemble Learning Using
Support Vector Machines

Marc Claesen
KU Leuven, ESAT – STADIUS/iMinds Future Health
Kasteelpark Arenberg 10, box 2446
3001 Leuven, Belgium

marc.claesen@esat.kuleuven.be

frank.desmet@cm.be
Frank De Smet
KU Leuven, Department of Public Health and Primary Care, Environment and Health
Kapucijnenvoer 35 blok d, box 7001
3000 Leuven, Belgium

Johan A.K. Suykens
Bart De Moor
KU Leuven, ESAT – STADIUS/iMinds Future Health
Kasteelpark Arenberg 10, box 2446
3001 Leuven, Belgium

Editor: Geoﬀ Holmes

johan.suykens@esat.kuleuven.be
bart.demoor@esat.kuleuven.be

Abstract

EnsembleSVM is a free software package containing eﬃcient routines to perform ensemble
learning with support vector machine (SVM) base models.
It currently oﬀers ensemble
methods based on binary SVM models. Our implementation avoids duplicate storage and
evaluation of support vectors which are shared between constituent models. Experimental
results show that using ensemble approaches can drastically reduce training complexity
while maintaining high predictive accuracy. The EnsembleSVM software package is freely
available online at http://esat.kuleuven.be/stadius/ensemblesvm.

Keywords:

classiﬁcation, ensemble learning, support vector machine, bagging

1. Introduction

Data sets are becoming increasingly large. Machine learning practitioners are confronted
with problems where the main computational constraint is the amount of time available.
Problems become particularly challenging when the training sets no longer ﬁt into memory.
Accurately solving the dual problem for SVM training with nonlinear kernels requires a run
time which is at least quadratic in the size of the training set n, thus training complexity
is Ω(n2 ) (Bottou and Lin, 2007; List and Simon, 2009).

EnsembleSVM employs a divide-and-conquer strategy by aggregating many SVM models,
trained on small subsamples of the training set. Through subdivision, total training time
decreases signiﬁcantly, even though more models need to be trained. For example, training
p classiﬁers on subsamples of size n/p, results in an approximate complexity of Ω(n2/p).
This reduction in complexity helps in dealing with large data sets and nonlinear kernels.

c(cid:13)2014 Marc Claesen, Frank De Smet, Johan A.K. Suykens and Bart De Moor.

Claesen, De Smet, Suykens and De Moor

Ensembles of SVM models have been used in various applications (Wang et al., 2009;
Linghu and Sun, 2010; Mordelet and Vert, 2011). Collobert et al. (2002) use ensembles
for large scale learning and employ a neural network to aggregate base models. Valentini
and Dietterich (2003) provide an implementation which allows base models to use diﬀerent
kernels. For eﬃciency reasons, we require base models to share a single kernel function.
While other implementations mainly focus on improving predictive performance, our
framework primarily aims to (i) make nonlinear large-scale learning feasible through com-
plexity reductions and (ii) enable fast prototyping of novel ensemble algorithms.

2. Software Description

The EnsembleSVM software is freely available online under a LGPL license. EnsembleSVM
provides ensembles of instance-weighted SVMs, as deﬁned in Equation (1). The default ap-
proach we oﬀer is bagging, which is commonly used to improve the performance of unstable
classiﬁers (Breiman, 1996). In bagging, base models are trained on bootstrap subsamples
of the training set and their predictions are aggregated through ma jority voting.
Base model ﬂexibility is maximized by using instance-weighted binary support vector
machine classiﬁers, as deﬁned in Equation (1). This formulation lets users deﬁne misclassiﬁ-
cation penalties per training instance Ci , i = 1, . . . , n and encompasses popular approaches
n(cid:88)
such as C-SVC and class-weighted SVM (Cortes and Vapnik, 1995; Osuna et al., 1997).
1
wT w +
min
Ci ξi ,
2
w,ξ ,ρ
i=1
sub ject to yi (wT φ(xi ) + ρ) ≥ 1 − ξi ,
ξi ≥ 0,

i = 1, . . . , n,

(1)

i = 1, . . . , n.

When aggregating SVM models, the base models often share support vectors (SVs).
The EnsembleSVM software intelligently caches distinct SVs to ensure that they are only
stored and used for kernel evaluations once. As a result, EnsembleSVM models are smaller
and faster in prediction than ensemble implementations based on wrappers.

2.1 Implementation

EnsembleSVM has been implemented in C++ and makes heavy use of the standard library.
The main implementation focus is training speed. We use facilities provided by the C++11
standard and thus require a moderately recent compiler, such as gcc ≥ 4.7 or clang ≥ 3.2.
A portable Makeﬁle system based on GNU autotools is used to build EnsembleSVM.
EnsembleSVM interfaces with LIBSVM to train base models (Chang and Lin, 2011). Our
code must be linked to LIBSVM but does not depend on a speciﬁc version. This allows users
to choose the desired version of the LIBSVM software in the back-end.
The EnsembleSVM programming framework is designed to facilitate prototyping of en-
semble algorithms using SVM base models. We particularly provide extensive support to
deﬁne novel aggregation schemes, should the available options be insuﬃcient. Key compo-
nents are extensively documented and on a broad overview is provided on our wiki.1

1. The EnsembleSVM development wiki is available at https://github.com/claesenm/EnsembleSVM/wiki.

142

EnsembleSVM

The EnsembleSVM library was built with extensibility and user contributions in mind.
Ma jor API functions are well documented to lower the threshold for external development.
The executable tools provided with EnsembleSVM are essentially wrappers for the library
itself. The tools can be considered as use cases of the main API functions to help developers.

2.2 Tools

The main tools in this package are esvm-train and esvm-predict, used to train and
predict with ensemble models. Both of these are pthread-parallelized. Additionally, the
merge-models tool can be used to merge standard LIBSVM models into ensembles. Finally,
esvm-edit provides facilities to modify the aggregation scheme used by an ensemble.
EnsembleSVM includes a variety of extra tools to facilitate basic operations such as
stratiﬁed bootstrap sampling, cross-validation, replacing categorical features by dummy
variables, splitting data sets and sparsifying standard data sets. We recommend retaining
the original ratio of positives and negatives in the training set when subsampling.

3. Benchmark Results

To illustrate the potential of our software, EnsembleSVM 2.0 has been benchmarked with
respect to LIBSVM 3.17. To keep the experiments simple, we use ma jority voting to aggregate
predictions, even though more sophisticated methods are oﬀered. For reference, we also list
the best obtained accuracy with a linear model, trained using LIBLINEAR (Fan et al., 2008).
Linear methods are common in large-scale learning due to their speed, but may result in
signiﬁcantly decreased accuracy. This is why scalable nonlinear methods are desirable.
We used two binary classiﬁcation problems, namely the covtype and ijcnn1 data sets.2
Both data sets are balanced. Features were always scaled to [0, 1]. We have used C-
SVC as SVM and base models (∀ i : Ci = C ). Reported numbers are averages of 5 test
runs to ensure reproducibility. We used the RBF kernel, deﬁned by the kernel function
κ(xi , xj ) = e−γ ||xi−xj ||2 . Optimal parameter selection was done through cross-validation.
The covtype data set is a common classiﬁcation benchmark featuring 54 dimensions
(Blackard and Dean, 1999). We randomly sampled balanced training and test sets of 100, 000
and 40, 000 instances respectively and classiﬁed class 2 versus all others. The ijcnn1 data
set was used in a machine learning challenge during IJCNN 2001 (Prokhorov, 2001).
It
contains 35, 000 training instances in 22 dimensions.
Results in Table 1 show several interesting trends. Training EnsembleSVM models is
orders of magnitude faster, because training SVMs on small subsets signiﬁcantly reduces
complexity. Subsampling induces smaller kernels per base model resulting in lower over-
all memory use. Due to our parallelized implementation, ensemble models were faster in
prediction than LIBSVM models in both experiments despite having twice as many SVs.
The ensembles in these experiments are competitive with a traditional SVM even though
we used simple ma jority voting. For covtype, ensemble accuracy is 3% lower than a single
SVM and for ijcnn1 the ensemble is marginally better (0.2%). Linear SVM falls far short
in terms of accuracy for both experiments, but is trained much faster (< 2 seconds).

2. Both data sets are available at http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.
html.

143

Claesen, De Smet, Suykens and De Moor

data set

test set accuracy
LIBSVM
LIBLINEAR
ESVM

no. of SVs
LIBSVM
ESVM

time (s)
LIBSVM
ESVM

covtype
ijcnn1

0.92
0.98

0.76
0.92

0.89
0.98

26516
3564

50590
7026

728
9.5

35
0.3

Table 1: Summary of benchmark results per data set: test set accuracy, number of sup-
port vectors and training time. Accuracies are listed for a single LIBSVM model,
LIBLINEAR model and an ensemble model.

We obtained good results with very basic aggregation. Collobert et al. (2002) illustrated
that more sophisticated aggregation methods can improve the predictive performance of en-
sembles. Others have reported performance improvements over standard SVM for ensembles
using ma jority voting (Valentini and Dietterich, 2003; Wang et al., 2009).

4. Conclusions

EnsembleSVM provides users with eﬃcient tools to experiment with ensembles of SVMs.
Experimental results show that training ensemble models is signiﬁcantly faster than training
standard LIBSVM models while maintaining competitive predictive accuracy.

Linear methods are frequently applied in large-scale learning, mainly due to their low
training complexity. Linear methods are known to have competitive accuracy for high
dimensional problems. As our benchmarks showed, the diﬀerence in accuracy may be large
for low dimensional problems. As such, fast nonlinear methods remain desirable in large-
scale learning, particularly for low dimensional tasks with many training instances. Our
benchmarks illustrate the potential of the ensemble approaches oﬀered by EnsembleSVM.

Ensemble performance may be improved by using more complex aggregation schemes.
EnsembleSVM currently oﬀers various aggregation schemes, both linear and nonlinear. Ad-
ditionally, it facilitates fast prototyping of novel methods.

EnsembleSVM strives to provide high-quality, user-friendly tools and an intuitive pro-
gramming framework for ensemble learning with SVM base models. The software will be
kept up to date by incorporating promising new methods and ideas when they are presented
in the literature. User requests and suggestions are welcome and appreciated.

Acknowledgments

Frank De Smet is a member of the medical management department of the National Alliance
of Christian Mutualities. Acknowledged funding sources: Marc Claesen (IWT grant number
111065); Research Council KU Leuven: GOA MaNet, CoE SymBioSys; EU: ERC AdG A-
DATADRIVE-B.

144

EnsembleSVM

References

Jock A. Blackard and Denis J. Dean. Comparative accuracies of artiﬁcial neural networks
and discriminant analysis in predicting forest cover types from cartographic variables.
Computers and Electronics in Agriculture, 24(3):131–151, December 1999.

L´eon Bottou and Chih-Jen Lin. Support vector machine solvers. In L´eon Bottou, Olivier
Chapelle, Dennis DeCoste, and Jason Weston, editors, Large Scale Kernel Machines,
pages 301–320, Cambridge, MA, USA, 2007. MIT Press.

Leo Breiman. Bagging predictors. Machine Learning, 24(2):123–140, August 1996.

Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines.
ACM Transactions on Intel ligent Systems and Technology, 2:27:1–27:27, 2011. Software
available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.

Ronan Collobert, Samy Bengio, and Yoshua Bengio. A parallel mixture of SVMs for very
large scale problems. Neural Computation, 14(5):1105–1114, 2002.

Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning, 20(3):
273–297, September 1995.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
LIBLINEAR: A library for large linear classiﬁcation. Journal of Machine Learning Re-
search, 9:1871–1874, June 2008.

Bin Linghu and Bing-Yu Sun. Constructing eﬀective SVM ensembles for image classiﬁcation.
In Know ledge Acquisition and Modeling (KAM), 2010 3rd International Symposium on,
pages 80–83, 2010.

Nikolas List and Hans Ulrich Simon. SVM-optimization and steepest-descent line search.
In Proceedings of the 22nd Annual Conference on Computational Learning Theory, 2009.

Fantine Mordelet and Jean-Philippe P. Vert. ProDiGe: Prioritization Of Disease Genes with
multitask machine learning from positive and unlabeled examples. BMC bioinformatics,
12(1):389+, 2011.

Edgar Osuna, Robert Freund, and Federico Girosi. Support Vector Machines: Training and
Applications. Technical Report AIM-1602, 1997.

Danil Prokhorov. IJCNN 2001 neural network competition. Slide Presentation in IJCNN’01,
2001.

Giorgio Valentini and Thomas G. Dietterich. Low bias bagged support vector machines.
In International Conference on Machine Learning, ICML-2003, pages 752–759. Morgan
Kaufmann, 2003.

Shi-jin Wang, Avin Mathew, Yan Chen, Li-feng Xi, Lin Ma, and Jay Lee. Empirical analysis
of support vector machine ensemble classiﬁers. Expert Systems with Applications, 36(3,
Part 2):6466 – 6476, 2009.

145

Journal of Machine Learning Research 16 (2015) 617-652

Submitted 11/12; Revised 9/14; Published 3/15

Generalized Hierarchical Kernel Learning

Pratik Jawanpuria
Jagarlapudi Saketha Nath
Ganesh Ramakrishnan
Department of Computer Science and Engineering
Indian Institute of Technology Bombay
Mumbai 400076, INDIA

Editor: Francis Bach

pratik.j@cse.iitb.ac.in
saketh@cse.iitb.ac.in
ganesh@cse.iitb.ac.in

Abstract

This paper generalizes the framework of Hierarchical Kernel Learning (HKL) and illustrates
its utility in the domain of rule learning. HKL involves Multiple Kernel Learning over
a set of given base kernels assumed to be embedded on a directed acyclic graph. This
paper proposes a two-fold generalization of HKL: the ﬁrst is employing a generic (cid:96)1 /(cid:96)ρ
block-norm regularizer (ρ ∈ (1, 2]) that alleviates a key limitation of the HKL formulation.
The second is a generalization to the case of multi-class, multi-label and more generally,
multi-task applications. The main technical contribution of this work is the derivation of a
highly specialized partial dual of the proposed generalized HKL formulation and an eﬃcient
mirror descent based active set algorithm for solving it. Importantly, the generic regularizer
enables the proposed formulation to be employed in the Rule Ensemble Learning (REL)
where the goal is to construct an ensemble of conjunctive propositional rules. Experiments
on benchmark REL data sets illustrate the eﬃcacy of the proposed generalizations.
Keywords: multiple kernel learning, mixed-norm regularization, multi-task learning, rule
ensemble learning, active set method

1. Introduction

A Multiple Kernel Learning (MKL) (Lanckriet et al., 2004; Bach et al., 2004) framework for
construction of sparse linear combinations of base kernels embedded on a directed acyclic
graph (DAG) was recently proposed by Bach (2008). Since the DAG induces hierarchical
relations between the base kernels, this framework is more commonly known as Hierarchical
Kernel Learning (HKL). It has been established that HKL provides a powerful algorithm
for task speciﬁc non-linear feature selection. HKL employs a carefully designed (cid:96)1/(cid:96)2 block-
norm regularizer: (cid:96)1 -norm across some predeﬁned components associated with the DAG
and (cid:96)2 -norm within each such component. However, the sparsity pattern of kernel (feature)
selection induced by this regularizer is somewhat restricted: a kernel is selected only if the
kernels associated with al l its ancestors in the DAG are selected.
In addition, it can be
proved that the weight of the kernel associated with a (selected) node will always be greater
than the weight of the kernels associated with its descendants. Such a restricted selection
pattern and weight bias may limit the applicability of HKL in real world problems.
This paper proposes a two-fold generalization of HKL. The ﬁrst is employing a (cid:96)1/(cid:96)ρ , ρ ∈
(1, 2), block-norm regularizer that mitigates the above discussed weight and selection bias

©2015 Pratik Jawanpuria, Jagarlapudi Saketha Nath and Ganesh Ramakrishnan.

Jawanpuria, Nath and Ramakrishnan

among the kernels, henceforth termed as gHKL. Note that for the special case of ρ = 2,
gHKL renders the HKL regularizer. Further, gHKL is generalized to the paradigm of Multi-
task Learning (MTL), where multiple related tasks need to be learnt jointly. We consider
the MTL setup where the given learning tasks share a common sparse feature space (Lounici
et al., 2009; Jawanpuria and Nath, 2011; Obozinski et al., 2011). Our goal is to construct a
shared sparse feature representation that is suitable for all the given related tasks. We pose
the problem of learning this shared feature space as that of learning a shared kernel, common
across all the tasks. The proposed generalization is henceforth referred to as gHKLMT . In
addition to learning a common feature representation, gHKLMT is generic enough to model
additional correlations existing among the given tasks.
Though employing a (cid:96)1/(cid:96)ρ , ρ ∈ (1, 2), regularizer is an incremental modiﬁcation to the
HKL formulation, devising an algorithm for solving it is not straightforward. The pro jected
gradient descent employed in the active set algorithm for solving HKL (Bach, 2008) can no
longer be employed for solving gHKL as pro jections onto (cid:96)ρ -norm balls are known to be
signiﬁcantly more challenging than those onto (cid:96)1 -norm balls (Liu and Ye, 2010). Hence naive
extensions of the existing HKL algorithm will not scale well. Further, the computational
challenge is compounded with the generalization for learning multiple tasks jointly. The
key technical contribution of this work is the derivation of a highly specialized partial
dual of the gHKL/gHKLMT formulations and an eﬃcient mirror descent (Ben-Tal and
Nemirovski, 2001; Beck and Teboulle, 2003) based active set algorithm for solving it. The
dual presented here is an elegant convex optimization problem with a Lipschitz continuous
ob jective and constrained over a simplex. Moreover, the gradient of the ob jective can
be obtained by solving a known and well-studied variant of the MKL formulation. This
motivates employing the mirror descent algorithm that is known to solve such problems
eﬃciently. Further eﬃciency is brought in by employing an active set method similar in
spirit to that in Bach (2008).
A signiﬁcant portion of this paper focuses on the application of Rule Ensemble Learn-
ing (REL) (Dembczy´nski et al., 2010, 2008), where HKL has not been previously explored.
Given a set of basic propositional features describing the data, the goal in REL is to con-
struct a compact ensemble of conjunctions with the given propositional features that gen-
eralizes well for the problem at hand. Such ensembles are expected to achieve a good
trade-oﬀ between interpretability and generalization ability. REL approaches (Cohen and
Singer, 1999; Friedman and Popescu, 2008; Dembczy´nski et al., 2010) have additionally
addressed the problem of learning a compact set of rules that generalize well in order to
maintain their readability. One way to construct a compact ensemble is to consider a linear
model involving all possible conjunctions of the basic propositional features and then per-
forming a (cid:96)1 -norm regularized empirical risk minimization (Friedman and Popescu, 2008;
Dembczy´nski et al., 2010). Since this is a computationally infeasible problem, even with
moderate number of basic propositions, the existing methods either approximate such a
regularized solution using strategies such as shrinkage (Friedman and Popescu, 2008; Dem-
bczy´nski et al., 2010, 2008) or resort to post-pruning (Cohen and Singer, 1999). This work
proposes to solve a variant of this regularized empirical risk minimization problem optimally
using the framework of gHKL. The key idea is to deﬁne kernels representing every possible
conjunction and arranging them on a DAG. The proposed gHKL regularizer is applied on
this DAG of kernels, leading to a sparse combination of promising conjunctions. Note that

618

Generalized Hierarchical Kernel Learning

with such a setup, the size of the gHKL optimization problem is exponential in the number
of basic propositional features. However, a key result in the paper shows that the proposed
gHKL algorithm is guaranteed to solve this exponentially large problem with a complexity
polynomial in the ﬁnal active set1 size. Simulations on benchmark binary (and multiclass)
classiﬁcation data sets show that gHKL (and gHKLMT ) indeed constructs a compact en-
semble that on several occasions outperforms state-of-the-art REL algorithms in terms of
generalization ability. These results also illustrate the beneﬁts of the proposed generaliza-
tions over HKL: i) the ensembles constructed with gHKL (with low ρ values) involve fewer
number of rules than with HKL; though the accuracies are comparable ii) gHKLMT can
learn rule ensemble on multiclass problems; whereas HKL is limited to two-class problems.
The rest of the paper2 is organized as follows. Section 2 introduces the classical Multi-
ple Kernel Learning setup, brieﬂy reviews the HKL framework and summarizes the existing
works in Multi-task Learning. In Section 3, we present the proposed gHKL and gHKLMT
formulations. The key technical derivation of the specialized dual is also presented in this
section. The proposed mirror descent based active set algorithm for solving gHKL/gHKLMT
formulations is discussed in Section 4. In Section 5, we propose to solve the REL problem
by employing the gHKL formulation and discuss its details. In Section 6, we report em-
pirical evaluations of gHKL and gHKLMT formulations for REL on benchmark binary and
multiclass data sets respectively. Section 7 concludes the paper.

2. Related Works

This section provides a brief introduction to the Multiple Kernel Learning (MKL) frame-
work, the HKL setup and formulation (Bach, 2008, 2009) as well as the existing works in
Multi-task Learning.

2.1 Multiple Kernel Learning Framework

We begin by discussing the regularized risk minimization framework (Vapnik, 1998), which
has been employed in the proposed formulations.
Consider a learning problem like classiﬁcation or regression and let its training data be
denoted by D = {(xi , yi ), i = 1, . . . , m | xi ∈ X , yi ∈ R ∀i}, where (xi , yi ) represents the ith
input-output pair. The aim is to learn an aﬃne prediction function F (x) that generalize
well on unseen data. Given a positive deﬁnite kernel k that induces a feature map φk (·), the
prediction function can be written as: F (x) = (cid:104)f , φk (x)(cid:105)Hk − b. Here Hk is the Reproducing
Kernel Hilbert Space (RKHS) (Sch¨olkopf and Smola, 2002) associated with the kernel k ,
endowed with an inner product (cid:104)·, ·(cid:105)Hk , and f ∈ Hk , b ∈ R are the model parameters to
be learnt. A popular framework to learn these model parameters is the regularized risk
m(cid:88)
minimization (Vapnik, 1998), which considers the following problem:
i=1

min
f ∈Hk ,b∈R

(1)

1
2

Ω(f )2 + C

(cid:96)(yi , F (xi )),

1. Roughly, this is the number of selected conjunctions and is potentially far less than the total number of
conjunctions.
2. Preliminary results of this work were reported in Jawanpuria et al. (2011).

619

Jawanpuria, Nath and Ramakrishnan

where Ω(·) is a norm based regularizer, (cid:96) : R×R → R is a suitable convex loss function and C
we know that the optimal f has the following form f (·) = (cid:80)m
is a regularization parameter. As an example, the support vector machine (SVM) (Vapnik,
1998) employs Ω(f ) = (cid:107)f (cid:107)Hk . From the representer theorem (Sch¨olkopf and Smola, 2002),
i αik(·, xi ) where α = (αi )m
i=1
is a vector of coeﬃcients to be learnt.
It can be observed from above that the kernel deﬁnition plays a crucial role in deﬁning
the quality of the solution obtained by solving (1). Hence learning a kernel suitable to the
problem at hand has been an active area of research over the past few years. One way
combination of the given base kernels k1 , . . . , kl : k = (cid:80)l
to learn kernels is via the Multiple Kernel Learning (MKL) framework (Lanckriet et al.,
2004; Bach et al., 2004). Lanckriet et al. (2004) proposed to learn the kernel k as a conic
i=1 ηiki , ηi ≥ 0 ∀ i. Here η = (ηi )l
i=1
√
is a coeﬃcient vector to be (additionally) learnt in the optimization problem (1). In this
ηiφki )l
setting, the feature map with respect to the kernel k is given by φk = (
i=1 (see
Rakotomamonjy et al., 2008, for details). It is a weighted concatenation of feature maps
induced by the individual base kernels. Hence, sparse kernel weights will result in a low
dimensional φk . Some of the additional constraints on η explored in the existing MKL works
are (cid:96)1 -norm constraint (Bach et al., 2004; Rakotomamonjy et al., 2008), (cid:96)p -norm constraint
(p > 1) (Kloft et al., 2011; Vishwanathan et al., 2010; Aﬂalo et al., 2011), etc.

2.2 Hierarchical Kernel Learning

Hierarchical Kernel Learning (HKL) (Bach, 2008) is a generalization of MKL and assumes
a hierarchy over the given base kernels. The base kernels are embedded on a DAG and a
carefully designed (cid:96)1/(cid:96)2 block-norm regularization over the associated RKHS is proposed
to induce a speciﬁc sparsity pattern over the selected base kernels. We begin by discussing
its kernel setup.
Let G (V , E ) be the given DAG with V denoting the set of vertices and E denoting
the set of edges. The DAG structure entails relationships like parent, child, ancestor and
descendant (Cormen et al., 2009). Let D(v) and A(v) represent the set of descendants and
ancestors of the node v in the G . It is assumed that both D(v) and A(v) include the node
(cid:91)
v . For any subset of nodes W ⊂ V , the hull and sources of W are deﬁned as:
hull(W ) =
sources(W ) = {w ∈ W | A(w) ∩ W = {w}} .
w∈W
The size and complement of W are denoted by |W | and W c respectively. Let kv : X ×X → R
be the positive deﬁnite kernel associated with the vertex v ∈ V . In addition, let Hkv be
(cid:88)
its associated RKHS and φkv be its induced feature map. Given this, HKL employs the
following prediction function:
(cid:104)fv , φkv (x)(cid:105)Hkv
v∈V
which is an aﬃne model parameterized by f = (fv )v∈V , the tuple with entries as fv ∈ Hkv
and b ∈ R. Some more notations follow: for any subset of nodes W ⊂ V , fW = (fv )v∈W
and φW = (φv )v∈W . In general, the entries in a vector are referred to using an appropriate
subscript, i.e., entries in u ∈ Rd are denoted by u1 , . . . , ud . The kernels are denoted by the
lower case ‘k ’ and the corresponding Gram matrices are denoted by the upper case ‘K ’.

F (x) =

A(w),

− b,

620

Generalized Hierarchical Kernel Learning

+ C

(cid:96) (yi , F (xi )) ,

(2)

dv (cid:107)fD(v)(cid:107)2

HKL formulates the problem of learning the optimal prediction function F as the fol-
(cid:32)(cid:88)
(cid:33)2
m(cid:88)
lowing regularized risk minimization problem:
1
min
fv ∈Hkv ∀v∈V ,b∈R
(cid:16)(cid:80)
w∈D(v) (cid:107)fw (cid:107)2(cid:17) 1
2
v∈V
i=1
2 ∀v ∈ V , (cid:96)(·, ·) is a suitable convex loss function and
where (cid:107)fD(v)(cid:107)2 =
(dv )v∈V are given non-negative parameters.
As is clear from (2), HKL employs a (cid:96)1/(cid:96)2 block-norm regularizer, which is known
to promote group sparsity (Yuan and Lin, 2006).
Its implications are discussed in the
following. For most of v ∈ V , (cid:107)fD(v)(cid:107)2 = 0 at optimality due to the sparsity inducing
nature of the (cid:96)1 -norm. Moreover ((cid:107)fD(v)(cid:107)2 = 0) ⇒ (fw = 0 ∀w ∈ D(v)). Thus it is
expected that most fv will be zero at optimality. This implies that the prediction function
involves very few kernels. Under mild conditions on the kernels (being strictly positive),
it can be shown that this hierarchical penalization induces the following sparsity pattern:
(fw (cid:54)= 0) ⇒ (fv (cid:54)= 0 ∀v ∈ A(w)). In other words, if the prediction function employs a kernel
kw then it certainly employs al l the kernels associated with the ancestor nodes of w.
(cid:88)
m(cid:88)
Bach (2008) proposes to solve the following equivalent variational formulation:
δw (γ )−1(cid:107)fw (cid:107)2 + C
1
min
min
γ∈∆1
fv ∈Hkv ∀v∈V ,b∈R
v∈V zv ≤ 1(cid:9) and δw (γ )−1 = (cid:80)
where ∆1 = (cid:8)z ∈ R|V | | z ≥ 0, (cid:80)
2
w∈V
i=1
in the HKL is: k = (cid:80)
d2
v∈A(w)
. From the repre-
v
γv
senter theorem (Sch¨olkopf and Smola, 2002), it follows that the eﬀective kernel employed
w∈V δw (γ )kw . Since the optimization problem (3) has a (cid:96)1 -norm
constraint over γ variables, most γv at optimality are expected to be zero. Moreover the
kernel weight δw (γ ) is zero whenever γv = 0 for any v ∈ A(w). Thus, the HKL performs
a sparse selection of the base kernels and can be understood as a generalization of the
classical MKL framework. However, the sparsity pattern for the kernels has the following
restriction: if a kernel is not selected then none of the kernels associated with its descen-
dants are selected, as (γv = 0) ⇒ (δw (γ ) = 0 ∀w ∈ D(v)). For the case of strictly positive
kernels, it follows that a kernel is selected only if all the kernels associated with its ances-
tors are selected. In addition, the following relationship holds among the kernels weights:
δv (γ ) ≥ δw (γ ) ∀w ∈ D(v) (strict inequality holds if δw (γ ) > 0). Hence, the weight of the
kernel associated with a (selected) node is always be greater than the weight of the kernels
associated with its descendants.
Since the size of γ is same as that of V and since the optimal γ is known to be sparse, Bach
(2008) proposes an active set based algorithm (Lee et al., 2007) for solving (3). At each
iteration of the active set algorithm, (3) is solved with respect to only those variables in the
active set via the pro jected gradient descent technique (Rakotomamonjy et al., 2008).
As illustrated in Bach (2008), the key advantage of HKL is in performing non-linear
feature selection. For example, consider the case where the input space is X = Rn and
let I be power set of {1, . . . , n}. Consider the following 2n kernels arranged on the usual
j ∀i ∈ I . HKL can be applied in this setup to select
subset lattice: ki (x, x(cid:48) ) = Πj∈ixj x(cid:48)

(cid:96) (yi , F (xi )) ,

(3)

621

Jawanpuria, Nath and Ramakrishnan

the promising sub-products of the input features over all possible sub-products. Please
refer to Bach (2008) for more such pragmatic examples of kernels and corresponding DAGs.
The most interesting result in Bach (2008) is that in all these examples where the size of
the DAG is exponentially large, the computational complexity of the active set algorithm
is polynomial in the training set dimensions and the active set size.
Importantly, the
complexity is independent of |V |!
Though encouraging, the above discussed weight bias (in favor of the kernels towards
the top of the DAG) and restricted kernel selection pattern may limit the applicability
of HKL in real world problems. For instance, in case of the sub-product kernel example
mentioned above, the following is true: a sub-product is selected only if all the products
including it are selected. This clearly may lead to selection of many redundant sub-products
(features). In Section 3, we present the proposed generalization that provides a more ﬂexible
kernel selection pattern by employing a (cid:96)1/(cid:96)ρ , ρ ∈ (1, 2), regularizer. A key result of this
paper (refer Corollary 6) is that for all the cases discussed in Bach (2008), the proposed
mirror descent based active set algorithm for solving the generalization has a computational
complexity that is still polynomial in the training set dimensions and the active set size.
In other words, the proposed generalization does not adversely aﬀect the computational
feasibility of the problem and hence is an interesting result in itself.

2.3 Multi-task Learning

Multi-task Learning (Caruana, 1997; Baxter, 2000) focuses on learning several prediction
tasks simultaneously. This is in contrast with the usual approach of learning each task
separately and independently. The key underlying idea behind MTL is that an appropriate
sharing of information while learning related tasks will help in obtaining better prediction
models. Various deﬁnitions of task-relatedness have been explored over the past few years
like proximity of task parameters (Baxter, 2000; Evgeniou and Pontil, 2004; Xue et al., 2007;
Jacob et al., 2008; Jawanpuria and Nath, 2012) or sharing common feature space (Ando
and Zhang, 2005; Ben-David and Schuller, 2008; Argyriou et al., 2008; Lounici et al., 2009;
Obozinski et al., 2011). Many learning settings like multiclass classiﬁcation, multi-label
classiﬁcation or learning vector-valued function may be viewed as a special case of multi-
task learning.
In this work, we consider the common setting in which the task parameters share a simul-
taneously sparse structure: only a small number of input features are relevant for each of the
tasks and the set of such relevant features is common across all the tasks (Turlach et al., 2005;
Lounici et al., 2009). Existing works in this setting typically employ a group lasso penalty
(cid:16)(cid:80)T
t=1 |fti |q (cid:17) 1
on the tasks parameters: (cid:96)1/(cid:96)2 block-norm (Lounici et al., 2009; Obozinski et al., 2011) or
propose a multi-task regularizer of the form: Ω(f1 , . . . , fT ) = (cid:80)d
the (cid:96)1/(cid:96)∞ block-norm (Turlach et al., 2005; Negahban and Wainwright, 2009). Thus, they
q where
i=1
the input feature space is assumed to be d dimensional, ft is the task parameter of the
tth task and ft = (fti )i=1,...,d and q = {2, ∞}. Note that in addition to (sparse) shared
feature selection, the (cid:96)1/(cid:96)∞ block-norm penalty also promote proximity among the task
parameters.
We pose the problem of learning the shared features as that of learning a shared ker-
nel, whose induced feature space is common across all the tasks. The shared kernel is

622

Generalized Hierarchical Kernel Learning

constructed as a sparse combination of the given base kernels. A hierarchical relationship
exists over the given kernels (feature spaces). We employ a graph based (cid:96)1/(cid:96)ρ block-norm
regularization over the task parameters that enable non-linear feature selection for multiple
tasks simultaneously. The details of the proposed MTL formulation are discussed in the
following section.

3. Generalized Hierarchical Kernel Learning

In this section, we present the proposed generalizations over HKL. As discussed earlier, the
ﬁrst generalization aims at mitigating the weight bias problem as well as the restrictions
imposed on the kernel selection pattern of HKL, and is termed as gHKL. The gHKL formu-
lation is then further generalized to the paradigm of MTL, the proposed formulation being
termed as gHKLMT . We begin by introducing the gHKL formulation.

3.1 gHKL Primal Formulation

Recall that HKL employs a (cid:96)1/(cid:96)2 block norm regularizer. As we shall understand in more
detail later, a key reason for the kernel weight bias problem and the restricted sparsity
pattern in HKL is the (cid:96)2 -norm regularization. One way to mitigate these restrictions is by
(cid:88)
employing the following generic regularizer:
dv (cid:107)fD(v)(cid:107)ρ ,
ΩS (f ) =
w∈D(v) (cid:107)fw (cid:107)ρ(cid:17) 1
(cid:16)(cid:80)
v∈V
where f = (fv )v∈V , (cid:107)fD(v)(cid:107)ρ =
ρ and ρ ∈ (1, 2]. The implications of the
(cid:96)1/(cid:96)ρ block-norm regularization are discussed in the following. Since the (cid:96)1 -norm promotes
sparsity, it follows that (cid:107)fD(v)(cid:107)ρ = 0 (that is fw = 0 ∀w ∈ D(v)) for most v ∈ V . This
phenomenon is similar as in HKL. But now, even in cases where (cid:107)fD(v)(cid:107)ρ is not forced to
zero by the (cid:96)1 -norm, many components of fD(v) tend to zero3 (that is fw → 0 for many
w ∈ D(v)) as the value of ρ tends to unity. Also note that ρ = 2 renders the HKL regularizer.
m(cid:88)
To summarize, the proposed gHKL formulation is
i=1

min
fv ∈Hkv ∀v∈V ,b∈R

(5)

(4)

1
2

(ΩS (f ))2 + C

(cid:96) (yi , F (xi )) .

We next present the gHKLMT formulation, which further generalizes gHKL to MTL paradigm.

3.2 gHKLMT Primal Formulation

We begin by introducing some notations for the multi-task learning setup. Let T be the
number of tasks and let the training data for the tth task be denoted by Dt = {(xti , yti ), i =
1, . . . , m | xti ∈ X , yti ∈ R ∀i}, where (xti , yti ) represents the ith input-output pair of the
3. Note that as (cid:96)ρ -norm (ρ > 1) is diﬀerentiable, it rarely induce sparsity (Szafranski et al., 2010). However,
as ρ → 1, they promote only a few leading terms due to the high curvatures of such norms (Szafranski
et al., 2007). In order to obtain a sparse solution in such cases, thresholding is commonly employed by
existing (cid:96)p -MKL (ρ > 1) algorithms (Vishwanathan et al., 2010; Orabona et al., 2012; Jain et al., 2012;
Jawanpuria et al., 2014). We employed thresholding in our experiments.

623

Jawanpuria, Nath and Ramakrishnan

2

(6)

1
2

dv

+ C

min
ft ,bt ∀t

(cid:96)(yti , Ft (xti )),

T(cid:88)
t=1

m(cid:88)
i=1

Ft (x) = (cid:80)
tth task. For the sake of notational simplicity, it is assumed that the number of training
examples is same for all the tasks. The prediction function for the tth task is given by:
− bt , where ft = (ftv )v∈V and bt are the task parameters to
v∈V (cid:104)ftv , φkv (x)(cid:105)Hkv
be learnt. We propose the following regularized risk minimization problem for estimating


these task parameters and term it as gHKLMT :
 1
 (cid:88)
(cid:88)
ρ
(cid:124)
(cid:125)
(cid:123)(cid:122)
(Qw (f1 , . . . , fT ))ρ
v∈V
w∈D(v)
ΩT (f1 ,...,fT )
where ρ ∈ (1, 2] and Qw (f1 , . . . , fT ) is a norm-based multi-task regularizer on the task
parameters ftw ∀t .
In the following, we discuss the eﬀect of the above regularization.
Firstly, there is a (cid:96)1 -norm regularization over the group of nodes (feature spaces) and a
(cid:96)ρ -norm regularization within each group. This (cid:96)1/(cid:96)ρ block-norm regularization is same as
that of gHKL and will have the same eﬀect on the sparsity pattern of the selected feature
spaces (kernels). Hence, only a few nodes (feature spaces) will be selected by the gHKLMT
regularizer ΩT (f1 , . . . , fT ). Secondly, nature of the task relatedness within each (selected)
feature space is governed by the Qw (f1 , . . . , fT ) regularizer.
For instance, consider the following deﬁnition of Qw (f1 , . . . , fT ) (Lounici et al., 2009;
(cid:33) 1
(cid:32) T(cid:88)
Jawanpuria and Nath, 2011):
2
t=1

Qw (f1 , . . . , fT ) =

(cid:107)ftw (cid:107)2

(7)

.

The above regularizer couples the task parameters within each feature space via (cid:96)2 -norm. It
encourages the task parameters within a feature space to be either zero or non-zero across
all the tasks. Therefore, ΩT (f1 , . . . , fT ) based on (7) has the following eﬀect: i) all the tasks
will simultaneously select or reject a given feature space, and ii) overall only a few feature
spaces will be selected in the gHKL style sparsity pattern.
Several multi-task regularizations (Evgeniou and Pontil, 2004; Evgeniou et al., 2005;
Jacob et al., 2008) have been proposed to encourage proximity among the task parameters
within a given feature space. This correlation among the tasks may be enforced while
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2 1
µ
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)ftw − 1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
learning a shared sparse feature space by employing the following Qw (f1 , . . . , fT ):
T(cid:88)
T(cid:88)
T(cid:88)
2
T + µ
t=1
t=1
t=1
where µ > 0 is a given parameter. The above Qw (f1 , . . . , fT ) consists of two terms: the ﬁrst
regularizes the mean while the second regularizes the variance of the task parameters in
the feature space induced by kernel kw . The parameter µ controls the degree of proximity
among the task parameters, with lower µ encouraging higher proximity. Note that when
µ = ∞, (8) simpliﬁes to (7). The gHKLMT regularizer ΩT (f1 , . . . , fT ) based on (8) has the

Qw (f1 , . . . , fT ) =

1
T + µ

ftw

ftw

(8)

+

,

624

Generalized Hierarchical Kernel Learning

following eﬀect: i) all the tasks will simultaneously select or reject a given feature space, ii)
overall only a few feature spaces will be selected in the gHKL style sparsity pattern, and
iii) within each selected feature space, the task parameters ftw ∀t are in proximity.
Thus, gHKLMT framework provides a mechanism to learn a shared feature space across
the tasks. In addition, it can also preserve proximity among the tasks parameters in the
learnt feature space. As we shall discuss in the next section, more generic correlations
among task parameters may be also modeled within the gHKLMT framework.
It is clear that the gHKL optimization problem (5) may be viewed as a special case
of the gHKLMT optimization problem (7), with the number of tasks set to unity. Hence
the rest of the discussion regarding dual derivation and optimization focuses primarily on
gHKLMT formulation.

3.3 gHKLMT Dual Formulation

As mentioned earlier, due to the presence of the (cid:96)ρ -norm term in gHKLMT formulation,
naive extensions of the pro jected gradient based active set method in Bach (2008) will
be rendered computationally infeasible on real world data sets. Hence, we ﬁrst re-write
gHKLMT formulation in an elegant form, which can then be solved eﬃciently. To this end,
we note the following variational characterization of ΩT (f1 , . . . , fT ).
Lemma 1 Given ΩT (f1 , . . . , fT ) and Qw (f1 , . . . , fT ) as deﬁned in (6) and (8) respectively,
(cid:88)
we have:
δw (γ , λ)−1Qw (f1 , . . . , fT )2 ,
ΩT (f1 , . . . , fT )2 = min
min
(9)
γ∈∆
λv ∈∆v
ˆρ ∀v∈V
v∈V zv ≤ 1(cid:9) and
, ∆1 = (cid:8)z ∈ R|V | | z ≥ 0, (cid:80)
2−ρ , δw (γ , λ)−1 = (cid:80)
w∈V
(cid:110)
(cid:111)
z ∈ R|D(v)| | z ≥ 0, (cid:80)
d2
where ˆρ = ρ
v∈A(w)
v
γv λvw
w ≤ 1
w∈D(v) zr
∆v
.
r =
Note that ρ ∈ (1, 2) ⇒ ˆρ ∈ (1, ∞). The proof of the above lemma is provided in Ap-
pendix A.2.
In order to keep the notations simple, in the remainder of this section, it is assumed
that the learning tasks at hand are binary classiﬁcation, i.e., yti ∈ {−1, 1} ∀t, i, and the
loss function is the hinge loss. However, one can easily extend these ideas to other loss
functions and learning problems. Refer Appendix A.8 for gHKLMT dual formulation with
general convex loss functions.

Lemma 2 Consider problem (6) with the regularizer term replaced with its variational char-
acterization (9) and the loss function as the hinge loss (cid:96)(y , Ft (x)) = max (0, 1 − yFt (x)).
Then the fol lowing is a partial dual of it with respect to the variables ft , bt ∀t = 1, . . . , T :
max
αt∈S (yt ,C )∀t
(cid:32)(cid:88)
w∈V

G(γ , λ, α) = 1(cid:62)α − 1
2

G(γ , λ, α),
(cid:33)

δw (γ , λ)Hw

Yα,

min
γ∈∆1

min
λv ∈∆v
ˆρ ∀v∈V

(10)

where

α(cid:62)Y

625

Jawanpuria, Nath and Ramakrishnan
T ](cid:62) , S (yt , C ) = {β ∈ Rm | 0 ≤ β ≤ C, (cid:80)m
i=1 ytiβi = 0}, yt = [yt1 , . . . , ytm ](cid:62) ,
α = [α(cid:62)
1 , . . . , α(cid:62)
, ∆1 = (cid:8)z ∈ R|V | | z ≥ 0, (cid:80)
v∈V zv ≤ 1(cid:9),
with entries as unity, δw (γ , λ)−1 = (cid:80)
T ](cid:62) , 1 is a mT × 1 vector
Y is the diagonal matrix corresponding to the vector [y(cid:62)
1 , . . . , y(cid:62)
(cid:110)
(cid:111)
z ∈ R|D(v)| | z ≥ 0, (cid:80)
d2
v∈A(w)
v
γv λvw
w ≤ 1
2−ρ , and Hw ∈ RmT ×mT is the multi-task
, ˆρ = ρ
∆v
w∈D(v) zr
r =
kernel matrix corresponding to the multi-task kernel hw ∀w ∈ V . The kernel function hw is
deﬁned as fol lows:

hw (xt1 i , xt2 j ) = kw (xt1 i , xt2 j )B (t1 , t2 ),
(11)
where B is a T × T matrix. B = I (identity matrix) when the multi-task regularizer (7)
is employed in (6). Alternatively, B = I + 11(cid:62)/µ (here 1 is a T × 1 vector with entries
as unity) in the case when the regularizer (8) is employed. The prediction function for the
(cid:33)
(cid:32)(cid:88)
task t1 is given by
m(cid:88)
T(cid:88)
δw (¯γ , ¯λ)kw (xt1 i , xt2 j )B (t1 , t2 )
w∈V
t2=1
i=1

Ft1 (xt1 j ) =

¯αt2 iyt2 i

,

where (¯γ , ¯λ, ¯α) is an optimal solution of (10).

Proof The proof follows from the representer theorem (Sch¨olkopf and Smola, 2002). Also
SVM with the eﬀective multi-task kernel as: h = (cid:80)
refer to Appendix A.3.
of the gHKL, the eﬀective kernel is k = (cid:80)
This lemma shows that gHKLMT essentially constructs the same prediction function as an
w∈V δw (γ , λ)hw . Similarly, in the case
w∈V δw (γ , λ)kw (since the terms T and B are
unity). Here, as well as in the rest of the paper, we employ the symbols ‘h’ and ‘H ’ for the
multi-task kernel and the corresponding Gram matrix respectively.
The multi-task kernel (11) consists of two terms: the ﬁrst term corresponds to the
similarity between two instances xt1 i and xt2 j in the feature space induced by the kernel
kw . The second term corresponds to the correlation between the tasks t1 and t2 . In the case
of the regularizer (7), the matrix B simpliﬁes to: B (t1 , t2 ) = 1 if t1 = t2 and B (t1 , t2 ) = 0
if t1 (cid:54)= t2 , thereby making the kernel matrices Hw (w ∈ V ) block diagonal. Hence, the
gHKLMT regularizer based on (7) promotes simultaneous sparsity in kernel selection among
the tasks, without enforcing any additional correlations among the tasks.
In general, any T × T positive semi-deﬁnite matrix may be employed as B to model
generic correlations among tasks. The multi-task kernel given by (11) will still remain a
valid kernel (Sheldon, 2008; ´Alvarez et al., 2012). The matrix B is sometimes referred to as
the output kernel in the setting of learning vector-valued functions. It is usually constructed
from the prior domain knowledge.
We now discuss the nature of the optimal solution of (10). Most of the kernel weights
δw (γ , λ) are zero at optimality of (10): δw (γ , λ) = 0 whenever γv = 0 or λvw = 0 for
any v ∈ A(w). The vector γ is sparse due to (cid:96)1 -norm constraint in (10).
In addition,
ρ → 1 ⇒ ˆρ → 1. Hence the vectors λv ∀v ∈ V get close to becoming sparse as ρ → 1 due
to the (cid:96) ˆρ -norm constraint in (10). The superimposition of these two phenomena leads to a

626

Generalized Hierarchical Kernel Learning

ﬂexible4 sparsity pattern in kernel selection. This is explained in detail towards the end of
this section.
Note that ρ = 2 ⇒ λvw = 1 ∀v ∈ A(w), w ∈ W at optimality in (10). Hence for
ρ = 2, the minimization problem in (10) can be eﬃciently solved using a pro jected gradient
method (Rakotomamonjy et al., 2008; Bach, 2009). However, as established in Liu and Ye
(2010), pro jection onto the kind of feasibility set in the minimization problem in (10) is
computationally challenging for ρ ∈ (1, 2). Hence, we wish to re-write this problem in a
relatively simpler form that can be solved eﬃciently. To this end, we present the following
important theorem.

g(η),

(12)

ζw (η)

,

(13)

α(cid:62)YHwYα

Theorem 3 The fol lowing is a dual of (6) considered with the hinge loss function, and the
objectives of (6) (with the hinge loss), (10) and (12) are equal at optimality:
min
η∈∆1
(cid:33) 1
(cid:32)(cid:88)
(cid:17) ¯ρ
(cid:16)
where g(η) is the optimal objective value of the fol lowing convex problem:
¯ρ
1(cid:62)α − 1
max
(cid:17) 1
(cid:16)(cid:80)
αt∈S (yt ,C )∀t
2
w∈V
β ≤ C, (cid:80)m
v η1−ρ
T ](cid:62) , S (yt , C ) = {β ∈ Rm | 0 ≤
1 , . . . , α(cid:62)
1−ρ , α = [α(cid:62)
v∈A(w) dρ
where ζw (η) =
v
i=1 ytiβi = 0}, yt = [yt1 , . . . , ytm ](cid:62) , Y is the diagonal matrix corresponding to
∆1 = (cid:8)z ∈ R|V | | z ≥ 0, (cid:80)
v∈V zv ≤ 1(cid:9), and Hw ∈ RmT ×mT is the multi-task kernel matrix
T ](cid:62) , 1 is a mT × 1 vector with entries as unity, ¯ρ = ˆρ
1 , . . . , y(cid:62)
the vector [y(cid:62)
ˆρ−1 , ˆρ = ρ
2−ρ ,
corresponding to the multi-task kernel (11).
The key idea in the proof of the above theorem is to eliminate the λ variables and the details
are presented in Appendix A.5. The expression for the prediction function F , in terms of
the variables η and α, is provided in Appendix A.9.
This theorem provides some key insights: ﬁrstly, we have that (12) is essentially a (cid:96)1 -
norm regularized problem and hence it is expected that most η will be zero at optimality.
Since (ηv = 0) ⇒ (ζw (η) = 0 ∀w ∈ D(v)), it follows that most nodes in V will not
contribute in the optimization problems (12) and (13). Secondly, in a single task learning
problem (13) essentially learns an eﬀective kernel of the form h = (cid:80)
setting (T = 1), the problem in (13) is equivalent to the (cid:96) ˆρ -norm MKL dual problem (Kloft
¯ρ kv ∀v ∈ V (cid:51) ζv (η) (cid:54)= 0. The optimization
1
et al., 2011) with the base kernels as (ζv (η))
1
v∈V θv (ζv (η))
¯ρ hv ,
where the θ are intermediate optimization variables constrained to be non-negative and lie
within a (cid:96) ˆρ -norm ball. The expression for θ in terms of the variables η and α is provided in
Appendix A.9.
The variable θ inﬂuence the nature of the eﬀective kernel h in two important ways: i)
(cid:17) 1
(cid:16)
it follows from the expression of θ that
¯ρ ∝ ζv (η)
( ˆρ−1) .
1

α(cid:62)YHvYα

θv (ζv (η))

4. The HKL dual formulation (Bach, 2009) is a special case of (10) with ρ = 2, T = 1 and B = 1. When
ρ = 2, ˆρ = ∞. This implies λvw = 1 ∀ v ∈ A(w), w ∈ V at optimality, resulting in the weight bias
towards kernels embedded in the ancestor nodes and restricted sparsity pattern in kernel selection

627

Jawanpuria, Nath and Ramakrishnan

Algorithm 1 Active Set Algorithm - Outline
Input: Training data D, the kernels (kv ) embedded on the DAG (V ), the T × T matrix
B that models task correlations and tolerance .
Initialize the active set W with sources(V ).
Compute η , α by solving (14)
while Optimal solution for (12) is NOT obtained do
Add some nodes to W
Recompute η , α by solving (14)
end while
Output: W , η , α

The above relation implies that the weight of the kernel hv in the DAG V is not only
dependent on the position5 of the node v , but also on the suitability of the kernel hv to
the problem at hand. This helps in mitigating the kernel weight bias in favour of the nodes
towards the top of the DAG from gHKLMT , but which is present in HKL, and ii) as ρ → 1
(and hence as ˆρ → 1), the optimal θ get close to becoming sparse (Szafranski et al., 2007;
Orabona et al., 2012). This superimposed with the sparsity of η promotes a more ﬂexible
sparsity pattern in kernel selection that HKL, especially when ρ → 1.
Next, we propose to solve the problem (12) by exploiting the sparsity pattern of the
η variables and the corresponding ζ (η) terms at optimality. We discuss it in detail in the
following section.

4. Optimization Algorithm

Note that problem (12) remains the same whether solved with the original set of variables
(η) or when solved with only those ηv (cid:54)= 0 at optimality (refer Appendix A.4 for details).
However the computational eﬀort required in the latter case can be signiﬁcantly lower since
it involves low number of variables and kernels. This motivates us to explore an active set
algorithm, which is similar in spirit to that in Bach (2008).
An outline of the proposed active set algorithm is presented in Algorithm 1. The algo-
rithm starts with an initial guess for the set W such that ηw (cid:54)= 0 (∀w ∈ W ) at the optimality
of (12). This set W is called the active set. Since the weight associated with the kernel hw
will be zero whenever ηv = 0 for any v ∈ A(w), the active set W must contain sources(V ),
else the problem has a trivial solution. Hence, the active set is initialized with sources(V ).
At each iteration of the algorithm, (12) is solved with variables restricted to those in W :
(cid:33) 1
(cid:32) (cid:88)
(cid:17) ¯ρ
(cid:16)
¯ρ
w∈W

α(cid:62)YHwYα

1(cid:62)α − 1
2

min
η∈∆1

max
αt∈S (yt ,C )∀t

ζw (η)

.

(14)

i) an eﬃcient algorithm for
In order to formalize the active set algorithm, we need:
solving problem (14), ii) a condition for verifying whether a candidate solution is optimal
5. Similar to the δv function in HKL (3), it follows from the deﬁnition of ζv that ζv (η) ≥ ζw (η) ∀w ∈ D(v)
(strict inequality holds if ζw (η) > 0).

628

Generalized Hierarchical Kernel Learning

Algorithm 2 Mirror Descent Algorithm for solving (14)
Input: Gram matrices Hw (w ∈ W ) and the regularization parameter C
Initialize ηW (w ∈ W ) such that ηW ∈ ∆1 (warm-start may be used)
Iteration number: i = 0
while convergence criterion is not met6 do
i = i + 1
Compute ζw (ηW ) ∀w ∈ W (Theorem 3)
Compute step size s = (cid:112)log(|W |)/i·(cid:107)∇g(ηW ))(cid:107)2∞
Compute αW (13) using (cid:96) ˆρ -norm MKL algorithm with kernels as
Compute ∇g(ηW ) as in (24)
Compute ηw = exp (1 + log(ηw ) − s · ∇g(ηW )w ) ∀w ∈ W
ηw(cid:80)
∀w ∈ W
Normalize ηw =
v∈W ηv
end while
Output: ηW , αW

(cid:16)

(ζw (ηW ))

(cid:17)

1
¯ρ Hw

w∈W

with respect to the optimization problem (12), and iii) a procedure for building/improving
the active set after each iteration.
We begin with the ﬁrst. We propose to solve the optimization problem (14) using the
mirror descent algorithm (Ben-Tal and Nemirovski, 2001; Beck and Teboulle, 2003). Mirror
descent algorithm is known to eﬃciently solve convex programs with Lipschitz continuous
and diﬀerentiable ob jectives constrained over a convex compact set.
It achieves a near-
optimal convergence rate whenever the feasibility set is a simplex (which is true in our
optimization problem (14)). Mirror descent is close in spirit to the pro jected gradient
descent algorithm and hence assumes that an oracle for computing the gradient of the
ob jective is available.
Following the common practice of smoothing (Bach, 2009), in the rest of the paper, we
employ ζw ((1 − ε)η + ε|V | ) instead7 of ζw (η) in (13) with ε > 0. The following theorem
establishes the applicability of mirror descent for solving (14):

Theorem 4 The function g(η) given by (13) is convex. Also, the expression for the ith
entry in the gradient (∇g(η))i is given in (24). If al l the eigenvalues of the Gram matrices
Hw are ﬁnite and non-zero, then g is Lipschitz continuous.

The proof of the above theorem is technical and is provided in Appendix A.6.
Algorithm 2 summarizes the proposed mirror descent based algorithm for solving (14).
One of its steps involve computing ∇g(ηW ) (expression provided in (24)), which in turn
requires solving (13). As noted before, (13) is similar to the (cid:96) ˆρ -norm MKL problem (Kloft
et al., 2011) but with a diﬀerent feasibility set for the optimization variables α. Hence,
(13) can be solved by employing a modiﬁed cutting planes algorithm (Kloft et al., 2011) or
a modiﬁed sequential minimal optimization (SMO) algorithm (Platt, 1999; Vishwanathan

6. Relative ob jective gap between two successive iteration being less than a given tolerance  is taken to be
the convergence criterion. Ob jective here is the value of g(ηW ), calculated after (cid:96) ˆρ -norm MKL step.
7. Note that this is equivalent to smoothing the regularizer ΩT while preserving its sparsity inducing
properties (Bach, 2009).

629

Jawanpuria, Nath and Ramakrishnan

Algorithm 3 Active Set Algorithm
Input: Training data D, the kernels (kv ) embedded on the DAG (V ), the T × T matrix
B that models task correlations and tolerance .
Initialize the active set W with sources(V )
Compute η , α by solving (14) using Algorithm 2
while suﬃcient condition for optimality (15) is not met do
Add those nodes to W that violate (15)
Recompute η , α by solving (14) using Algorithm 2
end while
Output: W , η , α

ζw (ηW )

α(cid:62)
W YHwYαW

(15)

w∈D(u)

+ 2( − W ),

et al., 2010). Empirically, we observed the SMO based algorithm to be much faster than
the cutting planes algorithm for gHKLMT (and gHKL) with SVM loss functions. In the
special case of ρ = 2, T = 1 and B = 1, (13) is simply a regular SVM problem.
Now we turn our attention to the second requirement of the active set algorithm: a
condition to verify the optimality of a candidate solution. We present the following theorem
that provides a suﬃcient condition for verifying optimality of a candidate solution.
Theorem 5 Suppose the active set W is such that W = hull(W ). Let (ηW , αW ) be a W -
approximate optimal solution of (14), obtained from Algorithm (2). Then, it is an optimal
(cid:33) 1
(cid:32) (cid:88)
(cid:17) ¯ρ
(cid:16)
solution for (12) with a duality gap less than  if the fol lowing condition holds:
¯ρ
w∈W

W YKuYαW ≤
α(cid:62)
max
where Ku = (cid:80)
u∈sources(W c )
((cid:80)
Hw
v∈A(w)∩D(u) dv )2 .
The proof is provided in Appendix A.7. It closely follows that for the case of HKL (Bach,
2008).
The summary of the proposed mirror descent based active set algorithm is presented in
Algorithm 3. At each iteration, Algorithm (3) veriﬁes optimality of the current iterate by
verifying the condition in (15). In case the current iterate does not satisfy this condition, the
nodes in sources(W c ) that violate the condition (15) are included in the active set.8 This
takes care of the third requirement of the active set algorithm. The algorithm terminates
if the condition (15) is satisﬁed by the iterate.
In the following, an estimate of the computational complexity of the active set algorithm
is presented. Let W be the ﬁnal active set size. The optimization problem (14) needs to
be solved at most W times, assuming the worst case scenario of adding one node per
active set iteration. Each run of the mirror descent algorithm requires at most O(log(W ))
iterations (Ben-Tal and Nemirovski, 2001; Beck and Teboulle, 2003). A conservative time
complexity estimate for computing the gradient ∇g(ηW ) by solving a variant of the (cid:96) ˆρ -
norm MKL problem (13) is O(m3T 3W 2 ). This amounts to O(m3T 3W 3 log(W )). As for
the computational cost of the suﬃcient condition, let z denote the maximum out-degree
8. It is easy to see that with this update scheme, W is always equal to hull(W ), as required in Theorem 5.

630

Generalized Hierarchical Kernel Learning

of a node in G , i.e., z is an upper-bound on the the maximum number of children of any
node in G . Then the size of sources(W c ) is upper-bounded by W z . Hence, a total of
O(ωm2T 2W z ) operations are required for evaluating the matrices K in (15), where ω is the
complexity of computing a single entry in any K. In all the pragmatic examples of kernels
and the corresponding DAGs provided by Bach (2008), ω is polynomial in the training
set dimensions. Moreover, caching of K usually renders ω to be a constant (Bach, 2009).
Further, the total cost of the quadratic computation in (15) is O(m2T 2W 2z ). Thus the
overall computational complexity is O(m3T 3W 3 log(W ) + ωm2T 2W z + m2T 2W 2z ). More
importantly, because the suﬃcient condition for optimality (Theorem 5) is independent of
ρ, we have the following result:

Corollary 6 In a given input setting, HKL algorithm converges in time polynomial in the
size of the active set and the training set dimensions if and only if the proposed mirror
descent based active set algorithm (i.e., gHKLMT algorithm) has a polynomial time conver-
gence in terms of the active set and training set sizes.

The proof is provided in Appendix A.10.
In the next section, we present an application of the proposed formulation that illustrate
the beneﬁts of the proposed generalizations over HKL.

5. Rule Ensemble Learning

In this section, we propose a solution to the problem of learning an ensemble of decision rules,
formally known as Rule Ensemble Learning (REL) (Cohen and Singer, 1999), employing
the gHKL and gHKLMT formulations. For the sake of simplicity, we only discuss the single
task REL setting in this section, i.e., REL as an application of gHKL. Similar ideas can
be applied to perform REL in multi-task learning setting, by employing gHKLMT . In fact,
we present empirical results of REL in both single and multiple task learning settings in
Section 6. We begin with a brief introduction to REL.
If-then decision rules (Rivest, 1987) are one of the most expressive and human readable
representations for learned hypotheses. It is a simple logical pattern of the form: IF condi-
tion THEN decision. The condition consists of a conjunction of a small number of simple
boolean statements (propositions) concerning the values of the individual input variables
while the decision speciﬁes a value of the function being learned. An instance of a decision
rule from Quinlan’s play-tennis example (Quinlan, 1986) is:

IF HUMIDITY==normal AND WIND==weak THEN PlayTennis==yes.

The dominant paradigm for induction of rule sets, in the form of decision list (DL) models
for classiﬁcation (Rivest, 1987; Michalski, 1983; Clark and Niblett, 1989), has been a greedy
sequential covering procedure.
REL is a general approach that treats decision rules as base classiﬁers in an ensemble.
This is in contrast to the more restrictive decision list models that are disjunctive sets of
rules and use only one in the set for each prediction. As pointed out in Cohen and Singer
(1999), boosted rule ensembles are in fact simpler, better-understood formally than other
state-of-the-art rule learners and also produce comparable predictive accuracy.

631

Jawanpuria, Nath and Ramakrishnan

REL approaches like SLIPPER (Cohen and Singer, 1999), LRI (Weiss and Indurkhya,
2000), RuleFit (Friedman and Popescu, 2008), ENDER/MLRules (Dembczy´nski et al.,
2008, 2010) have additionally addressed the problem of learning a compact set of rules that
generalize well in order to maintain their readability. Further, a number of rule learners like
RuleFit, LRI encourage shorter rules (i.e., fewer conjunctions in the condition part of the
rule) or rules with a restricted number of conjunctions, again for purposes of interpretability.
We build upon this and deﬁne our REL problem as that of learning a small set of simple
rules and their weights that leads to a good generalization over new and unseen data. The
next section introduces the notations and the setup in context of REL.

5.1 Notations and Setup
Let D = {(x1 , y1 ), . . . , (xm , ym )} be the training data described using p basic (boolean)
propositions, i.e., xi ∈ {0, 1}p . In case the input features are not boolean, such propositions
can be derived using logical operators such as ==, (cid:54)=, ≤ or ≥ over the input features (refer
Friedman and Popescu, 2008; Dembczy´nski et al., 2008, for details). Let V be an index-
set for all possible conjunctions with the p basic propositions and let φv : Rn (cid:55)→ {0, 1}
denote the v th conjunction in V . Let fv ∈ R denote the weight for the conjunction φv .
F (x) = (cid:80)
Then, the rule ensemble to be learnt is the weighted combination of these conjunctive rules:
v∈V fv φv (x) − b, where perhaps many weights (fv ) are equal to zero.
One way to learn the weights is by performing a (cid:96)1 -norm regularized risk minimization in
order to select few promising conjunctive rules (Friedman and Popescu, 2008; Dembczy´nski
et al., 2008, 2010). However, to the best of our knowledge, rule ensemble learners that iden-
tify the need for sparse f , either approximate such a regularized solution using strategies
such as shrinkage (Ruleﬁt, ENDER/MLRules) or resort to post-pruning (SLIPPER). This
is because the size of the minimization problem is exponential in the number of basic propo-
sitions and hence the problem becomes computationally intractable with even moderately
sized data sets. Secondly, conjunctive rules involving large number of propositions might
be selected. However, such conjunctions adversely eﬀect the interpretability. We present
an approach based on the gHKL framework that addresses these issues.
We begin by noting that (cid:104)V , ⊆(cid:105) is a subset-lattice; hereafter this will be referred to
as the conjunction lattice. In a conjunction lattice, ∀ v1 , v2 ∈ V , v1 ⊆ v2 if and only if
the set of propositions in conjunction v1 is a subset of those in conjunction v2 . As an
example, (HUMIDITY==normal) is considered to be a subset of (HUMIDITY==normal
AND WIND==weak). The top node of this lattice is a node with no conjunctions and is
also sources(V ). Its children, the second level nodes, are all the basic propositions, p in
propositions. The number of diﬀerent conjunctions of length r is (cid:0)p
(cid:1) and the total number
number. The third level nodes, children of these basic propositions, are the conjunctions
of length two and so on. The bottom node at (p + 1)th level is the conjunction of all basic
r
of nodes in this conjunction lattice is 2p . Figure (1) shows a complete conjunction lattice
with p = 4.

We now discuss how the proposed gHKL regularizer (5) provides an eﬃcient and optimal
solution to a regularized empirical risk minimization formulation for REL.

632

Generalized Hierarchical Kernel Learning

Figure 1: Example of a conjunction lattice with 4 basic propositions: (x1 = a), (x2 (cid:54)= b),
(x3 ≥ c) and (x4 ≤ d). The input space consist of four features: x1 , x2 , x3 and
x4 . The number of nodes in conjunction lattice is exponential in the number of
basic propositions. In this particular example, the number of nodes is 16 (= 24 ).

5.2 Rule Ensemble Learning with gHKL
with such a setup, the (cid:96)1/(cid:96)ρ block-norm regularizer in gHKL (ΩS (f ) = (cid:80)
The key idea is to employ gHKL formulation (5) with the DAG as the conjunction lattice
and the kernels as kv (xi , xj ) = φv (xi )φv (xj ) for learning an ensemble of rules. Note that
v∈V dv (cid:107)fD(v)(cid:107)ρ )
implies: 1) for most v ∈ V , fv = 0, and 2) for most v ∈ V , fw = 0 ∀ w ∈ D(v). In the context
of the REL problem, the former statement is equivalent to saying: selection of a compact
set of conjunctions is promoted, while the second reads as: selection of conjunctive rules
with small number of propositions is encouraged. Thus, gHKL formulation constructs a
compact ensemble of simple conjunctive rules. In addition, we set dv = a|Sv | (a > 1), where
Sv is the set of basic propositions involved in the conjunction φv . Such a choice further
encourages selection of short conjunctions and leads to the following elegant computational
result:

Theorem 7 The complexity of the proposed gHKL algorithm in solving the REL problem,
with the DAG, the base kernels and the parameters dv as deﬁned above, is polynomial in the
size of the active set and the training set dimensions. In particular, if the ﬁnal active set
size is W , then its complexity is given by O(m3W 3 log(W ) + m2W 2p).

The proof is provided in Appendix A.11.
We end this section by noting the advantage of the generic regularizer in gHKL formu-
lation over the that in HKL formulation in the context of REL application. Recall that

633

{ }x1 = ax2 ≠ bx3 ≥ cx4 ≤ dx1 = aΛx2 ≠ bx1=a Λ x2≠b Λ x3≥cx1=a Λ x2≠b Λ x4≤dx1=a Λ x3≥c Λ x4≤dx2≠b Λ x3≥c Λ x ≤dx1=a Λ x2≠b Λ x3≥c Λ x4≤dx1 = aΛx3 ≥ cx1 = aΛx4 ≤ dx2 ≠ bΛx3 ≥ cx2 ≠ bΛx4 ≤ dx3 ≥ cΛx4 ≤ dJawanpuria, Nath and Ramakrishnan

the sparsity pattern allowed by HKL has the following consequence: a conjunction is se-
lected only after selecting all the conjunctions which are subsets of it. This, particularly
in the context of REL, is psycho-visually redundant, because a rule with k propositional
statements, if included in the result, will necessarily entail the inclusion of (2k − 1) more gen-
eral rules in the result. This violates the important requirement for a small set (Friedman
and Popescu, 2008; Dembczy´nski et al., 2008, 2010) of human-readable rules. The gHKL
regularizer, with ρ ∈ (1, 2), alleviates this restriction by promoting additional sparsity in
selecting the conjunctions. We empirically evaluate the proposed gHKL based solution for
REL application in the next section.

6. Experimental Results

In this section, we report the results of simulation in REL on several benchmark binary and
multiclass classiﬁcation data sets from the UCI repository (Blake and Lichman, 2013). The
goal is to compare various rule ensemble learners on the basis of: (a) generalization, which
is measured by the predictive performance on unseen test data, and (b) ability to provide
compact set of simple rules to facilitate their readability and interpretability (Friedman and
Popescu, 2008; Dembczy´nski et al., 2010; Cohen and Singer, 1999). The latter is judged
using i) average number of rules learnt, and ii) average number of propositions per rule.
The following REL approaches were compared.
• RuleFit: Rule ensemble learning algorithm proposed by Friedman and Popescu
(2008). All the parameters were set to the default values mentioned by the authors. In
particular, the model was set in the mixed linear-rule mode, average tree size was set 4
and maximum number of trees were kept as 500. The same conﬁguration was also used
by Dembczy´nski et al. (2008, 2010) in their simulations. This REL system cannot han-
dle multi-class data sets and hence is limited to the simulations on binary classiﬁcation
data sets. Its code is available at www-stat.stanford.edu/~jhf/R-RuleFit.html.
• SLI: The SLIPPER algorithm proposed by Cohen and Singer (1999). Following Dem-
bczy´nski et al. (2008, 2010), all parameters were set to their defaults. We retained
the internal cross-validation for selecting the optimal number of rules.
• ENDER: State-of-the-art rule ensemble learning algorithm (Dembczy´nski et al.,
2010). For classiﬁcation setting, ENDER is same as MLRules (Dembczy´nski et al.,
2008). The parameters were set to the default values suggested by the authors. The
second order heuristic was used for minimization. Its code is available at www.cs.
put.poznan.pl/wkotlowski.
• HKL-(cid:96)1 -MKL: A two-stage rule ensemble learning approach. In the ﬁrst stage, HKL
is employed to prune the exponentially large search space of all possible conjunctive
rules and select a set of candidate rules (kernels). The rule ensemble is learnt by
employing (cid:96)1 -MKL over the candidate set of rules. In both the stages, a three-fold
cross validation procedure was employed to tune the C parameter with values in
{10−3 , 10−2 , . . . , 103}.
• gHKLρ : The proposed gHKL based REL formulation for binary classiﬁcation prob-
lem. We considered three diﬀerent values of ρ: 2, 1.5 and 1.1. Note that for binary

634

Generalized Hierarchical Kernel Learning

classiﬁcation, ρ = 2 renders the HKL formulation (Bach, 2008). In each case, a three-
fold cross validation procedure was employed to tune the C parameter with values in
{10−3 , 10−2 , . . . , 103}. As mentioned earlier, the parameters dv = 2|v | .
• gHKLMT−ρ : The proposed gHKLMT based REL formulation for multiclass classi-
ﬁcation problem. For each class, a one-vs-rest binary classiﬁcation task is created.
Since we did not have any prior knowledge about the correlation among the classes
in the data sets, we employed the multi-task regularizer (7) in the gHKLMT primal
formulation (6).

We considered three diﬀerent values of ρ: 2, 1.5 and 1.1. Its parameters and cross validation
details are same as that of gHKLρ . The implementations of both gHKLρ and gHKLMT−ρ are
available at http://www.cse.iitb.ac.in/~pratik.j/ghkl.

Note that the above methods diﬀer in the way they control the number of rules (M ) in
the ensemble. In the case of gHKLρ (gHKLMT−ρ ), M implicitly depends on the parameters: ρ,
C and dv . SLI has a parameter for maximum number of rules Mmax and M is decided via a
internal cross-validation such that M ≤ Mmax . For the sake of fairness in comparison with
gHKLρ , we set Mmax = max(M1.5 , M1.1 ), where Mρ is the average number of rules obtained
with gHKLρ (gHKLMT−ρ ). ENDER has an explicit parameter for the number of rules, which is
also set to max(M1.5 , M1.1 ). In case of RuleFit, the number of rules in the ensemble is
determined internally and is not changed by us.

6.1 Binary Classiﬁcation in REL

This section summarizes our results on binary REL classiﬁcation. Table 1 provides the
details of the binary classiﬁcation data sets. For every data set, we created 10 random
train-test splits with 10% train data (except for MONK-3 data set, whose train-test split
of 122 − 432 instances respectively was already given in the UCI repository). Since many
data sets were highly unbalanced, we report the average F1-score along with the standard
deviation (Table 5 in Appendix A.12 reports the average AUC). The results are presented in
Table 2. The best result, in terms of the average F1-score, for each data set is highlighted.

Data set
TIC-TAC-TOE
B-CANCER-W
DIABETES
HABERMAN
HEARTC
BLOOD TRANS

(TIC)
(BCW)
(DIA)
(HAB)
(HTC)
(BLD)

Num Bias
1.89
958
0.53
699
0.54
768
0.36
306
296
0.85
3.20
748

|V |
p
54 ≈ 1016
72 ≈ 1021
64 ≈ 1019
40 ≈ 1012
78 ≈ 1023
32 ≈ 109

Data set
(HTS)
HEARTSTAT
(MK3)
MONK-3
(VTE)
VOTE
(BCC)
B-CANCER
(MAM)
MAM. MASS
(LIV)
LIVER

Num Bias
0.8
270
1.08
554
0.87
232
0.41
277
829
0.94
1.38
345

|V |
p
76 ≈ 1022
30 ≈ 109
32 ≈ 109
76 ≈ 1022
46 ≈ 1013
48 ≈ 1014

Table 1: Data sets used for binary REL classiﬁcation. ‘Num’ is the number of instances in
the data set while ‘Bias’ denotes the ratio of # of +ve and −ve instances. The
number of number of basic propositions is ‘p’ and |V | represents the total number
of possible conjunctions. For each numerical input feature, 8 basic propositions
were derived. The letters in brackets are the acronym used for the corresponding
data set in Table 2.

635

Jawanpuria, Nath and Ramakrishnan

RuleFit

SLI

TIC 0.517 ± 0.092
0.665 ± 0.053
(57.7, 2.74)
(10.3, 1.96)
BCW 0.879 ± 0.025 0.928 ± 0.018
(4.4, 1.15)
(17.5, 2.03)
0.659 ± 0.027
DIA 0.428 ± 0.052
(4.9, 1.42)
(32.9, 2.66)
0.483 ± 0.057
HAB 0.175 ± 0.079
(2.1, 1)
(7.5, 1)
0.727 ± 0.05
HTC 0.581 ± 0.047
(3.2, 1.23)
(8.8, 1)
BLD 0.163 ± 0.088
0.476 ± 0.057
(2.0, 1)
(40.7, 2.26)
0.721 ± 0.065
HTS 0.582 ± 0.040
(9.3, 1)
(3.5, 1.07)

ENDER

HKL-
gHKLρ
(cid:96)1 -MKL
ρ = 2
ρ = 1.5
ρ = 1.1
0.668 ± 0.032
0.749 ± 0.040
0.889 ± 0.093
0.897 ± 0.093 0.905 ± 0.096∗
(187, 3.17)
(74.8, 1.89)
(161.7, 1.72)
(157.6, 1.72)
(186.6, 1.76)
0.900 ± 0.041
0.925 ± 0.032
0.923 ± 0.032
0.925 ± 0.032
0.924 ± 0.032
(21, 1.56)
(27,1.03)
(20.4, 1.02)
(30.9, 1)
(20, 1.03)
0.656 ± 0.027
0.658 ± 0.028
0.661 ± 0.023
0.661 ± 0.018 0.663 ± 0.017
(74.0, 2.65)
(62.6, 1.27)
(73.2, 1.17)
(47.6, 1.40)
(83.2, 1.31)
0.474 ± 0.057
0.521 ± 0.060
0.521 ± 0.060
0.506 ± 0.048 0.523 ± 0.062
(17.1, 1.142)
(51.2, 1.235)
(112.1, 1.366)
(52, 3.59)
(45.6, 1.48)
0.736 ± 0.055
0.735 ± 0.058
0.743 ± 0.038
0.724 ± 0.032 0.750 ± 0.038
(23.9, 1)
(46.7, 1.06)
(32.9, 1.09)
(32, 2.05)
(32, 1.09)
0.586 ± 0.029
0.433 ± 0
0.572 ± 0.029
0.587 ± 0.028 0.588 ± 0.027
(19, 1.29)
(62.8, 1.79)
(229.7, 1.98)
(63, 1.97)
(175.9, 2.13)
0.747 ± 0.028
0.746 ± 0.028
0.747 ± 0.031
0.713 ± 0.055 0.752 ± 0.036
(25, 2.02)
(24.6, 1.06)
(34.7, 1.02)
(25, 1.02)
(24.4, 1.03)

MK3

0.947
(52, 2.88)
VTE 0.913 ± 0.047
(2.7, 1)
BCC 0.254 ± 0.089
(8.1, 1)
MAM 0.668 ± 0.032
(26.4, 2.68)
LIV 0.357 ± 0.016
(10, 1)

0.802
0.972
0.972
(1, 3)
(93, 1.96)
(17, 1.88)
0.935 ± 0.055 0.951 ± 0.035
0.927 ± 0.045
(9, 1.07)
(1.3, 1.15)
(23.5, 1.17)
0.452 ± 0.079 0.588 ± 0.057
0.476 ± 0.086
(33.6, 1.17)
(1.2, 1)
(31, 2.93)
0.805 ± 0.028
0.808 ± 0.022 0.816 ± 0.018
(38.7, 1.32)
(48, 2.53)
(5.3, 1.43)
0.585 ± 0.071
0.563 ± 0.058
0.445 ± 0.083
(43.4, 1.56)
(59, 2.35)
(1.5, 1)

0.972
0.972
(200, 2.07)
(93, 1.84)
0.93 ± 0.042
0.929 ± 0.043
(39, 1.11)
(8.2, 1)
0.565 ± 0.059
0.563 ± 0.061
(39.6, 1.15)
(30.2, 1.07)
0.796 ± 0.026
0.796 ± 0.026
(92.2, 1.27)
(47.6, 1.24)
0.594 ± 0.046 0.595 ± 0.048
(242.5, 1.42)
(58.2, 1.32)

0.972
(7, 1.43)
0.934 ± 0.038
(6.4, 1)
0.569 ± 0.063
(29.4, 1.17)
0.797 ± 0.024
(40.5, 1.25)
0.588 ± 0.049
(45.7, 1.36)

Table 2: Results on binary REL classiﬁcation. We report the F1-score along with standard
deviation and, in brackets below, the number of the learnt rules as well as the
average length of the learnt rules. The proposed REL algorithm, gHKLρ (ρ =
1.5, 1.1), obtains better generalization performance than state-of-the-art ENDER in
most data sets, with the additional advantage of learning a smaller set of more
compact rules. The ‘*’ symbol denotes statistically signiﬁcant improvement. The
results are averaged over ten random train-test splits.

636

Generalized Hierarchical Kernel Learning

Additionally if the best result achieves a statistically signiﬁcant improvement over its nearest
competitor, it is marked with a ‘*’. Statistical signiﬁcance test is performed using the paired
t-test at 99% conﬁdence. We also report the average number of rules learnt (r) and the
average length of the rules (c), speciﬁed below each F1-score as: (r, c). As discussed earlier,
it is desirable that REL algorithms achieve high F1-score with a compact set of simple rules,
i.e., low r and c.

We can observe from Table 2 that gHKLρ obtains better generalization performance than
state-of-the-art ENDER in most of the data sets with the additional advantage of having
rules with smaller number of conjunctions.
In fact, when averaged over the data sets,
gHKL1.1 and gHKL1.5 output the shortest rules among all the methods. gHKL1.1 obtains
statistically signiﬁcant performance in TIC-TAC-TOE data set. Though the generalization
obtained by gHKL2 (HKL), gHKL1.5 and gHKL1.1 are similar, the number of rules selected by
gHKL2 is always higher than gHKL1.1 (by as much as 25 times in a few cases), hampering its
interpretability.

6.2 Multiclass Classiﬁcation in REL

This section summarizes our results on multiclass REL classiﬁcation. The details of the
multiclass data sets are provided in Table 3. Within the data sets, classes with too few
instances (< 3) were not considered for simulations since we perform a three-fold cross
validation for hyper-parameter selection. The results, averaged over 10 random train-test
splits with 10% train data are presented in Table 4. Following Dembczy´nski et al. (2008,
2010), we report the accuracy to compare generalization performance among the algorithms.
The number of rules as well as the average length of the rules is also reported to judge the
interpretability of the output.
We can observe that gHKLMT−ρ obtains the best generalization performance in seven data
sets, out of which four are statistically signiﬁcant. Moreover, gHKLMT−1.5 and gHKLMT−1.1
usually select the shortest rules among all the methods. The number of rules as well as
the average rule length of gHKLMT−2 is generally very large compared to gHKLMT−1.5 and
gHKLMT−1.1 . This again demonstrates the suitability of the proposed (cid:96)1/(cid:96)ρ regularizer in
obtaining a compact set of simple rules.

Data set Num c
625
3
BALANCE
4
1728
CAR
3
1473
C.M.C.
6
332
ECOLI
214
6
GLASS

|V |
p
32 ≈ 109
42 ≈ 1012
54 ≈ 1016
42 ≈ 1012
72 ≈ 1021

Data set Num c
150
3
IRIS
3
146
LYMPH
3
151
T.A.E.
10
1484
YEAST
101
7
ZOO

|V |
p
≈ 1015
50
≈ 1025
86
114 ≈ 1034
≈ 1016
54
≈ 1012
42

Table 3: Data sets used for multiclass REL classiﬁcation. ‘Num’ is the number of instances
in the data set while ‘c’ denotes the number of classes. The number of number of
basic propositions is ‘p’ and |V | represents the total number of possible conjunc-
tions. For each numerical input feature, 8 basic propositions were derived.

637

Jawanpuria, Nath and Ramakrishnan

SLI

ECOLI

GLASS

C.M.C.

ENDER

gHKLMT−ρ
ρ = 2
ρ = 1.5
ρ = 1.1
BALANCE 0.758 ± 0.025
0.795 ± 0.034 0.817 ± 0.028
0.808 ± 0.032
0.807 ± 0.034
(10.4, 1.7)
(2468.9, 2.84)
(112, 2.4)
(112, 1.64)
(85, 1.61)
0.823 ± 0.029
0.864 ± 0.020
0.835 ± 0.024
0.86 ± 0.028 0.875 ± 0.029∗
(9571.2, 3.14)
(18.3, 2.93)
(270, 3.05)
(269.3, 1.85)
(220.3, 1.64)
0.446 ± 0.016 0.485 ± 0.015∗
0.472 ± 0.014
0.463 ± 0.017
0.465 ± 0.016
(396.4, 1.88)
(10299.3, 2.85)
(513, 4.36)
(21.1, 1.9)
(512.9, 1.95)
0.778 ± 0.054
0.779 ± 0.057 0.784 ± 0.045∗
0.636 ± 0.028
0.726 ± 0.042
(32.4, 1.16)
(4790.2, 2.99)
(35, 2.15)
(7.8, 1.34)
(34.3, 1.05)
0.43 ± 0.061
0.465 ± 0.052
0.501 ± 0.049 0.525 ± 0.043∗
0.524 ± 0.046
(54.6, 1.04)
(5663.7, 2.40)
(70, 3.21)
(7.4, 1.41)
(69.1, 1.15)
0.893 ± 0.091
0.913 ± 0.083 0.927 ± 0.024∗
0.835 ± 0.093
0.766 ± 0.189
(8.6, 1)
(9.8, 1)
(567, 2.44)
(10, 1.34)
(2.2, 1.02)
0.722 ± 0.078
0.724 ± 0.078
0.709 ± 0.061
0.706 ± 0.058
0.61 ± 0.066
(33, 1.01)
(33.7, 1.01)
(34, 2.2)
(2.7, 1)
(4683.8, 2.30)
0.402 ± 0.046
0.399 ± 0.049
0.41 ± 0.065 0.418 ± 0.049
0.334 ± 0.035
(38.1, 1.05)
(38.3, 1.00)
(5707.4, 2.25)
(39, 1.86)
(1.1, 1)
0.486 ± 0.021
0.485 ± 0.022
0.487 ± 0.021
0.497 ± 0.015
0.478 ± 0.035
(179.6, 1.73)
(217.8, 1.80)
(8153.6, 2.85)
(218, 5.78)
(23.4, 1.63)
0.556 ± 0.062
0.938 ± 0.033
0.877 ± 0.06
0.928 ± 0.037
0.927 ± 0.039
(31.9, 1.01)
(32.3, 1.00)
(3322.2, 2.70)
(33, 1.29)
(7.1, 1.24)

IRIS

LYMPH

CAR

T.A.E.

YEAST

ZOO

Table 4: Results on multiclass REL classiﬁcation. We report the accuracy along with stan-
dard deviation and, in the brackets below, the number of learnt rules as well as the
average length of the learnt rules. The proposed REL algorithm, gHKLMT−ρ , obtains
the best generalization performance in most data sets. In addition, for ρ = 1.5 and
1.1, our algorithm learns a smaller set of more compact rules than state-of-the-art
ENDER. The ‘*’ symbol denotes statistically signiﬁcant improvement. The results
are averaged over ten random train-test splits.

7. Summary

This paper generalizes the HKL framework in two ways. First, a generic (cid:96)1/(cid:96)ρ block-norm
regularizer, ρ ∈ (1, 2), is employed that provides a more ﬂexible kernel selection pattern than
HKL by mitigating the weight bias towards the kernels that are nearer to the sources of the
DAG. Secondly, the framework is further generalized to the setup of learning a shared feature
representation among multiple related tasks. We pose the problem of learning shared fea-
tures across the tasks as that of learning a shared kernel. An eﬃcient mirror descent based
active set algorithm is proposed to solve the generalized formulations (gHKL/gHKLMT ).
An interesting computational result is that gHKL/gHKLMT can be solved in time polyno-
mial in the active set and training set sizes whenever the HKL formulation can be solved in
polynomial time. The other important contribution in this paper is the application of the
proposed gHKL/gHKLMT formulations in the setting of Rule Ensemble Learning (REL),

638

Generalized Hierarchical Kernel Learning

where HKL has not been previously explored. We pose the problem of learning an en-
semble of propositional rules as a kernel learning problem. Empirical results on binary
as well as multiclass classiﬁcation for REL demonstrate the eﬀectiveness of the proposed
generalizations.

Acknowledgments

We thank the anonymous reviewers for the valuable comments. We acknowledge Chiran-
jib Bhattacharyya for initiating discussions on optimal learning of rule ensembles. Pratik
Jawanpuria acknowledges support from IBM Ph.D. fellowship.

Appendix A.

In the appendix section, we provide the proofs of theorems/lemmas referred to in the main
paper.
(cid:111)
(cid:110)
z ∈ Rd | z ≥ 0, (cid:80)d
A.1 Lemma 26 of Micchelli and Pontil (2005)
i ≤ 1
Let ai ≥ 0, i = 1, . . . , d, 1 ≤ r < ∞ and ∆d,r =
i=1 zr
(cid:33)1+ 1
(cid:32) d(cid:88)
. Then, the
d(cid:88)
following result holds:
r
i=1
i=1
(cid:16)(cid:80)d
j=1 a
The proof follows from Holder’s inequality.

The minimum is attained at

∀i = 1, . . . , d.

min
z∈∆d,r

(cid:17) 1
r

1
r+1
a
i

ai
zi

=

r
r+1
a
i

.

zi =

r
r+1
j

A.2 Proof of Lemma 1

Proof Applying the above lemma (Appendix A.1) on the outermost (cid:96)1 -norm of the regu-
 2
 (cid:88)
larizer ΩT (f1 , . . . , fT )2 in (6), we get
(cid:88)
ρ
d2
v
(Qw (f1 , . . . , fT ))ρ
ΩT (f1 , . . . , fT )2 = min
,
γ∈∆1
where ∆1 = (cid:8)z ∈ R|V | | z ≥ 0, (cid:80)
v∈V zv ≤ 1(cid:9). Reapplying the above lemma on the individ-
γv
v∈V
w∈D(v)
 2
 (cid:88)
ual terms of the above summation gives
(cid:88)
ρ
w∈D(v)
w∈D(v)
639

Qw (f1 , . . . , fT )2
λvw

(Qw (f1 , . . . , fT )2 )

= min
λv ∈∆v
ˆρ

ρ
2

,

Jawanpuria, Nath and Ramakrishnan
(cid:111)
(cid:110)
z ∈ R|D(v)| | z ≥ 0, (cid:80)
w ≤ 1
where ˆρ = ρ
w∈D(v) zr
2−ρ and ∆v
r =
. Using the above two re-
sults and regrouping the terms will complete the proof.

A.3 Re-parameterization of the Multi-task Regularizer in (8)
(cid:80)T
The gHKLMT dual formulation (10) follows from the representer theorem (Sch¨olkopf and
Smola, 2002) after employing the following re-parameterization in (8).
t=1 ftw and f tw = ftw − f 0w . Then, Qw (f1 , . . . , fT ) in (8) may be
(cid:32)
(cid:33) 1
Deﬁne f 0w = 1
T(cid:88)
T +µ
rewritten as:
2
µ(cid:107)f 0w (cid:107)2 +
t=1

Qw (f1 , . . . , fT ) =

(cid:107)f tw (cid:107)2

.

,

)

(16)

Further, construct the following feature map (Evgeniou and Pontil, 2004)
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
φw (x)√
, φw (x), 0, . . . , 0
0, . . . , 0
Φw (x, t) = (
µ
for tasks before t
for tasks after t
√
prediction function: Qw (f1 , . . . , fT )2 = (cid:107)fw (cid:107)2 and Ft (x) = (cid:80)
µf 0w , f 1w , . . . , f T w ).
and deﬁne fw = (
With the above deﬁnitions, we rewrite the gHKLMT primal regularizer as well as the
w∈V (cid:104)fw , Φw (x, t)(cid:105) − bt ∀ t. It
follows from Lemma 1 that the gHKLMT primal problem based on (8) is equivalent to the
m(cid:88)
T(cid:88)
(cid:88)
following optimization problem:
w∈V
t=1
i=1

δw (γ , λ)−1(cid:107)fw (cid:107)2 + C

ˆρ ∀v∈V min
min
λv ∈∆v
f ,b

(cid:96)(yti , Ft (xti )),

min
γ∈∆1

(17)

1
2

where f = (fw )w∈V and b = [b1 , . . . , bT ].

A.4 Motivation for the Active Set Algorithm

Lemma 8 The problem (12) remains the same whether solved with the original set of vari-
ables (η) or when solved with only those ηv (cid:54)= 0 at optimality.

Proof The above follows from the following reasoning: a) variables η owe their presence
in (12) only via ζ (η) functions, b) (ηv = 0) ⇒ (ζw (η) = 0 ∀w ∈ D(v)), c) Let (η (cid:48) , α(cid:48) ) be
v (cid:54)= 0, then (η∗ , α(cid:48) ) is also an
an optimal solution of the problem (12). If ζv (η (cid:48) ) = 0 and η (cid:48)
w ∀w ∈ V \ v and η∗
optimal solution of the problem (12) where η∗
w = η (cid:48)
v = 0, and d) min-max
interchange in (12) yields an equivalent formulation.

Lemma 9 The fol lowing min-max interchange is equivalent:

min
η∈∆1

max
αt∈S (yt ,C )∀t

¯G(η , α) =

max
αt∈S (yt ,C )∀t

min
η∈∆1

¯G(η , α),

640

Generalized Hierarchical Kernel Learning

where

¯G(η , α) = 1(cid:62)α − 1
2

(cid:32)(cid:88)
w∈V

(cid:16)

ζw (η)

(cid:33) 1
(cid:17) ¯ρ
¯ρ

.

α(cid:62)YHwYα

Proof Note that G(η , α) is a convex function in η and a concave function in α. The
min-max interchange follows from Sion-Kakutani minimax theorem (Sion, 1958).

A.5 Proof of Theorem 3

Before stating the proof of Theorem 3, we ﬁrst prove the results in Lemma 10, Proposi-
(cid:111)
(cid:110)
tion 11 and Lemma 12, which will be employed therein (also see Bach, 2009, Lemma 10 and
z ∈ Rd | z ≥ 0, (cid:80)d
Proposition 11).
i=1 zi ≤ 1
Lemma 10 Let ai > 0 ∀i = 1, . . . , d, 1 < r < ∞ and ∆1 =
.
(cid:32) d(cid:88)
(cid:33)1−r
d(cid:88)
Then, the fol lowing holds true:
aizr
i =
i=1
i=1
−1
 d(cid:88)
j=1
− 1
∀ i = 1, . . . , d respectively.
1
i zi and a
Proof Take vectors u1 and u2 as those with entries a
r
r
1 u2 ≤ (cid:107)u1(cid:107)r (cid:107)u2(cid:107) r
i
The result follows from the Holder’s inequality: u(cid:62)
. Note that if any
r−1
ai = 0, then the optimal value of the above optimization problem is zero.

and the minimum is attained at

∀ i = 1, . . . , d.

1
1−r
zi = a
i

1
1−r
a
i

1
1−r
a
i

min
z∈∆1

Proposition 11 The fol lowing convex optimization problems are dual to each other and
(cid:88)
there is no duality gap:
(cid:88)
δw (γ , λ)Mw ,
max
γ∈∆1
w∈V
κ2
uw λuwMw
min
max
,
where L = {κ ∈ R|V |×|V | | κ ≥ 0, (cid:80)
u∈V
κ∈L
d2
w∈D(u)
u
∆1 = (cid:8)z ∈ R|V | | z ≥ 0, (cid:80)
v∈V zv ≤ 1(cid:9) and Mw ≥ 0 ∀w ∈ V .
v∈A(w) κvw = 1, κvw = 0 ∀v ∈ A(w)c , ∀w ∈ V },
sub ject to A ≥ (cid:88)
Proof The optimization problem (19) may be equivalently rewritten as:
w∈D(u)

κ2
uw λuwMw
d2
u

∀u ∈ V ,

min
A

min
κ∈L

(19)

(18)

A,

641

Jawanpuria, Nath and Ramakrishnan

(cid:88)
(cid:88)
γuκ2
uw λuwMw
 (cid:88)
 Mw
d2
(cid:88)
u∈V
w∈D(u)
u
γuλuw
κ2
−1
 (cid:88)
(cid:19)−1
(cid:18) γuλuw
uw
d2
w∈V
u∈A(w)
u
d2
u∈A(w)
u
δw (γ , λ)Mw

Mw

= min
κ∈L

max
γ∈∆1

= max
γ∈∆1

= max
γ∈∆1

= max
γ∈∆1

min
κ∈L
(cid:88)
(cid:88)
w∈V
w∈V

(Lagrangian dual with respect to A)

(min-max interchange and rearranging terms)

(Lemma 10 with respect to variables κ)

(cid:4)

Lemma 12 The fol lowing min-max interchange is equivalent:

min
γ∈∆1

min
λv ∈∆v
ˆρ ∀v∈V

max
αt∈S (yt ,C )∀t

G(γ , λ, α) =

max
αt∈S (yt ,C )∀t

min
γ∈∆1

min
ˆρ ∀v∈V G(γ , λ, α),
λv ∈∆v

where G(γ , λ, α) is as deﬁned in (10).

Proof We proceed by applying a change of variables. Note that γv = 0 implies that the
variables λvw (∀w ∈ D(v)) do not inﬂuence the ob jective of optimization problem (10). This
follows from the deﬁnition of the δ(γ , λ) function. Hence, we deﬁne βvw = γv λvw , ∀w ∈ D(v)
as it is a one-to-one transformation for γv (cid:54)= 0 (see also Szafranski et al., 2010). The gHKL
dual (10) (the L.H.S. of the proposed lemma) can be equivalently rewritten as:
(cid:80)
min
βvw≥0 ∀w∈D(v),v∈V
v (cid:107)βvD(v) (cid:107) ˆρ≤1

max
αt∈S (yt ,C )∀t
(cid:32)(cid:88)
w∈V

G(β , α), where βvD(v) = (βvw )w∈D(v) ,
(cid:33)
(cid:88)
v∈A(w)

Yα, and δw (β )−1 =

d2
v
βvw

.

G(β , α) = 1(cid:62)α − 1
2

α(cid:62)Y

δw (β )Hw

Note that δw (β ) is a concave function of β (in the given feasibility set) and hence G(β , α)
is convex-concave function with convex and compact feasibility sets. Therefore, we obtain
variables (γ , λ) by substituting γv = ((cid:80)
minβ maxα G(β , α) = maxα minβ G(β , α) (with constraints over β and α as stated above) by
applying the Sion-Kakutani minimax theorem (Sion, 1958). Finally, we revert to the original
∀w ∈
ˆρ ∀v ∈ V and λvw = βvw
1
w∈D(v) (βvw ) ˆρ )
D(v), ∀v ∈ V s.t. γv (cid:54)= 0. This gives us the equivalent R.H.S.
γv
Now we begin the proof of Theorem 3.
(cid:33)
(cid:32)(cid:88)
Proof From Lemma 12, the gHKL dual (10) can be equivalently written as:
δw (γ , λ)α(cid:62)YHwYα
(cid:125)
(cid:124)
(cid:123)(cid:122)
max
,
γ∈∆1
w∈V
O

1(cid:62)α − 1
2

max
λv ∈∆v
ˆρ ∀v∈V

max
α∈S (y,C )

(20)

642

Generalized Hierarchical Kernel Learning

∀v ∈ V

(Sion-Kakutani theorem)

=

=

max
u∈V

min
A

(Proposition 11)

(Eliminating u)

s.t. A ≥ d−2
v

(Holder’s inequality, ¯ρ= ˆρ
ˆρ−1 )

where ˆρ = ρ
2−ρ .
formulation.
O =
ˆρ ∀v∈V max
max
γ∈∆1
λv ∈∆v

(cid:88)
In the following, we equivalently rewrite the second part of the above
(cid:123)(cid:122)
(cid:125)
(cid:124)
δw (γ , λ) α(cid:62)YHwYα
(cid:88)
w∈V
Mw
κ2
uw λuwMw
d2
w∈D(u)
u
A

ˆρ ∀v∈V min
max
κ∈L
λv ∈∆v
s.t. A ≥ (cid:88)
max
ˆρ ∀v∈V min
κ∈L
λv ∈∆v
κ2
vw λvwMw
d2
w∈D(v)
v
s.t. A ≥ (cid:88)
max
min
= min
ˆρ ∀v∈V A
κ∈L
λv ∈∆v
A
κ2
∀v ∈ V
vw λvwMw
d2
w∈D(v)
v
 (cid:88)
 1
A
min
= min
κ∈L
(cid:0)κ2
(cid:1) ¯ρ
A
¯ρ
 (cid:88)
 1
vwMw
w∈D(v)
(cid:0)κ2
(cid:1) ¯ρ
¯ρ
u∈V d−2
max
= min
(21)
(Eliminating A)
uwMw
(cid:0)κ2
(cid:1) ¯ρ . Its Lagrangian
(cid:80)
κ∈L
u
w∈D(u)
d−2 ¯ρ
 .
−2 ¯ρ
Now consider the problem O ¯ρ = minκ∈L maxu∈V d
w∈D(u)
uwMw
(cid:88)
(cid:88)
(cid:0)κ2
(cid:1) ¯ρ − A
u
is
ηv
vwMw
v
v∈V
w∈D(v)
(cid:88)
(cid:88)
(cid:0)d−2
(cid:1) ¯ρ
Minimization of L with respect to A leads to the constraint η ∈ ∆1 . Hence, we have:
O ¯ρ = max
v κ2
.
ηv
min
vwMw
κ∈L
η∈∆1
v∈V
w∈D(v)
 min
 ,
(cid:88)
(cid:88)
(cid:0)ηv d−2 ¯ρ
(cid:1) κ2 ¯ρ
Using the special structure of L, the above can be rewritten as:
O ¯ρ = max
(Mw ) ¯ρ
(cid:110)
(cid:111)
η ∈ R|A(w)| | η ≥ 0, (cid:80)
κw ∈∆|A(w)|
η∈∆1
v
vw
w∈V
v∈A(w)
w∈A(w) ηw ≤ 1
where ∆|A(w)| =
. By applying Lemma 10 with re-
 (cid:88)
 1
spect to variables κ, we obtain the following equivalence:
(cid:88)
(cid:1) κ2 ¯ρ
(cid:0)ηv d−2 ¯ρ
1−ρ
vw = ζw (η) =
v
v∈A(w)
v∈A(w)

L(κ, A, η) = A +

v η1−ρ
dρ
v

.

(22)

∀v ∈ V

min
κw ∈∆|A(w)|

643

Jawanpuria, Nath and Ramakrishnan

(cid:32)(cid:88)
(cid:33) 1
(cid:16)
(cid:17) ¯ρ
From the above two results, we obtain the following equivalent dual of (21):
¯ρ
O = max
α(cid:62)YHwYα
ζw (η)
η∈∆1
w∈V
Substituting O in (20) by the above (23) and again interchanging the min-max completes
the proof.

(23)

.

×

dρ
i

×

ζ s
w (η)

¯α(cid:62)YHuY ¯α

A.6 Proof of Theorem 4
Proof We begin by noting that ζv (η) (v ∈ V ) is a concave function of η for all v (this is
because when ρ ∈ (1, 2], ζv is a weighted q -norm in η , where q ∈ [−1, 0) and hence is concave
in the ﬁrst quadrant). By simple observations regarding operations preserving convexity we
have that the ob jective in (13) is a convex function of η for a ﬁxed value of α. Hence g(η),
which is a point-wise maximum over convex functions, is itself convex. The expression for
∇g(η) is computed by employing Danskin’s theorem (Bertsekas, 1999, Proposition B.25)
(cid:123)
(cid:125)(cid:124)
(cid:122)
 (24)
 (cid:88)
and is as follows:
(cid:19)−ρ
(cid:18)
(cid:17) ¯ρ
u (η)ρ (cid:16)
P1
(∇g(η))i = − (1 − ε)
(1 − ε)ηi +
ε
ζ s
|V |
(cid:32)(cid:88)
(cid:33) 1
(cid:17) ¯ρ
(cid:16)
2 ¯ρ
u∈D(i)
¯ρ −1
¯α(cid:62)YHwY ¯α
(cid:125)
(cid:123)(cid:122)
(cid:124)
,
w∈V
P2
w (η) = ζw ((1 − ε)η + ε|V | ), i.e., the smoothed ζw (η) and ¯α is an optimal
where ¯ρ = ρ
2(ρ−1) , ζ s
solution of problem (13) with that η where the gradient is to be computed.
Next, we show that g is Lipschitz continuous by showing that its gradient is bounded.
Firstly, ρ ∈ (1, 2] and hence ¯ρ ∈ [1, ∞). Next, let the minimum and maximum eigenvalues
w (η) (cid:0) ¯α(cid:62)YHwY ¯α(cid:1) ¯ρ ≥ θ ¯ρ(cid:107) ¯α(cid:107)2 ¯ρ (cid:80)
σ(cid:107) ¯α(cid:107)2 . Using this, we obtain: (cid:80)
over all Hw (w ∈ V ) be θ and σ respectively. Then we have θ(cid:107) ¯α(cid:107)2 ≤ ¯α(cid:62)YHwY ¯α ≤
that (cid:80)
w∈V ζ s
w∈V ζ s
w (η). Note
maximum of dv (v ∈ V ). Thus we obtain: P2 ≤ (cid:0)θ ¯ρ(cid:107) ¯α(cid:107)2 ¯ρε/|V |(cid:1) 1
r (η) ≥ dρ/(1−ρ)
r (η) where r ∈ sources(V ) and ζ s
w (η) ≥ ζ s
ε|V | where dmax is the
w∈V ζ s
max
2−ρ
¯ρ −1 d
ρ−1
max .
i ((1 − ε)ηi + ε|V | )−ρ ζu (η)ρ ≤ d
Now, it is easy to see that ∀u ∈ D(i), dρ
ρ
1−ρ
i
where dmin is the minimum of dv (v ∈ V ). Hence P1 ≤ |V |σ ¯ρ(cid:107) ¯α(cid:107)2 ¯ρd
ρ
1−ρ
0 ≤ ¯α ≤ C , we have (cid:107) ¯α(cid:107) ≤ √
min . In addition, since
mT C . Summarizing these ﬁndings, we obtain the following
bound on the gradient:
(cid:107)∇g(η)(cid:107)1 ≤ (1 − ε)
2 ¯ρ
The proof will be similar for gHKLMT formulations in other learning settings.

2−ρ
1− ¯ρ
¯ρ |V | 2
ρ
ρ−1
1−ρ
ρ +1d
min d
max .

mT C 2θ1− ¯ρσ ¯ρε

≤ d
ρ
1−ρ
min ,

644

Generalized Hierarchical Kernel Learning

A.7 Proof of Theorem 5
Proof Given a candidate solution η and α = [α(cid:62)
1 , . . . , α(cid:62)
T ](cid:62) (with associated primal (f =
(f1 , . . . , fT ), b, ξ )), the duality gap (D) between the two variational formulations in Lemma 9
is as follows:

¯G(η , ˆα) − min
D = max
ˆη∈∆1
ˆαt∈S (yt ,C )∀t
ΩT (f )2 + C 1(cid:62) ξ − min
≤ 1
(cid:122)
(cid:125)(cid:124)
(cid:123)
ˆη∈∆1
2
Gap in solving with ﬁxed η
ΩT (f )2 + C 1(cid:62) ξ − 1(cid:62)α +

=

1
2

¯G( ˆη , α)
max
(cid:32)(cid:88)
¯G( ˆη , α)
(cid:124)
ˆη∈∆1
w∈V

(cid:16)

ζw ( ˆη)

α(cid:62)YHwYα
(cid:123)(cid:122)
Gap in solving with ﬁxed α

(cid:33) 1
(cid:17) ¯ρ
¯ρ − ΩT (f )2


(cid:125)

.

With this upper bound on the duality gap, it is easy to see that the following condition is
suﬃcient for the reduced solution (with active set W ) to have D ≤ :
(cid:33) 1
(cid:32)(cid:88)
(cid:17) ¯ρ
(cid:16)
¯ρ ≤ ΩT (fW )2 + 2( − W ),
w∈V

α(cid:62)YHwYα

max
η∈∆1

ζw (η)

(25)

where W is the duality gap9 associated with the computation of the dual variables αW .
Here as well as in the rest of the proof, the subscript (·)W implies the value of the variable
obtained when the gHKLMT formulation is solved with V restricted to the active set W . In
Appendix A.5, we had proved that the L.H.S. of the above inequality is equal to the R.H.S.
 (cid:88)
 1
of (21), i.e.,
(cid:33) 1
(cid:32)(cid:88)
(cid:0)κ2
(cid:1) ¯ρ
¯ρ
¯ρ
ζw (η) (Mw ) ¯ρ
vwMw
w∈V
w∈D(v)

v∈V d−2
max
v

= min
κ∈L

max
η∈∆1

(26)

,

where Mw = α(cid:62)
W YHwYαW .
Next, we obtain an upper bound of the above by substituting κ ∈ L in the R.H.S of (26).
In particular, we employ the following: the value of κvw v , w ∈ W is obtained by solving
the small10 problem (14). This is ﬁne because W = hull(W ). For v ∈ W c and w ∈ W , by
deﬁnition of L and W , we have κvw = 0. Next, κvw is set to zero ∀ v ∈ W , w ∈ W c . For the
(cid:16)(cid:80)
(cid:17)−1
remaining κvw , v ∈ W c and w ∈ W c , we use the value of κ obtained by solving (21) with
u∈A(v)∩W c du
(also see Section A.5 Bach, 2009). Note that the
ρ = 1, i.e., κvw = dv
above constructed value of κ is feasible in the set L. With this choice of κ substituted in

9. This is given by the gap associated with the ˆρ-norm MKL solver employed in the mirror descent algorithm
LW = {κ ∈ R|W |×|W | | κ ≥ 0, (cid:80)
for solving the small problem (14).
10. The value of κvw (∀v , w ∈ W ) obtained in this manner satisfy the constraint set L restricted to W , i.e.,
v∈A(w) κvw = 1, κvw = 0 ∀ v ∈ A(w)c ∩ W , ∀ w ∈ W }

645

Jawanpuria, Nath and Ramakrishnan

= max

max
η∈∆1

(cid:17)2

≤ max

max
u∈sources(W c )

¯ρ 
 ¯ρ 1
¯ρ 
 ¯ρ 1
(cid:17)2
¯ρ 
 ¯ρ 1

(cid:32)(cid:88)
(cid:33) 1
the R.H.S. of (26), we have the following inequalities:
(cid:16)
(cid:17) ¯ρ
¯ρ
α(cid:62)
ΩT (fW )2 , max
W YHwYαW
 α(cid:62)
 (cid:88)
ζw (η)
w∈V
(cid:16)(cid:80)
W YHwYαW
u∈W c
w∈D(u)
v∈A(w)∩W c dv
ΩT (fW )2 ,
 α(cid:62)
 (cid:88)
( Speciﬁc choice of κ)
(cid:16)(cid:80)
W YHwYαW
w∈D(u)
v∈A(w)∩W c dv
ΩT (fW )2 ,
 (cid:88)
 α(cid:62)
(∵ W = hull(W ))
(cid:17)2
(cid:16)(cid:80)
W YHwYαW
≤ max
max
v∈A(w)∩W c dv ≥ (cid:80)
(∵ (cid:80)
u∈sources(W c )
w∈D(u)
v∈A(w)∩D(u) dv

ΩT (fW )2 ,
(cid:88)
v∈A(w)∩D(u) dv )
(cid:16)(cid:80)
α(cid:62)
W YHwYαW
≤ max
max
u∈sources(W c )
w∈D(u)
v∈A(w)∩D(u) dv
(∵ (cid:107)β (cid:107)1 ≥ (cid:107)β (cid:107) ¯ρ ∀ ¯ρ ≥ 1)
Employing the above upper bound in (25) leads to the result in Theorem 5. Note that in
practice, the last upper bound is not loose for Rule Ensemble Learning (REL) application.
This is because most of the matrices, especially near the bottom of the lattice, will be (near)
zero-matrices — larger the conjunctive rule, the fewer are the examples which may satisfy
it.

(cid:17)2

A.8 gHKLMT with General Convex Loss Functions

In this section, we present extension of the proposed algorithm to other learning settings
like regression. In particular, we consider the case where the loss function (cid:96)(·, ·) is a general
convex loss function such as the hinge loss, the square loss, the Huber loss, etc.
The gHKLMT primal formulation with a general convex loss function (cid:96)(·, ·) was given in
(cid:32)(cid:88)
(cid:33) 1
equation (6). The specialized gHKLMT dual formulation corresponding to (6) is as follows:
(cid:17) − 1
(cid:16)− αti
(cid:16)
(cid:17) ¯ρ
T(cid:88)
m(cid:88)
¯ρ
C
2
w∈V
t=1
i=1

max
αt∈Rm ,1(cid:62)αt=0 ∀t

α(cid:62)Hw α

min
η∈∆1

ζw (η)

−C

ϕ∗
ti

,

646

Generalized Hierarchical Kernel Learning
(cid:17) 1
(cid:16)(cid:80)
v η1−ρ
where α = [α(cid:62)
1 , . . . , α(cid:62)
T ](cid:62) , ζw (η) =
1−ρ (refer Theorem 3 for details)
v∈A(w) dρ
v
and ϕ∗
ti denotes the Fenchel11 conjugate (Boyd and Vandenberghe, 2004) of the function
ϕti : z → (cid:96)(yti , z ).

¯θw =

,

(27)

1
ˆρ−1

.

(cid:16)
(ζw ( ¯ηW ))

(ζv ( ¯ηW ))

¯θw (ζw ( ¯ηW ))

¯ρ ¯α(cid:62)
1
W YHwY ¯αW
¯ρ ¯α(cid:62)
1
W YHvY ¯αW

A.9 Prediction Function for gHKLMT with the Hinge Loss Function
Let the ﬁnal active set be W and ( ¯ηW , ¯αW ) be the optimal solution of (12). Then the
(cid:32) (cid:88)
(cid:33)
prediction function for an instance xtj belonging to the tth task is given by
Ft (x) = ( ¯αW (cid:12) y)(cid:62)
¯ρ Hw (·, xtj )
1
w∈W
where symbol (cid:12) denote element-wise product, Hw is the kernel matrix corresponding to the
multi-task kernel (11), Hw (·, xtj ) = ((Hw (xt(cid:48) i , xtj ))m


i=1 )T
t(cid:48)=1 and
(cid:17) ¯ρ(cid:19) 1
(cid:18)(cid:80)
¯ρ
v∈W
A.10 Proof of Corollary 6
Note that proving the computational complexity of the matrix Ku (u ∈ sources(W c )) in
(15) to be polynomial time in size of the active set and the training set dimensions suﬃces
to prove the corollary. This is because all the other steps in Algorithms 3 and 2 are of
polynomial time complexity (discussed in Section 4).
We begin the proof by introducing some indexing notations related to the multi-task
matrices. Let the entries in Hw , the mT × mT multi-task kernel matrix, be arranged in the
following form: the entry corresponding to the input pair (xt1 i , xt2 j ) be in the ((t1 − 1) ∗
m + i)th row and ((t2 − 1) ∗ m + j )th column of Hw .

 (cid:88)
Next we observe that the expression for Ku in Theorem 5 may be rewritten as:
(cid:17)2
(cid:16)(cid:80)
(cid:124)
(cid:123)(cid:122)
(cid:125)
w∈D(u)
v∈A(w)∩D(u) dv
Tu
where: i) Kw is a mT × mT matrix corresponding to the base kernel kw and constructed
from the inputs from all the tasks, ii) KT is a mT × mT such that the entry corresponding
to the ((t1 − 1) ∗ m + i)th row and ((t2 − 1) ∗ m + j )th column (1 ≤ i, j ≤ m) of KT is
B (t1 , t2 ), and iii) (cid:12) is the symbol for element-wise product (Hadamard product).
(cid:26) zy
11. Fenchel conjugate ϕ∗ (z ) of a convex function ϕ(u) is given by ϕ∗ (z ) = supu z(cid:62)u − ϕ(u). As an example,
if zy ∈ [−1, 0]
for hinge loss ϕ(u) = (cid:96)(u, y) = max(0, 1 − uy), ϕ∗ (z ) =
∞ otherwise

Ku =

Kw

(cid:12)KT ,

647

Jawanpuria, Nath and Ramakrishnan

In the above expression, Ku is computable in polynomial time if and only if Tu is
computable in polynomial time. The proof of the corollary follows from observing the
expression of the suﬃcient condition for optimality of the HKL (Bach, 2009, Equation 21),
which also involves the term Tu .

φc (xi )

·

φc (xj )

=

kc (xi , xj ),

kv (xi , xj ) = φv (xi ) · φv (xj ) =

A.11 Proof of Theorem 7
Given an active set W of size W , proving that the computational complexity of the veriﬁ-
cation of the suﬃcient condition of optimality (15) is polynomial in terms of the active set
and the training set sizes suﬃces to prove Theorem 7. This is because all the other steps
in Algorithms 3 and 2 are of polynomial time complexity (discussed in Section 4).
In the REL setup, the DAG is the conjunction lattice and the embedded kernels kv v ∈ V
(cid:32) (cid:89)
(cid:33)
(cid:32) (cid:89)
(cid:33)
(cid:75)
may be rewritten as:
c∈Sv
c∈Sv
c∈Sv
where Sv is the set of basic propositions involved in the conjunction φv and (cid:12) is the symbol
for element-wise product (Hadamard product). The kernels corresponding to the basic
propositions are in fact the base kernels embedded in the second level nodes of the lattice
V . Employing the above deﬁnition of kv (xi , xj ), the matrices Ku (in L.H.S. of (15)) are
 (cid:75)
(1 + a)2 + 11(cid:62)(cid:19) ,
(cid:33)
(cid:32)(cid:75)
(cid:18) Kc
computed as:
(cid:88)
(cid:17)2 =
(cid:16)(cid:80)
c∈Su
w∈D(u)
c∈B/Su
v∈A(w)∩D(u) dv
where Kc is the kernel matrix corresponding to the basic proposition φc , B is the set of all
basic propositions and the parameters dv (v ∈ V ) are deﬁned as dv = a|Sv | (a > 0).
It is obvious that a trivial computational complexity of computing Ku (u ∈ V ) is O(pm2 ).
In practice, this complexity can be reduced to O(m2 ) by caching the matrices Ku . For
illustration, suppose Ku1 needs to be computed, given that Ku0 is cached and u0 is a parent
of u1 . Let the extra basic proposition contained in φu1 (with respect to φu0 ) be φe . Then
(cid:18) Ke
(cid:19)
(1 + a)2 + 11(cid:62)(cid:19)
(cid:18) Ke
Ku1 can be calculated as follows:
Ku1 = Ku0 (cid:12)
a2
where (cid:11) is the symbol for element-wise division of matrices.
Hence, plugging the REL speciﬁc values in the runtime complexity of the gHKL algo-
rithm, ω =constant and z = p, the runtime complexity of the gHKL based REL algorithm
is O(m3W 3 log(W ) + m2W 2p).

Kc
a2

(cid:12)

Ku =

Kw

(cid:11)

,

A.12 REL Binary Classiﬁcation Results in AUC

Table 5 reports the REL binary classiﬁcation results in AUC (area under the ROC curve).
The experimental details (and results measured in F1-score) are discussed in Section 6.

648

Generalized Hierarchical Kernel Learning

RuleFit

SLI

ENDER

0.783 ± 0.036
0.482 ± 0.21
0.736 ± 0.05
TIC
0.958 ± 0.039
BCW 0.941 ± 0.011 0.917 ± 0.051
0.67 ± 0.027 0.576 ± 0.115
0.761 ± 0.02
DIA
HAB 0.537 ± 0.054
0.17 ± 0.155 0.575 ± 0.039
0.805 ± 0.031
0.764 ± 0.03 0.541 ± 0.215
0.68 ± 0.028
0.546 ± 0.06 0.175 ± 0.256
BLD
HTS 0.765 ± 0.028 0.712 ± 0.085
0.801 ± 0.022

HTC

gHKLρ
HKL-
ρ = 1.5
(cid:96)1 -MKL
ρ = 2
ρ = 1.1
0.973 ± 0.02 0.975 ± 0.018
0.967 ± 0.023
0.836 ± 0.024
0.93 ± 0.099
0.981 ± 0.008 0.984 ± 0.005 0.93 ± 0.099
0.636 ± 0.118
0.746 ± 0.050 0.766 ± 0.046 0.733 ± 0.058
0.524 ± 0.078
0.556 ± 0.07
0.482 ± 0.11
0.383 ± 0.166
0.753 ± 0.118
0.802 ± 0.085 0.837 ± 0.035 0.763 ± 0.12
0.519 ± 0.079
0.660 ± 0.025
0.667 ± 0.034 0.634 ± 0.028
0.825 ± 0.032 0.849 ± 0.021 0.83 ± 0.027
0.811 ± 0.056

0.998
0.995
1
0.998
0.972
0.632
MK3
0.972 ± 0.016 0.948 ± 0.015
0.965 ± 0.014 0.977 ± 0.009
VTE 0.955 ± 0.022 0.919 ± 0.048
0.627 ± 0.063 0.637 ± 0.055 0.576 ± 0.089
0.578 ± 0.05 0.469 ± 0.078
0.622 ± 0.043
0.866 ± 0.028
0.763 ± 0.08 0.887 ± 0.006
0.818 ± 0.02
0.882 ± 0.023
0.85 ± 0.032
MAM
LIV 0.607 ± 0.017 0.093 ± 0.168
0.619 ± 0.038
0.619 ± 0.074 0.623 ± 0.038 0.583 ± 0.11

BCC

0.957
0.945 ± 0.016
0.513 ± 0.124
0.839 ± 0.03
0.565 ± 0.109

Table 5: Results on binary REL classiﬁcation. We report the average AUC along with
standard deviation, over ten random train-test splits.

References

J. Aﬂalo, A. Ben-Tal, C. Bhattacharyya, J. Saketha Nath, and S. Raman. Variable sparsity
kernel learning. Journal of Machine Learning Research, 12:565–592, 2011.

M. A. ´Alvarez, L. Rosasco, and N. D. Lawrence. Kernels for vector-valued functions: a
review. Foundations and Trends in Machine Learning, 4:195–266, 2012.

R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple
tasks and unlabeled data. Journal of Machine Learning Research, 6:1817–1853, 2005.

A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine
Learning, 73:243–272, 2008.

F. Bach. Exploring large feature spaces with hierarchical multiple kernel learning.
Advances in Neural Information Processing Systems, 2008.

In

F. Bach. High-dimensional non-linear variable selection through hierarchical kernel learning.
Technical report, INRIA, France, 2009.

F. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and
the SMO algorithm. In Proceedings of International Conference on Machine Learning,
2004.

649

Jawanpuria, Nath and Ramakrishnan

J. Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intel ligence Research,
12:149–198, 2000.

A. Beck and M. Teboulle. Mirror descent and nonlinear pro jected subgradient methods for
convex optimization. Operations Research Letters, 31(3):167–175, 2003.

S. Ben-David and R. Schuller. A notion of task relatedness yielding provable multiple-task
learning guarantees. Machine Learning, 73:273–287, 2008.

A. Ben-Tal and A. Nemirovski. Lectures on modern convex optimization: Analysis, algo-
rithms and engineering applications. MPS/ SIAM Series on Optimization, 1, 2001.

D. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 1999.

K. Blake and M. Lichman. UCI machine learning repository, 2013. URL http://archive.
ics.uci.edu/ml.

S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.

R. Caruana. Mutitask learning. Machine Learning, 28:41–75, 1997.

P. Clark and T. Niblett. The CN2 induction algorithm. Machine Learning, 3:261–283, 1989.

W. W. Cohen and Y. Singer. A simple, fast, and eﬀective rule learner. In AAAI Conference
on Artiﬁcial Intel ligence, 1999.

T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms (3.
ed.). MIT Press, 2009.

K. Dembczy´nski, W. Kot(cid:32)lowski, and R. S(cid:32)lowi´nski. Maximum likelihood rule ensembles. In
Proceedings of the International Conference of Machine Learning, 2008.

K. Dembczy´nski, W. Kot(cid:32)lowski, and R. S(cid:32)lowi´nski. ENDER - A statistical framework for
boosting decision rules. Data Mining and Know ledge Discovery, 21:52–90, 2010.

T. Evgeniou and M. Pontil. Regularized multi-task learning. In Proceedings of the ACM
SIGKDD International Conference on Know ledge Discovery and Data Mining, 2004.

T. Evgeniou, C. A. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods.
Journal of Machine Learning Research, 6:615–637, 2005.

J. H. Friedman and B. E. Popescu. Predictive learning via rule ensembles. Annals of Applied
Statistics, 2:916–954, 2008.

L. Jacob, F. Bach, and J.-P. Vert. Clustered multi-task learning: A convex formulation. In
Advances in Neural Information Processing Systems, 2008.

A. Jain, S. V. N. Vishwanathan, and M. Varma. SPG-GMKL: Generalized multiple ker-
nel learning with a million kernels. In Proceedings of the ACM SIGKDD International
Conference on Know ledge Discovery and Data Mining, 2012.

650

Generalized Hierarchical Kernel Learning

P. Jawanpuria and J. S. Nath. Multi-task multiple kernel learning. In SIAM International
Conference on Data Mining, 2011.

P. Jawanpuria and J. S. Nath. A convex feature learning formulation for latent task structure
discovery. In Proceedings of the International Conference on Machine Learning, 2012.

P. Jawanpuria, J. S. Nath, and G. Ramakrishnan. Eﬃcient rule ensemble learning using
hierarchical kernels. In Proceedings of the International Conference of Machine Learning,
2011.

P. Jawanpuria, M. Varma, and J. S. Nath. On p-norm path following in multiple kernel
learning for non-linear feature selection. In Proceedings of the International Conference
of Machine Learning, 2014.

M. Kloft, U. Brefeld, S. Sonnenburg, and A. Zien. (cid:96)p -norm multiple kernel learning. Journal
of Maching Learning Research, 12:953–997, 2011.

G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the
kernel matrix with semideﬁnite programming. Journal of Machine Learning Research, 5:
27–72, 2004.

H. Lee, A. Battle, R. Raina, and A. Y. Ng. Eﬃcient sparse coding algorithms. In Advances
in Neural Information Processing Systems, 2007.

J. Liu and J. Ye. Eﬃcient (cid:96)1/(cid:96)q norm regularization. Technical Report arXiv:1009.4766,
2010.

K. Lounici, M. Pontil, A. B. Tsybakov, and S. van de Geer. Taking advantage of sparsity in
multi-task learning. In Proceedings of the Annual Conference on Learning Theory, 2009.

C. A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of
Machine Learning Research, 6:1099–1125, 2005.

R. S. Michalski. A theory and methodology of inductive learning. Artiﬁcial Intel ligence,
20:111–161, 1983.

S. Negahban and M. Wainwright. Phase transitions for high-dimensional joint support
recovery. In Advances in Neural Information Processing Systems, 2009.

G. Obozinski, Martin J. Wainwright, and M.I. Jordan. Support union recovery in high-
dimensional multivariate regression. Annals of Statistics, 39:1–17, 2011.

F. Orabona, J. Luo, and B. Caputo. Multi kernel learning with online-batch optimization.
Journal of Machine Learning Research, 13:227–253, 2012.

J. C. Platt. Fast training of support vector machines using sequential minimal optimization.
In Advances in Kernel Methods - Support Vector Learning, 1999.

J. R. Quinlan. Induction of decision trees. Machine Learning, 1:81–106, 1986.

651

Jawanpuria, Nath and Ramakrishnan

A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. SimpleMKL. Journal of Machine
Learning Research, 9:2491–2521, 2008.

R. L. Rivest. Learning decision lists. Machine Learning, 2:229–246, 1987.

B. Sch¨olkopf and A. Smola. Learning with Kernels. MIT press, Cambridge, 2002.

D. Sheldon. Graphical multi-task learning. Technical report, Cornell University, 2008.

M. Sion. On general minimax theorem. Paciﬁc Journal of Mathematics, 1958.

M. Szafranski, Y. Grandvalet, and P. M. Mahoudeaux. Hierarchical penalization. In Ad-
vances in Neural Information Processing Systems, 2007.

M. Szafranski, Y. Grandvalet, and A. Rakotomamonjy. Composite kernel learning. Machine
Learning, 79:73–103, 2010.

B. A. Turlach, W. N. Venables, and S. J. Wright. Simultaneous variable selection. Techno-
metrics, 47:349–363, 2005.

V. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.

S. V. N. Vishwanathan, Z. Sun, N. T.-Ampornpunt, and M. Varma. Multiple kernel learning
and the SMO algorithm. In Advances in Neural Information Processing Systems, 2010.

S. M. Weiss and N. Indurkhya. Lightweight rule induction. In Proceedings of the Interna-
tional Conference of Machine Learning, 2000.

Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. Multi-task learning for classiﬁcation with
dirichlet process priors. Journal of Maching Learning Research, 8:35–63, 2007.

M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68:49–67,
2006.

652

Journal of Machine Learning Research 16 (2015) 227-254

Submitted 12/13; Revised 7/14; Published 2/15

Multi-layered Gesture Recognition with Kinect

Feng Jiang
School of Computer Science and Technology
Harbin Institute of Technology, Harbin 150001, China

Shengping Zhang
School of Computer Science and Technology
Harbin Institute of Technology, Weihai 264209, China

Shen Wu
Yang Gao
Debin Zhao
School of Computer Science and Technology
Harbin Institute of Technology, Harbin 150001, China

fjiang@hit.edu.cn

s.zhang@hit.edu.cn

wu.shen.eltshan@gmail.com
lambyy.hit@gmail.com
dbzhao@hit.edu.cn

Editors: Isabelle Guyon, Vassilis Athitsos, and Sergio Escalera

Abstract
This paper proposes a novel multi-layered gesture recognition method with Kinect. We
explore the essential linguistic characters of gestures: the components concurrent character
and the sequential organization character, in a multi-layered framework, which extracts
features from both the segmented semantic units and the whole gesture sequence and then
sequentially classiﬁes the motion, location and shape components. In the ﬁrst layer, an
improved principle motion is applied to model the motion component. In the second layer,
a particle-based descriptor and a weighted dynamic time warping are proposed for the loca-
tion component classiﬁcation. In the last layer, the spatial path warping is further proposed
to classify the shape component represented by unclosed shape context. The proposed
method can obtain relatively high performance for one-shot learning gesture recognition on
the ChaLearn Gesture Dataset comprising more than 50, 000 gesture sequences recorded
with Kinect.
Keywords: gesture recognition, Kinect, linguistic characters, multi-layered classiﬁcation,
principle motion, dynamic time warping

1. Introduction

Gestures, an unsaid body language, play very important roles in daily communication.
They are considered as the most natural means of communication between humans and
computers (Mitra and Acharya, 2007). For the purpose of improving humans’ interaction
with computers, considerable work has been undertaken on gesture recognition, which has
wide applications including sign language recognition (Vogler and Metaxas, 1999; Cooper
et al., 2012), socially assistive robotics (Baklouti et al., 2008), directional indication through
pointing (Nickel and Stiefelhagen, 2007) and so on (Wachs et al., 2011).
Based on the devices used to capture gestures, gesture recognition can be roughly cate-
gorized into two groups: wearable sensor-based methods and optical camera-based methods.
The representative device in the ﬁrst group is the data glove (Fang et al., 2004), which is

c(cid:13)2015 Feng Jiang, Shengping Zhang, Shen Wu, Yang Gao, and Debin Zhao.

Jiang, Zhang, Wu, Gao and Zhao

capable of exactly capturing the motion parameters of the user’s hands and therefore can
achieve high recognition performance. However, these devices aﬀect the naturalness of the
user interaction. In addition, they are also expensive, which restricts their practical applica-
tions (Cooper et al., 2011). Diﬀerent from the wearable devices, the second group of devices
are optical cameras, which record a set of images overtime to capture gesture movements
in a distance. The gesture recognition methods based on these devices recognize gestures
by analyzing visual information extracted from the captured images. That is why they are
also called vision-based methods. Although optical cameras are easy to use and also inex-
pensive, the quality of the captured images is sensitive to lighting conditions and cluttered
backgrounds, thus it is very diﬃcult to detect and track the hands robustly, which largely
aﬀects the gesture recognition performance.
Recently, the Kinect developed by Microsoft was widely used in both industry and
research communities (Shotton et al., 2011). It can capture both RGB and depth images
of gestures. With depth information, it is not diﬃcult to detect and track the user’s body
robustly even in noisy and cluttered backgrounds. Due to the appealing performance and
also reasonable cost, it has been widely used in several vision tasks such as face tracking (Cai
et al., 2010), hand tracking (Oikonomidis et al., 2011), human action recognition (Wang
et al., 2012) and gesture recognition (Doliotis et al., 2011; Ren et al., 2013). For example,
one of the earliest methods for gesture recognition using Kinect is proposed in Doliotis
et al. (2011), which ﬁrst detects the hands using scene depth information and then employs
Dynamic Time Warping for recognizing gestures. Ren et al. (2013) extracts the static ﬁnger
shape features from depth images and measures the dissimilarity between shape features
for classiﬁcation. Although, Kinect facilitates us to detect and track the hands, exact
segmentation of ﬁnger shapes is still very challenging since the ﬁngers are very small and
form many complex articulations.
Although postures and gestures are frequently considered as being identical, there are
signiﬁcant diﬀerences (Corradini, 2002). A posture is a static pose, such as making a palm
posture and holding it in a certain position, while a gesture is a dynamic process consisting of
a sequence of the changing postures over a short duration. Compared to postures, gestures
contain much richer motion information, which is important for distinguishing diﬀerent ges-
tures especially those ambiguous ones. The main challenge of gesture recognition lies in the
understanding of the unique characters of gestures. Exploring and utilizing these characters
in gesture recognition are crucial for achieving desired performance. Two crucial linguistic
models of gestures are the phonological model drawn from the component concurrent char-
acter (Stokoe, 1960) and the movement-hold model drawn from the sequential organization
character (Liddell and Johnson, 1989). The component concurrent character indicates that
complementary components, namely motion, location and shape components, simultane-
ously characterize a unique gesture. Therefore, an ideal gesture recognition method should
have the ability of capturing, representing and recognizing these simultaneous components.
On the other hand, the movement phases, i.e., the transition phases, are deﬁned as periods
during which some components, such as the shape component, are in transition; while the
holding phases are deﬁned as periods during which all components are static. The sequen-
tial organization character characterizes a gesture as a sequential arrangement of movement
phases and holding phases. Both the movement phases and the holding phases are deﬁned
as semantic units. Instead of taking the entire gesture sequence as input, the movement-

228

Multi-layered Gesture Recognition with Kinect

hold model inspires us to segment a gesture sequence into sequential semantic units and
then extract speciﬁc features from them. For example, for the frames in a holding phase,
shape information is more discriminative for classifying diﬀerent gestures.
It should be noted that the component concurrent character and the sequential orga-
nization character demonstrate the essences of gestures from spatial and temporal aspects,
respectively. The former indicates which kinds of features should be extracted. The later
implies that utilizing the cycle of movement and hold phases in a gesture sequence can
accurately represent and model the gesture. Considering these two complementary charac-
ters together provides us a way to improve gesture recognition. Therefore, we developed a
multi-layered classiﬁcation framework for gesture recognition. The architecture of the pro-
posed framework is shown in Figure 1, which contains three layers: the motion component
classiﬁer, the location component classiﬁer, and the shape component classiﬁer. Each of the
three layers analyzes its corresponding component. The output of one layer limits the pos-
sible classiﬁcation in the next layer and these classiﬁers complement each other for the ﬁnal
gesture classiﬁcation. Such a multi-layered architecture assures achieving high recognition
performance while being computationally inexpensive.

Figure 1: Multi-layered gesture recognition architecture.

The main contributions of this paper are summarized as follows:

• The phonological model (Stokoe, 1960) of gestures inspires us to propose a novel
multi-layered gesture recognition framework, which sequentially classiﬁes the motion,
location and shape components and therefore achieves higher recognition accuracy
while having low computational complexity.
• Inspired by the linguistic sequential organization of gestures (Liddell and Johnson,
1989), the matching process between two gesture sequences is divided into two steps:
their semantic units are matched ﬁrst, and then the frames inside the semantic units
are further registered. A novel particle-based descriptor and a weighted dynamic time
warping are proposed to classify the location component.

229

Motion Component ClassifierGesture Recognition ResultsGesture Depth and RGB Data Recorded by KinectInter-gesture SegmentationLocation Component ClassifierShape Component ClassifierJiang, Zhang, Wu, Gao and Zhao

• The spatial path warping is proposed to classify the shape component represented by
unclosed shape context, which is improved from the original shape context but the
computation complexity is reduced from O(n3 ) to O(n2 ).

Our proposed method participated the one-shot learning ChaLearn gesture challenge and
was top ranked (Guyon et al., 2013). The ChaLearn Gesture Dataset (CGD 2011) (Guyon
et al., 2014) is designed for one-shot learning and comprises more than 50, 000 gesture
sequences recorded with Kinect. The remainder of the paper is organized as follows. Re-
lated work is reviewed in Section 2. The detailed descriptions of the proposed method are
presented in Section 3. Extensive experimental results are reported in Section 4. Section 5
concludes the paper.

2. Related Work

Vision based gesture recognition methods encompasses two main categories: three dimen-
sional (3D) model based methods and appearance based methods. The former computes
a geometrical representation using the joint angles of a 3D articulated structure recovered
from a gesture sequence, which provides a rich description that permits a wide range of ges-
tures. However, computing a 3D model has high computational complexity (Oikonomidis
et al., 2011).
In contrast, appearance based methods extract appearance features from
a gesture sequence and then construct a classiﬁer to recognize diﬀerent gestures, which
have been widely used in vision based gesture recognition (Dardas, 2012). The proposed
multi-layered gesture recognition falls into the appearance based methods.

2.1 Feature Extraction and Classiﬁcation

The well known features used for gesture recognition are color (Awad et al., 2006; Maraqa
and Abu-Zaiter, 2008), shapes (Ramamoorthy et al., 2003; Ong and Bowden, 2004) and
motion (Cutler and Turk, 1998; Mahbub et al., 2013). In early work, color information is
widely used to segment the hands of a user. To simplify the color based segmentation, the
user is required to wear single or diﬀerently colored gloves (Kadir et al., 2004; Zhang et al.,
2004). The skin color models are also used (Stergiopoulou and Papamarkos, 2009; Maung,
2009) where a typical restriction is wearing of long sleeved clothes. When it is diﬃcult to
exploit color information to segment the hands from an image (Wan et al., 2012b), motion
information extracted from two consecutive frames is used for gesture recognition. Agrawal
and Chaudhuri (2003) explores the correspondences between patches in adjacent frames and
uses 2D motion histogram to model the motion information. Shao and Ji (2009) computes
optical ﬂow from each frame and then uses diﬀerent combinations of the magnitude and
direction of optical ﬂow to compute a motion histogram. Zahedi et al. (2005) combines
skin color features and diﬀerent ﬁrst- and second-order derivative features to recognize sign
language. Wong et al. (2007) uses PCA on motion gradient images of a sequence to obtain
features for a Bayesian classiﬁer. To extract motion features, Cooper et al. (2011) extends
Haar-like features from spatial domain to spatio-temporal domain and proposes volumetric
Haar-like features.
The features introduced above are usually extracted from RGB images captured by a
traditional optical camera. Due to the nature of optical sensing, the quality of the captured

230

Multi-layered Gesture Recognition with Kinect

images is sensitive to lighting conditions and cluttered backgrounds, thus the extracted
features from RGB images are not robust. In contrast, depth information from a calibrated
camera pair (Rauschert et al., 2002) or direct depth sensors such as LiDAR (Light Detec-
tion and Ranging) is more robust to noises and illumination changes. More importantly,
depth information is useful for discovering the distance between the hands and body or-
thogonal to the image plane, which is an important cue for distinguishing some ambiguous
gestures. Because the direct depth sensors are expensive, inexpensive depth cameras, e.g.,
Microsoft’s Kinect, have been recently used in gesture recognition (Ershaed et al., 2011;
Wu et al., 2012b). Although the skeleton information oﬀered by Kinect is more eﬀective in
the expression of human actions than pure depth data, there are some cases that skeleton
cannot be extracted correctly, such as interaction between human body and other ob jects.
Actually, in the CHALERAN gesture challenge (Guyon et al., 2013), the skeleton informa-
tion is not allowed to use. To extract more robust features from Kinect depth images for
gesture recognition, Ren et al. (2013) proposes the part based ﬁnger shape features, which
do not depend on the accurate segmentation of the hands. Wan et al. (2013, 2014b) extend
SIFT to spatio-temporal domain and propose 3D EMoSIFT and 3D SMoSIFT to extract
features from RGB and depth images, which are invariant to scale and rotation, and have
more compact and richer visual representations. Wan et al. (2014a) proposes a discrimina-
tive dictionary learning method on 3D EMoSIFT features based on mutual information and
then uses sparse reconstruction for classiﬁcation. Based on 3D Histogram of Flow (3DHOF)
and Global Histogram of Oriented Gradient (GHOG), Fanello et al. (2013) applies adaptive
sparse coding to capture high-level feature patterns. Wu et al. (2012a) utilizes both RGB
and depth information from Kinect and an extended-MHI representation is adopted as the
motion descriptors.

The performance of a gesture recognition method is not only related to the used fea-
tures but also to the adopted classiﬁers. Many classiﬁers can be used for gesture recognition,
e.g., Dynamic Time Warping (DTW) (Reyes et al., 2011; Lichtenauer et al., 2008; Sabinas
et al., 2013), linear SVMs (Fanello et al., 2013), neuro-fuzzy inference system networks (Al-
Jarrah and Halawani, 2001), hyper rectangular composite NNs (Su, 2000), and 3D Hopﬁeld
NN (Huang and Huang, 1998). Due to the ability of modeling temporal signals, Hidden
Markov Model (HMM) is possibly the most well known classiﬁer for gesture recognition.
Bauer (Bauer and Kraiss, 2002) proposes a 2D motion model and performs gesture recog-
nition with HMM. Vogler (2003) presents a parallel HMM algorithm to model gestures,
which can recognize continuous gestures. Fang et al. (2004) proposes a self-organizing fea-
ture maps/hidden Markov model (SOFM/HMM) for gesture recognition in which SOFM
is used as an implicit feature extractor for continuous HMM. Recently, Wan et al. (2012a)
proposes ScHMM to deal with the gesture recognition where sparse coding is adopted to
ﬁnd succinct representations and Lagrange dual is applied to obtain a codebook.

2.2 One-shot Learning Gesture Recognition and Gesture Characters

Although a large number of work has been done, gesture recognition is still very challenging
and has been attracting increasing interests. One motivation is to overcome the well-known
overﬁtting problem when training samples are insuﬃcient. The other one is to further
improve gesture recognition by developing novel features and classiﬁers.

231

Jiang, Zhang, Wu, Gao and Zhao

In the case of training samples being insuﬃcient, most of classiﬁcation methods are very
likely to overﬁt. Therefore, developing gesture recognition methods that use only a small
training data set is necessary. An extreme example is the one-shot learning that uses only
one training sample per class for training. The proposed work in this paper is also for one-
shot learning. In the literature, several previous work has been focused on one-shot learning.
In Lui (2012a), gesture sequences are viewed as third-order tensors and decomposed to three
Stiefel Manifolds and a natural metric is inherited from the factor manifolds. A geometric
framework for least square regression is further presented and applied to gesture recognition.
Mahbub et al. (2013) proposes a space-time descriptor and applies Motion History Imaging
(MHI) techniques to track the motion ﬂow in consecutive frames. The Euclidean distance
based classiﬁers is used for gesture recognition. Seo and Milanfar (2011) presents a novel
action recognition method based on space-time locally adaptive regression kernels and the
matrix cosine similarity measure. Malgireddy et al. (2012) presents an end-to-end temporal
Bayesian framework for activity classiﬁcation. A probabilistic dynamic signature is created
for each activity class and activity recognition becomes a problem of ﬁnding the most
likely distribution to generate the test video. Escalante et al. (2013) introduces principal
motion components for one-shot learning gesture recognition. 2D maps of motion energy
are obtained per each pair of consecutive frames in a video. Motion maps associated to a
video are further processed to obtain a PCA model, which is used for gesture recognition
with a reconstruction-error approach. More one-shot learning gesture recognition methods
are summarized by Guyon et al. (2013).
The intrinsic diﬀerence between gesture recognition and other recognition problems is
that gesture communication is highly complex and owns its unique characters. Therefore, it
is crucial to develop speciﬁed features and classiﬁers for gesture recognition by exploring the
unique characters of gestures as explained in Section 1. There are some eﬀorts toward this
direction and some work has modeled the component concurrent or sequential organization
and achieved signiﬁcant progress. To capture meaningful linguistic components of gestures,
Vogler and Metaxas (1999) proposes PaHMMs which models the movement and shape of
user’s hands in independent channels and then put them together at the recognition stage.
Chen and Koskela (2013) uses multiple Extreme Learning Machines (ELMs) (Huang et al.,
2012) as classiﬁers for simultaneous components. The outputs from the multiple ELMs
are then fused and aggregated to provide the ﬁnal classiﬁcation results. Chen and Koskela
(2013) proposes a novel representation of human gestures and actions based on component
concurrent character. They learn the parameters of a statistical distribution that describes
the location, shape, and motion ﬂow. Inspired by the sequential organization character of
gestures, Wang et al. (2002) uses the segmented subsequences instead of the whole gesture
sequence as the basic units that convey the speciﬁc semantic expression for the gesture and
encode the gesture based on these units. It is successfully applied in large vocabulary sign
gestures recognition.
To our best knowledge, there is no work in the literature modeling both the component
concurrent character and the sequential organization character in gesture recognition, espe-
cially for one-shot learning gesture recognition. It should be noted that these two characters
demonstrate the essences of gestures from spatial and temporal aspects, respectively. There-
fore, the proposed method that exploits both these characters in a multi-layered framework
is desirable to improve gesture recognition.

232

Multi-layered Gesture Recognition with Kinect

Test Avg. Acc. (%)

Identiﬁcation Strategy

Description

1
2
3
4
5
6

75.0
90.3
83.5
87.6
95.3
100.0

None
Motion
Shape
Location
Motion & Shape
Motion & Location & Shape

Memorizing all the training gestures, and identifying test gesture by recollection
Drawing lines to record motion direction of each training gesture
Drawing sketches to describe the hand shape of each training gesture
Drawing sketches to describe the location of each training gesture
Strategy 2 and 3
Strategy 2, 3 and 4

Table 1: Observations on CGD 2011.

3. Multi-layered Gesture Recognition

The proposed multi-layered classiﬁcation framework for one-shot learning gesture recogni-
tion contains three layers as shown in Figure 1. In the ﬁrst layer, an improved principle
motion is applied to model the motion component. In the second layer, a particle based
descriptor is proposed to extract dynamic gesture information and then a weighted dynamic
time warping is proposed for the location component classiﬁcation. In the last layer, we
extract unclosed shape contour from the key frame of a gesture sequence. Spatial path
warping is further proposed to recognize the shape component. Once the motion com-
ponent classiﬁcation at the ﬁrst layer is accomplished, the original gesture candidates are
divided into possible gesture candidates and impossible gesture candidates. The possible
gesture candidates are then fed to the second layer which performs the location compo-
nent classiﬁcation. Compared with the original gesture candidates, classifying the possible
gesture candidates is expected to reduce the computational complexity of the second layer
distinctly. The possible gesture candidates are further reduced by the second layer. In the
reduced possible gesture candidates, if the ﬁrst two best matched candidates are diﬃcult
to be discriminated, i.e., the absolute diﬀerence of their matching scores is lower than a
predeﬁned threshold, then the reduced gesture candidates are forwarded to the third layer;
otherwise the best matched gesture is output as the ﬁnal recognition result.
In the remaining of this section, the illuminating cues are ﬁrst observed in Section 3.1.
Inter-gesture segmentation is then introduced in Section 3.2. The motion, location and
shape component classiﬁers in each layer are ﬁnally introduced in Section 3.3, Section 3.4
and Section 3.5, respectively.

3.1 Gesture Meaning Expressions and Illuminating Cues

Although from the point of view of gesture linguistics, the basic components and how
gestures convey meaning are given (Stokoe, 1960), there is no reference to the importance
and complementarity of the components in gesture communication. This section wants
to draw some illuminating cues from observations. For this purpose, 10 undergraduate
volunteers are invited to take part in the observations.
Five batches of data are randomly selected from the development data of CGD 2011. The
pre-deﬁned identiﬁcation strategies are shown in Table 1. In each test, all the volunteers are
asked to follow these identiﬁcation strategies. For example, in Test 2, they are required to
only use the motion cue and draw simple lines to record the motion direction of each gesture
in the training set. Then the test gestures are shown to the volunteers to be identiﬁed using
these drawn lines. The results are brieﬂy summarized in Table 1.
From the observations above, the following illuminating cues can be drawn:

233

Jiang, Zhang, Wu, Gao and Zhao

• During gesture recognition, gesture components in the order of importance are motion,
location and shape.
• Understanding a gesture requires the observation of all these gesture components.
None of these components can convey the complete gesture meanings independently.
These gesture components complement each other.

3.2 Inter-gesture Segmentation Based on Movement Quantity

The inter-gesture segmentation is used to segment a multi-gesture sequence into several ges-
ture sequences.1 To perform the inter-gesture segmentation, we ﬁrst measure the quantity
of movement for each frame in a multi-gesture sequence and then threshold the quantity
of movement to get candidate boundaries. Then, a sliding window is adopted to reﬁne the
candidate boundaries to produce the ﬁnal boundaries of the segmented gesture sequences
in a multi-gesture sequence.

3.2.1 Quantity of Movement

In a multi-gesture sequence, each frame has the relevant movement with respect to its
adjacent frame and the ﬁrst frame. These movements and their statistical information are
useful for inter-gesture segmentation. For a multi-gesture depth sequence I , the Quantity
of Movement (QOM ) for frame t is deﬁned as a two-dimensional vector

QOM (I , t) = [QOMLocal (I , t), QOMGlobal (I , t)] ,

QOMLocal (I , t) =

σ(It (m, n), It−1 (m, n)) ,

where QOMLocal (I , t) and QOMGlobal (I , t) measure the relative movement of frame t re-
spective to its adjacent frame and the ﬁrst frame, respectively. They can be computed
(cid:88)
as
(cid:88)
m,n
QOMGlobal (I , t) =
σ(It (m, n), I1 (m, n)) ,
m,n
(cid:26) 1 if |x − y | ≥ T hresholdQOM
where (m, n) is the pixel location and the indicator function σ(x, y) is deﬁned as
0 otherwise
where T hresholdQOM is a predeﬁned threshold, which is set to 60 empirically in this paper.

σ(x, y) =

,

3.2.2 Inter-gesture Segmentation

We assume that there is a home pose between a gesture and another one in a multi-gesture
sequence. The inter-gesture segmentation is facilitated by the statistical characteristics of
QOMGlobal of the beginning and ending phases of the gesture sequences in the training

1. In this paper, we use the term “gesture sequence” to mean an image sequence that contains only one
complete gesture and “multi-gesture sequence” to mean an image sequence which may contain one or
multiple gesture sequences.

234

Multi-layered Gesture Recognition with Kinect

data. One advantage of using QOMGlobal is that it does not need to segment the user from
the background.
Firstly the average frame number L of all gestures in the training set is obtained. The
mean and standard deviation of QOMGlobal of the ﬁrst and last (cid:100)L/8(cid:101) frames of each gesture
sequence are computed. After that, a threshold T hresholdinter is obtained as the sum of
the mean and the doubled standard deviation. For a test multi-gesture sequence T which
has ts frames, the inter-gesture boundary candidate set is deﬁned as

B ca
inter = {i|QOMGlobal (T , i) ≤ T hresholdinter , i ∈ {1, · · · , ts}} .
The boundary candidates are further reﬁned through a sliding window of size (cid:100)L/2(cid:101),
deﬁned as {j + 1, j + 2, · · · , j + (cid:100)L/2(cid:101)} where j starts from 0 to ts − (cid:100)L/2(cid:101). In each sliding
window, only the candidate with the minimal QOMGlobal is retained and other candidates
are eliminated from B ca
inter . After the sliding window stops, the inter-gesture boundaries
are obtained, which are exempliﬁed as the blue dots in Figure 2. The segmented gesture
sequences will be used for motion, location, and shape component analysis and classiﬁcation.

Figure 2: An example of illustrating the inter-gesture segmentation results.

3.3 Motion Component Analysis and Classiﬁcation

Owing to the relatively high importance of the motion component, it is analyzed and clas-
siﬁed in the ﬁrst layer. The principal motion (Escalante and Guyon, 2012) is improved by
using the overlapping block partitioning to reduce the errors of motion pattern mismatch-
ings. Furthermore, our improved principal motion uses both the RGB and depth images.
The gesture candidates outputted by the ﬁrst layer is then fed to the second layer.

3.3.1 Principal Motion

Escalante and Guyon (2012) uses a set of histograms of motion energy information to
represent a gesture sequence and implements a reconstruction based gesture recognition
method based on principal components analysis (PCA). For a gesture sequence, motion
energy images are calculated by subtracting consecutive frames. Thus, the gesture sequence
with N frames is associated to N − 1 motion energy images. Next, a grid of equally spaced

235

010203040506070809010011012000.20.40.60.81  QOMGlobalSeg.BoundaryQOMGlobalThresholdinterJiang, Zhang, Wu, Gao and Zhao

blocks is deﬁned over each motion energy image as shown in Figure 3(c). For each motion
energy image, the average motion energy in each of the patches of the grid is computed by
averaging values of pixels within each patch. Then a 2D motion map for each motion energy
image is obtained and each element of the map accounts for the average motion energy of
the block centered on the corresponding 2D location. The 2D map is then vectorized into
an Nb -dimensional vector. Hence, an N frame gesture sequence is associated to a matrix Y
of dimensions (N − 1) × Nb . All gestures in the reference set with size V can be represented
with matrices Yv , v ∈ {1, · · · , V } and PCA is applied to each Yv . Then the eigenvectors
corresponding to the top c eigenvalues form a set Wv , v = {1, · · · , V }.
In the recognition stage, each test gesture is processed as like training gestures and
represented by a matrix S . Then, S is pro jected back to each of the V spaces induced
by Wv , v ∈ {1, · · · , V }. The V reconstructions of S are denoted by R1 , · · · , RV . The
(cid:118)(cid:117)(cid:117)(cid:116) m(cid:88)
reconstruction error of each Rv is computed by
n(cid:88)
i=1
j=1
where n and m are the number of rows and columns of S . Finally, the test gesture is
recognized as the gesture with label obtained by arg minv ε(v).

(Rv (i, j ) − S (i, j ))2 ,

ε(v) =

1
n

3.3.2 Improved Principle Motion

Gestures with large movements are usually performed with signiﬁcant deformation as shown
in Figure 3. In Escalante and Guyon (2012), motion information is represented by a his-
togram whose bins are related to spatial positions. Each bin is analyzed independently and
the space interdependency among the neighboring bins is not further considered. The inter-
dependency can be explored to improve the robustness of representing the gesture motion
component, especially for the gestures with larger movement. To this end, an overlapping
neighborhood partition is proposed. For example, if the size of bins is 20 × 20, the over-
lapping neighborhood contains 3 × 3 equally spaced neighboring bins in a 60 × 60 square
region. The averaged motion energy in the square region is taken as the current bin’s value
as shown in Figure 3.
The improved principle motion is applied to both the RGB and depth data. The RGB
images are transformed into gray images before computing their motion energy images. For
each reference gesture, the ﬁnal V reconstruction errors are obtained by multiplying the
reconstruction errors of the depth data and the gray data. These V reconstruction errors
are further clustered by K-means to get two centers. The gesture labels associated to those
reconstruction errors belonging to the center with smaller value are treated as the possible
gesture candidates. The remaining gesture labels are treated as the impossible gesture
candidates. Then the possible candidates are fed to the second layer.
We compare the performance of our improved principal motion model with the original
principal motion model (Escalante and Guyon, 2012) on the ﬁrst 20 development batches
of CGD 2011. Using the provided code (Guyon et al., 2014; Escalante and Guyon, 2012) as
baseline, the average Levenshtein distances (Levenshtein, 1966) are 44.92% and 38.66% for
the principal motion and the improved principal motion, respectively.

236

Multi-layered Gesture Recognition with Kinect

(a)

(b)

(c)

(d)

Figure 3: An example of a gesture with large movements. (a) and (b): two frames from
a gesture.
(c): the motion energy image of (a). The grid of equally spaced
bins adopted by the Principle Motion (Escalante and Guyon, 2012). (d): the
motion energy image of (b). The overlapped grid used by our method where the
overlapping neighborhood includes all 3 × 3 equally spaced neighbor bins.

3.4 Location Component Analysis and Classiﬁcation

Gesture location component refers to the positions of the arms and hands relative to the
body.
In the second layer, the sequential organization character of gestures is utilized
in the gesture sequence alignment. According to the movement-hold model, each gesture
sequence is segmented into semantic units, which convey the speciﬁc semantic meanings
of the gesture. Accordingly, when aligning a reference gesture and a test gesture, the
semantic units are aligned ﬁrst, then the frames in each semantic unit are registered. A
particle-based representation for the gesture location component is proposed to describe
the location component of the aligned frames and a Weighted Dynamic Time Warping
(WDTW) is proposed for the location component classiﬁcation.

3.4.1 Intra-gesture Segmentation and Alignment

To measure the distance between location components of a reference gesture sequence R =
{R1 , R2 · · · , RLR } and a test gesture sequence T = {T1 , T2 · · · , TLT }, an alignment Γ =
{(ik , jk )|k = 1, · · · , K, ik ∈ {1, · · · , LR }, jk ∈ {1, · · · , LT }} can be determined by the best
path in the Dynamic Time Warping (DTW) grid and K is the path length. Then the
dissimilarity between two gesture sequences can be obtained as the sum of the distances
between the aligned frames.
The above alignment does not consider the sequential organization character of gestures.
The movement-hold model proposed by Liddell and Johnson (1989) reveals sequential or-
ganization of gestures, which should be explored in the analysis and classiﬁcation of gesture
location component. QOMLocal (I , t), described in Section 3.2.1, measures the movement
between two consecutive frames. A large QOMLocal (I , t) indicates that the t-th frame is
in a movement phase, while a small QOMLocal (I , t) indicates that the frame is in a hold
phase. Among all the frames in a hold phase, the one with the minimal QOMLocal (I , t)
is the most representative frame and is marked as an anchor frame. Considering the se-
quential organization character of gestures, the following requirement should be satisﬁed to
compute Γ: each anchor frame in a test sequence must be aligned with one anchor frame in
the reference sequence.

237

Jiang, Zhang, Wu, Gao and Zhao

Figure 4: Intra-gesture segmentation and the alignment between test and reference se-
quences.

As shown in Figure 4, the alignment between the test and reference sequences has two
the associated best path (cid:98)Γ = {( (cid:98)ik , (cid:98)jk )|k = 1, · · · , (cid:98)K } in the DTW grid can be obtained. For
stages. In the ﬁrst stage, DTW is applied to align the reference and test sequences. Each
each ( (cid:98)ik , (cid:98)jk ), if both (cid:98)ik and (cid:98)jk are anchor frames, then (cid:98)ik and (cid:98)jk are the boundaries of the
anchor frame is represented by “1” and the remaining frames are represented by “0”. Then
semantic units. According to the boundaries, the alignment between semantic units of the
reference and test sequences is obtained. In the second stage, as shown in Figure 4, each
frame in a semantic unit is represented by [QOMLocal , QOMGlobal ] and DTW is applied
to align the semantic unit pairs separately. Then the ﬁnal alignment Γ is obtained by
concatenating the alignments of the semantic unit pairs.

3.4.2 Location Component Segmentation and its Particle Representation

After the frames of the test and reference sequences are aligned, the next problem is how
to represent the location information in a frame. Dynamic regions in each frame contain
the most meaningful location information, which are illustrated in Figure 5(i).
A simple thresholding-based foreground-background segmentation method is used to
segment the user in a frame. The output of the segmentation is a mask frame that indicates
which pixels are occupied by the user as shown in Figure 5(b). The mask frame is then
denoised by a median ﬁlter to get a denoised frame as shown in Figure 5(c). The denoised
frame is ﬁrst binarized and then dilated with a ﬂat disk-shaped structuring element with
radius 10 as shown in Figure 5(d). The swing frame as shown in Figure 5(h) is obtained by
subtracting the binarized denoised frame from the dilated frame. The swing region (those

238

Reference Sequence R                            of RQOMLocal                  of TQOMLocalTest Sequence T          Anchor FramesAnchor FramesDynamic Time WarpingSubsequence AlignmentFrame AlignmentAlignment Between R and TReference Subsequence 1        QOMReference Subsequence N        QOM...Segmented Reference SequencesTest Subsequence 1        QOMTest Subsequence N        QOM...Segmented Test SequencesRepresenting R00010000100...Representing T00100000010...Multi-layered Gesture Recognition with Kinect

Figure 5: Dynamic region segmentation.

white pixels in the swing frame) covers the slight swing of user’s trunk and can be used to
eliminate the inﬂuence of body swing. From frame t, deﬁne set Ξ as

{(m, n)|F1 (m, n) − Ft (m, n) ≥ T hresholdQOM } ,
where F1 and Ft are the user masks of the ﬁrst frame and frame t, respectively.
T hresholdQOM is the same as in Section 3.2.1. For each connected region in Ξ, only if
the number of pixels in this region exceeds Np and the proportion overlapped with swing
region is less than r, it is regarded as a dynamic region. Here Np = 500 is a threshold
used to remove the meaningless connected regions in the diﬀerence frame as shown in Fig-
ure 5(g). If a connected region has less than Np pixels, we think this region should not be
a good dynamic region for extracting location features, e.g., the small bright region on the
right hand of the user in Figure 5(g). This parameter can be set intuitively. The parameter
r = 50% is also a threshold used to complement with Np to remove the meaningless con-
nected regions in the diﬀerence frame. After using Np to remove some connected regions,
there may be a retained connected region which has more than Np pixels but it may still
not be a meaningful dynamic region for extracting position features if the connected region
is caused by the body swing. Obviously we can exploit the swing region to remove such
a region. To do this, we ﬁrst compute the overlap rate between this region and the swing
region. If the overlap rate is larger than r, it is reasonable to think this region is mainly
produced by the body swing. Therefore, it should be further removed. As like Np , this
parameter is also very intuitive to set and is not very sensitive to the performance.
To represent the dynamic region of frame t, a particle-based description is proposed to
reduce the matching complexity. The dynamic region of frame t can be represented by a
3D distribution: Pt (x, y , z ) where x and y are coordinates of a pixel and z = It (x, y) is the
represented by a set of (cid:98)N particles, PLocation (It ) = {(xn , yn , zn )|
(cid:98)N
depth value of the pixel. In the form of non-parametric representation, Pt (x, y , z ) can be
cluster all pixels inside the dynamic region into (cid:98)N clusters. Note that for a pixel, both its
n=1}. We use K-means to
spatial coordinates and depth value are used. Then the centers of clusters are used as the
representative particles. In this paper, 20 representative particles are used for each frame,
as shown in Figure 6.

239

−First FrameForeground SegmentationForeground SegmentationCurrent FrameDenoised FrameDilated FrameDifference FrameSwing RegionDynamic Region−∩!"#!$#!%#!&#!’#!(#!)#!*#!+#Jiang, Zhang, Wu, Gao and Zhao

(a)

(b)

(c)

(d)

Figure 6: Four examples of particle representation of the location component (the black
dots are the particles pro jected onto X-Y plane).

3.4.3 Location component Classification
sets, P = {P1 , P2 · · · P (cid:98)N } and Q = {Q1 , Q2 · · · Q (cid:98)N }. The matching cost between particle Pi
Assume the location component of two aligned frames can be represented as two particle
and Qj , denoted by C (Pi , Qj ), is computed as their Euclidean distance. The distance of
the location component between these two aligned gesture frames is deﬁned by the minimal
distance between P and Q. Computing the minimal distance between two particle sets is
indeed to ﬁnd an assignment Π to minimize the cost summation of all particle pairs
(cid:98)N(cid:88)
i=1

Π = arg min
Π

C (Pi , QΠ(i) ) .

(1)

This is a special case of the weighted bipartite graph matching and can be solved by the
Edmonds method (Edmonds, 1965). Edmonds method which ﬁnds an optimal assignment
for a given cost matrix is an improved Hungarian method (Kuhn, 1955) with time complexity
O(n3 ) where n is the number of particles. Finally, the distance of the location component
between two aligned gesture frames is obtained
(cid:98)N(cid:88)
i=1

C (Pi , QΠ(i) ) .

dis(P , Q) =

The distance between the reference sequence R and the test sequence T can be computed
K(cid:88)
as the sum of all distance between the location components of the aligned frames in Γ
k=1

dis(PLocation (Rik ), PLocation (Tjk )) .

DI SLocation (R, T |Γ) =

(2)

This measurement implicitly gives all the frames the same weights. However, in many cases
gestures are distinguished by only a few frames. Therefore, rather than directly computing

240

Multi-layered Gesture Recognition with Kinect

Equation 2, we propose the Weighted DTW (WDTW) to compute the distance of location
K(cid:88)
component between R and T as
k=1

W R
ik × dis(PLocation (Rik ), PLocation (Tjk )) ,

W DI SLocation (R, T |Γ) =

where W R = {W R
ik |ik ∈ {1, · · · , LR }} is the weight vector. Diﬀerent from the method
of evaluating the phase diﬀerence between the test and reference sequences (Jeong et al.,
2011) and the method of assigning diﬀerent weights to features (Reyes et al., 2011), we
assign diﬀerent weights to the frames of the reference gesture sequence. For each reference
gesture sequence, ﬁrstly we use the regular DTW to calculate and record the alignment
Γ between the current reference gesture sequence and all the other reference gesture se-
quences. Secondly for each frame in the current reference gesture sequence, we accumulate
its corresponding distances with the matched frames in the best path in the DTW. Then,
the current frame is weighted by the average distance between itself and all the correspond-
ing frames in the best path. The detailed procedure of computing the weight vector are
summarized in Algorithm 1.

Figure 7: Weighted Dynamic Time Warping framework.

In the second layer, we ﬁrst use K-means to cluster the input possible gesture candidates
into two cluster centers according to the matching scores between the test gesture sequence
and the possible gesture candidates. The candidates in the cluster with smaller matching
score are discarded. In the remaining candidates, if the ﬁrst two best matched candidates
are diﬃcult to be distinguished, i.e., the absolute diﬀerence of their normalized location
component distances is lower than a predeﬁned threshold , then these candidates are
forwarded to the third layer; otherwise the best matched candidate is output as the ﬁnal
recognition result. Two factors inﬂuence the choice of the parameter . The ﬁrst one is
the number of the gesture candidates and the other one is the type of gestures. When the
number of the gesture candidates is large or most of the gesture candidates are the shape

241

Reference Sequence R          Test Sequence T          Alignment between R and TParticle Representation of Location Component of RParticle Representation of Location Component of TBipartite Graph MatchingLocation Component Distance of Reference Sequence and Test SequenceWeight Vector           WRJiang, Zhang, Wu, Gao and Zhao

Algorithm 1 Computing weight vector W R for a reference R
Input: all the O reference gesture depth sequences: I 1 , I 2 , · · · , I O
Output: weight vector for R, W R = {W R
m |m ∈ {1, · · · , LR }}
1: for each m ∈ [1, LR ] do
2: W R
m = 0
3: N R
m = 0
4: end for
5: for each n ∈ [1, O] do
m + (cid:80)
Compute the alignment Γ = {(ik , jk )} between R and I n
6:
m + (cid:80)
for each m ∈ [1, LR ] do
7:
(ik=m,jk )∈Γ dis(PLocation (Rik ), PLocation (I n
W R
m = W R
jk ))
8:
N R
m = N R
(ik ,jk )∈Γ δ(ik = m)
9:
if n = O then
10:
(cid:30)N R
W R
m = W R
11:
m
m
end if
12:
end for
13:
14: end for

dominant gestures, a high threshold is preferred. In our experiments, we empirically set its
value with 0.05 by observing the matching scores between the test sample and each gesture
candidates.

3.5 Shape Component Analysis and Classiﬁcation

The shape in a hold phase is more discriminative than the one in a movement phase. The key
frame in a gesture sequence is deﬁned as the frame which has the minimization QOMLocal .
Shape component classiﬁer classiﬁes the shape features extracted from the key frame of
a gesture sequence using the proposed Spatial Path Warping (SPW), which ﬁrst extracts
unclosed shape context (USC) features and then calculates the distance between the USCs
of the key frames in the reference and the test gesture sequences. The test gesture sequence
is classiﬁed as the gesture whose reference sequence has the smallest distance with the test
gesture sequence.

3.5.1 Unclosed Shape Segmentation

The dynamic regions of a frame have been obtained in Section 3.4.2. In a key frame, the
largest dynamic region D is used for shape segmentation. Although shapes are complex
and do not have robust texture and structured appearance, in most cases shapes can be
distinguished by their contours. The contour points of D are extracted by the Canny
algorithm. The obtained contour point set is denoted by C1 as shown in Figure 8(a). K-
means is adopted to cluster the points in D into two clusters based on the image coordinates
and depth of each point. If a user faces to the camera, the cluster with smaller average
depth contains most of information for identifying the shape component. Canny algorithm
(cid:84) C1 as shown in Figure 8(c),
is used again to extract contour points of the cluster with smaller average depth. The
obtained closed contour point set is denoted by C2 as shown in Figure 8(b). Furthermore,
an unclosed contour point set can be obtained by C3 = C2
which will be used to reduce the computational complexity of matching shapes.

242

Multi-layered Gesture Recognition with Kinect

(a)

(b)

(c)

(d)

Figure 8: Unclosed shape segmentation and context representation. (a) is an example of
point set C1 , (b) is an example of point set C2 and (c) is an example of obtained
point set C3 ; (d) is the log-polar space used to decide the ranges of K bins.

3.5.2 Shape Representation and Classification
The contour of a shape consists of a 2-D point set P = {p1 , p2 , · · · , pN }. Their relative posi-
tions are important for the shape recognition. From the statistical point of view, Belongie
et al. (2002) develops a strong shape contour descriptor, namely Shape Context (SC). For
each point pi in the contour, a histogram hpi is obtained as the shape context of the point
whose k-th bin is calculated by
hpi (k) = (cid:93){(pj − pi ) ∈ bin(k)|pj ∈ P, i (cid:54)= j, k ∈ {1, · · · , K }} ,
where bin(k) deﬁnes the quantiﬁcation range of the k-th bin. The log-polar space for bins
is illustrated in Figure 8(d).
Assume P and Q are the point sets for the shape contours of two key frames, the matching
cost Φ(pi , qj ) between two points pi ∈ P and qj ∈ Q is deﬁned as
K(cid:88)
[hpi (k) − hqj (k)]2
1
2
hpi (k) + hqj (k)
k=1
Given the set of matching costs between all pairs of points pi ∈ P and qj ∈ Q, computing
(cid:88)
the minimal distance between P and Q is to ﬁnd a permutation Ψ to minimize the following
sum
i

Ψ = arg min
Ψ

Φ(pi , qΨ(i) ) ,

Φ(pi , qj ) =

.

which can also be solved by the Edmonds algorithm as like solving Equation 1.
An unclosed contour contains valuable spatial information. Thus, a Spatial Path Warp-
ing algorithm (SPW) is proposed to compute the minimal distance between two unclosed
contours. Compared with the Edmonds algorithm, the time complexity of the proposed
SPW is reduced from O(n3 ) to O(n2 ) where n is the size of the point set of an unclosed
shape contour. As shown in Figure 8(c), the points on an unclosed contour can be repre-
sented as a clockwise contour point sequence. SPW is used to obtain the optimal match
between two given unclosed contour point sequences. For two unclosed contour point se-
quences {p(cid:48)
1 , · · · , q (cid:48)
n}, {q (cid:48)
1 , · · · , p(cid:48)
m}, a dynamic window is set to constrain the points that
one point can match, which makes the matching more robust to local shape variation. We

243

Jiang, Zhang, Wu, Gao and Zhao

set the window size w with max(Ls , abs(n − m)). In most cases, the window size is the
absolute diﬀerence between the lengths of the two point sequences. In extreme cases, if two
sequences have very close lengths, i.e., their absolute diﬀerence is less then Ls , we set the
the window size with Ls . The details of proposed SPW are summarized in Algorithm 2.

Algorithm 2 Computing distance between two unclosed contour point sequences
Input: two unclosed contour point sequences {p(cid:48)
n}, {q (cid:48)
1 , · · · , p(cid:48)
1 , · · · , q (cid:48)
m }
Output: distance between these two point sequences SP W [n, m].
1: Set w = max(Ls , abs(n − m))
2: for each i ∈ [0, n] do
for each j ∈ [0, m] do
3:
SP W [i, j ] = ∞
4:
end for
5:
6: end for
7: SP W [0, 0] = 0
8: for each i ∈ [1, n] do
for each j ∈ [max(1, i − w), min(m, i + w)] do
9:
SP W [i, j ]=Φ(p(cid:48)
i , q (cid:48)
j ) + min(SP W [i − 1, j ], SP W [i, j − 1], SP W [i − 1, j − 1])
10:
end for
11:
12: end for

4. Experiments

In this section, extensive experiment results are presented to evaluate the proposed multi-
layered gesture recognition method. All the experiments are performed in Matlab 7.12.0 on
a Dell PC with Duo CPU E8400. The ChaLearn Gesture Dataset (CGD 2011) (Guyon et al.,
2014) is used in all experiments, which is designed for one-shot learning. The CGD 2011
consists of 50,000 gestures (grouped in 500 batches, each batch including 47 sequences and
each sequence containing 1 to 5 gestures drawn from one of 30 small gesture vocabularies
of 8 to 15 gestures), with frame size 240 × 320, 10 frames/second, recorded by 20 diﬀerent
users.
The parameters used in the proposed method are listed in Table 2. Noted that the
parameters c and Nb are set with the default values used in the sample code of the principal
model.2 The threshold for foreground and background segmentation is adaptively set to
the maximal depth minus 100 for each batch data. For example, the maximal depth of the
devel01 batch is 1964. Then the threshold for this batch is 1864. The number 100 is in
fact a small bias from the maximal depth, which is empirically set in our experiments. We
experiments, we empirically set (cid:98)N to 20, which achieves the desired recognition performance.
observed that slightly changing this number does not signiﬁcantly aﬀect the segmentation.
Considering the tradeoﬀ between the time complexity and recognition accuracy, in our
In our experiments, Levenshtein distance is used to evaluate the gesture recognition
performance, which is also used in the CHALERAN gesture challenge. It is the minimum
number of edit operations (substitution, insertion, or deletion) that have to be performed
from one sequence to another (or vice versa). It is also known as “edit distance”.

2. The code is available at http://gesture.chalearn.org/data/sample- code

244

Multi-layered Gesture Recognition with Kinect

Parameter and Description

Applied to

Value

From Prior
or Not

Sensitive to
Performance

Training Data
Used or Not

D
500
Np : Minimal number of pixels in a connected region
50%
D
r: Maximal overlap rate between a connected region and the swing region
D, E
0.05
: Threshold for the diﬀerence between the ﬁrst two largest matches
E
5
Ls : Minimal length of the sliding window
A, D, E
60
T hresholdQOM
A
adaptive
T hresholdinter
C
(cid:98)N : number of particles
10
c: number of eigenvalues for each gesture
C
192
Nb : number of bins for each motion energy image
D
20
D, E
Max depth - 100
Threshold for foreground and background segmentation
A: Inter-gesture segmentation; B: intra-gesture segmentation; C: Motion component analysis and classiﬁcation
D: Location component analysis and classiﬁcation; E: Shape component analysis and classiﬁcation; Training data: CGD 2011

Y
N
Y
N
Y
N
Y
Y
Y
Y

N
N
N
N
Y
Y
N
N
N
N

Y
N
Y
N
N
Y
N
N
N
Y

Table 2: The parameters used in the proposed multi-layered gesture recognition and their
descriptions.

4.1 Performance of Our Method with Diﬀerent Layers

We evaluate the performance of the proposed method with diﬀerent layers on the develop-
ment (devel01 ∼ devel480) batches of CGD 2011 and Table 3 reports the results. If only
the ﬁrst layer is used for classiﬁcation, the average Levenhstein distance is 37.53% with
running time 0.54 seconds per gesture. If only the second layer is used for recognition, the
average Levenhstein distance is 29.32% with running time 6.03 seconds per gesture. If only
the third layer is used, the average Levenhstein distance is 39.12% with the running time
6.64 seconds per gesture. If the ﬁrst two layers are used, the average Levenhstein distance is
24.36% with running time 2.79 seconds per gesture. If all three layers are used, the average
normalized Levenhstein distance is 19.45% with running time 3.75 seconds per gesture.

methods

First layer
for recognition

Second layer
for recognition

Third layer
for recognition

First two layers
for recognition

Three layers
for recognition

TeLev (%)
Recognition time
per gesture (s)

37.53

0.54

29.32

6.03

39.12

6.64

24.36

2.79

19.45

3.75

Table 3: Performance of using the ﬁrst layer, the second layer, the third layer, ﬁrst two
layers and three layers on ChaLearn gesture data set (devel01 ∼ devel480).

From these comparison results, we can see that the proposed method achieves high
recognition accuracy while having low computational complexity. The ﬁrst layer can identify
the gesture candidates at the speed of 80 fps (frames per second). The second layer has
relatively high computational complexity. If we only use the second layer for classiﬁcation,
the average computing time is roughly 11 times of the ﬁrst layer. Despite with relatively
high computational cost, the second layer has stronger classiﬁcation ability. Compared
with using only the second layer, the computational complexity of using the ﬁrst two layers
in the proposed method is distinctly reduced and can achieve 16 fps. The reason is that
although the second layer is relatively complex, the gesture candidates forwarded to it are
signiﬁcantly reduced by the ﬁrst layer. When all three layers are used, the proposed method
still achieve about 12 fps, which is faster than the video recording speed (10 fps) of CGD
2011.

245

Jiang, Zhang, Wu, Gao and Zhao

4.2 Comparison with Recent Representative Methods

We compare the proposed method with other recent representative methods on the ﬁrst
20 development data batches. Table 4 reports the performance of the proposed method on
each batch and also the average performance on all 20 batches. The average performance
of the proposed method and the compared methods are shown in Table 5.

Batch

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Average

Second layer for recognition
Recognize time
per gesture (s)

TeLev (%)

First two layers for recognition
Recognize time
per gesture (s)

TeLev (%)

Three layers for recognition
Recognize time
per gesture (s)

TeLev (%)

7.24
41.21
62.98
4.51
11.68
44.64
12.44
5.56
10.56
44.21
42.75
8.56
16.24
44.69
15.78
36.54
36.25
62.4
54.31
17.74
29.02

6.78
11.38
8.86
5.98
10.96
5.59
3.59
4.94
5.10
5.88
6.46
5.16
3.68
2.50
4.61
8.35
9.10
1.99
5.07
2.58
5.93

0.11
44.21
69.20
3.93
2.62
39.94
8.51
0.00
6.44
29.13
36.36
1.06
12.93
40.13
4.21
36.27
29.55
69.21
51.32
10.61
24.79

3.40
7.10
2.99
2.10
3.05
2.69
1.70
2.14
2.50
3.24
3.98
2.00
1.20
0.90
1.09
4.21
5.10
0.81
2.84
1.40
2.73

1.11
34.35
39.95
6.93
4.77
23.51
8.51
5.71
6.44
16.52
28.93
7.06
12.93
27.98
6.21
23.41
26.32
53.55
47.61
10.61
19.62

3.59
10.00
5.61
2.30
3.31
3.42
1.79
2.94
3.01
3.95
6.31
2.34
1.99
2.35
2.19
6.94
5.39
1.60
3.02
2.01
3.69

Table 4: Recognition performance of using the second layer, ﬁrst two layers and three layers
on ﬁrst 20 development batches of CGD 2011 (TeLev is the average Levenshtein
distance).

Methods

TeLev (%)
TeLen

Extend-MHI
Wu et al. (2012a)

Manifold LSR
Lui (2012a)

Sparse Coding
Fanel lo et al. (2013)

Temporal Bayesian
Malgireddy et al. (2012)

Motion History
Mahbub et al. (2013)

CSMMI+3D EMoSIFT
Wan et al. (2014a)

Proposed

26.00
#

28.73
6.24

25.11
5.02

24.09
#

31.25
18.01

18.76
#

19.62
5.91

Table 5: Performance comparison on the 20 development data batches (TeLen is the average
error made on the number of gestures).

For the comparison on each batch, the proposed method is compared with a manifold
and nonlinear regression based method (Manifold LSR) (Lui, 2012b), an extended motion-
history-image and correlation coeﬃcient based method (Extended-MHI) (Wu et al., 2012a),
and a motion silhouettes based method (Motion History) (Mahbub et al., 2013). The
comparison results are shown in Figure 9.
In batches 13, 14, 17, 18, 19, the proposed method does not achieve the best performance.
However, the proposed method achieves the best performance in the remaining 15 batches.

246

Multi-layered Gesture Recognition with Kinect

Figure 9: Performance comparison on the 20 development batches in CGD 2011.

In batches 3, 10 and 11, most of gestures consist of static shapes, which can be eﬃciently
identiﬁed by the shape classiﬁer in the third layer. Batches 1, 4, 7 and 8 consist of motion
dominated gestures, which can be classiﬁed by the motion and location component classiﬁers
in the ﬁrst and second layers. In batches 18 and 19, the proposed method has relatively
poor performance. As in batch 18, most of gestures have small motion, similar locations,
and non-stationary hand shapes. These gestures may be diﬃcult to be identiﬁed by the
proposed method. In batch 19, the gestures have similar locations and hands coalescence,
which is diﬃcult to be identiﬁed by the second layer and the third layer classiﬁers in our
method. Overall, the proposed method signiﬁcantly outperforms other recent competitive
methods.
The proposed method is further compared with DTW, continuous HMM (CHMM), semi-
continuous HMM (SCHMM) and SOFM/HMM (Fang et al., 2004) on the development
(devel01 ∼ devel480) batches of CGD 2011. All compared methods use one of three feature
descriptors including dynamic region grid representation (DP), dynamic region particle
representation (DG) and Dynamic Aligned Shape Descriptor (DS) (Forn´es et al., 2010).

• Dynamic region grid representation. For the dynamic region of the current frame
obtained in Section 3.4.2, a grid of equally spaced cells is deﬁned and the default size
of grid is 12 × 16. For each cell, the average value of depth in the square region is
taken as the value of current bin. So a 12 × 16 matrix is generated, which is vectorized
into the feature vector of the current frame.
• Dynamic region particle representation. The particles for the current frame
obtained in Section 3.4.2 cannot directly be used as an input feature vector and
they have to be reorganized. The 20 particles {(xn , yn , zn )|20
n=1} are sorted according
to (cid:107)(xn , yn )(cid:107)2 and then the sorted particles are concatenated in order to get a 60-
dimensional feature vector to represent the current frame.

247

123456789101112131415161718192001020304050607080Batch NumberTeLev  ProposedManifold LSRExtended−MHIMotion HistoryJiang, Zhang, Wu, Gao and Zhao

• Dynamic region D-Shape descriptor (Forn´es et al., 2010). Firstly, the location
of some concentric circles is deﬁned, and for each one, the locations of the equidistant
voting points are computed. Secondly, these voting points will receive votes from the
pixels of the shape of the dynamic region, depending on their distance to each voting
point. By locating isotropic equidistant points, the inner and external part of the
shape could be described using the same number of voting points. In our experiment,
we used 11 circles for the D-Shape descriptor. Once we have the voting points, the
descriptor vector is computed.

Here, each type of HMM is a 3-state left-to-right model allowing possible skips. For
CHMM and SCHMM, the covariance matrix is a diagonal matrix with all diagonal elements
being 0.2. The comparison results are reported in Table 6.

Method

Number of Mixtures
for each state

TeLev (%)

Recognition time
per gesture (s)
DS
DP DG DS DP DG

2.60
2.51
2.67
33.16
41.19
38.23
#
DTW
6.89
6.83
6.91
31.13
33.29
31.41
5
CHMM
6.79
6.75
6.82
29.35
32.92
31.01
30
SCHMM
SOFM/HMM
6.74
6.71
6.77
27.20
30.31
28.27
5
DP: dynamic region particle representation; DG: dynamic region grid representation
DS: dynamic region D-Shape descriptor

Table 6: Performance of diﬀerent sequence matching methods on 480 development batches
of CGD 2011.

Compared with these methods, the proposed method achieves the best performance.
Noted that in all compared methods, SOFM/HMM classiﬁer with the DS descriptor achieves
the second best performance. As explained in Section 1, sequentially modeling motion, po-
sition and shape components is very important for improving the performance of gesture
recognition. Except the proposed method, other compared methods do not utilize these
components. On the other hand, statistical models like CHMM, SCHMM and SOFM/HMM
need more training samples to estimate model parameters, which also aﬀect their perfor-
mance in the one-shot learning gesture recognition.

5. Conclusion

The challenges of gesture recognition lie in the understanding of the unique characters
and cues of gestures. This paper proposed a novel multi-layered gesture recognition with
Kinect, which is linguistically and perceptually inspired by the phonological model and
the movement-hold model. Together with the illuminating cues drawn from observations,
the component concurrent character and the sequential organization character of gestures
are all utilized in the proposed method. In the ﬁrst layer, an improved principle motion
is applied to model the gesture motion component. In the second layer, a particle based
descriptor is proposed to extract dynamic gesture information and then a weighted dynamic

248

Multi-layered Gesture Recognition with Kinect

time warping is proposed to classify the location component. In the last layer, the spatial
path warping is further proposed to classify the shape component represented by unclosed
shape context, which is improved from the original shape context but needs lower matching
time. The proposed method can obtain relatively high performance for one-shot learning
gesture recognition. Our work indicates that the performance of gesture recognition can be
signiﬁcantly improved by exploring and utilizing the unique characters of gestures, which
will inspire other researcher in this ﬁeld to develop learning methods for gesture recognition
along this direction.

Acknowledgments

We would like to acknowledge the editors and reviewers, whose valuable comments greatly
improved the manuscript. Specially, we would also like to thank Escalante and Guyon
who kindly provided us the principal motion source code and Microsoft Asian who kindly
provided two sets of Kinect devices. This work was supported in part by the Ma jor State
Basic Research Development Program of China (973 Program 2015CB351804) and the
National Natural Science Foundation of China under Grant No. 61272386, 61100096 and
61300111.

References

Tushar Agrawal and Subhasis Chaudhuri. Gesture recognition using motion histogram. In
Proceedings of the Indian National Conference of Communications, pages 438–442, 2003.

Omar Al-Jarrah and Alaa Halawani. Recognition of gestures in Arabic sign language using
neuro-fuzzy systems. Artiﬁcial Intel ligence, 133(1):117–138, 2001.

George Awad, Junwei Han, and Alistair Sutherland. A uniﬁed system for segmentation
and tracking of face and hands in sign language recognition. In Proceedings of the 18th
International Conference on Pattern Recognition, volume 1, pages 239–242, 2006.

Malek Baklouti, Eric Monacelli, Vincent Guitteny, and Serge Couvet. Intelligent assistive
exoskeleton with vision based interface. In Proceedings of the 5th International Conference
On Smart Homes and Health Telematics, pages 123–135, 2008.

Britta Bauer and Karl-Friedrich Kraiss. Video-based sign recognition using self-organizing
subunits. In Proceedings of the 16th International Conference on Pattern Recognition,
volume 2, pages 434–437, 2002.

Serge Belongie, Jitendra Malik, and Jan Puzicha. Shape matching and ob ject recognition
using shape contexts. IEEE Transactions on Pattern Analysis and Machine Intel ligence,
24(4):509–522, 2002.

Qin Cai, David Gallup, Cha Zhang, and Zhengyou Zhang. 3D deformable face tracking
In Proceedings of the 11th European Conference on
with a commodity depth camera.
Computer Vision, pages 229–242, 2010.

249

Jiang, Zhang, Wu, Gao and Zhao

Xi Chen and Markus Koskela. Online RGB-D gesture recognition with extreme learning
machines. In Proceedings of the 15th ACM International Conference on Multimodal In-
teraction, pages 467–474, 2013.

Helen Cooper, Brian Holt, and Richard Bowden. Sign language recognition.
Analysis of Humans, pages 539–562, 2011.

In Visual

Helen Cooper, Eng-Jon Ong, Nicolas Pugeault, and Richard Bowden. Sign language recog-
nition using sub-units. Journal of Machine Learning Research, 13:2205–2231, 2012.

Andrea Corradini. Real-time gesture recognition by means of hybrid recognizers. In Pro-
ceedings of International Gesture Workshop on Gesture and Sign Languages in Human-
Computer Interaction, pages 34–47, 2002.

Ross Cutler and Matthew Turk. View-based interpretation of real-time optical ﬂow for ges-
ture recognition. In Proceedings of the 10th IEEE International Conference and Work-
shops on Automatic Face and Gesture Recognition, pages 416–416, 1998.

Nasser Dardas. Real-time Hand Gesture Detection and Recognition for Human Computer
Interaction. PhD thesis, University of Ottawa, 2012.

Paul Doliotis, Alexandra Stefan, Chris Mcmurrough, David Eckhard, and Vassilis Athitsos.
Comparing gesture recognition accuracy using color and depth information. In Proceed-
ings of the 4th International Conference on PErvasive Technologies Related to Assistive
Environments, page 20, 2011.

Jack Edmonds. Maximum matching and a polyhedron with 0, 1-vertices. Journal of Re-
search of the National Bureau of Standards B, 69:125–130, 1965.

H Ershaed, I Al-Alali, N Khasawneh, and M Fraiwan. An Arabic sign language computer
interface using the Xbox Kinect. In Proceedings of the Annual Undergraduate Research
Conference on Applied Computing, volume 1, 2011.

Hugo Escalante and Isabelle Guyon. Principal motion. http://www.causality.inf.ethz.
ch/Gesture/principal_motion.pdf, 2012.

Hugo Jair Escalante, Isabelle Guyon, Vassilis Athitsos, Pat Jangyodsuk, and Jun Wan. Prin-
cipal motion components for gesture recognition using a single-example. arXiv preprint
arXiv:1310.4822, 2013.

Sean Ryan Fanello, Ilaria Gori, Giorgio Metta, and Francesca Odone. One-shot learning for
real-time action recognition. In Pattern Recognition and Image Analysis, pages 31–40,
2013.

Gaolin Fang, Wen Gao, and Debin Zhao. Large vocabulary sign language recognition based
on fuzzy decision trees. IEEE Transactions on Systems, Man and Cybernetics, Part A:
Systems and Humans, 34(3):305–314, 2004.

Alicia Forn´es, Sergio Escalera, Josep Llad´os, and Ernest Valveny. Symbol classiﬁcation using
dynamic aligned shape descriptor. In Proceedings of the 20th International Conference
on Pattern Recognition, pages 1957–1960, 2010.

250

Multi-layered Gesture Recognition with Kinect

Isabelle Guyon, Vassilis Athitsos, Pat Jangyodsuk, Hugo Jair Escalante, and Ben Hamner.
Results and analysis of the Chalearn gesture challenge 2012. In Proceedings of Interna-
tional Workshop on Advances in Depth Image Analysis and Applications, pages 186–204,
2013.

Isabelle Guyon, Vassilis Athitsos, Pat Jangyodsuk, and Hugo Jair Escalante. The
chalearn gesture dataset (CGD 2011). Machine Vision and Applications, 2014. DOI:
10.1007/s00138-014-0596-3.

Chung-Lin Huang and Wen-Yi Huang. Sign language recognition using model-based track-
ing and a 3D Hopﬁeld neural network. Machine Vision and Applications, 10(5-6):292–307,
1998.

Guang-Bin Huang, Hongming Zhou, Xiao jian Ding, and Rui Zhang. Extreme learning
IEEE Transactions on Systems,
machine for regression and multiclass classiﬁcation.
Man, and Cybernetics, Part B: Cybernetics, 42(2):513–529, 2012.

Young-Seon Jeong, Myong K Jeong, and Olufemi A Omitaomu. Weighted dynamic time
warping for time series classiﬁcation. Pattern Recognition, 44(9):2231–2240, 2011.

Timor Kadir, Richard Bowden, Eng Jon Ong, and Andrew Zisserman. Minimal training,
In Proceedings of the British
large lexicon, unconstrained sign language recognition.
Machine Vision Conference, volume 1, pages 1–10, 2004.

Harold W Kuhn. The Hungarian method for the assignment problem. Naval Research
Logistics Quarterly, 2(1-2):83–97, 1955.

Vladimir I Levenshtein. Binary codes capable of correcting deletions, insertions and rever-
sals. In Soviet Physics Doklady, volume 10, page 707, 1966.

Jeroen F Lichtenauer, Emile A Hendriks, and Marcel JT Reinders. Sign language recognition
IEEE Transactions on
by combining statistical DTW and independent classiﬁcation.
Pattern Analysis and Machine Intel ligence, 30(11):2040–2046, 2008.

Scott K Liddell and Robert E Johnson. American sign language: The phonological base.
Sign Language Studies, 64:195–278, 1989.

Yui Man Lui. Human gesture recognition on product manifolds. Journal of Machine Learn-
ing Research, 13(1):3297–3321, 2012a.

Yui Man Lui. A least squares regression framework on manifolds and its application to ges-
ture recognition. In Proceedings of the IEEE Computer Society Conference on Computer
Vision and Pattern Recognition Workshops, pages 13–18, 2012b.

Upal Mahbub, Tonmoy Roy, Md Shaﬁur Rahman, and Haﬁz Imtiaz. One-shot-learning
gesture recognition using motion history based gesture silhouettes. In Proceedings of the
International Conference on Industrial Application Engineering, pages 186–193, 2013.

251

Jiang, Zhang, Wu, Gao and Zhao

Manavender R Malgireddy, Ifeoma Inwogu, and Venu Govindara ju. A temporal Bayesian
model for classifying, detecting and localizing activities in video sequences. In Proceedings
of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition
Workshops, pages 43–48, 2012.

Manar Maraqa and Raed Abu-Zaiter. Recognition of arabic sign language (ArSL) using
recurrent neural networks. In Proceedings of the First International Conference on the
Applications of Digital Information and Web Technologies, pages 478–481, 2008.

Tin Hninn Hninn Maung. Real-time hand tracking and gesture recognition system using
neural networks. World Academy of Science, Engineering and Technology, 50:466–470,
2009.

Sushmita Mitra and Tinku Acharya. Gesture recognition: A survey. IEEE Transactions on
Systems, Man, and Cybernetics, Part C: Applications and Reviews, 37(3):311–324, 2007.

Kai Nickel and Rainer Stiefelhagen. Visual recognition of pointing gestures for human–robot
interaction. Image and Vision Computing, 25(12):1875–1884, 2007.

Iason Oikonomidis, Nikolaos Kyriazis, and Antonis Argyros. Eﬃcient model-based 3D
tracking of hand articulations using Kinect. In Proceedings of the British Machine Vision
Conference, pages 1–11, 2011.

Eng-Jon Ong and Richard Bowden. A boosted classiﬁer tree for hand shape detection. In
Proceedings of the Sixth IEEE International Conference on Automatic Face and Gesture
Recognition, pages 889–894, 2004.

Aditya Ramamoorthy, Namrata Vaswani, Santanu Chaudhury, and Subhashis Banerjee.
Recognition of dynamic hand gestures. Pattern Recognition, 36(9):2069–2081, 2003.

Ingmar Rauschert, Pyush Agrawal, Ra jeev Sharma, Sven Fuhrmann, Isaac Brewer, and
Alan MacEachren. Designing a human-centered, multimodal GIS interface to support
emergency management. In Proceedings of the 10th ACM International Symposium on
Advances in Geographic Information Systems, pages 119–124, 2002.

Zhou Ren, Junsong Yuan, Jingjing Meng, and Zhengyou Zhang. Robust part-based hand
gesture recognition using Kinect sensor. IEEE Transactions on Multimedia, 15(5):1110–
1120, 2013.

Miguel Reyes, Gabriel Dominguez, and Sergio Escalera. Feature weighting in dynamic time
warping for gesture recognition in depth data. In Proceedings of the IEEE International
Conference on Computer Vision Workshops, pages 1182–1188, 2011.

Yared Sabinas, Eduardo F Morales, and Hugo Jair Escalante. A One-Shot DTW-based
method for early gesture recognition. In Proceedings of 18th Iberoamerican Congress on
Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications,
pages 439–446, 2013.

Hae Jong Seo and Peyman Milanfar. Action recognition from one example. IEEE Trans-
actions on Pattern Analysis and Machine Intel ligence, 33(5):867–882, 2011.

252

Multi-layered Gesture Recognition with Kinect

Ling Shao and Ling Ji. Motion histogram analysis based key frame extraction for human
action/activity representation. In Proceedings of Canadian Conference on Computer and
Robot Vision, pages 88–92, 2009.

Jamie Shotton, Andrew W. Fitzgibbon, Mat Cook, Toby Sharp, Mark Finocchio, Richard
Moore, Alex Kipman, and Andrew Blake. Real-time human pose recognition in parts
from single depth images. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 1297–1304, 2011.

E Stergiopoulou and N Papamarkos. Hand gesture recognition using a neural network
shape ﬁtting technique. Engineering Applications of Artiﬁcial Intel ligence, 22(8):1141–
1158, 2009.

William C Stokoe. Sign language structure: An outline of the visual communication systems
of the american deaf. Studies in Linguistics, Occasional Papers, 8, 1960.

Mu-Chun Su. A fuzzy rule-based approach to spatio-temporal hand gesture recognition.
IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews,
30(2):276–281, 2000.

Christian Vogler and Dimitris Metaxas. Parallel hidden markov models for american sign
language recognition. In Proceedings of the Seventh IEEE International Conference on
Computer Vision, volume 1, pages 116–122, 1999.

Christian Philipp Vogler. American Sign Language Recognition: Reducing the Complexity of
the Task with Phoneme-based Modeling and Paral lel Hidden Markov Models. PhD thesis,
University of Pennsylvania, 2003.

J. Wachs, M. Kolsch, H. Stem, and Y. Edan. Vision-based hand-gesture applications.
Communications of the ACM, 54(2):60–71, 2011.

Jun Wan, Qiuqi Ruan, Gaoyun An, and Wei Li. Gesture recognition based on hidden
markov model from sparse representative observations. In Proceedings of the IEEE 11th
International Conference on Signal Processing, volume 2, pages 1180–1183, 2012a.

Jun Wan, Qiuqi Ruan, Gaoyun An, and Wei Li. Hand tracking and segmentation via
In Proceedings of IEEE 11th
graph cuts and dynamic model in sign language videos.
International Conference on Signal Processing, volume 2, pages 1135–1138. IEEE, 2012b.

Jun Wan, Qiuqi Ruan, Wei Li, and Shuang Deng. One-shot learning gesture recognition
from RGB-D data using bag of features. Journal of Machine Learning Research, 14(1):
2549–2582, 2013.

Jun Wan, Vassilis Athitsos, Pat Jangyodsuk, Hugo Jair Escalante, Qiuqi Ruan, and Isabelle
Guyon. CSMMI: Class-speciﬁc maximization of mutual information for action and gesture
recognition. IEEE Transactions on Image Processing, 23(7):3152–3165, 2014a.

Jun Wan, Qiuqi Ruan, Wei Li, Gaoyun An, and Ruizhen Zhao. 3D SMoSIFT: Three-
dimensional sparse motion scale invariant feature transform for activity recognition from
RGB-D videos. Journal of Electronic Imaging, 23(2):023017, 2014b.

253

Jiang, Zhang, Wu, Gao and Zhao

Chunli Wang, Wen Gao, and Shiguang Shan. An approach based on phonemes to large
vocabulary Chinese sign language recognition. In Proceedings of the IEEE Conference on
Automatic Face and Gesture Recognition, pages 411–416, 2002.

J. Wang, Z. Liu, Y. Wu, and J. Yuan. Mining actionlet ensemble for action recognition with
depth cameras. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1290–1297, 2012.

Shu-Fai Wong, Tae-Kyun Kim, and Roberto Cipolla. Learning motion categories using
In Proceedings of the IEEE Conference on
both semantic and structural information.
Computer Vision and Pattern Recognition, pages 1–6, 2007.

Di Wu, Fan Zhu, and Ling Shao. One shot learning gesture recognition from RGBD im-
ages. In Proceedings of the IEEE Computer Society Conference on Computer Vision and
Pattern Recognition Workshops, pages 7–12, 2012a.

Shen Wu, Feng Jiang, Debin Zhao, Shaohui Liu, and Wen Gao. Viewpoint-independent
hand gesture recognition system. In Proceedings of the IEEE Conference on Visual Com-
munications and Image Processing, pages 43–48, 2012b.

Morteza Zahedi, Daniel Keysers, and Hermann Ney. Appearance-based recognition of words
in american sign language. In Proceedings of Second Iberian Conference on Pattern Recog-
nition and Image Analysis, pages 511–519, 2005.

Liang-Guo Zhang, Yiqiang Chen, Gaolin Fang, Xilin Chen, and Wen Gao. A vision-based
sign language recognition system using tied-mixture density HMM. In Proceedings of the
6th International Conference on Multimodal Interfaces, pages 198–204, 2004.

254

Journal of Machine Learning Research 16 (2015) 553-557

Submitted 11/12; Revised 3/14; Published 3/15

The flare Package for High Dimensional Linear Regression
and Precision Matrix Estimation in R∗

Xingguo Li†
Department of Electrical and Computer Engineering
University of Minnesota Twin Cities
Minneapolis, MN, 55455, USA
Tuo Zhao†
Department of Computer Science
Johns Hopkins University
Baltimore, MD, 21210, USA
Xiaoming Yuan
Department of Mathematics
Hong Kong Baptist University
Hong Kong, China
Han Liu
Department of Operations Research and Financial Engineering
Princeton University
Princeton, NJ 08544, USA

Editor: Mikio Braun

lixx1661@umn.edu

tzhao5@jhu.edu

xmyuan@hkbu.edu.hk

hanliu@princeton.edu

Abstract
This paper describes an R package named flare, which implements a family of new high
dimensional regression methods (LAD Lasso, SQRT Lasso, (cid:96)q Lasso, and Dantzig selector)
and their extensions to sparse precision matrix estimation (TIGER and CLIME). These
methods exploit diﬀerent nonsmooth loss functions to gain modeling ﬂexibility, estimation
robustness, and tuning insensitiveness. The developed solver is based on the alternating
direction method of multipliers (ADMM). The package flare is coded in double precision
C, and called from R by a user-friendly interface. The memory usage is optimized by using
the sparse matrix output. The experiments show that flare is eﬃcient and can scale up
to large problems.
Keywords:
sparse linear regression, sparse precision matrix estimation, alternating di-
rection method of multipliers, robustness, tuning insensitiveness

1. Introduction

As a popular sparse linear regression method for high dimensional data analysis, Lasso has
been extensively studied by machine learning scientists (Tibshirani, 1996). It adopts the
(cid:96)1 -regularized least square formulation to select and estimate nonzero parameters simul-
taneously. Software packages such as glmnet and huge have been developed to eﬃciently
∗. The package vignette is an extended version of this paper, which contains more technical details.
†. Xingguo Li and Tuo Zhao contributed equally to this work.

c(cid:13)2015 Xingguo Li, Tuo Zhao, Xiaoming Yuan and Han Liu.

Li, Zhao, Yuan and Liu

solve large problems (Friedman et al., 2010; Zhao et al., 2012, 2014). Lasso further yields
a wide range of research interests, and motivates many variants by exploiting nonsmooth
loss functions to gain modeling ﬂexibility, estimation robustness, and tuning insensitive-
ness (See more details in the package vignette, Zhao and Liu (2014); Liu et al. (2014a)).
These nonsmooth loss functions pose a great challenge to computation. To the best of our
knowledge, no eﬃcient solver has been developed so far for these Lasso variants.
In this report, we describe a newly developed R package named flare (Family of Lasso
Regression). The flare package implements a family of linear regression methods including:
(1) LAD Lasso, which is robust to heavy tail random noise and outliers (Wang, 2013); (2)
SQRT Lasso, which is tuning insensitive (the optimal regularization parameter selection
does not depend on any unknown parameter, Belloni et al. (2011)); (3) (cid:96)q Lasso, which
shares the advantage of LAD Lasso and SQRT Lasso; (4) Dantzig selector, which can
tolerate missing values in the design matrix and response vector (Candes and Tao, 2007).
By adopting the column by column regression scheme, we further extend these regression
methods to sparse precision matrix estimation, including: (5) TIGER, which is tuning
insensitive (Liu and Wang, 2012); (6) CLIME, which can tolerate missing values in the
data matrix (Cai et al., 2011). The developed solver is based on the alternating direction
method of multipliers (ADMM), which is further accelerated by a multistage screening
approach (Boyd et al., 2011; Liu et al., 2014b). The global convergence result of ADMM
has been established in He and Yuan (2015, 2012). The numerical simulations show that
the flare package is eﬃcient and can scale up to large problems.

2. Algorithm

(cid:98)β = argmin
We are interested in solving convex programs in the following generic form
sub ject to r − Aβ = α.
Lλ (α) + (cid:107)β(cid:107)1
β , α
where λ > 0 is the regularization parameter. The possible choices of Lλ (α), A, and r for
diﬀerent regression methods are listed in Table 1. Note that LAD Lasso and SQRT Lasso
are special examples of (cid:96)q Lasso for q = 1 and q = 2 respectively.
(cid:13)(cid:13)ut + r − Aβ t − α(cid:13)(cid:13)2
All methods in Table 1 can be eﬃciently solved by the iterative scheme as follows
1
(cid:13)(cid:13)ut − αt+1 + r − Aβ(cid:13)(cid:13)2
2 +
2
1
β t+1 = argmin
2 +
2
β
ut+1 = ut + (r − αt+1 − Aβ t+1 ),

αt+1 = argmin
α

(4)

Lλ (α),
(cid:107)β(cid:107)1 ,

1
ρ

1
ρ

(1)

(2)

(3)

where u is the rescaled Lagrange multiplier (Boyd et al., 2011), and ρ > 0 is the penalty
parameter. For LAD Lasso, SQRT Lasso, or Dantzig selector, (2) has a closed form solution
via the winsorization, soft thresholding, and group soft thresholding operators respectively.
For Lq Lasso with 1 < q < 2, (2) can be solved by the bisection-based root ﬁnding algorithm.
(3) is a Lasso problem, which can be (approximately) solved by linearization or coordinate
descent. Besides the pathwise optimization scheme and the active set trick, we also adopt
In particular, we ﬁrst
the multistage screening approach to speedup the computation.

554

The flare Package in R

select k nested subsets of coordinates A1 ⊆ A2 ⊆ ... ⊆ Ak = Rd by the marginal correlation
between the covariates and responses. Then the algorithm iterates over these nested subsets
of coordinates to obtain the solution. The multistage screening approach can greatly boost
the empirical performance, especially for Dantzig selector.

Method

Lq Lasso

Dantzig selector Lλ (α) =

Loss function
√
(cid:107)α(cid:107)q
1
(cid:26) ∞ if (cid:107)α(cid:107)∞ > λ
Lλ (α) =
nλ
q
0
otherwise

A

X

r

y

Existing solver

L.P. or S.O.C.P.

n XT X 1
1
n XT y

L.P.

Table 1: All regression methods provided in the flare package. X ∈ Rn×d denotes the de-
sign matrix, and y ∈ Rn denotes the response vector. “L.P.” denotes the general
linear programming solver, and “S.O.C.P” denotes the second-order cone program-
ming solver.

3. Examples

We illustrate the user interface by analyzing the eye disease data set in flare.

> # Load the data set
> library(flare); data(eyedata)
> # SQRT Lasso
> out1 = slim(x,y,method="lq",nlambda=40,lambda.min.value=sqrt(log(200)/120))
> # Dantzig Selector
> out2 = slim(x,y,method="dantzig",nlambda=40,lambda.min.ratio=0.35)

The program automatically generates a sequence of 40 regularization parameters and es-
timates the corresponding solution paths of SQRT Lasso and the Dantzig selector. For
the Dantzig selector, the optimal regularization parameter is usually selected based on
some model selection procedures, such as cross validation. Note that Belloni et al. (2011)
tion parameter to be (cid:112)log(d)/n = (cid:112)log(200)/120. The minimum regularization parameter
√
has shown that the theoretically consistent regularization parameter of SQRT Lasso is
C
log d/n, where C is some constant. Thus we manually choose its minimum regulariza-
yields 19 nonzero coeﬃcients out of 200.

4. Numerical Simulation

All experiments below are carried out on a PC with Intel Core i5 3.3GHz processor, and
the convergence threshold of flare is chosen to be 10−5 . Timings (in seconds) are averaged
over 100 replications using 20 regularization parameters, and the range of regularization
parameters is chosen so that each method produces approximately the same number of
nonzero estimates.
We ﬁrst evaluate the timing performance of flare for sparse linear regression. We set
n = 100 and vary d from 375 to 3000 as is shown in Table 2. We independently generate

555

Li, Zhao, Yuan and Liu

each row of the design matrix from a d-dimensional normal distribution N (0, Σ), where
Σj k = 0.5|j−k| . Then we generate the response vector using yi = 3Xi1 + 2Xi2 + 1.5Xi4 + i ,
where i is independently generated from N (0, 1). From Table 2, we see that all methods
achieve good timing performance. Dantzig selector and (cid:96)q Lasso are slower than the others
due to more diﬃcult computational formulations.
We then evaluate the timing performance of flare for sparse precision matrix estima-
tion. We set n = 100 and vary d from 100 to 400 as is shown in Table 2. We independently
generate the data from a d-dimensional normal distribution N (0, Σ), where Σj k = 0.5|j−k| .
The corresponding precision matrix Ω = Σ−1 has Ωj j = 1.3333, Ωj k = −0.6667 for all
j, k = 1, ..., d and |j − k | = 1, and all other entries are 0. From Table 2, we see that both
TIGER and CLIME achieve good timing performance, and CLIME is slower than TIGER
due to a more diﬃcult computational formulation.

Method
LAD Lasso
SQRT Lasso
(cid:96)1.5 Lasso
Dantzig selector

Method
TIGER
CLIME

Sparse Linear Regression
d = 1500
d = 750
d = 375
1.8103(0.2919)
1.1046(0.3640)
1.1713(0.2915)
0.9485(0.2167)
0.7330(0.1234)
0.4888(0.0264)
14.382(0.7390)
12.995(0.5535)
14.071(0.5966)
0.3245(0.1871)
4.4669(5.9929)
1.5360(1.8566)
Sparse Precision Matrix Estimation
d = 100
d = 200
d = 300
7.1860(0.0795)
4.6251(0.0807)
1.0637(0.0361)
2.5761(0.3807)
20.137(3.2258)
42.882(18.188)

d = 3000
3.1378(0.7753)
1.2761(0.1510)
16.936(0.5696)
17.034(23.202)

d=400
11.085(0.1715)
112.50(11.561)

Table 2: Average timing performance (in seconds) with standard errors in the parentheses
on sparse linear regression and sparse precision matrix estimation.

5. Discussion and Conclusions

Though the glmnet package cannot handle nonsmooth loss functions, it is much faster than
flare for solving Lasso,1 and the glmnet package can also be applied to solve (cid:96)1 regularized
generalized linear model estimation problems, which flare cannot. Overall speaking, the
flare package serves as an eﬃcient complement to the glmnet package for high dimensional
data analysis. We will continue to maintain and support this package.

Acknowledgments

Tuo Zhao and Han Liu are supported by NSF Grants III-1116730 and NSF III-1332109,
NIH R01MH102339, NIH R01GM083084, and NIH R01HG06841, and FDA HHSF2232
01000072C. Xiaoming Yuan is supported by the General Research Fund form Hong Kong
Research Grants Council: 203311 and 203712.

1. See more detail in the package vignette.

556

The flare Package in R

References

A. Belloni, V. Chernozhukov, and L. Wang. Square-root lasso: pivotal recovery of sparse
signals via conic programming. Biometrika, 98(4):791–806, 2011.

S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and
statistical learning via the alternating direction method of multipliers. Foundations and
Trends R(cid:13) in Machine Learning, 3(1):1–122, 2011.

T. Cai, W. Liu, and X. Luo. A constrained (cid:96)1 minimization approach to sparse precision
matrix estimation. Journal of the American Statistical Association, 106:594–607, 2011.

E. Candes and T. Tao. The dantzig selector: Statistical estimation when p is much larger
than n. The Annals of Statistics, 35(6):2313–2351, 2007.

J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear
models via coordinate descent. Journal of Statistical Software, 33(1):1, 2010.

B. He and X. Yuan. On the O(1/n) convergence rate of the Douglas-Rachford alternating
direction method. SIAM Journal on Numerical Analysis, 50(2):700–709, 2012.

B. He and X. Yuan. On non-ergodic convergence rate of Douglas-Rachford alternating
direction method of multipliers. Numerische Mathematik, 2015. (Accepted).

H. Liu and L. Wang. Tiger: A tuning-insensitive approach for optimally estimating Gaussian
graphical models. Technical report, Princeton University, 2012.

H. Liu, L. Wang, and T. Zhao. Multivariate regression with calibration. In Advances in
Neural Information Processing Systems, pages 127–135, 2014a.

H. Liu, L. Wang, and T. Zhao. Sparse covariance matrix estimation with eigenvalue con-
straints. Journal of Computational and Graphical Statistics, 23(2):439–459, 2014b.

R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society, Series B, 58:267–288, 1996.

L. Wang. The L1 penalized LAD estimator for high dimensional linear regression. Journal
of Multivariate Analysis, 120:135–151, 2013.

T. Zhao and H. Liu. Calibrated precision matrix estimation for high-dimensional elliptical
distributions. IEEE Transactions on Information Theory, 60(12):7874–7887, 2014.

T. Zhao, H. Liu, K. Roeder, J. Laﬀerty, and L. Wasserman. The huge package for high-
dimensional undirected graph estimation in R. The Journal of Machine Learning Re-
search, 13(1):1059–1062, 2012.

T. Zhao, H. Liu, and T. Zhang. A general theory of pathwise coordinate optimization.
arXiv preprint arXiv:1412.7477, 2014.

557

Journal of Machine Learning Research 16 (2015) 285-322

Submitted 6/14; Published 2/15

An Asynchronous Parallel Stochastic Coordinate Descent
Algorithm

Ji Liu
Stephen J. Wright
Department of Computer Sciences
University of Wisconsin-Madison
Madison, WI 53706-1685

Christopher R´e
Department of Computer Science
Stanford University
353 Serra Mal l
Stanford, CA 94305-9025

Victor Bittorf
Srikrishna Sridhar
Department of Computer Sciences
University of Wisconsin-Madison
Madison, WI 53706-1685

Editor: Leon Bottou

ji.liu.uwisc@gmail.com
swright@cs.wisc.edu

chrismre@cs.stanford.edu

bittorf@cs.wisc.edu
srikris@cs.wisc.edu

Abstract
We describe an asynchronous parallel stochastic coordinate descent algorithm for mini-
mizing smooth unconstrained or separably constrained functions. The method achieves a
linear convergence rate on functions that satisfy an essential strong convexity property and
a sublinear rate (1/K ) on general convex functions. Near-linear speedup on a multicore
system can be expected if the number of processors is O(n1/2 ) in unconstrained optimiza-
tion and O(n1/4 ) in the separable-constrained case, where n is the number of variables. We
describe results from implementation on 40-core processors.
Keywords: asynchronous parallel optimization, stochastic coordinate descent

1. Introduction

Consider the convex optimization problem

f (x),

min
x∈Ω
where Ω ⊂ Rn is a closed convex set and f is a smooth convex mapping from an open neigh-
borhood of Ω to R. We consider two particular cases of Ω in this paper: the unconstrained
case Ω = Rn , and the separable case
Ω = Ω1 × Ω2 × . . . × Ωn ,

(1)

(2)

where each Ωi , i = 1, 2, . . . , n is a closed subinterval of the real line.

c(cid:13)2015 Liu, Wright, R´e, Bittorf, Sridhar.

Liu, Wright, R´e, Bittorf, and Sridhar

Formulations of the type (1,2) arise in many data analysis and machine learning prob-
lems, for example, support vector machines (linear or nonlinear dual formulation) (Cortes
and Vapnik, 1995), LASSO (after decomposing x into positive and negative parts) (Tib-
shirani, 1996), and logistic regression. Algorithms based on gradient and approximate or
partial gradient information have proved eﬀective in these settings. We mention in partic-
ular gradient pro jection and its accelerated variants (Nesterov, 2004), accelerated proximal
gradient methods for regularized ob jectives (Beck and Teboulle, 2009), and stochastic gra-
dient methods (Nemirovski et al., 2009; Shamir and Zhang, 2013). These methods are
inherently serial, in that each iteration depends on the result of the previous iteration. Re-
cently, parallel multicore versions of stochastic gradient and stochastic coordinate descent
have been described for problems involving large data sets; see for example Niu et al. (2011);
Richt´arik and Tak´aˇc (2012b); Avron et al. (2014).
This paper proposes an asynchronous stochastic coordinate descent (AsySCD) algo-
rithm for convex optimization. Each step of AsySCD chooses an index i ∈ {1, 2, . . . , n} and
subtracts a short, constant, positive multiple of the ith partial gradient ∇if (x) := ∂ f /∂xi
from the ith component of x. When separable constraints (2) are present, the update is
“clipped” to maintain feasibility with respect to Ωi . Updates take place in parallel across
the cores of a multicore system, without any attempt to synchronize computation between
cores. We assume that there is a bound τ on the age of the updates, that is, no more than
τ updates to x occur between the time at which a processor reads x (and uses it to evaluate
one element of the gradient) and the time at which this processor makes its update to a
single element of x. (A similar model of parallel asynchronous computation was used in
Hogwild! (Niu et al., 2011).) Our implementation, described in Section 6, is a little more
complex than this simple model would suggest, as it is tailored to the architecture of the
Intel Xeon machine that we use for experiments.
We show that linear convergence can be attained if an “essential strong convexity”
property (3) holds, while sublinear convergence at a “1/K ” rate can be proved for general
convex functions. Our analysis also deﬁnes a suﬃcient condition for near-linear speedup
in the number of cores used. This condition relates the value of delay parameter τ (which
relates to the number of cores / threads used in the computation) to the problem dimension
n. A parameter that quantiﬁes the cross-coordinate interactions in ∇f also appears in
this relationship. When the Hessian of f is nearly diagonal, the minimization problem can
almost be separated along the coordinate axes, so higher degrees of parallelism are possible.
We review related work in Section 2. Section 3 speciﬁes the proposed algorithm. Con-
vergence results for unconstrained and constrained cases are described in Sections 4 and 5,
respectively, with proofs given in the appendix. Computational experience is reported in
Section 6. We discuss several variants of AsySCD in Section 7. Some conclusions are given
in Section 8.

1.1 Notation and Assumption

We use the following notation.
• ei ∈ Rn denotes the ith natural basis vector (0, . . . , 0, 1, 0, . . . , 0)T with the ‘”1” in the
ith position.
• (cid:107) · (cid:107) denotes the Euclidean norm (cid:107) · (cid:107)2 .

286

AsySCD

• S ⊂ Ω denotes the set on which f attains its optimal value, which is denoted by f ∗ .
• PS (·) and PΩ (·) denote Euclidean pro jection onto S and Ω, respectively.
• We use xi for the ith element of x, and ∇if (x) for the ith element of the gradient
vector ∇f (x).
• We deﬁne the following essential strong convexity condition for a convex function f
with respect to the optimal set S , with parameter l > 0:
f (x) − f (y) ≥ (cid:104)∇f (y), x − y(cid:105) +
(cid:107)x − y(cid:107)2
for all x, y ∈ Ω with PS (x) = PS (y). (3)

l
2

This condition is signiﬁcantly weaker than the usual strong convexity condition, which
requires the inequality to hold for al l x, y ∈ Ω. In particular, it allows for non-singleton
solution sets S , provided that f increases at a uniformly quadratic rate with distance
from S .
(This property is noted for convex quadratic f in which the Hessian is
rank deﬁcient.) Other examples of essentially strongly convex functions that are not
strongly convex include:
– f (Ax) with arbitrary linear transformation A, where f (·) is strongly convex;
– f (x) = max(aT x − b, 0)2 , for a (cid:54)= 0.
• Deﬁne Lres as the restricted Lipschitz constant for ∇f , where the “restriction” is to
the coordinate directions: We have
(cid:107)∇f (x)−∇f (x+tei )(cid:107) ≤ Lres |t|,
for all i = 1, 2, . . . , n and t ∈ R, with x, x + tei ∈ Ω.
• Deﬁne Li as the coordinate Lipschitz constant for ∇f in the ith coordinate direction:
We have
f (x + tei ) − f (x) ≤ (cid:104)∇if (x), t(cid:105) +

for i ∈ {1, 2, . . . , n}, and x, x + tei ∈ Ω,

Li
2

t2 ,

or equivalently

|∇if (x) − ∇if (x + tei )| ≤ Li |t|.

• Lmax := maxi=1,2,...,n Li .
Note that Lres ≥ Lmax .
We use {xj }j=0,1,2,... to denote the sequence of iterates generated by the algorithm from
starting point x0 . Throughout the paper, we make the following assumption.

Assumption 1
• The optimal solution set S of (1) is nonempty.
• The radius of the iterate set {xj }j=0,1,2,... deﬁned by
(cid:107)xj − PS (xj )(cid:107)

R := sup
j=0,1,2,...

is bounded, that is, R < +∞.

287

Liu, Wright, R´e, Bittorf, and Sridhar

1.2 Lipschitz Constants

n.

The nonstandard Lipschitz constants Lres , Lmax , and Li , i = 1, 2, . . . , n deﬁned above are
crucial in the analysis of our method. Besides bounding the nonlinearity of f along various
directions, these quantities capture the interactions between the various components in the
gradient ∇f , as quantiﬁed in the oﬀ-diagonal terms of the Hessian ∇2f (x) — although the
stated conditions do not require this matrix to exist.
We have noted already that Lres/Lmax ≥ 1. Let us consider upper bounds on this ratio
under certain conditions. When f is twice continuously diﬀerentiable, we have
[∇2f (x)]ii .
max
Li = sup
x∈Ω
i=1,2,...,n
|[∇2f (x)]ij | ≤ (cid:112)LiLj ≤ Lmax ,
Since ∇2f (x) (cid:23) 0 for x ∈ Ω, we have that
∀ i, j = 1, 2, . . . , n.
Thus Lres , which is a bound on the largest column norm for ∇2f (x) over all x ∈ Ω, is
√
nLmax , so that
bounded by
≤ √
Lres
Lmax
argument leads to Lres/Lmax ≤ √
If the Hessian is structurally sparse, having at most p nonzeros per row/column, the same
p.
If f (x) is a convex quadratic with Hessian Q, we have
(cid:107)Q·i(cid:107)2 ,
Qii , Lres = max
Lmax = max
i
i
(cid:88)
where Q·i denotes the ith column of Q. If Q is diagonally dominant, we have for any column
i that
|Qj i | ≤ 2Qii ,
(cid:107)Q·i(cid:107)2 ≤ Qii + (cid:107)[Qj i ]j (cid:54)=i(cid:107)2 ≤ Qii +
j (cid:54)=i
which, by taking the maximum of both sides, implies that Lres/Lmax ≤ 2 in this case.
2 (cid:107)Ax − b(cid:107)2 and assume that A ∈ Rm×n is a
Finally, consider the ob jective f (x) = 1
random matrix whose entries are i.i.d from N (0, 1). The diagonals of the Hessian are AT·i A·i
(where A·i is the ith column of A), which have expected value m, so we can expect Lmax
to be not less than m. Recalling that Lres is the maximum column norm of AT A, we have
(cid:115)(cid:88)
E((cid:107)AT A·i(cid:107)) ≤ E(|AT·i A·i |) + E((cid:107)[AT·j A·i ]j (cid:54)=i(cid:107))
|AT·j A·i |2
= m + E
(cid:115)(cid:88)
j (cid:54)=i
≤ m +
E|AT·j A·i |2
= m + (cid:112)(n − 1)m,
j (cid:54)=i
where the second inequality uses Jensen’s inequality and the ﬁnal equality uses
We can thus estimate the upper bound on Lres/Lmax roughly by 1 + (cid:112)n/m for this case.
E(|AT·j A·i |2 ) = E(AT·j
E(A·iAT·i )A·j ) = E(AT·j I A·j ) = E(AT·j A·j ) = m.
288

AsySCD

2. Related Work

This section reviews some related work on coordinate relaxation and stochastic gradient
algorithms.

Among cyclic coordinate descent algorithms, Tseng (2001) proved the convergence of
a block coordinate descent method for nondiﬀerentiable functions with certain conditions.
Local and global linear convergence were established under additional assumptions, by Luo
and Tseng (1992) and Wang and Lin (2014), respectively. Global linear (sublinear) conver-
gence rate for strongly (weakly) convex optimization was proved by Beck and Tetruashvili
(2013). Block-coordinate approaches based on proximal-linear subproblems are described
by Tseng and Yun (2009, 2010). Wright (2012) uses acceleration on reduced spaces (cor-
responding to the optimal manifold) to improve the local convergence properties of this
approach.

Stochastic coordinate descent is almost identical to cyclic coordinate descent except
selecting coordinates in a random manner. Nesterov (2012) studied the convergence rate for
a stochastic block coordinate descent method for unconstrained and separably constrained
convex smooth optimization, proving linear convergence for the strongly convex case and a
sublinear 1/K rate for the convex case. Extensions to minimization of composite functions
are described by Richt´arik and Tak´aˇc (2012a) and Lu and Xiao (2013).

Synchronous paral lel methods distribute the workload and data among multiple proces-
sors, and coordinate the computation among processors. Ferris and Mangasarian (1994)
proposed to distribute variables among multiple processors and optimize concurrently over
each subset. The synchronization step searches the aﬃne hull formed by the current iterate
algorithm for functions of the form f (x) = (cid:80)N
and the points found by each processor. Similar ideas appeared in (Mangasarian, 1995), with
a diﬀerent synchronization step. Goldfarb and Ma (2012) considered a multiple splitting
k=1 fk (x) in which N models are optimized
separately and concurrently, then combined in an synchronization step. The alternating
direction method-of-multiplier (ADMM) framework (Boyd et al., 2011) can also be imple-
mented in parallel. This approach dissects the problem into multiple subproblems (possibly
after replication of primal variables) and optimizes concurrently, then synchronizes to up-
date multiplier estimates. Duchi et al. (2012) described a subgradient dual-averaging algo-
rithm for partially separable ob jectives, with subgradient evaluations distributed between
cores and combined in ways that reﬂect the structure of the ob jective. Parallel stochastic
gradient approaches have received broad attention; see Agarwal and Duchi (2011) for an
approach that allows delays between evaluation and update, and Cotter et al. (2011) for
a minibatch stochastic gradient approach with Nesterov acceleration. Shalev-Shwartz and
Zhang (2013) proposed an accelerated stochastic dual coordinate ascent method.

Among synchronous paral lel methods for (block) coordinate descent, Richt´arik and Tak´aˇc
(2012b) described a method of this type for convex composite optimization problems. All
processors update randomly selected coordinates or blocks, concurrently and synchronously,
at each iteration. Speedup depends on the sparsity of the data matrix that deﬁnes the loss
functions. Several variants that select blocks greedily are considered by Scherrer et al.
(2012) and Peng et al. (2013). Yang (2013) studied the parallel stochastic dual coordinate
ascent method and emphasized the balance between computation and communication.

289

Liu, Wright, R´e, Bittorf, and Sridhar

We turn now to asynchronous paral lel methods. Bertsekas and Tsitsiklis (1989) intro-
duced an asynchronous parallel implementation for general ﬁxed point problems x = q(x)
over a separable convex closed feasible region. (The optimization problem (1) can be formu-
lated in this way by deﬁning q(x) := PΩ [(I − α∇f )(x)] for some ﬁxed α > 0.) Their analysis
allows inconsistent reads for x, that is, the coordinates of the read x have diﬀerent “ages.”
Linear convergence is established if all ages are bounded and ∇2f (x) satisﬁes a diagonal
dominance condition guaranteeing that the iteration x = q(x) is a maximum-norm contrac-
the contraction condition requires diagonal dominance of the Hessian: Aii > (cid:80)
tion mapping for suﬃcient small α. However, this condition is strong — stronger, in fact,
than the strong convexity condition. For convex quadratic optimization f (x) = 1
2 xT Ax+ bx,
i (cid:54)=j |Aij | for
all i = 1, 2, . . . , n. By comparison, AsySCD guarantees linear convergence rate under the
essential strong convexity condition (3), though we do not allow inconsistent read. (We
require the vector x used for each evaluation of ∇if (x) to have existed at a certain point
in time.)
Hogwild! (Niu et al., 2011) is a lock-free, asynchronous parallel implementation of
a stochastic-gradient method, targeted to a multicore computational model similar to the
one considered here. Its analysis assumes consistent reading of x, and it is implemented
without locking or coordination between processors. Under certain conditions, convergence
of Hogwild! approximately matches the sublinear 1/K rate of its serial counterpart, which
is the constant-steplength stochastic gradient method analyzed in Nemirovski et al. (2009).
We also note recent work by Avron et al. (2014), who proposed an asynchronous linear
solver to solve Ax = b where A is a symmetric positive deﬁnite matrix, proving a linear
convergence rate. Both inconsistent- and consistent-read cases are analyzed in this paper,
with the convergence result for inconsistent read being slightly weaker.

3. Algorithm

In AsySCD, multiple processors have access to a shared data structure for the vector x, and
each processor is able to compute a randomly chosen element of the gradient vector ∇f (x).
Each processor repeatedly runs the following coordinate descent process (the steplength
parameter γ is discussed further in the next section):
R: Choose an index i ∈ {1, 2, . . . , n} at random, read x, and evaluate ∇if (x);
U: Update component i of the shared x by taking a step of length γ /Lmax in the direction
−∇if (x).
Since these processors are being run concurrently and without synchronization, x may
change between the time at which it is read (in step R) and the time at which it is updated
(step U). We capture the system-wide behavior of AsySCD in Algorithm 1. There is a
global counter j for the total number of updates; xj denotes the state of x after j updates.
The index i(j ) ∈ {1, 2, . . . , n} denotes the component updated at step j . k(j ) denotes the
x-iterate at which the update applied at iteration j was calculated. Obviously, we have
k(j ) ≤ j , but we assume that the delay between the time of evaluation and updating is
bounded uniformly by a positive integer τ , that is, j − k(j ) ≤ τ for all j . The value of τ
captures the essential parallelism in the method, as it indicates the number of processors
that are involved in the computation.

290

AsySCD

Algorithm 1 Asynchronous Stochastic Coordinate Descent Algorithm xK+1 =
AsySCD(x0 , γ , K )
Require: x0 ∈ Ω, γ , and K
Ensure: xK+1
1: Initialize j ← 0;
2: while j ≤ K do
(cid:16)
(cid:17)
Choose i(j ) from {1, . . . , n} with equal probability;
3:
xj+1 ← PΩ
ei(j )∇i(j )f (xk(j ) )
xj − γ
;
Lmax
j ← j + 1;
5:
6: end while

4:

The pro jection operation PΩ onto the feasible set is not needed in the case of uncon-
strained optimization. For separable constraints (2), it requires a simple clipping operation
on the i(j ) component of x.
We note several diﬀerences with earlier asynchronous approaches. Unlike the asyn-
chronous scheme in Bertsekas and Tsitsiklis (1989, Section 6.1), the latest value of x is
updated at each step, not an earlier iterate. Although our model of computation is similar
to Hogwild! (Niu et al., 2011), the algorithm diﬀers in that each iteration of AsySCD
evaluates a single component of the gradient exactly, while Hogwild!
computes only
a (usually crude) estimate of the full gradient. Our analysis of AsySCD below is com-
prehensively diﬀerent from that of Niu et al. (2011), and we obtain stronger convergence
results.

4. Unconstrained Smooth Convex Case

This section presents results about convergence of AsySCD in the unconstrained case
Ω = Rn . The theorem encompasses both the linear rate for essentially strongly convex
f and the sublinear rate for general convex f . The result depends strongly on the delay
parameter τ . (Proofs of results in this section appear in Appendix A.) In Algorithm 1, the
indices i(j ), j = 0, 1, 2, . . . are random variables. We denote the expectation over all random
variables as E, the conditional expectation in term of i(j ) given i(0), i(1), · · · , i(j − 1) as
Ei(j ) .
A crucial issue in AsySCD is the choice of steplength parameter γ . This choice involves
a tradeoﬀ: We would like γ to be long enough that signiﬁcant progress is made at each step,
but not so long that the gradient information computed at step k(j ) is stale and irrelevant
by the time the update is applied at step j . We enforce this tradeoﬀ by means of a bound
on the ratio of expected squared norms on ∇f at successive iterates; speciﬁcally,
ρ−1 ≤ E(cid:107)∇f (xj+1 )(cid:107)2
E(cid:107)∇f (xj )(cid:107)2 ≤ ρ,

(4)

where ρ > 1 is a user deﬁned parameter. The analysis becomes a delicate balancing act in
the choice of ρ and steplength γ between aggression and excessive conservatism. We ﬁnd,
however, that these values can be chosen to ensure steady convergence for the asynchronous

291

Liu, Wright, R´e, Bittorf, and Sridhar

method at a linear rate, with rate constants that are almost consistent with vanilla short-
step full-gradient descent.

Theorem 1 Suppose that Ω = Rn in (1) and that Assumption 1 is satisﬁed. For any ρ > 1,
deﬁne the quantity ψ as fol lows:

ψ := 1 +

2τ ρτ Lres
√
nLmax

.

Suppose that the steplength parameter γ > 0 satisﬁes the fol lowing three upper bounds:

γ ≤ 1
,
√
ψ
γ ≤ (ρ − 1)
nLmax
√
2ρτ +1Lres
γ ≤ (ρ − 1)
nLmax
Lresρτ (2 + Lres√
nLmax

,

.

)

Then we have that for any j ≥ 0 that
ρ−1E((cid:107)∇f (xj )(cid:107)2 ) ≤ E((cid:107)∇f (xj+1 )(cid:107)2 ) ≤ ρE((cid:107)∇f (xj )(cid:107)2 ).
(cid:18)
(cid:19)(cid:19)j
(cid:18)
Moreover, if the essential ly strong convexity property (3) holds with l > 0, we have
1 − 2lγ
1 − ψ
nLmax
2

E(f (xj ) − f ∗ ) ≤

(f (x0 ) − f ∗ ),

γ

while for general smooth convex functions f , we have

E(f (xj ) − f ∗ ) ≤

1
(f (x0 ) − f ∗ )−1 + j γ (1 − ψ
2 γ )/(nLmaxR2 )

.

(5)

(6a)

(6b)

(6c)

(7)

(8)

(9)

This theorem demonstrates linear convergence (8) for AsySCD in the unconstrained es-
sentially strongly convex case. This result is better than that obtained for Hogwild! (Niu
et al., 2011), which guarantees only sublinear convergence under the stronger assumption
of strict convexity.
The following corollary proposes an interesting particular choice of the parameters for
which the convergence expressions become more comprehensible. The result requires a
condition on the delay bound τ in terms of n and the ratio Lmax/Lres .

Corollary 2 Suppose that Assumption 1 holds, and that
√

τ + 1 ≤

nLmax
2eLres

.

Then if we choose

ρ = 1 +

√
2eLres
nLmax

,

292

(10)

(11)

AsySCD

(cid:19)j
(cid:18)
deﬁne ψ by (5), and set γ = 1/ψ , we have for the essential ly strongly convex case (3) with
l > 0 that
1 −

(f (x0 ) − f ∗ ),

(12)

E(f (xj ) − f ∗ ) ≤

l
2nLmax
while for the case of general convex f , we have
E(f (xj ) − f ∗ ) ≤

.

(13)

1
(f (x0 ) − f ∗ )−1 + j /(4nLmaxR2 )
We note that the linear rate (12) is broadly consistent with the linear rate for the
classical steepest descent method applied to strongly convex functions, which has a rate
constant of (1 − 2l/L), where L is the standard Lipschitz constant for ∇f . If we assume (not
unreasonably) that n steps of stochastic coordinate descent cost roughly the same as one step
of steepest descent, and note from (12) that n steps of stochastic coordinate descent would
achieve a reduction factor of about (1 − l/(2Lmax )), a standard argument would suggest
that stochastic coordinate descent would require about 4Lmax/L times more computation.
(Note that Lmax/L ∈ [1/n, 1].) The stochastic approach may gain an advantage from the
parallel implementation, however. Steepest descent requires synchronization and careful
division of gradient evaluations, whereas the stochastic approach can be implemented in an
asynchronous fashion.
For the general convex case, (13) deﬁnes a sublinear rate, whose relationship with the
rate of the steepest descent for general convex optimization is similar to the previous para-
graph.
As noted in Section 1, the parameter τ is closely related to the number of cores that
can be involved in the computation, without degrading the convergence performance of the
algorithm. In other words, if the number of cores is small enough such that (10) holds, the
convergence expressions (12), (13) do not depend on the number of cores, implying that
linear speedup can be expected. A small value for the ratio Lres/Lmax (not much greater
than 1) implies a greater degree of potential parallelism. As we note at the end of Section 1,
√
this ratio tends to be small in some important applications — a situation that would allow
O(
n) cores to be used with near-linear speedup.
We conclude this section with a high-probability estimate for convergence of the sequence
of function values.

Theorem 3 Suppose that the assumptions of Corol lary 2 hold, including the deﬁnitions of
ρ and ψ . Then for any  ∈ (0, f (x0 ) − f ∗ ) and η ∈ (0, 1), we have that
P (f (xj ) − f ∗ ≤ ) ≥ 1 − η ,
(cid:12)(cid:12)(cid:12)(cid:12)log
(cid:12)(cid:12)(cid:12)(cid:12) ,
provided that either of the fol lowing suﬃcient conditions hold for the index j . In the essen-
tial ly strongly convex case (3) with l > 0, it suﬃces to have
f (x0 ) − f ∗
j ≥ 2nLmax
η
l
(cid:18) 1
while in the general convex case, a suﬃcient condition is
−
j ≥ 4nLmaxR2
η

1
f (x0 ) − f ∗

(cid:19)

(16)

(14)

(15)

.

293

Liu, Wright, R´e, Bittorf, and Sridhar

5. Constrained Smooth Convex Case

(17)

This section considers the case of separable constraints (2). We show results about conver-
gence rates and high-probability complexity estimates, analogous to those of the previous
section. Proofs appear in Appendix B.
As in the unconstrained case, the steplength γ should be chosen to ensure steady progress
while ensuring that update information does not become too stale. Because constraints are
present, the ratio (4) is no longer appropriate. We use instead a ratio of squares of expected
diﬀerences in successive primal iterates:
E(cid:107)xj−1 − ¯xj (cid:107)2
E(cid:107)xj − ¯xj+1(cid:107)2 ,
where ¯xj+1 is the hypothesized full update obtained by applying the single-component
update to every component of xj , that is,
(cid:104)∇f (xk(j ) ), x − xj (cid:105) +
Lmax
¯xj+1 := arg min
x∈Ω
2γ
In the unconstrained case Ω = Rn , the ratio (17) reduces to
E(cid:107)∇f (xk(j−1) )(cid:107)2
E(cid:107)∇f (xk(j ) )(cid:107)2 ,
which is evidently related to (4), but not identical.
We have the following result concerning convergence of the expected error to zero.
√
Theorem 4 Suppose that Ω has the form (2), that Assumption 1 is satisﬁed, and that
(cid:19)
(cid:18)
−1 , and deﬁne the quantity ψ as fol lows:
n ≥ 5. Let ρ be a constant with ρ > (1 − 2/
n)
Lres τ ρτ
Lmax√
√
2τ
+
2 +
n
nLmax
nLres
(cid:18)
(cid:19) √
Suppose that the steplength parameter γ > 0 satisﬁes the fol lowing two upper bounds:
− 2√
1 − 1
γ ≤ 1
γ ≤
nLmax
4Lres τ ρτ .
ψ
ρ
n

(cid:107)x − xj (cid:107)2 .

ψ := 1 +

(18)

(19)

.

,

Then we have

E(cid:107)xj−1 − ¯xj (cid:107)2 ≤ ρE(cid:107)xj − ¯xj+1(cid:107)2 ,
If the essential strong convexity property (3) holds with l > 0, we have for j = 1, 2, . . . that
(cid:19)
(cid:19)j (cid:18)
(cid:18)
E(cid:107)xj − PS (xj )(cid:107)2 +
(Ef (xj ) − f ∗ )
2γ
Lmax
(f (x0 ) − f ∗ )
R2 +

j = 1, 2, . . . .

1 −

(21)

(20)

≤

.

l
n(l + γ−1Lmax )
For general smooth convex function f , we have
Ef (xj ) − f ∗ ≤ n(R2Lmax + 2γ (f (x0 ) − f ∗ ))
2γ (n + j )

2γ
Lmax

.

(22)

294

AsySCD

Similarly to the unconstrained case, the following corollary proposes an interesting par-
ticular choice for the parameters for which the convergence expressions become more com-
prehensible. The result requires a condition on the delay bound τ in terms of n and the
ratio Lmax/Lres .
Corollary 5 Suppose that Assumption 1 holds, that τ ≥ 1 and n ≥ 5, and that
√

τ (τ + 1) ≤

nLmax
4eLres

.

(23)

If we choose

√
4eτ Lres
nLmax
then the steplength γ = 1/2 wil l satisfy the bounds (19). In addition, for the essential ly
(cid:18)
(cid:19)j
strongly convex case (3) with l > 0, we have for j = 1, 2, . . . that

ρ = 1 +

(24)

,

E(f (xj ) − f ∗ ) ≤

1 −

l
n(l + 2Lmax )

(LmaxR2 + f (x0 ) − f ∗ ),

while for the case of general convex f , we have
E(f (xj ) − f ∗ ) ≤ n(LmaxR2 + f (x0 ) − f ∗ )
j + n

.

(25)

(26)

Similarly to Section 4, and provided τ satisﬁes (23), the convergence rate is not aﬀected
appreciably by the delay bound τ , and near-linear speedup can be expected for multicore
implementations when (23) holds. This condition is more restrictive than (10) in the uncon-
strained case, but still holds in many problems for interesting values of τ . When Lres/Lmax
is bounded independently of dimension, the maximal number of cores allowed is of the the
order of n1/4 , which is smaller than the O(n1/2 ) value obtained for the unconstrained case.
We conclude this section with another high-probability bound, whose proof tracks that
of Theorem 3.

Theorem 6 Suppose that the conditions of Corol lary 5 hold, including the deﬁnitions of ρ
and ψ . Then for  > 0 and η ∈ (0, 1), we have that
P (f (xj ) − f ∗ ≤ ) ≥ 1 − η ,
provided that one of the fol lowing conditions holds: In the essential ly strongly convex case (3)
(cid:12)(cid:12)(cid:12)(cid:12) ,
(cid:12)(cid:12)(cid:12)(cid:12)log
with l > 0, we require
while in the general convex case, it suﬃces that
j ≥ n(LmaxR2 + f (x0 ) − f ∗ )
η

LmaxR2 + f (x0 ) − f ∗
η

j ≥ n(l + 2Lmax )
l

− n.

295

Liu, Wright, R´e, Bittorf, and Sridhar

6. Experiments

We illustrate the behavior of two variants of the stochastic coordinate descent approach
on test problems constructed from several data sets. Our interests are in the eﬃciency of
multicore implementations (by comparison with a single-threaded implementation) and in
performance relative to alternative solvers for the same problems.
All our test problems have the form (1), with either Ω = Rn or Ω separable as in (2).
The ob jective f is quadratic, that is,

f (x) =

1
2

xT Qx + cT x,

with Q symmetric positive deﬁnite.
Our implementation of AsySCD is called DIMM-WITTED (or DW for short). It runs
on various numbers of threads, from 1 to 40, each thread assigned to a single core in our 40-
core Intel Xeon architecture. Cores on the Xeon architecture are arranged into four sockets
— ten cores per socket, with each socket having its own memory. Non-uniform memory ac-
cess (NUMA) means that memory accesses to local memory (on the same socket as the core)
are less expensive than accesses to memory on another socket. In our DW implementation,
we assign each socket an equal-sized “slice” of Q, a row submatrix. The components of x
are partitioned between cores, each core being responsible for updating its own partition of
x (though it can read the components of x from other cores). The components of x assigned
to the cores correspond to the rows of Q assigned to that core’s socket. Computation is
grouped into “epochs,” where an epoch is deﬁned to be the period of computation during
which each component of x is updated exactly once. We use the parameter p to denote
the number of epochs that are executed between reordering (shuﬄing) of the coordinates
of x. We investigate both shuﬄing after every epoch (p = 1) and after every tenth epoch
(p = 10). Access to x is lock-free, and updates are performed asynchronously. This update
scheme does not implement exactly the “sampling with replacement” scheme analyzed in
previous sections, but can be viewed as a high performance, practical adaptation of the
AsySCD method.
To do each coordinate descent update, a thread must read the latest value of x. Most
components are already in the cache for that core, so that it only needs to fetch those
components recently changed. When a thread writes to xi , the hardware ensures that this
xi is simultaneously removed from other cores, signaling that they must fetch the updated
version before proceeding with their respective computations.
Although DW is not a precise implementation of AsySCD, it largely achieves the
consistent-read condition that is assumed by the analysis. Inconsistent read happens on
a core only if the following three conditions are satisﬁed simultaneously:
• A core does not ﬁnish reading recently changed coordinates of x (note that it needs
to read no more than τ coordinates);
• Among these recently changed coordinates, modiﬁcations take place both to coordi-
nates that have been read and that are stil l to be read by this core;
• Modiﬁcation of the already-read coordinates happens earlier than the modiﬁcation of
the still-unread coordinates.

296

AsySCD

Inconsistent read will occur only if at least two coordinates of x are modiﬁed twice during
a stretch of approximately τ updates to x (that is, iterations of Algorithm 1). For the
DW implementation, inconsistent read would require repeated updating of a particular
component in a stretch of approximately τ iterations that straddles two epochs. This event
would be rare, for typical values of n and τ . Of course, one can avoid the inconsistent read
issue altogether by changing the shuﬄing rule slightly, enforcing the requirement that no
coordinate can be modiﬁed twice in a span of τ iterations. From the practical perspective,
this change does not improve performance, and detracts from the simplicity of the approach.
From the theoretical perspective, however, the analysis for the inconsistent-read model
would be interesting and meaningful, and we plan to study this topic in future work.
The ﬁrst test problem QP is an unconstrained, regularized least squares problem con-
structed with synthetic data. It has the form

(27)

f (x) :=

(cid:107)x(cid:107)2 .

(cid:107)Ax − b(cid:107)2 +

1
α
min
x∈Rn
2
2
All elements of A ∈ Rm×n , the true model ˜x ∈ Rn , and the observation noise vector δ ∈ Rm
are generated in i.i.d. fashion from the Gaussian distribution N (0, 1), following which each
column in A is scaled to have a Euclidean norm of 1. The observation b ∈ Rm is constructed
from A ˜x + δ(cid:107)A ˜x(cid:107)/(5m). We choose m = 6000, n = 20000, and α = 0.5. We therefore have
≈ 1 + (cid:112)n/m + α
Lmax = 1 + α = 1.5 and
Lres
1 + α
Lmax
This problem is diagonally dominant, and the condition (10) is satisﬁed when delay param-
eter τ is less than about 95. In Algorithm 1, we set the steplength parameter γ to 1, and we
choose initial iterate to be x0 = 0. We measure convergence of the residual norm (cid:107)∇f (x)(cid:107).
Our second problem QPc is a bound-constrained version of (27):

≈ 2.2.

min
x∈Rn
+

f (x) :=

1
2

(x − ˜x)T (AT A + αI )(x − ˜x).

(28)

The methodology for generating A and ˜x and for choosing the values of m, n, γ , and x0 is
the same as for (27). We measure convergence via the residual (cid:107)x − PΩ (x − ∇f (x))(cid:107), where
Ω is the nonnegative orthant Rn
+ . At the solution of (28), about half the components of x
are at their lower bound of 0.
Our third and fourth problems are quadratic penalty functions for linear programming
relaxations of vertex cover problems on large graphs. The vertex cover problem for an
(cid:88)
undirected graph with edge set E and vertex set V can be written as a binary linear
program:
sub ject to yu + yv ≥ 0,
v∈V
By relaxing each binary constraint to the interval [0, 1], introducing slack variables for the
(cid:88)
cover inequalities, we obtain a problem of the form
sub ject to yu + yv − suv = 0,
v∈V

min
yv ∈[0,1], suv ∈[0,1]

∀ (u, v) ∈ E .

∀ (u, v) ∈ E .

min
y∈{0,1}|V |

yv

yv

297

Liu, Wright, R´e, Bittorf, and Sridhar

This has the form

cT x sub ject to Ax = b,
min
x∈[0,1]n
for n = |V | + |E |. The test problem (29) is a regularized quadratic penalty reformulation
of this linear program for some penalty parameter β :
(cid:107)Ax − b(cid:107)2 +

(cid:107)x(cid:107)2 ,

(29)

1
2β

min
x∈[0,1]n

cT x +

β
2

with β = 5. Two test data sets Amazon and DBLP have dimensions n = 561050 and n =
520891, respectively.
We tracked the behavior of the residual as a function of the number of epochs, when
executed on diﬀerent numbers of cores. Figure 1 shows convergence behavior for each of our
four test problems on various numbers of cores with two diﬀerent shuﬄing periods: p = 1
and p = 10. We note the following points.
• The total amount of computation to achieve any level of precision appears to be
almost independent of the number of cores, at least up to 40 cores. In this respect,
the performance of the algorithm does not change appreciably as the number of cores
is increased. Thus, any deviation from linear speedup is due not to degradation of
convergence speed in the algorithm but rather to systems issues in the implementation.
• When we reshuﬄe after every epoch (p = 1), convergence is slightly faster in synthetic
unconstrained QP but slightly slower in Amazon and DBLP than when we do occasional
reshuﬄing (p = 10). Overall, the convergence rates with diﬀerent shuﬄing periods
are comparable in the sense of epochs. However, when the dimension of the variable
is large, the shuﬄing operation becomes expensive, so we would recommend using a
large value for p for large-dimensional problems.

Results for speedup on multicore implementations are shown in Figures 2 and 3 for DW
with p = 10. Speedup is deﬁned as follows:

runtime a single core using DW
runtime on P cores

.

Near-linear speedup can be observed for the two QP problems with synthetic data. For
Problems 3 and 4, speedup is at most 12-14; there are few gains when the number of cores
exceeds about 12. We believe that the degradation is due mostly to memory contention.
Although these problems have high dimension, the matrix Q is very sparse (in contrast to
the dense Q for the synthetic data set). Thus, the ratio of computation to data movement
/ memory access is much lower for these problems, making memory contention eﬀects more
signiﬁcant.
Figures 2 and 3 also show results of a global-locking strategy for the parallel stochastic
coordinate descent method, in which the vector x is locked by a core whenever it performs
a read or update. The performance curve for this strategy hugs the horizontal axis; it is
not competitive.
Wall clock times required for the four test problems on 1 and 40 cores, to reduce residuals
below 10−5 are shown in Table 1. (Similar speedups are noted when we use a convergence
tolerance looser than 10−5 .)

298

AsySCD

Figure 1: Residuals vs epoch number for the four test problems. Results are reported for
variants in which indices are reshuﬄed after every epoch (p = 1) and after every
tenth epoch (p = 10).

299

5101520253010−410−310−210−1100101Synthetic Unconstrained QP: n = 20000 p = 1# epochsresidual  thread= 1thread=10thread=20thread=30thread=4051015202530354010−410−310−210−1100101Synthetic Unconstrained QP: n = 20000 p = 10# epochsresidual  thread= 1thread=10thread=20thread=30thread=4051015202510−510−410−310−210−1100101Synthetic Constrained QP: n = 20000 p = 1# epochsresidual  thread= 1thread=10thread=20thread=30thread=4024681012141618202210−410−310−210−1100101Synthetic Constrained QP: n = 20000 p = 10# epochsresidual  thread= 1thread=10thread=20thread=30thread=4010203040506070809010−410−310−210−1100101102Amazon: n = 561050 p = 1# epochsresidual  thread= 1thread=10thread=20thread=30thread=401020304050607010−410−310−210−1100101102Amazon: n = 561050 p = 10# epochsresidual  thread= 1thread=10thread=20thread=30thread=40510152025303540455010−310−210−1100101102DBLP: n = 520891 p = 1# epochsresidual  thread= 1thread=10thread=20thread=30thread=4051015202530354010−310−210−1100101102DBLP: n = 520891 p = 10# epochsresidual  thread= 1thread=10thread=20thread=30thread=40Liu, Wright, R´e, Bittorf, and Sridhar

Figure 2: Test problems 1 and 2: Speedup of multicore implementations of DW on up to
40 cores of an Intel Xeon architecture. Ideal (linear) speedup curve is shown for
reference, along with poor speedups obtained for a global-locking strategy.

Figure 3: Test problems 3 and 4: Speedup of multicore implementations of DW on up to
40 cores of an Intel Xeon architecture. Ideal (linear) speedup curve is shown for
reference, along with poor speedups obtained for a global-locking strategy.

Problem 1 core
98.4
QP
59.7
QPc
17.1
Amazon
11.5
DBLP

40 cores
3.03
1.82
1.25
.91

Table 1: Runtimes (seconds) for the four test problems on 1 and 40 cores.

All problems reported on above are essentially strongly convex. Similar speedup prop-
erties can be obtained in the weakly convex case as well. We show speedups for the QPc
problem with α = 0. Table 2 demonstrates similar speedup to the essentially strongly
convex case shown in Figure 2.
Turning now to comparisons between AsySCD and alternative algorithms, we start
by considering the basic gradient descent method. We implement gradient descent in a

300

510152025303540510152025303540Synthetic Unconstrained QP: n = 20000threadsspeedup  IdealAsySCD−DWGlobal Locking510152025303540510152025303540Synthetic Constrained QP: n = 20000threadsspeedup  IdealAsySCD−DWGlobal Locking510152025303540510152025303540Amazon: n = 561050threadsspeedup  IdealAsySCD−DWGlobal Locking510152025303540510152025303540DBLP: n = 520891threadsspeedup  IdealAsySCD−DWGlobal LockingAsySCD

#cores Time(sec) Speedup
55.9
1
1
10.8
10
5.19
20.2
2.77
20
27.2
2.06
30
40
1.81
30.9

Table 2: Runtimes (seconds) and speedup for multicore implementations of DW on diﬀerent
number of cores for the weakly convex QPc problem (with α = 0) to achieve a
residual below 0.06.

#cores

1
10
20
30
40

Speedup
Time(sec)
SynGD / AsySCD SynGD / AsySCD
121. / 27.1
0.22 / 1.00
2.38 / 10.5
11.4 / 2.57
4.51 / 19.9
6.00 / 1.36
4.44 / 1.01
6.10 / 26.8
6.93 / 30.8
3.91 / 0.88

Table 3: Eﬃciency comparison between SynGD and AsySCD for the QP problem. The
running time and speedup are based on the residual achieving a tolerance of 10−5 .

Dataset

adult
news
rcv
reuters
w8a

# of
Train time(sec)
# of
Samples Features LIBSVM AsySCD
1.39
32561
16.15
123
7.22
214.48
1355191
19996
16.06
40.33
47236
20242
8293
18930
1.63
0.81
5.86
33.62
300
49749

Table 4: Eﬃciency comparison between LIBSVM and AsySCD for kernel SVM using 40
cores using homogeneous kernels (K (xi , xj ) = (xT
i xj )2 ). The running time and
speedup are calculated based on the “residual” 10−3 . Here, to make both algo-
rithms comparable, the “residual” is deﬁned by (cid:107)x − PΩ (x − ∇f (x))(cid:107)∞ .

parallel, synchronous fashion, distributing the gradient computation load on multiple cores
and updates the variable x in parallel at each step. The resulting implementation is called
SynGD. Table 3 reports running time and speedup of both AsySCD over SynGD, showing
a clear advantage for AsySCD.
Next we compare AsySCD to LIBSVM (Chang and Lin, 2011) a popular parallel solver
for kernel support vector machines (SVM). Both algorithms are run on 40 cores to solve the
dual formulation of kernel SVM, without an intercept term. All data sets used in 4 except

301

Liu, Wright, R´e, Bittorf, and Sridhar

reuters were obtained from the LIBSVM data set repository.1 The data set reuters is a sparse
binary text classiﬁcation data set constructed as a one-versus-all version of Reuters-2159.2
Our comparisons, shown in Table 4, indicate that AsySCD outperforms LIBSVM on these
test sets.

7. Extension

The AsySCD algorithm can be extended by partitioning the coordinates into blocks, and
modifying Algorithm 1 to work with these blocks rather than with single coordinates. If
Li , Lmax , and Lres are deﬁned in the block sense, as follows:
(cid:107)∇f (x) − ∇f (x + Ei t)(cid:107) ≤ Lres(cid:107)t(cid:107) ∀x, i, t ∈ R|i| ,
(cid:107)∇if (x) − ∇if (x + Ei t)(cid:107) ≤ Li(cid:107)t(cid:107) ∀x, i, t ∈ R|i| ,
Lmax = max
Li ,
i
where Ei is the pro jection from the ith block to Rn and |i| denotes the number of components
in block i, our analysis can be extended appropriately.
To make the AsySCD algorithm more eﬃcient, one can redeﬁne the steplength in
γ
γ
Algorithm 1 to be
rather than
. Our analysis can be applied to this variant by
Lmax
Li(j )
doing a change of variables to ˜x, with xi = Li
Lmax
of ˜x.

˜xi and deﬁning Li , Lres , and Lmax in terms

8. Conclusion

This paper proposes an asynchronous parallel stochastic coordinate descent algorithm for
minimizing convex ob jectives, in the unconstrained and separable-constrained cases. Sub-
linear convergence (at rate 1/K ) is proved for general convex functions, with stronger linear
convergence results for functions that satisfy an essential strong convexity property. Our
analysis indicates the extent to which parallel implementations can be expected to yield
near-linear speedup, in terms of a parameter that quantiﬁes the cross-coordinate inter-
actions in the gradient ∇f and a parameter τ that bounds the delay in updating. Our
computational experience conﬁrms the theory.

Acknowledgments

This pro ject is supported by NSF Grants DMS-0914524, DMS-1216318, and CCF-1356918;
NSF CAREER Award IIS-1353606; ONR Awards N00014-13-1-0129 and N00014-12-1-0041;
AFOSR Award FA9550-13-1-0138; a Sloan Research Fellowship; and grants from Oracle,
Google, and ExxonMobil.

Appendix A. Proofs for Unconstrained Case

This section contains convergence proofs for AsySCD in the unconstrained case.

1. http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/
2. http://www.daviddlewis.com/resources/testcollections/reuters21578/

302

AsySCD

We start with a technical result, then move to the proofs of the three main results of
Section 4.

Lemma 7 For any x, we have
(cid:107)x − PS (x)(cid:107)2(cid:107)∇f (x)(cid:107)2 ≥ (f (x) − f ∗ )2 .
If the essential strong convexity property (3) holds, we have
(cid:107)∇f (x)(cid:107)2 ≥ 2l(f (x) − f ∗ ).

Proof The ﬁrst inequality is proved as follows:
f (x) − f ∗ ≤ (cid:104)∇f (x), x − PS (x)(cid:105) ≤ (cid:107)∇f (x)(cid:107)(cid:107)PS (x) − x(cid:107).
For the second bound, we have from the deﬁnition (3), setting y ← x and x ← PS (x), that
(cid:107)x − PS (x)(cid:107)2
f ∗ − f (x) ≥ (cid:104)∇f (x), PS (x) − x(cid:105) +
l
2
(cid:107)∇f (x)(cid:107)2 ≥ − 1
∇f (x)(cid:107)2 − 1
(cid:107)PS (x) − x +
1
l
2
l
2l
2l

(cid:107)∇f (x)(cid:107)2 ,

=

as required.

(Theorem 1) We prove each of the two inequalities in (7) by induction. We start
Proof
E (cid:0)(cid:107)∇f (xj )(cid:107)2 − (cid:107)∇f (xj+1 )(cid:107)2(cid:1)
with the left-hand inequality. For all values of j , we have
= E(cid:104)∇f (xj ) + ∇f (xj+1 ), ∇f (xj ) − ∇f (xj+1 )(cid:105)
= E(cid:104)2∇f (xj ) + ∇f (xj+1 ) − ∇f (xj ), ∇f (xj ) − ∇f (xj+1 )(cid:105)
≤ 2E(cid:104)∇f (xj ), ∇f (xj ) − ∇f (xj+1 )(cid:105)
≤ 2E((cid:107)∇f (xj )(cid:107)(cid:107)∇f (xj ) − ∇f (xj+1 )(cid:107))
≤ 2LresE((cid:107)∇f (xj )(cid:107)(cid:107)xj − xj+1(cid:107))
≤ 2Lresγ
E((cid:107)∇f (xj )(cid:107)(cid:107)∇i(j )f (xk(j ) )(cid:107))
Lmax
E(n−1/2(cid:107)∇f (xj )(cid:107)2 + n1/2(cid:107)∇i(j )f (xk(j ) )(cid:107)2 )
≤ Lresγ
Lmax
E(n−1/2(cid:107)∇f (xj )(cid:107)2 + n1/2Ei(j ) ((cid:107)∇i(j )f (xk(j ) )(cid:107)2 ))
Lresγ
Lmax
E(n−1/2(cid:107)∇f (xj )(cid:107)2 + n−1/2(cid:107)∇f (xk(j ) )(cid:107)2 )
Lresγ
E (cid:0)(cid:107)∇f (xj )(cid:107)2 + (cid:107)∇f (xk(j ) )(cid:107)2(cid:1) .
=
Lmax
≤ Lresγ√
nLmax
We can use this bound to show that the left-hand inequality in (7) holds for j = 0. By
E (cid:0)(cid:107)∇f (x0 )(cid:107)2 − (cid:107)∇f (x1 )(cid:107)2(cid:1) ≤ Lresγ√
setting j = 0 in (30) and noting that k(0) = 0, we obtain
nLmax

2E((cid:107)∇f (x0 )(cid:107)2 ).

(30)

=

(31)

303

Liu, Wright, R´e, Bittorf, and Sridhar

From (6b), we have

= 1 − ρ−1 ,

√
2Lresγ
nLmax

ρτ ≤ ρ − 1
≤ ρ − 1
ρ
where the second inequality follows from ρ > 1. By substituting into (31), we obtain
ρ−1E((cid:107)∇f (x0 )(cid:107)2 ) ≤ E((cid:107)∇f (x1 )(cid:107)2 ), establishing the result for j = 1. For the inductive
step, we use (30) again, assuming that the left-hand inequality in (7) holds up to stage j ,
and thus that
E((cid:107)∇f (xk(j ) )(cid:107)2 ) ≤ ρτ E((cid:107)∇f (xj )(cid:107)2 ),
provided that 0 ≤ j − k(j ) ≤ τ , as assumed. By substituting into the right-hand side of
E (cid:0)(cid:107)∇f (xj )(cid:107)2(cid:1) .
E (cid:0)(cid:107)∇f (xj )(cid:107)2 − (cid:107)∇f (xj+1 )(cid:107)2(cid:1) ≤ 2Lresγ ρτ
(30) again, and using ρ > 1, we obtain
√
nLmax
By substituting (6b) we conclude that the left-hand inequality in (7) holds for all j .
E (cid:0)(cid:107)∇f (xj+1 )(cid:107)2 − (cid:107)∇f (xj )(cid:107)2(cid:1)
We now work on the right-hand inequality in (7). For all j , we have the following:
= E(cid:104)∇f (xj ) + ∇f (xj+1 ), ∇f (xj+1 ) − ∇f (xj )(cid:105)
≤ E((cid:107)∇f (xj ) + ∇f (xj+1 )(cid:107)(cid:107)∇f (xj ) − ∇f (xj+1 )(cid:107))
≤ LresE((cid:107)∇f (xj ) + ∇f (xj+1 )(cid:107)(cid:107)xj − xj+1(cid:107))
≤ LresE((2(cid:107)∇f (xj )(cid:107) + (cid:107)∇f (xj+1 ) − ∇f (xj )(cid:107))(cid:107)xj − xj+1(cid:107))
(cid:18) 2γ
≤ LresE(2(cid:107)∇f (xj )(cid:107)(cid:107)xj − xj+1(cid:107) + Lres(cid:107)xj − xj+1(cid:107)2 )
(cid:18) γ
Lresγ 2
≤ LresE
(cid:107)∇i(j )f (xk(j ) )(cid:107)2
(cid:107)∇f (xj )(cid:107)(cid:107)∇i(j )f (xk(j ) )(cid:107) +
L2
Lmax
max
(cid:18) γ
Lresγ 2
≤ LresE
(cid:107)∇i(j )f (xk(j ) )(cid:107)2
(n−1/2(cid:107)∇f (xj )(cid:107)2 + n1/2(cid:107)∇i(j )f (xk(j ) )(cid:107)2 +
L2
Lmax
max
(cid:19)
(n−1/2(cid:107)∇f (xj )(cid:107)2 + n1/2Ei(j ) ((cid:107)∇i(j )f (xk(j ) )(cid:107)2 ))+
= LresE
Lmax
(cid:18) γ
Lresγ 2
Ei(j ) ((cid:107)∇i(j )f (xk(j ) )(cid:107)2 )
L2
max
Lresγ 2
(cid:107)∇f (xk(j ) )(cid:107)2
(n−1/2(cid:107)∇f (xj )(cid:107)2 + n−1/2(cid:107)∇f (xk(j ) )(cid:107)2 ) +
= LresE
E (cid:0)(cid:107)∇f (xj )(cid:107)2 + (cid:107)∇f (xk(j ) )(cid:107)2(cid:1) +
nL2
Lmax
max
(cid:18) γLres
(cid:19)
γ 2L2
√
E((cid:107)∇f (xk(j ) )(cid:107)2 )
γLres
res
nL2
nLmax
max
γL2
√
√
E((cid:107)∇f (xj )(cid:107)2 ) +
≤ γLres
E((cid:107)∇f (xk(j ) )(cid:107)2 ),
res
+
nL2
nLmax
nLmax
max
where the last inequality is from the observation γ ≤ 1. By setting j = 0 in this bound,
(cid:18) 2γLres
(cid:19)
E (cid:0)(cid:107)∇f (x1 )(cid:107)2 − (cid:107)∇f (x0 )(cid:107)2(cid:1) ≤
and noting that k(0) = 0, we obtain
√
nLmax

E((cid:107)∇f (x0 )(cid:107)2 ).

(32)

(33)

(cid:19)

(cid:19)

(cid:19)

=

+

γL2
res
nL2
max

304

AsySCD

By using (6c), we have

√
2γLres
nLmax

+

γL2
res
nL2
max

=

Lresγ√
nLmax

(cid:18)
2 +

(cid:19)

Lres√
nLmax

≤ ρ − 1
ρτ < ρ − 1,

where the last inequality follows from ρ > 1. By substituting into (33), we obtain E((cid:107)∇f (x1 )(cid:107)2 ) ≤
ρE((cid:107)∇f (x0 )(cid:107)2 ), so the right-hand bound in (7) is established for j = 0. For the inductive
step, we use (32) again, assuming that the right-hand inequality in (7) holds up to stage j ,
and thus that

E((cid:107)∇f (xj )(cid:107)2 ) ≤ ρτ E((cid:107)∇f (xk(j ) )(cid:107)2 ),

provided that 0 ≤ j − k(j ) ≤ τ , as assumed. From (32) and the left-hand inequality in (7),
we have by substituting this bound that
(cid:18) 2γLresρτ
(cid:19)
E (cid:0)(cid:107)∇f (xj+1 )(cid:107)2 − (cid:107)∇f (xj )(cid:107)2(cid:1) ≤
√
nLmax

E((cid:107)∇f (xj )(cid:107)2 ).

γL2
resρτ
nL2
max

(34)

+

It follows immediately from (6c) that the term in parentheses in (34) is bounded above by
ρ − 1. By substituting this bound into (34), we obtain E((cid:107)∇f (xj+1 )(cid:107)2 ) ≤ ρE((cid:107)∇f (xj )(cid:107)2 ),
as required.

=

f

1
n

(cid:19)
ei(j )∇i(j )f (xk(j ) )
ei∇if (xk(j ) )

At this point, we have shown that both inequalities in (7) are satisﬁed for all j .
Next we prove (8) and (9). Take the expectation of f (xj+1 ) in terms of i(j ):
(cid:18)
(cid:19)
(cid:18)
xj − γ
Lmax
xj − γ
Lmax
f (xj ) − γ
Li
2L2
Lmax
max
γ 2
(cid:107)∇f (xk(j ) )(cid:107)2
(cid:104)∇f (xj ), ∇f (xk(j ) )(cid:105) +
2nLmax
(cid:123)(cid:122)
(cid:124)
(cid:125)
(cid:104)∇f (xk(j ) ) − ∇f (xj ), ∇f (xk(j ) )(cid:105)
(cid:19)
T1
− γ 2
(cid:107)∇f (xk(j ) )(cid:107)2 .
2nLmax

Ei(j )f (xj+1 ) = Ei(j )f
n(cid:88)
n(cid:88)
i=1
≤ 1
n
i=1
≤ f (xj ) − γ
nLmax
γ
= f (xj ) +
(cid:18) γ
nLmax
nLmax

(cid:104)∇f (xj ), ei∇if (xk(j ) )(cid:105) +

−

γ 2(cid:107)∇if (xk(j ) )(cid:107)2

(35)

305

Liu, Wright, R´e, Bittorf, and Sridhar

E

=

Lresγ
Lmax

The second term T1 is caused by delay. If there is no the delay issue, T1 should be 0
because of ∇f (xj ) = ∇f (xk(j ) ). We estimate the upper bound of (cid:107)∇f (xk(j ) ) − ∇f (xj )(cid:107):
(cid:107)∇f (xk(j ) ) − ∇f (xj )(cid:107) ≤ j−1(cid:88)
(cid:107)∇f (xd+1 ) − ∇f (xd )(cid:107)
j−1(cid:88)
d=k(j )
(cid:107)xd+1 − xd(cid:107)
≤ Lres
j−1(cid:88)
(cid:13)(cid:13)∇i(d)f (xk(d) )(cid:13)(cid:13) .
d=k(j )
d=k(j )
Then E(|T1 |) can be bounded by
 j−1(cid:88)

E(|T1 |) ≤ E((cid:107)∇f (xk(j ) ) − ∇f (xj )(cid:107)(cid:107)∇f (xk(j ) )(cid:107))
≤ Lresγ
(cid:107)∇i(d)f (xk(d) )(cid:107)(cid:107)∇f (xk(j ) )(cid:107)
 j−1(cid:88)

Lmax
d=k(j )
n1/2(cid:107)∇i(d)f (xk(d) )(cid:107)2 + n−1/2(cid:107)∇f (xk(j ) )(cid:107)2
E
 j−1(cid:88)
d=k(j )
n1/2Ei(d) ((cid:107)∇i(d)f (xk(d) )(cid:107)2 ) + n−1/2(cid:107)∇f (xk(j ) )(cid:107)2
 j−1(cid:88)

d=k(j )
j−1(cid:88)
d=k(j )
√
Lresγ
nLmax
2
d=k(j )
≤ τ ρτ Lresγ
√
E((cid:107)∇f (xk(j ) )(cid:107)2 )
nLmax

n−1/2(cid:107)∇f (xk(d) )(cid:107)2 + n−1/2(cid:107)∇f (xk(j ) )(cid:107)2

E((cid:107)∇f (xk(d) )(cid:107)2 + (cid:107)∇f (xk(j ) )(cid:107)2 )

≤ Lresγ
2Lmax

Lresγ
2Lmax

E

Lresγ
2Lmax

E



=

=

=

(36)

(37)

where the second line uses (36), and the ﬁnal inequality uses the fact for d between k(j )
and j − 1, k(d) lies in the range k(j ) − τ and j − 1, so we have |k(d) − k(j )| ≤ τ for all d.
Taking expectation on both sides of (35) in terms of all random variables, together with
(37), we obtain
(cid:19)
(cid:18) γ
E(f (xj+1 ) − f ∗ )
(cid:19)
(cid:18) γ
− γ 2
E(|T1 |) −
E((cid:107)∇f (xk(j ) )(cid:107)2 )
≤ E(f (xj ) − f ∗ ) +
γ
nLmax
nLmax
2nLmax
(cid:18)
(cid:19)
− τ ρτ Lresγ 2
− γ 2
E((cid:107)∇f (xk(j ) )(cid:107)2 )
n3/2L2
nLmax
2nLmax
max
E((cid:107)∇f (xk(j ) )(cid:107)2 ),
1 − ψ
= E(f (xj ) − f ∗ ) − γ
nLmax
2

≤ E(f (xj ) − f ∗ ) −

γ

306

AsySCD

,

.

(cid:41)

≥ max

which implies

E((cid:107)∇f (xk(j ) )(cid:107)2 ) ≥ max

which (because of (6a)) implies that E(f (xj ) − f ∗ ) is monotonically decreasing. From
(cid:41)
(cid:40)
Lemma 7 and the assumption (cid:107)xj − PS (xj )(cid:107) ≤ R for all j , we have
(f (xk(j ) ) − f ∗ )2
2l(f (xk(j ) ) − f ∗ ),
(cid:107)∇f (xk(j ) )(cid:107)2 ≥ max
(cid:41)
(cid:40)
(cid:107)xk(j ) − PS (xk(j ) )(cid:107)2
(f (xk(j ) ) − f ∗ )2
2l(f (xk(j ) ) − f ∗ ),
R2
(cid:40)
E(f (xk(j ) − f ∗ )2
2lE(f (xk(j ) − f ∗ ),
(cid:26)
(cid:27)
R2
E(f (xj ) − f ∗ )2
2lE(f (xj ) − f ∗ ),
≥ max
R2
(cid:19)
(cid:18)
From the ﬁrst upper bound (cid:107)∇f (xk(j ) )(cid:107)2 ≥ 2lE(f (xj ) − f ∗ ), we have
(cid:19)(cid:19)
(cid:18)
(cid:18)
E((cid:107)∇f (xk(j ) )(cid:107)2 )
1 − ψ
E(f (xj+1 ) − f ∗ ) ≤ E(f (xj ) − f ∗ ) − γ
nLmax
2
E(f (xj ) − f ∗ ),
1 − ψ
1 − 2lγ
nLmax
2
form which the linear convergence claim (8) follows by an obvious induction. From the
(cid:19)
(cid:18)
other bound (cid:107)∇f (xk(j ) )(cid:107)2 ≥ (f (xk(j ) )−f ∗ )2
, we have
R2
(cid:18)
(cid:19)
E(f (xj+1 ) − f ∗ ) ≤ E(f (xj ) − f ∗ ) − γ
E((cid:107)∇f (xk(j ) )(cid:107)2 )
1 − ψ
nLmax
2
(cid:18)
(cid:19)
1 − ψ
E((f (xj ) − f ∗ )2 )
γ
nLmaxR2
2
≤ E(f (xj ) − f ∗ ) −
1 − ψ
(E(f (xj ) − f ∗ ))2 ,
γ
γ
nLmaxR2
2
(cid:18)
(cid:19)
where the third line uses the Jensen’s inequality E(v2 ) ≥ (E(v))2 . Deﬁning
1 − ψ
γ
nLmaxR2
2

≤ E(f (xj ) − f ∗ ) −

C :=

≤

γ

,

γ

γ

γ

γ

we have

⇒

⇒

E(f (xj+1 ) − f ∗ ) ≤ E(f (xj ) − f ∗ ) − C (E(f (xj ) − f ∗ ))2
E(f (xj ) − f ∗ )
− C
≤
1
1
E(f (xj+1 ) − f ∗ )
E(f (xj+1 ) − f ∗ )
E(f (xj ) − f ∗ )
E(f (xj ) − f ∗ )
−
1
1
E(f (xj ) − f ∗ )
E(f (xj+1 ) − f ∗ )
E(f (xj+1 ) − f ∗ )
1
1
E(f (xj+1 ) − f ∗ )
f (x0 ) − f ∗ + C (j + 1)
⇒ E(f (xj+1 ) − f ∗ ) ≤
1
(f (x0 ) − f ∗ )−1 + C (j + 1)

≥ C

⇒

≥

,

≥ C

307

Liu, Wright, R´e, Bittorf, and Sridhar

ψ = 1 +

√
2eLres
nLmax

ρτ ≤ ρτ +1 =

nLmax ≤ e,
√
2eLres (τ +1)

which completes the proof of the sublinear rate (9).
 2eLres (τ +1)
(cid:18)
(cid:19) √
Proof (Corollary 2) Note ﬁrst that for ρ deﬁned by (11), we have
√
nLmax
nLmax ≤ e
2eLres
1 +
and thus from the deﬁnition of ψ (5) that
2τ ρτ Lres
√
√
2τ eLres
nLmax
nLmax
We show now that the steplength parameter choice γ = 1/ψ satisﬁes all the bounds in
(6), by showing that the second and third bounds are implied by the ﬁrst. For the second
bound (6b), we have
√
√
(ρ − 1)
≥ (ρ − 1)
nLmax
nLmax
2ρτ +1Lres
2eLres
where the second inequality follows from (11). For the third bound (6c), we have
√
(ρ − 1)
≥
nLmax
2eLres
2eLres
2
Lresρτ (2 + Lres√
Lresρτ (2 + Lres√
Lrese(2 + Lres√
2 + Lres√
nLmax
nLmax
nLmax
nLmax
We can thus set γ = 1/ψ , and by substituting this choice into (8) and using (38), we obtain
(12). We obtain (13) by making the same substitution into (9).

≥ 1 ≥ 1
ψ

,

≥ 1
ψ

.

≤ 1 +

≤ 2.

(38)

=

)

)

=

)

Proof (Theorem 3) From Markov’s inequality, we have
(cid:18)
(cid:19)j
P(f (xj ) − f ∗ ≥ ) ≤ −1E(f (xj ) − f ∗ )
(f (x0 ) − f ∗ )
≤ −1
1 −
l
(cid:12)(cid:12)(cid:12) (f (x0 ) − f ∗ ) with c = l/(2nLmax )
(cid:12)(cid:12)(cid:12)log f (x0 )−f ∗
2nLmax
≤ −1 (1 − c)
−(cid:12)(cid:12)(cid:12)log f (x0 )−f ∗
(cid:12)(cid:12)(cid:12)
(1/c)
η
≤ −1 (f (x0 ) − f ∗ )e
(cid:12)(cid:12)(cid:12)
−(cid:12)(cid:12)(cid:12)log f (x0 )−f ∗
η
= ηelog (f (x0 )−f ∗ )
e
η
η
≤ η ,
where the second inequality applies (12), the third inequality uses the deﬁnition of j (15),
and the second last inequality uses the inequality (1 − c)1/c ≤ e−1 ∀ c ∈ (0, 1), which proves
(cid:17) ≤ η ,
(cid:16)
the essentially strongly convex case. Similarly, the general convex case is proven by
f (x0 ) − f ∗
P(f (xj ) − f ∗ ≥ ) ≤ −1E(f (xj ) − f ∗ ) ≤
1 + j f (x0 )−f ∗
4nLmaxR2
where the second inequality uses (13) and the last inequality uses the deﬁnition of j (16).



308

AsySCD

Appendix B. Proofs for Constrained Case

We start by introducing notation and proving several preliminary results. Deﬁne
(∆j )i(j ) := (xj − xj+1 )i(j ) ,

(39)

and formulate the update in Step 4 of Algorithm 1 in the following way:
(cid:107)x − xj (cid:107)2 .
(cid:104)∇i(j )f (xk(j ) ), (x − xj )i(j ) (cid:105) +
Lmax
xj+1 = arg min
x∈Ω
2γ
(Note that (xj+1 )i = (xj )i for i (cid:54)= i(j ).) From the optimality condition for this formulation,
(cid:28)
(cid:29)
we have
(x − xj+1 )i(j ) , ∇i(j )f (xk(j ) ) − Lmax
≥ 0,
for all x ∈ Ω.
(∆j )i(j )
γ
This implies in particular that for all x ∈ Ω, we have
(cid:10)(PS (x) − xj+1 )i(j ) , ∇i(j )f (xk(j ) )(cid:11) ≥ Lmax
(cid:10)(PS (x) − xj+1 )i(j ) , (∆j )i(j )
γ
From the deﬁnition of Lmax , and using the notation (39), we have
f (xj+1 ) ≤ f (xj ) + (cid:104)∇i(j )f (xj ), −(∆j )i(j ) (cid:105) +
(cid:107)(∆j )i(j )(cid:107)2 ,

(cid:11) .

(40)

Lmax
2

which indicates that
(cid:104)∇i(j )f (xj ), (∆j )i(j ) (cid:105) ≤ f (xj ) − f (xj+1 ) +
(cid:28)
From optimality conditions for this deﬁnition, we have

Lmax
2
(cid:29)

(cid:107)(∆j )i(j )(cid:107)2 .

(41)

( ¯xj+1 − xj )

x − ¯xj+1 , ∇f (xk(j ) ) +
Lmax
γ
We now deﬁne ∆j := xj − ¯xj+1 , and note that this deﬁnition is consistent with (∆)i(j )
deﬁned in (39). It can be seen that
Ei(j ) ((cid:107)xj+1 − xj (cid:107)2 ) =

(cid:107) ¯xj+1 − xj (cid:107)2 .

≥ 0 ∀ x ∈ Ω.

(42)

1
n

We now proceed to prove the main results of Section 5.
Proof (Theorem 4) We prove (20) by induction. First, note that for any vectors a and b,
we have
(cid:107)a(cid:107)2 − (cid:107)b(cid:107)2 = 2(cid:107)a(cid:107)2 − ((cid:107)a(cid:107)2 + (cid:107)b(cid:107)2 ) ≤ 2(cid:107)a(cid:107)2 − 2(cid:104)a, b(cid:105) ≤ 2(cid:104)a, a − b(cid:105) ≤ 2(cid:107)a(cid:107)(cid:107)a − b(cid:107),

Thus for all j , we have
(cid:107)xj−1 − ¯xj (cid:107)2 − (cid:107)xj − ¯xj+1(cid:107)2 ≤ 2(cid:107)xj−1 − ¯xj (cid:107)(cid:107)xj − ¯xj+1 − xj−1 + ¯xj (cid:107).

(43)

309

Liu, Wright, R´e, Bittorf, and Sridhar

=

≤

≤

The second factor in the r.h.s. of (43) is bounded as follows:
(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)xj − PΩ (xj − γ
(cid:107)xj − ¯xj+1 − xj−1 + ¯xj (cid:107)
(cid:13)(cid:13)(cid:13)(cid:13)xj − γ
∇f (xk(j−1) )))
∇f (xk(j ) )) − (xj−1 − PΩ (xj−1 − γ
Lmax
Lmax
(cid:13)(cid:13)(cid:13)(cid:13) +
∇f (xk(j ) )) − (xj−1 − γ
∇f (xk(j ) ) − PΩ (xj − γ
∇f (xk(j−1) )
(cid:13)(cid:13)∇f (xk(j−1) ) − ∇f (xk(j ) )(cid:13)(cid:13)
Lmax
Lmax
Lmax
(cid:13)(cid:13)(cid:13)(cid:13)xj − γ
(cid:13)(cid:13)(cid:13)(cid:13)
∇f (xk(j−1) )))
−PΩ (xj−1 − γ
γ
Lmax
Lmax
(cid:13)(cid:13)∇f (xk(j−1) ) − ∇f (xk(j ) )(cid:13)(cid:13)
∇f (xk(j−1) )
∇f (xk(j ) ) − xj−1 +
γ
Lmax
Lmax
(cid:13)(cid:13)∇f (xk(j ) ) − ∇f (xk(j−1) )(cid:13)(cid:13)
γ
+
Lmax
≤ (cid:107)xj − xj−1(cid:107) + 2
γ
max{k(j−1),k(j )}−1(cid:88)
Lmax
(cid:107)∇f (xd ) − ∇f (xd+1)(cid:107)
max{k(j−1),k(j )}−1(cid:88)
d=min{k(j−1),k(j )}
d=min{k(j−1),k(j )}

≤ (cid:107)xj − xj−1(cid:107) + 2

≤ (cid:107)xj − xj−1(cid:107) + 2

(cid:107)xd − xd+1(cid:107),

γLres
Lmax

γ
Lmax

(44)

where the ﬁrst inequality follows by adding and subtracting a term, and the second inequal-
ity uses the nonexpansive property of pro jection:

(cid:107)(z − PΩ (z )) − (y − PΩ (y))(cid:107) ≤ (cid:107)z − y(cid:107).
One can see that j − 1 − τ ≤ k(j − 1) ≤ j − 1 and j − τ ≤ k(j ) ≤ j , which implies that
j − 1 − τ ≤ d ≤ j − 1 for each index d in the summation in (44). It also follows that

max{k(j − 1), k(j )} − 1 − min{k(j − 1), k(j )} ≤ τ .

(45)

We set j = 1, and note that k(0) = 0 and k(1) ≤ 1. Thus, in this case, we have that
the lower and upper limits of the summation in (44) are 0 and 0, respectively. Thus, this
(cid:19)
(cid:18)
summation is vacuous, and we have

(cid:107)x1 − ¯x2 + x0 − ¯x1(cid:107) ≤
γLres
Lmax
(cid:18)
(cid:19)
By substituting this bound in (43) and setting j = 1, we obtain
2 + 4

E((cid:107)x0 − ¯x1(cid:107)2 ) − E((cid:107)x1 − ¯x2(cid:107)2 ) ≤

1 + 2

(cid:107)x1 − x0(cid:107),

γLres
Lmax

E((cid:107)x1 − x0(cid:107)(cid:107) ¯x1 − x0(cid:107)).

(46)

310

AsySCD

For any j , we have

E(n1/2(cid:107)xj − xj−1(cid:107)2 + n−1/2(cid:107) ¯xj − xj−1(cid:107)2 )
E((cid:107)xj − xj−1(cid:107)(cid:107) ¯xj − xj−1(cid:107)) ≤ 1
2
E(n1/2Ei(j−1) ((cid:107)xj − xj−1(cid:107)2 ) + n−1/2(cid:107) ¯xj − xj−1(cid:107)2 )
1
2
E(n−1/2(cid:107) ¯xj − xj−1(cid:107)2 + n−1/2(cid:107) ¯xj − xj−1(cid:107)2 )
1
=
2
= n−1/2E(cid:107) ¯xj − xj−1(cid:107)2 .

=

(47)

Returning to (46), we have

E((cid:107)x0 − ¯x1(cid:107)2 ) − E((cid:107)x1 − ¯x2(cid:107)2 ) ≤ 2n−1/2E(cid:107) ¯x1 − x0(cid:107)2

which implies that

E((cid:107)x0 − ¯x1(cid:107)2 ) ≤

(cid:18)
1 − 2√
n

√
− 4γLres
nLmax

(cid:19)−1

E((cid:107)x1 − ¯x2(cid:107)2 ) ≤ ρE((cid:107)x1 − ¯x2(cid:107)2 ).

To see the last inequality above, we only need to verify that
(cid:19) √
(cid:18)
1 − ρ−1 − 2√
nLmax
4Lres
n

γ ≤

.

This proves that (20) holds for j = 1.
To take the inductive step, we assume that show that (20) holds up to index j − 1. We
have for j − 1 − τ ≤ d ≤ j − 2 that

=

E((cid:107)xd − xd+1(cid:107)(cid:107) ¯xj − xj−1(cid:107)) ≤ 1
E(n1/2(cid:107)xd − xd+1(cid:107)2 + n−1/2(cid:107) ¯xj − xj−1(cid:107)2 )
2
E(n1/2Ei(d) ((cid:107)xd − xd+1(cid:107)2 ) + n−1/2(cid:107) ¯xj − xj−1(cid:107)2 )
1
2
E(n−1/2(cid:107)xd − ¯xd+1(cid:107)2 + n−1/2(cid:107) ¯xj − xj−1(cid:107)2 )
1
=
2
≤ 1
E(n−1/2ρτ (cid:107)xj−1 − ¯xj (cid:107)2 + n−1/2(cid:107) ¯xj − xj−1(cid:107)2 )
2
≤ ρτ
E((cid:107) ¯xj − xj−1(cid:107)2 ),
n1/2

(48)

311

Liu, Wright, R´e, Bittorf, and Sridhar

≤ 2E

where the second inequality uses the inductive hypothesis. By substituting (44) into (43)
and taking expectation on both sides of (43), we obtain
E((cid:107)xj−1 − ¯xj (cid:107)2 ) − E((cid:107)xj − ¯xj+1(cid:107)2 )
(cid:107)xj − xj−1(cid:107) + 2
(cid:107) ¯xj − xj−1(cid:107)
≤ 2E((cid:107) ¯xj − xj−1(cid:107)(cid:107) ¯xj − ¯xj+1 + xj − xj−1(cid:107))
= 2E((cid:107) ¯xj − xj−1(cid:107)(cid:107)xj − xj−1(cid:107))+
max{k(j−1),k(j )}−1(cid:88)
E((cid:107) ¯xj − xj−1(cid:107)(cid:107)xd − xd+1(cid:107))
γLres
(cid:18)
(cid:19)
Lmax
d=min{k(j−1),k(j )}
4γLres τ ρτ
E((cid:107)xj−1 − ¯xj (cid:107)2 ),
≤ n−1/2
Lmax

max{k(j−1),k(j )}−1(cid:88)
d=min{k(j−1),k(j )}


(cid:107)xd − xd+1(cid:107)

γLres
Lmax

2 +

4

(cid:19)(cid:19)−1
(cid:18)
(cid:18)
where the last line uses (45), (47), and (48). It follows that
2 +

E((cid:107)xj−1 − ¯xj (cid:107)2 ) ≤

4γLres τ ρτ
E((cid:107)xj − ¯xj+1(cid:107)2 ) ≤ ρE((cid:107)xj − ¯xj+1(cid:107)2 ).
1 − n−1/2
Lmax
(cid:19) √
(cid:18)
(cid:18)
(cid:19)
To see the last inequality, one only needs to verify that
1 − ρ−1 − 2√
nLmax
4Lres τ ρτ ,
2 +
n

ρ−1 ≤ 1 − 1√
n

4γLres τ ρτ
Lmax

⇔ γ ≤

≤ n−1

f (xj + (∆j )i )

and the last inequality is true because of the upper bound of γ in (19). It proves (20).
Next we will show the expectation of ob jective is monotonically decreasing. We have
using the deﬁnition (39) that
n(cid:88)
Ei(j ) (f (xj+1 )) = n−1
(cid:20)
n(cid:88)
i=1
f (xj ) + (cid:104)∇if (xj ), ( ¯xj+1 − xj )i (cid:105) +
(cid:18)
i=1
(cid:18)
(cid:104)∇f (xj ), ¯xj+1 − xj (cid:105) +
= f (xj ) + n−1
(cid:18) Lmax
(cid:104)∇f (xk(j ) ), ¯xj+1 − xj (cid:105) +
1
n
(cid:18) 1
(cid:19) Lmax
(cid:107) ¯xj+1 − xj (cid:107)2 − Lmax
1
n
2
γ
(cid:107) ¯xj+1 − xj (cid:107)2 +
− 1
γ
2
n

Lmax
2
(cid:104)∇f (xj ) − ∇f (xk(j ) ), ¯xj+1 − xj (cid:105)
1
Lmax
n
2
(cid:104)∇f (xj ) − ∇f (xk(j ) ), ¯xj+1 − xj (cid:105)
(cid:107) ¯xj+1 − xj (cid:107)2
1
n
(cid:104)∇f (xj ) − ∇f (xk(j ) ), ¯xj+1 − xj (cid:105),

Lmax
2
(cid:107) ¯xj+1 − xj (cid:107)2
(cid:19)
(cid:107) ¯xj+1 − xj (cid:107)2

(cid:107)(xj+1 − xj )i(cid:107)2
(cid:19)
(cid:19)

= f (xj ) −

≤ f (xj ) +

= f (xj ) +

(cid:21)

(49)

+

+

1
n

312

AsySCD

where the second inequality uses (42). Consider the expectation of the last term on the
right-hand side of this expression. We have

E

≤ Lres
2

(cid:107)xd − xd+1(cid:107)(cid:107) ¯xj+1 − xj (cid:107)

E(cid:104)∇f (xj ) − ∇f (xk(j ) ), ¯xj+1 − xj (cid:105)
≤ E(cid:107)∇f (xj ) − ∇f (xk(j ) )(cid:107)(cid:107) ¯xj+1 − xj (cid:107)
j−1(cid:88)
(cid:107)∇f (xd ) − ∇f (xd+1 )(cid:107)(cid:107) ¯xj+1 − xj (cid:107)
≤ E
j−1(cid:88)
d=k(j )
≤ LresE
j−1(cid:88)
d=k(j )
(n1/2(cid:107)xd − xd+1(cid:107)2 + n−1/2(cid:107) ¯xj+1 − xj (cid:107)2 )
j−1(cid:88)
d=k(j )
(n1/2Ei(d) ((cid:107)xd − xd+1(cid:107)2 ) + n−1/2(cid:107) ¯xj+1 − xj (cid:107)2 )
j−1(cid:88)
d=k(j )
(n−1/2(cid:107)xd − ¯xd+1(cid:107)2 + n−1/2(cid:107) ¯xj+1 − xj (cid:107)2 )
j−1(cid:88)
d=k(j )
(1 + ρτ )(cid:107) ¯xj+1 − xj (cid:107)2
d=k(j )
E(cid:107) ¯xj+1 − xj (cid:107)2 ,

≤ Lres
2n1/2
≤ Lres τ ρτ
n1/2

Lres
2

Lres
2

E

E

=

=

E

(50)

where the ﬁfth inequality uses (20). By taking expectation on both sides of (49) and
substituting (50), we have
(cid:19)
(cid:19)
(cid:18)(cid:18) 1
γ

Lmax − Lres τ ρτ
n1/2

E(cid:107) ¯xj+1 − xj (cid:107)2 .

− 1
2

E(f (xj+1 )) ≤ E(f (xj )) − 1
n
(cid:16) 1
(cid:17)
γ − 1
2

To see

n1/2 ≥ 0, we only need to verify
Lmax − Lres τ ρτ
(cid:18) 1
(cid:19)−1
2

Lres τ ρτ
√
nLmax

γ ≤

+

which is implied by the ﬁrst upper bound of γ (19). Therefore, we have proved the mono-
tonicity E(f (xj+1 )) ≤ E(f (xj )).

313

Liu, Wright, R´e, Bittorf, and Sridhar

Next we prove the sublinear convergence rate for the constrained smooth convex case
in (22). We have

(cid:107)xj+1 − PS (xj+1 )(cid:107)2 ≤ (cid:107)xj+1 − PS (xj )(cid:107)2
= (cid:107)xj − (∆j )i(j )ei(j ) − PS (xj )(cid:107)2
= (cid:107)xj − PS (xj )(cid:107)2 − |(∆j )i(j ) |2 − 2 (cid:0)(xj − PS (xj ))i(j ) − (∆j )i(j )
(cid:1) (∆j )i(j )
= (cid:107)xj − PS (xj )(cid:107)2 + |(∆j )i(j ) |2 − 2(xj − PS (xj ))i(j ) (∆j )i(j )
= (cid:107)xj − PS (xj )(cid:107)2 − (cid:107)(∆j )i(j )(cid:107)2 + 2(PS (xj ) − xj+1 )i(j ) (∆j )i(j )
≤ (cid:107)xj − PS (xj )(cid:107)2 − |(∆j )i(j ) |2 +
(PS (xj ) − xj+1 )i(j )∇i(j )f (xk(j ) )
2γ
Lmax
= (cid:107)xj − PS (xj )(cid:107)2 − |(∆j )i(j ) |2 +
(PS (xj ) − xj )i(j )∇i(j )f (xk(j ) )+
2γ
(cid:0)(∆j )i(j )∇i(j )f (xj ) + (∆j )i(j )
(cid:0)∇i(j )f (xk(j ) ) − ∇i(j )f (xj )(cid:1)(cid:1)
Lmax
2γ
Lmax
(cid:18)
(PS (xj ) − xj )i(j )∇i(j )f (xk(j ) )+
≤ (cid:107)xj − PS (xj )(cid:107)2 − |(∆j )i(j ) |2 +
2γ
Lmax
(cid:0)∇i(j )f (xk(j ) ) − ∇i(j )f (xj )(cid:1) (cid:19)
f (xj ) − f (xj+1 ) +
|(∆j )i(j ) |2
Lmax
2γ
2
Lmax
+ (∆j )i(j )
(cid:124)
(cid:123)(cid:122)
(cid:125)
(PS (xj ) − xj )i(j )∇i(j )f (xk(j ) )
= (cid:107)xj − PS (xj )(cid:107)2 − (1 − γ )|(∆j )i(j ) |2 +
2γ
(cid:0)∇i(j )f (xk(j ) ) − ∇i(j )f (xj )(cid:1)
Lmax
T1
(cid:125)
(cid:123)(cid:122)
(cid:124)
(∆j )i(j )
,
T2

(f (xj ) − f (xj+1 )) +

2γ
Lmax

2γ
Lmax

+

(51)

314

AsySCD

where the second inequality uses (40) and the third inequality uses (41). We now seek upper
bounds on the quantities T1 and T2 in the expectation sense. For T1 , we have

(cid:19)

Lmax
2

(cid:107)xd − xd+1(cid:107)2

j−1(cid:88)
(cid:104)xd − xd+1 , ∇f (xk(j ) )(cid:105)
d=k(j )

E(T1 ) = n−1E(cid:104)PS (xj ) − xj , ∇f (xk(j ) )(cid:105)
= n−1E(cid:104)PS (xj ) − xk(j ) , ∇f (xk(j ) )(cid:105) + n−1E
= n−1E(cid:104)PS (xj ) − xk(j ) , ∇f (xk(j ) )(cid:105)
j−1(cid:88)
(cid:104)xd − xd+1 , ∇f (xd )(cid:105) + (cid:104)xd − xd+1 , ∇f (xk(j ) ) − ∇f (xd )(cid:105)
+ n−1E
(cid:18)
j−1(cid:88)
d=k(j )
≤ n−1E(f ∗ − f (xk(j ) )) + n−1E
f (xd ) − f (xd+1 ) +
j−1(cid:88)
d=k(j )
(cid:104)xd − xd+1 , ∇f (xk(j ) ) − ∇f (xd )(cid:105)
j−1(cid:88)
d=k(j )
= n−1E(f ∗ − f (xj )) +
j−1(cid:88)
d=k(j )
(cid:104)xd − xd+1 , ∇f (xk(j ) ) − ∇f (xd )(cid:105)
j−1(cid:88)
d=k(j )
= n−1E(f ∗ − f (xj )) +
j−1(cid:88)
d=k(j )
(cid:104)xd − xd+1 , ∇f (xk(j ) ) − ∇f (xd )(cid:105)
d=k(j )
Lmax τ ρτ
≤ n−1E(f ∗ − f (xj )) +
E(cid:107)xj − ¯xj+1(cid:107)2
j−1(cid:88)
2n2
(cid:123)(cid:122)
(cid:125)
(cid:124)
E(cid:104)xd − xd+1 , ∇f (xk(j ) ) − ∇f (xd )(cid:105)
,
d=k(j )
T3

(cid:107)xd − ¯xd+1(cid:107)2

(cid:107)xd − xd+1(cid:107)2

Lmax
2n

E

Lmax
2n2

E

+ n−1E

+ n−1

+ n−1E

+ n−1E

315

Liu, Wright, R´e, Bittorf, and Sridhar

where the last inequality uses (20). The upper bound of E(T3 ) is estimated by

E(T3 ) = E(cid:104)xd − xd+1 , ∇f (xk(j ) ) − ∇f (xd )(cid:105)
= E(Ei(d) (cid:104)xd − xd+1 , ∇f (xk(j ) ) − ∇f (xd )(cid:105))
= n−1E(cid:104)xd − ¯xd+1 , ∇f (xk(j ) ) − ∇f (xd )(cid:105)
≤ n−1E(cid:107)xd − ¯xd+1(cid:107)(cid:107)∇f (xk(j ) ) − ∇f (xd )(cid:107)
≤ n−1E((cid:107)xd − ¯xd+1(cid:107) d−1(cid:88)
(cid:107)∇f (xt ) − ∇f (xt+1 )(cid:107))
d−1(cid:88)
t=k(j )
E((cid:107)xd − ¯xd+1(cid:107)(cid:107)xt − xt+1(cid:107))
≤ Lres
d−1(cid:88)
n
t=k(j )
d−1(cid:88)
t=k(j )
E(n−1/2(cid:107)xd − ¯xd+1(cid:107)2 + n−1/2(cid:107)xt − ¯xt+1(cid:107)2 )
d−1(cid:88)
t=k(j )
≤ Lresρτ
n3/2
t=k(j )
≤ Lres τ ρτ
E((cid:107)xj − ¯xj+1(cid:107)2 ).
n3/2

E(n−1/2(cid:107)xd − ¯xd+1(cid:107)2 + n1/2(cid:107)xt − xt+1(cid:107)2 )

E((cid:107)xj − ¯xj+1(cid:107)2 )

≤ Lres
2n

≤ Lres
2n

where the second last inequality uses (20). Therefore, E(T1 ) can be bounded by

E(T1 ) = E(cid:104)(PS (xj ) − xj )i(j ) , ∇i(j )f (xk(j ) )(cid:105)
Lmax τ ρτ
E(f ∗ − f (xj )) +
E(cid:107)xj − ¯xj+1(cid:107)2 +
≤ 1
(cid:18)
(cid:18) Lmax τ ρτ
(cid:19)
2n2
n
f ∗ − Ef (xj ) +
2n

Lres τ 2ρτ
n3/2

1
n

=

j−1(cid:88)
E((cid:107)xj − ¯xj+1(cid:107)2 )
(cid:19)
d=k(j )
E((cid:107)xj − ¯xj+1(cid:107)2 )

Lres τ ρτ
n5/2

.

+

316

(52)

AsySCD

E

Lres
2n

≤ Lres
n

(cid:0)∇i(j )f (xk(j ) ) − ∇i(j )f (xj )(cid:1)
For T2 , we have
E(T2 ) = E(∆j )i(j )
= n−1E(cid:104)∆j , ∇f (xk(j ) ) − ∇f (xj )(cid:105)

 j−1(cid:88)
≤ n−1E((cid:107)∆j (cid:107)(cid:107)∇f (xk(j ) ) − ∇f (xj )(cid:107))
(cid:107)∆j (cid:107)(cid:107)∇f (xd ) − ∇f (xd+1 )(cid:107)
≤ n−1E

 j−1(cid:88)
d=k(j )
(cid:107)∆j (cid:107)(cid:107)xd − xd+1(cid:107)
E
 j−1(cid:88)
d=k(j )
n−1/2(cid:107)∆j (cid:107)2 + n1/2(cid:107)xd − xd+1(cid:107)2
 j−1(cid:88)
d=k(j )
 j−1(cid:88)
d=k(j )
 j−1(cid:88)
d=k(j )
E(cid:107)xj − ¯xj+1(cid:107)2 + E(cid:107)xd − ¯xd+1(cid:107)2
j−1(cid:88)
d=k(j )
≤ Lres (1 + ρτ )
2n3/2
d=k(j )
≤ Lres τ ρτ
E(cid:107)xj − ¯xj+1(cid:107)2 ,
n3/2


n−1/2(cid:107)xj − ¯xj+1(cid:107)2 + n1/2Ei(d)(cid:107)xd − xd+1(cid:107)2

n−1/2(cid:107)xj − ¯xj+1(cid:107)2 + n−1/2(cid:107)xd − ¯xd+1(cid:107)2


E(cid:107)xj − ¯xj+1(cid:107)2

E

Lres
2n

E

Lres
2n

=

=

Lres
2n3/2

=

=



(53)

where the second last inequality uses (20).
By taking the expectation on both sides of (51), using Ei(j ) (|(∆j )i(j ) |2 ) = n−1(cid:107)xj −
¯xj+1(cid:107)2 , and substituting the upper bounds from (52) and (53), we obtain
(cid:19)
(cid:18)
E(cid:107)xj+1 − PS (xj+1 )(cid:107)2 ≤ E(cid:107)xj − PS (xj )(cid:107)2
1 − γ − 2γLres τ ρτ
− 2γLres τ 2ρτ
− γ τ ρτ
− 1
E(cid:107)xj − ¯xj+1(cid:107)2
Lmaxn3/2
Lmaxn1/2
n
n
(f ∗ − Ef (xj )) +
(Ef (xj ) − Ef (xj+1 ))
2γ
2γ
+
Lmaxn
Lmax
(Ef (xj ) − Ef (xj+1 )).
(f ∗ − Ef (xj )) +
≤ E(cid:107)xj − PS (xj )(cid:107)2 +
2γ
2γ
(54)
Lmax
Lmaxn
In the second inequality, we were able to drop the term involving E(cid:107)xj − ¯xj+1(cid:107)2 by using
the fact that

1 − γ − 2γLres τ ρτ
Lmaxn1/2

− γ τ ρτ
n

− 2γLres τ 2ρτ
Lmaxn3/2

= 1 − γψ ≥ 0,

317

Liu, Wright, R´e, Bittorf, and Sridhar

which follows from the deﬁnition (18) of ψ and from the ﬁrst upper bound on γ in (19). It
follows that
E(cid:107)xj+1 − PS (xj+1 )(cid:107)2 +
(Ef (xj+1 ) − f ∗ )
2γ
Lmax
≤ E(cid:107)xj − PS (xj )(cid:107)2 +
(Ef (xj ) − f ∗ ) − 2γ
(Ef (xj ) − f ∗ )
2γ
j(cid:88)
Lmax
Lmaxn
(f (x0 ) − f ∗ ) − 2γ
Lmaxn
t=0
(Ef (xj+1 ) − f ∗ ),

2γ
Lmax
(f (x0 ) − f ∗ ) − 2γ (j + 1)
Lmaxn

≤ (cid:107)x0 − PS (x0 )(cid:107)2 +

(Ef (xt ) − f ∗ )

≤ R2 +

2γ
Lmax

(55)

where the second inequality follows by applying induction to the inequality
Sj+1 ≤ Sj − 2γ
E(f (xj ) − f ∗ ),
Lmaxn

where

E(f (xj ) − PS (xj )),
Sj := E((cid:107)xj − PS (xj )(cid:107)2 ) +
2γ
Lmax
and the last line uses the monotonicity of Ef (xj ) (proved above) and the assumed bound
(cid:107)x0 − PS (x0 )(cid:107) ≤ R. It implies that
E(cid:107)xj+1 − PS (xj+1 )(cid:107)2 +
2γ
Lmax
≤ R2 +
(f (x0 ) − f ∗ )
2γ
Lmax
⇒ 2γ (n + j + 1)
(f (x0 ) − f ∗ )
(Ef (xj+1 ) − f ∗ ) ≤ R2 +
2γ
Lmax
Lmaxn
⇒ Ef (xj+1 ) − f ∗ ≤ n(R2Lmax + 2γ (f (x0 ) − f ∗ ))
2γ (n + j + 1)

(Ef (xj+1 ) − f ∗ ) +

(Ef (xj+1 ) − f ∗ )

2γ (j + 1)
Lmaxn

.

This completes the proof of the sublinear convergence rate (22).
Finally, we prove the linear convergence rate (21) for the essentially strongly convex
case. All bounds proven above hold, and we make use the following additional property:
f (xj ) − f ∗ ≥ (cid:104)∇f (PS (xj )), xj − PS (xj )(cid:105) +
(cid:107)xj − PS (xj )(cid:107)2 ,
(cid:107)xj − PS (xj )(cid:107)2 ≥ l
l
2
2
due to feasibility of xj and (cid:104)∇f (PS (xj )), xj − PS (xj )(cid:105) ≥ 0. By using this result together
(cid:19)
(cid:18)
with some elementary manipulation, we obtain
(cid:19)
(cid:18)
(f (xj ) − f ∗ ) +
f (xj ) − f ∗ =
1 − Lmax
lγ + Lmax
(cid:18)
1 − Lmax
lγ + Lmax
(cid:107)xj − PS (xj )(cid:107)2 +
Lmax l
2(lγ + Lmax )

(f (xj ) − f ∗ )
Lmax
lγ + Lmax
(cid:19)
(cid:107)xj − PS (xj )(cid:107)2
Lmax l
2(lγ + Lmax )
(f (xj ) − f ∗ )
2γ
Lmax

(f (xj ) − f ∗ ) +

(56)

≥

=

.

318

AsySCD

Recalling (55), we have

E(cid:107)xj+1 − PS (xj+1 )(cid:107)2 +
2γ
Lmax
≤ E(cid:107)xj − PS (xj )(cid:107)2 +
2γ
Lmax

(Ef (xj+1 ) − f ∗ )
(Ef (xj ) − f ∗ ) − 2γ
Lmaxn

(Ef (xj ) − f ∗ ).

(57)

,

=

≤

1 −

which yields (21).

(cid:19)(cid:19)
(cid:19)
(Ef (xj ) − f ∗ )
(cid:19)

By taking the expectation of both sides in (56) and substituting in the last term of (57),
we obtain
(Ef (xj+1 ) − f ∗ )
E(cid:107)xj+1 − PS (xj+1 )(cid:107)2 +
2γ
Lmax
(cid:18)
(cid:18)
≤ E(cid:107)xj − PS (xj )(cid:107)2 +
(Ef (xj ) − f ∗ )
2γ
Lmax
(cid:18)
(cid:19) (cid:18)
− 2γ
E(cid:107)xj − PS (xj )(cid:107)2 +
Lmax l
2(lγ + Lmax )
Lmaxn
(cid:19)j+1 (cid:18)
(cid:18)
E(cid:107)xj − PS (xj )(cid:107)2 +
1 −
l
n(l + γ−1Lmax )
l
n(l + γ−1Lmax )

2γ
Lmax
(Ef (xj ) − f ∗ )
2γ
Lmax
(cid:107)x0 − PS (x0 )(cid:107)2 +
2γ
Lmax
(cid:16)
. Using the bound
Proof (Corollary 5) To apply Theorem 4, we ﬁrst show ρ >
(cid:19) 2√
(cid:18)
(cid:19)
(cid:18)
(cid:19)
(cid:19) (cid:18)
(cid:18)
(23), together with Lres/Lmax ≥ 1, we obtain
(cid:19) 2√
(cid:19) 2√
(cid:18)
(cid:18)
(cid:19)
(cid:18)
√
1 − 2√
√
√
4eτ Lres
4eτ Lres
4eτ Lres
1 +
1 +
1 +
nLmax
n
nLmax
n
nLmax
4eτ√
2eτ − 1 − 1
−
1
> 1,
1 +
= 1 +
τ + 1
τ + 1
n
n
n
where the last inequality uses τ ≥ 1. Note that for ρ deﬁned by (24), and using (23), we
 4eτ Lres (τ +1)
(cid:18)
(cid:19) √
have
√
nLmax
nLmax ≤ e
4eτ Lres
(cid:18)
(cid:19)
Thus from the deﬁnition of ψ (18), we have that
(cid:19)
(cid:18)
2τ
2 +
n
≤ 1 +
2 +

nLmax ≤ e.
√
4eτ Lres (τ +1)
(cid:18)
2 +

(f (x0 ) − f ∗ )
(cid:17)−1

ρτ ≤ ρτ +1 =

√
4eτ Lres
nLmax

1 − 2√
n

(cid:19)

1 +

+

ψ = 1 +

(cid:19)
(cid:18) 1
Lres τ ρτ
Lres τ ρτ
Lmax√
√
≤ 1 +
4eLres τ (τ + 1)
nLres
nLmax
1√
≤ 2.
1
1
2τ
1
16
4
n
4(τ + 1)
10
n
(The second last inequality uses n ≥ 5 and τ ≥ 1.) Thus, the steplength parameter choice
γ = 1/2 satisﬁes the ﬁrst bound in (19). To show that the second bound in (19) holds also,

≤ 1 +

1√
n

2τ
n

(58)

+

+

+

+

=

−

≥

1 +

319

Liu, Wright, R´e, Bittorf, and Sridhar

we have

(cid:18)
(cid:19) √
(cid:18) ρ − 1
− 2√
− 2√
1 − 1
nLmax
4Lres τ ρτ =
ρ
ρ
n
n
2Lres τ ρτ ≥ 1 − 1
4Lres τ ρτ +1 − Lmax
4eτ Lres
1
2
2
We can thus set γ = 1/2, and by substituting this choice into (21), we obtain (25). We
obtain (26) by making the same substitution into (22).

(cid:19) √
nLmax
4Lres τ ρτ

=

=

.

References

A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization. In Advances in
Neural Information Processing Systems 24, pages 873–881. 2011. URL http://papers.
nips.cc/paper/4247-distributed-delayed-stochastic-optimization.pdf.

H. Avron, A. Druinsky, and A. Gupta. Revisiting asynchronous linear solvers: Provable
convergence rate through randomization. IPDPS, 2014.

A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM J. Imaging Sciences, 2(1):183–202, 2009.

A. Beck and L. Tetruashvili. On the convergence of block coordinate descent type methods.
SIAM Journal on Optimization, 23(4):2037–2060, 2013.

D. P. Bertsekas and J. N. Tsitsiklis. Paral lel and Distributed Computation: Numerical
Methods. Pentice Hall, 1989.

S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and
statistical learning via the alternating direction method of multipliers. Foundations and
Trends in Machine Learning, 3(1):1–122, 2011.

C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines, 2011. URL
http://www.csie.ntu.edu.tw/~cjlin/libsvm/.

C. Cortes and V. Vapnik. Support vector networks. Machine Learning, pages 273–297,
1995.

A. Cotter, O. Shamir, N. Srebro, and K. Sridharan. Better mini-batch algorithms
In Advances in Neural Information Process-
via accelerated gradient methods.
ing Systems 24, pages 1647–1655. 2011.
URL http://papers.nips.cc/paper/
4432-better-mini-batch-algorithms-via-accelerated-gradient-methods.pdf.

J. C. Duchi, A. Agarwal, and M. J. Wainwright. Dual averaging for distributed optimization:
Convergence analysis and network scaling. IEEE Transactions on Automatic Control, 57
(3):592–606, 2012.

M. C. Ferris and O. L. Mangasarian. Parallel variable distribution. SIAM Journal on
Optimization, 4(4):815–832, 1994.

320

AsySCD

D. Goldfarb and S. Ma. Fast multiple-splitting algorithms for convex optimization. SIAM
Journal on Optimization, 22(2):533–556, 2012.

Z. Lu and L. Xiao. On the complexity analysis of randomized block-coordinate descent
methods. Technical Report arXiv:1305.4723, Simon Fraser University, 2013.

Z. Q. Luo and P. Tseng. On the convergence of the coordinate descent method for convex
diﬀerentiable minimization. Journal of Optimization Theory and Applications, 72:7–35,
1992.

O. L. Mangasarian. Parallel gradient distribution in unconstrained optimization. SIAM
Journal on Optimization, 33(1):916–1925, 1995.

A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation
approach to stochastic programming. SIAM Journal on Optimization, 19:1574–1609,
2009.

Y. Nesterov.
Introductory Lectures on Convex Optimization: A Basic Course. Kluwer
Academic Publishers, 2004.

Y. Nesterov. Eﬃciency of coordinate descent methods on huge-scale optimization problems.
SIAM Journal on Optimization, 22(2):341–362, 2012.

F. Niu, B. Recht, C. R´e, and S. J. Wright. Hogwild!: A lock-free approach to parallelizing
stochastic gradient descent. Advances in Neural Information Processing Systems 24, pages
693–701, 2011.

Z. Peng, M. Yan, and W. Yin. Parallel and distributed sparse optimization. Preprint, 2013.

P. Richt´arik and M. Tak´aˇc. Iteration complexity of randomized block-coordinate descent
methods for minimizing a composite function. Mathematrical Programming, 144:1–38,
2012a.

P. Richt´arik and M. Tak´aˇc. Parallel coordinate descent methods for big data optimization.
Technical Report arXiv:1212.0873, 2012b.

C. Scherrer, A. Tewari, M. Halappanavar, and D. Haglin. Feature clustering for accelerating
parallel coordinate descent. Advances in Neural Information Processing Systems 25, pages
28–36, 2012.

S. Shalev-Shwartz and T. Zhang. Accelerated mini-batch stochastic dual coordinate ascent.
Advances in Neural Information Processing Systems 26, pages 378–385, 2013.

O. Shamir and T. Zhang. Stochastic gradient descent for non-smooth optimization: Con-
vergence results and optimal averaging schemes. In Proceedings of the 30th International
Conference on Machine Learning, 2013.

R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society, Series B, 58:267–288, 1996.

321

Liu, Wright, R´e, Bittorf, and Sridhar

P. Tseng. Convergence of a block coordinate descent method for nondiﬀerentiable mini-
mization. Journal of Optimization Theory and Applications, 109:475–494, 2001.

P. Tseng and S. Yun. A coordinate gradient descent method for nonsmooth separable
minimization. Mathematical Programming, Series B, 117:387–423, June 2009.

P. Tseng and S. Yun. A coordinate gradient descent method for linearly constrained smooth
optimization and support vector machines training. Computational Optimization and
Applications, 47(2):179–206, 2010.

P.-W. Wang and C.-J. Lin.
Iteration complexity of feasible descent methods for convex
optimization. Journal of Machine Learning Research, 15:1523–1548, 2014.

S. J. Wright. Accelerated block-coordinate relaxation for regularized optimization. SIAM
Journal on Optimization, 22(1):159–186, 2012.

T. Yang. Trading computation for communication: Distributed stochastic dual coordinate
ascent. Advances in Neural Information Processing Systems 26, pages 629–637, 2013.

322

Journal of Machine Learning Research 16 (2015) 913-960

Submitted 10/13; Revised 9/14; Published 4/15

Distributed Matrix Completion and Robust Factorization

Lester Mackey†
Stanford University
Department of Statistics
390 Serra Mal l
Stanford, CA 94305
Ameet Talwalkar†
University of California, Los Angeles
Computer Science Department
4732 Boelter Hal l
Los Angeles, CA 90095

lmackey@stanford.edu

atalwalkar@gmail.com

Michael I. Jordan
University of California, Berkeley
Department of Electrical Engineering and Computer Science and Department of Statistics
465 Soda Hal l
Berkeley, CA 94720

jordan@cs.berkeley.edu

Editor: Nathan Srebro
† These authors contributed equally.

Abstract
If learning methods are to scale to the massive sizes of modern data sets, it is essential for
the ﬁeld of machine learning to embrace parallel and distributed computing. Inspired by
the recent development of matrix factorization methods with rich theory but poor compu-
tational complexity and by the relative ease of mapping matrices onto distributed architec-
tures, we introduce a scalable divide-and-conquer framework for noisy matrix factorization.
We present a thorough theoretical analysis of this framework in which we characterize the
statistical errors introduced by the “divide” step and control their magnitude in the “con-
quer” step, so that the overall algorithm enjoys high-probability estimation guarantees
comparable to those of its base algorithm. We also present experiments in collaborative
ﬁltering and video background modeling that demonstrate the near-linear to superlinear
speed-ups attainable with this approach.
Keywords:
collaborative ﬁltering, divide-and-conquer, matrix completion, matrix fac-
torization, parallel and distributed algorithms, randomized algorithms, robust matrix fac-
torization, video surveillance

1. Introduction

The scale of modern scientiﬁc and technological data sets poses ma jor new challenges for
computational and statistical science. Data analyses and learning algorithms suitable for
modest-sized data sets are often entirely infeasible for the terabyte and petabyte data sets
that are fast becoming the norm. There are two basic responses to this challenge. One
response is to abandon algorithms that have superlinear complexity, focusing attention on
simpliﬁed algorithms that—in the setting of massive data—may achieve satisfactory results

c(cid:13)2015 Lester Mackey, Ameet Talwalkar and Michael I. Jordan.

Mackey, Talwalkar and Jordan

because of the statistical strength of the data. While this is a reasonable research strategy,
it requires developing suites of algorithms of varying computational complexity for each
inferential task and calibrating statistical and computational eﬃciencies. There are many
open problems that need to be solved if such an eﬀort is to bear fruit.
The other response to the massive data problem is to retain existing algorithms but to
apply them to subsets of the data. To obtain useful results under this approach, one em-
braces parallel and distributed computing architectures, applying existing base algorithms
to multiple subsets of the data in parallel and then combining the results. Such a divide-
and-conquer methodology has two main virtues: (1) it builds directly on algorithms that
have proven their value at smaller scales and that often have strong theoretical guarantees,
and (2) it requires little in the way of new algorithmic development. The ma jor challenge,
however, is in preserving the theoretical guarantees of the base algorithm once one em-
beds the algorithm in a computationally-motivated divide-and-conquer procedure. Indeed,
the theoretical guarantees often refer to subtle statistical properties of the data-generating
mechanism (e.g., sparsity, information spread, and near low-rankedness). These may or
may not be retained under the “divide” step of a putative divide-and-conquer solution. In
fact, we generally would expect subsampling operations to damage the relevant statisti-
cal structures. Even if these properties are preserved, we face the diﬃculty of combining
the intermediary results of the “divide” step into a ﬁnal consilient solution to the original
problem. The question, therefore, is whether we can design divide-and-conquer algorithms
that manage the tradeoﬀs relating these statistical properties to the computational degrees
of freedom such that the overall algorithm provides a scalable solution that retains the
theoretical guarantees of the base algorithm.
In this paper,1 we explore this issue in the context of an important class of machine
learning algorithms—the matrix factorization algorithms underlying a wide variety of prac-
tical applications, including collaborative ﬁltering for recommender systems , e.g., Koren
et al. (2009) and the references therein, link prediction for social networks (Hoﬀ, 2005),
click prediction for web search (Das et al., 2007), video surveillance (Cand`es et al., 2011),
graphical model selection (Chandrasekaran et al., 2009), document modeling (Min et al.,
2010), and image alignment (Peng et al., 2010). We focus on two instances of the general
matrix factorization problem: noisy matrix completion (Cand`es and Plan, 2010), where the
goal is to recover a low-rank matrix from a small subset of noisy entries, and noisy robust
matrix factorization (Cand`es et al., 2011; Chandrasekaran et al., 2009), where the aim is
to recover a low-rank matrix from corruption by noise and outliers of arbitrary magnitude.
These two classes of matrix factorization problems have attracted signiﬁcant interest in the
research community.
Various approaches have been proposed for scalable noisy matrix factorization prob-
lems, in particular for noisy matrix completion, though the vast ma jority tackle rank-
constrained non-convex formulations of these problems with no assurance of ﬁnding optimal
solutions (Zhou et al., 2008; Gemulla et al., 2011; Recht and R´e, 2011; F. Niu et al., 2011; Yu
et al., 2012). In contrast, convex formulations of noisy matrix factorization relying on the
nuclear norm have been shown to admit strong theoretical estimation guarantees (Agarwal
et al., 2011; Cand`es et al., 2011; Cand`es and Plan, 2010; Negahban and Wainwright, 2012),

1. A preliminary form of this work appears in Mackey et al. (2011).

914

Distributed Matrix Completion and Robust Factorization

and a variety of algorithms (e.g., Lin et al., 2009b; Ma et al., 2011; Toh and Yun, 2010) have
been developed for solving both matrix completion and robust matrix factorization via con-
vex relaxation. Unfortunately, however, all of these methods are inherently sequential, and
all rely on the repeated and costly computation of truncated singular value decompositions
(SVDs), factors that severely limit the scalability of the algorithms. Moreover, previous
attempts at reducing this computational burden have introduced approximations without
theoretical justiﬁcation (Mu et al., 2011).

To address this key problem of noisy matrix factorization in a scalable and theoret-
ically sound manner, we propose a divide-and-conquer framework for large-scale matrix
factorization. Our framework, entitled Divide-Factor-Combine (DFC), randomly divides
the original matrix factorization task into cheaper subproblems, solves those subproblems
in parallel using a base matrix factorization algorithm for nuclear norm regularized for-
mulations, and combines the solutions to the subproblems using eﬃcient techniques from
randomized matrix approximation. We develop a thoroughgoing theoretical analysis for the
DFC framework, linking statistical properties of the underlying matrix to computational
choices in the algorithms and thereby providing conditions under which statistical estima-
tion of the underlying matrix is possible. We also present experimental results for several
DFC variants demonstrating that DFC can provide near-linear to superlinear speed-ups in
practice. Indeed, DFC naturally handles massive data sets that are too large to ﬁt on a
single machine, as DFC’s minimal communication footprint is particularly well-suited for
distributed computing environments.

The remainder of the paper is organized as follows. In Section 2, we deﬁne the setting of
noisy matrix factorization and introduce the components of the DFC framework. Secs. 3,
4, and 5 present our theoretical analysis of DFC, along with a new analysis of convex
noisy matrix completion and a novel characterization of randomized matrix approximation
algorithms. To illustrate the practical speed-up and robustness of DFC, we present exper-
imental results on collaborative ﬁltering, video background modeling, and simulated data
in Section 6. Finally, we conclude in Section 7.
Notation : For a matrix M ∈ Rm×n , we deﬁne M(i) as the ith row vector, M(j ) as the j th
column vector, and Mij as the ij th entry. If rank(M) = r, we write the compact singular
value decomposition (SVD) of M as UM ΣM V(cid:62)
M , where ΣM is diagonal and contains the
r non-zero singular values of M, and UM ∈ Rm×r and VM ∈ Rn×r are the corresponding
left and right singular vectors of M. We deﬁne M+ = VM Σ−1
M U(cid:62)
M as the Moore-Penrose
pseudoinverse of M and PM = MM+ as the orthogonal pro jection onto the column space
of M. We let (cid:107)·(cid:107)2 , (cid:107)·(cid:107)F , and (cid:107)·(cid:107)∗ respectively denote the spectral, Frobenius, and nuclear
norms of a matrix, (cid:107)·(cid:107)∞ denote the maximum entry of a matrix, and (cid:107)·(cid:107) represent the (cid:96)2
norm of a vector.

2. The Divide-Factor-Combine Framework

In this section, we present a general divide-and-conquer framework for scalable noisy matrix
factorization. We begin by deﬁning the problem setting of interest.

915

Mackey, Talwalkar and Jordan

2.1 Noisy Matrix Factorization (MF)

In the setting of noisy matrix factorization, we observe a subset of the entries of a matrix
M = L0 + S0 + Z0 ∈ Rm×n , where L0 has rank r (cid:28) m, n, S0 represents a sparse matrix
of outliers of arbitrary magnitude, and Z0 is a dense noise matrix. We let Ω represent the
locations of the observed entries and PΩ be the orthogonal pro jection onto the space of
m × n matrices with support Ω, so that
(PΩ (M))ij = Mij , if (i, j ) ∈ Ω and (PΩ (M))ij = 0 otherwise.2
Our goal is to estimate the low-rank matrix L0 from PΩ (M) with error proportional to the
noise level ∆ (cid:44) (cid:107)Z0(cid:107)F . We will focus on two speciﬁc instances of this general problem:
• Noisy Matrix Completion (MC): s (cid:44) |Ω| entries of M are revealed uniformly
without replacement, along with their locations. There are no outliers, so that S0 is
identically zero.
• Noisy Robust Matrix Factorization (RMF): S0 is identically zero save for s
outlier entries of arbitrary magnitude with unknown locations distributed uniformly
without replacement. All entries of M are observed, so that PΩ (M) = M.

2.2 Divide-Factor-Combine
The Divide-Factor-Combine (DFC) framework divides the expensive task of matrix factor-
ization into smaller subproblems, executes those subproblems in parallel, and then eﬃciently
combines the results into a ﬁnal low-rank estimate of L0 . We highlight three variants of this
general framework in Algorithms 1, 2, and 3. These algorithms, which we refer to as DFC-
Proj, DFC-RP, and DFC-Nys, diﬀer in their strategies for division and recombination
but adhere to a common pattern of three simple steps:

(D step) Divide input matrix into submatrices: DFC-Proj and DFC-RP randomly
partition PΩ (M) into t l-column submatrices, {PΩ (C1 ), . . . , PΩ (Ct )},3 while DFC-
Nys selects an l-column submatrix, PΩ (C), and a d-row submatrix, PΩ (R), uniformly
at random.

(F step) Factor each submatrix in parallel using any base MF algorithm: DFC-
Proj and DFC-RP perform t parallel submatrix factorizations, while DFC-Nys
performs two such parallel factorizations. Standard base MF algorithms output the
following low-rank approximations: { ˆC1 , . . . , ˆCt} for DFC-Proj and DFC-RP; ˆC
and ˆR for DFC-Nys. All matrices are retained in factored form.

(C step) Combine submatrix estimates: DFC-Proj generates a ﬁnal low-rank esti-
mate ˆLproj by pro jecting [ ˆC1 , . . . , ˆCt ] onto the column space of ˆC1 , DFC-RP uses
random pro jection to compute a rank-k estimate ˆLrp of [ ˆC1 · · · ˆCt ] where k is the me-
dian rank of the returned subproblem estimates, and DFC-Nys forms the low-rank
2. When Q is a submatrix of M we abuse notation and let PΩ (Q) be the corresponding submatrix of
PΩ (M).
3. For ease of discussion, we assume that t evenly divides n so that l = n/t. In general, PΩ (M) can always
be partitioned into t submatrices, each with either (cid:98)n/t(cid:99) or (cid:100)n/t(cid:101) columns.

916

Distributed Matrix Completion and Robust Factorization

Algorithm 1 DFC-Proj
Input: PΩ (M), t
{PΩ (Ci )}1≤i≤t = SampCol(PΩ (M), t)
do in parallel
ˆC1 = Base-MF-Alg(PΩ (C1 ))
...
ˆCt = Base-MF-Alg(PΩ (Ct ))
end do
ˆLproj = ColProjection( ˆC1 , . . . , ˆCt )

Algorithm 2 DFC-RP
Input: PΩ (M), t
{PΩ (Ci )}1≤i≤t = SampCol(PΩ (M), t)
do in parallel
ˆC1 = Base-MF-Alg(PΩ (C1 ))
...
ˆCt = Base-MF-Alg(PΩ (Ct ))
k = mediani∈{1...t}(cid:0)rank( ˆCi )(cid:1)
end do
ˆLproj = RandProjection( ˆC1 , . . . , ˆCt , k)

Algorithm 3 DFC-Nys
Input: PΩ (M), l, d
PΩ (C) , PΩ (R) = SampColRow(PΩ (M), l, d)
do in parallel
ˆC = Base-MF-Alg(PΩ (C))
ˆR = Base-MF-Alg(PΩ (R))
end do
ˆLnys = GenNystr¨om( ˆC, ˆR)

estimate ˆLnys from ˆC and ˆR via the generalized Nystr¨om method. These matrix
approximation techniques are described in more detail in Section 2.3.

2.3 Randomized Matrix Approximations

Underlying the C step of each DFC algorithm is a method for generating randomized low-
rank approximations to an arbitrary matrix M.
Column Projection : DFC-Proj (Algorithm 1) uses the column pro jection method of
Frieze et al. (1998). Suppose that C is a matrix of l columns sampled uniformly and
without replacement from the columns of M. Then, column pro jection generates a “matrix
pro jection” approximation (Kumar et al., 2009a) of M via
Lproj = CC+M = UC U(cid:62)
CM.
In practice, we do not reconstruct Lproj but rather maintain low-rank factors, e.g., UC and
U(cid:62)
CM.
Random Projection : The celebrated result of Johnson and Lindenstrauss (1984) shows
that random low-dimensional embeddings preserve Euclidean geometry. Inspired by this
result, several random pro jection algorithms (e.g., Papadimitriou et al., 1998; Liberty, 2009;
Rokhlin et al., 2009) have been introduced for approximating a matrix by pro jecting it onto
a random low-dimensional subspace (see Halko et al. 2011 for further discussion). DFC-RP
(Algorithm 2) uses such a random pro jection method due to Halko et al. (2011). Given a

917

Mackey, Talwalkar and Jordan

target low-rank parameter k , let G be an n × (k + p) standard Gaussian matrix G, where
p is an oversampling parameter. Next, let Y = (MM(cid:62) )qMG, and deﬁne Q ∈ Rm×k as the
top k left singular vectors of Y . The random pro jection approximation of M is then given
by

Lrp = QQ+M.

We work with an implementation (Tygert, 2009) of a numerically stable variant of this
algorithm described in Algorithm 4.4 of Halko et al. (2011). Moreover, the parameters p
and q are typically set to small positive constants (Tygert, 2009; Halko et al., 2011), and
we set p = 5 and q = 2.
Generalized Nystr¨om Method : The Nystr¨om method was developed for the discretization
of integral equations (Nystr¨om, 1930) and has since been used to speed up large-scale learn-
ing applications involving symmetric positive semideﬁnite matrices (Williams and Seeger,
2000). DFC-Nys (Algorithm 3) makes use of a generalization of the Nystr¨om method for
arbitrary real matrices (Goreinov et al., 1997). Suppose that C consists of l columns of M,
sampled uniformly without replacement, and that R consists of d rows of M, independently
sampled uniformly and without replacement. Let W be the d× l matrix formed by sampling
the corresponding rows of C.4 Then, the generalized Nystr¨om method computes a “spectral
reconstruction” approximation (Kumar et al., 2009a) of M via
W U(cid:62)
Lnys = CW+R = CVW Σ+
W R .
W and U(cid:62)
As with Mproj , we store low-rank factors of Lnys , such as CVW Σ+
W R.

Algorithm

Base Alg
DFC-Proj
DFC-RP
DFC-Nys

Factorization (Per Iteration)
Parallel
Serial
O(mnˆk)
O(mnˆk)
O(mlˆk)
O(tmlˆk)
O(tmlˆk)
O(mlˆk)
O((ml + nd)ˆk) O(max(ml, nd)ˆk)

Combine Step
Parallel
Serial
-
-
O(mˆk2 )
O(tmˆk2 )
O(tmˆk2 + nˆk) O(mˆk2 + tmˆk + nˆk)
O(mˆk2 )
O(mˆk2 )

Table 1: Summary of running time complexity of DFC variants in contrast to many stan-
dard start-of-the-art MF algorithms. This running time analysis assumes that
l ≤ m ≤ n and that all low-rank matrices considered have rank ˆk . See Section 2.4
for a more detailed analysis.

2.4 Running Time of DFC

Many state-of-the-art MF algorithms have Ω(mnkM ) per-iteration time complexity due to
the rank-kM truncated SVD performed on each iteration. DFC signiﬁcantly reduces the
per-iteration complexity to O(mlkCi ) time for Ci (or C) and O(ndkR ) time for R. The cost
of combining the submatrix estimates is even smaller when using column pro jection or the
generalized Nystr¨om method, since the outputs of standard MF algorithms are returned

4. This choice is arbitrary: W could also be deﬁned as a submatrix of R.

918

Distributed Matrix Completion and Robust Factorization

in factored form. Indeed, if we deﬁne k (cid:48) (cid:44) maxi kCi , then the column pro jection step of
DFC-Proj requires only O(mk (cid:48)2 + lk (cid:48)2 ) time: O(mk (cid:48)2 + lk (cid:48)2 ) time for the pseudoinversion
of ˆC1 and O(mk (cid:48)2 + lk (cid:48)2 ) time for matrix multiplication with each ˆCi in parallel. Similarly,
the generalized Nystr¨om step of DFC-Nys requires only O(l¯k2 + d¯k2 + min(m, n)¯k2 ) time,
where ¯k (cid:44) max(kC , kR ).
DFC-RP also beneﬁts from the factored form of the outputs of standard MF algorithms.
Assuming that p and q are positive constants, the random pro jection step of DFC-RP
requires O(mkt + mkk (cid:48) + lkk (cid:48) + nk) time where k is the low-rank parameter of Q: O(nk)
time to generate G, O(mkk (cid:48) + lkk (cid:48) + mkt) to compute Y in parallel, O(mk2 ) to compute
the SVD of Y , and O(mk (cid:48)2 + lk (cid:48)2 ) time for matrix multiplication with each ˆCi in parallel
in the ﬁnal pro jection step. Note that the running time of the random pro jection step
depends on t (even when executed in parallel) and thus has a larger complexity than the
column pro jection and generalized Nystr¨om variants. Nevertheless, the random pro jection
step need be performed only once and thus yields a signiﬁcant savings over the repeated
computation of SVDs required by typical base algorithms.
A summary of these running times is presented in Table 1.

2.5 Ensemble Methods

Ensemble methods have been shown to improve performance of matrix approximation al-
gorithms, while straightforwardly leveraging the parallelism of modern many-core and dis-
tributed architectures (Kumar et al., 2009b). As such, we propose ensemble variants of
the DFC algorithms that demonstrably reduce estimation error while introducing a negli-
gible cost to the parallel running time. For DFC-Proj-Ens, rather than pro jecting only
onto the column space of ˆC1 , we pro ject [ ˆC1 , . . . , ˆCt ] onto the column space of each ˆCi
in parallel and then average the t resulting low-rank approximations. For DFC-RP-Ens,
rather than pro jecting only onto a column space derived from a single random matrix G,
we pro ject [ ˆC1 , . . . , ˆCt ] onto t column spaces derived from t random matrices in parallel
and then average the t resulting low-rank approximations. For DFC-Nys-Ens, we choose a
random d-row submatrix PΩ (R) as in DFC-Nys and independently partition the columns
of PΩ (M) into {PΩ (C1 ), . . . , PΩ (Ct )} as in DFC-Proj and DFC-RP. After running the
base MF algorithm on each submatrix, we apply the generalized Nystr¨om method to each
( ˆCi , ˆR) pair in parallel and average the t resulting low-rank approximations. Section 6
highlights the empirical eﬀectiveness of ensembling.

3. Roadmap of Theoretical Analysis

While DFC in principle can work with any base matrix factorization algorithm, it oﬀers the
greatest beneﬁts when united with accurate but computationally expensive base procedures.
Convex optimization approaches to matrix completion and robust matrix factorization (e.g.,
Lin et al., 2009b; Ma et al., 2011; Toh and Yun, 2010) are prime examples of this class,
since they admit strong theoretical estimation guarantees (Agarwal et al., 2011; Cand`es
et al., 2011; Cand`es and Plan, 2010; Negahban and Wainwright, 2012) but suﬀer from poor
computational complexity due to the repeated and costly computation of truncated SVDs.
Section 6 will provide empirical evidence that DFC provides an attractive framework to

919

Mackey, Talwalkar and Jordan

improve the scalability of these algorithms, but we ﬁrst present a thorough theoretical
analysis of the estimation properties of DFC.
Over the course of the next three sections, we will show that the same assumptions
that give rise to strong estimation guarantees for standard MF formulations also guarantee
strong estimation properties for DFC. While these results represent an important ﬁrst step
toward understanding the theoretical behavior of DFC, we will see that certain gaps remain
between our theoretical characterization and the practical performance of DFC. We will
reﬂect on these gaps and the attendant opportunities for tightened theoretical analysis in
Section 6.4. In the remainder of this section, we ﬁrst introduce these standard assumptions
and then present simpliﬁed bounds to build intuition for our theoretical results and our
underlying proof techniques.

3.1 Standard Assumptions for Noisy Matrix Factorization

Since not all matrices can be recovered from missing entries or gross outliers, recent theo-
retical advances have studied suﬃcient conditions for accurate noisy MC (Cand`es and Plan,
2010; Keshavan et al., 2010; Negahban and Wainwright, 2012) and RMF (Agarwal et al.,
2011; Zhou et al., 2010). Informally, these conditions capture the degree to which informa-
tion about a single entry is “spread out” across a matrix. The ease of matrix estimation is
correlated with this spread of information. The most prevalent set of conditions are matrix
coherence conditions, which limit the extent to which the singular vectors of a matrix are
correlated with the standard basis. However, there exist classes of matrices that violate the
coherence conditions but can nonetheless be recovered from missing entries or gross outliers.
Negahban and Wainwright (2012) deﬁne an alternative notion of matrix spikiness in part
to handle these classes.

3.1.1 Matrix Coherence

Letting ei be the ith column of the standard basis, we deﬁne two standard notions of
coherence (Recht, 2011):
Deﬁnition 1 (µ0 -Coherence) Let V ∈ Rn×r contain orthonormal columns with r ≤ n.
Then the µ0 -coherence of V is:
r max1≤i≤n (cid:107)PV ei(cid:107)2 = n
r max1≤i≤n (cid:107)V(i)(cid:107)2 .
µ0 (V) (cid:44) n
Deﬁnition 2 (µ1 -Coherence) Let L ∈ Rm×n have rank r. Then, the µ1 -coherence of L
µ1 (L) (cid:44) (cid:112) mn
is:
L ej | .
r maxij |e(cid:62)
i ULV(cid:62)
For conciseness, we extend the deﬁnition of µ0 -coherence to an arbitrary matrix L ∈ Rm×n
with rank r via µ0 (L) (cid:44) max(µ0 (UL ), µ0 (VL )). Further, for any µ > 0, we will call a matrix
L (µ, r)-coherent if rank(L) = r, µ0 (L) ≤ µ, and µ1 (L) ≤ √
µ. Our analysis in Section 4
will focus on base MC and RMF algorithms that express their estimation guarantees in
terms of the (µ, r)-coherence of the target low-rank matrix L0 . For such algorithms, lower
values of µ correspond to better estimation properties.

920

Distributed Matrix Completion and Robust Factorization

3.1.2 Matrix Spikiness

The matrix spikiness condition of Negahban and Wainwright (2012) captures the intuition
that a matrix is easier to estimate if its maximum entry is not much larger than its average
entry (in the root mean square sense):
Deﬁnition 3 (Spikiness) The spikiness of L ∈ Rm×n is:
α(L) (cid:44) √
mn(cid:107)L(cid:107)∞/(cid:107)L(cid:107)F .
We cal l a matrix α-spiky if α(L) ≤ α.

Our analysis in Section 5 will focus on base MC algorithms that express their estimation
guarantees in terms of the α-spikiness of the target low-rank matrix L0 . For such algorithms,
lower values of α correspond to better estimation properties.

3.2 Prototypical Estimation Bounds
We now present a prototypical estimation bound for DFC. Suppose that a base MC algo-
rithm solves the noisy nuclear norm heuristic, studied in Cand`es and Plan (2010):
sub ject to (cid:107)PΩ (M − L)(cid:107)F ≤ ∆,
minimizeL (cid:107)L(cid:107)∗

and that, for simplicity, M is square. The following prototype bound, derived from a new
noisy MC guarantee in Theorem 10, describes the behavior of this estimator under matrix
coherence assumptions. Note that the bound implies exact recovery in the noiseless setting,
i.e., when ∆ = 0.

Proto-Bound 1 (MC under Incoherence) Suppose that L0 is (µ, r)-coherent, s en-
tries of M ∈ Rn×n are observed uniformly at random where s = Ω(µrn log2 (n)), and
(cid:107)M − L0(cid:107)F ≤ ∆. If ˆL solves the noisy nuclear norm heuristic, then
(cid:107)L0 − ˆL(cid:107)F ≤ f (n)∆

with high probability, where f is a function of n.

Now we present a corresponding prototype bound for DFC-Proj, a simpliﬁed version
of our Corollary 14, under precisely the same coherence assumptions. Notably, this bound
i) preserves accuracy with a ﬂexible (2 + ) degradation in estimation error over the base
algorithm, ii) allows for speed-up by requiring only a vanishingly small fraction of columns
to be sampled (i.e., l/n → 0) whenever s = ω(n log2 (n)) entries are revealed, and iii)
maintains exact recovery in the noiseless setting.

Proto-Bound 2 (DFC-MC under Incoherence) Suppose that L0 is (µ, r)-coherent, s
entries of M ∈ Rn×n are observed uniformly at random, and (cid:107)M − L0(cid:107)F ≤ ∆. Then it
suﬃces to choose

l ≥ c

µ2 r2n2 log2 (n)
s2

921

Mackey, Talwalkar and Jordan

random columns suﬃce to have
(cid:107)L0 − ˆLproj (cid:107)F ≤ (2 + )f (n)∆

with high probability when the noisy nuclear norm heuristic is used as a base algorithm,
where f is the same function of n deﬁned in Proto. 1 and c is a ﬁxed positive constant.

The proof of Proto. 2, and indeed of each of our main DFC results, consists of three high-
level steps:

1. Bound coherence of submatrices : Recall that the F step of DFC operates by applying
a base MF algorithm to submatrices. We show that, with high probability, uniformly
sampled submatrices are only moderately more coherent and moderately more spiky
than the matrix from which they are drawn. This allows for accurate estimation
of submatrices using base algorithms with standard coherence or spikiness require-
ments. The conservation of incoherence result is summarized in Lemma 4, while the
conservation of non-spikiness is presented in Lemma 17.

2. Bound error of randomized matrix approximations : The error introduced by the C
step of DFC depends on the framework variant. Drawing upon tools from random-
ized (cid:96)2 regression (Drineas et al., 2008), randomized matrix multiplication (Drineas
et al., 2006a,b), and matrix concentration (Hsu et al., 2012), we show that the same
assumptions on the spread of information responsible for accurate MC and RMF also
yield high ﬁdelity reconstructions for column pro jection (Corollary 6 and Theorem 18)
and the Nystr¨om method (Corollary 7 and Corollary 8). We additionally present gen-
eral approximation guarantees for random pro jection due to Halko et al. (2011) in
Corollary 9. These results give rise to “master theorems” for coherence (Theorem 12)
and spikiness (Theorem 20) that generically relate the estimation error of DFC to
the error of any base algorithm.

3. Bound error of submatrix factorizations : The ﬁnal step combines a master theorem
with a base estimation guarantee applied to each DFC subproblem. We study both
new (Theorem 10) and established bounds (Theorem 11 and Corollary 19) for MC and
RMF and prove that DFC submatrices satisfy the base guarantee preconditions with
high probability. We present the resulting coherence-based estimation guarantees for
DFC in Corollary 14 and Corollary 16 and the spikiness-based estimation guarantee
in Corollary 22.

The next two sections present the main results contributing to each of these proof steps,
as well as their consequences for MC and RMF. Section 4 presents our analysis under
coherence assumptions, while Section 5 contains our spikiness analysis.

4. Coherence-based Theoretical Analysis

This section presents our analysis of DFC under standard coherence assumptions encoun-
tered in the MC and RMF literature.

922

Distributed Matrix Completion and Robust Factorization

4.1 Coherence Analysis of Randomized Approximation Algorithms

We begin our coherence-based analysis by characterizing the behavior of randomized ap-
proximation algorithms under standard coherence assumptions. The derived properties will
aid us in deriving DFC estimation guarantees. Hereafter,  ∈ (0, 1] represents a prescribed
error tolerance, and δ, δ (cid:48) ∈ (0, 1] denote target failure probabilities.

4.1.1 Conservation of Incoherence

Our ﬁrst result bounds the µ0 and µ1 -coherence of a uniformly sampled submatrix in terms
of the coherence of the full matrix. This conservation of incoherence allows for accurate
submatrix completion or submatrix outlier removal when using standard MC and RMF
algorithms. Its proof is given in Section B.
Lemma 4 (Conservation of Incoherence) Let L ∈ Rm×n be a rank-r matrix and deﬁne
LC ∈ Rm×l as a matrix of l columns of L sampled uniformly without replacement.
If
l ≥ crµ0 (VL ) log(n) log(1/δ)/2 , where c is a ﬁxed positive constant deﬁned in Corol lary 6,
then

i) rank(LC ) = rank(L)

ii) µ0 (ULC ) = µ0 (UL )
iii) µ0 (VLC ) ≤ µ0 (VL )
1 − /2
1 (LC ) ≤ rµ0 (UL )µ0 (VL )
iv) µ2
1 − /2
al l hold jointly with probability at least 1 − δ/n.

4.1.2 Column Projection Analysis

Our next result shows that pro jection based on uniform column sampling leads to near
optimal estimation in matrix regression when the covariate matrix has small coherence.
This statement will immediately give rise to estimation guarantees for column pro jection
and the generalized Nystr¨om method.
Theorem 5 (Subsampled Regression under Incoherence) Given a target matrix B ∈
Rp×n and a rank-r matrix of covariates L ∈ Rm×n , choose l ≥ 3200rµ0 (VL ) log(4n/δ)/2 ,
let BC ∈ Rp×l be a matrix of l columns of B sampled uniformly without replacement, and
let LC ∈ Rm×l consist of the corresponding columns of L. Then,
(cid:107)B − BC L+
C L(cid:107)F ≤ (1 + )(cid:107)B − BL+L(cid:107)F
with probability at least 1 − δ − 0.2.

Fundamentally, Theorem 5 links the notion of coherence, common in matrix estimation
communities, to the randomized approximation concept of leverage score sampling (Ma-
honey and Drineas, 2009). The proof of Theorem 5, given in Section A, builds upon the

923

Mackey, Talwalkar and Jordan

randomized (cid:96)2 regression work of Drineas et al. (2008) and the matrix concentration re-
sults of Hsu et al. (2012) to yield a subsampled regression guarantee with better sampling
complexity than that of Drineas et al. (2008, Theorem 5).
A ﬁrst consequence of Theorem 5 shows that, with high probability, column pro jection
produces an estimate nearly as good as a given rank-r target by sampling a number of
columns proportional to the coherence and r log n.
Corollary 6 (Column Pro jection under Incoherence) Given a matrix M ∈ Rm×n
and a rank-r approximation L ∈ Rm×n , choose l ≥ crµ0 (VL ) log(n) log(1/δ)/2 , where c is
a ﬁxed positive constant, and let C ∈ Rm×l be a matrix of l columns of M sampled uniformly
without replacement. Then,
(cid:107)M − CC+M(cid:107)F ≤ (1 + )(cid:107)M − L(cid:107)F
with probability at least 1 − δ .
Our result generalizes Theorem 1 of Drineas et al. (2008) by providing improved sampling
complexity and guarantees relative to an arbitrary low-rank approximation. Notably, in the
“noiseless” setting, when M = L, Corollary 6 guarantees exact recovery of M with high
probability. The proof of Corollary 6 is given in Section C.

4.1.3 Generalized Nystr¨om Analysis

Theorem 5 and Corollary 6 together imply an estimation guarantee for the generalized
Nystr¨om method relative to an arbitrary low-rank approximation L.
Indeed, if the ma-
trix of sampled columns is denoted by C, then, with appropriately reduced probability,
O(µ0 (VL )r log n) columns and O(µ0 (UC )r log m) rows suﬃce to match the reconstruction
error of L up to any ﬁxed precision. The proof can be found in Section D.
Corollary 7 (Generalized Nystr¨om under Incoherence) Given a matrix M ∈ Rm×n
and a rank-r approximation L ∈ Rm×n , choose l ≥ crµ0 (VL ) log(n) log(1/δ)/2 with c a
constant as in Corol lary 6, and let C ∈ Rm×l be a matrix of l columns of M sampled
uniformly without replacement. Further choose d ≥ clµ0 (UC ) log(m) log(1/δ (cid:48) )/2 , and let
R ∈ Rd×n be a matrix of d rows of M sampled independently and uniformly without re-
placement. Then,
(cid:107)M − CW+R(cid:107)F ≤ (1 + )2(cid:107)M − L(cid:107)F
with probability at least (1 − δ)(1 − δ (cid:48) − 0.2).
Like the generalized Nystr¨om bound of Drineas et al. (2008, Theorem 4) and unlike our
column pro jection result, Corollary 7 depends on the coherence of the submatrix C and
holds only with probability bounded away from 1. Our next contribution shows that we
can do away with these restrictions in the noiseless setting, where M = L.
Corollary 8 (Noiseless Generalized Nystr¨om under Incoherence) Let L ∈ Rm×n
be a rank-r matrix. Choose l ≥ 48rµ0 (VL ) log(4n/(1−√
1 − δ)) and d ≥ 48rµ0 (UL ) log(4m/(1−
√
1 − δ)). Let C ∈ Rm×l be a matrix of l columns of L sampled uniformly without replace-
ment, and let R ∈ Rd×n be a matrix of d rows of L sampled independently and uniformly
without replacement. Then,

L = CW+R

924

Distributed Matrix Completion and Robust Factorization

with probability at least 1 − δ .

This result may appear surprising at ﬁrst sight, since only vanishingly small fractions of
rows and columns may participate in the generalized Nystr¨om reconstruction. The intuition
for the method’s success that when the rank of L is small, only a small number of well-
chosen rows and columns are needed to reconstruct the row and column space of L and
that, when L is incoherent, uniform random sampling is likely produce well-chosen rows
and columns. The proof of Corollary 8, given in Section E, adapts a strategy of Talwalkar
and Rostamizadeh (2010) developed for the analysis of positive semideﬁnite matrices.

4.1.4 Random Projection Analysis

We next present an estimation guarantee for the random pro jection method relative to an
arbitrary low-rank approximation L. The result implies that using a random matrix with
oversampled columns proportional to r log(1/δ) suﬃces to match the reconstruction error
of L up to any ﬁxed precision with probability 1 − δ . The result is a direct consequence of
the random pro jection analysis of Halko et al. (2011, Theorem 10.7), and the proof can be
found in Section F.
Corollary 9 (Random Pro jection) Given a matrix M ∈ Rm×n and a rank-r approxi-
mation L ∈ Rm×n with r ≥ 2, choose an oversampling parameter
p ≥ 242 r log(7/δ)/2 .
Draw an n × (r + p) standard Gaussian matrix G and deﬁne Y = MG. Then, with
probability at least 1 − δ ,

(cid:107)M − PY M(cid:107)F ≤ (1 + )(cid:107)M − L(cid:107)F .
Moreover, deﬁne Lrp as the best rank-r approximation of PY M with respect to the Frobenius
norm. Then, with probability at least 1 − δ ,
(cid:107)M − Lrp(cid:107)F ≤ (2 + )(cid:107)M − L(cid:107)F .

We note that, in contrast to Corollary 6 and Corollary 7, Corollary 9 does not depend on
the coherence of L and hence can be fruitfully applied even in the absence of an incoherence
assumption. We demonstrate such a use case in Section 5. We note moreover that past
empirical studies have demonstrated excellent estimation error with p ≤ 10 irrespective of
the target matrix rank (Halko et al., 2011); bridging the gap between theory and practice
in this instance represents an interesting open problem.

4.2 Base Algorithm Guarantees

As prototypical examples of the coherence-based estimation guarantees available for noisy
MC and noisy RMF, consider the following two theorems. The ﬁrst bounds the estimation
error of a convex optimization approach to noisy matrix completion, under the assumptions
of incoherence and uniform sampling.

925

Mackey, Talwalkar and Jordan

Theorem 10 (Noisy MC under Incoherence) Suppose that L0 ∈ Rm×n is (µ, r)-coherent
and that, for some target rate parameter β > 1,
s ≥ 32µr(m + n)β log2 (m + n)

entries of M are observed with locations Ω sampled uniformly without replacement. Then,
if m ≤ n and (cid:107)PΩ (M) − PΩ (L0 )(cid:107)F ≤ ∆ a.s., the minimizer ˆL of the problem
(cid:107)PΩ (M − L)(cid:107)F ≤ ∆.
minimizeL (cid:107)L(cid:107)∗

subject to

(1)

satisﬁes

(cid:114)
2m2n
∆ ≤ ce
(cid:107)L0 − ˆL(cid:107)F ≤ 8
1
+ m +
mn∆
16
s
with probability at least 1 − 4 log(n)n2−2β for ce a positive constant.

√

A similar estimation guarantee was obtained by Cand`es and Plan (2010) under stronger
assumptions. We give the proof of Theorem 10 in Section J.
The second result, due to Zhou et al. (2010) and reformulated for a generic rate pa-
rameter β , as described in Cand`es et al. (2011, Section 3.1), bounds the estimation error of
a convex optimization approach to noisy RMF, under the assumptions of incoherence and
uniformly distributed outliers.

minimizeL,S

Theorem 11 (Noisy RMF under Incoherence, Zhou et al. 2010, Theorem 2) Suppose
that L0 is (µ, r)-coherent and that the support set of S0 is uniformly distributed among al l
sets of cardinality s. Then, if m ≤ n and (cid:107)M − L0 − S0(cid:107)F ≤ ∆ a.s., there is a constant cp
such that with probability at least 1 − cpn−β , the minimizer ( ˆL, ˆS) of the problem
(cid:107)L(cid:107)∗ + λ(cid:107)S(cid:107)1
subject to (cid:107)M − L − S(cid:107)F ≤ ∆ with λ = 1/
F + (cid:107)S0 − ˆS(cid:107)2
F ≤ c(cid:48)2
satisﬁes (cid:107)L0 − ˆL(cid:107)2
e mn∆2 , provided that
r ≤ ρrm
and s ≤ (1 − ρsβ )mn
µ log2 (n)
for target rate parameter β > 2, and positive constants ρr , ρs , and c(cid:48)
e .

√

(2)

n

4.3 Coherence Master Theorem

We now show that the same coherence conditions that allow for accurate MC and RMF
also imply high-probability estimation guarantees for DFC. To make this precise, we let
M = L0 + S0 + Z0 ∈ Rm×n , where L0 is (µ, r)-coherent and (cid:107)PΩ (Z0 )(cid:107)F ≤ ∆. Then, our
next theorem provides a generic bound on the estimation error of DFC used in combination
with an arbitrary base algorithm. The proof, which builds upon the results of Section 4.1,
is given in Section G.

926

Distributed Matrix Completion and Robust Factorization

Theorem 12 (Coherence Master Theorem) Choose t = n/l, l ≥ crµ log(n) log(2/δ)/2 ,
where c is a ﬁxed positive constant, and p ≥ 242 r log(14/δ)/2 . Under the notation of Al-
gorithms 1 and 2, let {C0,1 , · · · , C0,t} be the corresponding partition of L0 . Then, with
probability at least 1 − δ , C0,i is ( rµ2
(cid:113)(cid:80)t
1−/2 , r)-coherent for al l i, and
i=1(cid:107)C0,i − ˆCi(cid:107)2
(cid:107)L0 − ˆL∗(cid:107)F ≤ (2 + )
F ,
where ˆL∗ is the estimate returned by either DFC-Proj or DFC-RP.
Under the notation of Algorithm 3, let C0 and R0 be the corresponding column and row
submatrices of L0 . If in addition d ≥ clµ0 ( ˆC) log(m) log(4/δ)/2 , then, with probability at
(cid:113)
least (1 − δ)(1 − δ − 0.2), DFC-Nys guarantees that C0 and R0 are ( rµ2
1−/2 , r)-coherent and
that
F + (cid:107)R0 − ˆR(cid:107)2
(cid:107)L0 − ˆLnys(cid:107)F ≤ (2 + 3)
(cid:107)C0 − ˆC(cid:107)2
F .
Remark 13 The DFC-Nys guarantee requires the number of rows sampled to grow in
proportion to µ0 ( ˆC), a quantity always bounded by µ in our simulations. Here and in the
consequences to fol low, the DFC-Nys result can be strengthened in the noiseless setting
(∆ = 0) by utilizing Corol lary 8 in place of Corol lary 7 in the proof of Theorem 12.

When a target matrix is incoherent, Theorem 12 asserts that – with high probability
for DFC-Proj and DFC-RP and with ﬁxed probability for DFC-Nys – the estimation
error of DFC is not much larger than the error sustained by the base algorithm on each
subproblem. Because Theorem 12 further bounds the coherence of each submatrix, we can
use any coherence-based matrix estimation guarantee to control the estimation error on
each subproblem. The next two sections demonstrate how Theorem 12 can be applied to
derive speciﬁc DFC estimation guarantees for noisy MC and noisy RMF. In these sections,
we let ¯n (cid:44) max(m, n).

4.4 Consequences for Noisy MC
As a ﬁrst consequence of Theorem 12, we will show that DFC retains the high-probability
estimation guarantees of a standard MC solver while operating on matrices of much smaller
dimension. Suppose that a base MC algorithm solves the convex optimization problem of
Eq. (1). Then, Corollary 14 follows from the Coherence Master Theorem (Theorem 12) and
the base algorithm guarantee of Theorem 10.

Corollary 14 (DFC-MC under Incoherence) Suppose that L0 is (µ, r)-coherent and
that s entries of M are observed, with locations Ω distributed uniformly. Fix any target rate
parameter β > 1. Then, if (cid:107)PΩ (M) − PΩ (L0 )(cid:107)F ≤ ∆ a.s., and the base algorithm solves
the optimization problem of Eq. (1), it suﬃces to choose t = n/l,
d ≥ clµ0 ( ˆC)(2β − 1) log2 (4¯n)¯n/(n2 ),
l ≥ cµ2 r2 (m + n)nβ log2 (m + n)/(s2 ),
and p ≥ 242 r log(14¯n2β−2 )/2 to achieve
DFC-Pro j : (cid:107)L0 − ˆLproj (cid:107)F ≤ (2 + )ce

mn∆

√

927

Mackey, Talwalkar and Jordan

√

DFC-RP : (cid:107)L0 − ˆLrp(cid:107)F ≤ (2 + )ce
mn∆
√
DFC-Nys : (cid:107)L0 − ˆLnys(cid:107)F ≤ (2 + 3)ce
with probability at least
DFC-Pro j / DFC-RP : 1 − (5t log(¯n) + 1) ¯n2−2β ≥ 1 − ¯n3−2β
DFC-Nys : 1 − (10 log( ¯n) + 2) ¯n2−2β − 0.2,

ml + dn∆

respectively, with c as in Theorem 12 and ce as in Theorem 10.

Remark 15 Corol lary 14 al lows for the fraction of columns and rows sampled to decrease
as the number of revealed entries, s, increases. Only a vanishingly smal l fraction of columns
(l/n → 0) and rows (d/ ¯n → 0) need be sampled whenever s = ω((m + n) log2 (m + n)).

To understand the conclusions of Corollary 14, consider the base algorithm of The-
orem 10, which, when applied to PΩ (M), recovers an estimate ˆL satisfying (cid:107)L0 − ˆL(cid:107)F ≤
√
mn∆ with high probability. Corollary 14 asserts that, with appropriately reduced prob-
ce
ability, DFC-Proj and DFC-RP exhibit the same estimation error scaled by an adjustable
factor of 2 + , while DFC-Nys exhibits a somewhat smaller error scaled by 2 + 3.
The key take-away is that DFC introduces a controlled increase in error and a controlled
decrement in the probability of success, allowing the user to interpolate between maximum
speed and maximum accuracy. Thus, DFC can quickly provide near-optimal estimation in
the noisy setting and exact recovery in the noiseless setting (∆ = 0), even when entries are
missing. The proof of Corollary 14 can be found in Section H.

4.5 Consequences for Noisy RMF

Our next corollary shows that DFC retains the high-probability estimation guarantees of a
standard RMF solver while operating on matrices of much smaller dimension. Suppose that
a base RMF algorithm solves the convex optimization problem of Eq. (2). Then, Corol-
lary 16 follows from the Coherence Master Theorem (Theorem 12) and the base algorithm
guarantee of Theorem 11.

Corollary 16 (DFC-RMF under Incoherence) Suppose that L0 is (µ, r)-coherent with

r2 ≤ min(m, n)ρr
2µ2 log2 (¯n)
for a positive constant ρr . Suppose moreover that the uniformly distributed support set of
(cid:17)
βs (cid:44) (cid:16)
S0 has cardinality s. For a ﬁxed positive constant ρs , deﬁne the undersampling parameter
1 − s
mn
and ﬁx any target rate parameter β > 2 with rescaling β (cid:48) (cid:44) β log(¯n)/ log(m) satisfying
4βs − 3/ρs ≤ β (cid:48) ≤ βs . Then, if (cid:107)M − L0 − S0(cid:107)F ≤ ∆ a.s., and the base algorithm solves

/ρs ,

928

Distributed Matrix Completion and Robust Factorization

(cid:19)
(cid:18) cr2µ2β log2 (2¯n)
the optimization problem of Eq. (2), it suﬃces to choose t = n/l,
4 log(¯n)β (1 − ρsβs )
(cid:32)
m(ρsβs − ρsβ (cid:48) )2
,
2ρr
4 log(¯n)β (1 − ρsβs )
clµ0 ( ˆC)β log2 (4¯n)
n(ρsβs − ρsβ (cid:48) )2
2

d ≥ max

l ≥ max

,

,

(cid:33)

√

mn∆

and p ≥ 242 r log(14¯nβ )/2 to have
DFC-Pro j : (cid:107)L0 − ˆLproj (cid:107)F ≤ (2 + )c(cid:48)
e
√
DFC-RP : (cid:107)L0 − ˆLrp(cid:107)F ≤ (2 + )c(cid:48)
mn∆
e
√
DFC-Nys : (cid:107)L0 − ˆLnys(cid:107)F ≤ (2 + 3)c(cid:48)
e
with probability at least
DFC-Pro j / DFC-RP : 1 − (t(cp + 1) + 1)¯n−β ≥ 1 − cp ¯n1−β
DFC-Nys : 1 − (2cp + 3)¯n−β − 0.2,
respectively, with c as in Theorem 12 and ρr , c(cid:48)
e , and cp as in Theorem 11.

ml + dn∆

Note that Corollary 16 places only very mild restrictions on the number of columns and
rows to be sampled.
Indeed, l and d need only grow poly-logarithmically in the matrix
dimensions to achieve estimation guarantees comparable to those of the RMF base algorithm
(Theorem 11). Hence, DFC can quickly provide near-optimal estimation in the noisy setting
and exact recovery in the noiseless setting (∆ = 0), even when entries are grossly corrupted.
The proof of Corollary 16 can be found in Section I.

5. Theoretical Analysis under Spikiness Conditions

This section presents our analysis of DFC under standard spikiness assumptions from the
MC and RMF literature.

5.1 Spikiness Analysis of Randomized Approximation Algorithms

We begin our spikiness analysis by characterizing the behavior of randomized approximation
algorithms under standard spikiness assumptions. The derived properties will aid us in
developing DFC estimation guarantees. Hereafter,  ∈ (0, 1] represents a prescribed error
tolerance, and δ, δ (cid:48) ∈ (0, 1] designates a target failure probability.

5.1.1 Conservation of Non-Spikiness

Our ﬁrst lemma establishes that the uniformly sampled submatrices of an α-spiky matrix
are themselves nearly α-spiky with high probability. This property will allow for accurate
submatrix completion or outlier removal using standard MC and RMF algorithms. Its proof
is given in Section K.

929

Mackey, Talwalkar and Jordan

Lemma 17 (Conservation of Non-Spikiness) Let LC ∈ Rm×l be a matrix of l columns
of L ∈ Rm×n sampled uniformly without replacement. If l ≥ α4 (L) log(1/δ)/(22 ), then
α(LC ) ≤ α(L)√
1 − 

with probability at least 1 − δ .

5.1.2 Column Projection Analysis

Our ﬁrst theorem asserts that, with high probability, column pro jection produces an ap-
proximation nearly as good as a given rank-r target by sampling a number of columns
proportional to the spikiness and r log(mn).
Theorem 18 (Column Pro jection under Non-Spikiness) Given a matrix M ∈ Rm×n
and a rank-r, α-spiky approximation L ∈ Rm×n , choose
l ≥ 8rα4 log(2mn/δ)/2 ,
and let C ∈ Rm×l be a matrix of l columns of M sampled uniformly without replacement.
Then,
(cid:107)M − Lproj (cid:107)F ≤ (cid:107)M − L(cid:107)F + 
√
with probability at least 1 − δ , whenever (cid:107)M(cid:107)∞ ≤ α/
mn.
The proof of Theorem 18 builds upon the randomized matrix multiplication work of
Drineas et al. (2006a,b) and will be given in Section L.

5.2 Base Algorithm Guarantee

The next result, a reformulation of Negahban and Wainwright (2012, Corollary 1), is a
prototypical example of a spikiness-based estimation guarantee for noisy MC. Corollary 19
bounds the estimation error of a convex optimization approach to noisy matrix completion,
under non-spikiness and uniform sampling assumptions.

Corollary 19 (Noisy MC under Non-Spikiness) (Negahban and Wainwright, 2012)
Suppose that L0 ∈ Rm×n is α-spiky with rank r and (cid:107)L0(cid:107)F ≤ 1 and that Z0 ∈ Rm×n
has i.i.d. zero-mean, sub-exponential entries with variance ν 2/mn. If, for an oversampling
parameter β > 0,
s ≥ α2β r(m + n) log(m + n)
entries of M = L0 + Z0 are observed with locations Ω sampled uniformly with replacement,
then any solution ˆL of the problem
with λ = 4ν(cid:112)(m + n) log(m + n)/s
(cid:107)PΩ (M − L)(cid:107)2
F + λ(cid:107)L(cid:107)∗
mn
minimizeL
subject to
2s
F ≤ c1 max(cid:0)ν 2 , 1(cid:1)/β
(cid:107)L0 − ˆL(cid:107)2
with probability at least 1 − c2 exp(−c3 log(m + n)) for positive constants c1 , c2 , and c3 .

(cid:107)L(cid:107)∞ ≤ α√
mn

(3)

satisﬁes

930

Distributed Matrix Completion and Robust Factorization

5.3 Spikiness Master Theorem

We now show that the same spikiness conditions that allow for accurate MC also imply high-
probability estimation guarantees for DFC. To make this precise, we let M = L0 + Z0 ∈
Rm×n , where L0 is α-spiky with rank r and that Z0 ∈ Rm×n has i.i.d. zero-mean, sub-
exponential entries with variance ν 2/mn. We further ﬁx any , δ ∈ (0, 1]. Then, our
Theorem 20 provides a generic bound on estimation error for DFC when used in combination
with an arbitrary base algorithm. The proof, which builds upon the results of Section 5.1,
is deferred to Section M.
Theorem 20 (Spikiness Master Theorem) Choose t = n/l, l ≥ 13rα4 log(4mn/δ)/2 ,
and p ≥ 242 r log(14/δ)/2 . Under the notation of Algorithms 1 and 2, let {C0,1 , · · · , C0,t}
be the corresponding partition of L0 . Then, with probability at least 1 − δ , DFC-Proj and
√
(cid:113)(cid:80)t
DFC-RP guarantee that C0,i is (
1.25α)-spiky for al l i and that
(cid:113)(cid:80)t
i=1(cid:107)C0,i − ˆCi(cid:107)2
(cid:107)L0 − ˆLproj (cid:107)F ≤ 2
F + 
(cid:107)L0 − ˆLrp(cid:107)F ≤ (2 + )
i=1(cid:107)C0,i − ˆCi(cid:107)2
F
√
whenever (cid:107) ˆCi(cid:107)∞ ≤ √
1.25 can be replaced with the smal ler term (cid:112)1 + /(4
1.25α/
ml for al l i.
√
√
Remark 21 The factor of
When a target matrix is non-spiky, Theorem 20 asserts that, with high probability, the
estimation error of DFC is not much larger than the error sustained by the base algorithm
on each subproblem. Theorem 20 further bounds the spikiness of each submatrix with
high probability, and hence we can use any spikiness-based matrix estimation guarantee
to control the estimation error on each subproblem. The next section demonstrates how
Theorem 20 can be applied to derive speciﬁc DFC estimation guarantees for noisy MC.

and

r).

5.4 Consequences for Noisy MC
Our corollary of Theorem 20 shows that DFC retains the high-probability estimation guar-
antees of a standard MC solver while operating on matrices of much smaller dimension.
Suppose that a base MC algorithm solves the convex optimization problem of Eq. (3).
Then, Corollary 22 follows from the Spikiness Master Theorem (Theorem 20) and the base
algorithm guarantee of Corollary 19.
Corollary 22 (DFC-MC under Non-Spikiness) Suppose that L0 ∈ Rm×n is α-spiky
with rank r and (cid:107)L0(cid:107)F ≤ 1 and that Z0 ∈ Rm×n has i.i.d. zero-mean, sub-exponential
entries with variance ν 2/mn. Let c1 , c2 , and c3 be positive constants as in Corol lary 19. If
s entries of M = L0 + Z0 are observed with locations Ω sampled uniformly with replacement,
and the base algorithm solves the optimization problem of Eq. (3), then it suﬃces to choose
(cid:114)
t = n/l,

nrα4 log(4mn)/2 ,

l ≥ 13(c3 + 1)

(m + n) log(m + n)β
s

931

Mackey, Talwalkar and Jordan

(cid:112)
and p ≥ 242 r log(14(m + l)c3 )/2 to achieve
(cid:112)
(cid:107)L0 − ˆLproj (cid:107)F ≤ 2
c1 max((l/n)ν 2 , 1)/β +  and
(cid:107)L0 − ˆLrp(cid:107)F ≤ (2 + )
c1 max((l/n)ν 2 , 1)/β
of Eq. (3) is used with λ = 4ν(cid:112)(m + n) log(m + n)/s.
with respective probability at least 1− (t+ 1)(c2 + 1) exp(−c3 log(m + l)), if the base algorithm
Remark 23 Corol lary 22 al lows for the fraction of columns sampled to decrease as the
number of revealed entries, s, increases. Only a vanishingly smal l fraction of columns
(l/n → 0) need be sampled whenever s = ω((m + n) log3 (m + n)).
To understand the conclusions of Corollary 22, consider the base algorithm of Corol-
(cid:107)L0 − ˆL(cid:107)F ≤ (cid:112)
lary 19, which, when applied to M, recovers an estimate ˆL satisfying
c1 max(ν 2 , 1)/β
with high probability. Corollary 14 asserts that, with appropriately reduced probability,
DFC-RP exhibits the same estimation error scaled by an adjustable factor of 2 + , while
DFC-Proj exhibits at most twice this error plus an adjustable factor of . Hence, DFC
can quickly provide near-optimal estimation for non-spiky matrices as well as incoherent
matrices, even when entries are missing. The proof of Corollary 22 can be found in Section N.

6. Experimental Evaluation

We now explore the accuracy and speed-up of DFC on a variety of simulated and real-world
data sets. We use the Accelerated Proximal Gradient (APG) algorithm of Toh and Yun
(2010) as our base noisy MC algorithm5 and the APG algorithm of Lin et al. (2009b) as our
base noisy RMF algorithm. In order to provide a fair comparison with baseline algorithms,
we perform all experiments on an x86-64 architecture using a single 2.60 Ghz core and 30GB
of main memory. In practice, one will typically run DFC jobs in a distributed fashion across
a cluster; our released code supports this standard use case. We use the default parameter
settings suggested by Toh and Yun (2010) and Lin et al. (2009b), and measure estimation
error via root mean square error (RMSE). To achieve a fair running time comparison, we
execute each subproblem in the F step of DFC in a serial fashion on the same machine
using a single core. Since, in practice, each of these subproblems would be executed in
parallel, the paral lel running time of DFC is calculated as the time to complete the D and
C steps of DFC plus the running time of the longest running subproblem in the F step.
We compare DFC with two baseline methods: the base algorithm APG applied to the full
matrix M and Partition, which carries out the D and F steps of DFC-Proj but omits
the ﬁnal C step (pro jection). We denote a particular sampling method along with the size
of its partitions as ‘method-xx %,’ e.g., Proj-25% refers to DFC-Proj with partitioned
submatrices containing 25% of the columns of the full matrix (i.e., t = 4). For Partition,
DFC-Proj, and DFC-RP, we orient our data matrices such that n ≥ m and partition by
column. Moreover, for DFC-RP we set p = 5 and q = 2.

5. Our experiments with the Augmented Lagrange Multiplier (ALM) algorithm of Lin et al. (2009a) as a
base algorithm (not reported) yield comparable relative speedups and performance for DFC.

932

Distributed Matrix Completion and Robust Factorization

6.1 Simulations
For our simulations, we focused on square matrices (m = n) and generated random low-rank
AB(cid:62) , where A and B are m × r matrices with independent N (0, (cid:112)1/r) entries such that
and sparse decompositions, similar to the schemes used in related work (Cand`es et al., 2011;
Keshavan et al., 2010; Zhou et al., 2010). We created L0 ∈ Rm×m as a random product,
each entry of L0 has unit variance. Z0 contained independent N (0, 0.1) entries. In the MC
setting, s entries of L0 + Z0 were revealed uniformly at random. In the RMF setting, the
support of S0 was generated uniformly at random, and the s corrupted entries took values
in [0, 1] with uniform probability. For each algorithm, we report error between L0 and the
estimated low-rank matrix, and all reported results are averages over ten trials.

(a)

(b)

(c)

(d)

Figure 1: Recovery error of DFC relative to base algorithms.

We ﬁrst explored the estimation error of DFC as a function of s, using (m = 10K,
r = 10) with varying observation sparsity for MC and (m = 1K, r = 10) with a varying
percentage of outliers for RMF. The results are summarized in Figure 1. In both MC and
RMF, the gaps in estimation between APG and DFC are small when sampling only 10%
of rows and columns. Moreover, of the standard DFC algorithms, DFC-RP performs the
best, as shown in Figures 1(a) and (b). Ensembling improves the performance of DFC-
Nys and DFC-Proj, as shown in Figures 1(c) and (d), and DFC-Proj-Ens in particular
consistently outperforms Partition and DFC-Nys-Ens, slightly outperforms DFC-RP,
and matches the performance of APG for most settings of s. In practice we observe that Lrp
equals the optimal (with respect to the spectral or Frobenius norm) rank-k approximation

933

024681000.050.10.150.20.25MCRMSE% revealed entries  Part−10%Proj−10%Nys−10%RP−10%Base−MC01020304050607000.050.10.150.20.25RMFRMSE% of outliers  Part−10%Proj−10%Nys−10%RP−10%Base−RMF024681000.050.10.150.20.25MC EnsembleRMSE% revealed entries  Part−10%Proj−Ens−10%Nys−Ens−10%RP−Ens−10%Proj−Ens−25%Base−MC01020304050607000.050.10.150.20.25RMF EnsembleRMSE% of outliers  Part−10%Proj−Ens−10%Nys−Ens−10%RP−Ens−10%Base−RMFMackey, Talwalkar and Jordan

of [ ˆC1 , . . . , ˆCt ], and thus the performance of DFC-RP consistently matches that of DFC-
RP-Ens. We therefore omit the DFC-RP-Ens results in the remainder this section.
We next explored the speed-up of DFC as a function of matrix size. For MC, we revealed
4% of the matrix entries and set r = 0.001 · m, while for RMF we ﬁxed the percentage of
outliers to 10% and set r = 0.01 ·m. We sampled 10% of rows and columns and observed that
estimation errors were comparable to the errors presented in Figure 1 for similar settings
of s; in particular, at all values of n for both MC and RMF, the errors of APG and DFC-
Proj-Ens were nearly identical. Our timing results, presented in Figure 2, illustrate a
near-linear speed-up for MC and a superlinear speed-up for RMF across varying matrix
sizes. Note that the timing curves of the DFC algorithms and Partition all overlap, a
fact that highlights the minimal computational cost of the ﬁnal matrix approximation step.

Figure 2: Speed-up of DFC relative to base algorithms.

6.2 Collaborative Filtering

Collaborative ﬁltering for recommender systems is one prevalent real-world application of
noisy matrix completion. A collaborative ﬁltering data set can be interpreted as the in-
complete observation of a ratings matrix with columns corresponding to users and rows
corresponding to items. The goal is to infer the unobserved entries of this ratings matrix.
We evaluate DFC on two of the largest publicly available collaborative ﬁltering data sets:
MovieLens 10M (http://www.grouplens.org/) with m = 10K, n = 72K, s > 10M, and
the Netﬂix Prize data set (http://www.netflixprize.com/) with m = 18K, n = 480K,
s > 100M. To generate test sets drawn from the training distribution, for each data set,
we aggregated all available rating data into a single training set and withheld test entries
uniformly at random, while ensuring that at least one training observation remained in
each row and column. The algorithms were then run on the remaining training portions
and evaluated on the test portions of each split. The results, averaged over three train-test
splits, are summarized in Table 2. Notably, DFC-Proj, DFC-Proj-Ens, DFC-Nys-Ens,
and DFC-RP all outperform Partition, and DFC-Proj-Ens performs comparably to
APG while providing a nearly linear parallel time speed-up. Similar to the simulation re-
sults presented in Figure 1, DFC-RP performs the best of the standard DFC algorithms,
though DFC-Proj-Ens slightly outperforms DFC-RP. Moreover, the poorer performance
of DFC-Nys can be in part explained by the asymmetry of these problems. Since these
matrices have many more columns than rows, MF on column submatrices is inherently

934

12345x 1040500100015002000250030003500MCtime (s)m  Part−10%RP−10%Proj−Ens−10%Nys−Ens−10%Base−RMF1000200030004000500000.511.522.5x 104RMFtime (s)m  Part−10%RP−10%Proj−Ens−10%Nys−Ens−10%Base−RMFDistributed Matrix Completion and Robust Factorization

Method

MovieLens 10M
RMSE
Time

Netﬂix
RMSE
Time

Base algorithm (APG) 0.8005

552.3s

0.8433 4775.4s

Partition-25%
Partition-10%

DFC-Nys-25%
DFC-Nys-10%

DFC-Nys-Ens-25%
DFC-Nys-Ens-10%

DFC-Proj-25%
DFC-Proj-10%

0.8146
0.8461

0.8449
0.8776

0.8085
0.8328

0.8061
0.8270

146.2s
56.0s

141.9s
82.5s

153.5s
96.2s

146.3s
56.0s

0.8451
0.8491

0.8832
0.9228

0.8486
0.8613

0.8436
0.8486

1274.6s
548.0s

1541.2s
797.4s

1661.2s
909.8s

1274.8s
548.1s

DFC-Pro j-Ens-25%
DFC-Pro j-Ens-10%

0.7944
0.8117

146.3s
56.0s

0.8411 1274.8s
0.8434
548.1s

DFC-RP-25%
DFC-RP-10%

0.8027
0.8074

147.4s
56.2s

0.8438
0.8448

1283.6s
550.1s

Table 2: Performance of DFC relative to base algorithm APG on collaborative ﬁltering
tasks.

easier than MF on row submatrices, and for DFC-Nys, we observe that ˆC is an accurate
estimate while ˆR is not.

6.3 Background Modeling in Computer Vision

Background modeling has important practical ramiﬁcations for detecting activity in surveil-
lance video. This problem can be framed as an application of noisy RMF, where each video
frame is a column of some matrix (M), the background model is low-rank (L0 ), and moving
ob jects and background variations, e.g., changes in illumination, are outliers (S0 ). We evalu-
ate DFC on two videos (treating each frame as a row): ‘Hall’ (200 frames of size 176 × 144)
contains signiﬁcant foreground variation and was studied by Cand`es et al. (2011), while
‘Lobby’ (1546 frames of size 168 × 120) includes many changes in illumination (a smaller
video with 250 frames was studied by Cand`es et al. 2011). We focused on DFC-Proj-Ens,
due to its superior performance in previous experiments, and measured the RMSE between
the background model estimated by DFC and that of APG. On both videos, DFC-Proj-
Ens estimated nearly the same background model as the full APG algorithm in a small
fraction of the time. On ‘Hall,’ the DFC-Proj-Ens-5% and DFC-Proj-Ens-0.5% models
exhibited RMSEs of 0.564 and 1.55, quite small given pixels with 256 intensity values. The
associated running time was reduced from 342.5s for APG to real-time (5.2s for a 13s video)
for DFC-Proj-Ens-0.5%. Snapshots of the results are presented in Figure 3. On ‘Lobby,’

935

Mackey, Talwalkar and Jordan

Original frame

APG
(342.5s)

5% sampled
(24.2s)

0.5% sampled
(5.2s)

Figure 3: Sample ‘Hall’ estimation by APG, DFC-Proj-Ens-5%, and DFC-Proj-Ens-
.5%.

the RMSE of DFC-Proj-Ens-4% was 0.64, and the speed-up over APG was more than
20X, i.e., the running time reduced from 16557s to 792s.

6.4 From Theory to Practice

Our experimental results suggest that the theoretical error bounds of Secs. 4 and 5 can be
further tightened. In particular, our master theorems Theorems 12 and 20 guarantee that
DFC-Proj-Ens and DFC-RP are never more than a constant factor worse than Par-
tition, yet in both real data experiments and simulations we observe signiﬁcant gains in
accuracy over Partition due to the incorporation of pro jection and ensembling. More-
over, our theory gives rise to comparable estimation guarantees for DFC-Nys, albeit under
stronger assumptions as noted in Remark 13. This is a surprising fact given that DFC-Nys
may make use of only a vanishingly small subset of all available matrix entries; however, we
ﬁnd that for data sets with high noise levels, methods that make use of all available data
like DFC-Proj and DFC-RP are unsurprisingly more accurate than DFC-Nys. We view
addressing these gaps between theory and practice as important directions for future work.

7. Conclusions

To improve the scalability of existing matrix factorization algorithms while leveraging the
ubiquity of parallel computing architectures, we introduced, evaluated, and analyzed DFC,
a divide-and-conquer framework for noisy matrix factorization with missing entries or out-
liers. DFC is trivially parallelized and particularly well suited for distributed environments
given its low communication footprint. Moreover, DFC provably maintains the estimation
guarantees of its base algorithm, even in the presence of noise, and yields linear to super-
linear speedups in practice. A number of natural follow-up questions suggest themselves:

• Can the sampling complexities and conclusions of our theoretical analyses be strength-
ened? For example, can the (2 + ) approximation guarantees of our master theorems
be sharpened to (1 + )? More generally, can we close the gaps between theory and
practice described in Section 6.4?

936

Distributed Matrix Completion and Robust Factorization

• How does DFC compare empirically with scalable heuristics for MC and RMF that
have little theoretical backing (see, e.g., Zhou et al., 2008; Gemulla et al., 2011; Recht
and R´e, 2011; F. Niu et al., 2011; Yu et al., 2012; Mu et al., 2011)? Is improved perfor-
mance obtained by pairing DFC with base algorithms lacking theoretical guarantees
but displaying other practical beneﬁts?
• Which algorithmic reﬁnements lead to enhanced performance for DFC? For instance,
could ensemble variants of DFC be improved by learning combination weights in a
manner analogous to that of Kumar et al. (2009b)? In the matrix completion setting,
could one use held-out entries to determine the optimal dimension (via rows or via
columns) for partitioning in DFC-Proj or DFC-RP?

These open questions are fertile ground for future work.

Acknowledgments

Lester Mackey gratefully acknowledges the support of DARPA through the National De-
fense Science and Engineering Graduate Fellowship Program. Ameet Talwalkar gratefully
acknowledges support from NSF award No. 1122732.

Appendix A. Proof of Theorem 5: Subsampled Regression under
Incoherence

We now give a proof of Theorem 5. While the results of this section are stated in terms of
i.i.d. with-replacement sampling of columns and rows, a concise argument due to Hoeﬀding
(1963, Section 6) implies the same conclusions when columns and rows are sampled without
replacement.
Our proof of Theorem 5 will require a strengthened version of the randomized (cid:96)2 regres-
sion work of Drineas et al. (2008, Theorem 5). The proof of Theorem 5 of Drineas et al.
(2008) relies heavily on the fact that (cid:107)AB − GH(cid:107)F ≤ 
2 (cid:107)A(cid:107)F (cid:107)B(cid:107)F with probability at
least 0.9, when G and H contain suﬃciently many rescaled columns and rows of A and
B, sampled according to a particular non-uniform probability distribution. A result of Hsu
et al. (2012), modiﬁed to allow for slack in the probabilities, establishes a related claim with
improved sampling complexity.6

Lemma 24 (Hsu et al. 2012, Example 4.3) Given a matrix A ∈ Rm×k with r ≥ rank(A),
an error tolerance  ∈ (0, 1], and a failure probability δ ∈ (0, 1], deﬁne probabilities pj sat-
(cid:88)
and (cid:80)k
isfying
j=1pj = 1
j

(cid:107)A(j )(cid:107)2 , Z =

pj ≥ β
Z

(cid:107)A(j )(cid:107)2 ,

6. The general conclusion of (Hsu et al., 2012, Example 4.3) is incorrectly stated as noted in Hsu (2012).
However, the original statement is correct in the special case when a matrix is multiplied by its own
transpose, which is the case of interest here.

937

Mackey, Talwalkar and Jordan

for some β ∈ (0, 1]. Let G ∈ Rm×l be a column submatrix of A in which exactly l ≥
1/(cid:112)lpj whenever the j -th column of A is selected on the t-th sampling trial, for t = 1, . . . , l.
48r log(4r/(β δ))/(β 2 ) columns are selected in i.i.d. trials in which the j -th column is chosen
with probability pj . Further, let D ∈ Rl×l be a diagonal rescaling matrix with entry Dtt =
Then, with probability at least 1 − δ ,
(cid:107)AA(cid:62) − GDDG(cid:62)(cid:107)2 ≤ 
(cid:107)A(cid:107)2
2 .
2

Using Lemma 24, we now establish a stronger version of Lemma 1 of Drineas et al.
(2008). For a given β ∈ (0, 1] and L ∈ Rm×n with rank r, we ﬁrst deﬁne column sampling
and (cid:80)n
probabilities pj satisfying
pj ≥ β
(cid:107)(VL )(j )(cid:107)2
j=1pj = 1.
(4)
r
We further let S ∈ Rn×l be a random binary matrix with independent columns, where a
Moreover, let D ∈ Rl×l be a diagonal rescaling matrix with entry Dtt = 1/(cid:112)lpj whenever
single 1 appears in each column, and Sj t = 1 with probability pj for each t ∈ {1, . . . , l}.
Sj t = 1. Postmultiplication by S is equivalent to selecting l random columns of a matrix,
independently and with replacement. Under this notation, we establish the following lemma:

l D)+ − (V(cid:62)
Lemma 25 Let  ∈ (0, 1], and deﬁne V(cid:62)
l D)(cid:62) .
L S and Γ = (V(cid:62)
l = V(cid:62)
l ≥ 48r log(4r/(β δ))/(β 2 ) for δ ∈ (0, 1] then with probability at least 1 − δ :

If

rank(Vl ) = rank(VL ) = rank(L)
(cid:107)Γ(cid:107)2 = (cid:107)Σ−1
− ΣV (cid:62)
l D (cid:107)
V (cid:62)
l D
2
l D)+Σ−1
(LSD)+ = (V(cid:62)
L U(cid:62)
√
L
(cid:107)Σ−1
l D (cid:107)
− ΣV (cid:62)
≤ /
2.
V (cid:62)
l D
2
Proof By Lemma 24, for all 1 ≤ i ≤ r,
l DDVl )|
l D)| = |σi (V(cid:62)
L VL ) − σi (V(cid:62)
|1 − σ2
i (V(cid:62)
≤ (cid:107)V(cid:62)
L VL − V(cid:62)
L SDDS(cid:62)VL(cid:107)2
L (cid:107)2
≤ /2(cid:107)V(cid:62)
2 = /2,
where σi (·) is the i-th largest singular value of a given matrix. Since /2 ≤ 1/2, each singu-
lar value of Vl is positive, and so rank(Vl ) = rank(VL ) = rank(L). The remainder of the
proof is identical to that of Lemma 1 of Drineas et al. (2008).

Lemma 25 immediately yields improved sampling complexity for the randomized (cid:96)2
regression of Drineas et al. (2008):
Proposition 26 Suppose B ∈ Rp×n and  ∈ (0, 1]. If l ≥ 3200r log(4r/(β δ))/(β 2 ) for
δ ∈ (0, 1], then with probability at least 1 − δ − 0.2:
(cid:107)B − BSD(LSD)+L(cid:107)F ≤ (1 + )(cid:107)B − BL+L(cid:107)F .

938

Distributed Matrix Completion and Robust Factorization

Proof The proof is identical to that of Theorem 5 of Drineas et al. (2008) once Lemma 25
is substituted for Lemma 1 of Drineas et al. (2008).

A typical application of Prop. 26 would involve performing a truncated SVD of M to
obtain the statistical leverage scores, (cid:107)(VL )(j )(cid:107)2 , used to compute the column sampling
probabilities of Eq. (4). Here, we will take advantage of the slack term, β , allowed in the
sampling probabilities of Eq. (4) to show that uniform column sampling gives rise to the
same estimation guarantees for column pro jection approximations when L is suﬃciently
incoherent.
To prove Theorem 5, we ﬁrst notice that n ≥ rµ0 (VL ) and hence
l ≥ 3200rµ0 (VL ) log(4rµ0 (VL )/δ)/2
≥ 3200r log(4r/(β δ))/(β 2 )
whenever β ≥ 1/µ0 (VL ). Thus, we may apply Prop. 26 with β = 1/µ0 (VL ) ∈ (0, 1] and
pj = 1/n by noting that
(cid:107)(VL )(j )(cid:107)2 ≤ β
β
1
r
for all j , by the deﬁnition of µ0 (VL ). By our choice of probabilities, D = I(cid:112)n/l, and hence
µ0 (VL ) =
= pj
n
n
r
r
C L(cid:107)F = (cid:107)B − BC D(LC D)+L(cid:107)F ≤ (1 + )(cid:107)B − BL+L(cid:107)F
(cid:107)B − BC L+
with probability at least 1 − δ − 0.2, as desired.

Appendix B. Proof of Lemma 4: Conservation of Incoherence

Since for all n > 1,
c log(n) log(1/δ) = (c/4) log(n4 ) log(1/δ) ≥ 48 log(4n2/δ) ≥ 48 log(4rµ0 (VL )/(δ/n))
for all j , and D = I(cid:112)n/l. When rank(LC ) = rank(L), Lemma 1 of Mohri and Talwalkar
as n ≥ rµ0 (VL ), claim i follows immediately from Lemma 25 with β = 1/µ0 (VL ), pj = 1/n
(2011) implies that PULC
= PUL , which in turn implies claim ii.
To prove claim iii given the conclusions of Lemma 25, assume, without loss of generality,
that Vl consists of the ﬁrst l rows of VL . Then if LC = ULΣLV(cid:62)
l has rank(LC ) =
rank(L) = r, the matrix Vl must have full column rank. Thus we can write
C LC = (ULΣLV(cid:62)
l )+ULΣLV(cid:62)
L+
l
L ULΣLV(cid:62)
= (ΣLV(cid:62)
l )+U+
l
l )+ΣLV(cid:62)
= (ΣLV(cid:62)
l
= (V(cid:62)
L ΣLV(cid:62)
l )+Σ+
l
l )+V(cid:62)
= (V(cid:62)
l
l Vl )−1V(cid:62)
= Vl (V(cid:62)
l ,

939

Mackey, Talwalkar and Jordan

where the second and third equalities follow from UL having orthonormal columns, the
fourth and ﬁfth result from ΣL having full rank and Vl having full column rank, and the
sixth follows from V(cid:62)
l having full row rank.
Now, denote the right singular vectors of LC by VLC ∈ Rl×r . Observe that PVLC
=
VLC V(cid:62)
= L+
C LC , and deﬁne ei,l as the ith column of Il and ei,n as the ith column of In .
LC
Then we have,

µ0 (VLC ) =

=

=

=

=

l
r
l
r
l
r
l
r
l
r

max
1≤i≤l

max
1≤i≤l

max
1≤i≤l

max
1≤i≤l

max
1≤i≤l

ei,l (cid:107)2

(cid:107)PVLC
e(cid:62)
i,lL+
C LC ei,l
l )+V(cid:62)
i,l (V(cid:62)
e(cid:62)
l ei,l
i,lVl (V(cid:62)
l Vl )−1V(cid:62)
e(cid:62)
l ei,l
l Vl )−1V(cid:62)
e(cid:62)
i,nVL (V(cid:62)
L ei,n ,

L ei,n for all 1 ≤ i ≤ l.
l ei,l = V(cid:62)
where the ﬁnal equality follows from V(cid:62)
Now, deﬁning Q = V(cid:62)
l Vl we have

µ0 (VLC ) =

=

l
r
l
r
l
r
≤ l
r

=

Tr

max
1≤i≤l

max
1≤i≤l

(cid:105)
(cid:104)
i,nVLQ−1V(cid:62)
e(cid:62)
L ei,n
(cid:105)
(cid:104)
i,nVLQ−1V(cid:62)
e(cid:62)
L ei,n
L ei,ne(cid:62)
Q−1V(cid:62)
max
i,nVL
Tr
1≤i≤l
(cid:107)V(cid:62)
(cid:107)Q−1(cid:107)2 max
i,nVL(cid:107)∗ ,
L ei,ne(cid:62)
1≤i≤l

L ei,ne(cid:62)
by H¨older’s inequality for Schatten p-norms. Since V(cid:62)
i,nVL has rank one, we can
explicitly compute its trace norm as (cid:107)V(cid:62)
L ei,n(cid:107)2 = (cid:107)PVL ei,n(cid:107)2 . Hence,
(cid:18) n
(cid:19)
(cid:107)Q−1(cid:107)2 max
1≤i≤l
(cid:107)Q−1(cid:107)2
r
max
1≤i≤n
n
r
(cid:107)Q−1(cid:107)2µ0 (VL ) ,

(cid:107)PVL ei,n(cid:107)2
(cid:107)PVL ei,n(cid:107)2

µ0 (VLC ) ≤ l
r
≤ l
r
l
n

=

by the deﬁnition of µ0 -coherence. The proof of Lemma 25 established that the smallest
l DDVl is lower bounded by 1 − 
2 and hence (cid:107)Q−1(cid:107)2 ≤
l Q = V(cid:62)
singular value of n
n
l(1−/2) .
Thus, we conclude that µ0 (VLC ) ≤ µ0 (VL )/(1 − /2).

940

Distributed Matrix Completion and Robust Factorization

µ1 (LC ) =

|e(cid:62)
i,mULC V(cid:62)
LC

(cid:114)
To prove claim iv under Lemma 25, we note that
ej,l |
ml
(cid:114)
max
1≤i≤m
r
1≤j≤l
(cid:19)(cid:18)(cid:114)
(cid:18)(cid:114) m
(cid:19)
(cid:107)U(cid:62)
(cid:107)V(cid:62)
ei,m(cid:107) max
ej,l (cid:107)
ml
max
1≤i≤m
1≤j≤l
LC
LC
r
√
(cid:113)
ej,l (cid:107)
(cid:107)PVLC
ei,m(cid:107)
(cid:107)PULC
l
rµ0 (ULC )µ0 (VLC ) ≤ (cid:112)rµ0 (UL )µ0 (VL )/(1 − /2)
max
max
r
1≤i≤m
1≤j≤l
r
r
by H¨older’s inequality for Schatten p-norms, the deﬁnition of µ0 -coherence, and claims ii
and iii.

≤

=

=

Appendix C. Proof of Corollary 6: Column Pro jection under Incoherence

Fix c = 48000/ log(1/0.45), and notice that for n > 1,

48000 log(n) ≥ 3200 log(n5 ) ≥ 3200 log(16n).
Hence l ≥ 3200rµ0 (VL ) log(16n)(log(δ)/ log(0.45))/2 .
Now partition the columns of C into b = log(δ)/ log(0.45) submatrices, C = [C1 , · · · , Cb ],
each with a = l/b columns,7 and let [LC1 , · · · , LCb ] be the corresponding partition of LC .
Since
a ≥ 3200rµ0 (VL ) log(4n/0.25)/2 ,

we may apply Prop. 26 independently for each i to yield
≤ (1 + )(cid:107)M − ML+L(cid:107)F ≤ (1 + )(cid:107)M − L(cid:107)F
(cid:107)M − CiL+
Ci
F
with probability at least 0.55, since ML+ minimizes (cid:107)M − YL(cid:107)F over all Y ∈ Rm×m .
Since each Ci = CSi for some matrix Si and C+M minimizes (cid:107)M − CX(cid:107)F over all
X ∈ Rl×n , it follows that

L(cid:107)

(5)

for each i. Hence, if

(cid:107)M − CC+M(cid:107)F ≤ (cid:107)M − CiL+
Ci

L(cid:107)
F

,

(cid:107)M − CC+M(cid:107)F ≤ (1 + )(cid:107)M − L(cid:107)F ,

fails to hold, then, for each i, Eq. (5) also fails to hold. The desired conclusion therefore
must hold with probability at least 1 − 0.45b = 1 − δ .

7. For simplicity, we assume that b divides l evenly.

941

Mackey, Talwalkar and Jordan

Appendix D. Proof of Corollary 7: Generalized Nystr¨om Method under
Incoherence

With c = 48000/ log(1/0.45) as in Corollary 6, we notice that for m > 1,
48000 log(m) = 16000 log(m3 ) ≥ 16000 log(4m).

Therefore,

d ≥ 16000rµ0 (UC ) log(4m)(log(δ (cid:48) )/ log(0.45))/2
≥ 3200rµ0 (UC ) log(4m/δ (cid:48) )/2 ,
for all m > 1 and δ (cid:48) ≤ 0.8. Hence, we may apply Theorem 5 and Corollary 6 in turn to
obtain
(cid:107)M − CW+R(cid:107)F ≤ (1 + )(cid:107)M − CC+M(cid:107)F ≤ (1 + )2(cid:107)M − L(cid:107)
with probability at least (1 − δ)(1 − δ (cid:48) − 0.2) by independence.

Appendix E. Proof of Corollary 8: Noiseless Generalized Nystr¨om
Method under Incoherence
Since rank(L) = r, L admits a decomposition L = Y(cid:62)Z for some matrices Y ∈ Rr×m
Y and Z as Y = (cid:2)Y1 Y2
(cid:3) and Z = (cid:2)Z1 Z2
(cid:3) for Y1 ∈ Rr×d and Z1 ∈ Rr×l , we may
and Z ∈ Rr×n . In particular, let Y(cid:62) = ULΣ
LV(cid:62)
1
1
L and Z = Σ
L . By block partitioning
2
2
write W = Y(cid:62)
1 Z1 , C = Y(cid:62)Z1 , and R = Y(cid:62)
1 Z. Note that we assume that the generalized
Nystr¨om approximation is generated from sampling the ﬁrst l columns and the ﬁrst d rows
of L, which we do without loss of generality since the rows and columns of the original
low-rank matrix can always be permuted to match this assumption.
Prop. 27 shows that, like the Nystr¨om method (Kumar et al., 2009a), the generalized
Nystr¨om method yields exact recovery of L whenever rank(L) = rank(W). The same result
was established in Wang et al. (2009) with a diﬀerent proof.
Proposition 27 Suppose r = rank(L) ≤ min(d, l) and rank(W) = r. Then L = Lnys .

Proof By appealing to our factorized block decomposition, we may rewrite the generalized
Nystr¨om approximation as Lnys = CW+R = Y(cid:62)Z1 (Y(cid:62)
1 Z1 )+Y(cid:62)
1 Z. We ﬁrst note that
rank(W) = r implies that rank(Y1 ) = r and rank(Z1 ) = r so that Z1Z(cid:62)
1 and Y1Y(cid:62)
1 are
full-rank. Hence, (Y(cid:62)
1 Z1 )+ = Z(cid:62)
1 (Z1Z(cid:62)
1 )−1 (Y1Y(cid:62)
1 )−1Y1 , yielding
1 )−1Y1Y(cid:62)
1 )−1 (Y1Y(cid:62)
1 (Z1Z(cid:62)
Lnys = Y(cid:62)Z1Z(cid:62)
1 Z = Y(cid:62)Z = L.

Prop. 27 allows us to lower bound the probability of exact recovery with the probability
of randomly selecting a rank-r submatrix. As rank(W) = r iﬀ both rank(Y1 ) = r and
rank(Z1 ) = r, it suﬃces to characterize the probability of selecting full rank submatrices of
Y and Z. Following the treatment of the Nystr¨om method in Talwalkar and Rostamizadeh

942

Distributed Matrix Completion and Robust Factorization

− 1
− 1
L = Vl where Vl ∈ Rl×r contains
L Z = V(cid:62)
L and hence that Z(cid:62)
(2010), we note that Σ
1 Σ
2
2
the ﬁrst l components of the leading r right singular vectors of L. It follows that rank(Z1 ) =
− 1
L ) = rank(Vl ). Similarly, rank(Y1 ) = rank(Ud ) where Ud ∈ Rd×r contains the
rank(Z(cid:62)
1 Σ
2
ﬁrst d components of the leading r left singular vectors of L. Thus, we have

P(rank(Z1 ) = r) = P(rank(Vl ) = r)
P(rank(Y1 ) = r) = P(rank(Ud ) = r).

and

(6)

(7)

Next we can apply the ﬁrst result of Lemma 25 to lower bound the RHSs of Eq. (6)
and Eq. (7) by selecting  = 1, S such that its diagonal entries equal 1, and β = 1
µ0 (VL ) for
the RHS of Eq. (6) and β = 1
µ0 (UL ) for the RHS of Eq. (7). In particular, given the lower
√
bounds on d and l in the statement of the corollary, the RHSs are each lower bounded by
1 − δ . Furthermore, by the independence of row and column sampling and Eq. (6) and
Eq. (7), we see that

1 − δ ≤ P(rank(Ud ) = r)P(rank(Vl ) = r)
= P(rank(Y1 ) = r)P(rank(Z1 ) = r)
= P(rank(W) = r).

Finally, Prop. 27 implies that

P(L = Lnys ) ≥ P(rank(W) = r) ≥ 1 − δ,

which proves the statement of the theorem.

Appendix F. Proof of Corollary 9: Random Pro jection

Our proof rests upon the following random pro jection guarantee of Halko et al. (2011):

Theorem 28 (Halko et al. 2011, Theorem 10.7) Given a matrix M ∈ Rm×n and a
rank-r approximation L ∈ Rm×n with r ≥ 2, choose an oversampling parameter p ≥ 4,
where r + p ≤ min(m, n). Draw an n × (r + p) standard Gaussian matrix G, let Y = MG.
For al l u, t ≥ 1,
(cid:107)M − PY M(cid:107)F ≤ (1 + t(cid:112)12r/p)(cid:107)M − Mr (cid:107)F + ut · e
√
with probability at least 1 − 5t−p − 2e−u2 /2 .
Fix (u, t) = ((cid:112)2 log(7/δ), e), and note that
1 − 5e−p − 2e−u2 /2 = 1 − 5e−p − 2δ/7 ≥ 1 − δ,

(cid:107)M − Mr (cid:107)

r + p
p + 1

943

Mackey, Talwalkar and Jordan

(cid:107)M − Mr (cid:107)2

e2(cid:112)2(r + p) log(7/δ)
since p ≥ log(7/δ). Hence, Theorem 28 implies that
(cid:107)M − PY M(cid:107)F ≤ (1 + e(cid:112)12r/p)(cid:107)M − Mr (cid:107)F +
(cid:33)
(cid:32)
e2(cid:112)2(r + p) log(7/δ)
1 + e(cid:112)12r/p +
p + 1
(cid:107)M − L(cid:107)F
≤
≤ (cid:16)
(cid:17)(cid:107)M − L(cid:107)F
1 + e(cid:112)12r/p + e2(cid:112)2r log(7/δ)/p
p + 1
≤ (cid:16)
(cid:17)(cid:107)M − L(cid:107)F ≤ (1 + )(cid:107)M − L(cid:107)F
1 + 11(cid:112)2r log(7/δ)/p
with probability at least 1 − δ , where the second inequality follows from (cid:107)M − Mr (cid:107)2 ≤
√
√
√
(cid:107)M − Mr (cid:107)F ≤ (cid:107)M − L(cid:107)F , the third follows from
p ≤ (p + 1)
r + p
r for all r and p,
and the ﬁnal follows from our choice of p ≥ 242 r log(7/δ)/2 .
Next, we note, as in the proof of Theorem 9.3 of Halko et al. (2011), that
(cid:107)PY M − Lrp(cid:107)F ≤ (cid:107)PY M − PY Mr (cid:107)F ≤ (cid:107)M − Mr (cid:107)F ≤ (cid:107)M − L(cid:107)F .
The ﬁrst inequality holds because Lrp is by deﬁnition the best rank-r approximation to
PY M and rank(PY Mr ) ≤ r. The second inequality holds since
(cid:107)M − Mr (cid:107)F = (cid:107)PY (M − Mr )(cid:107)F + (cid:107)P⊥
Y (M − Mr )(cid:107)F .
The ﬁnal inequality holds since Mr is the best rank-r approximation to M and rank(L) = r.
Moreover, by the triangle inequality,
(cid:107)M − Lrp(cid:107)F ≤ (cid:107)M − PY M(cid:107)F + (cid:107)PY M − Lrp(cid:107)F
≤ (cid:107)M − PY M(cid:107)F + (cid:107)M − L(cid:107)F .
Combining Eq. (8) with the ﬁrst statement of the corollary yields the second statement.

(8)

Appendix G. Proof of Theorem 12: Coherence Master Theorem

G.1 Proof of DFC-Pro j and DFC-RP Bounds
Let L0 = [C0,1 , . . . , C0,t ] and ˜L = [ ˆC1 , . . . , ˆCt ]. Deﬁne A(X) as the event that a matrix
1−/2 , r)-coherent and K as the event (cid:107) ˜L − ˆLproj (cid:107)F ≤ (1 + )(cid:107)L0 − ˜L(cid:107)F . When K
X is ( rµ2
holds, we have that
(cid:113)(cid:80)t
(cid:107)L0 − ˆLproj (cid:107)F ≤ (cid:107)L0 − ˜L(cid:107)F + (cid:107) ˜L − ˆLproj (cid:107)F ≤ (2 + )(cid:107)L0 − ˜L(cid:107)F
by the triangle inequality, and hence it suﬃces to lower bound P(K ∩ (cid:84)
i=1(cid:107)C0,i − ˆCi(cid:107)2
= (2 + )
F ,
iA(C0,i )). Our
choice of l, with a factor of log(2/δ), implies that each A(C0,i ) holds with probability at
least 1 − δ/(2n) by Lemma 4, while K holds with probability at least 1 − δ/2 by Corollary 6.
iA(C0,i )) ≥ 1 − P(K c ) − (cid:80)
P(K ∩ (cid:84)
Hence, by the union bound,
iP(A(C0,i )c ) ≥ 1 − δ/2 − tδ/(2n) ≥ 1 − δ.
An identical proof with Corollary 9 substituted for Corollary 6 yields the random pro jection
result.

944

Distributed Matrix Completion and Robust Factorization

˜L =

G.2 Proof of DFC-Nys Bound
(cid:21)
(cid:20) ˆC1
(cid:21)
(cid:20) ˆC1
To prove the generalized Nystr¨om result, we redeﬁne ˜L and write it in block notation as:
(cid:3)
ˆR = (cid:2) ˆR1
ˆR2
ˆC2 L0,22
ˆC2
and L0,22 ∈ R(m−d)×(n−l) is the bottom right submatrix of L0 . We further redeﬁne K as
the event (cid:107) ˜L − ˆLnys(cid:107)F ≤ (1 + )2(cid:107)L0 − ˜L(cid:107)F . As above,
(cid:107)L0 − ˆLnys(cid:107)F ≤ (cid:107)L0 − ˜L(cid:107)F + (cid:107) ˜L − ˆLnys(cid:107)F ≤ (2 + 2 + 2 )(cid:107)L0 − ˜L(cid:107)F ≤ (2 + 3)(cid:107)L0 − ˜L(cid:107)F ,

, where

ˆC =

,

ˆR2

when K holds, by the triangle inequality. Our choices of l and
d ≥ clµ0 ( ˆC) log(m) log(4/δ)/2 ≥ crµ log(m) log(1/δ)/2
imply that A(C) and A(R) hold with probability at least 1 − δ/(2n) and 1 − δ/(4n) re-
spectively by Lemma 4, while K holds with probability at least (1 − δ/2)(1 − δ/4 − 0.2) by
Corollary 7. Hence, by the union bound,
P(K ∩ A(C) ∩ A(R)) ≥ 1 − P(K c ) − P(A(C)c ) − P(A(R)c )
≥ 1 − (1 − (1 − δ/2)(1 − δ/4 − 0.2)) − δ/(2n) − δ/(4n)
≥ (1 − δ/2)(1 − δ/4 − 0.2) − 3δ/8
≥ (1 − δ)(1 − δ − 0.2)

for all n ≥ 2 and δ ≤ 0.8.

Appendix H. Proof of Corollary 14: DFC-MC under Incoherence

H.1 Proof of DFC-Pro j and DFC-RP Bounds

mn∆,

H be the event that

We begin by proving the DFC-Proj bound. Let G be the event that
√
(cid:107)L0 − ˆLproj (cid:107)F ≤ (2 + )ce
(cid:113)(cid:80)t
(cid:107)L0 − ˆLproj (cid:107)F ≤ (2 + )
i=1(cid:107)C0,i − ˆCi(cid:107)2
F ,
1−/2 , r)-coherent, and, for each i ∈ {1, . . . , t}, Bi
A(X) be the event that a matrix X is ( rµ2
√
be the event that (cid:107)C0,i − ˆCi(cid:107)F > ce
ml∆.
Note that, by assumption,
l ≥ cµ2 r2 (m + n)nβ log2 (m + n)/(s2 ) ≥ crµ log(n)2β log(m + n)/2
≥ crµ log(n)((2β − 2) log(¯n) + log(2))/2 = crµ log(n) log(2¯n2β−2 )/2 .

945

Mackey, Talwalkar and Jordan

Hence the Coherence Master Theorem (Theorem 12) guarantees that, with probability at
least 1 − ¯n2−2β , H holds and the event A(C0,i ) holds for each i. Since G holds whenever H
iA(C0,i ) ∩ (cid:84)
i ) ≥ P(H ∩ (cid:84)
P(G) ≥ P(H ∩ (cid:84)
holds and B c
i holds for each i, we have
= P(H ∩ (cid:84)
iA(C0,i ))P((cid:84)
i | H ∩ (cid:84)
iB c
iB c
= P(H ∩ (cid:84)
iA(C0,i ))(1 − P((cid:83)
iBi | H ∩ (cid:84)
i )
iB c
iA(C0,i ))
≥ (1 − ¯n2−2β )(1 − (cid:80)
iA(C0,i )))
≥ 1 − ¯n2−2β − (cid:80)
iP(Bi | A(C0,i )))
iP(Bi | A(C0,i )).
To prove our desired claim, it therefore suﬃces to show
P(Bi | A(C0,i )) ≤ 4 log(¯n)¯n2−2β + ¯n−2β ≤ 5 log(¯n)¯n2−2β

.

and

β (cid:48) (cid:44)

β log(¯n)
log(max(m, l))

for each i.
For each i, let Di be the event that si < 32µ(cid:48) r(m + l)β (cid:48) log2 (m + l), where si is the
number of revealed entries in C0,i ,
µ(cid:48) (cid:44) µ2 r
1 − /2
,
By Theorem 10 and our choice of β (cid:48) ,
P(Bi | A(C0,i )) ≤ P(Bi | A(C0,i ), Dc
i ) + P(Di | A(C0,i ))
≤ 4 log(max(m, l)) max(m, l)2−2β (cid:48)
+ P(Di )
≤ 4 log(¯n)¯n2−2β + P(Di ).
Further, since the support of S0 is uniformly distributed and of cardinality s, the variable si
has a hypergeometric distribution with E(si ) = sl
P(si ≤ E(si ) − st) ≤ exp(cid:0)−2st2(cid:1).
n and hence satisﬁes Hoeﬀding’s inequality
for the hypergeometric distribution (Hoeﬀding, 1963, Section 6):
Since, by assumption,
s ≥ cµ2 r2 (m + n)nβ log2 (m + n)/(l2 ) ≥ 64µ(cid:48) r(m + l)nβ (cid:48) log2 (m + l)/l,
sl2/n2 ≥ cµ2 r2 (m + n)lβ log2 (m + n)/(n2 ) ≥ 4 log(¯n)β ,
(cid:18)
(cid:18) l
(cid:19)(cid:19)
− 32µ(cid:48) r(m + l)β (cid:48) log2 (m + l)
(cid:19)
(cid:18)
(cid:19)(cid:19)
(cid:18) l
(cid:18)
si < E(si ) − s
s
n
(cid:18)
(cid:19)
si < E(si ) − s
− l
si < E(si ) − s
= P
2n
n
− sl2
≤ exp
≤ exp(−2 log( ¯n)β ) = ¯n−2β .
2n2
Hence, P(Bi | A(C0,i )) ≤ 4 log(¯n)¯n2−2β + ¯n−2β for each i, and the DFC-Proj result follows.
Since, p ≥ 242 r log(14¯n2β−2 )/2 , the DFC-RP bound follows in an identical manner
from the Coherence Master Theorem (Theorem 12).

it follows that

l
2n

and

P(Di ) = P

≤ P

946

Distributed Matrix Completion and Robust Factorization

√

H.2 Proof of DFC-Nys Bound
For DFC-Nys, let BC be the event that (cid:107)C0 − ˆC(cid:107)F > ce
√
ml∆ and BR be the event that
(cid:107)R0 − ˆR(cid:107)F > ce
dn∆. The Coherence Master Theorem (Theorem 12) and our choice of
d ≥ clµ0 ( ˆC)(2β − 1) log2 (4¯n)¯n/(n2 ) ≥ clµ0 ( ˆC) log(m) log(4¯n2β−2 )/2
(cid:113)
guarantee that, with probability at least (1 − ¯n2−2β )(1 − ¯n2−2β − 0.2) ≥ 1 − 2¯n2−2β − 0.2,
F + (cid:107)R0 − ˆR(cid:107)2
(cid:107)C0 − ˆC(cid:107)2
(cid:107)L0 − ˆLnys(cid:107)F ≤ (2 + 3)
F ,
and both A(C) and A(R) hold. Moreover, since
d ≥ clµ0 ( ˆC)(2β − 1) log2 (4¯n)¯n/(n2 ) ≥ cµ2 r2 (m + n)¯nβ log2 (m + n)/(s2 ),
reasoning identical to the DFC-Proj case yields P(BC | A(C)) ≤ 4 log(¯n)¯n2−2β + ¯n−2β and
P(BR | A(R)) ≤ 4 log( ¯n)¯n2−2β + ¯n−2β , and the DFC-Nys bound follows as above.

Appendix I. Proof of Corollary 16: DFC-RMF under Incoherence

I.1 Proof of DFC-Pro j and DFC-RP Bounds
We begin by proving the DFC-Proj bound. Let G be the event that
√
(cid:107)L0 − ˆLproj (cid:107)F ≤ (2 + )c(cid:48)
mn∆
e
(cid:113)(cid:80)t
for the constant c(cid:48)
e deﬁned in Theorem 11, H be the event that
(cid:107)L0 − ˆLproj (cid:107)F ≤ (2 + )
i=1(cid:107)C0,i − ˆCi(cid:107)2
F ,
1−/2 , r)-coherent, and, for each i ∈ {1, . . . , t}, Bi
A(X) be the event that a matrix X is ( rµ2
√
be the event that (cid:107)C0,i − ˆCi(cid:107)F > c(cid:48)
ml∆.
e
We may take ρr ≤ 1, and hence, by assumption,
l ≥ cr2µ2β log2 (2¯n)/(2ρr ) ≥ crµ log(n) log(2¯nβ )/2 .

Hence the Coherence Master Theorem (Theorem 12) guarantees that, with probability at
least 1 − ¯n−β , H holds and the event A(C0,i ) holds for each i. Since G holds whenever H
P(G) ≥ P(H ∩ (cid:84)
i ) ≥ P(H ∩ (cid:84)
iA(C0,i ) ∩ (cid:84)
holds and B c
i holds for each i, we have
= P(H ∩ (cid:84)
iA(C0,i ))P((cid:84)
i | H ∩ (cid:84)
iB c
iB c
= P(H ∩ (cid:84)
iA(C0,i ))(1 − P((cid:83)
iBi | H ∩ (cid:84)
i )
iB c
iA(C0,i ))
≥ (1 − ¯n−β )(1 − (cid:80)
iA(C0,i )))
≥ 1 − ¯n−β − (cid:80)
iP(Bi | A(C0,i )))
iP(Bi | A(C0,i )).
To prove our desired claim, it therefore suﬃces to show
P(Bi | A(C0,i )) ≤ (cp + 1) ¯n−β

947

Mackey, Talwalkar and Jordan

for each i.
Deﬁne ¯m (cid:44) max(m, l) and β (cid:48)(cid:48) (cid:44) β log(¯n)/ log( ¯m) ≤ β (cid:48) . By assumption,
≤ ρr l(1 − /2)
≤ ρrm(1 − /2)
ρr l2
r ≤
and r ≤
ρrm
.
cµ2 rβ log2 (2¯n)
µ2 r log2 ( ¯m)
2µ2 r log2 (¯n)
µ2 r log2 ( ¯m)
P(Bi | A(C0,i )) ≤ P(cid:0)Bi | A(C0,i ), si ≤ (1 − ρsβ (cid:48)(cid:48) )ml(cid:1) + P(cid:0)si > (1 − ρsβ (cid:48)(cid:48) )ml | A(C0,i )(cid:1)
Hence, by Theorem 11 and the deﬁnitions of β (cid:48) and β (cid:48)(cid:48) ,
+ P(cid:0)si > (1 − ρsβ (cid:48)(cid:48) )ml(cid:1)
≤ cp ¯n−β + P(cid:0)si > (1 − ρsβ (cid:48) )ml(cid:1),
≤ cp ¯m−β (cid:48)(cid:48)
where si is the number of corrupted entries in C0,i . Further, since the support of S0 is
uniformly distributed and of cardinality s, the variable si has a hypergeometric distribution
with E(si ) = sl
n and hence satisﬁes Bernstein’s inequality for the hypergeometric (Hoeﬀding,
P(si ≥ E(si ) + st) ≤ exp(cid:0)−st2/(2σ2 + 2t/3)(cid:1) ≤ exp(cid:0)−st2n/4l(cid:1),
1963, Section 6):
(cid:19)(cid:19)
(cid:18)
(cid:18) (1 − ρsβ (cid:48) )ml
n ) ≤ l
n (1 − l
for all 0 ≤ t ≤ 3l/n and σ2 (cid:44) l
P(cid:0)si > (1 − ρsβ (cid:48) )ml(cid:1) = P
n . It therefore follows that
(cid:18) (1 − ρsβ (cid:48) )
(cid:18)
(cid:19)(cid:19)
− l
si > E(si ) + s
n
s
(cid:19)2(cid:33)
(cid:32)
− 1
l
(cid:18) (1 − ρsβ (cid:48) )
(1 − ρsβs )
= P
si > E(si ) + s
n
− 1
−s
l
(cid:18)
(cid:19)
(1 − ρsβs )
4n
(ρsβs − ρsβ (cid:48) )2
− ml
≤ ¯n−β
(cid:16) (1−ρs β (cid:48) )
(cid:17) ≤ 3l/n whenever 4βs −
(1 − ρsβs )
= exp
4
(1−ρs βs ) − 1
by our assumptions on s and l and the fact that l
n
3/ρs ≤ β (cid:48) . Hence, P(Bi | A(C0,i )) ≤ (cp + 1)¯n−β for each i, and the DFC-Proj result
follows.
Since, p ≥ 242 r log(14¯nβ )/2 , the DFC-RP bound follows in an identical manner from
the Coherence Master Theorem (Theorem 12).

≤ exp

I.2 Proof of DFC-Nys Bound
For DFC-Nys, let BC be the event that (cid:107)C0 − ˆC(cid:107)F > c(cid:48)
√
ml∆ and BR be the event that
e
(cid:107)R0 − ˆR(cid:107)F > c(cid:48)
dn∆. The Coherence Master Theorem (Theorem 12) and our choice of
e
d ≥ clµ0 ( ˆC)β log2 (4¯n)/2 guarantee that, with probability at least (1− ¯n−β )(1− ¯n−β −0.2) ≥
(cid:113)
1 − 2¯n−β − 0.2,
F + (cid:107)R0 − ˆR(cid:107)2
(cid:107)C0 − ˆC(cid:107)2
F ,

(cid:107)L0 − ˆLnys(cid:107)F ≤ (2 + 3)

√

948

Distributed Matrix Completion and Robust Factorization

and both A(C) and A(R) hold. Moreover, since
d ≥ clµ0 ( ˆC)β log2 (4¯n)/2 ≥ cµ2 r2β log2 (¯n)/(2ρr ),
reasoning identical to the DFC-Proj case yields
P(BC | A(C)) ≤ (cp + 1)¯n−β
and P(BR | A(R)) ≤ (cp + 1) ¯n−β ,
and the DFC-Nys bound follows as above.

Appendix J. Proof of Theorem 10: Noisy MC under Incoherence

In the spirit of Cand`es and Plan (2010), our proof will extend the noiseless analysis of Recht
(2011) to the noisy matrix completion setting. As suggested in Gross and Nesme (2010),
we will obtain strengthened results, even in the noiseless case, by reasoning directly about
the without-replacement sampling model, rather than appealing to a with-replacement sur-
rogate, as done in Recht (2011).
: X ∈ Rr×n , Y ∈
the compact SVD of L0 , we let T = {UL0 X + YV(cid:62)
For UL0 ΣL0 V(cid:62)
Rm×r }, PT denote orthogonal pro jection onto the space T , and PT ⊥ represent orthogonal
L0
L0
pro jection onto the orthogonal complement of T . We further deﬁne I as the identity
operator on Rm×n and the spectral norm of an operator A : Rm×n → Rm×n as (cid:107)A(cid:107)2 =
sup(cid:107)X(cid:107)F ≤1 (cid:107)A(X)(cid:107)F .
We begin with a theorem providing suﬃcient conditions for our desired estimation guar-
antee.
(cid:13)(cid:13)(cid:13)PT PΩPT − s
(cid:13)(cid:13)(cid:13)2
Theorem 29 Under the assumptions of Theorem 10, suppose that
≤ 1
PT
mn
2
s
mn
(cid:114) s
and that there exists a Y = PΩ (Y) ∈ Rm×n satisfying
(cid:107)PT (Y) − UL0 V(cid:62)
L0 (cid:107)F ≤
(cid:114)
32mn
2m2n
∆ ≤ c(cid:48)(cid:48)
(cid:107)L0 − ˆL(cid:107)F ≤ 8
1
+ m +
mn∆.
e
16
s
Proof We may write ˆL as L0 + G + H, where PΩ (G) = G and PΩ (H) = 0. Then, under
ΩPT (H)(cid:11) ≥ (cid:104)H, PT PΩPT (H)(cid:105) ≥ s
F = (cid:10)H, PT P 2
Eq. (9),
(cid:107)PT (H)(cid:107)2
(cid:107)PΩPT (H)(cid:107)2
F .
2mn
Furthermore, by the triangle inequality, 0 = (cid:107)PΩ (H)(cid:107)F ≥ (cid:107)PΩPT (H)(cid:107)F − (cid:107)PΩPT ⊥ (H)(cid:107)F .
(cid:114) s
Hence, we have
2mn

(cid:107)PT (H)(cid:107)F ≤ (cid:107)PΩPT (H)(cid:107)F ≤ (cid:107)PΩPT ⊥ (H)(cid:107)F ≤ (cid:107)PT ⊥ (H)(cid:107)F ≤ (cid:107)PT ⊥ (H)(cid:107)∗ ,

(cid:107)PT ⊥ (Y)(cid:107)2 <

Then,

(9)

(10)

(11)

and

1
2

.

√

949

Mackey, Talwalkar and Jordan

where the penultimate inequality follows as PΩ is an orthogonal pro jection operator.
(cid:10)U⊥V(cid:62)
⊥ , PT ⊥ (H)(cid:11) = (cid:107)PT ⊥ (H)(cid:107)∗ and note that
Next we select U⊥ and V⊥ such that [UL0 , U⊥ ] and [VL0 , V⊥ ] are orthonormal and
(cid:107)L0 + H(cid:107)∗ ≥ (cid:68)
(cid:69)
(cid:69)
(cid:68)
L0 + U⊥V(cid:62)
UL0 V(cid:62)
⊥ , L0 + H
(cid:68)
(cid:69)
(cid:69) − (cid:104)PT ⊥ (Y), PT ⊥ (H)(cid:105)
(cid:68)
⊥ − Y , H
= (cid:107)L0(cid:107)∗ +
UL0 V(cid:62)
L0 + U⊥V(cid:62)
⊥ , PT ⊥ (H)
L0 − PT (Y), PT (H)
= (cid:107)L0(cid:107)∗ +
UL0 V(cid:62)
U⊥V(cid:62)
+
(cid:114) s
L0 − PT (Y)(cid:107)F (cid:107)PT (H)(cid:107)F + (cid:107)PT ⊥ (H)(cid:107)∗ − (cid:107)PT ⊥ (Y)(cid:107)2(cid:107)PT ⊥ (H)(cid:107)∗
≥ (cid:107)L0(cid:107)∗ − (cid:107)UL0 V(cid:62)
> (cid:107)L0(cid:107)∗ +
(cid:107)PT ⊥ (H)(cid:107)∗ −
(cid:107)PT (H)(cid:107)F
1
2
32mn
(cid:107)PT ⊥ (H)(cid:107)F
≥ (cid:107)L0(cid:107)∗ +
1
4

where the ﬁrst inequality follows from the variational representation of the trace norm,
(cid:107)A(cid:107)∗ = sup(cid:107)B(cid:107)2≤1 (cid:104)A, B(cid:105), the ﬁrst equality follows from the fact that (cid:104)Y , H(cid:105) = 0 for
Y = PΩ (Y), the second inequality follows from H¨older’s inequality for Schatten p-norms,
the third inequality follows from Eq. (10), and the ﬁnal inequality follows from Eq. (11).
Since L0 is feasible for Eq. (1), (cid:107)L0(cid:107)∗ ≥ (cid:107) ˆL(cid:107)∗ , and, by the triangle inequality, (cid:107) ˆL(cid:107)∗ ≥
(cid:107)L0 + H(cid:107)∗−(cid:107)G(cid:107)∗ . Since (cid:107)G(cid:107)∗ ≤ √
m(cid:107)G(cid:107)F and (cid:107)G(cid:107)F ≤ (cid:107)PΩ ( ˆL − M)(cid:107)F +(cid:107)PΩ (M − L0 )(cid:107)F ≤
2∆, we conclude that
(cid:18) 2mn
(cid:19)
F + (cid:107)PT ⊥ (H)(cid:107)2
F = (cid:107)PT (H)(cid:107)2
F + (cid:107)G(cid:107)2
(cid:107)L0 − ˆL(cid:107)2
F
(cid:19)
(cid:18) 2mn
≤
F + (cid:107)G(cid:107)2
(cid:107)PT ⊥ (H)(cid:107)2
+ 1
F
s
(cid:19)
(cid:18) 2m2n
(cid:107)G(cid:107)2∗ + (cid:107)G(cid:107)2
≤ 16
+ 1
F
s
≤ 64
1
(cid:114)
16
s

+ m +

Hence

∆2 .

√

mn∆

2m2n
(cid:107)L0 − ˆL(cid:107)F ≤ 8
s
for some constant c(cid:48)(cid:48)
e , by our assumption on s.

+ m +

∆ ≤ c(cid:48)(cid:48)
e

1
16

To show that the suﬃcient conditions of Theorem 29 hold with high probability, we will
require four lemmas. The ﬁrst establishes that the operator PT PΩPT is nearly an isometry
on T when suﬃciently many entries are sampled.
(cid:114)
(cid:13)(cid:13)(cid:13)PT PΩPT − s
(cid:13)(cid:13)(cid:13)2
Lemma 30 For al l β > 1,
PT
≤
mn
s
mn
with probability at least 1 − 2n2−2β provided that s > 16
3 µr(n + m)β log(n).

16µr(m + n)β log(n)
3s

950

Distributed Matrix Completion and Robust Factorization

The second states that a sparsely but uniformly observed matrix is close to a multiple
of the original matrix under the spectral norm.
(cid:114)
(cid:13)(cid:13)(cid:13)(cid:16) mn
(cid:13)(cid:13)(cid:13)2
PΩ − I (cid:17)
Lemma 31 Let Z be a ﬁxed matrix in Rm×n . Then for al l β > 1,
8βmn2 log(m + n)
(cid:107)Z(cid:107)∞
≤
(Z)
3s
s
with probability at least 1 − (m + n)1−β provided that s > 6βm log(m + n).
The third asserts that the matrix inﬁnity norm of a matrix in T does not increase under
the operator PT PΩ .
(cid:114)
(cid:13)(cid:13)(cid:13) mn
(cid:13)(cid:13)(cid:13)∞
Lemma 32 Let Z ∈ T be a ﬁxed matrix. Then for al l β > 2
≤
PT PΩ (Z) − Z
s
with probability at least 1 − 2n2−β provided that s > 8
3 βµr(m + n) log(n).
These three lemmas were proved in Recht (2011, Theorem 6, Theorem 7, and Lemma 8)
under the assumption that entry locations in Ω were sampled with replacement. They
admit identical proofs under the sampling without replacement model by noting that the
referenced Noncommutative Bernstein Inequality (Recht, 2011, Theorem 4) also holds under
sampling without replacement, as shown in Gross and Nesme (2010).
Lemma 30 guarantees that Eq. (9) holds with high probability. To construct a matrix
Y = PΩ (Y) satisfying Eq. (10), we consider a sampling with batch replacement scheme rec-
ommended in Gross and Nesme (2010) and developed in Chen et al. (2011). Let ˜Ω1 , . . . , ˜Ωp
be independent sets, each consisting of q random entry locations sampled without replace-
ment, where pq = s. Let ˜Ω = ∪p
˜Ωi , and note that there exist p and q satisfying
i=1
and p ≥ 3
q ≥ 128
3
4

8βµr(m + n) log(n)
3s

µr(m + n)β log(m + n)

(cid:107)Z(cid:107)∞

log(n/2).

It suﬃces to establish Eq. (10) under this batch replacement scheme, as shown in the next
lemma.
Lemma 33 For any location set Ω0 ⊂ {1, . . . , m} × {1, . . . , n}, let A(Ω0 ) be the event that
there exists Y = PΩ0 (Y) ∈ Rm×n satisfying Eq. (10). If Ω(s) consists of s locations sampled
uniformly without replacement and ˜Ω(s) is sampled via batch replacement with p batches of
size q for pq = s, then P(A( ˜Ω(s))) ≤ P(A(Ω(s))).
(cid:17)
(cid:16)
s(cid:88)
Proof As sketched in Gross and Nesme (2010)
A( ˜Ω(s))
=
≤ s(cid:88)
i=1
≤ s(cid:88)
i=1
i=1

P(| ˜Ω| = i)P(A(Ω(s))) = P(A(Ω(s))),

P(| ˜Ω| = i)P(A( ˜Ω(i)) | | ˜Ω| = i)

P(| ˜Ω| = i)P(A(Ω(i)))

P

951

Mackey, Talwalkar and Jordan

≤ 1
2

=

(12)

(cid:13)(cid:13)(cid:13)(cid:13)F

≤ 1
2

(cid:107)Wk−1(cid:107)F

PT P ˜Ωk
PT )(Wk−1 )
(cid:112)32rmn/s ,

since the probability of existence never decreases with more entries sampled without replace-
ment and, given the size of ˜Ω, the locations of ˜Ω are conditionally distributed uniformly
(without replacement).
(cid:80)k
We now follow the construction of Recht (2011) to obtain Y = P ˜Ω (Y) satisfying
−
j=1 P ˜Ωj
Eq. (10). Let W0 = UL0 V(cid:62)
(Wj−1 ) and Wk = UL0 V(cid:62)
and deﬁne Yk = mn
L0
L0
q
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)PT P ˜Ωk
PT (Yk ) for k = 1, . . . , p. Assume that
PT − q
PT
mn
q
mn
(cid:13)(cid:13)(cid:13)(cid:13)(PT − mn
(cid:13)(cid:13)(cid:13)(cid:13)F
(cid:13)(cid:13)(cid:13)(cid:13)Wk−1 − mn
for all k . Then
PT P ˜Ωk
(cid:107)Wk (cid:107)F =
(Wk−1 )
q
q
and hence (cid:107)Wk (cid:107)F ≤ 2−k (cid:107)W0(cid:107)F = 2−k√
r . Since
log2 (n/2) ≥ log2
log(n/2) ≥ 1
p ≥ 3
4
2
Y (cid:44) Yp satisﬁes the ﬁrst condition of Eq. (10).
(cid:13)(cid:13)(cid:13)(cid:13)∞
(cid:13)(cid:13)(cid:13)(cid:13)Wk−1 − mn
The second condition of Eq. (10) follows from the assumptions
(cid:115)
≤ 1
(cid:107)Wk−1(cid:107)∞
PT P ˜Ωk
(cid:13)(cid:13)(cid:13)(cid:13)(cid:18) mn
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:19)
(Wk−1 )
2
q
8mn2β log(m + n)
P ˜Ωk
(Wk−1 )
q
3q
for all k , since Eq. (13) implies (cid:107)Wk (cid:107)∞ ≤ 2−k (cid:107)UL0 V(cid:62)
(cid:107)∞ , and thus
(cid:13)(cid:13)(cid:13)(cid:13)PT ⊥ (
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13) mn
p(cid:88)
(cid:107)PT ⊥ (Yp )(cid:107)2 ≤ p(cid:88)
L0
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(
=
≤ p(cid:88)
q
j=1
j=1
− I )(Wj−1 )
mn
(cid:115)
≤ p(cid:88)
q
j=1
(cid:107)Wj−1(cid:107)∞
p(cid:88)
j=1
j=1

(cid:13)(cid:13)(cid:13)(cid:13)2
(Wj−1 ) − Wj−1 )

8mn2β log(m + n)
(cid:115)
3q

32µrnβ log(m + n)
3q

PT ⊥ P ˜Ωj

(Wj−1 )

(cid:107)Wk−1(cid:107)∞

2−j

8mn2β log(m + n)
3q

(cid:107)UW V(cid:62)
W (cid:107)∞ <

(13)

(14)

− I

≤

mn
q

P ˜Ωj

= 2

< 1/2

P ˜Ωj

(cid:115)

by our assumption on q . The ﬁrst line applies the triangle inequality; the second holds since
Wj−1 ∈ T for each j ; the third follows because PT ⊥ is an orthogonal pro jection; and the
ﬁnal line exploits (µ, r)-coherence.

952

Distributed Matrix Completion and Robust Factorization

We conclude by bounding the probability of any assumed event failing. Lemma 30
implies that Eq. (9) fails to hold with probability at most 2n2−2β . For each k , Eq. (12)
fails to hold with probability at most 2n2−2β by Lemma 30, Eq. (13) fails to hold with
probability at most 2n2−2β by Lemma 32, and Eq. (14) fails to hold with probability at
most (m + n)1−2β by Lemma 31. Hence, by the union bound, the conclusion of Theorem 29
holds with probability at least
1 − 2n2−2β − 3
log(n/2)(4n2−2β + (m + n)1−2β ) ≥ 1 − 15
4
4

log(n)n2−2β ≥ 1 − 4 log(n)n2−2β .

= Tr

= Tr

1
n

P

By assumption,

Appendix K. Proof of Lemma 17: Conservation of Non-Spikiness
l(cid:88)
LC L(cid:62)
L(ja ) (L(ja ) )(cid:62)
C =
a=1
where {j1 , . . . , jl } are random indices drawn uniformly and without replacement from {1, . . . , n}.
(cid:34)
(cid:34) l(cid:88)
(cid:35)(cid:35)
(cid:105)(cid:105)
(cid:104)
(cid:104)
(cid:105)
(cid:104)(cid:107)LC (cid:107)2
Hence, we have that
LC L(cid:62)
L(ja ) (L(ja ) )(cid:62)
 l(cid:88)
 =
E
Tr
= E
E
LL(cid:62)(cid:105)
(cid:104)
n(cid:88)
C
F
a=1
L(j ) (L(j ) )(cid:62)
l
Tr
n
a=1
j=1
Since (cid:107)L(j )(cid:107)4 ≤ m2(cid:107)L(cid:107)4∞ for all j ∈ {1, . . . , n}, Hoeﬀding’s inequality for sampling
(cid:16)−22(cid:107)L(cid:107)4
(cid:16)
(cid:17) ≤ exp
(cid:17)
without replacement (Hoeﬀding, 1963, Section 6) implies
= exp(cid:0)−22 l/α4 (L)(cid:1) ≤ δ,
F l2/(n2 lm2(cid:107)L(cid:107)4∞ )
F ≥ (cid:107)LC (cid:107)2
(1 − )(l/n)(cid:107)L(cid:107)2
F
√
n√
≤
1
1
(cid:107)L(cid:107)F
(cid:107)LC (cid:107)F
l
1 − 
with probability at least 1 − δ . Since, (cid:107)LC (cid:107)∞ ≤ (cid:107)L(cid:107)∞ almost surely, we have that
√
√
ml(cid:107)LC (cid:107)∞
mn(cid:107)L(cid:107)∞
α(L)√
√
≤
(cid:107)LC (cid:107)F
1 − (cid:107)L(cid:107)F
1 − 
with probability at least 1 − δ as desired.

by our choice of l. Hence,

α(LC ) =

=

(cid:107)L(cid:107)2
F .

l
n

√

=

Appendix L. Proof of Theorem 18: Column Pro jection under
Non-Spikiness

We now give a proof of Theorem 18. While the results of this section are stated in terms of
i.i.d. with-replacement sampling of columns and rows, a simple argument due to (Hoeﬀding,

953

Mackey, Talwalkar and Jordan

1963, Section 6) implies the same conclusions when columns and rows are sampled without
replacement.
Our proof builds upon two key results from the randomized matrix approximation lit-
erature. The ﬁrst relates column pro jection to randomized matrix multiplication:
Theorem 34 (Theorem 2 of Drineas et al. 2006b) Let G ∈ Rm×l be a matrix of l
columns of A ∈ Rm×n , and let r be a nonnegative integer. Then,
√
r(cid:107)AA(cid:62) − (n/l)GG(cid:62)(cid:107)F .
(cid:107)A − GrG+
r A(cid:107)F ≤ (cid:107)A − Ar (cid:107)F +
The second allows us to bound (cid:107)AA(cid:62) − (n/l)GG(cid:62)(cid:107)F in probability when entries are
bounded:
Lemma 35 (Lemma 2 of Drineas et al. 2006a) Given a failure probability δ ∈ (0, 1]
and matrices A ∈ Rm×k and B ∈ Rk×n with (cid:107)A(cid:107)∞ ≤ b and (cid:107)B(cid:107)∞ ≤ b, suppose that G is
a matrix of l columns drawn uniformly with replacement from A and that H is a matrix of
the corresponding l rows of B. Then, with probability at least 1 − δ ,
(cid:112)8 log(2mn/δ) ∀i, j.
|(AB)ij − (n/l)(GH)ij | ≤ kb2√
l
√
Under our assumption, (cid:107)M(cid:107)∞ is bounded by α/
mn. Hence, Lemma 35 with A = M
and B = M(cid:62) guarantees

F ≤ m2n2α48 log(2mn/δ)
(cid:107)MM(cid:62) − (n/l)CC(cid:62)(cid:107)2
m2n2 l
with probability at least 1 − δ , by our choice of l.
Now, Theorem 34 implies that
r M(cid:107)F ≤ (cid:107)M − Mr (cid:107)F +
(cid:107)M − CC+M(cid:107)F ≤ (cid:107)M − CrC+
≤ (cid:107)M − L(cid:107)F + 

√

≤ 2/r

r(cid:107)MM(cid:62) − (n/l)CC(cid:62)(cid:107)F

with probability at least 1 − δ , as desired.
r))-spiky. Since (cid:112)1 + /(4
Deﬁne A(X) as the event that a matrix X is (α(cid:112)1 + /(4
Appendix M. Proof of Theorem 20: Spikiness Master Theorem
√
√
r) ≤
√
√
1.25 for all  ∈ (0, 1] and r ≥ 1, X is (
1.25α)-spiky whenever A(X) holds.
Let L0 = [C0,1 , . . . , C0,t ] and ˜L = [ ˆC1 , . . . , ˆCt ], and deﬁne H as the event (cid:107) ˜L − ˆLproj (cid:107)F ≤
(cid:107)L0 − ˜L(cid:107)F + . When H holds, we have that
(cid:113)(cid:80)t
(cid:107)L0 − ˆLproj (cid:107)F ≤ (cid:107)L0 − ˜L(cid:107)F + (cid:107) ˜L − ˆLproj (cid:107)F ≤ 2(cid:107)L0 − ˜L(cid:107)F + 
by the triangle inequality, and hence it suﬃces to lower bound P(H ∩ (cid:84)
i=1(cid:107)C0,i − ˆCi(cid:107)2
= 2
F + ,
By assumption,
l ≥ 13rα4 log(4mn/δ)/2 ≥ α4 log(2n/δ)/(2˜2 )

iA(C0,i )).

954

Distributed Matrix Completion and Robust Factorization

it follows that

r) ≥ 1

1 − ˜ with

√
√
r). Hence, for each i, Lemma 17 implies that α(C0,i ) ≤ α/
where ˜ (cid:44) /(5
probability at least 1 − δ/(2n). Since
√
√
√
√
r)) = 1 + (1 − /
(1 − /(5
≤ (cid:113)
r)/(20
r))(1 + /(4
1(cid:112)1 − /(5
√
1√
√
1 + /(4
=
r),
1 − ˜
r)
so that each event A(C0,i ) also holds with probability at least 1 − δ/(2n).
mn for all i implies that (cid:107) ˜L(cid:107)∞ ≤ √
Our assumption that (cid:107) ˆCi(cid:107)∞ ≤ √
√
√
mn.
1.25α/
1.25α/
Our choice of l, with a factor of log(4mn/δ), therefore implies that H holds with probability
P(H ∩ (cid:84)
iA(C0,i )) ≥ 1 − P(H c ) − (cid:80)
at least 1 − δ/2 by Theorem 18. Hence, by the union bound,
iP(A(C0,i )c ) ≥ 1 − δ/2 − tδ/(2n) ≥ 1 − δ.
To establish the DFC-RP bound, redeﬁne H as the event (cid:107) ˜L − Lrp(cid:107)F ≤ (2+)(cid:107)L0 − ˜L(cid:107)F .
Since p ≥ 242 r log(14/δ)/2 , H holds with probability at least 1 − δ/2 by Corollary 9, and
the DFC-RP bound follows as above.

H be the event that

Appendix N. Proof of Corollary 22: Noisy MC under Non-Spikiness
N.1 Proof of DFC-Pro j Bound
(cid:112)
We begin by proving the DFC-Proj bound. Let G be the event that
(cid:107)L0 − ˆLproj (cid:107)F ≤ 2
c1 max((l/n)ν 2 , 1)/β + ,
(cid:113)(cid:80)t
i=1(cid:107)C0,i − ˆCi(cid:107)2
(cid:107)L0 − ˆLproj (cid:107)F ≤ 2
F + ,
F > (l/n)c1 max(cid:0)(l/n)ν 2 , 1(cid:1)/β .
√
1.25α)-spiky, and, for each i ∈ {1, . . . , t}, Bi be
A(X) be the event that a matrix X is (
the event that (cid:107)C0,i − ˆCi(cid:107)2
√
By deﬁnition, (cid:107) ˆCi(cid:107)∞ ≤ √
(cid:114)
1.25α/
ml for all i. Furthermore, we have assumed that
l ≥ 13(c3 + 1)
(m + n) log(m + n)β
nrα4 log(4mn)/2
s
≥ 13rα4 (log(4mn) + c3 log(m + n))/2 ≥ 13rα4 log(4mn(m + l)c3 )/2 .
Hence the Spikiness Master Theorem (Theorem 20) guarantees that, with probability at
least 1 − exp(−c3 log(m + l)), H holds and the event A(C0,i ) holds for each i. Since G holds
P(G) ≥ P(H ∩ (cid:84)
i ) ≥ P(H ∩ (cid:84)
iA(C0,i ) ∩ (cid:84)
whenever H holds and B c
i holds for each i, we have
= P(H ∩ (cid:84)
iA(C0,i ))P((cid:84)
i | H ∩ (cid:84)
iB c
iB c
= P(H ∩ (cid:84)
iA(C0,i ))(1 − P((cid:83)
iBi | H ∩ (cid:84)
i )
iB c
≥ (1 − exp(−c3 log(m + l)))(1 − (cid:80)
iA(C0,i ))
≥ 1 − (c2 + 1) exp(−c3 log(m + l)) − (cid:80)
iA(C0,i )))
iP(Bi | A(C0,i )))
iP(Bi | A(C0,i )).
955

Mackey, Talwalkar and Jordan

To prove our desired claim, it therefore suﬃces to show
P(Bi | A(C0,i )) ≤ (c2 + 1) exp(−c3 log(m + l))

for each i.
For each i, let Di be the event that si < 1.25α2β (n/l)r(m + l) log(m + l), where si is
the number of revealed entries in C0,i . Since rank(C0,i ) ≤ rank(L0 ) = r and (cid:107)C0,i(cid:107)F ≤
(cid:107)L0(cid:107)F ≤ 1, Corollary 19 implies that
P(Bi | A(C0,i )) ≤ P(Bi | A(C0,i ), Dc
i ) + P(Di | A(C0,i ))
≤ c2 exp(−c3 log(m + l)) + P(Di ).

(15)

Further, since the support of S0 is uniformly distributed and of cardinality s, the vari-
able si has a hypergeometric distribution with E(si ) = sl
n and hence satisﬁes Hoeﬀding’s
P(si ≤ E(si ) − st) ≤ exp(cid:0)−2st2(cid:1).
inequality for the hypergeometric distribution (Hoeﬀding, 1963, Section 6):
Our assumption on l implies that
r(m + l) log(m + l) + (cid:112)c3 log(m + l)/(2s),
≥ 169(c3 + 1)2α8β
n
r2 (m + n) log(m + n) log2 (4mn)/4
ls
≥ 1.25α2β
n
ls
(cid:18) l
(cid:18)
(cid:19)(cid:19)
(cid:16)
(cid:17)
si < E(si ) − s(cid:112)c3 log(m + l)/(2s)
− 1.25α2β
si < E(si ) − s
n
r(m + l) log(m + l)
ls
n
= P
≤ exp(−2sc3 log(m + l)/(2s)) = exp(−c3 log(m + l)).
Combined with Eq. (15), this yields P(Bi | A(C0,i )) ≤ (c2 + 1) exp(−c3 log(m + l)) for each
i, and the DFC-Proj result follows.

and therefore

l
n

P(Di ) = P

N.2 Proof of DFC-RP Bound

Let G be the event that
(cid:107)L0 − ˆLrp(cid:107)F ≤ (2 + )

(cid:112)
c1 max((l/n)ν 2 , 1)/β
(cid:113)(cid:80)t
and H be the event that
i=1(cid:107)C0,i − ˆCi(cid:107)2
(cid:107)L0 − ˆLrp(cid:107)F ≤ (2 + )
F .
Since p ≥ 242 r log(14(m + l)c3 )/2 , the DFC-RP bound follows in an identical manner
from the Spikiness Master Theorem (Theorem 20).

956

Distributed Matrix Completion and Robust Factorization

References

A. Agarwal, S. Negahban, and M. J. Wainwright. Noisy matrix decomposition via convex
relaxation: Optimal rates in high dimensions. In International Conference on Machine
Learning, 2011.

E. J. Cand`es, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? Journal
of the ACM, 58(3):1–37, 2011.

E.J. Cand`es and Y. Plan. Matrix completion with noise. Proceedings of the IEEE, 98(6):
925 –936, 2010.

V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, and A. S. Willsky. Sparse and low-rank
matrix decompositions. In Al lerton Conference on Communication, Control, and Com-
puting, 2009.

Y. Chen, H. Xu, C. Caramanis, and S. Sanghavi. Robust matrix completion and corrupted
columns. In International Conference on Machine Learning, 2011.

A. Das, M. Datar, A. Garg, and S. Ra jaram. Google news personalization: scalable online
collaborative ﬁltering. In WWW, 2007.

P. Drineas, R. Kannan, and M. W. Mahoney. Fast monte carlo algorithms for matrices
ii: Computing a low-rank approximation to a matrix. SIAM J. Comput., 36(1):158–183,
2006a.

P. Drineas, R. Kannan, and M. W. Mahoney. Fast monte carlo algorithms for matrices i:
Approximating matrix multiplication. SIAM J. Comput., 36(1):132–157, 2006b.

P. Drineas, M. W. Mahoney, and S. Muthukrishnan. Relative-error CUR matrix decompo-
sitions. SIAM Journal on Matrix Analysis and Applications, 30:844–881, 2008.

B. Recht F. Niu, C. R´e, and S. J. Wright. Hogwild!: A lock-free approach to parallelizing
stochastic gradient descent. In NIPS, 2011.

A. Frieze, R. Kannan, and S. Vempala. Fast Monte-Carlo algorithms for ﬁnding low-rank
approximations. In Foundations of Computer Science, 1998.

R. Gemulla, E. Nijkamp, P. J. Haas, and Y. Sismanis. Large-scale matrix factorization with
distributed stochastic gradient descent. In KDD, 2011.

S. A. Goreinov, E. E. Tyrtyshnikov, and N. L. Zamarashkin. A theory of pseudoskeleton
approximations. Linear Algebra and its Applications, 261(1-3):1 – 21, 1997.

D. Gross and V. Nesme. Note on sampling without replacing from a ﬁnite collection of
matrices. CoRR, abs/1001.2738, 2010.

N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Prob-
abilistic algorithms for constructing approximate matrix decompositions. SIAM Review,
53(2):217–288, 2011.

957

Mackey, Talwalkar and Jordan

W. Hoeﬀding. Probability inequalities for sums of bounded random variables. Journal of
the American Statistical Association, 58(301):13–30, 1963.

P. D. Hoﬀ. Bilinear mixed-eﬀects models for dyadic data. Journal of the American Statistical
Association, 100:286–295, March 2005.

D. Hsu. http://www.cs.columbia.edu/~djhsu/papers/randmatrix-errata.txt, 2012.

D. Hsu, S. Kakade, and T. Zhang. Tail inequalities for sums of random matrices that depend
on the intrinsic dimension. Electron. Commun. Probab., 17:no. 14, 1–13, 2012.

W. B. Johnson and J. Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space.
Contemporary Mathematics, 26:189–206, 1984.

R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. Journal
of Machine Learning Research, 99:2057–2078, 2010.

Y. Koren, R. M. Bell, and C. Volinsky. Matrix factorization techniques for recommender
systems. IEEE Computer, 42(8):30–37, 2009.

S. Kumar, M. Mohri, and A. Talwalkar. On sampling-based approximate spectral decom-
position. In International Conference on Machine Learning, 2009a.

S. Kumar, M. Mohri, and A. Talwalkar. Ensemble Nystr¨om method. In Advances in Neural
Information Processing Systems, 2009b.

E. Liberty. Accelerated Dense Random Projections. Ph.D. thesis, computer science depart-
ment, Yale University, New Haven, CT, 2009.

Z. Lin, M. Chen, L. Wu, and Y. Ma. The augmented lagrange multiplier method for exact
recovery of corrupted low-rank matrices. UIUC Technical Report UILU-ENG-09-2215,
2009a.

Z. Lin, A. Ganesh, J. Wright, L. Wu, M. Chen, and Y. Ma. Fast convex optimization
algorithms for exact recovery of a corrupted low-rank matrix. UIUC Technical Report
UILU-ENG-09-2214, 2009b.

S. Ma, D. Goldfarb, and L. Chen. Fixed point and bregman iterative methods for matrix
rank minimization. Mathematical Programming, 128(1-2):321–353, 2011.

L. Mackey, A. Talwalkar, and M. I. Jordan. Divide-and-conquer matrix factorization. In
J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. C. N. Pereira, and K. Q. Weinberger,
editors, Advances in Neural Information Processing Systems 24, pages 1134–1142. 2011.

M. W. Mahoney and P. Drineas. Cur matrix decompositions for improved data analysis.
Proceedings of the National Academy of Sciences, 106(3):697–702, 2009.

K. Min, Z. Zhang, J. Wright, and Y. Ma. Decomposing background topics from keywords by
principal component pursuit. In Conference on Information and Know ledge Management,
2010.

958

Distributed Matrix Completion and Robust Factorization

M. Mohri and A. Talwalkar. Can matrix coherence be eﬃciently and accurately estimated?
In Conference on Artiﬁcial Intel ligence and Statistics, 2011.

Y. Mu, J. Dong, X. Yuan, and S. Yan. Accelerated low-rank visual recovery by random
pro jection. In Conference on Computer Vision and Pattern Recognition, 2011.

S. Negahban and M. J. Wainwright. Restricted strong convexity and weighted matrix
completion: Optimal bounds with noise. J. Mach. Learn. Res., 13:1665–1697, 2012.

E. J. Nystr¨om. ¨Uber die praktische auﬂ¨osung von integralgleichungen mit anwendungen
auf randwertaufgaben. Acta Mathematica, 54(1):185–204, 1930.

C. H. Papadimitriou, H. Tamaki, P. Raghavan, and S. Vempala. Latent Semantic Indexing:
a probabilistic analysis. In Principles of Database Systems, 1998.

Y. Peng, A. Ganesh, J. Wright, W. Xu, and Y. Ma. Rasl: Robust alignment by sparse
and low-rank decomposition for linearly correlated images. In Conference on Computer
Vision and Pattern Recognition, 2010.

B. Recht. Simpler approach to matrix completion. J. Mach. Learn. Res., 12:3413–3430,
2011.

B. Recht and C. R´e. Parallel stochastic gradient algorithms for large-scale matrix comple-
tion. In Optimization Online, 2011.

V. Rokhlin, A. Szlam, and M. Tygert. A randomized algorithm for Principal Component
Analysis. SIAM Journal on Matrix Analysis and Applications, 31(3):1100–1124, 2009.

A. Talwalkar and A. Rostamizadeh. Matrix coherence and the Nystr¨om method. In Pro-
ceedings of the Twenty-Sixth Conference on Uncertainty in Artiﬁcial Intel ligence, 2010.

K. Toh and S. Yun. An accelerated proximal gradient algorithm for nuclear norm regularized
least squares problems. Paciﬁc Journal of Optimization, 6(3):615–640, 2010.

M. Tygert.
http://www.mathworks.com/matlabcentral/ﬁleexchange/21524-principal-
component-analysis, 2009.

J. Wang, Y. Dong, X. Tong, Z. Lin, and B. Guo. Kernel Nystr¨om method for light transport.
ACM Transactions on Graphics, 28(3), 2009.

C.K. Williams and M. Seeger. Using the Nystr¨om method to speed up kernel machines. In
Advances in Neural Information Processing Systems, 2000.

H.-F. Yu, C.-J. Hsieh, S. Si, and I. Dhillon. Scalable coordinate descent approaches to
parallel matrix factorization for recommender systems. In ICDM, 2012.

Y. Zhou, D. Wilkinson, R. Schreiber, and R. Pan. Large-scale parallel collaborative ﬁltering
for the netﬂix prize. In Proceedings of the 4th international conference on Algorithmic
Aspects in Information and Management, 2008.

959

Mackey, Talwalkar and Jordan

Z. Zhou, X. Li, J. Wright, E. J. Cand`es, and Y. Ma. Stable principal component pursuit. In
IEEE International Symposium on Information Theory Proceedings (ISIT), pages 1518
–1522, 2010.

960

Journal of Machine Learning Research 16 (2015) 149-153

Submitted 6/14; Revised 9/14; Published 1/15

SAMOA: Scalable Advanced Massive Online Analysis

Gianmarco De Francisci Morales
Albert Bifet
Yahoo Labs
Av. Diagonal 177, 8th ﬂoor, 08018, Barcelona, Spain

Editor: Geoﬀ Holmes

gdfm@apache.org
abifet@yahoo.com

Abstract
samoa (Scalable Advanced Massive Online Analysis) is a platform for mining big
data streams.
It provides a collection of distributed streaming algorithms for the most
common data mining and machine learning tasks such as classiﬁcation, clustering, and
regression, as well as programming abstractions to develop new algorithms. It features a
pluggable architecture that allows it to run on several distributed stream processing engines
such as Storm, S4, and Samza. samoa is written in Java, is open source, and is available
at http://samoa-project.net under the Apache Software License version 2.0.
Keywords: data streams, distributed systems, classiﬁcation, clustering, regression, tool-
box, machine learning

1. Introduction
Big data is “data whose characteristics forces us to look beyond the traditional methods
that are prevalent at the time” (Jacobs, 2009). Currently, there are two main ways to
deal with big data: streaming algorithms and distributed computing (e.g., MapReduce).
samoa aims at satisfying the future needs for big data stream mining by combining the two
approaches in a single platform under an open source umbrella.
Data mining and machine learning are well established techniques among web companies
and startups. Spam detection, personalization, and recommendation are just a few of the
applications made possible by mining the huge quantity of data available nowadays.
The usual pipeline for mining and modeling data (what “data scientists” do) involves
taking a sample from production data, cleaning and preprocessing it to make it amenable to
modeling, training a model for the task at hand, and ﬁnally deploying it to production. The
ﬁnal output of this process is a pipeline that needs to run (and be maintained) periodically
in order to keep the model up to date.
In order to cope with web-scale data sets, data scientists have resorted to paral lel and
distributed computing. MapReduce (Dean and Ghemawat, 2004) is currently the de-facto
standard programming paradigm in this area, mostly thanks to the popularity of Hadoop,1
an open source implementation of MapReduce started at Yahoo. Hadoop and its ecosys-
tem (e.g., Mahout2 ) have proven to be an extremely successful platform to support the
aforementioned process at web scale.
1. See http://hadoop.apache.org
2. See http://mahout.apache.org

c(cid:13)2015 Gianmarco De Francisci Morales and Albert Bifet.

De Francisci Morales and Bifet

Figure 1: Taxonomy of data mining tools.

However, nowadays most data is generated in the form of a stream. Batch data is just a
snapshot of streaming data obtained in an interval of time. Researchers have conceptualized
and abstracted this setting in the streaming model.
In this model data arrives at high
speed, one instance at a time, and algorithms must process it in one pass under very strict
constraints of space and time. Streaming algorithms make use of probabilistic guarantees
to give fast approximated answers.
On the one hand, MapReduce is not suited to express streaming algorithms. On the
other hand, traditional sequential online algorithms are limited by the memory and band-
width of a single machine. Distributed stream processing engines (DSPEs) are a new emer-
gent family of MapReduce-inspired technologies that address this issue. These engines allow
to express parallel computation on streams, and combine the scalability of distributed pro-
cessing with the eﬃciency of streaming algorithms. Examples of these engines include
Storm,3 S4,4 and Samza.5
Alas, currently there is no common solution for mining big data streams, that is, for
executing data mining and machine learning algorithms on a distributed stream processing
engine. The goal of samoa is to ﬁll this gap, as exempliﬁed by Figure 1.

2. Description
samoa (Scalable Advanced Massive Online Analysis) is a platform for mining big
data streams (De Francisci Morales, 2013). For a simple analogy, think of samoa as Mahout
for streaming. As most of the rest of the big data ecosystem, it is written in Java.
samoa is both a framework and a library. As a framework, it allows algorithm developers
to abstract from the underlying execution engine, and therefore reuse their code on diﬀerent
engines. It features a pluggable architecture that allows it to run on several distributed
stream processing engines such as Storm, S4, and Samza. This capability is achieved by
3. See http://storm.apache.org
4. See http://incubator.apache.org/s4
5. See http://samza.incubator.apache.org

150

Data Mining Distributed Batch Hadoop Mahout Stream Storm, S4, Samza SAMOA Non Distributed Batch R, WEKA,… Stream MOA SAMOA: Scalable Advanced Massive Online Analysis

designing a minimal API that captures the essence of modern DSPEs. This API also allows
to easily write new bindings to port samoa to new execution engines. samoa takes care of
hiding the diﬀerences of the underlying DSPEs in terms of API and deployment.
As a library, samoa contains implementations of state-of-the-art algorithms for dis-
tributed machine learning on streams. For classiﬁcation, samoa provides the Vertical Ho-
eﬀding Tree (VHT), a distributed version of a streaming decision tree (Domingos and Hul-
ten, 2000). For clustering, it includes an algorithm based on CluStream (Aggarwal et al.,
2003). For regression, a decision rule learner (Thu Vu et al., 2014). The library also includes
meta-algorithms such as bagging and boosting.
The platform is intended to be useful in both research and real world deployments.

3. Architecture
An algorithm in samoa is represented by a directed graph of nodes that communicate via
messages along streams which connect pairs of nodes. Borrowing the terminology from
Storm, this graph is called a Topology. Each node in a Topology is a Processor that sends
messages through a Stream. A Processor is a container for the code implementing the
algorithm. A Stream can have a single source but several destinations (akin to a pub-sub
system). A Topology is built by using a TopologyBuilder, which connects the various pieces
of user code to the platform code and performs the necessary bookkeeping in the background.
The following is a code snippet to build a topology that joins two data streams in samoa.

T o p o l o g y B u i l d e r b u i l d e r = n e w T o p o l o g y B u i l d e r ( ) ;
P r o c e s s o r s o u r c e O n e = n e w S o u r c e P r o c e s s o r ( ) ;
b u i l d e r . a d d P r o c e s s o r ( s o u r c e O n e ) ;
S t r e a m s t r e a m O n e = b u i l d e r . c r e a t e S t r e a m ( s o u r c e O n e ) ;

P r o c e s s o r s o u r c e T w o = n e w S o u r c e P r o c e s s o r ( ) ;
b u i l d e r . a d d P r o c e s s o r ( s o u r c e T w o ) ;
S t r e a m s t r e a m T w o = b u i l d e r . c r e a t e S t r e a m ( s o u r c e T w o ) ;

P r o c e s s o r j o i n = n e w J o i n P r o c e s s o r ( ) ;
b u i l d e r . a d d P r o c e s s o r ( j o i n ) . c o n n e c t I n p u t S h u f f l e ( s t r e a m O n e )
. c o n n e c t I n p u t K e y ( s t r e a m T w o ) ;

4. Machine Learning Algorithms
The Vertical Hoeﬀding Tree (VHT) is a distributed extension of the VFDT (Domingos and
Hulten, 2000). The VHT uses vertical parallelism to split the workload across several ma-
chines. Vertical parallelism leverages the parallelism across attributes in the same example,
rather than across diﬀerent examples in the stream. In practice, each training example is
routed through the tree model to a leaf. There, the example is split into its constituting
attributes, and each attribute is sent to a diﬀerent Processor instance that keeps track of
suﬃcient statistics. This architecture has two main advantages over one based on horizontal
parallelism. First, attribute counters are not replicated across several machines, thus reduc-
ing the memory footprint. Second, the computation of the ﬁtness of an attribute for a split
decision (via, e.g., entropy or information gain) can be performed in parallel. The drawback

151

De Francisci Morales and Bifet

is that in order to get good performance, there must be suﬃcient inherent parallelism in
the data. That is, the VHT works best for sparse data (e.g, bag-of-words models).
samoa includes a distributed version of CluStream, an algorithm for clustering evolving
data streams. CluStream keeps a small summary of the data received so far by computing
micro-clusters online. These micro-clusters are further reﬁned to create macro-clusters by
a micro-batch process, which is triggered periodically. The period can be conﬁgured via a
command line parameter (e.g., every 10 000 examples).
samoa also includes adaptive implementations of ensemble methods such as bagging and
boosting. These methods include state-of-the-art change detectors such as as ADWIN (Bifet
and Gavaldà, 2007), DDM (Gama et al., 2004), EDDM (Baena-García et al., 2006), and
Page-Hinckley (Gama et al., 2014). These meta-algorithms are most useful in conjunction
with external single-machine classiﬁers which can be plugged in samoa. For instance,
connectors for moa (Bifet et al., 2010) are provided by the samoa-moa package.6
The following listing shows how to download, build and run samoa.

# d o w n l o a d a n d b u i l d S A M O A
g i t c l o n e g i t @ g i t h u b . c o m : y a h o o / s a m o a . g i t
c d s a m o a
m v n p a c k a g e

# d o w n l o a d t h e F o r e s t C o v e r T y p e d a t a s e t
w g e t " h t t p : / / d o w n l o a d s . s o u r c e f o r g e . n e t / p r o j e c t / m o a - d a t a s t r e a m / D a t a s e t s /
C l a s s i f i c a t i o n / c o v t y p e N o r m . a r f f . z i p "
u n z i p " c o v t y p e N o r m . a r f f . z i p "

# r u n S A M O A i n l o c a l m o d e
b i n / s a m o a l o c a l t a r g e t / S A M O A - L o c a l - 0 . 2 . 0 - S N A P S H O T . j a r " P r e q u e n t i a l E v a l u a t i o n
- l c l a s s i f i e r s . e n s e m b l e . B a g g i n g - s ( A r f f F i l e S t r e a m - f c o v t y p e N o r m . a r f f )
- f 1 0 0 0 0 0 "

5. Conclusions
samoa is a platform for mining big data streams. It supports the most common machine
learning tasks such as classiﬁcation, clustering, and regression. It also provides an API for
algorithm developers that simpliﬁes implementing distributed streaming algorithms.
samoa can be found at http://www.samoa-project.net/. The website includes a wiki,
an API reference, and a developer’s manual. Several examples of how the software can be
used are available. The code is hosted on GitHub. samoa contains a test suite that is run
on each commit on the GitHub repository via a continuous integration server.7 Finally,
samoa is released as open source software under the Apache Software License version 2.0.
We are grateful to all the people who contributed to samoa,8 without whom the project
could not have existed. We also thank Yahoo Labs Barcelona and its Web Mining group
for the great support during the development of the project.

6. See https://github.com/samoa- moa/samoa- moa
7. See https://travis- ci.org/yahoo/samoa
8. See http://samoa- project.net/contributors.html

152

SAMOA: Scalable Advanced Massive Online Analysis

References
Charu C. Aggarwal, Jiawei Han, Jianyong Wang, and Philip S. Yu. A framework for
clustering evolving data streams. In VLDB ’03: 29th International Conference on Very
Large Data Bases, pages 81–92, 2003.
Manuel Baena-García, José del Campo-Ávila, Raúl Fidalgo, Albert Bifet, Ricard Gavaldá,
and Rafael Morales-Bueno. Early drift detection method. In IWKDDS ’06: 4th Interna-
tional Workshop on Know ledge Discovery from Data Streams, 2006.
Albert Bifet and Ricard Gavaldà. Learning from time-changing data with adaptive win-
dowing. In SDM ’07: Seventh SIAM International Conference on Data Mining, pages
443–448, 2007.
Albert Bifet, Geoﬀ Holmes, Richard Kirkby, and Bernhard Pfahringer. MOA: Massive
Online Analysis. Journal of Machine Learning Research, 11:1601–1604, 2010.
Gianmarco De Francisci Morales. SAMOA: A platform for mining big data streams. In
RAMSS ’13: 2nd International Workshop on Real-Time Analysis and Mining of Social
Streams @WWW ’13, 2013.
Jeﬀrey Dean and Sanjay Ghemawat. Mapreduce: Simpliﬁed data processing on large clus-
ters. In OSDI ’04: 6th Symposium on Operating Systems Design and Implementation,
pages 137–150. USENIX Association, 2004.
Pedro Domingos and Geoﬀ Hulten. Mining high-speed data streams.
In KDD ’00: 6th
International Conference on Know ledge Discovery and Data Mining, pages 71–80, 2000.
João Gama, Pedro Medas, Gladys Castillo, and Pedro Pereira Rodrigues. Learning with
drift detection. In SBIA ’04: 17th Brazilian Symposium on Artiﬁcial Intel ligence, pages
286–295, 2004.
João Gama,
Indre Zliobaite, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid
Bouchachia. A survey on concept drift adaptation. ACM Computing Surveys, 46(4),
2014.
Adam Jacobs. The pathologies of big data. Communications of the ACM, 52(8):36–44,
August 2009.
Anh Thu Vu, Gianmarco De Francisci Morales, João Gama, and Albert Bifet. Distributed
adaptive model rules for mining big data streams. In BigData ’14: Second IEEE Inter-
national Conference on Big Data, 2014.

153

Journal of Machine Learning Research 15 (2014) 59-98

Submitted 3/12; Revised 5/13; Published 1/14

Fast SVM Training Using Approximate Extreme Points

Manu Nandan
Department of Computer and Information Science and Engineering
University of Florida
Gainesvil le, FL 32611, USA

Pramod P. Khargonekar
Department of Electrical and Computer Engineering
University of Florida
Gainesvil le, FL 32611, USA

Sachin S. Talathi
Qualcomm Research Center
5775 Morehouse Dr
San Diego, CA 92121, USA

Editor: Sathiya Keerthi

mnandan@ufl.edu

ppk@ece.ufl.edu

talathi@gmail.com

Abstract

Applications of non-linear kernel support vector machines (SVMs) to large data sets is
seriously hampered by its excessive training time. We propose a modiﬁcation, called the
approximate extreme points support vector machine (AESVM), that is aimed at overcoming
this burden. Our approach relies on conducting the SVM optimization over a carefully
selected subset, called the representative set, of the training data set. We present analytical
results that indicate the similarity of AESVM and SVM solutions. A linear time algorithm
based on convex hulls and extreme points is used to compute the representative set in
kernel space. Extensive computational experiments on nine data sets compared AESVM
to LIBSVM (Chang and Lin, 2011), CVM (Tsang et al., 2005) , BVM (Tsang et al.,
2007), LASVM (Bordes et al., 2005), SVMperf (Joachims and Yu, 2009), and the random
features method (Rahimi and Recht, 2007). Our AESVM implementation was found to
train much faster than the other methods, while its classiﬁcation accuracy was similar
to that of LIBSVM in all cases. In particular, for a seizure detection data set, AESVM
training was almost 500 times faster than LIBSVM and LASVM and 20 times faster than
CVM and BVM. Additionally, AESVM also gave competitively fast classiﬁcation times.

Keywords:
support vector machines, convex hulls, large scale classiﬁcation, non-linear
kernels, extreme points

1. Introduction

Several real world applications require solutions of classiﬁcation problems on large data sets.
Even though SVMs are known to give excellent classiﬁcation results, their application to
problems with large data sets is impeded by the burdensome training time requirements.
Recently, much progress has been made in the design of fast training algorithms (Fan et al.,
2008; Shalev-Shwartz et al., 2011) for SVMs with the linear kernel (linear SVMs). However,
many applications require SVMs with non-linear kernels for accurate classiﬁcation. Training

c(cid:13)2014 Manu Nandan, Pramod P. Khargonekar and Sachin S. Talathi.

Nandan, Khargonekar and Talathi

time complexity for SVMs with non-linear kernels is typically quadratic in the size of the
training data set (Shalev-Shwartz and Srebro, 2008). The diﬃculty of the long training
time is exacerbated when grid search with cross-validation is used to derive the optimal
hyper-parameters, since this requires multiple SVM training runs. Another problem that
sometimes restricts the applicability of SVMs is the long classiﬁcation time. The time
complexity of SVM classiﬁcation is linear in the number of support vectors and in some
applications the number of support vectors is found to be very large (Guo et al., 2005).
In this paper, we propose a new approach for fast SVM training. Consider a two class
data set of N data vectors, X = {xi : xi ∈ RD , i = 1, 2, ..., N }, and the corresponding target
labels Y = {yi : yi ∈ [−1, 1], i = 1, 2, ..., N }. The SVM primal problem can be represented
as the following unconstrained optimization problem (Teo et al., 2010; Shalev-Shwartz et al.,
N(cid:88)
2011):
(cid:107)w(cid:107)2 +
1
C
min
F1 (w, b) =
l(w, b, φ(xi )),
N
2
w,b
i=1
where l(w, b, φ(xi )) = max{0, 1 − yi (wT φ(xi ) + b)}, ∀xi ∈ X
and φ : RD → H, b ∈ R, and w ∈ H, a Hilbert space.

(1)

Here l(w, b, φ(xi )) is the hinge loss of xi . Note that SVM formulations where the penalty
parameter C is divided by N have been used extensively (Sch¨olkopf et al., 2000; Franc and
Sonnenburg, 2008; Joachims and Yu, 2009). These formulations enable better analysis of
the scaling of C with N (Joachims, 2006). The problem in (1) requires optimization over
N variables. In general, for SVM training algorithms, the training time will reduce if the
size of the training data set is reduced.
In this paper, we present an alternative to (1), cal led approximate extreme points support
vector machines (AESVM), that requires optimization over only a subset of the training data
M(cid:88)
set. The AESVM formulation is:
(cid:107)w(cid:107)2 +
1
C
min
F2 (w, b) =
βt l(w, b, φ(xt )),
N
2
w,b
t=1
where xt ∈ X∗ , w ∈ H, and b ∈ R.

(2)

Here M is the number of vectors in the selected subset of X, called the representative set
X∗ . The constants βt are deﬁned in (9). We will prove in Section 3.2 that:
√
• F1 (w∗
1 ) − F2 (w∗
2 ) ≤ C
2 , b∗
1 , b∗
2 , b∗
1 ) and (w∗
1 , b∗
C , where (w∗
2 ) are the solutions of (1)
and (2) respectively.
√
2 ) − F1 (w∗
• Under the assumptions given in corollary 4, F1 (w∗
1 ) ≤ 2C
1 , b∗
2 , b∗
C .
• The AESVM problem minimizes an upper bound of a low rank Gram matrix approx-
imation of the SVM ob jective function.

Based on these results we claim that solving the problem in (2) yields a solution close
to that of (1) for a small value of , the approximation error bound. As a by-product of the

60

Fast SVM Training Using Approximate Extreme Points

reduction in size of the training set, AESVM is also observed to result in fast classiﬁcation.
Considering that the representative set will have to be computed several times if grid search
is used to ﬁnd the optimum hyper-parameter combination, we also propose fast algorithms
to compute Z∗ .
In particular, we present an algorithm of time complexity O(N ) and
P ) to compute Z∗ , where P is a
N
an alternative algorithm of time complexity O(N log2
predeﬁned large integer.
Our main contribution is the new AESVM formulation that can be used for fast SVM
training. We develop and analyze our technique along the following lines:

• Theoretical: Theorems 1 and 2 and Corollaries 3 to 5 provide some theoretical basis
for the use of AESVM as a computationally less demanding alternative to the SVM
formulation.

• Algorithmic: The algorithm DeriveRS, described in Section 4, computes the represen-
tative set in linear time.

• Experimental: Our extensive experiments on nine data sets of varying characteristics
illustrate the suitability of applying AESVM to classiﬁcation on large data sets.

in Section 2, we brieﬂy discuss recent research on
This paper is organized as follows:
fast SVM training that is closely related to this work. Next, we provide the deﬁnition of
the representative set and discuss properties of AESVM. In Section 4, we present eﬃcient
algorithms to compute the representative set and analyze its computational complexity.
Section 5 describes the results of our computational experiments. We compared AESVM
to the widely used LIBSVM library, core vector machines (CVM), ball vector machines
(BVM), LASVM, SVMperf , and the random features method by Rahimi and Recht (2007).
Our experiments used eight publicly available data sets and a data set on EEG from an
animal model of epilepsy (Talathi et al., 2008; Nandan et al., 2010). We conclude with a
discussion of the results of this paper in Section 6.

2. Related Work

Several methods have been proposed to eﬃciently solve the SVM optimization problem.
SVMs require special algorithms, as standard optimization algorithms such as interior point
methods (Boyd and Vandenberghe, 2004; Shalev-Shwartz et al., 2011) have large memory
and training time requirements that make it infeasible for large data sets. In the following
sections we discuss the most widely used strategies to solve the SVM optimization problem.
We present a comparison of some of these methods to AESVM in Section 6. SVM solvers
can be broadly divided into two categories as described below.

2.1 Dual Optimization

The SVM primal problem is a convex optimization problem with strong duality (Boyd and
Vandenberghe, 2004). Hence its solution can be arrived at by solving its dual formulation

61

Nandan, Khargonekar and Talathi

given below:

max
α

L1 (α) =

N(cid:88)
i=1
sub ject to 0 ≤ αi ≤ C
N

αi − 1
2

N(cid:88)
i=1

and

N(cid:88)
αiαj yiyj K (xi , xj ),
N(cid:88)
j=1
αiyi = 0.
i=1

(3)

Here K (xi , xj ) = φ(xi )T φ(xj ), is the kernel product (Sch¨olkopf and Smola, 2001) of the
data vectors xi and xj , and α is a vector of all variables αi . Solving the dual problem is
computationally simpler, especially for non-linear kernels and a ma jority of the SVM solvers
use dual optimization. Some of the ma jor dual optimization algorithms are discussed below.
Decomposition methods (Osuna et al., 1997) have been widely used to solve (3). These
methods optimize over a subset of the training data set, called the ‘working set’, at each al-
gorithm iteration. SVMlight (Joachims, 1999) and SMO (Platt, 1999) are popular examples
of decomposition methods. Both these methods have a quadratic time complexity for linear
and non-linear SVM kernels (Shalev-Shwartz and Srebro, 2008). Heuristics such as shrink-
ing and caching (Joachims, 1999) enable fast convergence of decomposition methods and
reduce their memory requirements. LIBSVM (Chang and Lin, 2011) is a very popular im-
plementation of SMO. A dual coordinate descent (Hsieh et al., 2008) SVM solver computes
the optimal α value by modifying one variable αi per algorithm iteration. Dual coordinate
descent SVM solvers, such as LIBLINEAR (Fan et al., 2008), have been proposed primarily
for the linear kernel.
Approximations of the Gram matrix (Fine and Scheinberg, 2002; Drineas and Mahoney,
2005), have been proposed to increase training speed and reduce memory requirements of
SVM solvers. The Gram matrix is the N xN square matrix composed of the kernel products
K (xi , xj ), ∀xi , xj ∈ X. Training set selection methods attempt to reduce the SVM training
time by optimizing over a selected subset of the training set. Several distinct approaches
have been used to select the subset. Some methods use clustering based approaches (Pavlov
et al., 2000) to select the subsets. In Yu et al. (2003), hierarchical clustering is performed
to derive a data set that has more data vectors near the classiﬁcation boundary than away
from it. Minimum enclosing ball clustering is used in Cervantes et al. (2008) to remove data
vectors that are unlikely to contribute to the SVM training. Random sampling of training
data is another approach followed by approximate SVM solvers. Lee and Mangasarian
(2001) proposed reduced support vector machines (RSVM), in which only a random subset
of the training data set is used. Bordes et al. (2005) proposed the LASVM algorithm that
uses active selection techniques to train SVMs on a subset of the training data set.
A core set (Clarkson, 2010) can be loosely deﬁned as the subset of X for which the
solution of an optimization problem such as (3) has a solution similar to that for the entire
data set X. Tsang et al. (2005) proved that the L2-SVM is a reformulation of the minimum
enclosing ball problem for some kernels. They proposed core vector machine (CVM) that
approximately solves the L2-SVM formulation using core sets. A simpliﬁed version of CVM
called ball vector machine (BVM) was proposed in Tsang et al. (2007), where only an
enclosing ball is computed. G¨artner and Jaggi (2009) proposed an algorithm to solve the
L1-SVM problem, by computing the shortest distance between two polytopes (Bennett and

62

Fast SVM Training Using Approximate Extreme Points

Bredensteiner, 2000) using core sets. However, there are no published results on solving
L1-SVM with non-linear kernels using their algorithm.
Another method used to approximately solve the SVM problem is to map the data
vectors into a randomized feature space that is relatively low dimensional compared to the
kernel space H (Rahimi and Recht, 2007). Inner products of the pro jections of the data
vectors are approximations of their kernel product. This eﬀectively reduces the non-linear
SVM problem into the simpler linear SVM problem, enabling the use of fast linear SVM
solvers. This method is referred as RfeatSVM in the following sections of this document.

2.2 Primal Optimization

In recent years, linear SVMs have found increased use in applications with high-dimensional
data sets. This has led to a surge in publications on eﬃcient primal SVM solvers, which are
mostly used for linear SVMs. To overcome the diﬃculties caused by the non-diﬀerentiability
of the primal problem, the following methods are used.
Stochastic sub-gradient descent (Zhang, 2004) uses the sub-gradient computed at some
data vector xi to iteratively update w. Shalev-Shwartz et al. (2011) proposed a stochastic
sub-gradient descent SVM solver, Pegasos, that is reported to be among the fastest linear
SVM solvers. Cutting plane algorithms (Kelley, 1960) solve the primal problem by succes-
sively tightening a piecewise linear approximation. It was employed by Joachims (2006)
to solve linear SVMs with their implementation SVMperf . This work was generalized in
Joachims and Yu (2009) to include non-linear SVMs by approximately estimating w with
arbitrary basis vectors using the ﬁx-point iteration method (Sch¨olkopf and Smola, 2001).
Teo et al. (2010) proposed a related method for linear SVMs, that corrected some stability
issues in the cutting plane methods.

3. Analysis of AESVM

As mentioned in the introduction, AESVM is an optimization problem on a subset of the
training data set called the representative set. In this section we ﬁrst deﬁne the representa-
tive set. Then we present some properties of AESVM. These results are intended to provide
theoretical justiﬁcations for the use of AESVM as an approximation to the SVM problem
(1).

3.1 Deﬁnition of the Representative Set

The convex hull of a set X is the smallest convex set containing X (Rockafellar, 1996) and
can be obtained by taking all possible convex combinations of elements of X. Assuming X
is ﬁnite, the convex hull is a polygon. The extreme points of X, EP (X), are deﬁned to be
the vertices of the convex polygon formed by the convex hull of X. Any vector xi in X can
(cid:88)
(cid:88)
be represented as a convex combination of vectors in EP (X):
πi,txt , where 0 ≤ πi,t ≤ 1, and
xt∈EP (X)
xt∈EP (X)

πi,t = 1.

xi =

We can see that functions of any data vector in X can be computed using only EP (X)
and the convex combination weights {πi,t}. The design of AESVM is motivated by the

63

Nandan, Khargonekar and Talathi

intuition that the use of extreme points may provide computational eﬃciency. However,
extreme points are not useful in all cases, as for some kernels all data vectors are extreme
points in kernel space. For example, for the Gaussian kernel, K (xi , xi ) = φ(xi )T φ(xi ) = 1.
This implies that all the data vectors lie on the surface of the unit ball in the Gaussian kernel
space1 and therefore are extreme points. Hence, we introduce the concept of approximate
extreme points.
Consider the set of transformed data vectors:
Z = {zi : zi = φ(xi ), ∀xi ∈ X}.

(4)

Here, the explicit representation of vectors in kernel space is only for the ease of under-
standing and all the computations are performed using kernel products. Let V be a positive
integer that is much smaller than N and  be a small positive real number. For notational
simplicity, we assume N is divisible by V . Let Zl be subsets of Z for l = 1, 2, ..., ( N
V ), such
that Z = ∪
Zl and Zl ∩ Zm = ∅ for l (cid:54)= m, where m = 1, 2, ..., ( N
V ). We require that the
subsets Zl satisfy |Zl | = V , ∀l and
l
∀zi , zj ∈ Zl , we have yi = yj ,
(5)
where |Zl | denotes the cardinality of Zl . Let Zlq be an arbitrary subset of Zl , Zlq ⊆ Zl .
(cid:107)zi − (cid:88)
Next, for any zi ∈ Zl we deﬁne:
(cid:88)
f (zi , Zlq ) = min
µi
zt∈Zlq
s.t. 0 ≤ µi,t ≤ 1, and
zt∈Zlq

µi,tzt(cid:107)2 ,

µi,t = 1.

(6)

A subset Z ∗
l

is said to be an  - approximate extreme points subset of Zl if:
l ) ≤ .
f (zi , Z ∗
max
zi∈Zl
We will drop the preﬁx  for simplicity and refer to Z ∗
l as approximate extreme points subset.
Note that it is not unique. Intuitively, its cardinality will be related to computational savings
obtained using the approach proposed in this paper. We have chosen to not use approximate
extreme points subset of smallest cardinality to maintain ﬂexibility.
It can be seen that µi,t for zt ∈ Z∗
l are analogous to the convex combination weights
πi,t for xt ∈ EP (X). The representative set Z∗ of Z is the union of the sets of approximate
extreme points of its subsets Zl .

N
V∪
Z∗ =
Z∗
l .
l=1
The representative set has properties that are similar to EP (X). Given any zi ∈ Z, we
can ﬁnd Zl such that zi ∈ Zl . Let γi,t = {µi,t for zt ∈ Z∗
l and zi ∈ Zl , and 0 otherwise}.
(cid:88)
Now using (6), we can write:
zt∈Z∗

γi,tzt + τi .

zi =

(7)

1. We deﬁne the square of the distance of x from origin in kernel space as K (x, x).

64

Fast SVM Training Using Approximate Extreme Points

Here τi is a vector that accounts for the approximation error f (zi , Zlq ) in (6). From (6) and
(7) we can conclude that:

(cid:107)τi(cid:107)2 ≤  ∀ zi ∈ Z.

(8)

Since  will be set to a very small positive constant, we can infer that τi is a very small
vector. The weights γi,t are used to deﬁne βt in (2) as:
N(cid:88)
i=1

βt =

γi,t .

(9)

For ease of notation, we refer to the set X∗ := {xt : zt ∈ Z∗} as the representative
set of X in the remainder of this paper. For the sake of simplicity, we assume that all
γi,t , βt , X, and X∗ are arranged so that X∗ is positioned as the ﬁrst M vectors of X, where
M = |Z∗ |.

3.2 Properties of AESVM

Consider the following optimization problem.

(10)

(cid:107)w(cid:107)2 +

N(cid:88)
F3 (w, b) =
M(cid:88)
i=1
γi,tzt , zt ∈ Z∗ , w ∈ H, and b ∈ R.
t=1

l(w, b, ui ),

C
N

1
2

min
w,b

where ui =

We use the problem in (10) as an intermediary between (1) and (2). The intermediate
problem (10) has a direct relation to the AESVM problem, as given in the following theorem.
The properties of the max function given below are relevant to the following discussion:

max(0, A + B ) ≤ max(0, A) + max(0, B ),
max(0, A − B ) ≥ max(0, A) − max(0, B ),
N(cid:88)
N(cid:88)
ci ,
i=1
i=1
for A, B , ci ∈ R and ci ≥ 0.
Theorem 1 Let F3 (w, b) and F2 (w, b) be as deﬁned in (10) and (2) respectively. Then,

max(0, ciA) = max(0, A)

(11)

(12)

(13)

F3 (w, b) ≤ F2 (w, b) , ∀w ∈ H and b ∈ R.

65

C
N

C
N

=

max

ui =

L3 (w, b, X∗ ) =

Nandan, Khargonekar and Talathi
N(cid:80)
M(cid:80)
N(cid:80)
γi,t and L3 (w, b, X∗ ) = C
Proof Let L2 (w, b, X∗ ) = C
M(cid:80)
l(w, b, zt )
l(w, b, ui ), where
N
N
t=1
i=1
i=1
(cid:40)
(cid:34)
(cid:41)(cid:35)
γi,tzt . From the properties of γi,t in (6), and from (5) we get:
N(cid:88)
M(cid:88)
t=1
1 − yi (wT
(cid:8)1 − yt (wT zt + b)(cid:9)(cid:35)
(cid:34)
γi,tzt + b)
0,
M(cid:88)
N(cid:88)
t=1
i=1
0,
t=1
i=1
N(cid:88)
M(cid:88)
Using properties (11) and (13) we get:
max (cid:2)0, γi,t
(cid:8)1 − yt (wT zt + b)(cid:9)(cid:3)
max (cid:2)0, 1 − yt (wT zt + b)(cid:3) N(cid:88)
M(cid:88)
t=1
i=1
C
=
γi,t
N
t=1
i=1
= L2 (w, b, X∗ ).
2 (cid:107)w(cid:107)2 to both sides of the inequality above we get
Adding 1
F3 (w, b) ≤ F2 (w, b).

L3 (w, b, X∗ ) ≤ C
N

max

γi,t

.

The following theorem gives a relationship between the SVM problem and the interme-
diate problem.
N(cid:88)
N(cid:88)
Theorem 2 Let F1 (w, b) and F3 (w, b) be as deﬁned in (1) and (10) respectively. Then,
max (cid:8)0, yiwT τi
max (cid:8)0, −yiwT τi
(cid:9) ≤ F1 (w, b) − F3 (w, b) ≤ C
(cid:9) ,
− C
N
N
i=1
i=1
∀w ∈ H and b ∈ R, where τi ∈ H is the vector deﬁned in (7).
N(cid:80)
Proof Let L1 (w, b, X) = C
l(w, b, zi ), denote the average hinge loss that is minimized
N
in (1) and L3 (w, b, X∗ ) be as deﬁned in Theorem 1. Using (7) and (1) we get:
i=1
N(cid:88)
max (cid:8)0, 1 − yi (wT zi + b)(cid:9)
(cid:41)
(cid:40)
M(cid:88)
N(cid:88)
i=1
0, 1 − yi (wT (
γi,tzt + τi ) + b)
max
t=1
i=1

L1 (w, b, X) =

C
N

C
N

=

.

66

Fast SVM Training Using Approximate Extreme Points

C
N

max

max

0,

+

C
N

L1 (w, b, X) =

L1 (w, b, X) ≤ C
N

Using (11) on (14), we get:
N(cid:88)
M(cid:88)
t=1
i=1
= L3 (w, b, X∗ ) +
(cid:34)
0,

(cid:40)
From the properties of γi,t in (6), and from (5) we get:
N(cid:88)
M(cid:88)
γi,t (1 − yt (wT zt + b)) − yiwT τi
0,
t=1
i=1
(cid:8)1 − yt (wT zt + b)(cid:9)(cid:35)
(cid:34)
γi,t
N(cid:88)
(cid:9) .
max (cid:8)0, −yiwT τi
C
N
i=1
(cid:8)1 − yt (wT zt + b)(cid:9)(cid:35)
Using (12) on (14), we get:
M(cid:88)
N(cid:88)
γi,t
N(cid:88)
(cid:9) .
max (cid:8)0, yiwT τi
t=1
i=1
= L3 (w, b, X∗ ) − C
N
i=1
From the two inequalities above we get,
N(cid:88)
max (cid:8)0, yiwT τi
i=1

L3 (w, b, X∗ ) − C
N

L1 (w, b, X) ≥ C
N

N(cid:88)
i=1

− C
N

max

(cid:9) ≤ L1 (w, b, X)
≤ L3 (w, b, X∗ ) +

2 (cid:107)w(cid:107)2 to the inequality above we get
Adding 1
N(cid:88)
(cid:9) ≤ F1 (w, b) ≤ F3 (w, b) +
max (cid:8)0, yiwT τi
i=1

F3 (w, b) − C
N

(cid:41)

.

(14)

(cid:9)

N(cid:88)
max (cid:8)0, −yiwT τi
i=1

max (cid:8)0, yiwT τi

(cid:9)

C
N

(cid:9) .

max (cid:8)0, −yiwT τi

N(cid:88)
i=1
N(cid:88)
max (cid:8)0, −yiwT τi
i=1

C
N

(cid:9) .

Using the above theorems we derive the following corollaries. These results provide the
theoretical justiﬁcation for AESVM.
2 , b∗
1 ) be the solution of (1) and (w∗
1 , b∗
Corollary 3 Let (w∗
2 ) be the solution of (2). Then,
√
2 ) ≤ C
1 ) − F2 (w∗
2 , b∗
1 , b∗
F1 (w∗

C .

67

Nandan, Khargonekar and Talathi

1 (cid:107) ≤ √
Proof It is known that (cid:107)w∗
2 (cid:107) ≤ √
C (see Shalev-Shwartz et al., 2011, Theorem 1). It is
straight forward to see that the same result also applies to AESVM, (cid:107)w∗
on (8) we know that (cid:107)τi(cid:107) ≤ √
C . Based
N(cid:88)
N(cid:88)
max (cid:8)0, −yiw∗T
(cid:9) ≤ C
. From Theorem 2 we get:
2 ) ≤ C
2 ) − F3 (w∗
2 , b∗
2 , b∗
F1 (w∗
2 τi
N(cid:88)
N
N
i=1
i=1
√
√
≤ C
N
i=1
1 ) ≤ F1 (w∗
2 , b∗
1 , b∗
1 ) is the solution of (1), F1 (w∗
1 , b∗
Since (w∗
2 ). Using this property and
Theorem 1 in the inequality above, we get:
1 ) − F3 (w∗
2 ) ≤ F1 (w∗
1 ) − F2 (w∗
2 , b∗
1 , b∗
F1 (w∗
1 , b∗
2 , b∗
2 )
≤ F1 (w∗
2 ) − F3 (w∗
2 ) ≤ C
2 , b∗
2 , b∗

(cid:107)w∗
2 (cid:107)(cid:107)τi(cid:107)

C  = C

C .

C .

√

max
ˆα

max
˘α

L3 ( ˘α) =

L2 ( ˆα) =

˘αi − 1
2

ˆαt ˆαsytyszT
t zs ,
M(cid:88)
ˆαtyt = 0.
t=1

Now we demonstrate some properties of AESVM using the dual problem formulations
M(cid:88)
M(cid:88)
M(cid:88)
of AESVM and the intermediate problem. The dual form of AESVM is given by:
ˆαt − 1
N(cid:88)
2
t=1
s=1
t=1
sub ject to 0 ≤ ˆαt ≤ C
γi,t and
N
i=1
N(cid:88)
N(cid:88)
N(cid:88)
The dual form of the intermediate problem is given by:
˘αi ˘αj yiyj uT
i uj ,
N(cid:88)
i=1
i=1
j=1
sub ject to 0 ≤ ˘αi ≤ C
and
N
i=1
Consider the mapping function h : RN → RM , deﬁned as
N(cid:88)
h( ˘α) = { ˜αt : ˜αt =
γi,t ˘αi}.
i=1
M(cid:88)
M(cid:88)
M(cid:88)
It can be seen that the ob jective functions L2 (h( ˘α)) and L3 ( ˘α) are identical.
˜αt − 1
N(cid:88)
N(cid:88)
N(cid:88)
2
s=1
t=1
t=1
˘αi − 1
˘αi ˘αj yiyj uT
i uj
2
i=1
j=1
i=1
= L3 ( ˘α).

˜αt ˜αsytyszT
t zs

L2 (h( ˘α)) =

˘αiyi = 0.

(15)

(16)

(17)

=

68

Fast SVM Training Using Approximate Extreme Points

It is also straight forward to see that, for any feasible ˘α of (16), h( ˘α) is a feasible point of
(15) as it satisﬁes the constraints in (15). However, the converse is not always true. With
that clariﬁcation, we present the following corollary.
1 ) be the solution of (1) and (w∗
Corollary 4 Let (w∗
2 , b∗
1 , b∗
2 ) be the solution of (2). Let ˆα2 be
the dual variable corresponding to (w∗
2 , b∗
2 ). Let h( ˘α2 ) be as deﬁned in (17). If there exists
an ˘α2 such that h( ˘α2 ) = ˆα2 and ˘α2 is a feasible point of (16), then,
√
2 ) − F1 (w∗
1 ) ≤ 2C
1 , b∗
2 , b∗
F1 (w∗

C .

Proof Let (w∗
3 , b∗
3 ) be the solution of (10) and ˘α3 the solution of (16). We know that
3 ). Since L3 ( ˘α3 ) ≥ L3 ( ˘α2 ), we get
3 , b∗
2 ) and L3 ( ˘α3 ) = F3 (w∗
2 , b∗
L3 ( ˘α2 ) = L2 ( ˆα2 ) = F2 (w∗
3 ) ≥ F2 (w∗
2 , b∗
F3 (w∗
3 , b∗
2 ).
3 ) ≤ F3 (w∗
2 ) ≤ F2 (w∗
2 , b∗
3 , b∗
But, from Theorem 1 we know F3 (w∗
2 , b∗
2 ). Hence
2 , b∗
F3 (w∗
3 , b∗
3 ) = F3 (w∗
2 ).

From the above result we get

(18)

(19)

(20)

(21)

− C
N

Adding (19) and (20) we get:

2 ) − F1 (w∗
1 ) ≤ R +
1 , b∗
2 , b∗
F1 (w∗

2 ) − F3 (w∗
1 ) ≤ 0.
1 , b∗
2 , b∗
F3 (w∗
N(cid:88)
(cid:9) ≤ F1 (w∗
max (cid:8)0, yiw∗T
From Theorem 2 we have the following inequalities:
1 ) − F3 (w∗
1 , b∗
1 , b∗
1 τi
1 ), and
N(cid:88)
max (cid:8)0, −yiw∗T
(cid:9) .
i=1
2 ) − F3 (w∗
2 ) ≤ C
2 , b∗
2 , b∗
F1 (w∗
2 τi
N
i=1
N(cid:88)
(cid:9) + max (cid:8)0, yiw∗T
(cid:2)max (cid:8)0, −yiw∗T
(cid:9)(cid:3) ,
C
2 τi
1 τi
N
i=1
2 (cid:107) ≤ √
C and (cid:107)w∗
1 (cid:107) ≤
1 ). Using (18) and the properties (cid:107)w∗
2 ) − F3 (w∗
where R = F3 (w∗
1 , b∗
2 , b∗
√
N(cid:88)
C in (21) we get
(cid:9)(cid:3)
(cid:9) + max (cid:8)0, yiw∗T
(cid:2)max (cid:8)0, −yiw∗T
2 τi
1 τi
N(cid:88)
i=1
2 (cid:107)(cid:107)τi(cid:107) + (cid:107)w∗
(cid:107)w∗
1 (cid:107)(cid:107)τi(cid:107)
N(cid:88)
i=1
√
√
2
i=1

1 ) ≤ C
2 ) − F1 (w∗
1 , b∗
2 , b∗
F1 (w∗
N

≤ C
N

≤ C
N

C  = 2C

C .

69

Nandan, Khargonekar and Talathi

Now we prove a relationship between AESVM and the Gram matrix approximation
methods mentioned in Section 2.1.
Corollary 5 Let L1 (α), L3 ( ˘α), and F2 (w, b) be the objective functions of the SVM dual
(3), intermediate dual (16) and AESVM (2) respectively. Let zi , τi , and ui be as deﬁned
in (4), (7), and (10) respectively. Let G and ˜G be the N xN matrices with Gij = yiyj zT
i zj
N(cid:80)
N(cid:80)
and ˜Gij = yiyj uT
i uj respectively. Then for any feasible ˘α, α, w, and b:
αi − 1
˘αi − 1
2 ˘α ˜G ˘αT , and
2 αGαT , L3 ( ˘α) =
M(cid:88)
N(cid:88)
i=1
i=1
t=1
i=1

Trace(G − ˜G) ≤ N  + 2

1. Rank of ˜G = M , L1 (α) =

γi,t τi .

zT
t

L1 (α) =

2. F2 (w, b) ≥ L3 ( ˘α).
N(cid:88)
Proof Using G, the SVM dual ob jective function L1 (α) can be represented as:
αi − 1
2
i=1
N(cid:88)
Similarly, L3 ( ˘α) can be represented using ˜G as:
˘αi − 1
M(cid:80)
2
i=1
γi,tzt , ∀zt ∈ Z∗ to the deﬁnition of ˜G, we get:
t=1

Applying ui =

L3 ( ˘α) =

αGαT .

˘α ˜G ˘αT .

˜G = ΓAΓT .

Trace(G − ˜G) =

t zs , ∀zt , zs ∈ Z∗ and Γ is the N xM
Here A is the M xM matrix comprised of Ats = ytyszT
matrix with the elements Γit = γi,t . Hence the rank of ˜G = M and intermediate dual
problem (16) is a low rank approximation of the SVM dual problem (3).
(cid:34)
(cid:35)
N(cid:88)
M(cid:88)
M(cid:88)
The Gram matrix approximation error can be quantiﬁed using (7) and (8) as:
i zi − (
(cid:35)
(cid:34)
γi,tzt )T (
zT
γi,szs )
N(cid:88)
M(cid:88)
M(cid:88)
N(cid:88)
s=1
t=1
i=1
≤ N  + 2
zT
γi,tzT
γi,t τi .
t τi
t
t=1
t=1
i=1
i=1
By the principle of duality, we know that F3 (w, b) ≥ L3 ( ˘α), ∀w ∈ H and b ∈ R, where
˘α is any feasible point of (16). Using Theorem 1 on the inequality above, we get
F2 (w, b) ≥ L3 ( ˘α), ∀w ∈ H, b ∈ R and feasible ˘α.

τ T
i τi + 2

=

70

Fast SVM Training Using Approximate Extreme Points

Thus the AESVM problem minimizes an upper bound F2 (w, b), of a rank M Gram matrix
approximation of L1 (α).

Based on the theoretical results in this section, it is reasonable to suggest that for small
values of , the solution of AESVM is close to the solution of SVM.

4. Computation of the Representative Set

In this section, we present algorithms to compute the representative set. The AESVM
formulation can be solved with any standard SVM solver such as SMO and hence we do
not discuss methods to solve it. As described in Section 3.1, we require an algorithm to
compute approximate extreme points in kernel space. Osuna and Castro (2002) proposed
an algorithm to derive extreme points of the convex hull of a data set in kernel space.
Their algorithm is computationally intensive, with a time complexity of O(N S (N )), and
is unsuitable for large data sets as S (N ) typically has a super-linear dependence on N. The
function S (N ) denotes the time complexity of a SVM solver (required by their algorithm),
to train on a data set of size N. We next propose two algorithms leveraging the work by
Osuna and Castro (2002) to compute the representative set in kernel space Z∗ with much
smaller time complexities.
We followed the divide and conquer approach to develop our algorithms. The data
set is ﬁrst divided into subsets Xq , q = 1, 2, .., Q, where |Xq | < P , Q ≥ N
P and X =
{X1 , X2 , .., XQ}. The parameter P is a predeﬁned large integer. It is desired that each
subset Xq contains data vectors that are more similar to each other than data vectors in
other subsets. Our notion of similarity of data vectors in a subset, is that the distances be-
tween data vectors within a subset is less than the distances between data vectors in distinct
subsets. Since performing such a segregation is computationally expensive, heuristics are
used to greatly simplify the process. Instead of computing the distance of all data vectors
from each other, only the distance from a few selected data vectors are used to segregate
the data in the methods FLS2 and SLS described below.
The ﬁrst level of segregation is followed by another level of segregation. We can regard
the ﬁrst level of segregation as coarse segregation and the second as ﬁne segregation. Finally,
the approximate extreme points of the subsets obtained after segregation, are computed.
The two diﬀerent algorithms to compute the representative set diﬀer only in the ﬁrst level
of segregation as described below.

4.1 First Level of Segregation

We propose the methods, FLS1 and FLS2 given below to perform a ﬁrst level of segregation.
In the following description we use arrays ∆(cid:48) and ∆(cid:48)
2 of N elements. Each element of ∆(cid:48)
(∆(cid:48)
2 ), δi (δ2
i ) , contains the index in X of the last data vector of the subset to which xi
belongs. It is straight forward to replace this N element array with a smaller array of size
equal to the number of subsets. We use a N element array for ease of description. The set
X(cid:48) denotes any set of data vectors.
1. FLS1(X(cid:48) , P )

71

Nandan, Khargonekar and Talathi

For some applications, such as anomaly detection on sequential data, data vectors are
found to be homogeneous within intervals. For example, the atmospheric conditions typi-
cally do not change within a few minutes and hence weather data is homogeneous for a short
span. For such data sets it is enough to segregate the data vectors based on its position in
the training data set. The same method can also be used on very large data sets without
any homogeneity, in order to reduce computation time. The complexity of this method is
O(N (cid:48) ), where N (cid:48) = |X(cid:48) | .

[X(cid:48) ,∆(cid:48) ] = FLS1(X(cid:48) , P )

|X(cid:48) |
1. For outerIndex = 1 t o ceiling(
P )
2. For innerIndex = (outerIndex - 1)P t o min((outerIndex)P ,|X(cid:48) |)
Set δinnerI ndex = min((outerI ndex)P , |X(cid:48) |)
3.

2. FLS2(X(cid:48) , P )
When the data set is not homogeneous within intervals or it is not excessively large we
N (cid:48)
use the more sophisticated algorithm, FLS2, of time complexity O(N (cid:48) log2
P ) given below.
In step 1 of FLS2, the distance di in kernel space of all xi ∈ X(cid:48) from xj is computed as
di = (cid:107)φ(xi ) − φ(xj )(cid:107)2 = k(xi , xi ) + k(xj , xj ) − 2k(xi , xj ). The algorithm FLS2(X(cid:48) , P ), in
eﬀect builds a binary search tree, with each node containing the data vector xk selected in
step 2 that partitions a subset of the data set into two. The size of the subsets successively
halve, on downward traversal from the root of the tree to the other nodes. When the size of
all the subsets at a level become ≤ P the algorithm halts. The complexity of FLS2 can be
derived easily when the algorithm is considered as an incomplete binary search tree building
method. The last level of such a tree will have O( N (cid:48)
P ) nodes and consequently the height
N (cid:48)
of the tree is O(log2
P ). At each level of the tree the calls to the BFPRT algorithm (Blum
et al., 1973) and the rearrangement of the data vectors in steps 2 and 3 are of O(N (cid:48) ) time
N (cid:48)
complexity. Hence the overall time complexity of FLS2(X(cid:48) , P ) is O(N (cid:48) log2
P ).

4.2 Second Level of Segregation
After the initial segregation, another method SLS(X(cid:48) , V , ∆(cid:48) ) is used to further segregate each
set Xq into smaller subsets Xqr of maximum size V , Xq = {Xq1 , Xq2 , ...., XqR }, where V is
|Xq |
V ). The algorithm SLS(X(cid:48) , V , ∆(cid:48) ) is given below.
predeﬁned (V < P ) and R = ceiling(
In step 2.b, xt is the data vector in Xq that is farthest from the origin in the space of the
data vectors. For some kernels, such as the Gaussian kernel, all data vectors are equidistant
from the origin in kernel space. If the algorithm chooses al in step 2.b based on distances in
such kernel spaces, the choice would be arbitrary and such a situation is avoided here. Each
iteration of the For loop in step 2 involves several runs of the BFPRT algorithm, with each
run followed by a rearrangement of Xq . Speciﬁcally, the BFPRT algorithm is ﬁrst run on P
data vectors, then on P − V data vectors, then on P − 2V data vectors and so on. The time
complexity of each iteration of the For loop including the BFPRT algorithm run and the
rearrangement of data vectors is: O(P + (P − V ) + (P − 2V ) + .. + V ) ⇒ O( P 2
V ). The overall

72

Fast SVM Training Using Approximate Extreme Points

[X(cid:48) ,∆(cid:48) ] = FLS2(X(cid:48) , P )
1. Compute distance di in kernel space of all xi ∈ X(cid:48) from the ﬁrst vector xj in X(cid:48)
|X(cid:48) |
2 data vectors xi ∈ X(cid:48) with di < dk , using the
2. Select xk such that there exists
linear time BFPRT algorithm
3. Using xk , rearrange X(cid:48) as X(cid:48) = {X1 , X2}, where X1 = {xi : di < dk , xi ∈ X(cid:48)} and
X2 = {xi : xi ∈ X(cid:48) and xi (cid:54)∈ X1}
|X(cid:48) |
2 ≤ P
4. If
For i where xi ∈ X1 , set δi = index of last data vector in X1 .
For i where xi ∈ X2 , set δi = index of last data vector in X2 .
|X(cid:48) |
2 > P
Run FLS2(X1 , P ) and FLS2(X2 , P )

5. If

complexity of SLS(X(cid:48) , V , ∆(cid:48) ) considering the Q For loop iterations is O( N (cid:48)
P
since Q = O( N (cid:48)
P ).
[X(cid:48) ,∆(cid:48)
2 ] = SLS(X(cid:48) , V , ∆(cid:48) )
1. Initialize l = 1

V ) ⇒ O( N (cid:48)P
P 2
V ),

2. For q = 1 t o Q
(a) Identify subset Xq of X(cid:48) using ∆(cid:48)
(b) Set al = φ(xt ), where xt ∈ argmax
(cid:107)xi(cid:107)2 , xi ∈ Xq
i
(c) Compute distance di in kernel space of all xi ∈ Xq from al
(d) Select xk such that, there exists V data vectors xi ∈ Xq with di < dk , using the
BFPRT algorithm
(e) Using xk , rearrange Xq as Xq = {X1 , X2}, where X1 = {xi : di < dk , xi ∈ Xq }
and X2 = {xi : xi ∈ Xq and xi (cid:54)∈ X1}
(f ) For i where xi ∈ X1 , set δ2
i = index of last data vector in X1 , where δ2
i is the ith
element of ∆(cid:48)
2
(g) Remove X1 from Xq
(h) If |X2 | > V
Set: l = l + 1 and al = xk
Repeat steps 2.c to 2.h
(i) If |X2 | ≤ V
For i where xi ∈ X2 , set δ2
i = index of last data vector in X2

73

Nandan, Khargonekar and Talathi

4.3 Computation of the Approximate Extreme Points

After computing the subsets Xqr , the algorithm DeriveAE is applied to each Xqr to compute
its approximate extreme points. The algorithm DeriveAE is described below. DeriveAE uses
three routines. SphereSet(Xqr ) returns all xi ∈ Xqr that lie on the surface of the smallest
hypersphere in kernel space that contains Xqr .
It computes the hypersphere as a hard
margin support vector data descriptor (SVDD) (Tax and Duin, 2004). SphereSort(Xqr )
returns data vectors xi ∈ Xqr sorted in descending order of distance in the kernel space
from the center of the SVDD hypersphere. CheckPoint(xi , Ψ) returns TRUE if xi is an
approximate extreme point of the set Ψ in kernel space. The operator A\B indicates a
set operation that returns the set of the members of A excluding A ∩ B . The matrix X∗
qr
contains the approximate extreme points of Xqr and βqr is a |X∗
qr | sized vector.
[X∗
qr , βqr ] = DeriveAE(Xqr )
qr = SphereSet(Xqr ) and Ψ = ∅
1. Initialize: X∗
2. Set ζ = SphereSort(Xqr \X∗
qr )
qr ∪ Ψ)
3. For each xi taken in order from ζ , call the routine CheckPoint(xi , X∗
If it returns F ALSE , then set Ψ = Ψ ∪ xi
4. Initialize a matrix Γ of size |Xqr |x|X∗
qr | with all elements set to 0
Set µk,k = 1 ∀xk ∈ X∗
qr , where µi,j is the element in the ith row and j th column

of Γ
5. For each xi ∈ Xqr and xi (cid:54)∈ X∗
qr , execute CheckPoint(xi , X∗
qr )
Set the ith row of Γ = µi , where µi is the result of CheckPoint(xi , X∗
qr )
6. For j = 1 t o |X∗
qr |
|Xqr |(cid:80)
k=1

Set β j
qr =

µk,j

min
µi

p(xi , Ψ) = (cid:107)φ(xi ) −

CheckPoint(xi , Ψ) is computed by solving the following quadratic optimization problem:
|Ψ|(cid:88)
t=1
s.t. xt ∈ Ψ, 0 ≤ µi,t ≤ 1 and
where (cid:107)φ(xi ) − |Ψ|(cid:80)
|Ψ|(cid:80)
|Ψ|(cid:80)
|Ψ|(cid:80)
µi,tφ(xt )(cid:107)2 = K (xt , xt ) +
µi,tµi,sK (xt , xs ) − 2
µi,tK (xi , xt ). If the
optimized value of p(xi , Ψ) ≤ , CheckPoint(xi , Ψ) returns TRUE and otherwise it returns
t=1
s=1
t=1
t=1
FALSE. It can be seen that the formulation of p(xi , Ψ) is similar to (6). The value of µi
computed by CheckPoint(zi , Ψ0 ), is used in step 5 of DeriveAE.

µi,tφ(xt )(cid:107)2 ,
|Ψ|(cid:88)
µi,t = 1,
t=1

74

Fast SVM Training Using Approximate Extreme Points

Now we compute the time complexity of DeriveAE. We use the fact that the opti-
mization problem in CheckPoint(xi , Ψ) is essentially the same as the dual optimization
problem of SVM given in (3). Since DeriveAE solves several SVM training problems in
steps 1,3, and 5, it is necessary to know the training time complexity of a SVM. As any
SVM solver method can be used, we denote the training time complexity of each step
of DeriveAE that solves an SVM problem as O(S (Aqr )). Here Aqr is the largest value
qr ∪ Ψ during the run of DeriveAE(Xqr ). This enables us to derive a generic ex-
of X∗
pression for the complexity of DeriveAE, independent of the SVM solver method used.
Hence the time complexity of step 1 is O(S (Aqr )). The time complexity of steps 3 and
5 are O(V S (Aqr )) and O(Aqr S (Aqr )) respectively. The time complexity of step 2 is
O(V |Ψ1 | + V log2V ), where Ψ1 = SphereSet(Xqr ). Hence the time complexity of De-
riveAE is O(V |Ψ1 | + V log2V + V S (Aqr ). Since |Ψ1 | is typically very small, we denote the
time complexity of DeriveAE by O(V log2V + V S (Aqr )). For SMO based implementations
of DeriveAE, such as the implementation we used for Section 5, typically S (Aqr ) = O(A2
qr ).

4.4 Combining All the Methods to Compute X ∗
To derive X ∗ , it is required to ﬁrst rearrange X, so that data vectors from each class
are grouped together as X = {X+ , X−}. Here X+ = {xi : yi = 1, xi ∈ X} and X− =
{xi
: yi = −1, xi ∈ X}. Then the selected segregation methods are run on X+ and
X− separately. The algorithm DeriveRS given below, combines all the algorithms deﬁned
earlier in this section with a few additional steps, to compute the representative set of
X. The complexity of DeriveRS2 can easily be computed by summing the complexities of
its steps. The complexity of steps 1 and 6 is O(N). The complexity of step 2 is O(N ) if
N
In step 3, the O( N P
FLS1 is run or O(N log2
P ) if FLS2 is run.
V ) method SLS is run.
Q(cid:80)
R(cid:80)
In steps 4 and 5, DeriveAE is run on all the subsets Xqr giving a total complexity of
R(cid:80)
Q(cid:80)
S (Aqr )). Here we use the fact that the number of subsets Xqr is
r=1
q=1
Q(cid:80)
R(cid:80)
V ). Thus the complexity of DeriveRS is O(N ( P
O( N
S (Aqr )) when FLS1
V + log2V ) + V
q=1
r=1
S (Aqr )) when FLS2 is used.
r=1
q=1

N
P + P
V + log2V ) + V

O(N log2V + V

is used and O(N (log2

5. Experiments

We focused our experiments on an SMO (Fan et al., 2005) based implementation of AESVM
and DeriveRS. We evaluated the classiﬁcation performance of AESVM using the nine data
sets, described below. Next, we present an evaluation of the algorithm DeriveRS, followed
by an evaluation of AESVM.

2. We present DeriveRS as one algorithm in spite of its two variants that use FLS1 or FLS2, for simplicity
and to conserve space.

75

Nandan, Khargonekar and Talathi

[X∗ , Y∗ , β ] = DeriveRS(X,Y ,P,V)
1. Set X+ = {xi : xi ∈ X, yi = 1} and X− = {xi : xi ∈ X, yi = −1}
2. Run [X+ , ∆+ ] = FLS(X+ ,P) and [X− , ∆− ] = FLS(X− ,P), where FLS is FLS1 or
FLS2

2 ] = SLS(X+ ,V,∆+ ) and [X− , ∆−
2 ] = SLS(X− ,V,∆− )
3. Run [X+ , ∆+
2 , identify each subset Xqr of X+ and run [X∗
4. Using ∆+
qr , βqr ] = DeriveAE(Xqr )
Set N +∗ = sum of number of data vectors in all X∗
qr derived from X+
5. Using ∆−
2 , identify each subset Xqr of X− and run [X∗
qr , βqr ] = DeriveAE(Xqr )
Set N −∗ = sum of number of data vectors in all X∗
qr derived from X−
qr to obtain X∗ and all βqr to obtain β
6. Combine in the same order, all X∗
Set Y∗ = {yi : yi = 1 for i = 1, 2, .., N +∗ ; and yi = −1 for i = 1 + N +∗ , 2 +
N +∗ , .., N −∗ + N +∗}

5.1 Data Sets

Nine data sets of varied size, dimensionality and density were used to evaluate DeriveRS
and our AESVM implementation. For data sets D2, D3 and D4, we performed ﬁve fold cross
validation. We did not perform ﬁve fold cross-validation on the other data sets, because
they have been widely used in their native form with a separate training and testing set.

D1 KDD’99 intrusion detection data set:3 This data set is available as a training set of
4898431 data vectors and a testing set of 311027 data vectors, with forty one features
(D = 41). As described in Tavallaee et al. (2009), a huge portion of this data set is
comprised of repeated data vectors. Experiments were conducted only on the distinct
data vectors. The number of distinct training set vectors was N = 1074974 and the
number of distinct testing set vectors was N = 77216. The training set density =
33%.

D2 Localization data for person activity:4 This data set has been used in a study on agent-
based care for independent living (Kaluˇza et al., 2010). It has N = 164860 data vectors
of seven features. It is comprised of continuous recordings from sensors attached to
ﬁve people and can be used to predict the activity that was performed by each person
at the time of data collection. In our experiments we used this data set to validate
a binary problem of classifying the activities ‘lying’ and ‘lying down’ from the other
activities. Features 3 and 4, that gives the time information, were not used in our
experiments. Hence for this data set D = 5. The data set density = 96%.

3. D1 is available for download at http://archive.ics.uci.edu/ml/datasets/KDD+Cup+1999+Data.
4. D2 is available for download at http://archive.ics.uci.edu/ml/datasets/Localization+Data+for+
Person+Activity.

76

Fast SVM Training Using Approximate Extreme Points

D3 Seizure detection data set: This data set has N = 982863 data vectors, three features
(D = 3) and density = 100%. It is comprised of continuous EEG recordings from rats
induced with status epilepticus and is used to evaluate algorithms that classify seizure
events from seizure-free EEG. An important characteristic of this data set is that it
is highly unbalanced, the total number of data vectors corresponding to seizures is
minuscule compared to the remaining data. Details of the data set can be found in
Nandan et al. (2010), where it is used as data set A.

D4 Forest cover type data set:5 This data set has N = 581012 data vectors and ﬁfty four
features (D = 54) and density = 22%. It is used to classify the forest cover of areas
of 30mx30m size into one of seven types. We followed the method used in Collobert
et al. (2002), where a classiﬁcation of forest cover type 2 from the other cover types
was performed.

D5 IJCNN1 data set:6 This data set was used in IJCNN 2001 generalization ability chal-
lenge (Chang and Lin, 2001). The training set and testing set have 49990 (N = 49990)
and 91701 data vectors respectively. It has 22 features (D = 22) and training set den-
sity = 59%

D6 Adult income data set:7 This data set derived from the 1994 Census database, was used
to classify incomes over $50000 from those below it. The training set has N = 32561
with D = 123 and density = 11%, while the testing set has 16281 data vectors. The
data is pre-processed as described in Platt (1999).

D7 Epsilon data set:8 This is a data set that was used for 2008 Pascal large scale learning
challenge and in Yuan et al. (2011). It is comprised of 400000 data vectors that are
100% dense with D = 2000. Since this is too large for our experiments, we used the
ﬁrst 10% of the training set9 giving N = 40000. The testing set has 100000 data
vectors.

D8 MNIST character recognition data set:10 The widely used data set (Lecun et al., 1998)
of hand written characters has a training set of N = 60000, D = 780 and density =
19%. We performed the binary classiﬁcation task of classifying the character ‘0’ from
the others. The testing set has 10000 data vectors.

5. D4 is available for download at http://archive.ics.uci.edu/ml/datasets/Covertype.
6. D5 is available for download at http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.
html#ijcnn1.
7. D6 is available for download at http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.
html#a9a.
8. D7 is available for download at http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.
html#epsilon.
9. AESVM and the other SVM solvers are fully capable of training on this data set. However, the excessive
training time makes it impractical to train the solvers on the entire data set for this paper.
10. D8 is available for download at http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/
multiclass.html#mnist.

77

Nandan, Khargonekar and Talathi

D9 w8a data set:11 This artiﬁcial data set used in Platt (1999) was randomly generated
and has D = 300 features. The training set has N = 49749 with a density = 4% and
the testing set has 14951 data vectors.

5.2 Evaluation of DeriveRS

We began our experiments with an evaluation of the algorithm DeriveRS, described in
Section 4. The performance of the two methods FLS1 and FLS2 were compared ﬁrst.
DeriveRS was run on D1, D2, D4 and D5 with the parameters P = 104 , V = 103 ,  = 10−2 ,
and g = [2−4 , 2−3 , 2−2 , ..., 22 ], ﬁrst with FLS1 and then FLS2. For D2, DeriveRS was run
on the entire data set for this particular experiment, instead of performing ﬁve fold cross-
validation. This was done because, D2 is a small data set and the diﬀerence between the
two ﬁrst level segregation methods can be better observed when the data set is as large as
possible. The relatively small value of P = 104 was also chosen considering the small size
of D2 and D5. To evaluate the eﬀectiveness of FLS1 and FLS2, we also ran DeriveRS with
FLS1 and FLS2 after randomly reordering each data set. The results are shown in Figure
1.

Figure 1: Performance of variants of DeriveRS with g = [2−4 , 2−3 , 2−2 , ..., 22 ], for data sets
D1, D2, D4, and D5. The results of DeriveRS with FLS1 and FLS2, after ran-
domly reordering the data sets are shown as Random+FLS1 and Random+FLS2,
respectively

11. D9 is available for download at http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.
html#w8a.

78

Fast SVM Training Using Approximate Extreme Points

For all data sets, FLS2 gave smaller representative sets than FLS1. For D1, DeriveRS
with FLS2 was signiﬁcantly faster and gave much smaller results than FLS1. For D2, D4
and D5, even though the representative sets derived by FLS1 and FLS2 are almost equal
in size, FLS1 took noticeably less time. The results of DeriveRS obtained after randomly
rearranging the data sets, indicate the utility of FLS2. For all the data sets, the results of
FLS2 after random reordering was seen to be signiﬁcantly better than the results of FLS1
after random rearrangement. Hence we can infer that the good results obtained with FLS2
are not caused by any pre-existing order in the data sets. A sharp increase was observed in
representative set sizes and computation times for FLS1, when the data sets were randomly
rearranged.
Next we investigated the impact of changes in the values of the parameters P and
V on the performance of DeriveRS. All combinations of P = {104 , 5x104 , 105 , 2x105} and
V = {102 , 5x102 , 103 , 2x103 , 3x103} were used to compute the representative set of D1. The
computations were performed for  = 10−2 and g = 1. The method FLS2 was used for the
Q(cid:80)
R(cid:80)
ﬁrst level segregation in DeriveRS. The results are shown in Table 1. As expected for an
N
P + P
V + log2V ) + V
q=1
r=1
time was generally observed to increase for an increase in the value of V or P . It should be
noted that our implementation of DeriveRS was based on SMO and hence S (Aqr ) = O(A2
qr ).
Q(cid:80)
R(cid:80)
In some cases the computation time decreased when P or V increased. This is caused by a
R(cid:80)
size of the representative set M (M ≈ Q(cid:80)
A2
qr ), which is inferred from the observed decrease of the
decrease in the value of O(
r=1
q=1
Aqr ). A sharp decrease in M was observed
r=1
q=1
when V was increased. The impact of increasing P on the size of the representative set was
found to be less drastic. This observation indicates that DeriveAE selects fewer approximate
extreme points when V is larger.

algorithm of time complexity O(N (log2

S (Aqr )), the computation

M
N x100% (Computation time in seconds)
V = 2x103
V = 103
V = 5x102
V = 102
2.2(161)
2.5(87)
7(27)
3(51)
2.9(59)
6.9(66)
2.4(92)
2.1(166)
2.1(169)
2.3(98)
2.9(69)
7(121)
6.9(237)
2.9(94)
2.3(110)
2(176)

P
104
5x104
105
2x105

V = 3x103
2.1(233)
2(239)
1.9(248)
1.9(250)

Table 1: The impact of varying P and V on the result of DeriveRS

As described in Section 5.3, we compared several SVM training algorithms with our
implementation of AESVM. We performed a grid search with all combinations of the SVM
hyper-parameters C (cid:48) = {2−4 , 2−3 , ..., 26 , 27} and g = {2−4 , 2−3 , 2−2 , ..., 21 , 22}. The hyper-
parameter C (cid:48) is related to the hyper-parameter C as C (cid:48) = C
N . We represent the grid in
terms of C (cid:48) as it is used in several SVM solvers such as LIBSVM, LASVM, CVM and
BVM. Furthermore, the use of C (cid:48) enables the application of the same hyper-parameter grid
to all data sets. To train AESVM with all the hyper-parameter combinations in the grid,

79

Nandan, Khargonekar and Talathi

the representative set has to be computed using DeriveRS for all values of kernel hyper-
parameter g in the grid. This is because the kernel space varies when the value of g is
varied. For all the computations, the input parameters were set as P = 105 and V = 103 .
The ﬁrst level segregation in DeriveRS was performed using FLS2. Three values of the
tolerance parameter  were investigated,  = 10−2 , 10−3 or 10−4 .
The results of the computation for data sets D1 - D5, are shown in the Table 2. The
percentage of data vectors in the representative set was found to increase with increasing
values of g . This is intuitive, as when g increases the distance between the data vectors in
kernel space increases. With increased distances, more data vectors xi become approximate
extreme points. The increase in the number of approximate extreme points with g causes
the rising trend of computation time shown in Table 2. For a decrease in the value of ,
M increases. This is because, for smaller  fewer xi would satisfy the condition: optimized
p(xi , Ψ) ≤  in CheckPoint(xi , Ψ). This results in the selection of a larger number of
approximate extreme points in DeriveAE.



10−2

10−3

10−4

Data
set
D1
D2
D3
D4
D5
D1
D2
D3
D4
D5
D1
D2
D3
D4
D5

g = 1
24

M
N x100% (Computation time in seconds)
g = 1
g = 1
g = 1
g = 1
23
22
2

g = 21

g = 22

4.6(163)
3.3(157)
2.4(151)
1.3(140) 1.7(147)
0.9(139) 1(138)
4.7(17)
2.8(15)
1.8(14)
1.2(13)
0.8(13)
0.7(13)
0.6(12)
0.6(78)
0.6(79)
0.6(79)
0.6(80)
0.6(79)
0.6(79)
0.6(80)
25.2(111)
14.5(91)
8.5(78)
3.1(61)
5.1(68)
1.9(58)
1.3(55)
71(15)
58(15)
42.1(14)
17.7(10) 28.1(12)
5.6(7)
10.4(8)
12.1(231)
8.5(208)
6(188)
1.6(142) 2.2(149) 3(160)
4.2(168)
14.4(35)
8.8(29)
5.7(23)
3.8(19)
2.6(16)
1.8(14)
1.3(13)
0.5(80)
0.6(81)
0.5(80)
0.6(79)
0.6(79)
0.6(79)
0.6(80)
31.1(172) 48.7(203) 71.3(204)
5.5(71)
8.6(86)
13(106)
19.9(136)
76.2(22)
86.1(21)
93.5(19)
25.8(15) 36.4(19) 49.5(22) 63.5(23)
15.2(358) 20.4(418) 26.8(479)
3.8(189) 5.4(217) 7.7(253) 10.9(304)
35.8(100)
22.8(79)
14.3(61)
9.6(52)
6.9(40)
5.1(28)
3.8(21)
0.5(78)
0.7(83)
0.9(86)
1.2(90)
0.5(79)
0.5(80)
0.6(81)
19.4(175) 27.1(249) 38.1(333) 54.3(394.3) 75.5(387) 92.6(310) 98.8(244)
99.7(22)
98.3(26)
94.9(32)
56.9(40) 69.1(43) 80.1(41) 88.6(38)

Table 2: The percentage of the data vectors in X∗ (given by M
N x100) and its computation
time for data sets D1-D5

The results of applying DeriveRS to the high-dimensional data sets D6-D9 are shown in
Table 3. It was observed that M
N was much larger for D6-D9 than for the other data sets.
We computed the representative set with  = 10−2 only, as for smaller values of  we expect
the representative set to be close to 100% of the training set. The increasing trend of the
size of the representative set with increasing g values can be observed in Table 3 also.

80

Fast SVM Training Using Approximate Extreme Points

g = 1
24

M
N x100% (Computation time in seconds)
g = 1
g = 1
g = 1
g = 1
23
22
2

g = 21

g = 22

83.1(12)
86(9)
82.7(9)
83.1(9)
83.1(12)
83.1(13)
83.1(12)
97.2(317) 99.7(309) 100(325) 100(332) 100(360) 100(330) 100(280)
100(64)
100(64)
100(67)
100(63)
100(62)
100(75)
100(97)
72.2(21)
72.2(22)
72.2(21)
72.7(17)
72.8(15)
74.4(14)
76.1(15)

Data
set
D6
D7
D8
D9

Table 3: The percentage of data vectors in X∗ and its computation time for data sets D6-D9
with  = 10−2

5.3 Comparison of AESVM to SVM Solvers

To judge the accuracy and eﬃciency of AESVM, its classiﬁcation performance was compared
with the SMO implementation in LIBSVM, ver. 3.1. We chose LIBSVM because it is a state-
of-the-art SMO implementation that is routinely used in similar comparison studies. To
compare the eﬃciency of AESVM to other popular approximate SVM solvers we chose CVM,
BVM, LASVM, SVMperf , and RfeatSVM. A description of these methods is given in Section
2. We chose these methods because they are widely cited, their software implementations
are freely available and other studies (Shalev-Shwartz et al., 2011) have reported fast SVM
training using some of these methods. LASVM is also an eﬃcient method for online SVM
training. However, since we do not investigate online SVM learning in this paper, we did not
test the online SVM training performance of LASVM. We compared AESVM with CVM
and BVM even though they are L2-SVM solvers, as they has been reported to be faster
alternatives to SVM implementations such as LIBSVM.
The implementation of AESVM and DeriveRS were built upon the LIBSVM implemen-
tation. All methods except SVMperf were allocated a cache of size 600 MB. The parameters
for DeriveRS were P = 105 and V = 103 , and the ﬁrst level segregation was performed
using FLS2. To reﬂect a typical SVM training scenario, we performed a grid search with
all eighty four combinations of the SVM hyper-parameters C (cid:48) = {2−4 , 2−3 , ..., 26 , 27} and
g = {2−4 , 2−3 , 2−2 , ..., 21 , 22}. As mentioned earlier, for data sets D2, D3 and D4, ﬁve
fold cross-validation was performed. The results of the comparison have been split into
sub-sections given below, due to the large number of SVM solvers and data sets used.

5.3.1 Comparison to CVM, BVM, LASVM and LIBSVM

First we present the results of the performance comparison for D2 in Figures 2 and 3.
For ease of representation, only the results of grid points corresponding to combinations of
C (cid:48) = {2−4 , 2−2 , 1, 22 , 24 , 26} and g = {2−4 , 2−2 , 1, 22} are shown in Figures 2 and 3. Figure
2 shows the graph between training time and classiﬁcation accuracy for the ﬁve algorithms.
Figure 3 shows the graph between the number of support vectors and classiﬁcation accuracy.
We present classiﬁcation accuracy as the ratio of the number of correct classiﬁcations to the
total number of classiﬁcations performed. Since the classiﬁcation time of an SVM algorithm
is directly proportional to the number of support vectors, we represent it in terms of the

81

Nandan, Khargonekar and Talathi

number of support vectors.
It can be seen that, AESVM generally gave more accurate
results for a fraction of the training time of the other algorithms, and also resulted in less
classiﬁcation time. The training time and classiﬁcation times of AESVM increased when 
was reduced. This is expected given the inverse relation of M to  shown in Tables 2 and
3. The variation in accuracy with  is not very noticeable.

Figure 2: Plot of training time against classiﬁcation accuracy of the SVM algorithms on D2

Figures 2 and 3 indicate that AESVM gave better results than the other algorithms for
SVM training and classiﬁcation on D2, in terms of standard metrics. To present a more
quantitative and easily interpretable comparison of the algorithms, we deﬁne the seven
performance metrics given below. These metrics combine the results of all runs of each
algorithm into a single value, for each data set. For the ﬁrst ﬁve metrics, we take LIBSVM
as a baseline of comparison, as it gives the most accurate solution among the tested methods.
Furthermore, an important ob jective of these experiments is to show the similarity of the
results of AESVM and LIBSVM. In the description given below, F can refer to any SVM
algorithm such as AESVM, CVM, LASVM etc.

82

0.40.50.60.70.8−4−2024681012log(Training time)Classification accuracyAESVM, ε= 10−2AESVM, ε= 10−3AESVM, ε= 10−4CVMBVMLASVMLIBSVMFast SVM Training Using Approximate Extreme Points

Figure 3: Plot of classiﬁcation time, represented by the number of support vectors, against
classiﬁcation accuracy of the SVM algorithms on D2

.

ET S =

1. Expected training time speedup, ET S : The expected speedup in training time is indi-
R(cid:88)
S(cid:88)
cated by:
T Lr
1
s
T Fr
RS
s
s=1
r=1
s and T Fr
s are the training times of LIBSVM and F respectively, in the sth
Here T Lr
cross-validation fold with the rth set of hyper-parameters of grid search.
2. Overal l training time speedup, OT S : It indicates overall training time speedup for
the entire grid search with cross-validation, including the time taken to compute the
representative set. The total time taken by DeriveRS to compute the representative
set for all values of g is represented as TX∗ . For methods other than AESVM and
RfeatSVM2 (see Section 5.3.3), TX∗ = 0.
S(cid:80)
R(cid:80)
R(cid:80)
S(cid:80)
T Lr
s
s=1
r=1
s + TX∗
T Fr
s=1
r=1

OT S =

.

83

0.40.50.60.70.86789101112log(Number of support vectors)Classification accuracyAESVM, ε= 10−2AESVM, ε= 10−3AESVM, ε= 10−4CVMBVMLASVMLIBSVMNandan, Khargonekar and Talathi

.

1
RS

EC S =

N Lr
s
N Fr
s

3. Expected classiﬁcation time speedup, EC S : The expected speedup in classiﬁcation
R(cid:88)
S(cid:88)
time is indicated by:
r=1
s=1
s and N Fr
Here N Lr
s are the number of support vectors in the solution of LIBSVM and
F respectively.
4. Classiﬁcation time speedup for optimal hyper-parameters, C T S : The speedup in classi-
ﬁcation time for the optimal hyper-parameters (hyper-parameters that result in max-
S(cid:80)
imum classiﬁcation accuracy) chosen by grid search is indicated by:
S(cid:80)
N Lr
s
s=1
N Fr
s
s=1

max
r

max
r

C T S =

.

.

1
S

RM SE =

s − C Fr
(C Lr
s )2

max. acc. = max
r

5. Root mean squared error of classiﬁcation accuracy, RM SE : The similarity of the
(cid:33)0.5
(cid:32)
solution of F to LIBSVM, in terms of its classiﬁcation accuracy, is indicated by:
R(cid:88)
S(cid:88)
1
RS
r=1
s=1
s and C Fr
s are the classiﬁcation accuracy of LIBSVM and F respectively.
Here C Lr
6. Maximum classiﬁcation accuracy: It gives the best classiﬁcation results of an SVM
S(cid:88)
solver, for the set of SVM hyper-parameters that are tested.
s=1
7. Mean and standard deviation of classiﬁcation accuracies: It indicates the classiﬁcation
performance of an SVM solver, that can be expected for arbitrary hyper-parameter
(cid:118)(cid:117)(cid:117)(cid:116) 1
(cid:33)2
(cid:32)
values.
S(cid:88)
R(cid:88)
S(cid:88)
R(cid:88)
C Fr
s , and std. acc. =
R
s=1
r=1
s=1
r=1
The results of the classiﬁcation performance comparison on data sets D1-D5, are shown
in Table 4. It was observed that for all tested values of , AESVM resulted in large reductions
in training and classiﬁcation times when compared to LIBSVM for a very small diﬀerence
in classiﬁcation accuracy. Most notably, for D3 the expected and overall training time
speedups were 41728.8 and 488.5 respectively, which is outstanding. Comparing the results
of AESVM for diﬀerent  values, we see that RM SE generally improves by decreasing when
 decreases, while the metrics improve by increasing when  increases. The increase in ET S
and OT S is of a larger order than the increase in RM SE when  increases.

s − mean acc.
C Fr

mean acc. =

C Fr
s .

1
RS

1
S

.

84

Fast SVM Training Using Approximate Extreme Points

Data
set

D1

D2

D3

D4

D5

Solver

ET S

OT S EC S C T S RM SE
(x102 )
0.22
0.14
0.06
0.44
0.6
0.12

156
50.4
14.7
6.2
21.6
0.8

5.8
3.8
2.4
1.2
2
1.1

3.3
2.6
1.8
2.3
1.9
1

134.5 77.7 17.8 3.85
86.1
29
9.4
2.43
1.73
6.2
10.9
21.8
26.59
4.3
4.7
0.5
0.5
5
5.6
24.06
2.18
1
1
0.1

AESVM1 1188.9
AESVM2 314.8
AESVM3 72.7
8.9
CVM
BVM
28.6
LASVM 0.8
LIBSVM
AESVM1 6067.6
AESVM2 1202.5
AESVM3 164.5
0.7
CVM
BVM
0.8
LASVM 0.2
LIBSVM
AESVM1 41728.8 488.5 71.5 64.4 0.2
AESVM2 21689.3
468
39.5
51.5
0.1
0.09
36
17.1
429.9
AESVM3 12792
0.33
0.1
0.4
23.9
60.4
CVM
0.39
0.2
0.6
22.8
BVM
76.8
LASVM 0.9
0.5
0.6
0.7
55.2
LIBSVM
AESVM1 962
AESVM2 68.8
AESVM3 6.7
8
CVM
BVM
6.6
LASVM -
LIBSVM
AESVM1 26.6
AESVM2 3.1
AESVM3 1.3
0.3
CVM
BVM
0.5
LASVM 0.6
LIBSVM

24.5 72.8 1.5
6.3
17.1
0.7
0.3
5
2.3
9.4
28
12.4
9.44
8.9
12.1
-
-
-

34.6
6.1
2.3
6.2
4.4
-

0.5
0.39
0.25
0.74
0.84
0.13

4.1
1.8
1.1
0.2
0.3
0.5

3.3
1.5
1.1
0.8
1
1

1.6
0.9
0.9
0.6
0.9
1.1

max. acc.
(x102 )
94.2
93.6
93.8
94.1
94.4
94.3
93.9
76.5
76.7
77.4
70.3
67.1
78.1
78.2
99.9
99.9
99.9
99.9
99.9
99.9
99.9
68.3
68.1
68.1
63.7
62.3
-
68.2
98.8
98.9
99
99
99.1
99.2
99

mean & std.
acc. (x102 )
92.4, 0.8
92.3, 0.7
92.4, 0.8
92.7, 0.8
92.6, 0.9
92.5, 0.8
92.4, 0.8
71.1, 3.3
72.4, 3.6
73.1, 3.6
52.2, 0.8
54.6, 0.7
73.5, 0.5
74.1, 3.5
99.8, 0.1
99.8, 0.1
99.8, 0.1
99.8, 0.2
99.8, 0.2
69.3, 29.9
99.8, 0.1
61.6, 3.1
61, 3.3
60.8, 3.2
55.5, 3.1
54.9, 3.4
-
60.6, 3.2
96.2, 2.6
96.3, 2.6
96.4, 2.6
96.6, 2.5
97, 2
97, 2
96.6, 2.4

Table 4: Performance comparison of AESVM, CVM, BVM, LASVM and LIBSVM on data
sets D1-D5. AESVM1, AESVM2 and AESVM3 represent the results of AESVM
with  = 10−2 , 10−3 , and 10−4 respectively.

85

Nandan, Khargonekar and Talathi

Comparing AESVM to CVM, BVM and LASVM, we see that AESVM in general gave
the least values of RM SE and the largest values of ET S , OT S , EC S and C T S .
In a
few cases LASVM gave low RM SE values. However, in all our experiments LASVM took
longer to train than the other algorithms including LIBSVM. We could not complete the
evaluation of LASVM for D4 due to its large training time, which was more than 40 hours
for some hyper-parameter combinations. The ﬁve algorithms under comparison were found
to give similar maximum classiﬁcation accuracies for D1, D3 and D5. For D2 and D4, CVM
and BVM gave signiﬁcantly smaller maximum classiﬁcation accuracies. Another interesting
result is that for D3, the mean and standard deviation of classiﬁcation accuracy of LASVM
was found to be widely diﬀerent from the other algorithms. For all the tested values of
 the maximum, mean and standard deviation of the classiﬁcation accuracies of AESVM
were found to be similar.
Next we present the results of performance comparison of CVM, BVM, LASVM, AESVM,
and LIBSVM on the high-dimensional data sets D6-D9. As described in Section 5.2, De-
riveRS was run with only  = 10−2 for these data sets. The results of the performance
comparison are shown in Table 5. CVM was found to take longer than 40 hours to train
on D6, D7 and D8 with some hyper-parameter values and hence we could not complete its
evaluation for those data sets. BVM also took longer than 40 hours to train on D7 and it
was also not evaluated for D7. AESVM consistently reported ET S , OT S , EC S and C T S
values that are larger than 1 unlike the other algorithms, except for D9 where the C T S
value for AESVM was 0.6. However it should be noted that the other methods also had sim-
ilarly low C T S values for D9. Similar to the results in Table 4, LASVM and BVM resulted
in very large RM SE values for some data sets. The maximum classiﬁcation accuracies of
all algorithms were similar. On some data sets, BVM and LASVM were observed to give
signiﬁcantly lower mean and higher standard deviation of classiﬁcation accuracy.

5.3.2 Comparison to SVMperf

SVMperf diﬀers from the other SVM solvers in its ability to compute a solution close to
the SVM solution for a given number of support vectors (k). The algorithm complexity
depends on k as O(k2 ) per iteration. We ﬁrst used a value of k = 1000 for our experiments,
as it has been reported to give good performance (Joachims and Yu, 2009). SVMperf was
tested on data sets D1, D4, D5, D6, D8 and D9, with the Gaussian kernel12 and the same
hyper-parameter grid as described earlier. The results of the grid search are presented in
Table 6. The results of our experiments on AESVM (with  = 10−2 ) and LIBSVM are
repeated in Table 6 for ease of reference. The maximum, mean and standard deviation of
classiﬁcation accuracies are represented as max. acc., mean & std. acc. respectively.
Based on the results obtained for k = 1000, other values of k were also tested. For data
sets D1, D4 and D5, though SVMperf gave classiﬁcation accuracies similar to the that of
LIBSVM and AESVM, the training times were similar to or higher than the training times of
LIBSVM. To test the ability of SVMperf to give fast training, we also tested it with k = 400
for D1, D4 and D5. For the high dimensional data sets (D6, D8 and D9), the RM SE values
were signiﬁcantly higher for SVMperf , while the mean classiﬁcation accuracy was noticeably
lower than AESVM. Considering the possibility that the value of k = 1000 is insuﬃcient to

12. We used the software parameters ‘-t 2 -w 9 –i 2 –b 0’ as suggested in the author’s website.

86

Fast SVM Training Using Approximate Extreme Points

Data
set

Solver

ET S OT S EC S C T S RM SE
(x102 )
0
-
7.8
0.85

1.2
-
1.2
1.1

D6

D7

D8

D9

1.4
-
0.6
0.5

1.1
-
1.5
1

AESVM 1.5
-
CVM
BVM
0.6
LASVM 0.8
LIBSVM
AESVM 1
-
CVM
BVM
-
LASVM 0.9
LIBSVM
AESVM 1
CVM
-
BVM
4.7
LASVM 1
LIBSVM
1.1
1.3
AESVM 1.4
1.4
CVM
1.8
1.2
17.5 16.9 4.9
BVM
LASVM 0.6
0.5
2.3
LIBSVM

1
-
3.2
1

1
-
-
1

1
-
-
0.7

1
-
2.6
0.9

max. acc.
(x102 )
85.1
-
85.2
85
85.1
88.3
-
-
88.4
88.6
99.7
-
99.7
99.7
99.7
99.5
99.5
99.5
99.5
99.5

mean & std.
acc. (x102 )
81.4, 2.8
-
80.2, 8.9
81.1, 2.9
81.4, 2.8
85.3, 5.7
-
-
85.2, 6.2
85.7, 4.8
92.3, 3.6
-
88.5, 18.1
92.3, 3.6
92.3, 3.6
98.8, 0.8
98.9 , 0.8
98.9 , 0.8
85.5, 23.9
98.8, 0.8

1.1
-
-
0.9

1
-
3.1
1

0.6
0.3
0.6
0.1

0.01
-
-
2.37

0
-
17.55
0

0
1
0.09
27.5

Table 5: Performance comparison of AESVM (with  = 10−2 ), CVM, BVM, LASVM and
LIBSVM on data sets D6-D9

result in an accurate solution for these data sets, we tested D6 and D9 with k = 2000 and
D8 with k = 3000. Even though the training time increased signiﬁcantly with an increase in
k , the values of RM SE and the mean and standard deviation of accuracies did not improve
signiﬁcantly. The training time speedup values of SVMperf are much lower than AESVM
for all tested k values for all data sets, except for D8. The maximum accuracies of all the
algorithms were similar. Due to the ability of SVMperf to approximate w with a small set of
k vectors, the classiﬁcation time speedups of SVMperf are signiﬁcantly higher than AESVM.
However, this approximation comes at the cost of increased training time and sometimes
results in a loss of accuracy, as illustrated in Table 6.

5.3.3 Comparison to RfeatSVM

Rahimi and Recht (2007) proposed a promising method to approximate non-linear kernel
SVM solutions using simpler linear kernel SVMs. This is accomplished by ﬁrst pro jecting
the training data set into a randomized feature space and then using any SVM solver with
the linear kernel on the pro jected data set. We ﬁrst investigated the classiﬁcation accuracy
of the solution of RfeatSVM and its similarity to the SVM solution. LIBSVM with the

87

Nandan, Khargonekar and Talathi

OT S EC S C T S RM SE
(x102 )
0.22
0.89

3.3
6.6

5.8
17

max. acc.
(x102 )
94.2
93.9

mean & std.
acc. (x102 )
92.4, 0.8
92.7, 0.4

3.7

0.9

2.6

2.6

0.74

94

92.7, 0.5

34.6 24.5
1.5
72.8
467.1 694.3 3.7
3.7

93.9
68.3
68.4

92.4, 0.8
61.6, 3.1
62.9, 2.2

Data
set

D1

D4

D5

D6

D8

D9

Solver

ET S

3.1

1.2

0.2

0.1

4.1
0.4

AESVM 1188.9 156
SVMperf
1.6
6.7
k = 400
SVMperf
k = 1000
LIBSVM
AESVM 962
SVMperf
10.2
k = 400
SVMperf
k = 1000
LIBSVM
AESVM 26.6
SVMperf
0.8
k = 400
SVMperf
k = 1000
LIBSVM
AESVM 1.5
SVMperf
1.1
k = 1000
SVMperf
k = 2000
LIBSVM
AESVM 1
SVMperf
37.6
k = 1000
SVMperf
k = 3000
LIBSVM
AESVM 1.4
SVMperf
1.2
k =1000
SVMperf
k =2000
LIBSVM

1.4
0.9

1.3
0.9

0.3

0.4

0.3

3.5

1.2

186.8

277.7

2.14

68.1

61.8, 2.7

3.3
14.6

1.6
8.2

0.5
2.9

68.2
98.8
98.8

60.6, 3.2
96.2, 2.6
96.5, 2.4

5.8

3.3

0.26

99

96.7, 2.4

1.1
20

10

0.2

1
1
23.8 49

1.2
12.1

0
9.39

99
85.1
85.2

96.6, 2.4
81.4, 2.8
79.6, 10.7

6

6.5

85.1

80.1, 7.8

1
9.9

0
54.2

85.1
99.7
99.9

81.4, 2.8
92.3, 3.6
55.7, 42.3

16.3

3.3

51.4

99.8

59.2, 41.6

1.1
21.3

0.6
3

0
22.6

99.7
99.5
99.2

92.3, 3.6
98.8, 0.8
86.1, 18.8

10.7

1.5

20.6

99.4

87.3, 17.3

99.5

98.8, 0.8

Table 6: Performance comparison of SVMperf , AESVM (with  = 10−2 ), and LIBSVM

linear kernel was used to compute the RfeatSVM solution on the pro jected data sets. This
combination of RfeatSVM and LIBSVM is denoted as RfeatSVM1. We used LIBSVM,

88

Fast SVM Training Using Approximate Extreme Points

in spite of the availability of faster linear SVM implementations, as it is an exact SVM
solver. Hence only the performance metrics related to accuracy were used to compare the
performance of AESVM, LIBSVM and RfeatSVM1. The random Fourier features method,
described in Algorithm 1 of Rahimi and Recht (2007), was used to pro ject the data sets
D1, D5, D6 and D9 into a randomized feature space of dimension E.

Data
set

D1

D5

D6

D9

Solver

AESVM
RfeatSVM1
E = 100
LIBSVM
AESVM
RfeatSVM1
E = 100
LIBSVM
AESVM
RfeatSVM1
E = 1000
LIBSVM
AESVM
RfeatSVM1
E = 1000
LIBSVM

RM SE
(x102 )
0.25
56.18

max. acc.
(x102 )
93.5
37.8

mean & std.
acc. (x102 )
92.2,0.9
36.1,1.3

0.9
5.3

0.16
4

0.15
0.6

93.6
98.6
94.7

98.9
85.1
81.6

85
99.3
98.7

92.3,0.9
95.7,2.8
91.6,1.4

96.2 ,2.7
81.2,2.9
78,2.2

81.3,3
98.6,0.8
97.4,0.6

99.5

98.8,0.9

Table 7: Performance comparison of RfeatSVM1 (RfeatSVM solved using LIBSVM),
AESVM (with  = 10−2 ), and LIBSVM

The results of the accuracy comparison are given in Table 7. We used a smaller hyper-
parameter grid of all twenty four combinations of C (cid:48) = {2−4 , 2−2 , 1, 22 , 24 , 26} and g =
{2−4 , 2−2 , 1, 22} for our experiments. The results reported in Table 7 for AESVM and
LIBSVM were computed for this smaller grid. We selected the number of dimensions (E)
of the randomized feature space for D1 and D6 based on Rahimi and Recht (2007). The
maximum accuracy for RfeatSVM1 was found to be much less than AESVM and LIBSVM
for all data sets. The RM SE values for RfeatSVM1 were signiﬁcantly higher than AESVM
and mean accuracy noticeably lower for most data sets, especially for D1 and D6.

Next we investigated the training and classiﬁcation time requirements of RfeatSVM by
solving it using the fast linear SVM solver LIBLINEAR (Fan et al., 2008), referred to as
RfeatSVM2 in the remainder of this paper. The entire hyper-parameter grid used in the
previous sections were used in this experiment. The results of the performance comparison
of RfeatSVM2, AESVM and LIBSVM are presented in Table 8. The classiﬁcation time
shown in Table 8 is the time taken for classiﬁcation when the SVM solver was trained with

89

Nandan, Khargonekar and Talathi

Data
set

D1

D5

D6

D9

Solver

AESVM
RfeatSVM2
E = 100
RfeatSVM2
E = 500
LIBSVM
AESVM
RfeatSVM2
E = 100
RfeatSVM2
E = 500
RfeatSVM2
E = 1000
RfeatSVM2
E = 5000
LIBSVM
AESVM
RfeatSVM2
E = 1000
RfeatSVM2
E = 5000
RfeatSVM2
E = 10000
LIBSVM
AESVM
RfeatSVM2
E = 1000
RfeatSVM2
E = 5000
RfeatSVM2
E = 10000
LIBSVM

ET S OT S Classiﬁcation
time (s)
6.1
0.9

1188.9 156
56.4
176.3

RM SE
(x102 )
0.22
50.3

max. acc.
(x102 )
94.2
63.5

mean & std.
acc. (x102 )
92.4,0.8
43.7,12.9

77.5

47.7

4.4

43.4

89.3

56,24.1

26.6
80.7

4.1
9.2

15
9.7
0.9

0.5
38.6

93.9
98.8
90.5

92.4,0.8
96.2,2.6
64.4,20

33.2

6.5

4.5

30.9

90.5

70.8,15.5

18.4

3.6

13.8

31.5

90.5

70.2,17.8

3.9

0.85

64.5

33.8

90.5

70.2,19.8

1.5
1.4
205.7 43.9

16.8
16
2.1

0
27.8

99
85.1
75.3

96.6 ,2.4
81.4,2.8
54.9,9.7

48.8

8.9

10.7

29.1

76.4

53.1,8.1

24.8

5.1

30.9

28.5

76.4

54,9.2

1.4
1.3
245.1 50

30.5
10.5
2.9

0
36.9

85.1
99.5
92.8

81.4,2.8
98.8,0.8
63.3,9.9

57.4

12

15.3

39

95.1

61.5,11.2

28.9

6.5

45.5

37.4

96.3

63.8,12.9

5.1

99.5

98.8,0.8

Table 8: Performance comparison of RfeatSVM2 (RfeatSVM solved using LIBLINEAR),
AESVM (with  = 10−2 ), and LIBSVM

its optimal hyper-parameters. For RfeatSVM2 the classiﬁcation time includes the time
taken to derive the random Fourier features of the test vectors.

The classiﬁcation time for RfeatSVM2 was generally less than AESVM, for small val-
ues of E. Moreover, it was found that RfeatSVM2 has signiﬁcantly higher training time

90

Fast SVM Training Using Approximate Extreme Points

speed-ups than AESVM for small values of E, except for D1 where AESVM was much
faster. However, with increasing E the classiﬁcation time and training time increased to
more than AESVM for most data sets. For all data sets, the RM SE , and maximum, mean
and standard deviation of accuracy of RfeatSVM2 were signiﬁcantly worse than AESVM.
Increasing the number of dimensions E, resulted in only a slight improvement in the clas-
siﬁcation performance of RfeatSVM2. An important observation was that the pro jected
data sets were found to be almost 100% dense, which results in large memory requirements
for RfeatSVM1 and RfeatSVM2. Even though, technically the value of E can be increased
arbitrarily, its value is practically limited by the memory requirements of RfeatSVM.

5.4 Performance with the Polynomial Kernel

To validate our proposal of AESVM as a fast alternative to SVM for all non-linear kernels,
we performed a few experiments with the polynomial kernel, k(x1 , x2 ) = (1 + xT
1 x2 )d . The
hyper-parameter grid composed of all twelve combinations of C (cid:48) = {2−4 , 2−2 , 1, 22} and
d = {2, 3, 4} was used to compute the solutions of AESVM and LIBSVM on the data sets
D1, D4 and D6. The results of the computation of the representative set using DeriveRS
are shown in Table 9. The parameters for DeriveRS were P = 105 , V = 103 and  = 10−2 ,
and the ﬁrst level segregation was performed using FLS2. The performance comparison of
AESVM and LIBSVM with the polynomial kernel is shown in Table 10. Like in the case
of the Gaussian kernel, we found that AESVM gave results similar to LIBSVM with the
polynomial kernel, while taking shorter training and classiﬁcation times.

M
N x100% (Computation time in seconds)
Data
d = 2
d = 3
d = 4
set
D1
D4
D6

8(109)
13.2(199) 26(638)
20.1(67) 48(260.1) 81.3(1166.4)
87.8(11) 84(12.5)
91(13.7)

Table 9: Results of DeriveRS for the polynomial kernel

6. Discussion

AESVM is a new problem formulation that is almost identical to, but less complex than, the
SVM primal problem. AESVM optimizes over only a subset of the training data set called
the representative set, and consequently, is expected to give fast convergence with most
SVM solvers. In contrast, the other studies mentioned in Section 2 are mostly algorithms
that solve the SVM primal or related problems. Methods such as RSVM also use diﬀerent
problem formulations. However, they require special algorithms to solve, unlike AESVM.
In fact, AESVM can be solved using many of the methods in Section 2. As described in
Corollary 5, there are some similarities between AESVM and the Gram matrix approxi-
mation methods discussed earlier. It would be interesting to see a comparison of AESVM,
with the core set based method proposed by G¨artner and Jaggi (2009). However, due to the

91

Nandan, Khargonekar and Talathi

Data
set

Solver

ET S OT S EC S C T S RM SE
(x102 )
0.13

6.4

2.7

2.6

D1

D4

D6

AESVM 21.1
LIBSVM
AESVM 7
LIBSVM
AESVM 3.8
LIBSVM

1.6

2.6

1.9

0.8

5.3

1.1

1.1

0.04

max. acc.
(x102 )
93.9
94.1
64.9
64.5
84.6
84.6

mean & std.
acc. (x102 )
93.4, 0.4
93.5, 0.4
61.2, 2.7
60.7, 2.5
81, 2.4
81, 2.3

Table 10: Performance comparison of AESVM (with  = 10−2 ), and LIBSVM with the
polynomial kernel

Figure 4: Plot of mean classiﬁcation accuracy of all SVM solvers

lack of availability of a software implementation and of published results on L1-SVM with
non-linear kernels using their approach, the authors ﬁnd such a comparison study beyond
the scope of this paper.
The theoretical and experimental results presented in this paper demonstrate that the
solutions of AESVM and SVM are similar in terms of the resulting classiﬁcation accuracy.
A summary of the experiments in Section 5, that compared an SMO based AESVM im-
plementation, CVM, BVM, LASVM, LIBSVM, SVMperf (with k = 1000) and RfeatSVM1,
is presented in Figures 4 to 7. The results of RfeatSVM2 are omitted from Figures 4 to
7, for ease of representation. It can be seen that AESVM typical ly gave classiﬁcation per-

92

D1D2D3D4D5D6D7D8D9020406080100Mean Accuracy x 102DatasetsAESVM, ε= 10−2CVMBVMLASVMSVMperfRfeatSVM1LIBSVMFast SVM Training Using Approximate Extreme Points

Figure 5: Plot of maximum classiﬁcation accuracy of all SVM solvers

formance similar to LIBSVM, while giving highest overal l training time speedup (OT S ).
Even though RfeatSVM2 gave higher OT S values in some cases, the degradation in clas-
siﬁcation accuracy was worse than in RfeatSVM1 as shown in Tables 7 and 8. AESVM
also gave competitively high classiﬁcation time speedup for the optimal hyper-parameters
(C T S ) in comparison with the other algorithms except SVMperf and RfeatSVM2. It was
found that the maximum classiﬁcation accuracies of all the algorithms except RfeatSVM1
and RfeatSVM2 were similar. RfeatSVM1 and RfeatSVM2, and in some cases CVM and
BVM, gave lower maximum classiﬁcation accuracies. Apart from the excellent experimen-
tal results for AESVM with the Gaussian kernel, AESVM also gave good results with the
polynomial kernel as described in Section 5.4.
The algorithm DeriveRS was generally found to be eﬃcient, especially for the lower
dimensional data sets D1-D5. For the high dimensional data sets D6-D9, the representative
set was almost the same size as the training data set, resulting in small gains in training and
classiﬁcation time speedups for AESVM. In particular, for D7 and D8 the representative set
computed by DeriveRS was almost 100% of the training set. A similar result was reported
for this data set in Beygelzimer et al. (2006), where a divide and conquer method was used
to speed up nearest neighbor search. Data set D8 is reported to have resulted in nearly
no speedup, compared to a speedup of almost one thousand for other data sets when their
method was used. Their analysis found that the data vectors in D8 were very distant
from each other in comparison with the other data sets.13 This observation can explain
the performance of DeriveRS on D8, as data vectors that are very distant from each other

13. This is indicated by the large expansion constant for D8 illustrated in Beygelzimer et al. (2006).

93

D1D2D3D4D5D6D7D8D9405060708090100Maximum Accuracy x 102DatasetsAESVM, ε= 10−2CVMBVMLASVMSVMperfRfeatSVM1LIBSVMNandan, Khargonekar and Talathi

Figure 6: Plot of overall training time speedup (compared to LIBSVM) of all SVM solvers

are expected to have large representative sets. It should be noted that irrespective of the
dimensionality of the data sets, AESVM always resulted in excellent performance in terms
of classiﬁcation accuracy. There seems to be no relation between data set density and the
performance of DeriveRS and AESVM.

The authors will provide the software implementation of AESVM and DeriveRS upon
request. Based on the presented results, we suggest the parameters  = 10−2 , P = 105
and V = 103 for DeriveRS. A possible extension of this paper is to apply the idea of
the representative set to other SVM variants and support vector clustering. It would be
interesting to investigate AESVM solvers implemented using methods other than SMO.
Modiﬁcations to DeriveRS using the methods in Section 2 might improve its performance
on high dimensional data sets. The authors will investigate improvements to DeriveRS and
the application of AESVM to the linear kernel in their future work.

Acknowledgments

Dr. Khargonekar acknowledges support from the Eckis professor endowment at the Uni-
versity of Florida. Dr. Talathi was partially supported by the Children’s Miracle Network,
and the Wilder Center of Excellence in Epilepsy Research. The authors acknowledge Mr.
Shivakeshavan R. Giridharan, for providing assistance with computational resources.

94

D1D2D3D4D5D6D7D8D9020406080100120140160180↑OTS value of AESVM for D3 is 488.5Overall Training Time SpeedupDatasetsAESVM, ε= 10−2CVMBVMLASVMSVMperfFast SVM Training Using Approximate Extreme Points

Figure 7: Plot of classiﬁcation time speedup for optimal hyper-parameters (compared to
LIBSVM) of all SVM solvers

References

K. P. Bennett and E. J. Bredensteiner. Duality and geometry in SVM classiﬁers.
In
Proceedings of the Seventeenth International Conference on Machine Learning, pages 57–
64, 2000.

A. Beygelzimer, S. Kakade, and J. Langford. Cover trees for nearest neighbor. In Proceedings
of the 23rd International Conference on Machine Learning, pages 97–104, 2006.

M. Blum, R. W. Floyd, V. Pratt, R. L. Rivest, and R. E. Tarjan. Time bounds for selection.
Journal of Computer and System Sciences, 7:448–461, August 1973.

A. Bordes, S. Ertekin, J. Weston, and L. Bottou. Fast kernel classiﬁers with online and
active learning. Journal of Machine Learning Research, 6:1579–1619, December 2005.

S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.

J. Cervantes, X. Li, W. Yu, and K. Li. Support vector machine classiﬁcation for large data
sets via minimum enclosing ball clustering. Neurocomputing, 71:611–619, January 2008.

C. C. Chang and C. J. Lin. IJCNN 2001 challenge: Generalization ability and text decoding.
In Proceedings of International Joint Conference on Neural Networks, volume 2, pages
1031 –1036, 2001.

95

D1D2D3D4D5D6D7D8D901020304050607080←CTS value of SVMperffor D4 is 277.7Classification Time SpeedupDatasetsAESVM, ε= 10−2CVMBVMLASVMSVMperfNandan, Khargonekar and Talathi

C.C Chang and C.J Lin. LIBSVM: A library for support vector machines. ACM Trans-
actions on Intel ligent Systems and Technology, 2:1–27, 2011. Software available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm.

K. L. Clarkson. Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm.
ACM Transaction on Algorithms, 6(4):63:1–63:30, September 2010.

R. Collobert, S. Bengio, and Y. Bengio. A parallel mixture of SVMs for very large scale
problems. Neural Computing, 14(5):1105–1114, 2002.

P. Drineas and M. W. Mahoney. On the Nystr¨om method for approximating a gram matrix
for improved kernel-based learning. Journal of Machine Learning Research, 6:2153–2175,
December 2005.

R. E. Fan, P. H. Chen, and C. J. Lin. Working set selection using second order information
for training support vector machines. Journal of Machine Learning Research, 6:1889–
1918, 2005.

R. E. Fan, K. W. Chang, C. J. Hsieh, X. R. Wang, and C. J. Lin. LIBLINEAR: A library
for large linear classiﬁcation. Journal of Machine Learning Research, 9:1871–1874, June
2008.

S. Fine and K. Scheinberg. Eﬃcient SVM training using low-rank kernel representations.
Journal of Machine Learning Research, 2:243–264, 2002.

V. Franc and S. Sonnenburg. Optimized cutting plane algorithm for support vector ma-
chines. In Proceedings of the 25th International Conference on Machine Learning, pages
320–327, 2008.

B. G¨artner and M. Jaggi. Coresets for polytope distance. In Proceedings of the 25th Annual
Symposium on Computational Geometry, pages 33–42, 2009.

J. Guo, N. Takahashi, and T. Nishi. A learning algorithm for improving the classiﬁcation
speed of support vector machines. In Proceedings of the 2005 European Conference on
Circuit Theory and Design, volume 3, pages 381 – 384, 2005.

C. J. Hsieh, K. W. Chang, C. J. Lin, S. S. Keerthi, and S. Sundarara jan. A dual coordinate
descent method for large-scale linear SVM.
In Proceedings of the 25th International
Conference on Machine Learning, pages 408–415, 2008.

T. Joachims. Making large-scale support vector machine learning practical. In Advances in
Kernel Methods, pages 169–184. MIT Press, 1999.

T. Joachims. Training linear SVMs in linear time. In Proceedings of the 12th ACM SIGKDD
International Conference on Know ledge Discovery and Data Mining, pages 217–226.
ACM, 2006.

T. Joachims and C. N. J. Yu. Sparse kernel SVMs via cutting-plane training. Machine
Learning, 76:179–193, September 2009.

96

Fast SVM Training Using Approximate Extreme Points

B. Kaluˇza, V. Mirchevska, E. Dovgan, M. Luˇstrek, and M. Gams. An agent-based approach
to care in independent living. In Ambient Intel ligence, pages 177–186. Springer, 2010.

J. Kelley. The cutting-plane method for solving convex programs. Journal of the Society
for Industrial and Applied Mathematics, 8(4):703–712, 1960.

Y. Lecun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning applied to docu-
ment recognition. Proceedings of the IEEE, 86(11):2278 –2324, 1998.

Y. J. Lee and O. L. Mangasarian. Rsvm: Reduced support vector machines.
In Pro-
ceedings of the First SIAM International Conference on Data Mining, pages 5–7. SIAM
Philadelphia, 2001.

M. Nandan, S. S. Talathi, S. Myers, W. L. Ditto, P. P. Khargonekar, and P. R. Carney.
Support vector machines for seizure detection in an animal model of chronic epilepsy.
Journal of Neural Engineering, 7(3), 2010.

E. Osuna and O. Castro. Convex hull in feature space for support vector machines. In Pro-
ceedings of the 8th Ibero-American Conference on AI: Advances in Artiﬁcial Intel ligence,
pages 411–419, 2002.

E. Osuna, R. Freund, and F. Girosi. Training support vector machines: An application to
face detection. In IEEE Computer Society Conference on Computer Vision and Pattern
Recognition, pages 130 –136, 1997.

D. Pavlov, D. Chudova, and P. Smyth. Towards scalable support vector machines us-
ing squashing. In Proceedings of the Sixth ACM SIGKDD International Conference on
Know ledge Discovery and Data Mining, pages 295–299. ACM, 2000.

J. C. Platt. Fast training of support vector machines using sequential minimal optimization.
In Advances in Kernel Methods, pages 185–208. MIT Press, 1999.

A. Rahimi and B. Recht. Random features for large-scale kernel machines. Advances in
Neural Information Processing Systems, pages 1177–1184, 2007.

R. T. Rockafellar. Convex Analysis. Princeton University Press, 1996.

B. Sch¨olkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regular-
ization, Optimization, and Beyond. MIT Press, 2001.

B. Sch¨olkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett. New support vector
algorithms. Neural Computation, 12(5):1207–1245, 2000.

S. Shalev-Shwartz and N. Srebro. SVM optimization: Inverse dependence on training set
size. In Proceedings of the 25th International Conference on Machine Learning, pages
928–935, 2008.

S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal estimated sub-
gradient solver for SVM. Mathematical Programming, 127:3–30, March 2011.

97

Nandan, Khargonekar and Talathi

S. S. Talathi, D. U. Hwang, M. L. Spano, J. Simonotto, M. D. Furman, S. M. Myers, J. T.
Winters, W. L. Ditto, and P. R. Carney. Non-parametric early seizure detection in an
animal model of temporal lobe epilepsy. Journal of Neural Engineering, 5:85–98, 2008.

M. Tavallaee, E. Bagheri, W. Lu, and A. A. Ghorbani. A detailed analysis of the KDD CUP
99 data set. In Proceedings of the 2009 IEEE Symposium Computational Intel ligence for
Security and Defense Applications, pages 53–58, 2009.

D. Tax and R. Duin. Support vector data description. Machine Learning, 54(1):45–66,
2004.

C. H. Teo, S. V. N. Vishwanthan, A. J. Smola, and Q. V. Le. Bundle methods for regularized
risk minimization. Journal of Machine Learning Research, 11:311–365, 2010.

I. W. Tsang, J. T. Kwok, P. Cheung, and N. Cristianini. Core vector machines: Fast SVM
training on very large data sets. Journal of Machine Learning Research, 6:363–392, 2005.

I. W. Tsang, A. Kocsor, and J. T. Kwok. Simpler core vector machines with enclosing
balls. In Proceedings of the 24th International Conference on Machine Learning, pages
911–918, 2007.

H. Yu, J. Yang, and J. Han. Classifying large data sets using SVMs with hierarchical clus-
ters. In Proceedings of the Ninth ACM SIGKDD International Conference on Know ledge
Discovery and Data Mining, pages 306–315, 2003.

G. X. Yuan, C. H. Ho, and C. J. Lin. An improved GLMNET for l1-regularized logis-
tic regression. In Proceedings of the 17th ACM SIGKDD International Conference on
Know ledge Discovery and Data Mining, pages 33–41, 2011.

T. Zhang. Solving large scale linear prediction problems using stochastic gradient descent
algorithms. In Proceedings of the 21st International Conference on Machine Learning,
pages 919–926, 2004.

98

Journal of Machine Learning Research 16 (2015) 155-186

Submitted 7/13; Revised 7/14; Published 2/15

Online Learning via Sequential Complexities

Alexander Rakhlin
Department of Statistics
University of Pennsylvania
Philadelphia, PA 19104

Karthik Sridharan
Department of Computer Science
Cornel l University
Ithaca, NY 14853

Ambuj Tewari
Department of Statistics
University of Michigan
Ann Arbor, MI 48109

Editor: Mehryar Mohri

rakhlin@wharton.upenn.edu

skarthik@wharton.upenn.edu

tewaria@umich.edu

Abstract
We consider the problem of sequential prediction and provide tools to study the minimax
value of the associated game. Classical statistical learning theory provides several useful
complexity measures to study learning with i.i.d. data. Our proposed sequential complex-
ities can be seen as extensions of these measures to the sequential setting. The developed
theory is shown to yield precise learning guarantees for the problem of sequential predic-
tion. In particular, we show necessary and suﬃcient conditions for online learnability in
the setting of supervised learning. Several examples show the utility of our framework: we
can establish learnability without having to exhibit an explicit online learning algorithm.
Keywords: online learning, sequential complexities, regret minimization

1. Introduction

This paper is concerned with sequential prediction problems where no probabilistic assump-
tions are made regarding the data generating mechanism. Our viewpoint is expressed well
by the following quotation from Cover and Shenhar (1977):

“We are interested in sequential prediction procedures that exploit any ap-
parent order in the sequence. We do not assume the existence of any underlying
distributions, but assume that the sequence is an outcome of a game against a
malevolent intelligent nature.”

We will, in fact, take the game theoretic viewpoint seriously. All our investigations will
proceed by analyzing the minimax value of a repeated game between a player or learner
develop the theory in a somewhat abstract setting. Towards this end, ﬁx the sets F and
and a “malevolent intelligent nature”, or the adversary.
Even though we have the setting of prediction problems in mind, it will be useful to
©2015 Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari.

Rakhlin, Sridharan and Tewari
Z , as well as a loss function (cid:96) ∶ F × Z → R, and consider the following T -round repeated
round t ∈ {1, . . . , T }, the learner chooses ft ∈ F , the adversary picks zt ∈ Z , and the learner
suﬀers loss (cid:96)(ft , zt ). At the end of T rounds we deﬁne regret
two-player game, which we term the online learning or sequential prediction model. On
R(f1∶T , z1∶T ) á TQ
(cid:96)(ft , zt ) − inf
(cid:96)(f , zt )
f ∈F TQ
t=1
t=1
best ﬁxed decision. For the given pair (F , Z ), the problem is said to be online learnable
as the diﬀerence between the cumulative loss of the player and the cumulative loss of the
if there exists an algorithm for the learner such that regret grows sublinearly in the time
horizon T , no matter what strategy the adversary employs.
The origin of the online learning (or sequential prediction) model can be traced back to
the work of Robbins (1950) on compound statistical decision problems. Some of the earliest
sequential prediction algorithms were proposed by Blackwell (1956a,b) and Hannan (1957).
Blackwell’s method was based on his celebrated approachability theorem whereas Hannan’s
was based on minimizing a randomly perturbed sum of previous losses. Hannan’s ideas
were to later resurface in the inﬂuential Follow-the-Perturbed-Leader family (Kalai and
Vempala, 2005) of online learning algorithms. The seminal ideas in the work of Robbins,
Blackwell and Hannan led to further developments in many diﬀerent ﬁelds. Cover (1967),
Davisson (1973), Ziv and Lempel (1977), Rissanen (1984), Feder et al. (1992), and others laid
the foundation of universal coding, compression and prediction in the Information Theory
literature. Within Computer Science, Littlestone and Warmuth (1994), Cesa-Bianchi et al.
(1997), Vovk (1998), and others studied the online learning model and the prediction with
expert advice framework. The connections between regret minimization and convergence
to equilibria was studied in Economics by Foster and Vohra (1997), Hart and Mas-Colell
(2000) and others.
We have no doubt left out many interesting works above. But even our partial list will
convince the reader that research in online learning and sequential prediction has beneﬁted
from contributions by researchers from a variety of ﬁelds including Computer Science, Eco-
nomics, Information Theory, and Statistics. For an excellent synthesis and presentation of
results from these diﬀerent ﬁelds we refer the reader to the book by Cesa-Bianchi and Lugosi
(2006). Many of the ideas in the ﬁeld are constructive, resulting in beautiful algorithms,
or algorithmic techniques, associated with names such as Follow-the-Regularized-Leader,
Follow-the-Perturbed-Leader, Weighted Ma jority, Hedge, and Online Gradient Descent.
However, analyzing speciﬁc algorithms has obvious disadvantages. The algorithm may not
be “optimal” for the task at hand. Even if it is optimal, one cannot prove that fact unless
one develops tools for analyzing the inherent complexity of the online learning problem.
Our goal is precisely to provide such tools. We will begin by deﬁning the minimax value
of the game underlying the abstract online learning model. Then we will develop tools for
controlling the minimax value resulting in a theory that parallels statistical learning theory.
In particular, we develop analogues of combinatorial dimensions, covering numbers, and
Rademacher complexities. We will also provide results relating these complexities.
Note that our approach is non-constructive : controlling the sequential complexities
mentioned above will only guarantee the existence of a good online learning algorithm but

156

Online Learning via Sequential Complexities

will not explicitly create one. However, it turns out that that the minimax point of view
can indeed lead to constructive algorithms as shown by Rakhlin et al. (2012).

2. Minimax Value and Online Learnability
F is a subset of a separable metric space. Let Q be the set of probability measures on F
To proceed further in our analysis of the minimax value of the repeated game between the
and assume that Q is weakly compact. In order to allow randomized prediction, we allow
learner and the adversary, we need to make a few technical assumptions. We assume that
the learner to choose a distribution qt ∈ Q on every round. The minimax value of the game
 inf
VT (F , Z ) á inf
 TQ
(cid:96)(ft , zt ) − inf
(cid:96)(f , zt ) .
is then deﬁned as
f ∈F TQ
q1 ∈Q sup
z1 ∈Z E
qT ∈Q sup
zT ∈Z E
f1 ∼q1
fT ∼qT
t=1
t=1
(1)
f ∼q stands for the expectation operator integrating out the ran-
Henceforth, the notation E
choose each zt based on the history of moves f1∶t−1 and z1∶t−1 .
dom variable f with distribution q . We consider here the adaptive adversary who gets to
The ﬁrst key step in the study of the value of the game is to appeal to the minimax
theorem and exchange the pairs of inﬁma and suprema in (1). This dual formulation is
assumption of weak compactness of Q and lower semi-continuity of the loss function.1 Under
easier to analyze because the choice of the player comes after the choice of the mixed
strategy of the adversary. We remark that the minimax theorem holds under a very general
these conditions, we can appeal to Theorem 1 stated below, which is adapted for our needs
Theorem 1 Let F and Z be the sets of moves for the two players, satisfying the necessary
from the work of Abernethy et al. (2009).
conditions for the minimax theorem to hold. Denote by Q and P the sets of probability
measures (mixed strategies) on F and Z , respectively. Then
 sup
VT (F , Z ) = sup
(cid:96)(f , zt ) ,
[(cid:96)(ft , zt )] − inf
 TQ
f ∈F TQ
z1 ∼p1
zT ∼pT
zt ∼pt
ft ∈F E
t=1
t=1
E
E
inf
where suprema over pt range over al l distributions in P .
p1
pT
VT (F , Z ), taking (2) as the starting point.
The question of learnability in the online learning model is now reduced to the study of
Deﬁnition 2 A class F is said to be online learnable with respect to the given Z and (cid:96) if
T →∞ VT (F , Z )
≤ 0 .
lim sup
T
Note that our notion of learnability is related to, but distinct from, Hannan consistency
(Hannan, 1957; Cesa-Bianchi and Lugosi, 2006). The latter notion requires the iterated
game to go on for an inﬁnite number of rounds and is formulated in terms of almost sure

(2)

1. We refer to Appendix A for a precise statement of the minimax theorem, as well as suﬃcient conditions.

157

Rakhlin, Sridharan and Tewari

convergence. In contrast, we consider a distinct game for each T and look at expected regret.
Nevertheless, it is possible to obtain Hannan consistency using the techniques developed in
is allowed to make decisions in a larger set G , while the best-in-hindsight term in the regret
this paper by considering a slightly diﬀerent game (Rakhlin et al., 2011).
deﬁnition is computed with respect to F ⊆ G . Such a setting—interesting especially with
We also remark that the statements in this paper extend to the case when the learner
choose Y ⊂ R, Z = X × Y , F ⊆ Y X = G and (cid:96)(f , (x, y)) =  f (x) − y  . This setting will be
regard to computational concerns—is termed improper learning. For example, prediction
studied later in the paper. Note that in the proper learning scenario, VT (F , Z ) ≥ 0 (e.g.,
with side information (or, the supervised learning problem) is one such case, where we
This paper is aimed at understanding the value of the game VT (F , Z ) for various func-
since all zt ’s can be chosen to be the same), and thus the “lim sup” in Deﬁnition 2 can be
tion classes F . Since our focus is on the complexity of F , we shall often write VT (F ) keeping
simply replaced with the limit being equal to zero.
the dependence on Z (and (cid:96)) implicit. As we show, the sequential complexity notions—
Numbers—also give us a handle on the value VT (F ). In the next section, we brieﬂy deﬁne
that were shown by Rakhlin et al. (2014) to characterize uniform martingale Laws of Large
these sequential complexity notions and mention some of the key relations between them.
A more detailed account of the relationships between sequential complexity measures along
with complete proofs can be found in (Rakhlin et al., 2014).

3. Sequential Complexities

Unlike the well-studied statistical learning scenario with i.i.d. data, the online learning
problem possesses a certain sequential dependence. Such dependence cannot be captured
by classical notions of complexity that are based on a batch of data given as a tuple of T
examples. A basic unit that does capture temporal dependence is a binary tree. Surprisingly,
A Z -valued tree z of depth T is a complete rooted binary tree with nodes labeled by
for the sequential prediction problems considered in this paper, one need not look further
elements of Z . Such a tree z is identiﬁed with the sequence (z1 , . . . , zT ) of labeling functions
than binary trees to capture the relevant complexity.
zi ∶ {±1}i−1 → Z which provide the labels for each node. Therefore, z1 ∈ Z is the label for
the root of the tree, while zi for i > 1 is the label of the node obtained by following the
path of length i − 1 from the root, with +1 indicating ‘right’ and −1 indicating ‘left’. A path
of length T is given by the sequence  = (1 , . . . , T ) ∈ {±1}T . For brevity, we shall often
write zt (), where  = (1 , . . . , T ), but it is understood that zt depends only on the preﬁx
(1 , . . . , t−1 ).
Now, let 1 , . . . , T be independent Rademacher random variables. Given a Z -valued tree
z of depth T , we deﬁne the sequential Rademacher complexity of a function class G ⊆ RZ
on a Z -valued tree z as
RT (G , z) á E sup
tg(zt ()) ,
TQ
g∈G 1
t=1
and we denote by RT (G ) = supz RT (G , z) its supremum over all Z -valued trees of depth T .
T
The importance of the introduced notion stems from the following result (Rakhlin et al.,

158

Online Learning via Sequential Complexities
2014, Theorem 2): for any distribution over a sequence (Z1 , . . . , ZT ), we have
E g(Zt ) Z t−1 (cid:6) − g(Zt ) ≤ 2 RT (G ) ,
E sup
TQ
g∈G 1
t=1
where Z t−1 = (Z1 , . . . , Zt−1 ). In other words, the martingale version of the uniform devi-
(3)
T
sequences in Z T . It then follows that a uniform martingale Law of Large Numbers holds
ations of means from expectations is controlled by the worst-case sequential Rademacher
for G if and only if RT (G ) → 0. For i.i.d. random variables, a similar statement can be
complexity. A matching lower bound also holds for the supremum over distributions on
made in terms of the classical Rademacher complexity, and so one might hope that many
other complexity notions from empirical process theory have martingale (or we may say,
sequential) analogues. Luckily, this is indeed the case (Rakhlin et al., 2014). As we show
in this paper, these generalizations of the classical notions also give a handle on (as well
as necessary and suﬃcient conditions for) online learnability, thus painting a picture that
completely parallels statistical learning theory. But before we present our main results, let
us recall some key deﬁnitions and results in (Rakhlin et al., 2014).
In providing further upper bounds on sequential Rademacher complexity, the following
respect to (cid:96)p norm) of G ⊆ RZ on a tree z of depth T if
deﬁnitions of an “eﬀective size” of a function class generalize the classical notions of a
covering number. A set V of R-valued trees of depth T is a (sequential) α-cover (with
 vt () − g(zt ()) p1~p ≤ α.
 1
∀g ∈ G , ∀ ∈ {±1}T , ∃v ∈ V s.t.
TQ
t=1
The (sequential) covering number of a function class G on a given tree z is deﬁned as
T
Np (α, G , z) á min { V   ∶ V is an α-cover w.r.t. (cid:96)p norm of G on z} .
It is straightforward to check that Np (α, G , z) ≤ Nq (α, G , z) whenever 1 ≤ p ≤ q ≤ ∞.
Further deﬁne Np (α, G , T ) = supz Np (α, G , z), the maximal (cid:96)p covering number of G over
depth T trees. For a class G of binary-valued functions, we also deﬁne a so-called 0-cover
(or, cover at scale 0), denoted by N (0, G , z), as equal to any Np (0, G , z). The deﬁnition of
a 0-cover can be seen as the correct analogue of the size of a projection of G onto a tuple of
When G ⊆ [−1, 1]Z is a ﬁnite class of bounded functions, one can show (Rakhlin et al.,
points in the i.i.d. case. The size of this pro jection in the i.i.d. case was the starting point
of the work of Vapnik and Chervonenkis.
RT (G , z) ≤ 
2 log  G  
2014, Lemma 1) that
T
a bound that should (correctly) remind the reader of the Exponential Weights regret bound.
any G ⊆ [−1, 1]Z , for any α > 0,
With the deﬁnition of an α-cover with respect to (cid:96)1 norm, one can easily extend (4) beyond
RT (G , z) ≤ α + 
the ﬁnite case. Immediately from the deﬁnition of (cid:96)1 covering number, it follows that for
2 log N1 (α, G , z)
T
159

(5)

,

(4)

Rakhlin, Sridharan and Tewari

entropy integral bound. For p ≥ 1, the integrated complexity of a function class G ⊆ [−1, 1]Z
(Rakhlin et al., 2014, Eq. 9). A tighter control is obtained by integrating the covering
on a Z -valued tree of depth T is deﬁned as
numbers at diﬀerent scales. To this end, consider the following analogue of the Dudley

4α + 12√
T (G , z) á inf
log Np (δ, G , z) dδ
T S 1
α≥0
Dp
T (G , z), with D2
T (G , z) denoted simply by DT (G , z). We have previously
T (G ) = supz Dp
(6)
α
shown (Rakhlin et al., 2014, Theorem 3) that, for any function class G ⊆ [−1, 1]Z and any
Z -valued tree z of depth T ,
and Dp
RT (G , z) ≤ DT (G , z).
We next turn to the description of sequential combinatorial parameters. A Z -valued
(7)
tree z of depth d is shattered by a function class G ⊆ {±1}Z if for all  ∈ {±1}d , there exists
g ∈ G such that g(zt ()) = t for all t ∈ [d]. The Littlestone dimension Ldim(G , Z ) is the
largest positive integer d such that G shatters a Z -valued tree of depth d (Littlestone, 1988;
follows. A Z -valued tree z of depth d is α-shattered by a function class G ⊆ RZ if there
Ben-David et al., 2009). The scale-sensitive version of Littlestone dimension is deﬁned as
∀ ∈ {±1}d , ∃g ∈ G s.t. ∀t ∈ [d], t (g(zt ()) − st ()) ≥ α~2.
exists an R-valued tree s of depth d such that
fatα (G , Z ) at scale α is the largest d such that G α-shatters a Z -valued tree of depth d.
The tree s will be called a witness to shattering. The (sequential) fat-shattering dimension
The notions introduced above can be viewed as sequential generalizations of the VC
dimension and the fat-shattering dimension where tuples of points get replaced by complete
binary trees. In fact, one recovers the classical notions if the tree z in the above deﬁnitions is
restricted to have the same values within a level (hence, no temporal dependence). Crucially,
First, let G ⊆ {0, . . . , k}Z be a class of functions with fat2 (G ) = d. Then, it can be shown
the sequential combinatorial analogues provide control for the growth of sequential covering
(Rakhlin et al., 2014, Theorem 4) that for any T ≥ 1,
numbers, justifying the deﬁnitions.
T
N∞ (1~2, G , T ) ≤ dQ
i k i ≤ (ekT )d .
i=0
For the second result (Rakhlin et al., 2014, Corollary 1), suppose G is a class of [−1, 1]-valued
functions on Z . Then, for any α > 0, and any T ≥ 1,
α fatα (G )
N∞ (α, G , T ) ≤  2eT
(8)
.
parameter (Rakhlin et al., 2014, Theorem 5). For a class G ⊆ {0, . . . , k}Z with fat1 (G ) = d,
Finally, we recall a bound on the size of the 0-cover in terms of the fat1 combinatorial
T
N (0, G , T ) ≤ dQ
i k i ≤ (ekT )d .
we have
i=0
160

(9)

Online Learning via Sequential Complexities
In particular, for k = 1 (that is, binary classiﬁcation) we have fat1 (G ) = Ldim(G ). The
inequality (9) is therefore a sequential analogue of the celebrated Vapnik-Chervonenkis-
Sauer-Shelah lemma.

4. Structural Properties
properties that RT (G ) satisﬁes. These properties allow one to establish online learnability
For the examples developed in this paper, it will be crucial to exploit a number of useful
for complex function classes even if no explicit learning algorithms are available.
Lemma 3 Let F , G ⊆ RZ and let conv(G ) denote the convex hul l of G . Let z be any Z -
We ﬁrst state some properties that are easily proved but are nevertheless very useful.
1. If F ⊆ G , then RT (F , z) ≤ RT (G , z).
valued tree of depth T . Then the fol lowing properties hold.
2. RT (conv(G ), z) = RT (G , z)
3. RT (cG , z) =  c RT (G , z) for al l c ∈ R.
4. For any h ∶ Z → R, RT (G + h, z) = RT (G , z) where G + h = {g + h ∶ g ∈ G }.
These properties match those of the classical Rademacher complexity (Bartlett and Mendel-
son, 2003) and can be proved in essentially the same way (we therefore skip the straight-
forward proofs).
The next property is a key tool for many of the applications: it allows us to bound the
sequential Rademacher complexity for the Cartesian product of function classes composed
Lemma 4 Let G = G1 × . . . × Gk where each Gj ⊆ [−1, 1]Z . Further, let φ ∶ Rk × Z → R be
with a Lipschitz mapping in terms of complexities of the individual classes.
such that φ(⋅, z) is L-Lipschitz with respect to  ⋅ ∞ for al l z ∈ Z , and let
φ ○ G = {z  φ((g1 (z), . . . , gk (z)), z) ∶ gj ∈ Gj } .
√
RT (φ ○ G ) ≤ 8 L 1 + 4
2 log3~2 (eT 2 ) ∑k
j=1 RT (Gj )
Then we have
as long as RT (Gj ) ≥ 1~T for each j .
Let us explicitly state the more familiar contraction property, an immediate corollary
Corollary 5 Fix a class G ⊆ [−1, 1]Z with RT (G ) ≥ 1~T and a function φ ∶ R × Z → R.
of the above result.
Assume φ(⋅, z) is L-Lipschitz for al l z ∈ Z . Then
√
2 log3~2 (eT 2 ) ⋅ RT (G ),
RT (φ ○ G ) ≤ 8 L 1 + 4
where φ ○ G = {z  φ(g(z), z) ∶ g ∈ G }.
We state another useful corollary of Lemma 4.

161

Rakhlin, Sridharan and Tewari
Corollary 6 For a ﬁxed binary function b ∶ {±1}k → {±1} and classes G1 , . . . , Gk of {±1}-
RT (b(G1 , . . . , Gk )) ≤ O log3~2 (T ) ∑k
j=1 RT (Gj ).
valued functions,
Note that, in the classical case, the Lipschitz contraction property holds without any
extra poly-logarithmic factors in T (Ledoux and Talagrand, 1991). It is an open question
whether the poly-logarithmic factors can be removed in the results above.
It is worth
pointing out ahead of time that Theorem 8 below—in the setting of supervised learning with
convex Lipschitz loss—does allow us to avoid the extraneous factor that would otherwise
appear from a combination of Theorem 7 and Corollary 5.

5. Main Results

We now relate the value of the game to the worst case expected value of the supremum
of an empirical process. However, unlike empirical processes that involve i.i.d. sums, our
process involves a sum of martingale diﬀerences. In view of (3), the expected supremum
can be further upper-bounded by the sequential Rademacher complexity.
E[g(Zt ) Z1 , . . . , Zt−1 ] − g(Zt ) ≤ 2 RT ((cid:96)(F )),
g∈(cid:96)(F )  1
T VT (F ) ≤ sup
TQ
Theorem 7 The minimax value is bounded as
t=1
1
E sup
where (cid:96)(F ) = {(cid:96)(f , ⋅) ∶ f ∈ F } and the supremum is taken over al l distributions P over
P
T
(Z1 , . . . , ZT ).
We can now employ the tools developed earlier in the paper to upper bound the value of
the game. Interestingly, any non-trivial upper bound guarantees existence of a prediction
strategy that has sublinear regret irrespective of the sequence of the moves of the adversary.
This complexity-based approach of establishing learnability should be contrasted with the
purely algorithm-based approaches found in the literature.
5.1 Supervised Learning
In this improper learning scenario, the learner at time t picks a function ft ∶ X → R and the
adversary provides the input target pair zt = (xt , yt ) ∈ X × Y where Y ⊂ R. In particular,
In this subsection we study the supervised learning problem mentioned earlier in the paper.
the binary classiﬁcation problem corresponds to the case Y = {±1}. Let F ⊆ Y X and let us
ﬁx the absolute value loss function (cid:96)( ˆy , y) =   ˆy − y  . While we focus on the absolute loss, it
is easy to see that all the results hold (with modiﬁed rates) for any loss (cid:96)( ˆy , y) such that
for all ˆy and y , φ((cid:96)( ˆy , y)) ≤   ˆy − y   ≤ Φ((cid:96)( ˆy , y)) where Φ and φ are monotonically increasing
functions. For instance, the squared loss ( ˆy − y)2 is a classic example.
We now observe that the value of the improper supervised learning game can be equiv-
(cid:96)(f (xt ), yt ) ,
(cid:96)( ˆyt , yt ) − inf
 TQ
T (F ) = sup
V S
 sup
alently written as
f ∈F TQ
t=1
t=1
q1 ∈ ˜Q sup
inf
y1
x1
xT

qT ∈ ˜Q sup
inf
yT

ˆy1 ∼q1
E

ˆyT ∼qT
E

162

(10)

(11)

Online Learning via Sequential Complexities
where ˜Q denotes the set of probability distributions over Y and ˆyt has distribution qt . This
equivalence is easy to verify: we may view the choice ft ∶ X → Y as pre-specifying predictions
ft (x) for all the possible x ∈ X , while alternatively we can simply make the choice ˆyt ∈ Y
having observed the particular move xt ∈ X . The advantage of rewriting the game in the
shown for the set of distributions on the original space of functions of the type X → Y .
form (10) is that the minimax theorem only needs to be applied to the pair ˆyt and yt , given
the ﬁxed choice xt . The minimax theorem then holds even if weak compactness cannot be
An examination of the proof of Theorem 7 reveals that the value (10) is upper bounded
in exactly the same way, and the side information simply appears as an additional tree x
in sequential Rademacher complexity, giving us:
t (cid:96)(f (xt ()), yt ()) .
E sup
T (F ) ≤ 2 sup
T V S
TQ
f ∈F 1
t=1
1
T
x,y
However, for the supervised learning setting, we can strengthen Theorem 7. The following
theorem allows us to remove any convex Lipschitz loss (including the absolute loss) before
Theorem 8 Let Y = [−1, 1] and suppose, for any y ∈ Y , (cid:96)(⋅, y) is convex and L-Lipschitz.
passing to the sequential Rademacher complexity.
T (F ) ≤ 2 L RT (F ).
T V S
Then the minimax value of a supervised learning problem is upper bounded as
1
We remark that the contraction property for sequential Rademacher complexity, stated in
Section 4, yields an extraneous logarithmic factor when applied to (11); here, we achieve
the desired bound by removing the Lipschitz function directly during the symmetrization
step.
Proposition 9 Consider the supervised learning problem with a function class F ⊆ [−1, 1]X
Armed with the theorem, we now prove the following result.
and absolute loss (cid:96)( ˆy , y) =   ˆy − y  . Then, for any T ≥ 1, we have

 ≤ RT (F ) ≤ 1
α
min {fatα , T }
√
T (F ) ≤ 2RT (F ) ≤ 2DT (F )
T V S

1
4α + 12√
 ,
sup
≤ 2 inf
β  dβ
fatβ log  2eT
T
4
2
T S 1
α
where fatα = fatα (F ).
α
α
The proposition above implies that ﬁniteness of the fat-shattering dimension at all scales
is necessary and suﬃcient for online learnability of the supervised learning problem. Fur-
ther, all the complexity notions introduced so far are within a poly-logarithmic factor from
each other whenever the problem is learnable. These results are summarized in the next
Theorem 10 For any function class F ⊆ [−1, 1]X , the fol lowing statements are equivalent
theorem:
163

(12)

Rakhlin, Sridharan and Tewari
1. Function class F is online learnable in the supervised setting with absolute loss.
2. Sequential Rademacher complexity satisﬁes limT →∞ RT (F ) = 0.
3. For any α > 0, the scale-sensitive dimension fatα (F ) is ﬁnite.
T (F ), the sequential Rademacher complexity RT (F ), and the integrated complexity DT (F )
V S
are within a multiplicative factor of O(log3~2 T ) of each other.
Moreover, if the function class is online learnable, then the value of the supervised game
Remark 11 Additional ly, the three statements of Theorem 10 are equivalent to F satisfy-
ing a martingale version of the uniform Law of Large Numbers. This property is termed
Sequential Uniform Convergence by Rakhlin et al. (2014), and we refer to their paper for
For binary classiﬁcation, we write V Binary
for V S
more details.
T . This case has been investigated
T
thoroughly by Ben-David et al. (2009) and indeed served as a key motivation for this paper.
setting is simply the 0-1 loss (cid:96)( ˆy , y) = 1 { ˆy ≠ y}, where 1 {U } is 1 if U is true and 0 otherwise.
As a consequence of Proposition 9 and (9), we have a tight control on the value of the game
for the binary classiﬁcation problem. Note that the absolute loss in the binary classiﬁcation
Corollary 12 For the binary classiﬁcation problem with function class F and the 0-1 loss,


T Ldim(F ) log T
(F ) ≤ K2
T min {Ldim(F ), T } ≤ V Binary
we have
for some universal constants K1 , K2 > 0.
K1
T
Both the upper and the lower bound in the above result were originally derived in Ben-
David et al. (2009). Notably, we achieved the same bounds non-constructively through
purely combinatorial and covering number arguments.
It is natural to ask whether being able to learn in the online model is diﬀerent from
learning in the i.i.d. model (in the distribution-free supervised setting). The standard
example that exhibits a gap between the two frameworks (e.g., Littlestone, 1988; Ben-David
F = {fθ (x) = 1 {x ≤ θ} ∶ θ ∈ [0, 1]}
et al., 2009) is binary classiﬁcation using the class of step functions
on [0, 1]. This class has VC dimension 1, but is not learnable in the online setting. Indeed,
related class of “ramp” functions with slope L > 0
it is possible to verify that the Littlestone dimension is inﬁnite. Interestingly, the closely-
FL = fθ (x) = 1 {x ≤ θ} + (1 − L(x − θ))1 {θ < x ≤ θ + 1~L} ∶ θ ∈ [0, 1]
is learnable (say for supervised learning using absolute loss) in the online setting (and hence
also in the i.i.d. case). Furthermore, the larger class of all bounded L-Lipschitz functions
on a bounded interval is also online learnable (see Eq. 14 and proof of Proposition 18).
Once again, we are able to make these statements from purely complexity-based considera-
tions, without exhibiting an algorithm. Further examples where we can demonstrate online
learnability are explored in Section 6.

164

Online Learning via Sequential Complexities

5.2 Online Convex Optimization

Over the past decade, Online Convex Optimization (OCO) has emerged as a uniﬁed on-
line learning framework (Zinkevich, 2003; Shalev-Shwartz, 2011). Various methods, such
as Exponential Weights, can be viewed as instances of online mirror descent, solving the
associated OCO problem. Much research eﬀort has been devoted to understanding this
abstract and simpliﬁed setting. It is tempting to say that any problem of online learning,
as deﬁned in the Introduction, can be viewed as OCO (in fact, online linear optimization)
unnecessary dependence on the number of functions in the class F . Nevertheless, OCO
over the set of probability distributions; however, one should also recognize that by lineariz-
ing the problem, any interesting structure is lost and one instead suﬀers from the possibly
is a central part of the recent literature, and we will study this scenario using techniques
the learner F is a bounded closed convex subset of a Banach space (B ,  ⋅ ) with f  ≤ D
developed in this paper.
for all f ∈ F (the reader can think of Rd equipped with an (cid:96)p norm for simplicity). Let  ⋅ 
The standard setting of online convex optimization is as follows. The set of moves of
be the dual norm. The adversary’s set Z consists of convex G-Lipschitz (with respect to
 ⋅  ) functions over F :
Z = Zcvx = {g ∶ F → R ∶ g convex and G-Lipschitz w.r.t.  ⋅ } .
Let the loss function be (cid:96)(f , g) = g(f ), the evaluation of the adversarially chosen function
Z = Zlin = {f  f , z  ∶ z ≤ G}
at f . For the particular case of online linear optimization, we instead take
with Z now a subset of the dual space. It is well-known (e.g., Abernethy et al., 2008) that
Zcvx ) is as hard as the corresponding linear optimization problem with Zlin if one considers
the online convex optimization problem (without further assumptions on the functions in
Lemma 13 Suppose F , Zcvx , Zlin be deﬁned as above. Then we have
deterministic algorithms. The same trivially extends to randomized methods:
VT (F , Zcvx ) = VT (F , Zlin ) .
OCO. The reader may wonder why we do not directly try to bound the value VT (F , Zcvx )
by RT (F , Zcvx ). In fact, this proof strategy cannot give a non-trivial bound if F is a subset
We will now show how to use the above result to derive minimax regret guarantees for
of a high-dimensional (or inﬁnite-dimensional) space (Shalev-Shwartz et al., 2009, Sec. 4.1).
A function Ψ ∶ F → R is (σ, q)-uniformly convex (for q ∈ [2, ∞)) on F with respect to a
Instead, we use the lemma above to bound the value of the game where adversary plays
norm  ⋅  if, for all θ ∈ [0, 1] and f1 , f2 ∈ F ,
convex functions with that of the game where adversary plays linear functions.
Ψ(θf1 + (1 − θ)f2 ) ≤ θΨ(f1 ) + (1 − θ)Ψ(f2 ) − σ θ (1 − θ)
f1 − f2 q .
A (σ, 2)-uniformly convex function will be called σ -strongly convex.
q
165

Rakhlin, Sridharan and Tewari

exploited in its proof is that Ψ is (σ, q)-uniformly convex with respect to  ⋅  if and only if
We will give examples shortly but we ﬁrst state a proposition that is useful to bound
Ψ is (1~σ, p)-uniformly smooth with respect to  ⋅  where 1~p + 1~q = 1.
the sequential Rademacher complexity of linear function classes. The crucial duality fact
Proposition 14 (Rakhlin et al., 2014) Let F be a subset of some Banach space B with
norm  ⋅  and let Z be a subset of the dual space B equipped with norm  ⋅  . Suppose that
Ψ ∶ F → R is (σ, q)-uniformly convex with respect to  ⋅  and 0 ≤ Ψ(f ) ≤ Ψmax for al l f ∈ F .
σ T p−1 1~p
RT (F ) ≤ Cp Z   Ψp−1
Then we have
max
,
where Z  = supz∈Z z , p is such that 1~p + 1~q = 1, and Cp = (p~(p − 1)) p−1
p .

Using the above Proposition in conjunction with Lemma 13 and Theorem 7, we can
immediately conclude thatVT (F , Zcvx ) ≤ 2 T RT (F ) ≤ 2G
2 Ψmax T
for any non-negative function Ψ ∶ F → R that is σ -strongly convex w.r.t.  ⋅ . Note that,
σ
typically, Ψmax will depend on D. For example, in the particular case when  ⋅  =  ⋅  =  ⋅ 2 ,
√
we can take Ψ(u) = 1
2 u2
for the online gradient descent algorithm. In general, for  ⋅  =  ⋅ p and  ⋅  =  ⋅ q , we

can use Ψ(u) = 1
T ~(p − 1) since Ψ is (p − 1)-strongly convex
2 u2
2 and the above bound becomes 2GD
T and recovers the guarantee
w.r.t.  ⋅ p . These O(√
T ) regret rates are not new but we re-derive them to illustrate the
p to get a bound of 2GD
usefulness of the tools we developed.

6. Further Examples

Now we present some further applications of the tools we have developed in this paper for
some speciﬁc learning problems. To begin, we show how to bound the sequential Rade-
macher complexity of functions computed by neural networks. Then, we derive margin
based regret bounds in a fairly general setting. The classical analogues of these margin
bounds have played a big role in the modern theory of supervised learning where they help
explain the success of linear classiﬁers in high dimensional spaces (e.g., Schapire et al., 1997;
Koltchinskii and Panchenko, 2002). We then study the complexity of classes formed by de-
cision trees, analyze the setting of transductive learning, and consider an online version of
the Isotron problem. Finally, we make a connection to the seminal work of Cesa-Bianchi
and Lugosi (1999) by re-deriving their bound on the minimax regret in a static experts
game in terms of the classical Rademacher averages.

6.1 Neural Networks

We provide below a bound on the sequential Rademacher complexity for classic multi-layer
neural networks thus showing they are learnable in the online setting. The model of neural

166

Online Learning via Sequential Complexities

Consider a k-layer 1-norm neural network, deﬁned by a base function class F1 and,
networks we consider below and the bounds we provide are analogous to the ones considered
recursively, for each 2 ≤ i ≤ k ,
in the i.i.d. setting by Bartlett and Mendelson (2003).
 ,
Fi = x  Q
j σ (fj (x))  ∀j fj ∈ Fi−1 , w i 1 ≤ Bi
w i
j
Proposition 15 Suppose σ ∶ R → [−1, 1] is L-Lipschitz with σ(0) = 0. Then it holds that
where σ is a Lipschitz transfer function, such as the sigmoid function.
√
RT (Fk ) ≤  kM
16Bi Lk−1 1 + 4
RT (F1 ).
2 log3~2 (eT 2 )k
i=2
 w1 ≤ B1
In particular, for the case ofF1 = x  ∑j w1
and X ⊂ Rd we have the bound
j xj
√
2 log3~2 (eT 2 )k
RT (Fk ) ≤  kM
16Bi Lk−1 1 + 4
i=1
where X∞ is such that ∀x ∈ X , x∞ ≤ X∞ .
Our result is a non-constructive guarantee, and, to the best of our knowledge, no algorithms
for learning neural networks within the online learning model exist. It is not clear if the
above bounds could be obtained via computationally eﬃcient methods.

2 log d
T



X∞

6.2 Margin Based Regret

In the classical statistical setting, margin bounds provide guarantees on the expected zero-
one loss of a classiﬁer based on the empirical margin zero-one error. These results form the
basis of the theory of large margin classiﬁers (see Schapire et al., 1997; Koltchinskii and
Panchenko, 2002). Recently, in the online setting, bounds of a similar ﬂavor have been shown
for general function classes F based on their sequential Rademacher complexity. We use
through the concept of margin via the Littlestone dimension (Ben-David et al., 2009). We
show that our machinery can easily lead to margin bounds for binary classiﬁcation problems
Proposition 16 For any function class F ⊂ [−1, 1]X , there exists a randomized prediction
ideas from (Koltchinskii and Panchenko, 2002) to do this.
strategy given by τ such that for any sequence z1 , . . . , zT where each zt = (xt , yt ) ∈ X × {±1},
E ˆyt ∼τt (z1∶t−1 ) [1 { ˆytyt < 0}]
TQ
t=1
√
√
1 {f (xt )yt < 2γ } + 16
 inf
≤ inf
T 1 + log log  1
2 log3~2 (eT 2 ) T RT (F ) + 2
γ 1 + 4
γ  .
f ∈F TQ
γ >0
t=1

167

Rakhlin, Sridharan and Tewari

by some function f ∈ F . The upper bound guarantees that there exists a strategy (that does
To interpret the above bound, suppose that the sequence of yt ’s is predicted with a margin 2γ
complexity of F divided by the margin, up to poly-logarithmic factors. Crucially, the bound
does not directly depend on the dimensionality of the input space X .
not need to know the value of γ ) with cumulative loss given by the sequential Rademacher
6.3 Decision Trees
of decision trees of depth no more than d. The function class F for this problem is deﬁned
as follows. Each f ∈ F is deﬁned by choosing a rooted binary tree of depth no more than
We consider here the binary classiﬁcation problem where the learner competes with a set
d and associating to each node a binary valued decision function from a set H ⊆ {±1}X . A
binary value for a given x can be obtained by traversing the tree from the root according
to the value of the decision function at each node and then reading oﬀ the label of the
leaf. Importantly, x “reaches” only one leaf of the tree. Alternatively, for any leaf l, the
1 hl,i (x) = 1
membership of x is given by the conjunctionM
i
To complete the deﬁnition of f , we choose weights wl > 0, ∑l wl = 1, along with the value
σl ∈ {±1} of the function on each leaf l. The resulting function f can be written as
where hl,i is either the decision function at node i along the path to the leaf l, or its negation.
1 hl,i (x) = 1
f (x) = Q
wlσl M
i
l
where the sum runs over all the leaves of the tree.
The following proposition is the online analogue of a result about decision tree learning
Proposition 17 Denote by F the class of decision trees of depth at most d with decision
that Bartlett and Mendelson (2003) proved in the i.i.d. setting.
functions in H. There exists a randomized strategy τ for the learner such that for any
sequence of instances z1 , . . . , zT , with zt = (xt , yt ) ∈ X × {±1},
E ˆyt ∼τt (z1∶t−1 ) [1 { ˆyt ≠ yt}] ≤ inf
1 {f (xt ) ≠ yt}
TQ
f ∈F TQ
t=1
t=1
min C (l), d log3 (T ) T R(H) + √
+ O Q
T log(N ) ,
where C (l) denotes the number of instances that reach the leaf l and are correctly classiﬁed
l
t=1 1 {ytf (xt ) ≤ 0}, with N > 2 being the number of
in the decision tree f that minimizes ∑T
leaves in this tree.

It is not clear whether computationally feasible online methods exist for learning decision
trees, and this represents an interesting avenue of further research.

168

Online Learning via Sequential Complexities

Let F be a class of functions from X to R. Let
6.4 Transductive Learning
̂N∞ (α, F ) = min  G  ∶ G ⊆ RX s.t. ∀f ∈ F ∃g ∈ G satisfying f − g∞ ≤ α
be the (cid:96)∞ covering number at scale α, where the cover is pointwise on all of X . It is easy
(13)
∀T , N∞ (α, F , T ) ≤ ̂N∞ (α, F ) .
to verify that
Indeed, let G be a minimal cover of F at scale α. We claim that for any X -valued tree of
(14)
depth T , the set V = {vg = g ○ x ∶ g ∈ G} of R-valued trees is an (cid:96)∞ cover of F on x. Fix any
 ∈ {±1}T and f ∈ F , and let g ∈ G be such that f − g∞ ≤ α. Clearly  vg
t () − f (xt ())  ≤ α
for any 1 ≤ t ≤ T , concluding the proof.
of transductive learning, where the set X = {x1 , . . . , xn} is a ﬁnite set. To ensure online
learnability, it is suﬃcient to consider an assumption on the dependence of ̂N∞ (α, F ) on α.
This simple observation can be applied in several situations. First, consider the problem
An obvious example of such a class is a VC-type class with ̂N∞ (α, F ) ≤ (c~α)d for some c
which can depend on n. Assume that F ⊂ [−1, 1]X . Substituting this bound on the covering
number into (6) and choosing α = 0, we observe that the value of the supervised game is
upper bounded by 2DT (F ) ≤ 48 √dT log c by Proposition 9. It is easy to see that if n is
ﬁxed and the problem is learnable in the batch (i.e., i.i.d.) setting, then the problem is
n ≤ T and F consists of binary-valued functions. If F is a class with VC dimension d, the
learnable in the online transductive model.
Sauer-Shelah lemma ensures that the (cid:96)∞ cover is smaller than (en~d)d ≤ (eT ~d)d . Using
In the transductive setting considered by Kakade and Kalai (2006), it is assumed that

the previous argument with c = eT , we obtain a bound of 4
dT log(eT ) for the value of
the game, matching the bound of Kakade and Kalai (2006) up to a constant factor.

6.5 Isotron

Kalai and Sastry (2009) introduced a method called Isotron for learning Single Index Models
(SIM). These models generalize linear and logistic regression, generalized linear models, and
revealed at once as a set {(xt , yt )}T
t=1 ∈ Rd ×R where yt = u(w, xt ) for some unknown w ∈ Rd
classiﬁcation by linear threshold functions. For brevity, we only describe the Idealized SIM
of bounded norm and an unknown non-decreasing u ∶ R → R with a bounded Lipschitz
problem considered by the authors. In its “batch” version, we assume that the data are
t=1 (fi (xt ) − yt )2 ,
T ∑T
where fi (x) = ui (wi , x) is the iterative approximation found by the algorithm on the ith
constant. Given this data, the goal is to iteratively ﬁnd the function u and the direction
w, making as few mistakes as possible. The error is measured as 1
round. The elegant computationally eﬃcient method presented by Kalai and Sastry (2009)
is motivated by Perceptron, and a natural open question posed by the authors is whether
there is an online variant of Isotron. Before even attempting a quest for such an algorithm,
we can ask a more basic question:
is the (Idealized) SIM problem even learnable in the
online framework? After all, most online methods deal with convex functions, but u is only
assumed to be Lipschitz and non-decreasing. We answer the question easily with the tools
we have developed.

169

Rakhlin, Sridharan and Tewari

H = f (x, y) = (y − u(w, x))2   u ∶ [−1, 1] → [−1, 1] 1-Lipschitz , w2 ≤ 1
We are interested in online learnability of
in the supervised setting, over X = B2 (the unit Euclidean ball in Rd ) and Y = [−1, 1]. In
(15)
It is evident that H is a composition with three levels: the squared loss, the Lipschitz non-
particular, we prove the result for Lipschitz, but not necessarily non-decreasing functions.
decreasing function, and the linear function. The proof of the following proposition shows
Proposition 18 The class H deﬁned in (15) is online learnable in the (improper) super-
that the covering number of the class does not increase much under these compositions.
O(√
T log3~2 (T )).
vised learning setting. Moreover, the minimax regret is
Once again, it is not clear whether a computationally eﬃcient method attaining the
above guarantee exists.

6.6 Prediction of Individual Sequences with Static Experts

We also consider the problem of prediction of individual sequences, which has been studied
both in information theory and in learning theory.
In particular, in the case of binary
prediction, Cesa-Bianchi and Lugosi (1999) proved upper bounds on the minimax value in
terms of the (classical) Rademacher complexity and the (classical) Dudley integral. One of
we deﬁne static experts as vectors ¯f = (f1 , . . . , fT ) ∈ [0, 1]T , and let F denote a class of
the assumptions made by Cesa-Bianchi and Lugosi (1999) is that experts are static. That is,
such experts. Let Y = {0, 1}, putting us in the scenario of binary classiﬁcation with no side
their prediction only depends on the current round, not on the past information. Formally,
information. Then regret on a particular sequence y1 , . . . , yT can be written as
(cid:96)t ( ¯f , yt ),
(cid:96)t ( ¯ft , yt ) − inf
¯f ∈F Q
TQ
t=1
t=1
where ¯ft is the expert chosen by the learning algorithm at time t. Observe that the proof
of Theorem 7 does not require the loss to be time independent. In the case of absolute loss,
t  ft − yt ()  .
sup
t (cid:96)t ( ¯f , yt ()) = sup
sup
the Rademacher complexity appearing on the right hand side in Theorem 7 becomes
¯f ∈F TQ
¯f ∈F TQ
t=1
t=1
E
E
sup
where the supremum is over all Y -valued trees of depth T . Noting that for f ∈ [0, 1], y ∈
y
y
{0, 1},  f − y   can be written as (1 − 2y)f + y , the above equals
 .
sup
tyt () = sup

ï + TQ
ïïsup
t (1 − 2yt ())ft
t (1 − 2yt ())ft
¯f ∈F TQ
¯f ∈F TQ
t=1
t=1
t=1
E
E
sup
y
y
170

Online Learning via Sequential Complexities
It can be easily veriﬁed that the joint distribution of {t (1 − 2yt ())}T
t=1 is still i.i.d. Rade-
sup
 ,
macher and hence the value of the game is upper bounded by
¯f ∈F TQ
t=1
recovering the upper bound of Theorem 3 in (Cesa-Bianchi and Lugosi, 1999). We note that
for this particular scenario, the factor of 2 (that appears because of symmetrization) is not
needed. This factor is the price we pay for deducing the result from the general statement
of Theorem 7.

2E

tft

7. Discussion

The tools provided in this paper allow us to establish existence of regret minimization
algorithms by working directly with the minimax value. The non-constructive nature of
our results is due to the application of the minimax theorem: the dual strategy does not
give a handle on the primal strategy. Furthermore, by passing to upper bounds on the
dual formulation (2) of the value of the game, we remove the dependence on the dual
round t can be obtained by appealing to the minimax theorem for rounds t + 1 to T , yet
strategy altogether. After the original paper (Rakhlin et al., 2010) appeared, the algorithmic
approach has been developed by Rakhlin et al. (2012) who showed that the prediction for
keeping the minimax expression for round t as is. The notion of a relaxation (in the spirit of
approximate dynamic programming) then allowed the authors to develop a general recipe
for deriving computationally feasible prediction methods. The techniques of the present
paper form the basis for the algorithmic developments of Rakhlin et al. (2012). We refer
the reader to (Rakhlin and Sridharan, 2014; Rakhlin et al., 2012) for details.

Acknowledgments

We would like to thank J. Michael Steele and Dean Foster for helpful discussions. We
gratefully acknowledge the support of NSF under grants CAREER DMS-0954737 and CCF-
1116928.

Appendix A. A Minimax Theorem

The minimax theorem is one of this paper’s main workhorses. For completeness, we state
a general version of this theorem — the von Neumann-Fan minimax theorem — due to
Theorem 19 (Borwein, 2014) Let A and B be Banach spaces. Let A ⊂ A be nonempty,
Borwein (2014) (see also Borwein and Zhuang, 1986).
weakly compact, and convex, and let B ⊂ B be nonempty and convex. Let g ∶ A × B → R be
concave with respect to b ∈ B and convex and lower-semicontinuous with respect to a ∈ A,
g(a, b).
g(a, b) = inf
and weakly continuous in a when restricted to A. Then
a∈A
a∈A
b∈B
b∈B
sup
inf
sup

(16)

171

Rakhlin, Sridharan and Tewari

pt ∈P E [(cid:96)(ft , zt ) + ξ (zt )] = sup
qt ∈Q E [(cid:96)(ft , zt ) + ξ (zt )] ,
In the proof of Theorem 1, the minimax theorem is invoked to assure that
qt ∈Q sup
pt ∈P inf
(17)
inf
where ξ (zt ) is a rather complicated function that includes the repeated inﬁma and suprema
from steps t + 1 to T of regret expression that includes the variable zt (but not ft ). The
expectation in (17) is with respect to ft ∼ qt and zt ∼ pt . To apply (16), we take g to be
the bilinear form in qt and pt , with A = Q and B = P . Equipped with the total variation
distance, Q and P can be seen as subsets of a Banach space of measures on F and Z ,
In terms of conditions, it is enough to check weak compactness of Q and
respectively.
assume continuity of the loss function (lower semi-continuity can be used as well).
Bogachev 2007, Theorem 8.6.2., and van der Vaart and Wellner 1996). If F itself is compact,
Weak compactness of the set of probability measures on a complete separable metric
then the set ∆(F ) of probability measures on F is tight, and hence (under the continuity
space is equivalent to uniform tightness by the fundamental result of Prohorov (see, e.g.,
of the loss) the minimax theorem holds. If F is not compact, tightness can be established
a family ∆(F ) of Borel probability measures on a separable reﬂexive Banach space E is
uniformly tight (under the weak topology) precisely when there exists a function V ∶ E →
under the following general condition. According to Example 8.6.5 (ii) in Bogachev (2007),
[0, ∞) continuous in the norm topology such that
q∈∆(F ) Ef ∼q V (f ) < ∞.
limf →∞ V (f ) = ∞ and
sup
As an example, if F is a subset of a ball in E , it is enough to take V (f ) = f .
not need to invoke the minimax theorem on the space of functions F , but rather (see the
Finally, we remark that in the supervised learning case by considering the improper
learning scenario we allow xt to be observed before the choice ˆyt is made. Therefore, we do
proof of Theorem 8) for two real-valued decisions in a bounded interval. This makes the
application of the minimax theorem straightforward.
Proof [of Theorem 1] For brevity, denote ψ(z1∶T ) = inf f ∈F ∑T
t=1 (cid:96)(f , zt ). The ﬁrst step in
Appendix B. Proofs
the proof is to appeal to the minimax theorem for every couple of inf and sup:
VT (F ) = inf
(cid:96)(ft , zt ) − ψ(z1∶T )
 TQ
Ef1 ∼q1
EfT ∼qT
t=1
z1 ∼p1
zT ∼pT
sup
= sup
 TQ
(cid:96)(ft , zt ) − ψ(z1∶T )
p1
q1
Ef1 ∼q1
EfT ∼qT
t=1
z1 ∼p1
zT ∼pT
. . . sup
inf
inf
(cid:96)(ft , zt ) − ψ(z1∶T ) ,
EzT ∼pT  TQ
= sup
pT
p1
q1
qT
Ez1 ∼p1 . . . sup
t=1
inf
inf
where qt and pt range over Q and P , the sets of distributions on F and Z , respectively.
p1
pT
fT
f1
From now on, it will be understood that zt has distribution pt . By moving the expectation

. . . inf
qT

sup
pT

172

Online Learning via Sequential Complexities

(18)

E
z1

pT −1
. . . sup

with respect to zT and then the inﬁmum with respect to fT inside the expression, we arrive
at
T −1Q
ψ(z1∶T )
(cid:96)(fT , zT ) − E
(cid:96)(ft , zt ) + inf
t=1
E
E
E
zT −1
pT −1
fT −1
sup
inf
. . . sup
sup
inf
T −1Q
(cid:96)(fT , zT ) − ψ(z1∶T ) .
(cid:96)(ft , zt ) + inf
= sup
zT
zT
pT
p1
z1
fT
f1
t=1
E
E
E
E
zT −1
pT −1
fT −1
sup
inf
. . . sup
inf
Let us now repeat the procedure for step T − 1. The above expression is equal to
zT
zT
pT
z1
p1
fT
f1
T −1Q
(cid:96)(fT , zT ) − ψ(z1∶T )
inf
(cid:96)(ft , zt ) + sup
t=1
E
E
E
E
pT −1
zT −1
fT −1
inf
. . . sup
inf
sup
pT
p1
z1
zT
zT
f1
fT
which, in turn, is equal to
T −2Q
(cid:96)(fT −1 , zT −1 )
(cid:96)(ft , zt ) +  inf
t=1
E
zT −1
fT −1
sup
inf
(cid:96)(fT , zT ) − ψ(z1∶T )
inf
+ E
p1
f1
E
E
zT −1
sup
T −2Q
= sup
(cid:96)(ft , zt ) +  inf
(cid:96)(fT −1 , zT −1 )
pT
zT
zT
fT
t=1
E
E
E
E
pT −1
zT −1
zT −1
fT −1
inf
. . . sup
sup
+ inf
(cid:96)(fT , zT ) − ψ(z1∶T ) .
p1
z1
pT
zT
f1
E
Continuing in this fashion for T − 2 and all the way down to t = 1 proves the theorem.
zT
fT
Proof [of Lemma 4] Without loss of generality assume that the Lipschitz constant L = 1,
as the general case follows by scaling φ. Fix a Z -valued tree z of depth T . We ﬁrst claim
log N2 (β , φ ○ G , z) ≤ kQ
log N∞ (β , Gj , z) .
that
j=1
Suppose V1 , . . . , Vk are minimal β -covers with respect to (cid:96)∞ for G1 , . . . , Gk on the tree z.
V φ = {vφ ∶ v ∈ V1 × . . . × Vk },
Consider the set
t () = φ(vt (), zt ()). Then, for any g = (g1 , . . . , gk ) ∈ G
and any  ∈ {±1}T , with representatives (v1 , . . . , vk ) ∈ V1 × . . . × Vk , we have,
where vφ is the tree such that vφ
¿``(cid:192) 1
t ()
t ()2 ≤ max
t∈[T ] φ(g(zt ()), zt ()) − vφ
φ(g(zt ()), zt ()) − vφ
TQ
t=1
t∈[T ]  gj (zt ())) − vj
t∈[T ]  φ(g(zt ()), zt ()) − φ(vt (), zt ())  ≤ max
= max
t ()  ≤ β .
T
j ∈[k] max
173

Rakhlin, Sridharan and Tewari
Thus we see that V φ is an β -cover with respect to (cid:96)∞ for φ ○ G on z. Hence
log N2 (β , φ ○ G , z) ≤ log( V φ  ) = kQ
log( Vj  ) = kQ
log N∞ (β , Gj , z).
j=1
j=1
(19)
For any g ∈ G and z ∈ Z , the value φ(g(z), z) is contained in the interval [−1 + φ(0, z), +1 +
φ(0, z)] by the Lipschitz property. Consider the R-valued tree φ(0, ⋅) ○ z. We now center
{φ(g(⋅), ⋅) ○ z − φ(0, ⋅) ○ z ∶ g ∈ G }.
by this tree and consider the set of trees
invoke (7) since the function values are now in [−1, 1]:
¿```(cid:192) kQ
The centering does not change the size of the cover calculated in (19), but allows us to
4α + 12√

log N∞ (β , Gj , z) dβ
RT (φ ○ G , z) ≤ inf
T S 1
j=1
4α + 12√
 .

≤ inf
log N∞ (β , Gj , z) dβ
α
kQ
α
S 1
j=1
(20)
We substitute the upper bound on covering numbers in (8) for each Gj and arrive at an
T
α
α
4α + 12√
 .

upper bound of
fatβ (Gj ) log(2eT ~β )dβ
kQ
S 1
j=1
inf
Lemma 2 of Rakhlin et al. (2014) implies that for any β > 2RT (Gj ),
T
α
α
fatβ (Gj ) ≤ 32T RT (Gj )2
RT (Gj ). Substituting this together with the value of α = 2RT (Gj ∗ ) into
Let j ∗ = argmax
.
β 2
j

√
(21) yields an upper bound
8 RT (Gj ∗ ) + 48
log(2eT ~β )dβ .
RT (Gj ) S 1
kQ
2RT (Gj∗ ) 1
j=1
2
Using the fact that for any b > 1 and α ∈ (0, 1)
β


≤ 2
log3~2 (x)b~α
log3~2 (b~α)
log xdx = 2
log(b~β )dβ = S b~α
S 1
1
1
β
x
3
3
b
α
b
√
we obtain a further upper bound of
RT (Gj ∗ )  .
8 RT (Gj ∗ ) + 32
RT (Gj ) log3~2 
kQ
j=1
eT

(21)

(22)

2

174

Online Learning via Sequential Complexities
Replacing the ﬁrst term by 8 ∑j RT (Gj ), we conclude that
√
RT (φ ○ G , z) ≤ 8 1 + 4
2 log3~2 (eT 2 ) kQ
RT (Gj )
j=1
as long as RT (Gj ) ≥ 1~T for each j . The statement is concluded by observing that z was
chosen arbitrarily.
Proof [of Corollary 6] We ﬁrst extend the binary function b to a function ¯b to any x ∈ Rk
if x − a∞ < 1 for some a ∈ {±1}k
¯b(x) =  (1 − x − a∞ )b(a)
as follows :
0
otherwise
First note that ¯b is well-deﬁned since all points in the k-cube are separated by L∞ distance
2. Further note that ¯b is 1-Lipschitz w.r.t. the L∞ norm and so applying Lemma 4 we
conclude the statement of the corollary.
Proof [of Theorem 7] Let Et−1 [⋅] = E[⋅ Z1 , . . . , Zt−1 ] denote the conditional expectation.
Using Theorem 1 we have,
VT (F ) = sup
(cid:96)(f , Zt )
ft ∈F Et−1 (cid:96)(ft , ⋅) − inf
 TQ
f ∈F TQ
Z1 ∼p1
ZT ∼pT
t=1
t=1
E
E
inf
= sup
sup
f ∈F  TQ
ft ∈F Et−1 (cid:96)(ft , ⋅) − TQ
(cid:96)(f , Zt )
p1
Z1 ∼p1
ZT ∼pT
t=1
t=1
E
E
inf
≤ sup
f ∈F  TQ
sup
(cid:96)(f , Zt ) .
Et−1 (cid:96)(f , ⋅) − TQ
p1
Z1 ∼p1
ZT ∼pT
t=1
t=1
E
E
. . . sup
(23)
p1
pT
also holds if the choice ft of the learner comes from a larger set G , as long as F ⊆ G . The
The upper bound is obtained by replacing each inﬁmum by a particular choice f . This step
proof is concluded by appealing to (3).
Let ˜Q denote the set of distributions on Y = [−1, 1]. By convexity,
Proof [of Theorem 8]
(cid:96)′ ( ˆyt , yt ) ( ˆyt − f (xt )) ,
(cid:96)(f (xt ), yt ) ≤ sup
(cid:96)( ˆyt , yt ) − inf
f ∈F TQ
f ∈F TQ
TQ
t=1
t=1
t=1
where (cid:96)′ ( ˆyt , yt ) is a subgradient of the function y  (cid:96)(⋅, yt ) at ˆyt . Then the minimax value
(10) can be upper bounded as
(cid:96)′ ( ˆyt , yt ) ( ˆyt − f (xt )) .
E ˆyT ∼qT sup
T (F ) ≤ sup
V S
f ∈F TQ
ˆy1 ∼q1
t=1
q1 ∈ ˜Q
E
inf
x1

qT ∈ ˜Q
inf

. . . sup
xT

. . . sup
pT

. . . sup
pT

sup
y1

sup
yT

175

(24)

sup
yT

Rakhlin, Sridharan and Tewari
By the Lipschitz property of (cid:96), we can replace each subgradient (cid:96)′ ( ˆyt , yt ) with a number
st ∈ [−L, L] to obtain the upper bound
sT ∈[−L,L] sup
st ( ˆyt − f (xt )) .
f ∈F TQ
ˆyT ∼qT
ˆy1 ∼q1
s1 ∈[−L,L] . . . sup
t=1
qT ∈ ˜Q
q1 ∈ ˜Q
E
E
sup
inf
sup
sup
inf
sup
x1
y1
xT
st ( ˆyt − f (xt ))
sT ∈[−L,L] sup
Since yt ’s no longer appear in the optimization ob jective, we can simply write the above as
f ∈F TQ
ˆyT ∼qT
ˆy1 ∼q1
s1 ∈[−L,L] . . . sup
t=1
qT ∈ ˜Q
q1 ∈ ˜Q
E
E
sup
inf
sup
inf
sup
st ( ˆyt − f (xt )) ,
sT ∈[−L,L] sup
= sup
f ∈F TQ
x1
xT
ˆyT ∈[−1,1]
s1 ∈[−L,L] . . . sup
ˆy1 ∈[−1,1]
t=1
sup
inf
sup
inf
x1
xT
where the equality follows because inﬁma are obtained at point distributions. By the same
stf (xt ) .
st ⋅ ˆyt − inf
EsT ∼pT  TQ
reasoning, we now pass to distributions over st ’s:
f ∈F TQ
s1 ∼p1
ˆy1 ∈[−1,1] sup
ˆyT ∈[−1,1] sup
t=1
t=1
E
. . . sup
inf
sup
inf
xT
x1
p1
pT
supported on [−L, L], for any t, and st has distribution pt . Now note that
From now on, it will be understood that the supremum over pt ranges over all distributions
st ⋅ f (xt )
st ⋅ ˆyt − inf
EsT  TQ
f ∈F TQ
t=1
t=1
stf (xt ) = sup
st ⋅ ˆyt − inf
EsT  TQ
ˆyT ∈[−1,1] EsT  TQ
stf (xt )
st ⋅ ˆyt − inf
is concave (linear) in pT and is convex in ˆyT and hence by the minimax theorem,
f ∈F TQ
f ∈F TQ
ˆyT ∈[−1,1] sup
t=1
t=1
t=1
t=1

stf (xt ) ,
inf
inf
= T −1Q
st ⋅ ˆyt + sup
ˆyT ∈[−1,1] EsT [sT ] ⋅ ˆyT − inf
f ∈F TQ
pT
pT
t=1
t=1
EsT
inf
pT

T −1Q
stf (xt )

where the last step is similar to the one in the proof of Theorem 1, speciﬁcally (18). Similarly
ˆyT ∈[−1,1] EsT [sT ] ⋅ ˆyT − inf
st ⋅ ˆyt + sup
f ∈F TQ
note that the term
EsT −1
t=1
t=1
EsT
inf
pT ,xT
is concave (linear) in pT −1 and is convex in ˆyT −1 and hence again by the minimax theorem,

 T −1Q
stf (xt )

ˆyT ∈[−1,1] EsT [sT ] ⋅ ˆyT − inf
st ⋅ ˆyt + sup
f ∈F TQ
ˆyT −1 ∈[−1,1] sup
t=1
t=1

 T −1Q
E
E
pT −1
sT −1
stf (xt )

inf
inf
ˆyT ∈[−1,1] EsT [sT ] ⋅ ˆyT − inf
st ⋅ ˆyt + sup
= sup
f ∈F TQ
pT ,xT
sT
ˆyT −1 ∈[−1,1] E
t=1
t=1
EsT
stf (xt ) .
 TQ
pT −1
sT −1
inf
inf
= T −2Q
ˆyt ∈[−1,1] Est [st ] ⋅ ˆyt − inf
st ⋅ ˆyt + sup
f ∈F TQ
pT ,xT
t=1
t=1
t=T −1
EsT
E
sT −1
pT −1
inf
sup
pT ,xT
176

Online Learning via Sequential Complexities

sup
pT

Proceeding in similar fashion and using this in (24) we conclude that,
EsT ∼pT  TQ
st ⋅ ˆyt − inf
stf (xt )
V S
T (F ) ≤ sup
f ∈F TQ
s1 ∼p1
ˆy1 ∈[−1,1] sup
ˆyT ∈[−1,1] sup
t=1
t=1
 TQ
stf (xt )
E
inf
. . . sup
inf
ˆyt ∈[−1,1] Est ∼pt [st ] ⋅ ˆyt − inf
= sup
f ∈F TQ
pT
x1
xT
p1
sT ∼pT
s1 ∼p1
t=1
t=1
E
E
inf
sup
. . . sup
sup
EsT ∼pT sup
(Est ∼pt [st ] − st ) f (xt ) ,
≤ sup
f ∈F TQ
pT
x1
p1
xT
s1 ∼p1
t=1
E
sup
. . . sup
sup
where we replaced each ˆyt with a potentially suboptimal choice f (xt ). Passing the expec-
pT
x1
p1
xT
tation past the suprema we obtain an upper bound
s′
t − st f (xt )
T ∼pT sup
f ∈F TQ
EsT ,s′
1 ∼p1
t=1
s1 ,s′
E
sup
. . . sup
sup
t s′
t − st f (xt )
= sup
ET sup
f ∈F TQ
x1
p1
xT
1 ∼p1
T ∼pT
t=1
s1 ,s′
sT ,s′
E
E
E
. . . sup
sup
sup
≤ sup
tstf (xt )
sT ∈[−2L,2L] ET sup
f ∈F TQ
xT
p1
1
x1
pT
s1 ∈[−2L,2L] E
t=1
sup
. . . sup
sup
= sup
tstf (xt )
sT ∈{−2L,2L} ET sup
f ∈F TQ
1
x1
xT
s1 ∈{−2L,2L} E
t=1
. . . sup
sup
sup
= 2L sup
sT ∈{−1,1} ET sup
tstf (xt ) ,
f ∈F TQ
x1
1
xT
s1 ∈{−1,1} E
t=1
sup
. . . sup
sup
(27)
where the last inequality is because, for every t ∈ [T ], we have convexity in st and so
xT
x1
1
supremum is achieved at either −2L or 2L. Notice that after using convexity to go to
ψ ∶ {±1}  R, we have that
gradients, the proof technique above basically mimics the proofs of Theorems 1 and 7 to get
to a symmetrized term as we did in those theorems. Now consider any arbitrary function
2 (ψ(+s) + ψ(−s)) = 1
2 (ψ(+1) + ψ(−1)) = E [ψ()] .
s∈{±1} E [ψ(s ⋅ )] = sup
s∈{±1} 1
sup
Since in (27), for each t, st and t appear together as t ⋅ st using the above equation
repeatedly, we conclude that
sT ∈{−1,1} ET sup
tstf (xt )
T (F ) ≤ 2L sup
V S
f ∈F TQ
s1 ∈{−1,1} E1 . . . sup
t=1
sup
sup
tf (xt ) .
ET sup
= 2L sup
f ∈F TQ
xT
x1
t=1
E1 . . . sup
(28)
We now claim that the above supremum can be written in terms of an X -valued tree. Brieﬂy,
x1
xT
the solution for x1 in (28) is attained (for simplicity, assume the supremum is attained) at

(25)

(26)

177

Rakhlin, Sridharan and Tewari
2 can be calculated for 1 = 1 and 1 = −1. Arguing
1 . The optimal value x∗
an optimal value x∗
in this manner leads to a tree x. We conclude
tf (xt ()) = 2 L T RT (F ).
E1∶T sup
T (F ) ≤ 2L sup
V S
f ∈F TQ
t=1
x

Proof [of Proposition 9] For the upper bound, we start by using Theorem 8 for absolute
loss, which has a Lipschitz constant of 1, to bound the value of the game by sequential
T V S
T (F ) ≤ 2 RT (F ) .
Rademacher complexity,
1
joint distribution on sequences (x1 , y1 ), . . . , (xt , yt ) in (2):
We combine the above inequality with (7) and (8) to obtain the upper bound.
Observe that a lower bound on the value can be obtained by choosing any particular
T (F ) ≥ E  TQ
V S
 yt − f (xt )  .
ft ∈F E(xt ,yt )  yt − ft (xt )   (x, y)1∶t−1  − inf
f ∈F TQ
t=1
t=1
inf
To this end, choose any X -valued tree x of depth T . Let y1 , . . . , yT be i.i.d. Rademacher
random variables and deﬁne xt = x(y1∶t−1 ) deterministically (that is, the conditional dis-
tribution of xt is a point distribution on x(y1∶t−1 )). It is easy to see that this distribution
makes the choice ft irrelevant, yielding
T (F ) ≥ E  TQ
V S
 yt − f (xt )  = Ey1 ,...,yT sup
1 − inf
ytf (xt ).
f ∈F TQ
f ∈F TQ
t=1
t=1
t=1
T (F ) ≥ RT (F ). The
Since this holds for any tree x, we obtain the desired lower bound V S
ﬁnal lower bound on RT (F ) (in terms of the fat-shattering dimensions) is proved by Rakhlin
et al. (2014, Lemma 2).
First, suppose that fatα is inﬁnite for some α > 0. Then, the lower bound says that V S
T (F ) ≥
√
√
2) and hence lim supT →∞ V S
T (F )~T ≥ α~(4
2). Thus, the class F is not online
αT ~(4
Proof [of Theorem 10] The equivalence of 1 and 2 follows directly from Proposition 9.
learnable in the supervised setting. Now, assume that fatα is ﬁnite for all α. Fix an  > 0
and choose α = ~16. Using the upper bound, we have

√
T (F ) ≤ 8T α + 24
β  dβ
fatβ log  2eT
V S
T S 1
T (1 − α)
√
≤ 8T α + 24
fatα log  2eT
α 
α
≤ T ~2 + T ~2

178

Online Learning via Sequential Complexities
for T large enough. Thus, lim supT →∞ V S
T (F )~T ≤ . Since  > 0 was arbitrary, this proves
that F is online learnable in the supervised setting.
The statement that V S
T (F ), RT (F ), and DT (F ) are within a multiplicative factor of
O(log3~2 T ) of each other whenever the problem is online learnable follows immediately
from Eq. (10) in (Rakhlin et al., 2014) and Proposition 9.
Proof [of Lemma 13] Consider the game (F , Zcvx ) and ﬁx a randomized strategy π of
the player. Then, the expected regret of a randomized strategy π against any adversary
playing g1 , . . . , gT can be lower-bounded via Jensen’s inequality as
gt Eut ∼πt (g1∶t−1 ) [ut ] − inf
Eut ∼πt (g1∶t−1 ) [gt (ut )] − inf
gt (u) ≥ TQ
gt (u),
TQ
u∈F TQ
u∈F TQ
t=1
t=1
t=1
t=1
which is simply regret of a deterministic strategy obtained from π by playing Eut ∼πt (g1∶t−1 ) [ut ]
T (F , Zcvx ) where V det
tic ones. Hence, VT (F , Zcvx ) = V det
on round t. Thus, to any randomized strategy corresponds a deterministic one that is no
worse. On the other hand, the set of randomized strategies contains the set of determinis-
Abernethy et al. (2008) that says V det
T (F , Zcvx ) = V det
T (F , Zlin ). Note that Abernethy et al.
is deﬁned as the minimax regret
T
obtainable only using deterministic player strategies. Now, we appeal to Theorem 14 of
(2008) deal with convex sets in ﬁnite dimensional spaces only. However, their proof relies
bounds the convex function). Since Zlin also consists of convex (in fact, linear) functions,
on fundamental properties of convex functions that are true in any general vector space
the above argument again gives V det
T (F , Zlin ) = VT (F , Zlin ). This ﬁnishes the proof of the
(such as the fact that the ﬁrst order Taylor expansion of a convex function globally lower
lemma.
Proof [of Proposition 15] We shall prove that for any i ∈ {2, . . . , k},
√
RT (Fi ) ≤ 16LBi 1 + 4
2 log3~2 (eT 2 ) RT (Fi−1 ).
To see this note that for any x, RT (Fi , x) is equal to



j σ (fj (xt ()))ï
ïïQ
≤ E
TQ
wi ∶wi 1 ≤Bi
wi ∶wi 1 ≤Bi
t=1
E
w i
sup
sup
t
∀j fj ∈Fi−1
∀j fj ∈Fi−1
j
by H¨older’s inequality. Then RT (Fi ) is upper bounded as
tσ (f (xt ()))
tσ (f (xt ())) , − TQ
max  TQ
E Bi sup
f ∈Fi−1
t=1
t=1
sup
−tσ (f (xt ())) .
tσ (f (xt ())) , sup
≤ sup
E Bi max  sup
TQ
TQ
x
f ∈Fi−1
f ∈Fi−1
t=1
t=1
x

tσ (fj (xt ()))



w i 1 max
j

 TQ
t=1

179

(29)

Rakhlin, Sridharan and Tewari
Since 0 ∈ Fi together with the assumption of σ(0) = 0, both terms are non-negative, and
thus the maximum above can be upper bounded by the sum
E Bi sup
E Bi sup
tσ (f (xt ())) + sup
−tσ (f (xt ())) .
TQ
TQ
f ∈Fi−1
f ∈Fi−1
t=1
t=1
sup
Indeed, let x∗ be the tree achieving the
x
x
achieved). Then the mirror tree x deﬁned via xt () = x∗
t (−) yields the same value for the
We now claim that the two terms are equal.
supremum in the ﬁrst term (a modiﬁed analysis can be carried out if the supremum is not
second term. Since the argument can be carried out in the reverse direction, the two terms
are equal, and the upper bound of
E  sup
tσ (f (xt ()))
TQ
f ∈Fi−1
t=1
2Bi sup
x
√
2 log3~2 (eT 2 ) RT (Fi−1 ).
16BiL 1 + 4
follows. In view of contraction in Corollary 5, we obtain a further upper bound of
To ﬁnish the proof we note that for the base case of i = 1, RT (F1 ) is equal to

twxt ()
TQ
w∈Rd ∶w1 ≤B1
t=1
E
sup
sup
x

 ≤ B1 sup
which is upper bounded by
txt ()∞
w1  TQ
i∈[d]  TQ
E max
txt ()[i] .
w∈Rd ∶w1 ≤B1
t=1
t=1
E
sup
sup
Note that the instances x ∈ X are vectors in Rd and so for a given instance tree x, for any
x
x
i ∈ [d], x[i] given by only taking the ith co-ordinate is a valid real valued tree. By (4),

txt ()[i] ≤ B1
E max
T ⋅ RT (F1 ) ≤ B1 sup
i∈[d]  TQ
2T X 2∞ log d.
t=1
x
Using the above and (29) repeatedly we conclude the proof.
Proof [of Proposition 16] Fix a γ > 0 and use loss
(cid:96)( ˆy , y) =  1
ˆyy ≤ 0
1 − ˆyy~γ 0 < ˆyy < γ
ˆyy ≥ γ
Since this loss is 1~γ -Lipschitz, we can use (11) and the Rademacher contraction Corollary 5
0
to show that for each γ > 0 there exists a randomized strategy τ γ such that for any data
(cid:96)(f (xt ), yt ) + γ −1ρT T RT (F ),
t (z1∶t−1 ) [(cid:96)( ˆyt , yt )] ≤ inf
f ∈F TQ
TQ
sequence
ˆyt ∼τ γ
t=1
t=1
E
180

Online Learning via Sequential Complexities
√
2 log3~2 (eT 2 ) throughout the proof. Further, observe that the loss
where ρT = 16 1 + 4
function is lower bounded by the zero-one loss 1 { ˆyy < 0} and is upper bounded by the
margin zero-one loss 1 { ˆyy < γ }. Hence,
t (z1∶t−1 ) [1 { ˆytyt < 0}] ≤ inf
1 {ytf (xt ) < γ } + γ −1ρT T RT (F ).
TQ
f ∈F TQ
ˆyt ∼τ γ
t=1
t=1
E
(30)
we discretize the set of γ ’s as γi = 1~2i and use the output of the randomized strategies
The above bound holds for randomized each strategy given by τ γ , for any given γ . Now
experts algorithm (Algorithm 1) with initial weight for expert i as pi =
algorithm achieves O(√
τ γ1 , τ γ2 , . . ., that attain the regret bounds given in (30), as experts. We then run a countable
T log(1~pi )) regret w.r.t. expert i. In view of Proposition 20, for
6
π2 i2 . Such an
i ρT T RT (F ) + √
T 1 + 2 log  iπ√
 .
1 {ytf (xt ) < γi} + γ −1
E ˆyt ∼τt (z1∶t−1 ) [1 { ˆytyt < 0}] ≤ inf
this randomized strategy τ , for any i,
f ∈F TQ
TQ
t=1
t=1
For any γ > 0, let iγ ∈ 0, 1, . . . , be such that 2−(iγ +1) < γ ≤ 2−iγ . Then above right-hand side
6
1 {ytf (xt ) < 2γ } + γ −1ρT T RT (F ) + √
T 1 + 2 log  iγ π√
 .
is upper bounded by
f ∈F TQ
t=1
The proof is concluded using the inequality iγ ≤ log(1~γ ) and upper bounding constants.
inf
6
Proof [of Proposition 17] Fix some L > 0. The loss
φL (α) = 
if α ≤ 0
1 − Lα if 0 < α ≤ 1~L
1
is L-Lipschitz and so by Theorem 7 and Corollary 5 we have that for every L > 0, there exists
otherwise
0
a randomized strategy τ L for the player, such that for any sequence z1 = (x1 , y1 ), . . . , zT =
(xT , yT ),
t (z1∶t−1 ) [φL (yt ˆyt )] ≤ inf
φL (ytf (xt )) + LρT T RT (F ),
f ∈F TQ
TQ
ˆyt ∼τ L
t=1
t=1
√
E
(31)
where ρT = 16 1 + 4
2 log3~2 (eT 2 ) throughout this proof. Since φL dominates the step
t (z1∶t−1 ) [1 { ˆyt ≠ yt}] .
function, the left hand side of (31) also upper-bounds the expected indicator loss
TQ
ˆyt ∼τ L
t=1
E
For any f ∈ F , we can relate the φL -loss to the indicator loss by
C (l)φL (wl ).
1 {ytf (xt ) ≤ 0} + Q
φL (ytf (xt )) = TQ
TQ
t=1
t=1
l

181

Rakhlin, Sridharan and Tewari
Let us now use the above decomposition in (31). Crucially, the sign of f (x) does not depend
on wl , but only on the label σl of the unique leaf l reached by x. Thus, the inﬁmum in (31)
can be split into two inﬁma:
C (l)φL (wl ),
1 {ytf (xt ) ≤ 0} + inf
φL (ytf (xt )) = inf
Q
f ∈F TQ
f ∈F TQ
t=1
t=1
inf
where it is understood that the C (l) term on the right hand side is computed using the
wl
l
C (l)φL (wl ) ≤ Q
C (l) max(0, 1 − Lwl ) = Q
max (0, (1 − Lwl )C (l)) .
Q
function f minimizing the ﬁrst sum on the right hand side. We can further write
l
l
l
experts corresponding to the values L ∈ N. The prior on expert L is taken to be pL = 6
So far, we have derived a regret bound for a given L. Let us now remove the requirement
π2 L−2
so that ∑ pL = 1. For the randomized strategy τ obtained in this manner, from Proposition
to know L a priori by running the experts Algorithm 1 with τ 1 , τ 2 , . . . as a countable set of
20, for any sequence of instances and any L ∈ N,
max (0, (1 − Lwl )C (l))
1 {ytf (xt ) ≤ 0} + inf
E ˆyt ∼τt (z1∶t−1 ) [1 { ˆy ≠ yt}] ≤ inf
f ∈F Q
f ∈F TQ
TQ
+ LρT T RT (F ) + √
√
T log(Lπ~√
t=1
t=1
T + 2
6).
l
Now we pick L =  {l ∶ C (l) > ρT T RT (F )}  ≤ N and upper bound the second inﬁmum by
choosing wl = 0 if C (l) ≤ ρT T RT (F ) and wl = 1~L otherwise:
C (l)1 {C (l) ≤ ρT T RT (F )}
max (0, (1 − Lwl )C (l)) + LρT T RT (F ) ≤ Q
Q
+ ρT T RT (F ) Q
1 {C (l) > ρT T RT (F )}
inf
wl
l
l
l
min{C (l), ρT T RT (F )}.
which can be written succinctly asQ
l
E ˆyt ∼τt (z1∶t−1 ) [1 { ˆyt ≠ yt}] ≤ inf
1 {ytf (xt ) ≤ 0}
We conclude that
f ∈F TQ
TQ
min(C (l), ρT T RT (F )) + √
T 1 + 2 log(N π~√
t=1
t=1
+ Q
6) .
Finally, we apply Corollary 6 and Lemma 3(2) to bound RT (F ) ≤ dO(log3~2 T ) RT (H) and
l
thus conclude the proof.
(1959), the class G of all bounded Lipschitz functions on a bounded interval has small metric
Proof [of Proposition 18] First, by the classical result of Kolmogorov and Tikhomirov
182

Online Learning via Sequential Complexities
log ̂N∞ (α, G ) = Θ(1~α). For the particular class of non-decreasing 1-Lipschitz
functions, it is trivial to verify that the entropy is in fact bounded by 2~α. Considering all
1-Lipschitz functions increases this to c0 ~α for some universal constant c0 .
entropy:
Next, consider the class F = {w, x   w2 ≤ 1} over the Euclidean ball. By Proposi-
tion 14, RT (F ) ≤ 1~√
T . Using the lower bound of Proposition 9, fatα ≤ 32~α2 whenever
2~√
2~√
√
√
T . This implies that N∞ (α, F , T ) ≤ (2eT ~α)32~α2
whenever α > 4
α > 4
that this bound does not depend on the ambient dimension of X .
Next, we show that a composition of G with any “small” class F ⊂ [−1, 1]X also has
T . Note
a small cover. To this end, suppose N∞ (α, F , T ) is the covering number for F . Fix a
particular tree x and let V = {v1 , . . . , vN } be an (cid:96)∞ cover of F on x at scale α. Analogously,
let W = {g1 , . . . , gM } be an (cid:96)∞ cover of G with M = ̂N∞ (α, G ). Consider the class G ○ F =
{g ○ f ∶ g ∈ G , f ∈ F }. The claim is that {g(v) ∶ v ∈ V , g ∈ W } provides an (cid:96)∞ cover for G ○F on
x. Fix any f ∈ F , g ∈ G and  ∈ {±1}T . Let v ∈ V be such that maxt∈[T ]  f (xt ()) − vt ()  ≤ α,
and let g ′ ∈ W be such that g − g ′ ∞ ≤ α. Then, using the fact that functions in G are
1-Lipschitz, for any t ∈ [T ],
 g(f (xt ())) − g ′ (vt ())  ≤  g(f (xt ())) − g ′ (f (xt ())  +  g ′ (f (xt ()) − g ′ (vt ())  ≤ 2α .
Hence, N∞ (2α, G ○ F , T ) ≤ ̂N∞ (α, G ) × N∞ (α, F , T ).
by 8T times the sequential Rademacher complexity of the class G ○ F = {u(w, x)   u ∶
[−1, 1] → [−1, 1] is 1-Lipschitz , w2 ≤ 1} since the squared loss is 4-Lipschitz on the space
Finally, we put all the pieces together. By Theorem 8, the minimax value is bounded

√
T DT (G ○ F ) ≤ 32
T log N (δ, G ○ F , T ) dδ
T + 12 S 1
of possible values. The latter complexity is then bounded by
8~√

√
√
≤ 32
log(2eT )dδ .
δ + 128
T + 12
T S 1
T
8~√
4c0
bounded by O(√
δ2
T log3~2 (T )).
T
We therefore conclude that the value of the game for the supervised learning problem is

Appendix C. Exponentially Weighted Average (EWA) Algorithm on
Countable Experts

We consider here a version of the exponentially weighted experts algorithm for a countable
(possibly inﬁnite) number of experts and provide a bound on the expected regret of the
randomized algorithm. The proof of the result closely follows the ﬁnite case (e.g., Cesa-
Bianchi and Lugosi, 2006, Theorem 2.2). This result is well known and we include it here
for completeness, as it is needed in the proofs of Proposition 16 and Proposition 17.
produces an element of F at round t. Here we also assume that F ⊆ [0, 1]X . Denote by
Suppose we are provided with countable experts E1 , E2 , . . ., where each expert can
herself be thought of as a randomized/deterministic player strategy which, given history,
each expert p1 , p2 , . . . such that ∑i pi = 1.
f i
t the function output by expert i at round t given the history. The EWA algorithm we
consider needs access to the countable set of experts and also needs an initial weighting on

183

Rakhlin, Sridharan and Tewari

i ← pi
for t = 1 to T do
Algorithm 1 EWA (E1 , E2 , . . ., p1 , p2 , . . .)
Initialize each w1
Play ft = f t
Pick randomly an expert i with probability wt
i
= wt
i
Update for each i, wt+1
i e−ηf t
i (xt )
Receive xt
∑i wt
−ηf t
i (xt )
i
i e
end for
Proposition 20 The exponential ly weighted average forecaster (Algorithm 1) with η =
T −1~2 enjoys the regret bound
i (xt ) + √
8 + √
T log (1~pi )
E [ft (xt )] ≤ TQ
TQ
t=1
t=1
T
f t

for any i ∈ N.
References

J. Abernethy, P. L. Bartlett, A. Rakhlin, and A. Tewari. Optimal strategies and minimax
lower bounds for online convex games. In Proceedings of the 21st Annual Conference on
Learning Theory, pages 414–424. Omnipress, 2008.

J. Abernethy, A. Agarwal, P. L. Bartlett, and A. Rakhlin. A stochastic view of optimal regret
through minimax duality.
In Proceedings of the 22nd Annual Conference on Learning
Theory, 2009.

P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: risk bounds and
structural results. Journal of Machine Learning Research, 3:463–482, 2003.

S. Ben-David, D. Pal, and S. Shalev-Shwartz. Agnostic online learning. In Proceedings of
the 22th Annual Conference on Learning Theory, 2009.

D. Blackwell. An analog of the minimax theorem for vector payoﬀs. Paciﬁc Journal of
Mathematics, 6(1):1–8, 1956a.

D. Blackwell. Controlled random walks. In Proceedings of the International Congress of
Mathematicians, 1954, volume 3, pages 336–338. North Holland, 1956b.

V.I. Bogachev. Measure Theory, volume 2. Springer, 2007. ISBN 3540345132.

J.M. Borwein. A very complicated proof of the minimax theorem. Minimax Theory and Its
Applications, 1(1), 2014.

J.M. Borwein and D Zhuang. On Fan’s minimax theorem. Mathematical programming, 34
(2):232–234, 1986.

184

Online Learning via Sequential Complexities

N. Cesa-Bianchi and G. Lugosi. On prediction of individual sequences. Annals of Statistics,
pages 1865–1895, 1999.

N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University
Press, 2006.

N. Cesa-Bianchi, Y. Freund, D. Haussler, D. P. Helmbold, R. E. Schapire, and M. K.
Warmuth. How to use expert advice. Journal of the ACM, 44(3):427–485, 1997.

T. Cover. Behavior of sequential predictors of binary sequences. In Transactions of the
Fourth Prague Conference on Information Theory, Statistical Decision Functions, Ran-
dom Processes, 1965, pages 263–272. Publishing House of the Czechoslovak Academy of
Sciences, 1967.

T. M. Cover and A. Shenhar. Compound Bayes predictors for sequences with apparent
Markov structure. IEEE Transactions on Systems, Man and Cybernetics, 7(6):421–424,
1977.

L. Davisson. Universal noiseless coding. Information Theory, IEEE Transactions on, 19
(6):783–795, 1973.

M. Feder, N. Merhav, and M. Gutman. Universal prediction of individual sequences. In-
formation Theory, IEEE Transactions on, 38(4):1258–1270, 1992.

D. P. Foster and R. V. Vohra. Calibrated learning and correlated equilibrium. Games and
Economic Behavior, 21(1):40–55, 1997.

J. Hannan. Approximation to Bayes risk in repeated play. Contributions to the Theory of
Games, 3:97–139, 1957.

S. Hart and A. Mas-Colell. A simple adaptive procedure leading to correlated equilibrium.
Econometrica, 68(5):1127–1150, 2000.

S. M. Kakade and A. T. Kalai. From batch to transductive online learning. In Y. Weiss,
B. Sch¨olkopf, and J.C. Platt, editors, Advances in Neural Information Processing Systems
18, pages 611–618. MIT Press, 2006.

A. Kalai and S. Vempala. Eﬃcient algorithms for online decision problems. Journal of
Computer and System Sciences, 71(3):291–307, 2005.

A. T. Kalai and R. Sastry. The isotron algorithm: High-dimensional isotonic regression. In
Proceedings of the 22th Annual Conference on Learning Theory, 2009.

A.N. Kolmogorov and V.M. Tikhomirov. ε-entropy and ε-capacity of sets in function spaces.
Uspekhi Matematicheskikh Nauk, 14(2):3–86, 1959.

V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the gen-
eralization error of combined classiﬁers. Annals of Statistics, 30(1):1–50, 2002.

M. Ledoux and M. Talagrand. Probability in Banach Spaces. Springer-Verlag, New York,
1991.

185

Rakhlin, Sridharan and Tewari

N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold
algorithm. Machine Learning, 2(4):285–318, 04 1988.

N. Littlestone and M. K. Warmuth. The weighted ma jority algorithm. Information and
Computation, 108(2):212–261, 1994.

A. Rakhlin and K. Sridharan. Statistical learning and sequential prediction, 2014. Available
at http://stat.wharton.upenn.edu/~rakhlin/courses/stat928/stat928_notes.pdf.

A. Rakhlin, K. Sridharan, and A. Tewari. Online learning: Random averages, combinatorial
parameters, and learnability. In Advances in Neural Information Processing Systems 23,
pages 1984–1992, 2010.

A. Rakhlin, K. Sridharan, and A. Tewari. Online learning: Beyond regret. In Proceedings
of the 24th Annual Conference on Learning Theory, volume 19 of JMLR Workshop and
Conference Proceedings, pages 559–594, 2011.

A. Rakhlin, O. Shamir, and K. Sridharan. Relax and randomize: From value to algorithms.
In Advances in Neural Information Processing Systems 25, pages 2150–2158, 2012.

A. Rakhlin, K. Sridharan, and A. Tewari. Sequential complexities and uniform laws of large
numbers. Probability Theory and Related Fields, 2014.

J. Rissanen. Universal coding, information, prediction, and estimation. Information Theory,
IEEE Transactions on, 30(4):629–636, 1984.

H. Robbins. Asymptotically subminimax solutions of compound statistical decision prob-
lems. In Proceedings of the Second Berkeley Symposium on Mathematical Statistics and
Probability, pages 131–149. University of California Press, 1950.

R. E. Schapire, Y. Freund, P. Bartlett, and W.S. Lee. Boosting the margin: A new expla-
nation for the eﬀectiveness of voting methods. The Annals of Statistics, pages 322–330,
1997.

S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends
in Machine Learning, 4(2):107–194, 2011.

S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Stochastic convex optimization.
In Conference on Learning Theory, 2009.

A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes with
Applications to Statistics. Springer-Verlag, New York, 1996.

V. Vovk. A game of prediction with expert advice. Journal of Computer and System
Sciences, 56(2):153–173, 1998.

M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.
In Proceedings of the Twentieth International Conference on Machine Learning, pages
928–936, 2003.

J. Ziv and A. Lempel. A universal algorithm for sequential data compression. Information
Theory, IEEE Transactions on, 23(3):337–343, 1977.

186

Journal of Machine Learning Research 16 (2015) 103-147

Submitted 6/11; Revised 7/14; Published 1/15

Links Between Multiplicity Automata, Observable Operator
Models and Predictive State Representations — a Uniﬁed
Learning Framework

Michael Thon
Herbert Jaeger
Jacobs University Bremen
28759 Bremen, Germany

Editor: Joelle Pineau

m.thon@jacobs-university.de
h.jaeger@jacobs-university.de

Abstract

Stochastic multiplicity automata (SMA) are weighted ﬁnite automata that generalize prob-
abilistic automata. They have been used in the context of probabilistic grammatical infer-
ence. Observable operator models (OOMs) are a generalization of hidden Markov models,
which in turn are models for discrete-valued stochastic processes and are used ubiquitously
in the context of speech recognition and bio-sequence modeling. Predictive state represen-
tations (PSRs) extend OOMs to stochastic input-output systems and are employed in the
context of agent modeling and planning.
We present SMA, OOMs, and PSRs under the common framework of sequential sys-
tems, which are an algebraic characterization of multiplicity automata, and examine the
precise relationships between them. Furthermore, we establish a uniﬁed approach to learn-
ing such models from data. Many of the learning algorithms that have been proposed can
be understood as variations of this basic learning scheme, and several turn out to be closely
related to each other, or even equivalent.
Keywords: multiplicity automata, hidden Markov models, observable operator models,
predictive state representations, spectral learning algorithms

1. Introduction

Multiplicity automata (MA) (Sch¨utzenberger, 1961) are weighted nondeterministic au-
tomata which generalize both ﬁnite and probabilistic automata. The discovery that MA
are eﬃciently learnable (Bergadano and Varricchio, 1994; Ohnishi et al., 1994) in the ex-
act learning model of Angluin (Angluin, 1987) sparked an interest in these, and several
versions have been studied. One such version is stochastic multiplicity automata (SMA),
which model rational stochastic languages and have been used in the context of proba-
bilistic grammatical inference (Denis et al., 2006; Bailly et al., 2009). Independent of this
line of research, hidden Markov models (HMMs) (see Rabiner, 1989) for discrete-valued
stochastic processes have been extensively studied and are now a standard tool in many
pattern recognition domains such as speech recognition, natural language processing and
bio-sequence modeling. Observable operator models (OOMs) are a generalization of HMMs
that was introduced by Jaeger (1998) following previous work on deciding the equivalence
of HMMs (Ito et al., 1992). Finally, predictive state representations (PSRs) are mod-

c(cid:13)2015 Michael Thon and Herbert Jaeger.

Thon and Jaeger

els for stochastic input-output systems developed by Littman, Sutton, and Singh (2001)
and inspired by OOMs. PSRs generalize partially observable Markov decision processes
(POMDPs) (Kaelbling et al., 1998) and have been used in the context of agent modeling
and planning (James et al., 2004; James and Singh, 2005; Wolfe and Singh, 2006; Boots
et al., 2010). As it turns out, all of these models are instances of MA and thereby closely
related, though this is not widely perceived, due in part to the disjoint scientiﬁc communi-
ties.
All of SMA, OOMs and PSRs model some form of probability distribution. A central
task common to all cases is therefore to estimate a model from a given sample. This may
also be referred to as learning, system identiﬁcation or model induction depending on the
context.
In this paper we present SMA, OOMs, and PSRs under a common framework and exam-
ine the precise relationships between them. Furthermore, we establish a uniﬁed approach to
learning such models from data. Many of the learning algorithms that have been proposed
can be understood as variations of this basic learning theme, and several turn out to be
closely related or even equivalent.
In Section 2 we cover the essential theory for sequential systems (SSs) — a term coined
by Carlyle and Paz (1971) for a purely algebraic characterization of MA. Though not new,
we present this theory in a way that can be readily turned into algorithms, and with
full proofs, because they give much insight and pave the way to the presented learning
approach. The ﬁrst result concerns the relationship between SSs and the ob jects that they
describe, namely formal series f : Σ∗
→ K for K = R or K = C (see Section 1.1 for
details). Any such function can be associated with a linear function space F , and has a SS
representation if and only if the space F is ﬁnite dimensional. In fact, a SS can be seen as
a representation of f w.r.t. some basis of F , and a change of basis will correspond to an
equivalence transformation of SSs, where equivalence of two SSs means that they represent
the same function. The remaining theory will be concerned with such transformations of
SSs.
It is shown how to transform any SS to an equivalent minimal SS, how to decide
equivalence, how to normalize SSs and how to convert SSs into a so-called “interpretable”
form.
In Section 3 we mention the relationship between MA and the more general class
of weighted ﬁnite automata (WFA) and their extension to input-output systems called
weighted ﬁnite-state transducers (WFST). We then present SMA, OOMs and PSRs as
instances of SSs with speciﬁc additional constraints that model probabilistic languages,
stochastic processes and controlled processes, respectively, via the formal series f that they
describe. We only sketch the basic concepts and give pointers to relevant literature. The
main emphasis is on exploring the relations among the various model classes. We show that
SMA are related to OOMs in the same way that probabilistic ﬁnite automata are related to
HMMs, and show how to trivially convert any HMM into an OOM. OOMs and PSRs share
the notion of a “predictive state” for the modeled process, which can be either implicit (as
in the case of OOMs) or explicit (as for PSRs). Any PSR is essentially an input-output
(IO)-OOM, while any OOM can be converted to a PSR by making the state “interpretable”.
Finally, PSRs generalize POMDPs in the same way that OOMs generalize HMMs.
Section 4 on learning is the main technical contribution of this paper. We present a
learning framework that covers the cases of SMA, OOMs and PSRs in a uniﬁed way. The

104

Links Between MA, OOMs and PSRs

only diﬀerence for the model classes concerns the way that estimates are obtained from
the sample data. To turn the learning framework into a concrete algorithm, several design
choices need to be made. Depending on these, many algorithms that have been proposed
in the literature are recovered. This uniﬁed viewpoint has several advantages. First of all,
modiﬁcations and improvements made for a speciﬁc model class can be generalized to other
learning algorithms. Additionally, the general learning framework allows us to identify
the key points responsible for statistical eﬃciency and thereby indicates a clear path for
improvements. In this section, we present generalized and simpliﬁed versions of two key
OOM learning algorithms — error controlling (EC) and eﬃciency sharpening (ES) — and
show that these are in fact closely related to spectral learning algorithms.

1.1 Notation
Let Σ∗ be the set of words over a ﬁnite alphabet Σ, including the empty word ε. Symbols
from the alphabet Σ will be denoted by normal variables as in x, y ∈ Σ, while words will
be denoted by variables with a bar over them, e.g., x, y ∈ Σ∗ . For x and y in Σ∗ , let xy be
the concatenation of words, and |x| denote the length of the word x. Furthermore, let Σk
denote the subset of words of length k . Let {xi | i ∈ N} = Σ∗ be an enumeration of Σ∗ such
that x0 = ε. We will be interested in characterizing functions f : Σ∗
→ K for K = R or
K = C, since these can be used to describe probabilistic languages, stochastic processes and
controlled processes (cf. Deﬁnitions 18, 20, and 28). These form a K -vector space which
we denote by K (cid:104)(cid:104)Σ(cid:105)(cid:105). For a given function f : Σ∗
→ K , we deﬁne the system matrix F as
the inﬁnite matrix F = [f (xj y i )]i,j∈N . Note that this is the transpose of what is commonly
known as the Hankel matrix. Furthermore, for a given function f we deﬁne the functions
fx : Σ∗
→ K by setting fx (y) := f (xy) for any sequences x, y ∈ Σ∗ . Note that these
functions correspond to the columns of the system matrix F . Let F := span{fx | x ∈ Σ∗
}
be the linear space spanned by these functions / the columns of F . Clearly, F ⊆ K (cid:104)(cid:104)Σ(cid:105)(cid:105).
We deﬁne rank(f ) := rank(F ) = rank(F ).
A d-dimensional sequential system (SS) is a structure M = (σ, {τz }, ωε ), which consists
of an initial state vector ωε ∈ K d , a matrix τz ∈ K d×d for each z ∈ Σ and an evaluation
we call a state of the SS M. Let τΣ = (cid:80)
function σ : K d → K . For x = x1 · · · xn ∈ Σ∗ let τx = τxn · · · τx1 , and let ωx = τxωε , which
z∈Σ τz .
If the function σ is linear, we call the sequential system linear. In this paper, we will be
dealing only with the linear case, so σ will just be a row vector, i.e., σ(cid:62)
∈ K d .
For a given SS M, we deﬁne its (external) function to be
∗
fM : Σ
fM (x) = στxωε
→ K,
Finally, we deﬁne the rank of a SS M to be rank(M) := rank(fM ).

(1)

2. Basic Properties of Sequential Systems

In this section we present the basic theory for sequential systems. This goes back to
Sch¨utzenberger (1961), to Carlyle and Paz (1971) who coined the term sequential systems,
and to Fliess (1974) but has been presented in various forms also for OOMs (Jaeger, 2000b)
and PSRs (Singh et al., 2004). Here, we present the theory in a concise, self-contained
fashion that can readily be turned into algorithms.

105

Thon and Jaeger

We begin with a technical result that lies at the heart of the whole theory.
Proposition 1 Let f : Σ∗
→ K be given. If rank(f ) = d < ∞, then there exist linear
operators ˜τz : F → F for each z ∈ Σ and a linear functional ˜σ : F → K that satisfy
˜τz (fx ) = fxz and ˜σ(fx ) = f (x) for al l x ∈ Σ∗ . Furthermore, ˜σ( ˜τx (fε )) = f (x) for al l
x ∈ Σ∗ , where ˜τx = ˜τxn ◦ · · · ◦ ˜τx1 .
Proof Let J ⊂ N be an index set denoting a maximal set of linearly independent columns
of the matrix F . Then clearly, B = {fxj | j ∈ J } is a basis for F . Deﬁne linear operators ˜τz
and a linear functional ˜σ by their action on these basis elements:
• ˜τz (fxj ) := fxj z for all z ∈ Σ,
Then fx = (cid:80)
j∈J λj fxj for suitable coordinates λj , and fxz = (cid:80)
• ˜σ(fxj ) := fxj (ε) = f (xj ).
We will show that then ˜τz (fx ) = fxz and ˜σ(fx ) = f (x) for all x ∈ Σ∗ . For this, let x ∈ Σ∗ .
y ∈ Σ∗ , we have fxz (y) = fx (zy) = (cid:80)
j∈J λj fxj (zy) = (cid:80)
˜τz ((cid:80)
j∈J λj fxj ) = (cid:80)
j∈J λj fxj z = fxz , and ˜σ(fx ) = ˜σ((cid:80)
j∈J λj fxj ) = (cid:80)
j∈J λj fxj z , since for any
j∈J λj fxj z (y). Therefore, ˜τz (fx ) =
j∈J λj fxj (ε) =
fx (ε) = f (x).
Finally, ˜σ( ˜τx (fε )) = ˜σ(fx ) = f (x) for all x ∈ Σ∗ .

The above proposition establishes a crucial property that makes this theory appealing,
as it means that the functions f = fε , fx = ˜τx (f ), the linear operators ˜τz and the linear
functional ˜σ have coordinate representations as vectors and matrices with respect to some
basis B for F . Note that this remains true even if rank(f ) = ∞, but the coordinate rep-
resentations will then be inﬁnite and of little practical use. The property f (x) = ˜σ( ˜τx (fε ))
(cf. Equation 1) means that the function f is fully described by the data ( ˜σ , { ˜τz }, fε ). If
these are given in some coordinate representation, then we have a SS representation:
Proposition 2 Let f : Σ∗
→ K be given.
d-dimensional SS M such that f = fM .
Proof Let B be a basis for F , and let M = (σ, {τz }, ωε ) be the coordinate representations
of ( ˜σ , { ˜τz }, fε ) with respect to B , where we are using the deﬁnitions for ˜σ and ˜τz from the
above Proposition 1. Then for any x ∈ Σ∗ , we have f (x) = ˜σ( ˜τx (fε )) = στxωε = fM (x).

If rank(f ) = d < ∞, then there exists a

Note that for the SS M constructed in Proposition 2 as a coordinate representation with
respect to some basis B of F , the states ωx = τxωε will be the coordinate representations
of the functions fx = ˜τx (f ) with respect to the basis B . Also note that due to Equation (1)
we may evaluate f (x) using the SS M without knowledge of the basis B .
The above proposition suggests that two SS might describe the same function f if and
only if they are representations for f with respect to diﬀerent bases for F . However, this is
only correct for so-called minimal SS, as will be detailed out in the following.
(cid:48) are equivalent, denoted by M ∼= M
(cid:48) , if they deﬁne the
Deﬁnition 3 Two SSs M and M
same function, i.e., if fM = fM(cid:48) . It is clear that this notion is an equivalence relation on
the set of al l SSs.

106

Links Between MA, OOMs and PSRs

We now introduce concepts needed to characterize the equivalence on SS. We give such
a characterization for minimal SS in Proposition 12. For this, we introduce the concept of
minimal SS, give a criteria for minimality in Corollary 8 and a procedure in Algorithm 2 to
construct an equivalent minimal SS.
Deﬁnition 4 For a given SS M we cal l the linear spaces W = span {τxωε | x ∈ Σ∗
state space and ˜W = span {(στx )(cid:62)
| x ∈ Σ∗
} the co-state space of M.
Deﬁnition 5 We cal l a d-dimensional SS M trimmed if it has ful l state and co-state
spaces, i.e., if W = ˜W = K d . We cal l a SS minimal if no equivalent model of lower
dimension exists.

} the

It will turn out in Corollary 8 that a SS is minimal if and only if it is trimmed. But
ﬁrst, we show how to construct bases for the state (and co-state) space of a given SS.

Proposition 6 The fol lowing procedure constructs a basis B for the state space W of a
given d-dimensional SS in time O(max{d, |Σ|}d3 ) (the construction of a basis ˜B for the
co-state space ˜W is analogous):
Algorithm 1: Compute a basis B for the state space W of a given SS
B ← {}, C ← {ωε}
while |C | > 0 do
ω ← some element of C , C ← C \ {ω}
if ω is linearly independent of B then
B ← B ∪ {ω}
C ← C ∪ {τz ω | z ∈ Σ}

Proof At any time during the run of the algorithm, B is a set of linearly independent
vectors. Furthermore the set C of “candidate vectors” increases by |Σ| elements each time a
new vector is added to the set B , but decreases by one element each run through the main
loop. Therefore, the algorithm terminates after at most d|Σ| + 1 runs through the main
loop, since there are at most d linearly independent vectors that can be added to B . Next
we examine the runtime of the algorithm. Checking ω for linear independence from B can
be done by checking PB ω = ω in time O(d2 ) if the orthogonal pro jection matrix PB onto
span(B) is known. This check is performed at most d|Σ| + 1 times, yielding a complexity of
O(d3 |Σ|). Clearly, the matrix PB must be updated every time a vector is added to B , which
is a O(d3 ) operation that needs to performed at most d times, giving a total complexity of
O(d4 ). Finally, every time a vector ω is added to B , the set C is increased by |Σ| vectors,
each of which requires time O(d2 ) to be computed from ω , for a total time complexity of
O(d3 |Σ|). Adding these together gives the claimed time complexity.
Finally, we show that the returned set B is indeed a basis of the state-space W . Ob-
serve that for all ω ∈ B and for all z ∈ Σ, the vectors τz ω have been added as “candidate
vectors” to the set C at some point during the run of the algorithm — namely when ω was
added to B . Each of these vectors is checked in turn and is at that point either linearly
dependent on B , or added to B . Therefore, these vectors τz ω are all linearly dependent on
the ﬁnal set B , i.e., τz (B) ⊆ span(B) for all z ∈ Σ. By linearity of τz this implies that also

107

Thon and Jaeger

τz (span(B)) ⊆ span(B) for all z ∈ Σ. So span(B) contains ωε and is closed under the action
of τz for all z ∈ Σ, which implies that {τxωε | x ∈ Σ∗
} ⊆ span(B). But B ⊂ {τxωε | x ∈ Σ∗
}
by construction of B . Together, this implies span(B) = span({τxωε | x ∈ Σ∗
}) = W .

The above is a polynomial time algorithm for which we have explicitly stated the runtime
complexity, since it is the workhorse for the operations of this section and dominates their
runtimes. Note further that the computed bases are by construction of the form B =
{τxj ωε | j ∈ J } and ˜B = {(στxi )(cid:62)
| i ∈ I } for suitable index sets I , J and corresponding
words xi and xj of length at most d, where d is the dimension of the SS. Also, the above
procedure allows us to check whether a given SS is trimmed.
The following proposition is the core technical result needed to establish the connection
between a SS being trimmed, having full rank, and being minimal.
Proposition 7 For a d-dimensional SS M, let {τxj ωε | j ∈ J } and {(στxi )(cid:62)
| i ∈ I } be
bases for W and ˜W respectively, and deﬁne F I ,J = [fM (xj xi )](i,j )∈I×J , then rank(M) =
rank(F I ,J ) ≤ min{|I |, |J |} ≤ d. Furthermore, if |I | = d or |J | = d then rank(M) =
min{|I |, |J |}.
k∈N and Φ = (τxk ωε )k∈N , as well as ΠI = ((στxi )(cid:62) )(cid:62)
i∈I ∈ K |I |×d
Proof Deﬁne Π = ((στxk )(cid:62) )(cid:62)
and ΦJ = (τxj ωε )j∈J ∈ K d×|J | . The rows of ΠI are a basis for the row space of Π and the
columns of ΦJ are a basis for the column space of Φ. Now F = ΠΦ and F I ,J = ΠI ΦJ .
Therefore rank(M) := rank(F ) = rank(ΠΦ) = rank(ΠI Φ) = rank(ΠI ΦJ ) = rank(F I ,J ).
Moreover, rank(ΠI ) = |I | and rank(ΦJ ) = |J | imply that rank(ΠI ΦJ ) ≤ min{|I |, |J |} ≤ d
as well as rank(ΠI ΦJ ) = |J | if |I | = d and rank(ΠI ΦJ ) = |I | if |J | = d.

From this, we obtain the following result, which allows us to check a d-dimensional SS
for minimality by checking whether the SS is trimmed, i.e., by constructing bases for the
state and co-state space and checking if these have dimension d.

Corollary 8 Let M be a d-dimensional SS. The fol lowing are equivalent:
(i) M is trimmed
(ii) rank(M) = d
(iii) M is minimal
Proof
If M has full rank, i.e., rank(M) = d, then M must be minimal, as any lower-
dimensional SS must have a lower rank and therefore cannot be equivalent. Conversely,
if M is minimal, then we must have rank(M) = d, since by Proposition 2 there exists a
rank(M)-dimensional equivalent SS. By Proposition 7 — and using the notation from the
proposition — we see that rank(M) = d ⇔ |I | = |J | = d, i.e., if and only if M is trimmed.
Next, we deﬁne the transformation of a SS by linear maps ρ and ρ(cid:48) . Such transformations
will serve as the basic operation on SS for all conversion operations.

108

Links Between MA, OOMs and PSRs

Deﬁnition 9 For a d-dimensional SS M = (σ, {τz }, ωε ) and any matrices ρ ∈ K n×d and
ρ(cid:48)
∈ K d×n , we deﬁne the n-dimensional SS ρMρ(cid:48) := (σρ(cid:48) , {ρτz ρ(cid:48)
}, ρωε ).
If ρ is non-singular, and ρ(cid:48) = ρ−1 , then this transformation will yield an equivalent
conjugated SS. If the SS is minimal, then this corresponds to a change of basis for the
underlying function space F .
Lemma 10 Let M = (σ, {τz }, ωε ) be a d-dimensional SS, and ρ ∈ Rd×d be non-singular.
Then M ∼= ρMρ−1 . We wil l cal l ρMρ−1 a conjugate of M.
Proof ∀x ∈ Σ∗ : fρMρ−1 (x) = (σρ−1 )(ρτxn ρ−1 ) · · · (ρτx1 ρ−1 )(ρωε ) = στxωε = fM (x).

We already know how to check for minimality. We now show how to convert a given SS
to an equivalent minimal SS using the introduced transformations on SSs.
Proposition 11 For a given SS M, the fol lowing procedure constructs an equivalent min-
(cid:48)(cid:48) :
imal SS M
Algorithm 2: Minimization of a SS M
1 Construct a basis {τxj ωε | j ∈ J } for the state space W of M
Set Φ = (τxj ωε )j∈J .
MΦ, where Φ† denotes the Moore-Penrose pseudoinverse of Φ.
(cid:48) = Φ†
Set M
2 Construct a basis {(σ (cid:48) τ (cid:48)
xi )(cid:62)
| i ∈ I (cid:48)
} for the co-state space ˜W (cid:48) of M
(cid:48) .
xi )(cid:62) )(cid:62)
Set Π(cid:48) = ((σ (cid:48) τ (cid:48)
i∈I (cid:48) .
(cid:48)(cid:48) = Π(cid:48)
(cid:48)Π(cid:48)† .
Set M
M
Proof Note that by construction the columns of Φ and Π(cid:48)(cid:62) form bases for the spaces W
and ˜W (cid:48) respectively. Therefore, Φ†Φ = id and ΦΦ†
|W = id, as well as (Π(cid:48)(cid:62) )†Π(cid:48)(cid:62) = id and
| ˜W (cid:48) = id. We can simply check equivalence, i.e., that for any x ∈ Σ∗ ,
Π(cid:48)(cid:62) (Π(cid:48)(cid:62) )†
fM(cid:48)(cid:48) (x) = σ (cid:48)(cid:48) τ (cid:48)(cid:48)
xn · · · τ (cid:48)(cid:48)
x1 ω (cid:48)(cid:48)
ε
(cid:48) τ (cid:48)
(cid:48)†
= σ (cid:48)
(cid:48) τ (cid:48)
(cid:48)†
(cid:48)ω (cid:48)
(cid:48)†
· · · Π
xn Π
Π
Π
Π
x1 Π
ε
= ω (cid:48)(cid:62)
(cid:48)(cid:62)
(cid:48)(cid:62)
† τ (cid:48)(cid:62)
† τ (cid:48)(cid:62)
(cid:48)(cid:62)
(cid:48)(cid:62)
· · · (Π
ε Π
(Π
)
)
x1 Π
xn Π
= σ (cid:48) τ (cid:48)
xn · · · τ (cid:48)
x1 ω (cid:48)
ε
†ωε
† τx1 ΦΦ
† τxn Φ · · · Φ
= σΦΦ
= στxωε = fM (x).
ε )j∈J = (Φ† τxj ωε )j∈J = Φ†Φ = id. This implies that M
Next, consider (τ (cid:48)
ω (cid:48)
(cid:48) has full state
xj
(cid:48) is |J | by
ε | j ∈ J } is a basis for W (cid:48) , since the dimension d(cid:48) of M
space W (cid:48) and that {τ (cid:48)
ω (cid:48)
xj
construction. By Proposition 7, |J | = d(cid:48) implies rank(M
(cid:48) ) = min(|I (cid:48)
|, |J |) = |I (cid:48)
|. By con-
struction |I (cid:48)
| = d(cid:48)(cid:48) where d(cid:48)(cid:48) is the dimension of M
(cid:48)(cid:48) . Furthermore, rank(M
(cid:48) ) = rank(M
(cid:48)(cid:48) )
(cid:48)(cid:48) is minimal.
(cid:48)(cid:48) so by Corollary 8 M
(cid:48) ∼= M
since M

†σ (cid:48)(cid:62)

(Π

(cid:48)(cid:62)

(cid:48)(cid:62)

)

As we can convert any SS to an equivalent minimal SS using the above Algorithm 2,
it will be suﬃcient to characterize equivalence only for minimal SS. This is done by the
following result.

109

Thon and Jaeger

z }, ω (cid:48)
(cid:48) = (σ (cid:48) , {τ (cid:48)
ε ) be minimal d-dimensional

Proposition 12 Let M = (σ, {τz }, ωε ) and M
SS. The fol lowing are equivalent:
(cid:48)
(i) M ∼= M
(cid:48) = ρMρ−1 for some non-singular ρ ∈ K d×d
(ii) M
z Φ(cid:48) , where {τxj ωε | j ∈
ε , σΦ = σ (cid:48)Φ(cid:48) and ∀z ∈ Σ : Πτz Φ = Π(cid:48) τ (cid:48)
(iii) ΠΦ = Π(cid:48)Φ(cid:48) , Πωε = Π(cid:48)ω (cid:48)
J } and {(στxi )(cid:62)
| i ∈ I } are bases for the state and co-state spaces W and ˜W of M
respectively, and Π = ((στxi )(cid:62) )(cid:62)
i∈I , Φ = (τxj ωε )j∈J , Π(cid:48) = ((σ (cid:48) τ (cid:48)
xi )(cid:62) )(cid:62)
i∈I , and Φ(cid:48) =
ω (cid:48)
(τ (cid:48)
ε )j∈J .
xj

Proof Lemma 10 establishes (ii) ⇒ (i). For (i) ⇒ (iii) note that fM = fM(cid:48) implies that
Πτ ¯z Φ = [f (xj ¯zxi )]i,j∈I×J = Π(cid:48) τ (cid:48)
¯z Φ(cid:48) for all ¯z ∈ Σ∗ , as well as Πωε = (f (xi ))(cid:62)
i∈I = Π(cid:48)ω (cid:48)
ε and
σΦ = (f (xj ))j∈J = σ (cid:48)Φ(cid:48) . Finally, to see (iii) ⇒ (ii), note that Π and Φ have full rank,
since M is minimal, so Π(cid:48) and Φ(cid:48) must also have full rank. Let ρ = Π(cid:48)−1Π = Φ(cid:48)Φ−1 , then
ρ−1 = ΦΦ(cid:48)−1 . We can now easily check that M
(cid:48) = ρMρ−1 .

(cid:48) by ﬁrst
Note that this allows us to decide equivalence for any given SS M and M
(cid:48) respectively using Algorithm 2, and
converting them to equivalent minimal SS ˜M and ˜M
then checking for equivalence by criteria (iii) from the above Proposition 12. The required
(cid:48) can be computed by Algorithm 1.
bases for the state and co-state spaces of ˜M and ˜M
The following proposition shows that any SS can be transformed into an equivalent SS
where σ and ωε can be essentially any desired vectors. This implies that it is no restriction
to assume some ﬁxed form for σ , as is sometimes done. For instance, in the case of OOMs
often σ = (1, . . . , 1) is used, while for MA often σ = (1, 0, . . . , 0) is assumed.
Proposition 13 Let M = (σ, {τz }, ωε ) be a d-dimensional SS, and let σ (cid:48)(cid:62) , ω (cid:48)
ε ∈ K d such
that σ (cid:48)ω (cid:48)
ε = σωε . Then there exists a non-singular linear map ρ such that ρMρ−1 =
(σ (cid:48) , {τ (cid:48)
z }, ω (cid:48)
ε ).
} to an orthogonal basis {σ(cid:62) , v2 , . . . , vd} of K d , and {σ (cid:48)(cid:62)
Proof Extend {σ(cid:62)
} to an or-
thogonal basis {σ (cid:48)(cid:62) , v (cid:48)
2 , . . . , v (cid:48)
d} of K d . We distinguish two cases:
If c := σωε = σ (cid:48)ω (cid:48)
ε (cid:54)= 0, then ρ1 = (ωε , v2 , . . . , vd )−1 and ρ2 = (ω (cid:48)
2 , . . . , v (cid:48)
ε , v (cid:48)
d ) are non-
ε and σρ−1 = σρ−1
1 ρ−1
singular. Let ρ = ρ2ρ1 . We can easily see that ρ2ρ1ωε = ρ2e1 = ω (cid:48)
2 =
1 ρ−1
c · e(cid:62)
2 = σ (cid:48) , since σ (cid:48)ρ2 = c · e(cid:62)
1 .
i ) ρ1 = ( σ(cid:62)
If σωε = σ (cid:48)ω (cid:48)
ε = 0, then (perhaps after reordering vi and v (cid:48)
σσ(cid:62) , ωε , v3 , . . . , vd )−1
and ρ2 = ( σ (cid:48)(cid:62)
σ (cid:48) σ (cid:48)(cid:62) , ω (cid:48)
3 , . . . , v (cid:48)
ε , v (cid:48)
d ) are non-singular. Let ρ = ρ2ρ1 . We can again check that
ε and σρ−1 = σρ−1
1 ρ−1
2 = e1ρ−1
2 = σ (cid:48) , since σ (cid:48)ρ2 = e1 .
ρ2ρ1ωε = ρ2e2 = ω (cid:48)

Finally, we introduce a special property called interpretability that a SS can have. This
concept has led to some confusion in the past — especially regarding the relationship be-
tween OOMs and PSRs. This is due to the fact that it has been deﬁned diﬀerently for
OOMs, IO-OOMs and PSRs, as will be discussed later. Another source of confusion is that
interpretability has been regarded as a crucial property for learning, which is however only

110

Links Between MA, OOMs and PSRs

true for the the very early learning algorithms. Here we give a deﬁnition of interpretability
that works for all models, and we will defer the discussion of the diﬀerent uses to the later
sections.
fM (xY ) = (cid:80)
Deﬁnition 14 A d-dimensional SS M is said to be interpretable w.r.t. the sets Y1 , . . . , Yd ⊂
Σ∗ if the states ωx take the form ωx = [fM (xY1 ), . . . , fM (xYd )](cid:62) for al l x ∈ Σ∗ , where
y∈Y fM (xy).
The following proposition and algorithm show how to make a SS interpretable, i.e., how
to convert any given SS into an equivalent interpretable form.
If ρ = [(στY1 )(cid:62) , . . . , (στYd )(cid:62) ](cid:62) is non-singular, where τY = (cid:80)
Proposition 15 Let M = (σ, {τz }, ωε ) be a d-dimensional minimal SS, and Y1 , . . . , Yd ⊂
Σ∗ .
(cid:48) :=
y∈Y τy , then M
(cid:48) is interpretable w.r.t. Y1 , . . . , Yd .
ρMρ−1 ∼= M and M
x = ρωx = [στY1 τxωε , . . . , στYd τxωε ](cid:62) = [fM (xY1 ), . . . , fM (xYd )](cid:62) .
Proof ∀x ∈ Σ∗ : ω (cid:48)

Corollary 16 For a SS M, the fol lowing algorithm returns an equivalent interpretable SS.
Algorithm 3: Make a SS M of rank d interpretable
(cid:48) using Algorithm 2.
1 Minimize M, i.e., ﬁnd an equivalent minimal SS M
2 Construct a basis {(σ (cid:48) τ (cid:48)
xi )(cid:62)
| i ∈ I } of the co-state space ˜W (cid:48) of M
(cid:48) using Algorithm 1
Select sets Yk = {xik } where {i1 , . . . , id} = I .
Set ρ = [(σ (cid:48) τ (cid:48)
Y1 )(cid:62) , . . . , (σ (cid:48) τ (cid:48)
)(cid:62) ](cid:62) .
Yd
(cid:48)ρ−1 .
3 Return ρM

Proof The above algorithm indeed returns an equivalent SS that is interpretable w.r.t. the
(cid:48) is minimal and therefore ρ is non-singular by construction.
selected sets Yk , since M

3. Versions of Sequential Systems

In this section we ﬁrst show that SS are an algebraic characterization of multiplicity au-
tomata (MA), and we mention the relationship to the more general class of weighted ﬁnite
automata (WFA) and its extension to weighted ﬁnite-state transducers (WFST). We then
deﬁne stochastic multiplicity automata (SMA), observable operator models (OOMs) and
predictive state representations (PSRs), which are known to generalize probabilistic ﬁnite
automata (PFA), hidden Markov models (HMMs) and partially observable Markov decision
processes (POMDPs), respectively. We show that these are all instances of SSs that are
used to model diﬀerent kinds of ob jects. Furthermore, we examine the relations between
these models. An overview is given in Figure 1.

111

Thon and Jaeger

Figure 1: SMA, OOMs and PSRs are versions of SSs that model probabilistic languages,
stochastic processes and controlled processes respectively, and strictly generalize
PFA, HMMs and POMDPs respectively.

3.1 Multiplicity Automata and Weighted Automata

The above deﬁnition of linear ﬁnite dimensional SS is an equivalent algebraic way of looking
at a type of automata that were introduced by Sch¨utzenberger (1961) and are most com-
monly known as multiplicity automata (Salomaa and Soittola, 1978; Berstel and Reutenauer,
1988). We will give a very brief introduction.

Deﬁnition 17 A K -multiplicity automaton (MA) is a structure (cid:104)Σ, Q, ϕ, ι, τ (cid:105), where Σ
is an alphabet, Q is a ﬁnite set of states, ϕ : Q × Σ × Q → K is the state transition
ϕ(q , xz , q (cid:48) ) = (cid:80)
function, ι : Q → K is the initialization function, and τ : Q → K is the termination
function. The state transition function is extended to words by setting ∀x ∈ Σ∗ , z ∈ Σ :
s∈Q ϕ(q , x, s)ϕ(s, z , q (cid:48) ), and ϕ(q , ε, q (cid:48) ) = 1 if q = q (cid:48) and 0 otherwise. A
(cid:88)
multiplicity automaton M then deﬁnes a function
∗
ι(q)ϕ(q , x, q (cid:48)
fM : Σ
fM (x) =
→ K,
q ,q (cid:48)∈Q
j . Then we have τxz = [ϕ(qj , xz , qi )]i,j = [(cid:80)
The formal equivalence of MA to linear ﬁnite-dimensional SS is easily seen by rewriting
the deﬁnition of MA in terms of matrix multiplication: Set ωε = [ι(qi )]i , τz = [ϕ(qj , z , qi )]i,j ,
and σ = [τ (qj )](cid:62)
qk ∈Q ϕ(qj , x, qk )ϕ(qk , z , qi )]i,j =
[ϕ(qk , z , qi )]i,k [ϕ(qj , x, qk )]k,j = τz τx and similarly fM (x) = στxωε . However, the above
deﬁnition of MA makes it apparent how MA are an extension of non-deterministic ﬁnite

)τ (q (cid:48)

).

112

stochasticmultiplicityautomata(SMA)predictivestaterepresentations(PSRs)≡input-outputOOMs(IO-OOMs)observableoperatormodels(OOMs)probabilisticﬁniteautomata(PFA)hiddenMarkovmodels(HMMs)partiallyobservableMarkovdecisionprocesses(POMDPs)stochasticprocessescontrolledprocessesprobabilisticlanguagesmultiplicityautomata(MA)≡(linear)sequentialsystems(SS)Links Between MA, OOMs and PSRs

automata (NFA) to WFA that add weights to the initial and terminal states as well as the
state transitions. The weight of a path from an initial state to a termination state is then
given by the product of the corresponding weights (hence the name multiplicity automata),
while the value fM (x) is computed by summing the weights of all paths compatible with x.
At this point we should mention that MA as deﬁned here are merely a special case of
WFA. The diﬀerence is that for MA we consider weights from a ﬁeld K (here K = R or
K = C), while for WFA the weights are only required to come from an algebraic structure K
called a semiring. There exists a large body of theory for WFA that generalizes the theory
of SS that we have presented in Section 2, which can be found in the recent textbook by
Droste et al. (2009). Note that while MA and WFA are formally closely related, there is
a diﬀerence in the way they are viewed and used. For instance, WFA are often considered
over the semiring R+ with weights given the interpretation of transition probabilities, which
are then called probabilistic ﬁnite automata (PFA). Such PFA are graphical models, and
the states Q are latent states. For R-MA, however, the weights are allowed to be negative,
and the weights as well as the states Q become abstract notions.
In other words, PFA
(and WFA in general) are typically used when the states and transition structure carry
some meaning, while MA are typically used as an abstract tool to characterize functions
f : Σ∗
→ K . This diﬀerence in perspective is reﬂected in the relationship of PFA to SMA,
HMM to OOM and POMDP to PSR described in the remainder of this Section 3. Note
that PFA are a special case of MA, as R+ ⊂ R. In fact, there exist functions f : Σ∗
→ R+
that can be described by a MA, but not by a PFA, i.e., MA are strictly more general than
PFA. This sequence of increasing generalization starting with ﬁnite automata (FA) can be
summarized as follows:

FA ⊂ NFA ⊂ PFA ⊂ MA ≡ SS ⊂ WFA.

Furthermore, there exists a natural extension of WFA to input-output systems that
are called weighted ﬁnite-state transducers (WFST). Here, the alphabet Σ is split as Σ =
ΣI × ΣO , where ΣI is regarded as input alphabet and ΣO as output alphabet. The function
fM : Σ∗
I × Σ∗
O → K is then viewed as describing a relation between ΣI and ΣO . Again,
K is in general only required to be a semiring, but a typical choice is K = R+ with
the interpretation of state transition probabilities, yielding a latent variable model called
probabilistic ﬁnite-state transducers (PFST). WFST are a ﬂexible class of models that have
been shown to unify several common approaches used in the the context of language and
speech processing; a survey is given by Mohri et al. (2002). Furthermore, IO-OOMs and
thereby PSRs (cf. Section 3.2 and Section 3.3) are in fact WFST with weights in K = R,
although they are not usually viewed this way, as WFST are typically seen as latent variable
models, while IO-OOMs and PSRs are not. However, since PFST are MA, as long as the
desired application merely requires the characterization of the function fM : Σ∗
I ×Σ∗
O → R+ ,
the SS learning algorithms described in Section 4 can be applied to the case of WFST as
well, as has been done recently by Balle et al. (2011).
Note that in the context of MA one is usually interested in characterizing functions
f : Σ∗
→ K , which are also called formal series in general and recognizable series if they
are computed by a MA. However, a MA M can also be used to recognize a language L ⊆ Σ∗
by setting LM = {x ∈ Σ∗
| fM (x) ⊆ J } for some subset J ⊆ K , e.g., J = {k ∈ K : k > κ}

113

Thon and Jaeger

for some threshold parameter κ ≥ 0. The class of languages recognizable by MA is known
to be strictly more general than the class of regular languages (Cortes and Mohri, 2000).
MA have received a lot of attention in the context of learning theory following the
discovery of eﬃcient learning algorithms (Bergadano and Varricchio, 1994; Ohnishi et al.,
1994) in an extended version of the exact learning model of Angluin (1987). This led to
further results on the learnability of several classes of DNF formulae (Bergadano et al.,
1996), the class of polynomials over ﬁnite ﬁelds, decision trees and others (Beimel et al.,
1996, 2000).

3.1.1 Stochastic Multiplicity Automata and Stochastic Languages
Additionally, MA have been applied in the context of probabilistic grammatical infer-
ence (Denis et al., 2006; Bailly et al., 2009), which is of particular interest to us because of
→ R that satisﬁes 0 ≤ f ≤ 1 and f (Σ∗ ) = (cid:80)
the close relationship of these approaches to OOMs and PSRs — as we shall see.
Deﬁnition 18 A function f : Σ∗
x∈Σ∗ f (x) =
1 is cal led a stochastic language, probabilistic language or just distribution over Σ∗ . A dis-
tribution fM on Σ∗ that is deﬁned by some MA M is cal led a rational stochastic language,
and a MA that deﬁnes such a distribution is cal led a stochastic MA (SMA).

Denis and Esposito (2008) give a comprehensive overview of rational stochastic languages
over various ﬁelds K , their relationships and relations to subclasses such as the important
class of probabilistic regular languages.
q∈Q ι(q) and ϕ(q , Σ, Q) = (cid:80)
(cid:80)
1, where ι(Q) = (cid:80)
Deﬁnition 19 A probabilistic (ﬁnite) automaton (PFA) is a SMA with the fol lowing re-
strictions: (i) ι, τ , ϕ have values in [0, 1], and (ii) ι(Q) = 1 and ∀q ∈ Q : τ (q) + ϕ(q , Σ, Q) =
q (cid:48)∈Q ϕ(q , x, q (cid:48) ). The stochastic lan-
x∈Σ
guages that can be represented by PFA are cal led probabilistic regular languages.
PFA are closely related to hidden Markov models (HMMs), and the relationship has
been detailed out by Dupont et al. (2005). It is however less well known that SMA are
closely related to observable operator models — a class of models for stochastic processes
that generalize HMM in a similar way that SMA generalize PFA.
We point out two results that are relevant in the context of modeling probabilistic lan-
guages by MA. First of all, it is known that it is an NP-hard problem to compute the
maximum likelihood estimate of parameters of a PFA with known structure from a given
training set of words (Abe and Warmuth, 1992). In practice, algorithms based on expec-
tation maximization (EM) (Dempster et al., 1977) are used which compute locally optimal
models instead. In contrast to this, the algebraic theory for SSs allows for powerful learning
algorithms (see Section 4) that often outperform EM-trained PFA or HMMs (Rosencrantz
et al., 2004; Jaeger et al., 2006a). However, these learning algorithms may return MA
that are arbitrarily close to SMA but fail to represent stochastic languages. It is in fact
undecidable whether a MA represents a stochastic language (Denis and Esposito, 2004).

3.2 Observable Operator Models and Stochastic Processes

Observable operator models were introduced by Jaeger (1997) as a concise algebraic charac-
terization of stochastic processes (see also Jaeger, 1998, 2000b; Jaeger et al., 2006b). These

114

Links Between MA, OOMs and PSRs

models are closely related to other algebraic characterizations of stochastic processes (Heller,
1965; Ito, 1992; Upper, 1997) that were studied in the context of deciding the equivalence
for HMMs (Gilbert, 1959), which came to a successful conclusion by framing HMMs in the
more general class of linearly dependent processes by Ito et al. (1992).
satisﬁes (i) f (ε) = 1 and (ii) ∀x ∈ Σ∗ : f (x) = (cid:80)
Deﬁnition 20 A (discrete-valued) stochastic process is a function f : Σ∗
→ [0, 1] that
x∈Σ f (xx). Such a function f deﬁnes the
probabilities of initial observation sequences. An observable operator model (OOM) is a
linear SS M such that fM is a stochastic process. A stochastic process that can be modeled
by a ﬁnite dimensional OOM is cal led a linearly dependent process.

One of the interesting features of OOMs is their notion of “state” of a (stochastic)
process. The idea that goes back to Zadeh (1969) is that a system state is really nothing
more than the information that is required to predict the future. In the case of OOMs, the
states ωx not only carry enough information to predict the future, they are (in a certain
sense) just future predictions.
To see this, recall that the states ωx of a SS are coordinate representations of the
functions fx w.r.t. some unknown basis B of the function space F . In the case of OOMs,
these functions take on the meaning that fx (y) = P (xy), i.e., they give the probability of
observing the sequence x followed by y . These functions are therefore called future prediction
functions in the context of OOMs. The operators {τz } are then state update operators
that update a state ωx (corresponding to the future prediction function fx after an initial
observation of x) according to the new observation z to the new state ωxz (corresponding
to the future prediction function fxz after an initial observation of xz ) — hence the name
“observable operators” (Jaeger, 1998).
For convenience, these functions fx , as well as the corresponding states ωx , are often
normalized to fx/f (x) and ωx/σωx respectively, since fx (y)/f (x) = στy ωx/σωx = P (y |x),
the probability of observing y given that x has been observed. Therefore, an OOM started
SMA, which is obtained by starting a SMA in the (normalized) state ωx/ (cid:80)
in the normalized state ωx/σωx represents a stochastic process started after an initial ob-
servation of x. This corresponds to the notion of a residual automaton in the context of
z∈Σ∗ στz ωx and
then represents a residual language (Denis and Esposito, 2004).

3.2.1 Relation to Hidden Markov Models

Any HMM can be trivially converted into an OOM. A hidden Markov model (HMM)
consists of an unobserved Markov process Xt that takes values in a ﬁnite set of states
Q = {s1 , . . . , sn}, and is governed by a stochastic state transition matrix T = [P (Xt+1 =
sj | Xt = si )]i,j . At each time step an observation Yt from Σ is made according to the emis-
sion vector Ez = [P (Yt = z | Xt = si )]i . Finally, an initial state vector π = [P (X0 = si )]i is
needed to fully specify the distribution of the stochastic process Yt (Rabiner, 1989).

Proposition 21 (Jaeger, 2000b) A given HMM (T , {Ez }z∈Σ , π) with N states is equivalent
to the OOM (σ, {τz }, ωε ) deﬁned by σ = (1, . . . , 1), τz = T (cid:62)diag(Ez ) and ωε = π . The rank
of the OOM is less than or equal to N .

115

Thon and Jaeger

Moreover, there are examples of OOMs of ﬁnite rank that cannot be modeled by any
HMM with a ﬁnite number of states. A prototypical example is the so-called “probability
clock” (Jaeger, 1998). It is an open question how to ﬁnd a “close” HMM for a given OOM.
While OOMs can be seen as a generalization of HMMs, one should keep in mind that there
is a fundamental diﬀerence in the notion of the state of the process. The state vector
in the case of a HMM is a stochastic vector that expresses the belief about the underlying
hidden state, while for an OOM it is a coordinate representation of the corresponding future
prediction function. However, under certain conditions it is possible to recover HMM-like
hidden states from an OOM (Hsu et al., 2009; Anandkumar et al., 2012).

3.2.2 Relationship to Stochastic Multiplicity Automata

The main diﬀerence between OOMs and SMA is that OOMs model stochastic processes,
while SMA model distributions on words. However, we can use a stochastic process to
model a distribution on words if we introduce a termination symbol $.
(cid:80)
Deﬁnition 22 An OOM M over the alphabet Σ$ = Σ ∪ {$} is terminating if fM (Σ∗$) :=
x∈Σ∗ στ$ τxωε = 1.
$ = pτΣ for some ﬁxed termination probability p ∈ (0, 1), where τΣ = (cid:80)
Proposition 23 An OOM M = (σ, {τz }, ωε ) over the alphabet Σ can be extended to a
z }, ωε ) over the alphabet Σ$ = Σ∪{$} by setting τ (cid:48)
(cid:48) = (σ, {τ (cid:48)
terminating OOM M
z = (1−p)τz
and τ (cid:48)
z∈Σ τz .
(cid:48) describes a stochastic process. Clearly, fM(cid:48) ≥ 0 and
x ωε = (cid:80)
Proof We ﬁrst show that M
fM(cid:48) (ε) = σωε = 1. To show property (ii), take any x ∈ Σ∗
$ by p (cid:80)
z∈Σ τz ). Then (cid:80)
fM(cid:48) (xz ) = σ((cid:80)
$ and note that by linearity
τ (cid:48)
k λk τxk ωε for suitable λk ∈ R and sequences xk ∈ Σ∗ (this is obtained by re-
x ωε = (cid:80)
k λk στΣ τxk ωε = (cid:80)
placing all occurrences of τ (cid:48)
τ (cid:48)
z )τ (cid:48)
xωε = (cid:80)∞
(cid:80)
x∈Σl σpτΣ (1 − p)l τxωε = (cid:80)∞
(cid:80)
z∈Σ$
z∈Σ$
x ωε =
στΣ τ (cid:48)
x ωε = fM(cid:48) (x ). Furthermore, fM(cid:48) (Σ∗$) =
k λk στxk ωε = στ (cid:48)
x∈Σ∗ στ (cid:48)
$ τ (cid:48)
l=0 p(1 − p)l = 1.
l=0
Deﬁnition 24 A terminating OOM M over the alphabet Σ ∪ {$} and a SMA A over the
Lemma 25 If A = (σ, {τz }, ωε ) is a minimal d-dimensional SMA, then τΣ∗ = (cid:80)∞
alphabet Σ are related, if fM (x$) = fA (x) for al l x ∈ Σ∗ .
exists and is equal to (Id − τΣ )−1 , where τΣ = (cid:80)
k=0 τ k
Σ
z∈Σ τz .
Proof We will show that the spectral radius1 ρ(τΣ ) satisﬁes ρ(τΣ ) < 1, which implies
i.e., there exists some λ ∈ C, |λ| ≥ 1 and v ∈ Cd
the lemma. Assume ρ(τΣ ) ≥ 1,
such that τΣv = λv . As A is minimal, we may ﬁnd sequences xj , xi ∈ Σ∗ such that
Π = ((στxi )(cid:62) )(cid:62)
SMA property fA (Σ∗ ) = (cid:80)∞
i∈I and Φ = (τxj ωε )j∈J with |I | = |J | = d are non-singular using Algo-
rithm 1. Then v = Φa for some a ∈ Cd , and Πτ k
ΣΦa = λkΠΦa for any k ∈ N. Now the
k=0 στ k
Σωε = 1 implies that Πτ k
ΣΦ → 0 as k → ∞, while the
right hand side λkΠΦa does not (note ΠΦa (cid:54)= 0), which is a contradiction.

1. For A ∈ Cn×n with eigenvalues λ1 , . . . , λk , the spectral radius is deﬁned as ρ(A) := max
i

|λi |.

116

Links Between MA, OOMs and PSRs

Proposition 26 Let A = (σ, {τz }, ωε ) be a minimal d-dimensional SMA. Then M =
(σ (cid:48) , {τ (cid:48)
z }, ω (cid:48)
ε ) is a related (d + 1)-dimensional terminating OOM over the alphabet Σ$ =
• σ (cid:48) = [σ (cid:80)∞
Σ ∪ {$}, if
z = (cid:2) τz 0
(cid:3) ,
Σ , 1] = [σ(Id − τΣ )−1 , 1],
k=0 τ k
• τ (cid:48)
τ (cid:48)
$ = [ 0 0
σ 1 ], and
0 0
• ω (cid:48)
ε = [ ωε
0 ].
σ((cid:80)∞
Proof We can simply check that for all z ∈ Σ∗
$
if z ∈ Σ∗ ,
k=0 τ k
Σ )τz ωε
fM (z ) = σ (cid:48) τ (cid:48)
z ω (cid:48)
if z = x$ . . . $ for some x ∈ Σ∗ ,
στxωε
ε =
otherwise.
0
This implies fM ≥ 0, fM (x$) = fA (x) for all x ∈ Σ∗ (M and A are related), as well as
= [σ (cid:80)∞
fM (Σ∗$) = fA (Σ∗ ) = 1 (M is terminating if it is an OOM). Furthermore, σ (cid:48)ω (cid:48)
ε = fA (Σ∗ ) =
Σ τΣ + σ, 1] = σ (cid:48) , which imply property (i) and (ii) for a stochastic
1 and σ (cid:48) τ (cid:48)
k=0 τ k
Σ$
process respectively.

Proposition 27 Conversely, let M = (σ, {τz }, ωε ) be a d-dimensional terminating OOM
over the alphabet Σ ∪ {$}. Then A = (στ$ , {τz }, ωε ) is a related d-dimensional SMA over
the alphabet Σ.
Proof Clearly, fA (x) = fM (x$) ≥ 0 for all x ∈ Σ∗ and fA (Σ∗ ) = fM (Σ∗$) = 1.

3.2.3 Historical Remarks

Note that our deﬁnition of OOMs given in Deﬁnition 20 diﬀers slightly from the deﬁnition
typically found in the literature.
First of all, the property (ii) for a stochastic process means that an OOM must satisfy
στΣωx = σωx for all x ∈ Σ∗ , which implies (ii)’ στ = σ if the OOM is minimal, but not in
general. The property (ii)’ is however often stated as part of the deﬁnition for OOMs. Our
above Deﬁnition 20 is therefore slightly more relaxed than the standard deﬁnition in the
case of non-minimal models, but this has no practical consequences.
Furthermore, for purely historical reasons, OOMs are sometimes required to satisfy
σ = (1, . . . , 1), which is mainly an issue of normalization (cf. Proposition 13). However,
(cid:80)
i = (cid:80)
this in turn has led to a more restrictive deﬁnition of interpretability for OOMs, since due
to property (i) of stochastic processes, an OOM that satisﬁes σ = (1, . . . , 1) can only be
interpretable with respect to sets Yk , if 1 = σωε = (1, . . . , 1) · [fM (Yi )](cid:62)
y∈Yk
P (y).
k
This is typically assured by requiring the sets Yk to partition Σl for some l. One can relax
this restriction on the sets Yk for the deﬁnition of interpretability — as we have done in
Deﬁnition 14 — if one is willing to drop the normalization requirement σ = (1, . . . , 1) as
well.

117

Thon and Jaeger

Nevertheless, even though the normalization requirement σ = (1, . . . , 1) is superﬂuous,
several of the OOM learning algorithms have been designed to yield OOMs normalized
such that σ = (1, . . . , 1) — oftentimes unnecessarily complicating the algorithms — and
some proofs have made use of this normalization as well. Later in Section 4 we present
simpliﬁed and generalized versions of the EC and ES learning algorithms by removing this
normalization restriction from the algorithms and proofs.

3.3 Predictive State Representations and Controlled Processes

Following the development of OOMs for stochastic processes, extensions to the case of
controlled processes — stochastic processes that depend on an external input at each time
step — were proposed by Jaeger (1998) as input-output OOMs, by Littman et al. (2001) as
predictive state representations and as a further variant as transformed PSRs by Rosencrantz
et al. (2004). All approaches are (in the linear case) equivalent and can be easily understood
in the framework of linear SSs.
p(x) = (cid:80)
Deﬁnition 28 A (discrete-valued) controlled (stochastic) process with input from ΣI and
output in ΣO is a function p : Σ∗
→ [0, 1] that satisﬁes (i) p(ε) = 1 and (ii) ∀x ∈ Σ∗ , a ∈ ΣI :
o∈ΣO
p(xao), where Σ = (ΣI ×ΣO ) and ao = (a, o). We deﬁne p(y |x) = p(xy)/p(x)
for p(x) (cid:54)= 0 and zero otherwise. An input-output OOM (IO-OOM) is just a SS that models
a control led process.

Note that the values of p are not probabilities. One may interpret p(a1o1 . . . anon ) as
P (o1 . . . on |a1 . . . an ), i.e., as the conditional probability of observing the outputs o1 . . . on
given the inputs a1 . . . an . However, one must take care, as the sequence of inputs may
depend on the observed outputs as well. This is explained in more detail in Section 4.1.

Deﬁnition 29 Let p be a control led process with predictive states ˙ωh deﬁned as ˙ωh =
[p(q1 |h), . . . , p(qd |h)](cid:62)
∈ Rd for h ∈ Σ∗ and some ﬁxed set of sequences q i ∈ Σ∗ . If ˙ωh is a
suﬃcient statistic for any history h ∈ Σ∗ , i.e., for every x ∈ Σ∗ there is a function mx :
Rd → [0, 1] such that p(x|h) = mx ( ˙ωh ) for al l h ∈ Σ∗ , then the sequences {q1 , . . . , qd} are
cal led core tests, which together with the initial state ˙ωε and pro jection functions mx form
a d-dimensional predictive state representation (PSR) for p. If the projection functions are
linear functionals (i.e., just row vectors in Rd ), then the PSR is cal led linear.

Note that PSRs share the notion of “state” with OOMs in that the state consists of the
information required to predict the future, but PSRs additionally require the entries of the
state vectors ˙ωh to be “predictions” p(q i |h) for the core tests q i . Such states are therefore
called predictive states.
We will only consider linear PSRs for controlled processes here, and show that these are
essentially SS for controlled processes (i.e., IO-OOMs) that are additionally interpretable
with respect to singleton sets (core tests). Note that there has been some confusion about
the precise relationship between PSRs and IO-OOMs, which we address in Sections 3.3.2
and 3.3.3 below.

118

Links Between MA, OOMs and PSRs

(cid:62)

(cid:62)

σ =

mao for any a ∈ ΣI .

Proposition 30 Let a d-dimensional linear PSR consisting of core tests q i , projection func-
tions mx and an initial state ˙ωε for a control led process p be given. Then an equivalent SS
(cid:88)
M = (σ, {τz }, ωε ) is obtained by setting
(cid:62) , . . . , (mzqd )
ωε = ˙ωε ,
τz = [(mzq1 )
and
]
o∈ΣO
Proof First note that σ ˙ωx = (cid:80)
mao ˙ωx = (cid:80)
Furthermore, M wil l be interpretable w.r.t. the sets {q i}.
p(ao|x) = 1 for all x ∈ Σ∗ such that
o∈ΣO
o∈ΣO
p(x) (cid:54)= 0 because p is a controlled process. Next, we prove that (*) ωx = p(x) ˙ωx and (**)
fM (x) = p(x) by induction on the length l of x:
• For l = 0 we have ωε = p(ε) ˙ωε and fM (ε) = σωε = σ ˙ωε = 1 = p(ε).
• Assume (*) and (**) are true for all x ∈ Σl . Let xz ∈ Σl+1 . Then (*) ωxz =
τz ωx = τz ˙ωxp(x) = [p(zq i |x)](cid:62)
i p(x) = [p(q i |xz )](cid:62)
i p(z |x)p(x) = ˙ωxz p(xz ) and (**)
fM (xz ) = σωxz = σ ˙ωxz p(xz ) = p(xz ).
Note that property (*) says that ωx = p(x) ˙ωx = [p(xq1 ), . . . , p(xqd )](cid:62) for all x, i.e., that M
is interpretable w.r.t. the sets {q i}.

Proposition 31 Conversely, let M = (σ, {τz }, ωε ) be a SS for a control led process p. Then
an equivalent PSR is obtained by making the SS interpretable with respect to singleton sets
{y i} for appropriate sequences y i ∈ Σ∗ (e.g., using Algorithm 3). We can then use these as
core tests for the PSR, and set mx = στx for al l x ∈ Σ∗ .
Proof We assume that the SS has been made interpretable w.r.t. the sequences y1 , . . . , yd .
Then the normalized states ˙ωh = ωh/σωh have the form ˙ωh = [p(y1 |h), . . . , p(yd |h)](cid:62) . Fur-
thermore, for all h ∈ Σ∗ : mx ˙ωh = στx ˙ωh = στx τhωε/στhωε = p(x|h), as desired.

Corollary 32 A linear PSR can be speciﬁed by the parameters ({mao}, {Mao}, ω(cid:62)
ε ) for
ao ∈ ΣI × ΣO , where Mao = τ (cid:62)
ao and mao = (στao )(cid:62) , and deﬁnes a control led process via
p(a1o1 · · · anon ) = ω(cid:62)
ε Ma1 o1 · · · Man−1 on−1 man on .
This is the usual way of specifying a PSR.

Note that transformed PSRs (TPSRs) are just PSRs that model controlled processes in
σ = ((cid:80)
the form of Corollary 32 without any further requirements (i.e., without the requirement
that the states need to be interpretable). These are readily converted to SSs by setting
mao )(cid:62) for any a ∈ ΣI and using the equations from the Corollary 32 otherwise.
o∈ΣO
Note that this may not give equivalent models if the PSR does not model a controlled
process.

119

Thon and Jaeger

3.3.1 Relation to Partially Observable Markov Decision Processes

Finally, we note how to convert POMDPs into SSs (which can then be further converted
to PSRs by making the SS interpretable, as described above). A POMDP with d states
Q = {s1 , . . . , sd} for a controlled process with input alphabet ΣI and output alphabet ΣO
consists of an initial belief state b ∈ Rd whose i-th element is the probability of the model
starting in state si , a state transition matrix Ta ∈ Rd×d for each action a ∈ A such that the
i, j -th entry of Ta is the probability of transitioning to state si from state sj if action a is
taken, and a vector Oao ∈ Rd for each action-observation pair ao ∈ (ΣI × ΣO ) whose i-th
entry is the probability of observing o after arriving in state si by taking action a (Kaelbling
et al., 1998).
Setting O (cid:48)
ao = diag (Oao ) we can summarize the belief-state update procedure for the
POMDP concisely by stating that a POMDP models a controlled stochastic process p via
the equation
p(a1o1 · · · anon ) = (1, . . . , 1)(O (cid:48)
an on Tan ) · · · (O (cid:48)
a1 o1 Ta1 )b.
Clearly, setting σ = (1, . . . , 1), τao = O (cid:48)
aoTa and ωε = b yields an equivalent SS.

3.3.2 IO-OOMs, Interpretable IO-OOMs, PSRs and TPSRs

We have shown above that IO-OOMs, PSRs and TPSRs are equivalent models in the sense
that they model the same class of controlled processes and that they can be readily converted
into one another. Furthermore, TPSRs are essentially IO-OOMs (except that the evaluation
functional σ is replaced by the set {mao} of evaluation functionals), while PSRs are TPSRs
(and therefore essentially IO-OOMs) with predictive states, which corresponds to IO-OOMs
being interpretable w.r.t. singleton sets (core tests). This is summarized in Table 1.

SSs for controlled
processes with. . .

single evaluation functional
σ

abstract,
uninterpretable states

predictive states

IO-OOMs

IO-OOMs that are
interpretable w.r.t.
singleton sets

set of evaluation functionals
{mao}
TPSRs

PSRs

Table 1: The diﬀerences between IO-OOMs, PSRs and TPSRs

Note that we have written “IO-OOMs that are interpretable w.r.t. singleton sets” in-
stead of simply “interpretable IO-OOMs” for a reason. This is because interpretability was
originally deﬁned for IO-OOMs in a more restrictive way (cf. Section 3.3.3). It has been
shown that not every IO-OOM has an equivalent “interpretable IO-OOM” (in the original
sense) (Singh et al., 2004), i.e., that “interpretable IO-OOMs” are less general than IO-
OOMs and PSRs. At the same time it was believed that some notion of interpretability
would be crucial for the learnability of such models, which is however not the case, as we
shall see in Section 4. Together, this has led to the false impression that PSRs are more
general than IO-OOMs.

120

Links Between MA, OOMs and PSRs

As the original notion of interpretability for IO-OOMs has turned out to be overly
restrictive, we propose to employ the notion of interpretability that we have introduced
here for SSs as the “correct” notion for IO-OOMs, and consider the original notion as
deprecated.
3.3.3 Historical Remarks
OOMs. Namely, IO-OOMs were originally required to satisfy (ii)’: ∀a ∈ ΣI : σ (cid:80)
The same remarks that we have made above in Section 3.2.3 for OOMs also apply to IO-
o∈ΣO
τao =
σ instead of the the property (ii) for a controlled process. This is equivalent for minimal
models, but slightly more restrictive in general. However, as every SS can be minimized,
this has no practical consequences.
Furthermore, IO-OOMs were originally typically required to satisfy σ = (1, . . . , 1), which
(cid:80)
(cid:80)
is again merely a matter of normalization. However, an IO-OOM that satisﬁes σ = (1, . . . , 1)
can only be interpretable with respect to the sets Yk , if 1 = σωε = (1, . . . , 1) · [fM (Yi )](cid:62)
i =
y∈Yk
p(y). It turns out that this can be assured by requiring the sets Yk to partition
k
Σl
O × {a1} × · · · × {al } for some l and a ﬁxed sequence a1 . . . al of inputs called a characteri-
zation frame. This restriction on the choice of sets Yk therefore became part of the original
deﬁnition of interpretability for IO-OOMs.
Unfortunately, unlike the case for OOMs, the resulting original notion of interpretability
for IO-OOMs has turned out to be a severe limitation (Singh et al., 2004).
However, one may use the more general notion of interpretability given in Deﬁnition 14
for IO-OOMs instead, if one is willing to drop the (unnecessary) normalization requirement
σ = (1, . . . 1).

3.4 Extensions

In this section we have presented SMAs, OOMs and PSRs as versions of linear sequen-
tial systems — or more generally weighted ﬁnite automata — that model probabilistic
languages, stochastic processes and controlled processes respectively, as is summarized in
Figure 1. For completeness, we wish to brieﬂy mention some extensions of these basic model
types that have been studied, but which are beyond the scope of this paper.
First of all, various non-linear SSs exists. For instance, several versions of quantum
ﬁnite automata have been studied (Kondacs and Watrous, 1997; Moore and Crutchﬁeld,
2000). One form are SSs (σ, {τx}, ωε ∈ CP d ) where the operators τx are unitary and
SSs (σ, {τx}, ωε ∈ Rd ) such that (cid:80)
σ(τxωε ) = ||πτxωε ||2 for some pro jection π and the Fubini-Study metric || · || (Moore and
Crutchﬁeld, 2000). A similar type of OOMs exist which are called norm-OOMs. These are
x∈Σ τ (cid:62)
x τx = I and σ(τxωε ) = ||τxωε ||2 . Such norm-OOMs
describe stochastic processes and can always be converted into an equivalent OOM (Zhao
and Jaeger, 2010). Recently, quadratic weighted automata have been proposed by Bailly
(2011), where a SS M is learnt for √f and a product SS M ⊗ M is constructed that
satisﬁes fM⊗M = f 2M ≈ f . All of these approaches avoid the “negative probabilities
problem”, where the estimated model fM may violate the requirement fM ≥ 0. Non-linear
versions of PSRs have also been investigated, which have been shown to in some cases yield
representations for deterministic dynamical systems that are exponentially smaller than a
minimal OOM representation (Rudary and Singh, 2003).

121

Thon and Jaeger

Furthermore, OOMs and PSRs are models for discrete-valued stochastic (controlled)
processes. Many real-world processes of interest are, however, continuous-valued. A con-
tinuous version of OOMs exists that extends semi-continuous HMMs (Jaeger, 2000a), and
WFST have been similarly extended to allow for continuous inputs (Recasens and Quattoni,
2013). Multivariate continuous inputs and outputs are handled using features of observa-
tions by reduced-rank HMMs (Siddiqi et al., 2010). So called predictive linear Gaussian
models (PLGs), which are based on PSRs, closely resemble linear dynamical system mod-
els (Rudary et al., 2005; Wingate and Singh, 2006a,b; Rudary and Singh, 2006, 2008) and
are further generalized by exponential family PSRs (Wingate and Singh, 2008b,a). A gen-
eralization of OOMs using Hilbert space embeddings was introduced by Song et al. (2010).
This has been further reﬁned and extended to include features and can now be employed
— among other things — for controlled processes and to planning in reinforcement learning
tasks (Boots and Gordon, 2010; Boots et al., 2010, 2013).

4. Learning

In this section we present a general approach to learning SSs from data. We show how
several of the learning algorithms that have been proposed for SMA, OOMs and PSRs can
be understood in this framework, and that in fact many of the proposed learning algorithms
are closely related.
We begin by establishing a result that lies at the heart of the learning algorithms, which
was formulated by Kretzschmar (2001) for the case of OOMs. Assuming a function fM can
(cid:48) from
be described by some minimal SS M, it allows us to reconstruct an equivalent SS M
data given in the form of ﬁnitely many function values of fM — as long as these are given
exactly and we know the rank d of the underlying model M. We will therefore refer to the
Equations (2) as the learning equations.

Proposition 33 For a minimal d-dimensional SS M = (σ, {τz }, ωε ), let {τxj ωε | j ∈ J }
and {(στ (cid:62)
xi )(cid:62)
| i ∈ I } be ﬁnite sets that span the state space W and the co-state space ˜W
respectively. Deﬁne F I ,J = [fM (xj xi )](i,j )∈I×J and F I ,J
z = [fM (xj zxi )](i,j )∈I×J . Further-
j∈J . Let C ∈ Rd×|I | and Q ∈ R|J |×d
more, deﬁne F I ,0 = [fM (xi )]i∈I and F 0,J = [fM (xj )](cid:62)
(cid:48) = (σ (cid:48) , {τ (cid:48)
z }, ω (cid:48)
be rank d matrices such that C F I ,J Q is invertible. Then the SS M
ε ) deﬁned
as fol lows is equivalent to M:
−1 ,
σ (cid:48)
= F 0,J Q(C F I ,J Q)
τ (cid:48)
−1 ,
z = C F I ,J
z Q(C F I ,J Q)
ω (cid:48)
ε = C F I ,0 .
xω (cid:48)
x = τ (cid:48)
xj z )j∈J , where ω (cid:48)
z = (ω (cid:48)
Furthermore, C F I ,J = (ω (cid:48)
xj )j∈J and C F I ,J
ε are states of the
(cid:48) .
SS M
Proof Let Π = ((στxi )(cid:62) )(cid:62)
i∈I , Φ = (τxj ωε )j∈J . Then F I ,J = ΠΦ, F I ,J
z = Πτz Φ, F I ,0 = Πωε
and F 0,J = σΦ. We can then simply calculate τ (cid:48)
z = CΠτz ΦQ(CΠΦQ)−1 = CΠτz (CΠ)−1 ,
as well as ω (cid:48)
ε = CΠωε and σ (cid:48) = σΦQ(CΠΦQ)−1 = σ(CΠ)−1 . That is, we have shown that
(cid:48) = ρMρ−1 for the non-singular transformation ρ = CΠ. Furthermore, C F I ,J = CΠΦ =
M
122

(2)

Links Between MA, OOMs and PSRs

ρΦ = (ρτxj ωε )j∈J = (τ (cid:48)
xj

ω (cid:48)
ε )j∈J , and analogously for C F I ,J
z

.

The matrices C and Q that appear in the learning Equations (2) are indeed arbitrary
(provided that C F I ,J Q has the correct dimension d and full rank), as long as the function
values fM (x) are given exactly. However, if one only has access to estimates ˆf (x), then
the selection of C and Q plays a crucial role in obtaining good model estimates, as will be
further discussed in Section 4.4.
Furthermore, note that we generally do not know a priori which sets of words to consider
such that {τxj ωε | j ∈ J } and {(στ (cid:62)
xi )(cid:62)
| i ∈ I } span the state and co-state spaces W and ˜W
of M. Proposition 6 guarantees that it suﬃces to consider all words of length at most d,
but the rank d of M is generally unknown as well. Selecting appropriate sets of words xi
and xj and an appropriate model dimension d are therefore crucial and non-trivial steps in
learning models from data.
We can turn the above Proposition 33 into a generic learning procedure for SSs:

Algorithm 4: General procedure for learning a SS from data
1 Obtain estimates ˆf (x) of the function values f (x) for words x ∈ Σ∗ .
2 Choose ﬁnite sets {xj | j ∈ J }, {xi | i ∈ I } ⊂ Σ∗ , which we call sets of indicative and
characteristic words respectively. Then assemble the estimates ˆf (x) into estimates of
the matrices ˆF I ,J , ˆF I ,J
, ˆF I ,0 and ˆF 0,J .
z

3 Find a reasonable target dimension d for the model to be learnt.
4 Choose C ∈ Rd×|I | and Q ∈ R|J |×d called the characterizer and indicator, such that
C ˆF I ,J Q is invertible.
5 Apply the learning Equations (2) to obtain a model estimate ˆM.
At this point we should clarify what is meant here by learning a model from data. For
general MA the goal is often to reconstruct an automaton from as few membership queries
— obtaining the value f (x) for some x ∈ Σ∗ — and equivalence queries — proposing a
function h and receiving a counterexample x such that h(x) (cid:54)= f (x) if h (cid:54)= f — as possible.
This is an extended version of the exact learning model of Angluin (1987). However, in the
case of SMA, OOMs and PSRs, the external function represents a distribution. Therefore,
in these cases it is usual to assume that we observe samples from this distribution and wish
to estimate model parameters from the given samples such that the estimated model best
describes the underlying distribution — “best” in a sense that depends on the context and
the approach taken by a speciﬁc learning algorithm.
We should also mention one common problem when learning SMAs, OOMs and PSRs
from data. Namely, even if the function fM in question can be described by a SMA, OOM
or PSR model M, the learnt model ˆM will only be an approximation to M and will describe
a function f ˆM that may not satisfy the properties of a probabilistic language, stochastic
process or controlled process, respectively, i.e., the learnt model ˆM may not be a SMA,
OOM or PSR. What typically happens is that the learnt model ˆM will predict “negative
probabilities” for certain sequences x. Moreover, it is an undecidable problem whether a

123

Thon and Jaeger

given SS ˆM satisﬁes f ˆM ≥ 0, and therefore, whether it is a SMA — a result that carries over
to OOMs and PSRs as well (Wiewiora, 2008). In practice, there are three basic ways to deal
with this “negative probabilities problem”: First of all, one can resort to alternative models
as described in Section 3.4 that preclude the problem by design. For the particular case
of quadratic weighted automata the learning procedure presented here still applies (Bailly,
2011), but in general one will need alternative learning algorithms. Secondly, one may
attempt to learn a restricted class of SS such as PFA, HMMs or POMDPs by enforcing
additional constraints on the parameters of the SS. This can be achieved either by adding a
set of convex constraints to a generalized version of the spectral learning method presented
in Section 4.4.2 (Balle et al., 2012), or by an additional conversion step (Anandkumar et al.,
2012), which however may fail. Finally, one may work with such an “invalid” SS model by
employing a simple and eﬀective heuristic as described by Jaeger et al. (2006b, Appendix
J) to normalize all model predictions to fall into the desired range.
Finally, we will brieﬂy remark on the runtime characteristics of the above learning
procedure. Steps 1 and 2 can be accomplished in time O(N ), where N is the size of
the training data, for most strategies mentioned in Section 4.2 by employing a suﬃx tree
or similar representation of the training data. For a given target dimension d, Step 4,
when solved via the EC (Section 4.4.3) or spectral algorithms (Section 4.4.3), requires the
O(d|I ||J |) computation of a d-truncated singular value decomposition (SVD) of ˆF I ,J , while
the ES algorithm (Section 4.4.4) requires O(d2 l max{|I |, |J |}) operations to compute C ,
where l is the (generally very small) average length of characteristic and indicative words,
and O(d|I ||J |) operations to compute Q — per iteration (but one typically uses a constant
number of iterations), which therefore amounts to a run-time of O(d|I ||J |) as well. Solving
the learning Equations (2) for Step 5 essentially requires the computation of the operators
ˆτz , which costs O(d|I ||J ||Σ|) operations. So for a known target dimension d, the above
learning procedure typically requires O(N + d|I ||J ||Σ|) operations. Step 3 can be solved
by computing a dmax -truncated SVD of ˆF I ,J for some upper bound dmax < min{|I |, |J |}
on the target dimension, which incurs a runtime costs of O(dmax |I ||J |), or by using cross-
validation, which requires repeatedly performing, for various choices of d, Steps 4 and 5
as well as evaluations on test data of size T , which we assume to be constant, incurring a
runtime cost of O(d log(d)|I ||J ||Σ|), where d is the ﬁnally selected model dimension.
In the following, we will discuss the steps of the learning procedure in more detail.

4.1 Obtaining Estimates ˆf (x)

This step clearly depends on the context we are dealing with. Recall that in the context
of SMA, the functions we are considering are distributions on words, while in the context
of OOMs and PSRs they represent stochastic processes and controlled processes respec-
tively. The following Remarks 34 to 36 summarize how to obtain these estimates in the
diﬀerent scenarios of probabilistic languages, stochastic processes and controlled processes,
respectively.

Remark 34 Let f : Σ∗
→ [0, 1] be a distribution on Σ∗ , and let S = (s1 , s2 , . . . , sN ) be
a col lection of N samples from f . Then ˆf (x) = #(x)
N , where #(x) denotes the number of
occurrences of x in the sample S , is a consistent estimator for f (x).

124

Links Between MA, OOMs and PSRs

In the case of stochastic processes, one typically observes few (or even just one) long
initial realization of the process. In this case it is still possible to obtain the desired estimates
if the stochastic process is stationary and ergodic2 by invoking the ergodic theorem and
using time-averages as estimates. The same idea is commonly used in the case of controlled
processes as well and called suﬃx-history method in the PSR community.
Remark 35 Let f : Σ∗
→ [0, 1] be a stationary and ergodic stochastic process, and let
¯s = s1s2 . . . sN be a ﬁnite initial realization of length N from this process. Then

#(x)
N − |x| + 1
where #(x) denotes the number of occurrences of x in the sequence ¯s is a consistent esti-
mator for f (x).

ˆf (x) =

,

In the case of controlled processes the situation is more complicated. It is important
to have a good understanding of the meaning of the value f (x) when f is a controlled
process and x = a1o1 . . . anon ∈ (ΣI × ΣO )n is some input-output sequence. Intuitively, this
is the probability of the system output o1 . . . on conditioned on the system input a1 . . . an .
This is sometimes written as f (a1o1 . . . anon ) = P (o1 . . . on | a1 . . . an ) even though this
notation is misleading, as it suggests that P (o1 . . . on | a1 . . . an ) = P (a1 o1 ...an on )
P (a1ΣO ...anΣO ) , which is
false (Bowling et al., 2006). To clarify this, consider the stochastic process that is speciﬁed
by the controlled process f together with some system input speciﬁcation. This stochastic
n(cid:89)
n(cid:89)
process is governed by probabilities of the form
k=1
k=1

P (ak | a1o1 . . . ak−1ok−1 ).

P (ok | a1o1 . . . ak ) ·

P (a1o1 . . . anon ) =

The second factor in the equation models the system input and is sometimes called the
input policy π , while the ﬁrst factor models the system output and is just the controlled
n(cid:89)
process f . Therefore, for x = a1o1 . . . anon ,
k=1

f (x) = P (o1 . . . on | a1 . . . an ) =

P (ok | a1o1 . . . ak ) =

P (x)
π(x)

(3)

.

Note that for the special case of a blind input policy π — one that does not depend on
the observed output, i.e., that satisﬁes P (ak | a1o1 . . . ak−1ok−1 ) = P (ak | a1 . . . ak−1 ) for all
x — we in fact do have π(x) = P (a1ΣO . . . anΣO ).
From the above Equation (3), the following estimates are derived (Bowling et al., 2006):
Remark 36 Let f : Σ∗
→ [0, 1] be a control led process, and let ¯s = a1o1 . . . aN oN be a ﬁnite
initial sample from f according to some input policy π , such that the resulting stochastic
n(cid:89)
process is stationary and ergodic. Then
k=1

#(a1o1 . . . ak ok )
#(a1o1 . . . ak )

ˆf (x) =

2. A stationary ergodic process is a stochastic process where the statistical properties do not change with
time (stationarity) and where these can be estimated as time-averages from a single long sample (ergod-
icity). For details, see for example the textbook by Gray (1988)

125

Thon and Jaeger

is a consistent estimator for f (x). If the input policy π is known, then

#(x)
N − |x| + 1 ·
is also a consistent estimator which may be used instead. Again, #(x) denotes the number
of occurrences of x in the sequence ¯s.

1
π(x)

ˆf (x) =

None of the above estimates exploits the rich structure of the matrix F . If required,
some of the convex constraints that the matrix F must satisfy can be ensured by applying
an additional normalization step to the estimated matrix ˆF , as done by McCracken and
Bowling (2006). These convex constraints — including a convex relaxation of the rank
constraint — may also be used to infer missing values if some entries ˆf (x) cannot be
obtained directly, which becomes relevant in the context of learning more general (e.g.,
non-stochastic) weighted automata (Balle and Mohri, 2012), or to infer sequence alignment
when learning WFST from unaligned input-output sequences (Bailly et al., 2013).

4.2 Choosing Indicative and Characteristic Words
Choosing indicative and characteristic words {xj | j ∈ J }, {xi | i ∈ I } ⊂ Σ∗ is equivalent
to selecting which columns J and rows I of the system matrix F to estimate. Clearly,
it is only possible to obtain a correct estimate for f if I and J are selected such that
rank(F ) = d = rank(F I ,J ). It is however unclear how to satisfy this if the true rank is
unknown or even impossible if rank(F ) = ∞ — as may often be the case for real-world
examples. Determining an appropriate rank for the model will be discussed in the following
section.
One approach is, however, to attempt to select minimal sets of indicative and charac-
teristic words such that rank(F ) = rank(F I ,J ). Such minimal sets are called sets of core
histories and core tests in the context of PSRs, and their selection is called the discovery
problem. This problem is easily solved by Algorithm 1 once a (minimal) SS model for f is
known. For the case where only function values of f are available, an iterative procedure has
been proposed (James and Singh, 2004) that, starting with the empty words, adds in each
iteration all length-one extensions of previously found core histories and tests, but retains
only a minimal set needed to span ˆF I ,J . Since any noisy matrix is typically non-singular,
some notion of numerical linear independence is used to decide which words to retain in
each step. It is important to note that there exist simple examples of ﬁnite rank where this
iterative procedure fails to deliver sets of core histories and tests (James and Singh, 2004),
i.e., it does not in general solve the discovery problem. A similar algorithm called DEES
has been proposed in the context of learning SMA (Denis et al., 2006). The algorithms for
learning MA in the exact learning framework also work by ﬁnding a minimal set of indica-
tive and characteristic words, but there it is assumed that the function f may be queried
exactly, and furthermore equivalence queries are employed to ﬁnd additional core tests and
histories (Ohnishi et al., 1994; Bergadano and Varricchio, 1994; Beimel et al., 2000).
It is important to note that there is no requirement to ﬁnd minimal or even small sets of
indicative and characteristic words, i.e., one does not need to solve the discovery problem
when learning SS models from data (and once a SS model has been learnt, the problem is
easily solved by Algorithm 1). In fact, using small such sets means that less of the available

126

Links Between MA, OOMs and PSRs

training data will enter the model estimation, i.e., the available data will be under-exploited.
It is therefore desirable to use (much) larger sets of indicative and characteristic words than
strictly needed.
An approach which is in some sense complementary is to use all sequences of a given
length l. By Proposition 6 one can ensure rank(F I ,J ) = d by choosing l ≥ d. However,
this is highly impractical, since the size of ˆF I ,J grows exponentially with l. Also, many of
the estimates in ˆF I ,J will be based on very few — if any — occurrences in the available
training data. Nevertheless, choosing a length l (cid:28) d and utilizing as indicative as well as
characteristic words all words of length l that occur at least once in the training data often
gives good results (Zhao et al., 2009a).
A further approach is to select as indicative and characteristic words all those that
actually occur in the data and therefore allow data-based estimates (Bailly et al., 2009).
However, it is reasonable to disallow indicative (resp. characteristic) words that are suﬃxes
(resp. preﬁxes) of some other indicative (resp. characteristic) word if they always occur
at the same positions in the training data, as these would just lead to identical columns
(resp. rows) in the estimated matrices that are based on the same parts of the training
data (Jaeger et al., 2006b). Moreover, one may select only the words that occur most
frequently in the data (Balle et al., 2014). These approaches yield a choice of indicative and
characteristic words that is matched to the available training data and can be computed
in time O(N ) where N is the size of the training data by using a suﬃx tree or similar
representation of the training data.
Finally, it is also possible to group words into sets of words (as is also done in Deﬁni-
tion 14) that we call events, and to use indicative and characteristic events in place of words.
This corresponds to adding the respective columns and rows in the matrices ˆF I ,J , ˆF I ,J
, etc.
z
and can be formally accomplished by a special selection of the indicator and character-
izer matrices Q and C . Finding good indicative and characteristic events was the strategy
adopted by early OOM learning algorithms (Jaeger, 2000b). A further generalization of
data can be performed more eﬃciently or accurately than computing ˆf (Y ) = (cid:80)
this idea of considering events in place of words is proposed by Wingate et al. (2007). Using
such events may carry an additional advantage if the estimation of ˆf (Y ) from the available
ˆf (x).
x∈Y
4.3 Determining the Model Rank

We should note that the goal of this step may be stated in two diﬀerent ways. First of all, we
may be interested in estimating the true rank of the external function f and use this as the
model rank. On the other hand, we may rather be interested in choosing any model rank
that allows for a good approximation of the external function f from the available data.
These goals are related, as one can only hope to estimate an exact model if the model rank
is at least rank(f ). However, they are not the same, and it depends on the context which
approach is most appropriate. For instance, if it is known that the external function f must
have a small ﬁnite rank, which may even carry some meaning, it may be desirable (and
well-deﬁned) to estimate this true rank from the data. On the other hand, when dealing
with real-world systems of possibly inﬁnite rank, and faced with generally limited training
data, it may not even make sense to speak of the correct model rank. In such cases one will
typically use the second approach, which is really an instance of the bias-variance dilemma.

127

Thon and Jaeger

4.3.1 Estimating the True Rank

For suitably chosen indicative and characteristic words, one can expect to have rank(f ) =
rank(F I ,J ). However, since one only has access to an estimate ˆF I ,J of this matrix, a
typical approach is to determine what is known as the numerical rank (or eﬀective rank or
pseudorank ). We give a brief description following Hansen (1998).
The numerical ε-rank rε of a matrix A may be deﬁned as the smallest rank of any matrix
that can be obtained from A by a small perturbation E of size at most ε:

rε (A) = min
rank(A + E ).
||E ||≤ε
alternatively that rε is the smallest k such that (cid:80)K
In terms of the singular values σ1 ≥ · · · ≥ σK of A this means that rε satisﬁes σrε >
ε ≥ σrε+1 if the size of the perturbation E is measured by the spectral norm || · ||2 , or
i=k+1 σ2
i ≤ ε2 if the Frobenius norm || · ||F
is used instead. Both criteria can be used to determine rε .
Assuming that A is only an estimate of an underlying matrix ˜A, it makes sense to
choose ε to be of the same order as the expected size of the error, i.e., ε ≈ E [||A − ˜A||]. The
numerical rank of A is then rε (A) for some reasonable choice of ε. Note that the notion of
numerical rank makes sense if the errors on matrix entries of A are of comparable magnitudes
and can be reasonably quantiﬁed, and if there is a signiﬁcant gap between σrε and σrε+1 .
Otherwise, the numerical rank is somewhat arbitrary. It is furthermore important to note
that the numerical rank measures how many dimensions can be signiﬁcantly distinguished
from noise. It is therefore only a lower bound for the true rank of the underlying matrix.
The main diﬃculty in determining the numerical rank of the matrix ˆF I ,J therefore lies
in ﬁnding a suitable ε. This may be approached by obtaining estimates for or bounds on the
variances of the individual matrix entries (Jaeger, 1998; James and Singh, 2004), which may,
however, diﬀer widely across ˆF I ,J . These approaches will therefore lead to very conservative
estimates of the rank. Still, these estimates will be consistent, i.e., will converge to the true
rank in the limit of inﬁnite training data.
Independent of such error estimates it may be reasonable to assume that there will be a
relative “gap” between σd+1 and σd in the singular value spectrum of ˆF I ,J around the true
rank d = rank(F I ,J ). A recently proposed method searches for such a gap starting from
σrε , where the numerical rank rε of ˆF I ,J is used as a lower bound for the true rank (Bailly
et al., 2009).

4.3.2 Finding a Suitable Model Rank

Intuitively speaking, the model rank should be chosen suﬃciently large to be able to repre-
sent the complexity of the data, but not too large, as otherwise overﬁtting results.
One standard approach is to use cross-validation. For this, one needs to split the avail-
able data into training and test data. One then estimates models of various ranks from the
training data and evaluates these on the test data, for instance by calculating the log likeli-
hood of the test data under the models. Finally, one chooses the model rank that gives the
best performance. Care must be taken when estimating models for controlled or stochastic
processes from one long training sequence ¯s, as this sequence cannot be partitioned arbi-
trarily into training and test sets, and the distribution over future observations given a

128

Links Between MA, OOMs and PSRs

history of observations at some time t may diﬀer from the initial distribution. Additionally,
performing cross-validation is computationally intense.
In comparison, the above methods based on calculating the numerical rank of ˆF I ,J are
elegant algebraic approaches to the problem. Recall that the numerical rank will reﬂect the
number of dimensions present in the training data that can be distinguished from noise. It
is therefore reasonable to postulate that the numerical rank of ˆF I ,J might be a well-suited
choice for the model dimension.
Interestingly, though, there is some evidence that at least the EC and spectral learning
procedures described in the following section do not seem to suﬀer much from overﬁt-
ting (Zhao et al., 2009a).
In practical applications it may therefore be viable to simply
pre-select a high model dimension.
Deeper insight into this crucial part of the learning procedure is unfortunately lacking.
Further research into this question is therefore needed.

4.4 Selecting the Characterizer and Indicator
The eﬀect of the characterizer C and indicator Q is to reduce the available data in ˆF I ,J , ˆF I ,J
,
z
ˆF I ,0 and ˆF 0,J to a d-dimensional representation, where d is the chosen target dimension for
the model to be learnt.
Assuming that d = rank(F ) = rank(C F I ,J Q), the matrices C F I ,J Q, C F I ,J
z Q, C F I ,0 ,
and F 0,J Q together contain the same information as F and are suﬃcient to reconstruct a
SS model for f via the learning Equations (2). The requirement that C F I ,J Q must have
full rank d therefore ensures that no information is lost.
In fact — provided that C F I ,J Q has full rank d — really any choice of characterizer and
indicator may be used and will lead to a consistent model estimation, i.e., a correct model
will be obtained in the limit of inﬁnite training data. Hamilton et al. (2013) show that for
certain dynamical systems a random choice of characterizer C does indeed work well.
However, in general the choice of characterizer C and indicator Q is central to achieving
statistical eﬃciency, i.e., making eﬃcient use of the available training data. This step lies
at the heart of the learning procedure, and in fact much research — even if not explicitly
stated — can be seen as optimizing this step of the learning algorithm.

4.4.1 By Selection / Grouping of Rows and Columns of ˆF

It is important to note that the choice of indicative and characteristic words discussed in
Section 4.2 can be viewed equivalently as a special choice of characterizer and indicator. To
see this, assume one could estimate the entire matrix ˆF from data. Then any selection of
rows I and columns J from ˆF can be achieved by characterizer and indicator matrices C, Q
of the form C = C (cid:48)C I and Q = QJ Q(cid:48) , where C I and QI are appropriate binary matrices
with a single one entry in the corresponding columns or rows, and zeros otherwise, such
that C I ˆF QJ = ˆF I ,J . This can easily be extended to account for groupings of words into
events by allowing several one entries per column / row of C I , QJ respectively.
One advantage of this point of view is that this immediately justiﬁes grouping of words
into events, as suggested in Section 4.2. But more importantly, this highlights that choosing
indicative and characteristic words as described in Section 4.2 is in fact a restricted approach
to the more general problem of ﬁnding appropriate characterizer and indicator matrices. We

129

Thon and Jaeger

argue that a good choice of characterizer and indicator is the key to achieving high statistical
eﬃciency of the learning procedure and that therefore the (pre-)selection of indicative and
characteristic words should be guided by trying to retain as much information from the
available training data as possible.
In other words, the (pre-)selection of indicative and
characteristic words in Section 4.2 is primarily a practical necessity that should rather be
seen as discarding rows and columns from ˆF that carry only little or no information.

4.4.2 Spectral Methods

Recall that the j -th columns of the matrices F and Fz correspond to the functions fxj
and fxj z , and that the operator τz of any minimal model M for f — regarded as a linear
operator ˜τz on the space F — satisﬁes ˜τz (fxj ) = fxj z (cf. Proposition 1). The matrix τz
is just a representation of this operator with respect to some basis of F . We can therefore
regard the columns of F and Fz as argument-value pairs for the operator ˜τz , from which
we can recover ˜τz . To obtain a matrix representation τz , we need to ﬁx some basis for the
column space F , which corresponds to mapping the columns of F and Fz to Rd — this is
accomplished by the characterizer C .
We are only given estimates ˆF I ,J and ˆF I ,J
. The idea of the spectral methods is to
z
ﬁnd an estimate of the column space ˆF by pro jecting the columns of ˆF I ,J and ˆF I ,J
to a
z
best rank d representation (best in the least squares sense). This is accomplished by the
d-truncated SVD. We then estimate the matrices ˆτz via least squares linear regression from
the so obtained argument-value pairs. Note that the column space F is already spanned by
the columns of F I ,J — if I and J are chosen appropriately — and we may therefore base
the estimate of the principal subspace ˆF on the estimate ˆF I ,J only. Formally, this means:
Algorithm 5: Spectral method for computing characterizer C and indicator Q
1 Compute UdSdV (cid:62)
d , the d-truncated SVD of ˆF I ,J .
†
d and Q = (C ˆF I ,J )† = VdS
2 Set C = U (cid:62)
d .
Note that UdSdV (cid:62)
indeed gives the best rank d approximation to ˆF I ,J with respect
d
the Frobenius norm by the Eckart-Young theorem (Eckart and Young, 1936). However, the
matrix F I ,J
ˆM reconstructed via the so learnt model ˆM — which will clearly have rank at
most d — will in general not be a best rank d approximation to ˆF I ,J . This is due to the
fact that constructing F I ,J
from the model ˆM enforces additional structure. Interestingly,
ˆM
we have observed that the reconstructed matrix F I ,J
ˆM is often a better approximation to the
true matrix F I ,J than either of ˆF I ,J and its best rank d approximation.
This spectral approach is often referred to as principal component analysis (PCA).
However, PCA typically involves mean-centering the data ﬁrst. PCA pro jects the data
onto a d-dimensional aﬃne subspace that contains the data mean, while here we know that
the data ˆF I ,J lie approximately on a true subspace (even though they do not have zero
mean). Mean-centering the data is therefore inappropriate in this context — nevertheless,
it it sometimes done anyway (Bailly et al., 2009). To avoid confusion, we refer to learning
algorithms based on this idea simply as spectral learning algorithms (Rosencrantz et al.,
2004; Hsu et al., 2009; Bailly et al., 2009; Siddiqi et al., 2010; Boots and Gordon, 2010;
Bailly, 2011; Balle et al., 2011, 2014). Furthermore, an online version of this spectral

130

Links Between MA, OOMs and PSRs

learning algorithm has been developed by Boots and Gordon (2011), whereas a modiﬁcation
that combines the subspace estimation step (determining the characterizer C ) and linear
regression step (solving the learning Equations 2) into a single optimization problem is given
by Balle et al. (2012).
Clearly, these methods are motivated by trying to ﬁnd a model ˆM of rank d such that
its external function f ˆM best approximates the estimated external function ˆf . To make
this precise, one needs to deﬁne a distance measure on functions in R(cid:104)(cid:104)Σ(cid:105)(cid:105). In the case of
stochastic languages the functions all lie in the Hilbert space l2 (Σ∗ ) and the metric of this
function space may be used. For stochastic processes, a natural choice may be the cross-
entropy. This will be related to ﬁnding a maximum-likelihood estimate of model parameters
from data. So far, none of these questions has been resolved. However, sample complexity
results that fall into the probably approximately correct (PAC) learning framework (Valiant,
1984) are available for several spectral learning algorithms (Hsu et al., 2009; Bailly et al.,
2009; Siddiqi et al., 2010; Bailly, 2011). These give bounds on the number or size N of
samples that are required to obtain a model estimate M that is approximately correct (i.e.,
such that |fM − f | < ε for a given ε and a speciﬁed distance measure) with probability at
least 1 − δ for a given δ . Typically, the required size N is shown to be polynomial in the
PAC parameters 1/ and 1/δ , as well as other parameters that depend on f such as the
alphabet size |Σ| and the rank of f .
Finally, we mention a shortcoming of the spectral methods as they are commonly used.
They implicitly assume that the variances of the estimates ˆf (xj xi ) are all of the same order.
This, however, is clearly not the case, which suggests that replacing the SVD computation by
a weighted low-rank matrix approximation (Markovsky and Huﬀel, 2007a) and the linear
regression of the learning Equations (2) by weighted total least squares (Markovsky and
Huﬀel, 2007b) may give better results, as long as weights that reﬂect the precision of the
estimates ˆf (x) can be estimated reliably from the available data. In fact, if the variances
Var( ˆf (xj xi )) can be estimated and — even approximately — factored as Var( ˆf (xj xi )) =
vj wi > 0, then this leads to a simple row and column weighted spectral learning method:

Algorithm 6: Row and column weighted spectral learning
− 1
− 1
1 Let DI = [diag(wi )i∈I ]
2 and DJ = [diag(vj )j∈J ]
2 be suitable row and column
weight matrices
2 Let ˜F I ,J = DI ˆF I ,J DJ and ˜F I ,J
z = DI ˆF I ,J
z DJ
3 Let ˜Ud ˜Sd ˜V (cid:62)
d be the d-truncated SVD of ˜F I ,J
†
d DI and Q = DJ (C ˜F I ,J DJ )† = DJ ˜Vd ˜S
4 Let C = ˜U (cid:62)
d .
We mention this particular row and column weighted approach here, as it is simple,
eﬀective, and we will show that it is closely related to the ES approach described in Sec-
tion 4.4.4.

4.4.3 The EC Algorithm

The error controlling (EC) approach selects characterizer and indicator matrices C and Q
that minimize an error bound for the relative approximation error of the estimated model
parameters (Zhao et al., 2009a). This algorithm was originally formulated for OOMs only,
and made use of the normalization σ = (1, . . . , 1) that is often used in the context of OOMs.

131

Thon and Jaeger

This in turn imposed additional restrictions on the admissible selections of indicative and
characteristic words. Here, we present a more general and yet simpliﬁed EC approach that
eliminates these restrictions and applies to learning SMA, OOMs, IO-OOMs and PSRs
alike.
To formalize this, ﬁrst assume we have ﬁxed C and Q, and derived estimated operators
ˆτz and correct operators τz from the estimates ˆF I ,J , ˆF I ,J
and the correct matrices F I ,J ,
z
F I ,J
respectively using the learning Equations (2). Note that these depend on the choice
z
of C and Q. To write things more concisely, denote the matrix obtained by stacking the
τz operators by τ∗ = [τz1 ; . . . ; τzl ] (using MATLAB notation), where Σ = {z1 , . . . , zl }, and
ˆτ∗ = [ ˆτz1 ; . . . ; ˆτzl ]. Similarly, construct the matrices F I ,J∗
and ˆF I ,J∗
by stacking the F I ,J
and
z
ˆF I ,J
respectively.
z

≤ κ

Proposition 37 For a given choice of C and Q, and using the above deﬁnitions, the esti-
(cid:33)
(cid:32)
mate ˆτ∗ has a relative approximation error
(cid:107)F I ,J − ˆF I ,J (cid:107)F +

(cid:107)τ∗ − ˆτ∗(cid:107)F
(cid:107)τ∗(cid:107)F
where ρ(τΣ ) is the spectral radius of the matrix τΣ , which is independent of the choice of C
and Q, and κ = (cid:107)C (cid:107)F (cid:107)Q(C ˆF I ,J Q)−1(cid:107)F .
This is a slightly improved and more general version of the central Proposition 3 pre-
sented in (Zhao et al., 2009a). For completeness, the proof is given in the appendix.
The EC algorithm then selects C, Q in such a way that the quantity κ is minimized,
which is equivalent to the optimization problem

√l
ρ(τΣ ) (cid:107)F I ,J∗ − ˆF I ,J∗ (cid:107)F

,

(4)

(C,Q) {(cid:107)C (cid:107)F (cid:107)Q(cid:107)F : C ˆF I ,J Q = Id},
(C, Q) = argmin
since every (C, Q) that minimizes κ gives a solution (C, Q(cid:48) ) to Equation (4) by substituting
Q(cid:48) = Q(C ˆF I ,J Q)−1 and noting (C ˆF I ,J Q(cid:48) ) = Id . This optimization problem can be solved
eﬃciently by the following iterative procedure (Zhao et al., 2009a):
Algorithm 7: The C , Q optimization resulting from the EC approach
initialize C ∈ Rd×|I | randomly
repeat
Q = (C ˆF I ,J )† , C = ( ˆF I ,J Q)†
until convergence of (cid:107)C (cid:107)F (cid:107)Q(cid:107)F
Although not previously realized, this turns out to be related to a well-known EM-based
algorithm for principal component analysis for which it is known that the rows of C (upon
convergence) will span the space of the ﬁrst d principle components of ˆF I ,J (Roweis, 1998).
We can use this relationship to gain the following insight.

Proposition 38 Assuming the model rank d is chosen such that the singular values σi
of ˆF I ,J satisfy σd > σd+1 , the EC algorithm as presented here and the spectral method
presented in the previous section wil l lead to equivalent models.

132

Links Between MA, OOMs and PSRs

Proof Note that the condition σd > σd+1 merely says that rank( ˆF I ,J ) ≥ d and that the
d-dimensional principal subspace of ˆF I ,J is unique. Let C and Q = (C ˆF I ,J )† be the char-
acterizer and indicator obtained by the spectral method, and let C (cid:48) and Q(cid:48) = (C (cid:48) ˆF I ,J )† be
the result of the above iterative procedure after convergence. Then the rows of C and C (cid:48)
will each span the same d-dimensional space (Roweis, 1998). This means that C = ρC (cid:48) for
some non-singular ρ ∈ Rd×d , and therefore Q = (ρC (cid:48) ˆF I ,J )† = (C (cid:48) ˆF I ,J )†ρ−1 = Q(cid:48)ρ−1 . By
Proposition 12 the learning Equations (2) will result in equivalent models.

In fact, the above optimization problem can also be solved non-iteratively by a d-
truncated SVD. This is a new result for which we give the full proof in the appendix:
− 1
d ≈ ˆF I ,J be the d-truncated SVD of ˆF I ,J . Then C ∗ = S
Proposition 39 Let UdSdV (cid:62)
d U (cid:62)
2
d
− 1
and Q∗ = (C ∗ ˆF I ,J )† = VdS
are a solution to the optimization problem in Equation (4)
2
d
— provided a solution exists at al l, i.e., rank( ˆF I ,J ) ≥ d.
Clearly, this solution (C ∗ , Q∗ ) will again yield an equivalent model. Finally, we note that
other versions of bounds on the relative approximation error than given in Proposition 37
may be considered instead, which can lead to choices of C and Q that give non-equivalent
models. The performance of these seems to be comparable, though (Zhao et al., 2009b).

4.4.4 Efficiency Sharpening

The ES algorithm has previously been worked out only for the case of stationary stochastic
processes and “traditional” OOMs where σ = (1, . . . , 1). Here we give an account of the ES
principle that is more general than in the original work, and we establish connections to
the spectral algorithms. The basic ES principle as we present it here may also be applied
to learning SMA, IO-OOMs and PSRs from data. However, the concrete ES algorithm
presented in Algorithm 8 makes use of several variance approximations and resulting sim-
pliﬁcations that are only valid for the estimators from Remark 35 for the case of stationary
stochastic processes.
The idea of the eﬃciency sharpening (ES) (Jaeger et al., 2006b) learning algorithm
is to view the learning Equations (2) as a model estimator parameterized by C (and Q),
and to select C such that the resulting estimator has minimum variance while still being
consistent. Furthermore, this optimal choice of C is derived from knowledge of a model M
for f , or in practice from a previous estimate thereof. To make this approach tractable,
some simplifying assumptions are made.
First, a simpliﬁed version of the learning Equations (2) is used, where the indicator is
taken to be Q = (C F I ,J )† . This leads to operator estimates
† .
(C ˆF I ,J )

ˆτz = C ˆF I ,J
z

Jaeger et al. (2006b) now argue that due to the (pseudo)inversion, the variance of ˆτz
is dominated by the variance of the factor C ˆF I ,J . The variance of a matrix is here taken
w.r.t. the Frobenius norm. The ES algorithm therefore strives to ﬁnd an admissible C
such that the variance of C ˆF I ,J is minimized — assuming knowledge of a model M for

133

Thon and Jaeger

f . A characterizer C is admissible if C F I ,J Q is invertible. This is solved by the following
proposition, which we state here in a more general form than in the original work (Jaeger
et al., 2006b):

Proposition 40 Let M = (σ, {τz }, ωε ) be a d-dimensional minimal SS for a function f :
Σ∗
→ R, and assume that ˆf (x) are unbiased and uncorrelated estimators for al l x ∈ Σ∗ .
(cid:88)
Deﬁne
(cid:62)
(cid:62)D2
C ∗
† .
(cid:62)
)i∈I , and D2
= ((στxi )
I ,
= Π
I = [diag(
j∈J
Then Var[C ˆF I ,J ] is minimized by the characterizer C ∗ + 0 among al l characterizers of the
form C ∗ + G that satisfy GΠ = 0.

Var[ ˆf (xj xi )])i∈I ]

where Π

The proof is given in the appendix, however, some explanatory remarks are in order.
First of all, the assumptions that the estimates ˆf (x) are unbiased and uncorrelated are
reasonable, yet not strictly correct, meaning that the characterizer C ∗ will only approximate
the theoretically optimal characterizer.
Next, we need a technical lemma to understand why it suﬃces to consider only charac-
terizers of the form (C ∗ + G) for some G satisfying GΠ = 0:
Lemma 41 If C ∗ has ful l row rank, then any admissible characterizer C can be written as
ρ(C ∗ + G) for some non-singular ρ ∈ Rd×d and G such that GΠ = 0.
Proof Let C be some admissible characterizer. Then CΠ ∈ Rd×d must be invertible. Also,
C ∗Π = (DI Π)(cid:62) (DI Π) will be invertible if C ∗ has full row rank. Choosing ρ = (CΠ)(C ∗Π)−1
and G = ρ−1 (C − ρC ∗ ) we can easily verify that C = ρ(C ∗ + G) and GΠ = 0.
Note that the characterizers C ∗ + G and ρ(C ∗ + G) will lead to equivalent models via
the learning Equations (2). Therefore, if the characterizer C ∗ is best among the class of
characterizers C ∗ + G where GΠ = 0 then it is also the overall best choice.
Furthermore, the condition that C ∗ must have full row rank can be assured by (i)
choosing indicative and characteristic sequences and the modeling dimension d accordingly,
so that d = rank(M) = rank(F I ,J ) = rank(Π) and (ii) assuming that the variance of the
estimators ˆf (x) is non-zero, ensuring that DI is invertible — which will typically be the
case in practice.
Finally, to compute C ∗ via Proposition 40, we need to know the variances of the esti-
mators ˆf (x) occurring in DI . Instead, we will replace DI by an approximation that can
be computed directly from the model M. The approximation we present here is only valid
for the case of stationary stochastic processes, but may be modiﬁed to cover the case of
probabilistic languages as well.
Consider the estimators ˆf (x) as in Remarks 34 and 35. It is reasonable to assume that
the counts #(x) follow a binomial distribution, i.e., #(x) ∼ bN ,p , where N is the length
of the training sequence ¯s and p = f (x). This gives Var[ ˆf (x)] = f (x)(1 − f (x))/N , which
we may further approximate by f (x)/N , as in practice the values of f (x) will typically be

134

Links Between MA, OOMs and PSRs

small for most sequences x. Also, the division by N is superﬂuous, as it cancels via the
(cid:88)
learning Equations (2). Using the approximation Var[ ˆf (x)] ≈ f (x), one can approximate
†
† ,
I ≈ ˜D2
D2
f (xj xi ))i∈I ]
= [diag(ΠτxJ ωε )]
where τxJ = (cid:80)
I := [diag(
j∈J

j∈J τxj . The approximation
(cid:62) ˜D2
C ∗
≈ C r := Π
I
is the characterizer that is actually used in the ES algorithm.
In the case of a stationary stochastic process and a choice of indicative words that
partition Σl or Σ≤l for some l one will have τxJ ωε = ωε , and therefore ˜D2
I = [diag(Πωε )]† .
In this case, the columns ci = (στxi )(cid:62)/στxi ωε of C r can be seen as the normalized states
(cid:62) = (ω(cid:62)
ε , {τ (cid:62)
z }, σ(cid:62) ),
r /ω(cid:62)
r under the reversed model M
ω r
ε ω r
r for the reversed words xi
xi
xi
(xi )1 · · · τ (cid:62)
r = τ (cid:62)
σ(cid:62) . This is essentially the original version given by Jaeger et al.
where ω r
xi
(xi )k
(2006b), and the reason why this characterizer was called the reverse characterizer. This
make-up of C r from states of the reversed process is also instrumental for the practical
algorithms given by Jaeger et al. (2006b).
Additionally, the ES algorithm further exploits the interpretation of columns of C F I ,J
and C F I ,J
as model states ωxj and ωxj z as given in Proposition 33. These columns give
z
be weighted by ((cid:80)
argument-value pairs from which the operators τz can be deduced — as we have seen before.
However, it is argued that in the face of estimates ˆF I ,J and ˆF I ,J
the j -th columns should
z
− 1
ˆf (xj xi ))
i∈I
2 prior to performing linear regression to better reﬂect the
weight of evidence that each column estimate is based on.
In practice a true model M is unknown. Therefore, the ES algorithm employs the
following iterative procedure (again, our treatment here is more general than the original
account by Jaeger et al. (2006b)):

2

Algorithm 8: The ES algorithm (for the case of stochastic processes)
1 Select some initial model estimate ˆM (e.g., via the learning Equations 2 using a
random choice of C and Q).
I = [diag( ˆΠ (cid:80)
repeat
Using the current model estimate ˆM, compute C = ˆΠ(cid:62)D2
Let Q = DJ (C ˆF I ,J DJ )† , where DJ = [diag((cid:80)
I ,
j∈J ˆτxj ˆωε )i∈I ]† .
where ˆΠ(cid:62) = (( ˆσ ˆτxi )(cid:62) )i∈I and D2
† 1
ˆf (xj xi ))j∈J ]
i∈I
2 .
Obtain a new model estimate ˆM via the learning Equations (2).
until some ﬁxed number of iterations, or some performance criteria of the estimated
models stops increasing.

3

4

Note that this procedure constructs a sequence of estimators along with a sequence of
model estimates. The rationale of such ES algorithms is that the sequence of estimators
increases in statistical eﬃciency, hence the name eﬃciency sharpening algorithms. The
ES iterations come with no convergence guarantees. Nevertheless, this procedure has been
found in practice to converge in very few iterations (3 – 5 typically suﬃce), and the results
are of a similar quality as obtained by spectral algorithms (comparisons in Zhao et al.,
2009a,b).

135

Thon and Jaeger

The ES algorithm is closely related to the row and column weighted spectral algorithm
presented in Section 4.4.2. Precisely:
z }, σ(cid:62) ) of rank d and let Π(cid:62) = ((στxi )(cid:62) )i∈I , DI = [diag((cid:80)
and DJ = [diag((cid:80)
Proposition 42 Assume F I ,J of rank d is determined by some underlying minimal model
† 1
M = (ω(cid:62)
ε , {τ (cid:62)
j∈J f (xj xi ))i∈I ]
2
† 1
2 . Let C r = Π(cid:62)D2
i∈I f (xj xi ))j∈J ]
I be the reverse characterizer, and let
C (cid:48) = ˜U (cid:62)
d DI be the characterizer obtained by the weighted spectral method, where ˜Ud ˜Sd ˜V (cid:62)
d is
the d-truncated SVD of DI F I ,J DJ . Then C r = ρC (cid:48) for some non-singular transformation
ρ.
Proof First, ˜Ud ˜Sd ˜V (cid:62)
d = DI F I ,J DJ , since F I ,J is assumed to have rank d. Now observe
that ˜Ud ˜Sd ˜V (cid:62)
d = DI F I ,J DJ = DI ΠΦDJ , where Φ = (τxj ωε )j∈J , and therefore the columns
of DI F I ,J , ˜Ud and DI Π all span im(DI F I ,J ). So C (cid:48) = ˜U (cid:62)
d DI and C r = (DI Π)(cid:62)DI = Π(cid:62)D2
I
have the same row space, and we can therefore ﬁnd such a transformation ρ.

This means that the reverse characterizer C r also gives a representation of the principal
subspace of the weighted matrix DI F I ,J . The main diﬀerence to the weighted spectral
method described in Section 4.4.2 is that C r is derived algebraically from an underlying
the data, e.g., ˆDI = [diag((cid:80)
model estimate, while the weighted spectral method estimates the principle subspace from
the weighted data matrix ˆDI ˆF I ,J with weights ˆDI that also need to be determined from
† 1
ˆf (xj xi ))i∈I ]
j∈J
2 .
5. Conclusion

We have shown that OOMs, PSRs and SMA are closely related instances of MA, and we
have presented a uniﬁed learning framework for estimating such models from data that
subsumes many of the existing learning algorithms. In presenting the learning framework,
we have isolated the key design choices that need to be made to obtain a concrete learning
algorithm. For each design choice we have surveyed the approaches that have been taken
in the past and have tried to give some guidance.
We brieﬂy summarize the choices that need to be made to obtain a concrete learning
algorithm. First of all, estimates of the system matrices ˆF I ,J and ˆF I ,J
z must be obtained
from the available training data. Individual entries may be estimated by the formulas given
in Section 4.1. However, it is of much greater importance to decide which entries need to
be estimated, that is, which rows I and columns J should be selected. This is discussed
in Section 4.2. While many of the existing algorithms attempt to choose as few rows and
columns to estimate as possible, we argue that this leads to poor statistical eﬃciency, and
that the selection should ideally be matched to the available training data. Next, one
must select a suitable model dimension d. This may be achieved by an algebraic criterion,
as described in Section 4.3.1, or by cross-validation. It is also possible to treat this as a
learning parameter that can be hand-tuned by the modeler. We note that it is generally
neither necessary nor advisable to set the target dimension to the correct rank of the
underlying system, as the optimal choice depends on the available training data. Finally,
the estimated system matrices ˆF I ,J and ˆF I ,J
need to be “compressed” to d × d matrices by
z
suitable characterizer and indicator matrices C and Q. A good selection of C and Q is vital

136

Links Between MA, OOMs and PSRs

to obtaining high statistical eﬃciency, and this is treated in detail in Section 4.4. We show
that several of the proposed approaches to selecting C and Q can be seen as variations of
a spectral learning algorithm presented in Section 4.4.2.
We conclude with a remark on implementing such a learning algorithm in practice.
Clearly, the main limiting factor is the size of the matrices ˆF I ,J and ˆF I ,J
, as these may
z
become very large. However, it is possible to obtain an eﬃcient sparse representation of
these matrices by employing a suﬃx tree representation of the training data (Zhao et al.,
2009b,a; Jaeger et al., 2006b). Furthermore, if one uses the method described in Section 4.4.4
one can avoid evaluating these matrices explicitly and instead calculate C ˆF I ,J and C ˆF I ,J
z
directly (Jaeger et al., 2006b).

Acknowledgements

We gratefully acknowledge the funding by the German Research Foundation (DFG) under
the pro ject JA 1210/5-1. We would also like to thank the anonymous reviewers for their
constructive and very helpful comments.

Appendix

=

τ∗ =

Proof [of Proposition 37](adapted from Zhao et al., 2009a) Let C∗ = diag(C, . . . , C ) (l
copies of C ). Using the introduced notation the learning Equations (2) can be written
(cid:16)
(cid:17) (cid:16)
(cid:17)−1
concisely to obtain:
−1(cid:17)−1
−1 (cid:16)
(cid:17)
(cid:16)
C ˆF I ,J Q + C (F I ,J − ˆF I ,J )Q
C∗ ˆF I ,J∗ Q + C∗ (F I ,J∗ − ˆF I ,J∗
)Q
−1(cid:17)−1
−1(cid:17) (cid:16)
(cid:16)
(cid:16)
(cid:17)
Id + C (F I ,J − ˆF I ,J )Q(C ˆF I ,J Q)
C∗ ˆF I ,J∗ Q + C∗ (F I ,J∗ − ˆF I ,J∗
(C ˆF I ,J Q)
)Q
Id + C (F I ,J − ˆF I ,J )Q(C ˆF I ,J Q)
(C ˆF I ,J Q)
C∗ (F I ,J∗ − ˆF I ,J∗
ˆτ∗ +
,
)Q
=
)Q)(C ˆF I ,J Q)−1 .
which implies τ∗ + τ∗C (F I ,J − ˆF I ,J )Q(C ˆF I ,J Q)−1 = ˆτ∗ + (C∗ (F I ,J∗ − ˆF I ,J∗
By rearranging, taking Frobenius norms and using the triangle inequality and submulti-
(cid:18)
(cid:19)
plicativity, we obtain
(cid:107)τ∗(cid:107)F (cid:107)F I ,J − ˆF I ,J (cid:107)F + (cid:107)C∗(cid:107)F
−1(cid:107)F
(cid:107)τ∗ − ˆτ∗(cid:107)F ≤ (cid:107)C (cid:107)F (cid:107)Q(C ˆF I ,J Q)
(cid:107)C (cid:107)F (cid:107)F I ,J∗ − ˆF I ,J∗ (cid:107)F
F = (cid:80)
F ≥ ρ(τΣ )2 , where τΣ = (cid:80)
= √l, and (cid:107)τ∗(cid:107)2
(cid:107)C∗ (cid:107)F
z∈Σ (cid:107)τz (cid:107)2
F ≥ (cid:107)τΣ(cid:107)2
z∈Σ τz , and
(cid:107)C (cid:107)F
Now
the result follows.
Note that in the original paper the inequality (cid:107)τ∗(cid:107)F ≥ 1√
was used instead, which depended
l
on the columns of τ∗ summing to 1. This was in turn insured by adding additional restric-
tions on the choice of characteristic words and characterizer C . These are now no longer
needed.

.

Lemma 43 Let D = diag(d1 , . . . , dn ) and S = diag(s1 , . . . , sn ) satisfying d1 ≥ · · · ≥ dn ≥ 0
and 0 ≤ s1 ≤ · · · ≤ sn , and let U be an orthogonal n × n matrix, i.e., U (cid:62)U = U U (cid:62) = I .
Then (cid:107)DU S (cid:107)F ≥ (cid:107)DS (cid:107)F .

137

Thon and Jaeger

F = (cid:80)n
(cid:80)n
i,j = 1 for all j and (cid:80)n
i,j=1 (diuij sj )2 . Furthermore, U (cid:62)U = U U (cid:62) = In implies that (∗)
Proof (cid:107)DU S (cid:107)2
i=1 u2
j=1 u2
i,j = 1 for all i. We will show the slightly stronger claim
that (cid:107)DU S (cid:107)2
F ≥ (cid:107)DS (cid:107)2
F for any matrix U satisfying (∗), which allows us to assume w.l.o.g.
that ui,j ≥ 0 for all entries in U , since only the squared entries u2
i,j appear in the expressions
First note that if U is lower triangular, then (∗) implies that U = In : (cid:80)n
for (cid:107)DU S (cid:107)2
F and (∗). So from now on we assume that U merely satisﬁes (∗) and that all
i,n = 0 for i < n. Then (cid:80)n
entries in U are non-negative.
(cid:3), and the condition (∗) must therefore hold for
n,n = 1. That is, U = (cid:2) Un−1 0
i=1 u2
i,n = 1
implies that u2
n,n = 1 and u2
j=1 u2
n,j = 1 implies that u2
n,j = 0 for
j < n, since u2
0
1
Un−1 as well. By induction on n, U = In . In this case (cid:107)DU S (cid:107)2
F = (cid:107)DS (cid:107)2
F .
So assume U is not lower triangular. Consider a row-wise ordering of matrix positions,
i.e., deﬁne ord(i, j ) = (i − 1)n + j , and let (i(cid:48) , j (cid:48) ) = argmin
(i,j ) {ord(i, j ) : j > i, ui,j (cid:54)= 0}, i.e., i(cid:48)
is the ﬁrst row of U to contain a non-zero element above the diagonal, and j (cid:48) is the column
index of the ﬁrst such entry within the i(cid:48) -th row. We call ord(i(cid:48) , j (cid:48) ) the order of U , and say
Now consider the i(cid:48) -th column of U . By the choice of i(cid:48) we must have (cid:80)i(cid:48)−1
i(cid:48) ,i(cid:48) = (cid:80)n
therefore (cid:80)n
that a lower triangular matrix has inﬁnite order.
i,i(cid:48) as well as (cid:80)n
i=1 u2
i,i(cid:48) = 0, and
i(cid:48) ,j − u2
i(cid:48) ,i(cid:48) ≥ u2
j=1 u2
i,i(cid:48) = 1 − u2
i=i(cid:48)+1 u2
i(cid:48) ,j (cid:48) . We can therefore ﬁnd a vector
v such that vi = 0 for i < i(cid:48) , vi(cid:48) = −u2
i=i(cid:48)+1 vi = u2
i(cid:48) ,j (cid:48) , and 0 ≤ vi ≤ u2
i(cid:48) ,j (cid:48)
for i = i(cid:48) + 1, . . . , n. Let U 2 = [u2
i,j ]i,j=1...n be the matrix of element-wise squares of entries
in U , and let ˜U 2 be obtained by subtracting the vector v from the i(cid:48) -th column of U 2 and
adding v to the j (cid:48) -th column of U 2 . Let ˜U be the matrix of element-wise square roots of
Also ˜U satisﬁes (∗), since (cid:80)n
entries in ˜U 2 .
We can easily check that all entries in ˜U 2 are non-negative, so that this is well-deﬁned.
i=1 vi = 0 by construction, and adding such a vector to one
column of ˜U 2 and subtracting from another does not change the row and column sums.
n(cid:88)
(cid:0)d2
j (cid:48) (cid:1)
Furthermore,
i vis2
i vis2
i(cid:48) − d2
n(cid:88)
i=1
(cid:33)
(cid:32)
= d2
i(cid:48) − s2
i(cid:48) vi(cid:48) (s2
i(cid:48) − s2
d2
i vi (s2
j (cid:48) ) +
j (cid:48) )
n(cid:88)
i=i(cid:48)+1
i(cid:48) − s2
= (s2
d2
i(cid:48) (cid:80)n
j (cid:48) ≤ 0 since j (cid:48) > i(cid:48) , and (cid:80)n
i(cid:48) vi(cid:48) +
j (cid:48) )
i=i(cid:48)+1
i(cid:48) vi(cid:48) + (cid:80)n
i vi ≤ d2
i(cid:48) ,j (cid:48) , while d2
i=i(cid:48)+1 vi = d2
Now s2
i(cid:48) u2
i=i(cid:48)+1 d2
i(cid:48) − s2
i(cid:48) vi(cid:48) =
F ≥ (cid:107)D ˜U S (cid:107)2
i vi ) ≤ 0. This shows that (cid:107)DU S (cid:107)2
−d2
i(cid:48) u2
i(cid:48) ,j (cid:48) , so (d2
i=i(cid:48)+1 d2
F . And ﬁnally,
the order of ˜U is larger than the order of U , as we have eliminated the non-zero element of
lowest order above the diagonal in U , and in turn have introduced only non-zero elements
above the diagonal of higher order (in rows below the i(cid:48) -th), or none at all.
By iterating this construction we arrive at a lower triangular matrix U ∗ with non-
F ≥ (cid:107)DU ∗S (cid:107)2
F = (cid:107)DS (cid:107)2
negative entries that satisﬁes (∗) and (cid:107)DU S (cid:107)2
F .

F − (cid:107)D ˜U S (cid:107)2
(cid:107)DU S (cid:107)2
F =

d2
i vi

.

138

Links Between MA, OOMs and PSRs

Proof [Proof of Proposition 39] Assume r = rank( ˆF I ,J ) ≥ d and let U SV (cid:62) = ˆF I ,J be
− 1
d U (cid:62)
d U SV (cid:62) )† =
the full SVD of ˆF I ,J . We can simply verify that indeed (C ∗ ˆF I ,J )† = (S
2
F = (cid:80)d
− 1
− 1
d )† = VdS
d V (cid:62)
, which implies that C ∗ ˆF I ,J Q∗ = C ∗ ˆF I ,J (C ∗ ˆF I ,J )† = Id , as required.
(S
2
2
d
− 1
i=1 σ−1
Furthermore, (cid:107)C ∗
(cid:107)F (cid:107)Q∗
d (cid:107)2
, where the σi are the singular values
(cid:107)F = (cid:107)S
2
i
of ˆF I ,J , which are also the diagonal elements of S . We will show that this is indeed the
minimum of (cid:107)C (cid:107)F (cid:107)Q(cid:107)F sub ject to C ˆF I ,J Q = Id .
Using the substitution C = C (cid:48)U (cid:62) and Q = V Q(cid:48) , we can see that minimizing (cid:107)C (cid:107)F (cid:107)Q(cid:107)F
sub ject to C ˆF I ,J Q = Id is equivalent to minimizing (cid:107)C (cid:48)
(cid:107)F (cid:107)Q(cid:48)
(cid:107)F sub ject to C (cid:48)SQ(cid:48) = Id and
that this will have the same minimal value. Let C (cid:48)
r , Q(cid:48)
r and Sr be truncated versions of C (cid:48) ,
Q(cid:48) and S that consist of the ﬁrst r columns, rows or rows and columns, respectively. Then
minimizing (cid:107)C (cid:48)
r (cid:107)F (cid:107)Q(cid:48)
r (cid:107)F sub ject to C (cid:48)
r SrQ(cid:48)
r = Id is equivalent and has the same minimal
value, because C (cid:48)SQ(cid:48) = C (cid:48)
r SrQ(cid:48)
r (since σi = 0 for i > r) and the additional columns in C (cid:48)
and rows in Q(cid:48) are best set to zero.
r (cid:107)F sub ject to C (cid:48)
r (cid:107)F (cid:107)Q(cid:48)
r Sr )† minimize (cid:107)C (cid:48)
Assume now that C (cid:48)
r SrQ(cid:48)
r = (C (cid:48)
r and Q(cid:48)
(cid:107)F = (cid:80)d
r = Id .
r SrQ(cid:48)
r (cid:107)F sub ject to C (cid:48)
We can select Q(cid:48)
r = (C (cid:48)
r Sr )† , as this minimizes (cid:107)C (cid:48)
r (cid:107)F (cid:107)Q(cid:48)
r = Id for a
i=1 σ−1
given C (cid:48)
r . It remains to show that (cid:107)C (cid:48)
r (cid:107)F (cid:107)Q(cid:48)
r (cid:107)F ≥ (cid:107)C ∗
(cid:107)F (cid:107)Q∗
.
i
r = LDR(cid:62)S−1
, and Q(cid:48)
r = (C (cid:48)
r Sr )† =
r Sr . Then C (cid:48)
r Sr be the SVD of C (cid:48)
Let LDR(cid:62) = C (cid:48)
r
RD†L(cid:62) . Let d1 , . . . , dd be the diagonal elements of D and let Dr be the r × r matrix
d(cid:88)
obtained by extending D with zero rows. Then
i=1

(cid:107)Dr S−1
r (cid:107)2
F =

i σ−2
d2
i

,

Lemma 43
≥

F (cid:107)Q(cid:48)
(cid:107)C (cid:48)
r (cid:107)2
r (cid:107)2
F =

F = (cid:107)DrR(cid:62)S−1
F = (cid:107)LDR(cid:62)S−1
(cid:107)C (cid:48)
r (cid:107)2
r (cid:107)2
r (cid:107)2
d(cid:88)
F
d−2
F = (cid:107)D†
(cid:107)Q(cid:48)
F = (cid:107)RD†L(cid:62)
(cid:107)2
(cid:107)2
r (cid:107)2
.
F =
i
i=1
(cid:33) (cid:32) d(cid:88)
(cid:32) d(cid:88)
(cid:33)
i = a2
Multiplying these expressions and substituting d2
i σi , we obtain
i σ−1
a−2
i σ−1
(cid:32)
(cid:33)
a2
d(cid:88)
d(cid:88)
i
i
i=1
i=1
a2
σ−2
i σ−1
σ−1
i
i +
+
(cid:32)(cid:18) ai
(cid:33)
j
(cid:19)2
a2
d(cid:88)
d(cid:88)
j
i=1
i,j=1
i<j
σ−2
aj −
i +
(cid:33)2
(cid:32) d(cid:88)
i=1
i,j=1
i<j
i=1

σ−1
i

a2
j
a2
i

aj
ai

+ 2

=

=

≥

i σ−1
σ−1
j

(cid:80)d
since this expression is clearly minimal when ai = 1 for all i. So we can conclude that
i=1 σ−1
(cid:107)C (cid:48)
. Therefore, C ∗ and Q∗ are in fact a minimal solution to the
r (cid:107)F (cid:107)Q(cid:48)
r (cid:107)F ≥
i
optimization problem (4).

,

139

Thon and Jaeger

(cid:105)

Var[C ˆF I ,J ]

=

(∗)
=

(cid:35)
cki ˆf (xj xi ) −

cki ˆf (xj xi )

(cid:88)
i∈I

(cid:33)2
ckif (xj xi )

(cid:104)
Proof [of Proposition 40] First, we calculate:
(cid:32)(cid:88)
(∗)
(cid:88)
d(cid:88)
||C ˆF I ,J − C F I ,J ||2
= E
F
(cid:34)(cid:88)
E
(cid:88)
d(cid:88)
j∈J
i∈I
k=1
Var
(cid:88)
(cid:88)
d(cid:88)
j∈J
i∈I
k=1
(∗∗)
(cid:88)
(cid:88)
(cid:88)
kiVar[ ˆf (xj xi )]
c2
=
j∈J
i∈I
k=1
Var[ ˆf (xj xi )] =
(cid:107)(C )i(cid:107)2
vi(cid:107)(C )i(cid:107)2
F ,
=
where (C )i is the i-th column of C , and vi = (cid:80)
F
i∈I
j∈J
i∈I
Our goal is now to minimize J (G) = Var[(C ∗ + G) ˆF I ,J ] = (cid:80)
j∈J Var[ ˆf (xj xi )]. Note that we have used
unbiasedness in (∗) and uncorrelatedness in (∗∗).
i∈I vi ||(C ∗ + G)i ||2
F sub ject
to the constraints hk,l (G) = [GΠ]k,l = 0 for k , l = 1 . . . d. Note that if vi = 0 for some i, then
the i-th column of G does not inﬂuence the value of J (G), and we may w.l.o.g. ﬁx (G)i = 0
and replace the equality constraints by ˜hk,l (G) = [GDD†Π]k,l = 0, where D = diag[(vi )i∈I ].
This is a convex quadratic programming problem, therefore G = 0 will be a solution if and
d(cid:88)
only if it satisﬁes the KKT conditions
∂hk,l
∂ J
∂G
∂G
k,l=1
∀k , l = 1 . . . d : ˜hk,l (G) = 0,
isﬁed for all k , l by G = 0. We can calculate (cid:80)d
for some Lagrange multipliers λk,l ∈ R. Clearly, the latter condition ˜hk,l (G) = 0 is sat-
∂G (G) = λΠ(cid:62)D†D , where
∂ ˜hk,l
k,l=1 λk,l
∂G (G) = 2(C ∗ + G)D = 2(Π(cid:62)D2
λ ∈ Rd×d , [λ]k,l = λk,l , as well as ∂ J
I + G)D . The ﬁrst
condition is then satisﬁed by G = 0 with λ = −2I , since Π(cid:62)D2
I D = Π(cid:62)D†D by deﬁnition
of DI .

(G) = 0, and

(G) +

λk,l

References

Naoki Abe and Manfred K. Warmuth. On the computational complexity of approximating
distributions by probabilistic automata. Machine Learning, 9:205–260, 1992.

Animashree Anandkumar, Daniel Hsu, and Sham M. Kakade. A method of moments
for mixture models and hidden markov models. In Shie Mannor, Nathan Srebro, and
Robert C. Williamson, editors, Proceedings of the 25th Annual Conference on Learning

140

Links Between MA, OOMs and PSRs

Theory (COLT 2012), volume 23 of JMLR Workshop & Conference Proceedings, pages
33.1–33.34, 2012.

Dana Angluin. Queries and concept learning. Machine Learning, 2(4):319–342, 1987.

Rapha¨el Bailly. Quadratic weighted automata: Spectral algorithm and likelihood maxi-
In Chun-Nan Hsu and Wee Sun Lee, editors, Proceedings of the 3rd Asian
mization.
Conference on Machine Learning (ACML 2011), volume 20 of JMLR Workshop & Con-
ference Proceedings, pages 147–163, 2011.

Rapha¨el Bailly, Fran¸cois Denis, and Liva Ralivola. Grammatical inference as a princi-
pal component analysis problem.
In Andrea Pohoreckyj Danyluk, L´eon Bottou, and
Michael L. Littman, editors, Proceedings of the 26th International Conference on Ma-
chine Learning (ICML 2009), volume 382 of ACM Proceedings, pages 33–40, 2009.

Raphael Bailly, Xavier Carreras, and Ariadna Quattoni. Unsupervised spectral learning of
ﬁnite state transducers. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and
K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26 (NIPS
2013), pages 800–808. Curran Associates, Inc., 2013.

Borja Balle and Mehryar Mohri. Spectral learning of general weighted automata via con-
strained matrix completion. In Peter L. Bartlett, Fernando C. N. Pereira, Christopher
J. C. Burges, L´eon Bottou, and Kilian Q. Weinberger, editors, Advances in Neural Infor-
mation Processing Systems 25 (NIPS 2012), pages 2168–2176, 2012.

Borja Balle, Ariadna Quattoni, and Xavier Carreras. A spectral learning algorithm for
ﬁnite state transducers. In Dimitrios Gunopulos, Thomas Hofmann, Donato Malerba, and
Michalis Vazirgiannis, editors, Machine Learning and Know ledge Discovery in Databases
- European Conference (ECML/PKDD 2011), Proceedings, Part I, volume 6911 of Lecture
Notes in Computer Science, pages 156–171. Springer, 2011.

Borja Balle, Ariadna Quattoni, and Xavier Carreras. Local loss optimization in operator
models: A new insight into spectral learning. In Proceedings of the 29th International
Conference on Machine Learning (ICML 2012). icml.cc / Omnipress, 2012.

Borja Balle, Xavier Carreras, Franco M. Luque, and Ariadna Quattoni. Spectral learning of
weighted automata – a forward-backward perspective. Machine Learning, 96(1-2):33–63,
2014.

Amos Beimel, Francesco Bergadano, Nader H. Bshouty, Eyal Kushilevitz, and Stefano Var-
In Proceedings of
ricchio. On the applications of multiplicity automata in learning.
the 37th Annual Symposium on Foundations of Computer Science (FOCS 1996), pages
349–358. IEEE Computer Society, 1996.

Amos Beimel, Francesco Bergadano, Nader H. Bshouty, Eyal Kushilevitz, and Stefano Var-
ricchio. Learning functions represented as multiplicity automata. Journal of the ACM,
47(3):506–530, 2000.

141

Thon and Jaeger

Francesco Bergadano and Stefano Varricchio. Learning behaviors of automata from mul-
tiplicity and equivalence queries.
In Maurizio A. Bonuccelli, Pierluigi Crescenzi, and
Rossella Petreschi, editors, Proceedings of the 2nd Italian conference on Algorithms and
Complexity (CIAC 1994), volume 778 of Lecture Notes in Computer Science, pages 54–62.
Springer, 1994.

Francesco Bergadano, Dario Catalano, and Stefano Varricchio. Learning sat-k-DNF formu-
las from membership queries. In Proceedings of the 28th Annual ACM Symposium on
Theory of Computing (STOC 1996), pages 126–130. ACM, 1996.

Jean Berstel, Jr. and Christophe Reutenauer. Rational Series and Their Languages, vol-
ume 12 of EATCS Monographs on Theoretical Computer Science. Springer, 1988.

Byron Boots and Geoﬀrey J. Gordon. Predictive state temporal diﬀerence learning.
In
J. Laﬀerty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors,
Advances in Neural Information Processing Systems 23 (NIPS 2010), pages 271–279.
MIT Press, 2010.

Byron Boots and Geoﬀrey J. Gordon. An online spectral learning algorithm for partially
observable nonlinear dynamical systems. In Wolfram Burgard and Dan Roth, editors,
Proceedings of the 25th AAAI Conference on Artiﬁcial Intel ligence (AAAI 2011). AAAI
Press, 2011.

Byron Boots, Sa jid M. Siddiqi, and Geoﬀrey J. Gordon. Closing the learning-planning loop
In Proceedings of the 9th International Confer-
with predictive state representations.
ence on Autonomous Agents and Multiagent Systems (AAMAS 2010), pages 1369–1370.
IFAAMAS, 2010.

Byron Boots, Geoﬀrey J. Gordon, and Arthur Gretton. Hilbert space embeddings of pre-
dictive state representations. In Proceedings of the 29th Conference on Uncertainty in
Artiﬁcial Intel ligence (UAI 2013), pages 92–101. AUAI Press, 2013.

Michael Bowling, Peter McCracken, Michael James, James Neufeld, and Dana F. Wilkinson.
Learning predictive state representations using non-blind policies. In William W. Cohen
and Andrew Moore, editors, Proceedings of the 23rd International Conference on Machine
Learning (ICML 2006), volume 148 of ACM Proceedings, pages 129–136, 2006.

Jack W. Carlyle and Azaria Paz. Realizations by stochastic ﬁnite automata. Journal of
Computer and System Sciences, 5(1):26–40, 1971.

Corinna Cortes and Mehryar Mohri. Context-free recognition with weighted automata.
Grammars, 3(2/3):133–150, 2000.

Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. Maximum likelihood estimation
from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39
(1):1–38, 1977.

Fran¸cois Denis and Yann Esposito. Learning classes of probabilistic automata. In Proceed-
ings of the 17th Annual Conference on Learning Theory (COLT 2004), volume 3120 of
Lecture Notes in Computer Science, pages 124–139. Springer, 2004.

142

Links Between MA, OOMs and PSRs

Fran¸cois Denis and Yann Esposito. On rational stochastic languages. Fundamenta Infor-
maticae, 86(1):41–77, 2008.

Fran¸cois Denis, Yann Esposito, and Amaury Habrard. Learning rational stochastic lan-
guages. In G´abor Lugosi and Hans-Ulrich Simon, editors, Proceedings of the 19th Annual
Conference on Learning Theory (COLT 2006), volume 4005 of Lecture Notes in Computer
Science, pages 274–288. Springer, 2006.

Manfred Droste, Werner Kuich, and Heiko Vogler. Handbook of Weighted Automata.
Springer, 2009.

Pierre Dupont, Fran¸cois Denis, and Yann Esposito. Links between probabilistic automata
and hidden Markov models: probability distributions, learning models and induction
algorithms. Pattern Recognition, 38(9):1349–1371, 2005.

Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank.
Psychometrika, 1(3):211–218, 1936.

Michel Fliess. Matrices de Hankel. Journal de Math´ematiques Pures et Appliqu´ees, 53:
197–222, 1974.

Edgar J. Gilbert. On the identiﬁability problem for functions of ﬁnite Markov chains. The
Annals of Mathematical Statistics, 30(3):688–697, 1959.

Robert M. Gray. Probability, Random Processes, and Ergodic Properties. Springer, 1988.

William L. Hamilton, Mahdi M. Fard, and Joelle Pineau. Modelling sparse dynamical sys-
tems with compressed predictive state representations. In Sanjoy Dasgupta and David
Mcallester, editors, Proceedings of the 30th International Conference on Machine Learn-
ing (ICML 2013), volume 28 of JMLR Workshop & Conference Proceedings, pages 178–
186, 2013.

Per Christian Hansen. Rank-Deﬁcient and Discrete Il l-Posed Problems: Numerical Aspects
of Linear Inversion. Society for Industrial and Applied Mathematics, Philadelphia, PA,
USA, 1998.

Alex Heller. On stochastic processes derived from Markov chains. The Annals of Mathe-
matical Statistics, 36(4):1286–1291, 1965.

Daniel Hsu, Sham M. Kakade, and Tong Zhang. A spectral algorithm for learning hidden
In Proceedings of the 22nd Annual Conference on Learning Theory
Markov models.
(COLT 2009), 2009.

Hisashi Ito. An Algebraic Study of Discrete Stochastic Systems. Unpublished doctoral
dissertation, University of Tokyo, Bunkyo-ku, Tokyo, 1992.

Hisashi Ito, Shun ichi Amari, and Kingo Kobayashi. Identiﬁability of hidden Markov infor-
mation sources and their minimum degrees of freedom. IEEE Transactions on Informa-
tion Theory, 38(2):324–333, 1992.

143

Thon and Jaeger

Herbert Jaeger. Observable operator models and conditioned continuation representations.
Arbeitspapiere der GMD 1043, GMD Forschungszentrum Informationstechnik, Sankt Au-
gustin, Germany, 1997.

Herbert Jaeger. Discrete-time, discrete-valued observable operator models: a tutorial. Tech-
nical Report 42, GMD Forschungszentrum Informationstechnik, Sankt Augustin, Ger-
many, 1998.

Herbert Jaeger. Modeling and learning continuous-valued stochastic processes with OOMs.
GMD Report 102, GMD Forschungszentrum Informationstechnik, Sankt Augustin, Ger-
many, 2000a.

Herbert Jaeger. Observable operator models for discrete stochastic time series. Neural
Computation, 12(6):1371–1398, 2000b.

Herbert Jaeger, MingJie Zhao, and Andreas Kolling. Eﬃcient estimation of ooms.
In
Y. Weiss, B. Sch¨olkopf, and J. Platt, editors, Advances in Neural Information Processing
Systems 18 (NIPS 2005), pages 555–562. MIT Press, 2006a.

Herbert Jaeger, MingJie Zhao, Klaus Kretzschmar, Tobias Oberstein, Dan Popovici, and
Andreas Kolling. Learning observable operator models via the ES algorithm. In Simon
Haykin, Jos´e C. Pr´ıncipe, Terrence J. Sejnowski, and John McWhirter, editors, New
Directions in Statistical Signal Processing: From Systems to Brains, Neural Information
Processing, chapter 14, pages 417–464. MIT Press, Cambridge, MA, USA, 2006b.

Michael R. James and Satinder P. Singh. Learning and discovery of predictive state rep-
resentations in dynamical systems with reset. In Carla E. Brodley, editor, Proceedings
of the 21st International Conference on Machine Learning (ICML 2004), volume 69 of
ACM Proceedings, pages 53–60, 2004.

Michael R. James and Satinder P. Singh. Planning in models that combine memory with
predictive representations of state. In Manuela M. Veloso and Subbarao Kambhampati,
editors, Proceedings of the 20th National Conference on Artiﬁcial Intel ligence (AAAI
2005), pages 987–992. AAAI Press, 2005.

Michael R. James, Satinder Singh, and Michael L. Littman. Planning with predictive state
representations. In Proceedings of the 3rd International Conference on Machine Learning
and Applications (ICMLA 2004), pages 304–311. IEEE Computer Society, 2004.

Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting
in partially observable stochastic domains. Artiﬁcial Intel ligence, 101(1-2):99–134, 1998.

Attila Kondacs and John Watrous. On the power of quantum ﬁnite state automata. In
Proceedings of the 37th Annual Symposium on Foundations of Computer Science (FOCS
1996), pages 66–75. IEEE Computer Society, 1997.

Klaus Kretzschmar. Learning symbol sequences with observable operator models. GMD
Report 161, GMD Forschungszentrum Informationstechnik, Sankt Augustin, Germany,
2001.

144

Links Between MA, OOMs and PSRs

Michael L. Littman, Richard S. Sutton, and Satinder P. Singh. Predictive representations
of state. In Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani, editors,
Advances in Neural Information Processing Systems 14 (NIPS 2001), pages 1555–1561.
MIT Press, 2001.

Ivan Markovsky and Sabine Van Huﬀel. Left vs right representations for solving weighted
low-rank approximation problems. Linear Algebra and its Applications, 422(2-3):540–552,
2007a.

Ivan Markovsky and Sabine Van Huﬀel. Overview of total least-squares methods. Signal
Processing, 87(10):2283–2302, 2007b.

Peter McCracken and Michael H. Bowling. Online discovery and learning of predictive
In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors, Advances in
state representations.
Neural Information Processing Systems 18 (NIPS 2005). MIT Press, 2006.

Mehryar Mohri, Fernando Pereira, and Michael Riley. Weighted ﬁnite-state transducers in
speech recognition. Computer Speech & Language, 16(1):69–88, 2002.

Cristopher Moore and James P. Crutchﬁeld. Quantum automata and quantum grammars.
Theoretical Computer Science, 237(1-2):275–306, 2000.

Hiroyuki Ohnishi, Hiroyuki Seki, and Tadao Kasami. A polynomial time learning algorithm
IEICE Transactions on Information and Systems, E77-D(10):
for recognizable series.
1077–1085, 1994.

Lawrence R. Rabiner. A tutorial on hidden Markov models and selected applications in
speech recognition. Proceedings of the IEEE, 77(2):257–286, 1989.

Adri`a Recasens and Ariadna Quattoni. Spectral learning of sequence taggers over continuous
sequences. In Hendrik Blockeel, Kristian Kersting, Siegfried Nijssen, and Filip Zelezn´y,
editors, Machine Learning and Know ledge Discovery in Databases - European Conference
(ECML/PKDD 2013), Proceedings, Part I, volume 8188 of Lecture Notes in Computer
Science, pages 289–304. Springer, 2013.

Matthew Rosencrantz, Geoﬀrey J. Gordon, and Sebastian Thrun. Learning low dimensional
predictive representations. In Carla E. Brodley, editor, Proceedings of the 21st Interna-
tional Conference on Machine Learning (ICML 2004), volume 69 of ACM Proceedings,
pages 695–702, 2004.

Sam Roweis. EM algorithms for PCA and SPCA. In Michael I. Jordan, Michael J. Kearns,
and Sara A. Solla, editors, Advances in Neural Information Processing Systems 10 (NIPS
1997), pages 626–632. MIT Press, 1998.

Matthew Rudary and Satinder P. Singh. Predictive linear-Gaussian models of controlled
stochastic dynamical systems. In William W. Cohen and Andrew Moore, editors, Pro-
ceedings of the 23rd International Conference on Machine Learning (ICML 2006), volume
148 of ACM Proceedings, pages 777–784, 2006.

145

Thon and Jaeger

Matthew Rudary and Satinder P. Singh. Predictive linear-Gaussian models of stochastic
dynamical systems with vector-value actions and observations. In Proceedings of the 10th
International Symposium on Artiﬁcial Intel ligence and Mathematics (ISAIM 2008), 2008.

Matthew Rudary, Satinder P. Singh, and David Wingate. Predictive linear-Gaussian mod-
els of stochastic dynamical systems. In Fahiem Bacchus and Tommi Jaakkola, editors,
Proceedings of the 21st Conference in Uncertainty in Artiﬁcial Intel ligence (UAI 2005),
pages 501–508. AUAI Press, 2005.

Matthew R. Rudary and Satinder Singh. A nonlinear predictive state representation. In
S. Thrun S. Becker and K. Obermayer, editors, Advances in Neural Information Process-
ing Systems 15 (NIPS 2002), pages 855–862. MIT Press, 2003.

Arto Salomaa and Matti Soittola. Automata-Theoretic Aspects of Formal Power Series.
Texts and Monographs in Computer Science. Springer, 1978.

Marcel Paul Sch¨utzenberger. On the deﬁnition of a family of automata. Information and
Control, 4(2-3):245–270, 1961.

Sa jid M. Siddiqi, Byron Boots, and Geoﬀrey J. Gordon. Reduced-rank hidden markov
In Yee Whye Teh and D. Mike Titterington, editors, Proceedings of the 13th
models.
International Conference on Artiﬁcial Intel ligence and Statistics (AISTATS 2010), vol-
ume 9 of JMLR Workshop & Conference Proceedings, pages 741–748, 2010.

Satinder Singh, Michael R. James, and Matthew R. Rudary. Predictive state representa-
tions: A new theory for modeling dynamical systems. In Joseph Halpern, editor, Proceed-
ings of the 20th Conference on Uncertainty in Artiﬁcial Intel ligence (UAI 2004), pages
512–519. AUAI Press, 2004.

Le Song, Byron Boots, Sa jid M. Siddiqi, Geoﬀrey J. Gordon, and Alex J. Smola. Hilbert
space embeddings of hidden Markov models.
In Johannes F¨urnkranz and Thorsten
Joachims, editors, Proceedings of the 27th International Conference on Machine Learning
(ICML 2010), pages 991–998. Omnipress, 2010.

Daniel R. Upper. Theory and Algorithms for Hidden Markov Models and Generalized Hidden
Markov Models. PhD thesis, University of California at Berkeley, 1997.

Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–
1142, 1984.

Eric W. Wiewiora. Modeling Probability Distributions with Predictive State Representations.
PhD thesis, University of California, San Diego, 2008.

David Wingate and Satinder P. Singh. Kernel predictive linear Gaussian models for non-
linear stochastic dynamical systems. In William W. Cohen and Andrew Moore, editors,
Proceedings of the 23rd International Conference on Machine Learning (ICML 2006),
volume 148 of ACM Proceedings, pages 1017–1024, 2006a.

146

Links Between MA, OOMs and PSRs

David Wingate and Satinder P. Singh. Mixtures of predictive linear Gaussian models for
nonlinear, stochastic dynamical systems. In Anthony Cohn, editor, Proceedings of the
21st National Conference on Artiﬁcial Intel ligence (AAAI 2006). AAAI Press, 2006b.

David Wingate and Satinder P. Singh. Exponential family predictive representations of
state. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural
Information Processing Systems 20 (NIPS 2007), pages 1617–1624. MIT Press, 2008a.

David Wingate and Satinder P. Singh. Eﬃciently learning linear-linear exponential family
predictive representations of state. In William W. Cohen, Andrew McCallum, and Sam T.
Roweis, editors, Proceedings of the 25th International Conference on Machine Learning
(ICML 2008), volume 307 of ACM Proceedings, pages 1176–1183, 2008b.

David Wingate, Vishal Soni, Britton Wolfe, and Satinder P. Singh. Relational knowledge
with predictive state representations. In Manuela M. Veloso, editor, Proceedings of the
20th International Joint Conference on Artiﬁcial Intel ligence (IJCAI 2007), pages 2035–
2040. AAAI Press, 2007.

In
Britton Wolfe and Satinder P. Singh. Predictive state representations with options.
William W. Cohen and Andrew Moore, editors, Proceedings of the 23rd International
Conference on Machine Learning (ICML 2006), volume 148 of ACM Proceedings, pages
1025–1032, 2006.

Lotﬁ Asker Zadeh. The concept of system, aggregate, and state in system theory.
In
Lotﬁ Asker Zadeh and Elijah Polak, editors, System Theory, volume 8 of Inter-University
Electronics Series, pages 3–42. McGraw-Hill, New York, 1969.

MingJie Zhao and Herbert Jaeger. Norm observable operator models. Neural Computation,
22(7):1927–1959, 2010.

MingJie Zhao, Herbert Jaeger, and Michael Thon. A bound on modeling error in observable
operator models and an associated learning algorithm. Neural Computation, 21(9):2687–
2712, 2009a.

MingJie Zhao, Herbert Jaeger, and Michael Thon. Making the error-controlling algorithm of
observable operator models constructive. Neural Computation, 21(12):3460–3486, 2009b.

147

Journal of Machine Learning Research 16 (2015) 47-75

Submitted 2/13; Revised 7/14; Published 1/15

Simultaneous Pursuit of Sparseness and Rank Structures for
Matrix Decomposition

Qi Yan
School of Statistics
University of Minnesota
Minneapolis, MN 55414, USA

Jieping Ye
Computer Science and Engineering
Arizona State University
Tempe, AZ 85287 USA

Xiaotong Shen
School of Statistics
University of Minnesota
Minneapolis, MN 55414, USA

Editor: Aapo Hyvarinen

yanxx195@umn.edu

jieping.ye@asu.edu

xshen@umn.edu

Abstract

In multi-response regression, pursuit of two diﬀerent types of structures is essential to battle
the curse of dimensionality. In this paper, we seek a sparsest decomposition representation
of a parameter matrix in terms of a sum of sparse and low rank matrices, among many
overcomplete decompositions. On this basis, we propose a constrained method sub ject
to two nonconvex constraints, respectively for sparseness and low- rank properties. Com-
putationally, obtaining an exact global optimizer is rather challenging. To overcome the
diﬃculty, we use an alternating directions method solving a low-rank subproblem and a
sparseness subproblem alternatively, where we derive an exact solution to the low-rank
subproblem, as well as an exact solution in a special case and an approximated solution
generally through a surrogate of the L0 -constraint and diﬀerence convex programming, for
the sparse subproblem. Theoretically, we establish convergence rates of a global minimizer
in the Hellinger-distance, providing an insight into why pursuit of two diﬀerent types of de-
composed structures is expected to deliver higher estimation accuracy than its counterparts
based on either sparseness alone or low-rank approximation alone. Numerical examples are
given to illustrate these aspects, in addition to an application to facial imagine recognition
and multiple time series analysis.
Keywords: blockwise decent, nonconvex minimization, matrix decomposition, structure
pursuit

1. Introduction

In multivariate analysis, data as well as parameters are usually expressed in terms of a
matrix form, as opposed to a vector representation in univariate analysis. This occurs fre-
quently in multi-class classiﬁcation (Amit et al., 2007), matrix completion (Cai et al., 2010;
Jain et al., 2010), collaborative ﬁltering (Srebro et al., 2005), computer vision (Wright,

c(cid:13)2015 Qi Yan, Jieping Ye and Xiaotong Shen.

Yan, Ye and Shen

2009), among others.
In situations as such, it essential to identify and employ certain
lower-dimensional structures to battle the curse of dimensionality due to an increase in
dimensionality from multivariate attributes. In this article, we explore rank and sparse-
ness structures through matrix decomposition simultaneously in estimating large matrices
through a novel notation of seeking a sparsest decomposition from a class of overcomplete
decompositions.
Statistically, diﬀerent structures have dramatically diﬀerent interpretations. A low rank
property of a matrix describes global information across diﬀerent tasks, whereas sparseness
concerns local information of speciﬁc task. For instance, for face images, the global infor-
mation corresponds to the overall shape of a face, but the local information characterizes
speciﬁc facial expression such as laugh and cry. In linear time-invariant (LTI) system, a low
rank property corresponds to a low-order LTI system and a sparseness property captures an
LTI system with a sparse impulse response (Porat, 1997). In a high-dimensional situation,
betting on one type of structure may not be adequate to battle the curse of dimensionality.
In this article, we seek a sparsest decomposition for the purpose of dimension reduction,
from a class of overcomplete decompositions into simpler sparse and low-rank components.
Speciﬁcally, a matrix Θ is decomposed as Θ1 + Θ2 , for a sparse Θ1 and low-rank Θ2
components, where Θ1and Θ2 are chosen from many such decompositions, with a small-
est eﬀective degrees of freedom, leading to high accuracy of parameter estimation. Our
ob jective is to reconstruct the parameter matrix by identifying a sparsest decomposition
consisting of simpler components. Such a decomposition can be used to provide a simpler
and more eﬃcient description of a complex system in terms of its simpler components. This
results in more eﬃcient structure representations leading to higher accuracy of parameter
estimation in high-dimensional data analysis.
In this paper, we consider a multi-response linear regression problem in which a random
sample (ai , zi )n
i=1 is observed with a k-dimensional response vector zi following

zi = aT
i Θ + i , E i = 0, C ov(i ) = σ2I ;

i = 1, . . . , n,

(1)

where ai is a p-dimensional design vector, is independent of random error i , and I is
the identity matrix. Model (1) reduces to the univariate case when k = 1, and becomes
a multivariate autoregressive model when ai = zi−1 . Through matrix decomposition, we
decompose a p × k regression parameter matrix Θ into a sum of a sparse matrix Θ1 and a
low rank matrix Θ2 for structure exploration, that is, Θ = Θ1 +Θ2 . Model (1) is expressible
in a matrix form

Z = AΘ + e;
(2)
where Z = (z1 , · · · , zn )T ∈ Rn×k , A = (a1 , · · · , an )T is a n×p matrix, and e = (1 , · · ·, n )T ∈
Rn×k are the data, design and error matrices. In (1), we estimate Θ based on n paired
observation vectors (ai , zi )n
i=1 , with prior knowledge that Θ1 is sparse in the number of its
nonzero entries, and rank r(Θ2 ) is low relative to min(n, k , p). Our goal is to recover the
parameter Θ by identifying Θ1 and Θ2 .
In the literature, the simultaneous exploration of rank and sparseness structures through
matrix decomposition has received some attention, yet has not been well-studied. For robust
principal component analysis (RPCA) where A = In×p is the n × p identity matrix with its

48

Simultaneous Pursuit of Sparseness and Rank Structures for Matrix Decomposition

diagonals and oﬀ-diagonals being one and zero, Yuan & Yang (2013) and Chandrasekaran
et al. (2011) employed a linear combination of the L1 sparsity regularization and the nuclear-
norm regularization, and Zhou & Tao (2011) used a randomized pro jections based low
rank approximations and thresholding for sparsity pursuit. Moreover, Wright et al. (2013)
recovers the sparse and low-rank components by minimizing a linear combination of the
L1 -norm for sparsity and the nuclear-norm for low rank pursuit, while Waters et al. (2011)
develops a greedy algorithm to pursue the sparse and low rank structures. For multiple
task learning, Chen et al. (2010) studies sparse and low rank structures separately through
convex regularization.
In essence, most the existing literature focuses exclusively on a
unique matrix decomposition of Θ with A = In×p or A to be a set of random linear
measurements, and without noise or with small noise that is essentially ignorable. For
instance, Chandrasekaran et al. (2011) provided suﬃcient conditions for exact recovery of
a convex relaxation method without noise; Wright et al. (2013) proved that recovering a
target matrix is possible from a small set of randomly selected linear measurements when the
number of measurements is suﬃciently large. Among these, Agarwal et al. (2012) considered
a general A and derived a theorem that bounds the Frobenius-norm error obtained through
regularized convex relaxation under a ”spikiness” condition that the max-norm of the low
rank component (cid:107)Θ2(cid:107)max is less than α√
for some ﬁxed α > 0.
pk
In this paper, we consider a general design matrix A and parameter matrices (Θ1 , Θ2 ),
for regression analysis, where A represents features of observations which is deterministic,
and can be any matrix with n rows and p columns. Of particular interest is reconstruc-
tion of Θ in a high-dimensional situation in which (p, k) may exceed the sample size n.
Computationally, we use an alternating direction method separating low-rank pursuit from
sparsity pursuit alternatively, where an exact solution to the low-rank problem and that to
the sparsity pursuit problem when A = In×p or an approximated solution for a general A
is obtained. In either case, the ﬁnal solution is shown to be stationary without and with
maximum block improvement (Chen et al., 2012) for A = In×p and a general A. Theo-
retically, we establish error bound for the proposed method in the Hellinger-distance for
reconstruction of Θ, based on which rates of convergence are obtained. Numerically, the
proposed method compares favorably against two strong competitors in simulations.
The paper is organized as follows. Section 2 develops a computational method through
the alternating directions method and a closed-form solution for a rank problem. Section
3 investigates statistical properties of the proposed method, followed by simulation studies
and a real data example in Section 4. Finally, technical proofs are contained in Section 5.

2. Proposed Method

In this section, we explore a structure decomposition of a parameter matrix in the form
Θ = Θ1 + Θ2 under model (1), then develops computational methods in two situations and
discuss their properties.

2.1 Structure Decomposition

Due to non-uniqueness of such a decomposition under model (1), we seek one decomposition,
among many overcomplete decompositions, that minimizes the eﬀective degrees of freedom

49

Yan, Ye and Shen

of Θ Efron (2004), deﬁned as

(cid:107)Θ1(cid:107)0 + (p + k − r(Θ2 ))r(Θ2 ),

Eﬀ(Θ) =

min
{Θ=Θ1+Θ2 :(cid:107)Θ1 (cid:107)0≤max(0,p+k−2r(Θ2 )−2)}
where (cid:107) · (cid:107)0 is the L0 -norm of a matrix, or the number of nonzero entries of the matrix, and
r(·) denotes the rank of a matrix. In other words, we identify a decomposition minimizing
the eﬀective degrees of freedom Eﬀ(Θ), among all candidate decompositions. Lemma 1
below says that the minimal of Eﬀ(Θ) is unique in ((cid:107)Θ1(cid:107)0 , r(Θ2 )) under the constraint
that (cid:107)Θ1(cid:107)0 ≤ max(0, p + k − 2r(Θ2 ) − 2) ≤ 2 max(p, k).
Lemma 1 The minimizer of Eﬀ(Θ) is unique with respect to ((cid:107)Θ1(cid:107)0 , r(Θ2 )) if (cid:107)Θ1(cid:107)0 ≤
max(0, p + k − 2r(Θ2 ) − 2). Moreover,
Eﬀ(Θ) ≤ min((p + k − r(Θ))r(Θ), (cid:107)Θ(cid:107)0 )).

Model (1) is identiﬁable with respect to Θ but may not be so in (Θ1 , Θ2 ) even when A
is of full rank, due to non-uniqueness of a decomposition Θ = Θ1 + Θ2 .

2.2 Estimation

To pursue structures of low-rank and sparsity through matrix decomposition simultaneously,
we propose a constrained likelihood method sub ject to two nonconvex constraints:
(cid:107)AΘ1 + AΘ2 − Z (cid:107)2
r(Θ2 ) ≤ s2 ,
sub ject to (cid:107)Θ1(cid:107)0 ≤ s1 ,
F ,

min
Θ1 ,Θ2
where (cid:107) · (cid:107)F is the Frobenius-norm deﬁned as the L2 -norm of all entries of a matrix, and
s1 and s2 are integer-valued tuning parameters with 0 ≤ s1 ≤ max(p, k) and 1 ≤ s2 ≤
min(n, k , p) based on the consideration that the rank function and the sparsity measure are
integer-valued.
When A = In×p , (3) is simpliﬁed as
(cid:107)Z − Θ1 − Θ2(cid:107)2
F

min
Θ1 ,Θ2
where a special structure may be taken into account to solve this nonconvex minimization.
When A (cid:54)= In×p is any matrix of full rank, the two constraints in (3) are either deﬁned
eﬃcient algorithm to solve (3), we approximate the (cid:107)Θ1(cid:107)0 = (cid:80)
putational surrogate—the truncated L1 -function (cid:80)
by the L0 -function or the rank function, imposing computational challenges. To develop an
i,j I (|θij | (cid:54)= 0) by its com-
τ min(|θij |, τ ) Shen et al. (2012)
1
θij ∈Θ1
as τ → 0+ . This leads to a computational surrogate of (3):
(cid:88)
i,j

sub ject to (cid:107)Θ1(cid:107)0 ≤ s1 , r(Θ2 ) ≤ s2 ,

min
Θ1 ,Θ2

f (Θ1 , Θ2 ), sub ject to

min(|θij |, τ ) ≤ s1 , r(Θ2 ) ≤ s2 ,

(5)

(3)

(4)

1
τ

where f (Θ1 , Θ2 ) = (cid:107)A(Θ1 + Θ2 ) − Z (cid:107)2
F and τ is a nonnegative tuning parameter.

50

Simultaneous Pursuit of Sparseness and Rank Structures for Matrix Decomposition

2.3 Method for Nonconvex Minimization

This section will develop computational strategies for (4) and (5) separately, based on block-
wise coordinate decent as well as maximum block improvement (MBI, Chen et al., 2012).
First, we separate the task of sparsity pursuit for Θ1 from that of rank minimization for
Θ2 , where Θ1 and Θ2 correspond to two blocks for decent. Second, we apply MBI to
assure that blockwise coordinate decent yields a stationary solution for nonconvex mini-
mization, which would be otherwise impossible. In addition, for (5), we develop a gradient
pro ject method to permit fast computation of a constrained problem through the means of
unconstrained optimization.
The strategy of blockwise coordinate decent proceeds as follows. For (4) and (5), we
solve it in Θ2 given Θ1 and solve them in Θ1 given Θ2 , alternatively.
In each step of
alternating blocks, we proceed with the block giving the maximum block improvement.

2.3.1 Nonconvex minimization (4): a special case
For (4), when Θ2 is held ﬁxed, (4) has a global minimizer can be obtained through compo-
(cid:111) · (zij − θ(2)
(cid:110)|zij − θ(2)
(cid:16)
(cid:17)
nentwise thresholding deﬁned by the L0 -function as follows:
ij | > λ
ij )
where θ(2)
is the ij th entry of Θ2 and λ is any number between the s1 th and (s1 + 1)th
largest entries of |Z − Θ2 |.
ij
When Θ1 is held ﬁxed, a global minimizer of (4) is

ˆΘ1 (Z , Θ2 ) =

(6)

I

,

p×k

ˆΘ2 (Z , Θ1 ) = U Ds2 V T ,
(7)
where U and V are given by singular value decomposition (SVD) of Z − Θ1 = U DV T and
Ds2 is a diagonal matrix retaining the largest s2 singular values of Z − Θ1 and truncating
other singular values at zero.
Our algorithm for computing (4) is summarized.
1 , ˆΘ(0)
Step 1.(Initialization) Supply a good initial estimate ( ˆΘ(0)
2 ) in (4). Specify
precision δ > 0.
in (7) with Θ1 = ˆΘ(m−1)
Step 2.(Iteration) At iteration m, update ˆΘ(m)
2
1
in (6) with Θ2 = ˆΘ(m)
update ˆΘ(m)
.
1
2
, ˆΘ(m−1)
) − f ( ˆΘ(m−1)
)| ≤ δ , where
Step 3.(Stopping rule) Terminate if |f ( ˆΘ(m)
, ˆΘ(m)
2
1
2
1
f (Θ1 , Θ2 ) = (cid:107)Θ1 + Θ2 − Z (cid:107)2
F . Let m∗ be the index at termination. The estimate is then
, ˆΘ(m∗ )
( ˆΘ(m∗ )
).
1
2

. Then

2.3.2 Nonconvex minimization (5): A general case

The problem of solving for Θ2 in (5) given Θ1 reduces to that of constrained rank mini-
mization

(cid:107)AΘ2 − (Z − AΘ1 )(cid:107)2
F

sub ject to r(Θ2 ) ≤ s2 ,

min
Θ2

(8)

51

Yan, Ye and Shen

provided that Θ1 satisﬁes the sparsity constraint in (5). Now write Θ2 ≡ C F , where C
and F are p × r and r × k matrices with r ≤ s2 , consisting of a basis of the column space
and that of the row space of Θ2 , respectively. Note that {Θ2 : r(Θ2 ) ≤ s2} = {Θ2 : Θ2 =
C F , r ≤ s2}. Then solving (8) is equivalent to that
(cid:107)A(C F ) − (Z − AΘ1 )(cid:107)2
min
F ,
C ,F

(9)

An application of an argument of (Xing et al., 2012) yields a global minimizer of (9),
which has an analytic form

min
Θ1

ˆC = V D−1Uw ,
ˆF = DwV T
ˆΘ2 (Θ1 ) = ˆC ˆF ,
(10)
w ,
where D is a r(A) × r(A) diagonal singular vector matrix based on SVD of A = U DV T ,
Dw is also a diagonal matrix of s2 leading singular values of W ≡ U T (Z − AΘ1 ) and Uw ,
Vw are matrices consisting of the corresponding right and left singular vectors.
Note that computation involves only the ﬁrst s2 largest singular values. Therefore, we
employ the randomized truncated SVD method (Halko et al., 2011), for eﬃcient computa-
tion of a large problem. This amounts to a complexity of order O(pk log r), as compared to
O(min(pk2 , p2k)) of a conventional SVD method (Golub & Van, 1996).
Solving for Θ1 in (5) given Θ2 , on the other hand, becomes the problem of sparsity
(cid:88)
pursuit. In particular, we solve, assuming that r(Θ2 ) ≤ s2 ,
(cid:107)AΘ1 − (Z − AΘ2 )(cid:107)2
min(|θij |, τ ) ≤ s1 ,
1
F ,
sub ject to
(11)
τ
θij ∈Θ1
(cid:80) max(|θij | − τ , 0)
(cid:80) |θij | and S2 (Θ1 ) = 1
which is solved iteratively by a diﬀerence of convex (DC) programming, constructing a
convex set containing the original constrained set. The constraint in (5) is deﬁned by
J (Θ1 ) = S1 (Θ1 ) − S2 (Θ1 ) with S1 (Θ1 ) = 1
(cid:16) |θij |
(cid:17)
At iteration step m by J (m) (Θ1 ) = (cid:80)
τ
τ
are convex in Θ1 . Then a sequence of upper approximations of J (Θ1 ) is constructed:
| ≤ τ ) + I (| ˆθ(m−1)
τ I (| ˆθ(m−1)
| > τ )
θij ∈Θ1
. This
ij
ij
yields a sequence of convex minimization subproblems with convex constraints: At iteration
step m, we solve
sub ject to J (m) (Θ1 ) ≤ s1 .
minΘ1 (cid:107)AΘ1 − (Z − AΘ2 )(cid:107)2
F ,
For (12), we develop a gradient pro jection method. First, we generalize an l1 -ball result of
(Liu & Ye, 2009) to (12).
Lemma 2 (Projection) For any set K ⊆ {1, 2, · · · , n},
x∈Rn :(cid:80)
x∗ = TK,z (v) =
1
argmin
i∈K |xi |≤z
2
where TK,z : Rn → Rn is a projection operator deﬁned by
TK,z (v)i = sign(vi ) max(|vi | − λ∗ , 0)
where λ∗ = 0 if (cid:80)
(cid:80)
(cid:80)
|vi |−z
i∈K |vi | ≤ z or i /∈ K and λ∗ =
i∈K \K0
|K |−|K0 |
i∈K max(|vi | − |vj |, 0) − z > 0}.

otherwise, and K0 = {j :

(12)

(cid:107)x − v(cid:107)2
2 ,

52

Simultaneous Pursuit of Sparseness and Rank Structures for Matrix Decomposition

Before solving (12), we simply extend the fast iterative shrinkage-thresholding (FISTA)
algorithm (Beck & Teboulle, 2009) to solving (13).

Lemma 3 For any set K deﬁned in Lemma 2, a global minimizer of
x∈Rn :(cid:80)
(cid:107)Ax − b(cid:107)2
1
min
i∈K |xi |≤z
2
2
(cid:16)
(cid:17)
can be obtained by FISTA iteratively: At iteration step t:
1 + (cid:112)1 + 4ρ2
AT (Ay (t) − b)
y (t) − 1
x(t) = TK,z
2L
(cid:18) ρt − 1
(cid:19)
t
,
2
y (t+1) = x(t) +
ρt+1
where L is the largest singular value of A.

(x(t) − x(k−1) ),

ρt+1 =

,

(13)

,

(14)

ˆΘ(m,t)
1

( ˆΘ(m,t)
1
I (| ˆθ(m−1)
ij

− ˆΘ(m,t−1)
1
| > τ )) and λmax (·)

Next we solve (12) using Lemma 3, which yields an analytic updating formula in a
matrix form.
Then a global minimizer of (12) is computed using an iterative scheme with respect to
t as follows:
(cid:18)
(cid:19)
= ˆΘ(m−1)
v (1) = ˆΘ(m,0)
,
ρ1 = 1,
1
1
1 + (cid:112)1 + 4ρ2
(cid:18) ρt − 1
(cid:19)
= TK (m) ,z (m)
v (t) −
AT [Av (t) − (Z − AΘ2 )]
1
2λmax (AT A)
v (t+1) = ˆΘ(m,t)
| ≤ τ }, z (m) = τ (s1 − (cid:80)
t
,
+
ρt+1 =
1
ρt+1
2
where K (m) = {(i, j ) : | ˆθ(m−1)
θij ∈Θ1
ij
denotes the largest eigenvalue of a matrix.
The algorithm is summarized as follows.
Algorithm 2:
1 , ˆΘ(0)
Step 1.(Initialization) Supply a good initial estimate ( ˆΘ(0)
2 ) in (5). Specify
precision δ > 0.
Step 2.(Iteration) At iteration m, compute candidate ˆΘ2 in (10) with Θ1 = ˆΘ(m−1)
1
and candidate ˆθij ∈ ˆΘ1 in (14) with AΘ2 = A ˆΘ(m−1)
.
2
Step 3.(Maximum block improvement) At each iteration m, determine which of the
) and ( ˆΘ(m−1)
two candidates ( ˆΘ1 , ˆΘ(m−1)
, ˆΘ2 ) for updating according to the amounts of im-
2
1
) ≤ f ( ˆΘ(m−1)
) if f ( ˆΘ1 , ˆΘ(m−1)
) = ( ˆΘ1 , ˆΘ(m−1)
provement. That is, update ( ˆΘ(m)
, ˆΘ(m)
, ˆΘ2 );
1
2
2
2
1
) = ( ˆΘ(m−1)
, ˆΘ(m)
update ( ˆΘ(m)
, ˆΘ2 ) otherwise.
1
2
1
, ˆΘ(m−1)
) − f ( ˆΘ(m−1)
Step 4.(Stopping rule) Terminate if |f ( ˆΘ(m)
, ˆΘ(m)
2
1
1
2
note by m∗ the index at termination. The ﬁnal estimate is
ˆΘ1 = ˆΘ(m∗ )
ˆΘ2 = ˆC ˆF ,
1
where ˆC and ˆF are deﬁned in (10) with Θ1 = ˆΘ1 .

)| ≤ δ . De-

),

,

53

Yan, Ye and Shen

2.4 Computational Properties

This section discusses computational properties of Algorithms 1 and 2. For nonconvex
minimization, our methods may not guarantee a global minimizer for (3). However, the
following lemma says that our solution of Algorithms 1 and 2 yields a stationary point of
the cost function. Note that the scheme of maximum block improvement is essential for the
result of Lemma 5.

Lemma 4 The minimal cost function f ( ˆΘ(m)
, ˆΘ(m)
) in Algorithm 1 is strictly decreasing
2
1
in m before termination. Moreover, the solution is a stationary point of f (Θ1 , Θ2 ) in that
θ(∗)
2 ) \ θij ), where (Θ1 , Θ2 ) \ θij is the set of parameters of
ij = argminθij ∈Θk ;k=1,2 f ((Θ∗
1 , Θ∗
(Θ1 , Θ2 ) without one component θij in Θ1 or Θ2 , and (Θ1 , Θ2 ) satisfy the constraints in
(5).

Lemma 5 If A is of ful l rank, then ˆΘ1 computed from Algorithm 2 satisﬁes the con-
straints in (12). Moreover, the minimal cost function f ( ˆΘ(m)
, ˆΘ(m)
) is strictly decreasing
1
2
in m before termination. Final ly, if the solution ( ˆΘ1 , ˆΘ2 ) satisﬁes (5) and it is a stationary
point of f (Θ1 , Θ2 ) in that
θ(∗)
ij = argmin
θij ∈Θk ;k=1,2
where (Θ1 , Θ2 ) \ θij is the set of parameters of (Θ1 , Θ2 ) without one component θij in Θ1
or Θ2 , and (Θ1 , Θ2 ) satisfy the constraints in (5).

1 , Θ∗
f ((Θ∗
2 ) \ θij ),

With regard to the computational complexity of Algorithms 1 and 2, the method of
truncated SVD yields an approximated SVD with a complexity of O(pk log r + (p + k)r2 )
operations (Halko et al., 2011). Sorting requires a complexity of O(pk log(pk)). For FISTA,
the convergence rate is O(1/t2 ) (Beck & Teboulle, 2009), where t is the number of iterations.
Overall, the computational complexity of Algorithm 1 is O(pk log(pk) + (p + k)r2 )I2 , while
that of Algorithm 2 is O((pk log r + (p + k)r2 + I1/ε2 )I2 , where ε denotes the precision
speciﬁed in Algorithm 2, and I1 and I2 is the number of DC iteration and blockwise iteration,
respectively. Based on our experience, I1 and I2 are about between 3 and 20.

3. Theory

This section drives a ﬁnite-sample probability error bound for reconstruction of the true Θ0
by ˆΘL0 , which is a global minimizer of (3) in that ˆΘL0 = ˆΘL0
1 + ˆΘL0
2 . Note that existence
of a global minimizer is assured by the fact that the cost function (3) is bounded blow by
zero. Moreover, we will provide an insight into simultaneous pursuit of the low rank and
sparsity structures through matrix decomposition by contrasting the proposed method with
(cid:80)
(s1 , s2 ) against low rank approximation alone with (s1 = 0, s2 ) and sparsity pursuit alone
with (s1 , s2 = 0).
j |θij | and (cid:107)Θ(cid:107)max = maxij |θij | are the L∞ -norm and max norm
Let (cid:107)Θ(cid:107)∞ = maxi
respectively. Before proceeding, we deﬁne a parameter space Λ as {Θ = Θ1 +Θ2 : (cid:107)Θ1(cid:107)0 ≤
s1 , (cid:107)Θ1(cid:107)max ≤ l1 , Θ2 = C F , max((cid:107)C (cid:107)∞ , (cid:107)F T (cid:107)∞ ) ≤ l2}, where l1 , l2 > 0 are constant, C
is a p × s2 matrix, F is a s2 × k matrix, F T is the transport of F and s2 > 0 is an upper

54

Simultaneous Pursuit of Sparseness and Rank Structures for Matrix Decomposition

bound of r(Θ2 ). Let g(Θ, Z ) be the probability density of Z with respect to dominating
(cid:18)(cid:90)
(cid:19)1/2
measure ν on Λ. Deﬁne the Hellinger distance between two densities as

(g1/2 (Θ, Z ) − g1/2 (Θ(cid:48) , Z ))2dν

,

(15)

h(Θ, Θ(cid:48) ) =

1
2

which will be used to measure estimation accuracy.
The following technical assumptions are made.
(cid:90)
Assumption A: (Norm-relation) For any Θ, Θ(cid:48) ∈ Λ and any δ > 0,

(g1/2 (Θ, y) − g1/2 (Θ(cid:48) , y))2dν (y) ≤ M 2 δ2 ,

sup
(cid:107)Θ−Θ(cid:48) (cid:107)max≤δ

where M might depend on p, k , s1 , s2 and l1 , l2 .
Assumption A speciﬁes a norm relation between the metric (cid:107) · (cid:107)max over parameters and
the Hellinger distance over the corresponding densities. This can be veriﬁed given a speciﬁc
form of g .
Theorem 1 gives a probability error bound for ˆΘL0 under probability P under the true
2 ) be the degree of sparsity and rank, as deﬁned in Eﬀ(Θ0 ) in Lemma 1.
1 , s0
Θ0 . Let (s0
(cid:16)
(cid:17) ≤ 5 exp(−c1n2 ),
Theorem 6 Under Assumptions A, for any  ≥ n,p,k
h( ˆΘL0 , Θ0 ) ≥ 
(cid:113)
(cid:115)
log(
(cid:18)
(cid:113)
s0
1 log

(p + k)s0
2 + s0
1 + c2
(cid:113)
If log(r(Θ0 )) ≤ ds0
(cid:112)log(M )
2 for some d > 0, then it can be simpliﬁed:
(p + k − s0
2 )s0
Cp,k = c3
2 ,
where c1 − c3 are positive constants and M is deﬁned in Assumption A. Moreover, as
n, p, k → ∞, h2 ( ˆΘL0 , Θ0 ) = Op (2
n,p,k ), where Op (·) and E
n,p,k ), and E h2 ( ˆΘL0 , Θ0 ) = O(2
denote the stochastic order and the expectation under P .

log(29M c4 (l3
2 + l1 ))

(p + k − r(Θ0 ))r(Θ0 )
s0
1

e

n,p,k = Cp,k√
n

√
n
Cp,k

Cp,k = c2

P

) with

(cid:113)

.

(16)

(cid:19)

Corollary 1 gives an order of n,p,k in three extreme situations with M held ﬁxed.
Corollary 1 Suppose M in Assumptions A is a constant independent of (p, k , s1 , s2 ).
(cid:16)(cid:112)(cid:107)Θ0(cid:107)0 log((p + k − r(Θ0 ))r(Θ0 )/(cid:107)Θ0(cid:107)0 )
(cid:17)
(i) When Θ0 is extremely sparse, that is, (cid:107)Θ0(cid:107)0 ≤ p + k − 2, Cp,k in (16) is no worse
(cid:16)(cid:112)(p + k − r(Θ0 ))r(Θ0 )
(cid:17)
.
than O
(ii) When Θ0 is a low-rank matrix, Cp,k in (16) is no worse than O

.

55

Yan, Ye and Shen

(cid:16)
)(cid:1)(cid:17)
(cid:113)
max (cid:0)(cid:112)(p + k − s0
(iii) When Θ0 is dense, say (cid:107)Θ0(cid:107)0 ≥ cpk for a constant 0 < c ≤ 1, and of ful l rank, Cp,k
1 log( pk
(cid:16)(cid:112)(p + k − r(Θ0 ))r(Θ0 )
(cid:17)
2 )s0
s0
2 ,
in (16) is O
.
s0
1
Then C L
.
p,k = O
Corollary 2 and Theorem 2 give a similar result under the Hellinger distance and the
Kullback-Leibler distance, respectively, assuming that i follows a normal distribution.
Corollary 2 If i in (1) fol lows N (0, σ2 Ik×k ), (cid:107)A(cid:107)∞ is bounded, then the results in Corol-
lary 1 continue to hold.
(cid:16)
K (Θ0 , ˆΘL0 ) ≥ 42(cid:17) ≤ 5 exp(−c1n2 ).
Theorem 7 Under the same assumptions in Corol lary 2, we have, for any  ≥ n,p,k ,
P
where K (·, ·) is Kul lback-Leibler distance under normality and n,p,k and c2 remain to be
the same as in Theorem 1. As n, p, k → ∞, K (Θ0 , ˆΘL0 ) = Op (2
n,p,k ) and EK (Θ0 , ˆΘL0 ) =
O(2
n,p,k ).
Theorem 3 gives an error bound for (cid:107) ˆΘL0 − Θ0(cid:107)2
F under the normal assumption when
A = In×p .

Theorem 8 Assume that A = In×p with n = max(p, k). Under the same assumptions in
1√
), as n, p, k → ∞, (cid:107) ˆΘL0 − Θ0(cid:107)2
F = Op (C (cid:48)
(cid:16)
(cid:17)
p,k log( 1
Corol lary 2 with σ = O(
)),
C (cid:48)
max(p,k)
p,k
e (p+k−r(Θ0 ))r(Θ0 )
log(max(p, k)) · [(p + k)s0
2 + s0
1 ] + s0
1 log
s0
1
max(p2 , k2 )

C (cid:48)
p,k =

where

4. Numerical Examples

This section examines operating characteristics of the proposed method through simulations,
and demonstrates its eﬀectiveness on applications in image reconstruction and in time series
analysis.
In the literature, it is known that the state-of-art methods are the low-rank
approximation method sub ject to rank restriction as well as its regularized version, which
outperforms the low-rank approximation method with the trace-norm (Xing et al., 2012;
She, 2013; Zhou & Tao, 2011).
In Section 4.1, we contrast our proposed method with
pursuing low rank and sparsity structures through matrix decomposition simultaneously,
with the former low rank approximation method sub ject to rank restriction (low-rank alone),
as well as the method based on sparsity pursuit alone (sparsity alone). Here Algorithm 2 are
used. Most importantly, in Section 4.2, we compare the proposed method using Algorithm 1
with two strong competitors the method of Go Decomposition (GoDec, Zhou & Tao, 2011)
and the method augmented Lagrange multipliers (ALM, Lin et al., 2009) when A = In×p
in (2). In simulations, codes for ALM and GoDec are used at the authors’ website, and the
initial values for Algorithms 1 and 2 are set to be the zero-matrix

56

Simultaneous Pursuit of Sparseness and Rank Structures for Matrix Decomposition

(17)

4.1 Simulation I: Operating Characteristics
The simulated example is generated as follows. First, a n × p design matrix A is sampled
with each entry being iid N (0, 1). Second, the true Θ1 is a p × k matrix with all diagonals
one and two more non-zeros (2 and 2) being randomly chosen with equal probability, and
the true Θ2 is generated by multiplying a p × r matrix with a r × k matrix with each entry
following N (1, 1). Moreover, each entry of E is iid N (0, 0.25). Throughout the simulations,
Θ1 and Θ2 are held ﬁxed with diﬀerent values of (n, p, k).
The proposed method is trained with a training set, and the optimal tuning parameters,
minimizing the prediction mean squares error over an independent tuning set, are obtained
through a bisection search over integer values. Then a method’s performance is examined
over a test set. The training, tuning and testing data sizes are n, 4n and 2n.
For parameter estimation, we employ the mean squares error to evaluate performance
(cid:107)A( ˆΘ − Θ0 )(cid:107)2
F .

1
4n
For rank recovery, we calculate the absolute diﬀerence between an estimated rank ˆr and
the true rank r0 , that is |ˆr − r0 |. For sparsity pursuit, we deﬁne the true positive (TP) as a
ratio of the true positive numbers of nonzero estimates over the number of nonzeros in the
true model, and the false positive (FP) as a ratio of the false positive numbers of nonzero
estimates over the number of zeros in the true model. Here “Low rank alone”, “Sparsity
alone” and “Ours” indicate the low rank method sub ject to rank restriction, the sparsity
pursuit method, and the proposed method
As indicated in Table 1, the proposed method performs favorably against its counterpart—
the low rank approximation method sub ject to rank restriction and sparsity pursuit alone,
across all situations with diﬀerent values of n, p and k . Moreover, the proposed method
enables to identify two structures through matrix decomposition simultaneously. In partic-
ular, it recovers the true rank of the matrix with nearly zero |ˆr − r0 |-values as compared to
relatively large |ˆr − r0 |-values, ranging from 6.7 to 29.6, for its low-rank counterpart. At
the same time, the proposed method has high true positives ranging from .92 to 1.00 and
low false positives between 0.00 and 0.01, as compared to true positives ranging 0.04 to .44
and false positives between 0.03 and 0.20 of its counterpart based on sparsity pursuit. This
suggests that pursuit of two types of structures is indeed advantageous than that of either
one structure individually. This is mainly because these two structures are complementary
to each other. As a result, higher parameter estimation accuracy, as measured by the MSE
values, can be realized. In fact, the amount of improvement is large, which ranges from
147% to 1185400%. To see how each method performs as (n, p) increases, we ﬁx k = 5.
As suggested by Table 2, the proposed method yields more stable performance than
its two counterparts whose performance deteriorates rapidly, as the level of diﬃculty of a
problem escalates when p and k increase.

4.2 Simulation II: Comparison

To compare with ALM (Lin et al., 2009) and GoDec (Zhou & Tao, 2011) for RPCA, consider
the case of A = In×p in (2) and p = k as in these papers. GoDec minimizes
sub ject to card(Θ1 ) ≤ s1 , rank(Θ2 ) ≤ s2 ,
(cid:107)Z − Θ1 − Θ2(cid:107)2
F

(18)

min
Θ1 ,Θ2

57

Yan, Ye and Shen

p = 20, k = 10
Low-rank alone
|ˆr − r0 | MSE
6.71
1.68
(0.28)
(0.52)
p = 30, k = 20
Low-rank alone
|ˆr − r0 | MSE
7.79
15.69
(1.03)
(2.41)
16.94
2.16
(0.24)
(0.18)
p = 20, k = 30
Low-rank alone
|ˆr − r0 | MSE
16.66
5.06
(0.62)
(0.76)
1.88
16.99
(0.10)
(0.16)
p = 40, k = 30
Low-rank alone
|ˆr − r0 | MSE
1.88
19.39
(0.59)
(1.57)
p = 50, k = 20
Low-rank alone
|ˆr − r0 | MSE
16.86
5.05
(0.40)
(0.35)
p = 200, k = 100
Low-rank alone
|ˆr − r0 | MSE
54.24
29.56
(7.84)
(0.81)

MSE
0.68
(0.15)

MSE
1.54
(0.30)
0.51
(0.08)

MSE
1.06
(0.17)
0.46
(0.05)

MSE
4.08
(1.21)

MSE
0.95
(0.15)

MSE
8.26
(0.86)

n
50

n
50

100

n
50

100

n
50

n
100

n
300

|ˆr − r0 |
0.00
(0.00)

Ours
TP
1.00
(0.00)

FP
0.01
(0.03)

|ˆr − r0 |
0.00
(0.00)
0.00
(0.00)

|ˆr − r0 |
0.00
(0.00)
0.00
(0.00)

Ours
TP
1.00
(0.00)
1.00
(0.00)

FP
0.00
(0.00)
0.00
(0.00)

Ours
TP
1.00
(0.00)
1.00
(0.00)

FP
0.00
(0.00)
0.00
(0.00)

|ˆr − r0 |
0.00
(0.00)

Ours
TP
1.00
(0.00)

FP
0.00
(0.00)

|ˆr − r0 |
0.00
(0.00)

Ours
TP
1.00
(0.00)

FP
0.00
(0.00)

|ˆr − r0 |
3.76
(1.24)

Ours
TP
0.92
(0.23)

FP
0.00
(0.00)

Sparsity alone
MSE
FP
0.07
1367.03
(173.50)
(0.01)

TP
0.44
(0.29)

Sparsity alone
FP
MSE
4650.35
0.14
(511.55)
(0.02)
4399.38
0.05
(0.01)
(429.41)

TP
0.12
(0.21)
0.13
(0.22)

Sparsity alone
MSE
FP
0.06
4276.25
(508.06)
(0.01)
4087.58
0.06
(0.01)
(406.97)

TP
0.43
(0.28)
0.53
(0.20)

Sparsity alone
MSE
FP
12018.68
0.20
(0.04)
(1422.84)

TP
0.09
(0.20)

Sparsity alone
MSE
FP
0.03
11262.97
(1003.69)
(0.01)

TP
0.04
(0.14)

Sparsity alone

–
(–)

–
(–)

–
(–)

Table 1: Results of Simulation I. Algorithm 2 is used for computation.

min
Θ1 ,Θ2

(19)

where card(·) denotes the cardinality, and sj ≥ 0 are tuning parameters as in our case.
(cid:88)
Similarly, ALM that focuses on the non-noisy situation minimizes
|θij |,
(cid:107)Θ2(cid:107)∗ + λ
θij ∈Θ1
where (cid:107) · (cid:107)∗ is the nuclear-norm of a matrix.
Our simulation example remains the same as before except that the positions of nonzero
elements in Θ2 are randomly sampled with equal probability, in particular, .1p and .3p
nonzeros are randomly chosen without replacement. For tuning, grid search is employed for
GoDec in (18), with 1 ≤ s1 ≤ (p + k) and 1 ≤ s2 ≤ min(p, k , 50); λ is ﬁxed at 1√
p for (19).

sub ject to Z = Θ1 + Θ2 ,

58

Simultaneous Pursuit of Sparseness and Rank Structures for Matrix Decomposition

n
50

50

50

50

p
20

30

40

50

100

20

100

30

100

40

|ˆr − r0 |
0.00
(0.00)
0.00
(0.00)
0.00
(0.00)
0.82
(0.84)
0.00
(0.00)
0.00
(0.00)
0.00
(0.00)

Ours
FP
0.002
(0.006)
0.01
(0.01)
0.001
(0.003)
0.01
(0.01)
0.01
(0.03)
0.01
(0.01)
0.01
(0.02)

TP
1.00
(0.00)
0.57
(0.17)
1.00
(0.00)
0.36
(0.38)
1.00
(0.00)
0.71
(0.25)
0.98
(0.10)

MSE
0.58
(0.14)
1.29
(0.32)
3.57
(1.58)
487.43
(1081.68)
0.23
(0.05)
0.36
(0.05)
0.53
(0.08)

Low-rank alone
|ˆr − r0 | MSE
0.84
2.00
(0.00)
(0.18)
1.98
1.97
(0.42)
(0.17)
5.43
1.67
(0.60)
(1.73)
12255
0.82
(79570)
(0.81)
0.32
2.00
(0.00)
(0.05)
0.54
2.00
(0.08)
(0.00)
0.83
2.00
(0.00)
(0.11)

Sparsity alone
MSE
FP
570
0.08
(0.02)
(73)
3772.33
0.08
(542.38)
(0.01)
1998
0.05
(0.01)
(257)
3797
0.03
(539)
(0.01)
541
0.08
(0.02)
(58)
1461
0.03
(147)
(0.01)
1929
0.03
(0.01)
(179)

TP
0.433
(0.30)
0.18
(0.27)
0.07
(0.18)
0.05
(0.15)
0.53
(0.21)
0.19
(0.21)
0.10
(0.20)

Table 2: Results for Simulation I with ﬁxed k = 5. Algorithm 2 is used for computation.

From Tables 3, it is evidenced that the proposed method outperforms ALM uniformly
in terms of the MSE while being comparable to GoDec, in all the situations with diﬀerent
values of (p, k , σ). Moreover, it always recovers the true rank of the matrix perfectly with
|ˆr − r0 | = 0. Although ALM has comparable high TP values, its FP values are high as well
in that they are at least 0.6488. As a result, ALM never captures the true rank.

4.3 AR Face Database 20pt Markup

For face image reconstruction, we use a subset of AR Face Data for this experiment. The
original image is available at http://www-prima.inrialpes.fr/FGnet/data/05-ARFace/
markup_large.png, which is a colored one with size of 186 × 200 × 3. To enable detailed
testing, the image has been labeled with 20 facial features on the face. We convert the
image into black and white and reduce it to size 171 × 180. The target image is displayed
in Figure 1.

Figure 1: The converted AR face image with markup points.

Twenty one markup points around eyes, nose, mouth and cheeks, which are used to test
face recognition or veriﬁcation performance when the exact location of the face and features

59

Yan, Ye and Shen

nonzeros

p

k

σ Method
Ours

0.1p

0.3p

50

30

200

100

50

30

200

100

0.1

1

0.1

1

0.1

1

0.1

1

GoDec

GoDec

GoDec

Ours

GoDec

GoDec

Ours

Ours

|ˆr − r0 |
0.0000
(0.0000)
ALM 13.0300
(0.6735)
0.0000
(0.0000)
0.0000
(0.0000)
ALM 13.3900
(0.6651)
0.0000
(0.0000)
0.0000
(0.0000)
ALM 54.3100
(0.7745)
0.0000
(0.0000)
0.0000
(0.0000)
ALM 54.2400
(0.7264)
0.0000
(0.0000)
0.0000
(0.0000)
ALM 13.0000
(0.6195)
0.0000
(0.0000)
0.0000
(0.0000)
13.37
(0.6301)
0.0000
(0.0000)
0.0000
(0.0000)
ALM 54.3500
(0.6571)
0.0000
(0.0000)
0.0000
(0.0000)
ALM 54.2200
(0.6289)
0.0000
(0.0000)

ALM

Ours

Ours

GoDec

Ours

GoDec

Ours

GoDec

TP
0.9940
(0.0343)
1.000
(0.0000)
0.9940
(0.0342)
0.0320
(0.0839)
0.9280
(0.1223)
0.0300
(0.0823)
0.9770
(0.0337)
1.0000
(0.0000)
0.9755
(0.0344)
0.0075
(0.0206)
0.9456
(0.0456)
0.0085
(0.0236)
0.9933
(0.0201)
1.0000
(0.0000)
0.9953
(0.0171)
0.0373
(0.0624)
0.9407
(0.0621)
0.0327
(0.0653)
0.9867
(0.0164)
1.0000
(0.0000)
0.9882
(0.0152)
0.0080
(0.0122)
0.9467
(0.0297)
0.0075
(0.0135)

FP
0.0000
(0.0002)
0.6488
(0.0082)
0.0000
(0.0001)
0.0000
(0.0001)
0.6540
(0.0080)
0.0001
(0.0003)
0.0000
(0.0000)
0.7034
(0.0022)
0.0000
(0.0000)
0.0000
(0.0000)
0.7059
(0.0023)
0.0000
(0.0000)
0.0002
(0.0003)
0.6472
(0.0079)
0.0001
(0.0003)
0.0000
(0.0001)
0.6531
(0.0080)
0.0001
(0.0002)
0.0001
(0.0001)
0.7030
(0.0023)
0.0000
(0.0001)
0.0000
(0.0000)
0.7054
(0.0022)
0.0000
(0.0000)

MSE
0.2366
(0.0251)
1.5057
(0.0576)
0.2363
(0.0245)
2.5308
(0.2418)
15.0569
(0.5758)
2.5537
(0.2523)
0.2345
(0.0169)
4.9984
(0.0510)
0.2330
(0.0160)
2.4469
(0.1387)
49.9838
(0.5095)
2.4476
(0.1395)
0.2507
(0.0277)
1.5057
(0.0576)
0.2489
(0.0271)
2.8870
(0.2410)
15.0569
(0.5758)
2.8983
(0.2504)
0.2495
(0.0198)
4.9984
(0.0510)
0.2479
(0.0191)
2.8254
(0.1402)
49.9838
(0.5095)
2.8237
(0.1409)

Table 3: Results for Simulation II . Algorithm 1 is used for computation.

60

Simultaneous Pursuit of Sparseness and Rank Structures for Matrix Decomposition

are known. To identify the locations, we extract sparse (Θ1 ) and low-rank (Θ2 ) structures
for the face images as described by the matrix decomposition into Θ1 and Θ2 . For this
purpose, A in (3) is set to be the identity matrix of size 171 × 171. Figures 2 and 3 display
two decomposed structures for the AR face images by the proposed method with diﬀerent
sparse and rank constraint parameters in (3).

Figure 2: Extracted sparsity (ﬁrst),
low-rank (second) structures as well as the recon-
structed image by the proposed method for AR face images; where the tuning
parameters are set to s1 = 2500, s2 = 5.

Figure 3: Extracted sparsity (ﬁrst),
low-rank (second) structures as well as the recon-
structed image by the proposed method for AR face images; where the tuning
parameters are set to s1 = 2100, s2 = 10.

As indicated in Figures 2 and 3, the sparseness structure describes characteristics/detailed
marks of the face, whereas the low-rank structure displays the rough outlook of the human
face. This conﬁrms our discussion regarding local and global features in the Introduction.
Visually, both the ﬁrst panels in Figures 2 and 3 preserve at least 60% markup points,
especially the points around nose two sides of face and lip. In other words, the sparsity
structure captures most of markup points. Similarly, the second panels retain the overall
look of the face. Most interestingly, this decomposition tends to remove the glasses from
the human face.

61

Yan, Ye and Shen

4.4 Greek Letters Image Reconstruction
Now consider a 26 × 31 black-white image of two Greek letters β and φ, where its noisy
version is obtained by adding noise N (0, 1) after dividing the original matrix values by 100.
The ratio of the maximum value of the image to the noise standard deviation is about 2.5.
The images are displayed in Figure 4.

Figure 4: Original image (left) versus its noisy version (right).

Our goal is reconstruction of the original image from its noise version, with a focus on
restoration of detailed structures of the letters. Towards this end, we apply the proposed
method and contrast with its counterpart based on sparse pursuit alone and low-rank ap-
proximations. Speciﬁcally, let A to be the identity matrix of size 31 × 31 and Θ be a
31 × 26 parameter matrix in (3). For each method, grid search is performed for tuning, with
s1 = (10, 20, 30, 50), 1 ≤ s2 ≤ min(p, k) = 26 and τ = (0.05, 0.1, 0.2). For each method, the
10-f old cross-validation is employed. The reconstructed images are displayed in Figure 5.

Figure 5: Reconstructed images based on sparsity alone (ﬁrst), low-rank alone (second) and
our method (third). Algorithm 2 is used for computation.

Visually, the ﬁrst two reconstructed images by the low-rank method and the sparsity
method give the rough shape of two letters, but the letters β and φ not distinguishable with
blurred segments in places, especially the right middle of β and the top of φ. By comparison,
the third reconstructed image by our method enables to reconstruct the complete shape of
these two letters, and yield the best quality of reconstruction.

62

Simultaneous Pursuit of Sparseness and Rank Structures for Matrix Decomposition

4.5 US Macroeconomic Time Series

This subsection examines multiple time series data described in (Stock & Watson, 2012).
The data measures 143 US macroeconomic variables quarterly over a time span from Febru-
ary 1, 1959 to November 1, 2008. These variables are categorized into 13 groups and are
summarized in Table 4.

Group Description
GDP component
1
IP
2
Employment
3
4
Unemployment rate
Housing
5
Inventories
6
7
Prices

Examples of series
GDP, consumption, investment
IP, capacity utilization
Sectoral&total employment and hours
Unemployment rate, total and by duration
Housing starts, total and by region
NAPM inventories, new orders
Price indexes, aggregate&disaggregate,
commodity prices
Average hourly earning, unit labor cost
Treasuries, corporate, term spreads, public-
private spreads
M1, M2, business loans, consumer credit
Money
Average&selected trading partners
Exchange rates
Stock prices
Various stock price indexes
Consumer expectations Michigan consumer expectations

Wages
Interest rates

8
9

10
11
12
13

# series
16
14
20
7
6
6

37

6

13

7
5
5
1

Table 4: Economic indicators collected for U.S. macroeconomic time series.

For data analysis, we consider time series starting from August 1, 1959 to November
1, 2008 due to incomplete initial observations. Our goal is one-step ahead forecasting, and
contrast the proposed method with low-rank alone and sparsity alone in terms of forecasting
accuracy. Using a multivariate autoregressive model, that is, yt = yT
t−1Θ + i , we place it in
the framework of (1), where yt is a vector that records the values of various macroeconomic
variables at time point t, and i follows normal distribution. In the presence of multiplicity
and non-stationarity for economics data like this, we consider some transformations. For
instance, log growth rates for quantity variables are diﬀerenced, nominal interest rates are
diﬀerenced, as well as the logarithms of changes in rates of inﬂation for price series are
diﬀerenced. See (Stock & Watson, 2012) for processing the data set. For this data set,
p = k = 143 in (1) and the design matrix A is speciﬁed by the time series, which can
written as A = (yt0 , yt0+1 , . . . , yt0+d−1 )T .
A one-step ahead K -fold cross validation (CV) criterion is used for tuning the time
series (Arlot & Celisse, 2010). In particular, for design matrix A, at each fold i, we use
observations i to n − K + i − 1 for training and the observation n − K + i for tuning, where
K is a pre-assigned integer and K − 1 indicates the number of folds. Note that the values
of p and k are close to the sample size n for this time series. We therefore choose K ≤ 20
to maintain adequate training samples.
For tuning, the CV is optimized over a set of grids for s1 = (10, 20, 50, 100, 200), 1 ≤
s2 ≤ min(p, k) and τ = (0.02, 0.05, 0.1, 0.2). The results for K = 11 are reported in Table 5.
The results for other K values are omitted due to similarity.
As suggested by Table 5, the proposed method outperforms its counterparts pursuing
sparseness and low-rank alone. The amount of improvement over the low rank method and

63

Yan, Ye and Shen

K = 11

Ours
301.22

Low-rank alone
348.02

Sparsity alone
3111.89

Table 5: Prediction errors of U.S. macroeconomic data for K = 11. Here “Low rank alone”,
“Sparsity alone” and ”Ours” indicate our method for low rank pursuit only, for
sparsity pursuit only and for simultaneous pursuit of low rank and sparsity. Algo-
rithm 2 is used for computation.

the sparsity method is 15% and 933%, respectively. The Q-Q plots in Figure 6 indicate
that the model assumption is adequate although some departure from normality has been
detected. Overall, the proposed method performs reasonably well.

Acknowledgments

We would like to acknowledge support for this pro ject from the National Science Founda-
tion Grants IIS-0953662 and DMS-0906616, and National Institute of Health Grants R01
LM010730, 2R01GM081535 and 1R01HL105397. The authors thank the editor, the action
editor and three referees for their helpful comments and suggestions.

Appendix
Proof of Lemma 1: Let df (s, r) = s + (p + k − r)r. By deﬁnition of the eﬀective degrees
of freedom, we obtain that
Eﬀ(Θ) ≤ min(df (0, r(Θ0 )), df ((cid:107)Θ0(cid:107)0 , 0)).
To prove uniqueness in terms of (s, r), suppose there exist (¯s, ¯r) (cid:54)= (¯s(cid:48) , ¯r (cid:48) ) such that
df (¯s, ¯r) = df (¯s(cid:48) , ¯r (cid:48) ) = mins,r df (s, r). Without loss of generality, assume ¯r = ¯r (cid:48) − n0 < ¯r (cid:48) ,
If n0 ≤ min(p, k) − ¯r and ¯r < min(p, k), then
where n0 > 0 is a positive integer.
¯s = ¯s(cid:48) + n0 (p + k − 2¯r − n0 ) ≥ n0 (p + k −
¯s + (p + k − ¯r)¯r = ¯s(cid:48) + (p + k − ¯r (cid:48) )¯r (cid:48) implies that
2¯r − n0 ) > p + k − 2¯r − 1, which contradicts with the assumption that s < p + k − 2r − 1.
(cid:4)
Otherwise, if ¯r = min(p, k), ¯s must be zero. This completes the proof.
Proof of Lemma 2: Let xi = vi for i /∈ K . Then the problem reduces to the standard l1
(cid:88)
ball problem.
(cid:80)
argmin
i∈K |xi |≤z
i∈K
The results follows by the proof of Theorem 1 of (Liu & Ye, 2009).

(xi − vi )2 .

1
2

(cid:4)

Proof of Lemma 3: It suﬃces to derive the basic step of ISTA in (Amit et al., 2007) for
(13). Consider the following quadratic approximation of problem (13) at a given point y :
x∈Rn :(cid:80)
2 + (cid:104)x − y , AT (Ay − b)(cid:105) +
(cid:107)x − y(cid:107)2
QL (x, y) = (cid:107)Ay − b(cid:107)2
2 ,
min
i∈K |xi |≤z

(20)

L
2

64

Simultaneous Pursuit of Sparseness and Rank Structures for Matrix Decomposition

Figure 6: Q-Q plots for each-fold in U.S. macroeconomic time series data example, where
points on a straight line indicates non-departure from normality.

65

100140180110130150170CV1Quantile100140180110130150170CV2Quantile100140180110130150170CV3Quantile100140180100120140160CV4Quantile100140180110130150170CV5Quantile100140180100120140160CV6Quantile100140180110130150170CV7Quantile100140180110130150170CV8Quantile100140180110130150170CV9Quantile100140180100120140160CV10QuantileYan, Ye and Shen

where L is a Lipschitz constant of the function AT (Ax − b) with respect to x. Solving (20)
AT (Ay − b)(cid:1)(cid:107)2
(cid:107)x − (cid:0)y − 1
is equivalent to that of
x∈Rn :(cid:80)
min
2 .
L AT (Ay − b)(cid:1). The basic step of ISTA thus can be
(cid:0)y − 1
i∈K |xi |≤z
L
(cid:0)x(t−1) − 1
L AT (Ax(t−1) − b)(cid:1). Then, Lemma 3 follows by taking L to
By Lemma 2, the solution is TK,z
written as x(t) = TK,z
be λmax (AT A), where λmax (·) denotes the largest singular value.
(cid:4)
Proof of Lemma 4: By (6) and (7), for any integer m ≥ 1,

f ( ˆΘ(m)
1

, ˆΘ(m)
2

) ≥ f ( ˆΘ(m)
1

, ˆΘ(m+1)
2

) ≥ f ( ˆΘ(m+1)
1

, ˆΘ(m+1)
2

).

Meanwhile, it follows from (6) that

f ( ˆΘ(m)
1

, ˆΘ(m)
2

1 (cid:107)2
F − (cid:107) ˆΘ(m)
2 (cid:107)2
) = (cid:107)Z − ˆΘ(m)
F
1 (cid:107)2
− ˆΘ(m)
≥ (cid:107)Z − ˆΘ(m+1)
F
2
1 (cid:107)2
≥ (cid:107)Z − ˆΘ(m+1)
(cid:107)2
F − (cid:107) ˆΘ(m)
F .
2

Therefore (cid:107)Z − ˆΘ(m)
2 (cid:107)2
F is lower bounded and decreasing in m. Moreover, by the mono-
F converges as m → ∞. Then there exists a
), (cid:107) ˆΘ(m)
1 (cid:107)2
tone properties of f ( ˆΘ(m)
, ˆΘ(m)
1
2
, ˆΘ(m∗ )
) → ( ˆΘ(m∗ )
subsequence {mk } such that ( ˆΘ(mk )
, ˆΘ(mk )
).
2
1
2
1
Let Rij (Θ1 , Θ2 ) ∈ argminθij ∈Θ1 or θij ∈Θ2 f ((Θ1 , Θ2 ) \ θij ). Let the cost function for θij
) \ θij ), where other components of ( ˆΘ(m)
, ˆΘ(m)
to be fm (θij ) = f (( ˆΘ(m)
, ˆΘ(m)
) are held
1
2
1
2
ﬁxed. Then
(cid:17)
(cid:16)
fmk (Rij ( ˆΘ(m∗ )
)) ≥ fmk (Rij ( ˆΘ(mk )
, ˆΘ(mk )
))
1
1
2
≥ min
f (( ˆΘ(mk )
, ˆΘ(mk+1)
))
1
2
≥ f (( ˆΘ(mk+1)
, ˆΘ(mk+1)
2
1
As m → ∞ , by continuity of f (·), f(m∗ ) (Rij ( ˆΘ(m∗ )
, ˆΘ(m∗ )
)) ≥ f ( ˆΘ(m∗ )
, ˆΘ(m∗ )
)) , where
1
2
1
2
the equality holds by the deﬁnition of Rij . Hence, for each θij ∈ Θl ; l = 1, 2, ˆθ(m∗ )
ij
Rij ( ˆΘ(m∗ )
, ˆΘ(m∗ )
) is the optimal componentwise solution. The results of Lemma 4 then
1
2
(cid:4)
follow.

)), f (( ˆΘ(mk )
1

, ˆΘ(m∗ )
2

, ˆΘ(mk )
2

)).

=

(cid:32) | ˆθ(m)
Proof of Lemma 5: First we prove that ˆΘ(m)
(cid:88)
1
ij
τ
θij ∈Θ1

I (| ˆθ(m)
ij

|

| ≤ τ ) + I (| ˆθ(m)
ij

satisﬁes

(cid:33)

| > τ )

≤ s1 .

(21)

66

Simultaneous Pursuit of Sparseness and Rank Structures for Matrix Decomposition

|

+ Im ,

(cid:17)

+

I (| ˆθ(m)
ij

I (| ˆθ(m−1)
ij

| ≤ τ ) + I (| ˆθ(m)
ij

| ≤ τ ) − I (| ˆθ(m−1)
ij

(cid:33)
(cid:32) | ˆθ(m)
Toward this end, we rewrite the left side of (21) as
(cid:88)
|
| ≤ τ ) + I (| ˆθ(m−1)
I (| ˆθ(m−1)
| > τ )
(cid:32) | ˆθ(m)
ij
(cid:88)
ij
ij
τ
θij ∈Θ1
|
| > τ ) − | ˆθ(m)
(cid:32) | ˆθ(m)
(cid:33)
ij
ij
(cid:88)
τ
τ
θij ∈Θ1
| ≤ τ ) + I (| ˆθ(m−1)
I (| ˆθ(m−1)
| > τ )
ij
=
(cid:16)
ij
ij
τ
where Im = (cid:80)
θij ∈Θ1
| ˆθ(m)
|−τ
I (| ˆθ(m)
ij
θij ∈Θ1
ij
τ
(cid:32) | ˆθ(m)
the DC construction that(cid:88)
ij
τ
θij ∈Θ1

Thus, to establish (21), we only need to prove Im ≤ 0. Rewrite I as
(cid:80)
|, | ˆθ(m−1)
|) > τ or max(| ˆθ(m)
if min(| ˆθ(m)
0
ij
ij
ij
− (cid:80)
| ˆθ(m)
|
| ≤ τ and | ˆθ(m−1)
| > τ ,
if | ˆθ(m)
τ − 1)
ij
θij ∈Θ1
ij
ij
| ˆθ(m)
|
| > τ and | ˆθ(m−1)
if | ˆθ(m)
τ − 1)
| ≤ τ ,
ij
θij ∈Θ1
(
ij
ij
implying that Im ≤ 0. Then, (21) follows.
For stationarity, note that it follows from (21) that
) ≥ f ( ˆΘ(m−1)
, ˆΘ(m−1)
f ( ˆΘ(m−1)
, ˆΘ2 ) ≥ f ( ˆΘ(m)
1
2
1
1

| ≤ τ ) + I (| ˆθ(m−1)
ij

I (| ˆθ(m−1)
ij

| ≤ τ )

Im =

, ˆΘ(m)
2

),

|

(

|

| ≤ τ ) − I (| ˆθ(m−1)
ij

(cid:33)

| > τ )

(22)

. Note that it follows from
(cid:33)
| > τ )

≤ s1 .

|, | ˆθ(m−1)
ij

|) ≤ τ ,

where ˆΘ2 is deﬁned in Step 2 of Algorithm 2.
1 → ˆΘ(m∗ )
Suppose that termination index m∗ is inﬁnite. Then we will prove that ˆΘ(m)
1
as m → m∗ = ∞. When m∗ = ∞, ˆΘ(m)
1 must be updated inﬁnitely because ˆΘ(m)
is
2
analytically solved. First consider, at step m, Θ1 is updated whereas Θ2 = ˆΘ(m)
. Denote
by Λ(Θ1 , Θ2 , λ∗ ) the dual problem of (12), where λ∗ is the optimal Lagrange multiplier and
2
Θ2 = ˆΘm
2 . Then
(cid:32) | ˆθ(m)
(cid:33)
, λ∗ ) − Λ( ˆΘ(m+1)
) − f ( ˆΘ(m+1)
− λ∗ (cid:88)
, ˆΘ(m)
, ˆΘ(m)
) = Λ( ˆΘ(m)
2
1
2
1
1
|
I (| ˆθ(m)
| ≤ τ ) + I (| ˆθ(m)
| > τ ) − s1
ij
ij
ij
τ
θij ∈Θ1
(cid:19)
(cid:18) | ˆθ(m+1)
ing at constraint boundaries, i.e (cid:80)
The equality holds because ˆΘ(m+1)
is the global minimizer of a convex problem (12), attain-
1
|
| > τ ) − s1
| ≤ τ ) + I (| ˆθ(m)
I (| ˆθ(m)
ij
ij
ij
τ

, ˆΘ(m+1)
2

f ( ˆΘ(m)
1

, ˆΘ(m)
2

, λ∗ )

θij ∈Θ1

= 0.

67

Yan, Ye and Shen

, λ∗ ) at Θ1 = ˆΘ(m+1)
1

yields that

An application of the Taylor expansion to Λ(Θ1 , ˆΘ(m)
2
) − f ( ˆΘ(m+1)
, ˆΘ(m+1)
, ˆΘ(m)
f ( ˆΘ(m)
)
1
2
1
2
(cid:32) | ˆθ(m)
= (cid:104) ∂Λ
1 − ˆΘ(m+1)
, λ∗ ), ˆΘ(m)
, ˆΘ(m)
( ˆΘ(m+1)
− λ∗ (cid:88)
1
2
1
∂Θ1
|
ij
τ
θij ∈Θ1
where (cid:104)·, ·(cid:105) denotes the inner product. The ﬁrst term in the right side of the equality is
zero, because ˆΘ(m+1)
is the global minimizer and the third term is no less than zero by
1
(21). Thus,

(cid:33)
(cid:104)A( ˆΘ(m)
1 − ˆΘ(m+1)
1

1
2
| > τ ) − s1

1 − ˆΘ(m+1)
), A( ˆΘ(m)
1

| ≤ τ ) + I (| ˆθ(m)
ij

I (| ˆθ(m)
ij

(cid:105) +

)(cid:105)

,

)(cid:105)

(23)

, ˆΘ(m)
2

f ( ˆΘ(m)
1

, ˆΘ(m+1)
2

) − f ( ˆΘ(m+1)
1

1 − ˆΘ(m+1)
1 − ˆΘ(m+1)
(cid:104)A( ˆΘ(m)
) ≥ 1
), A( ˆΘ(m)
1
1
2
≥ λmin (AT A)
(cid:107)2
1 − ˆΘ(m+1)
(cid:107) ˆΘ(m)
F ,
1
2
where λmin(·) is the smallest eigenvalue of a matrix. Therefore f ( ˆΘ(m)
, ˆΘ(m)
) is lower
1
2
) converges to some limit f ∗ as m →
bounded and decreasing in m, implying f ( ˆΘ(m)
, ˆΘ(m)
1
2
1 → ˆΘ(m∗ )
∞. By (23), convergence of ˆΘ(m)
is established. Next consider the case in which
1
Θ2 is only updated ﬁnitely, say before step m0 , using the same notation with proof of
Lemma 4, then for any m > m0
, ˆΘ(m∗ )
fm (Rij ( ˆΘ(m∗ )
)) ≥ fm (Rij ( ˆΘ(m)
, ˆΘ(m+1)
)) = f (( ˆΘ(m+1)
, ˆΘ(m)
)).
2
1
2
1
2
1
The second equality holds because the MBI is employed. As m → m∗ , by continuity of
function f , f(m∗ ) (Rij ( ˆΘ(m∗ )
, ˆΘ(m∗ )
)) ≥ f ( ˆΘ(m∗ )
, ˆΘ(m∗ )
)) , where the equality holds by the
1
2
1
2
deﬁnition of Rij . Finally, we consider the case in which Θ2 is updated inﬁnitely. Then there
, ˆΘ(m∗ )
. Similarly, fm∗ (Rij ( ˆΘ(m∗ )
2 → ˆΘ(m∗ )
is a subsequence {mk } such that ˆΘ(mk )
)) =
2
1
2
, ˆΘ(m∗ )
= Rij ( ˆΘ(m∗ )
)). Hence, for each θij ∈ Θl , l = 1, 2, ˆθ(m∗ )
, ˆΘ(m∗ )
f ( ˆΘ(m∗ )
) is the
1
2
1
2
ij
(cid:4)
optimal componentwise solution. The results of Lemma 5 then follow.
1 + Θ2 : r(Θ2 ) = r} ∩ Λ, a sub-parameter space with known sparsity
Let BS,r = {Θ = ΘS
structure S and rank r. Denote H (·, Λ) and H B (·, Λ) to be the L∞ entropy and bracketing
Hellinger metric entropy for set Λ, respectively. The next two technical lemmas concern the
size of the parameter space.

Lemma 9 Suppose that Assumptions A is met.
H B (t, BS,r ) ≤ |S | log(2M l1/t) + (p + k)r log(2M l3
2 /t),
where l1 , l2 are constant and M > 1 is deﬁned in Assumption A.
Lemma 10 Suppose that Assumptions A is satisﬁed. If s1 = s0
1 , s2 = s0
2 , then
(cid:18)
(cid:19)
H B (t, Λ) ≤2(p + k)s0
2 log(2M l3
2 /t) + s0
1 log((1 + 2M l1 )/t)
(p + k − r(Θ0 ))r(Θ0 )
s0
1

+ 2s0
1 log

e

.

68

Simultaneous Pursuit of Sparseness and Rank Structures for Matrix Decomposition

Proof of Lemmas 6 and 7: For Lemma 6, note that Λ = ∪|S |≤s0
∪r≤s0
BS,r . It suﬃces
to calculate the entropy for each BS,r .
1
2
Let Λ2 = {(Θ1 , Θ2 ) : Θ1 , Θ2 satisfy conditions deﬁned in Λ}. For Θ = Θ1 + Θ2 and
(Θ1 , Θ2 ) ∈ Λ2 , deﬁne Bδ (Θ1 , Θ2 ) = {(Θ(cid:48)
1(cid:107)max + (cid:107)Θ2 − Θ(cid:48)
2 ) ∈ Λ2 : (cid:107)Θ1 − Θ(cid:48)
2(cid:107)max ≤ δ}
1 , Θ(cid:48)
2 ) ∈ Bδ (Θ1 , Θ2 ),
to be the neighborhood of (Θ1 , Θ2 ). For any Θ(cid:48) = Θ(cid:48)
1 + Θ(cid:48)
2 with (Θ(cid:48)
1 , Θ(cid:48)
by Assumption A, (cid:90)

(g1/2 (Θ, y) − g1/2 (Θ(cid:48) , y))2dν (y) ≤ M 2 δ2 .

sup
Bδ (Θ1 ,Θ2 )

Combined the above with Lemma 2.1 of (Ossiander, 1987), we have
H B (t, BS,r ) ≤ H (M −1 t, BS,r ).
(24)
Since (cid:107)Θ1(cid:107)max is bounded by l1 , by constructing a 2t-net on BS,r through the outer product
of the t-nets on ΘS
1 and Θ2 deﬁned in the parameter space Λ, we can show that
H (M −1 t, BS,r ) ≤ |S | log(2M l1/t) + Hr (M −1 t)
(25)
where |S | is the number of nonzeros in Θ1 and Hr (M −1 t) is the entropy for Θ2 with rank r.
Let C be a basis of column of Θ2 , then there exists an k × r matrix F such that Θ2 = C F .
Hence
(cid:80)k
(cid:107)Θ2 − Θ(cid:48)
2(cid:107)max = (cid:107)C F − C (cid:48)F (cid:48)(cid:107)max ≤ (cid:107)C (cid:107)∞(cid:107)F − F (cid:48)(cid:107)max + (cid:107)F T (cid:107)∞(cid:107)C − C (cid:48)(cid:107)max .
i=1 |θij | is the L∞ matrix-norm and (cid:107)Θ(cid:107)max = maxθij ∈Θ |θij |
where (cid:107)Θp×k (cid:107)∞ = max1≤i≤p
is the max norm. Note that (cid:107)C (cid:107)∞ and (cid:107)F T (cid:107)∞ are bounded by l2 . This yields
2l3
Hr (M −1 t) ≤ (p + k)r log
2M
t

.

=

≤

(cid:19)

This, together with (24) and (25), implies Lemma 6.
For Lemma 7, note that
(cid:19)(cid:18)(p + k − r(Θ0 ))r(Θ0 ) − s0
(cid:18)s0
exp(H B (t, Λ)) ≤ exp(H (M −1 t, Λ))
|S |(cid:88)
2(cid:88)
1(cid:88)
s0
s0
(cid:19)  s0
1
1
|S | − i
(cid:19)
(cid:18) s0
(cid:18)(p + k − r(Θ0 ))r(Θ0 )
i
1(cid:88)
|S |=0
r=0
i=0
(2M l1/t)|S |
(cid:19)
(cid:18)(p + k − r(Θ0 ))r(Θ0 )
1|S |
s0
|S |=0
1
× I × I I .
≡
(cid:1)ak bn−k = (a+b)n . Then I = (1+ 2M l1
(cid:0)n
Note that (cid:80)n
s0
1
(cid:19)
(cid:18)(p + k − r(Θ0 ))r(Θ0 )
k=0
k
t
Thus,
H B (t, Λ) ≤ log
s0
1
≤ 2(p + k)s0
2 /t) + s0
2 log(2M l3
1 log(

(cid:18)
2 + 1) + s0
+ log(s0
1 log(1 +

1 + 2M l1
t

) + 2s0
1 log

e

69

exp(H (M −1 t, BS,r ))
  s0
2(cid:88)
r=0


2 /t)(p+k)r
(2M l3
(cid:16) 2M l3
2 
t

1 and I I ≤ (s0
)s0
2+1)

(cid:17)(p+k)s0
2 .

(cid:19)
2M l3
2M l1
2
) + (p + k)s0
2 log(
t
t
(p + k − r(Θ0 ))r(Θ0 )
s0
1

,

)

Yan, Ye and Shen

(Stanica & Montgomery, 2001) that (cid:0) b
(cid:1) ≤
where e is the natural number and 0 < t < 1. The last inequality follows Theorem 2.6 of
2πaa+1/2 (b−a)b−a+1/2 ≤ exp((a + 1/2) log(b/a) +
√
bb+1/2
a
a) ≤ exp(2a log(b/a) + a) for any integer 0 < a < b. This completes the proof.
(cid:4)

Proof of Theorem 1: We apply a large deviation inequality in Theorem 2 of (Wong &
(cid:90) 21/2 
(cid:90) 21/2 
(cid:113)
Shen, 1995). To this end, we verify (1.2) there. By Lemma 7,
(cid:0)H B (t/c4 , Λ)(cid:1)1/2
(cid:115)
(cid:19)
(cid:18)
(cid:90) 21/2 
2 log(2M l3
2(p + k)s0
2 c4/t) + s0
1 log((1 + 2M l1 )c4 /t)dt
2 /28
2 /28
(p + k − r(Θ0 ))r(Θ0 )
e
s0
2 /28
1

dt ≡ I1 + I2 ,

2s0
1 log

dt ≤

+

(cid:113)
for some constant c4 > 0, say c4 = 10. Then, for  small,
√
(cid:114)
(cid:113)
I1 ≤
2 log(29M l3
2 c4/2 ) + s0
2(p + k)s0
1 log((1 + 2M l1 )28 c4/2 )
2
(cid:114)
(cid:113)
(cid:113)
≤ 2
1
log(29M c4 (l3
2 + s0
(p + k)s0
2 + l1 )) + 2 log
1

√
≤ 2
1 ·
2 + s0
(p + k)s0
log(29M c4 (l3
2 + l1 ))
2
(cid:115)
(cid:18)
I2 ≤ 2
s0
1 log

log
(cid:19)
.

(p + k − r(Θ0 ))r(Θ0 )
s0
1

Similarly,

1


e

.

s0
1 log

2c−1
5

(cid:115)

(cid:19)
.

(cid:18)
e

√
Cp,k = 2

1 + 2c−1
(p + k)s0
2 + s0
5

(p + k − r(Θ0 ))r(Θ0 )
s0
1

Let n,p,k = Cp,k√
n log( Cp,k√
n ) where
(cid:113)

(cid:113)
log(29M c4 (l3
2 + l1 ))
Then, for any  ≥ n,p,k and c5 = 512
(cid:90) 21/2 
(cid:0)H B (t/c4 , Λ)(cid:1)1/2dt ≤ c−1
(2/3)5/12
√
n2 .
(cid:16)
(cid:17) ≤ 5 exp(−c1n2 ), which yields
5
2 /28
h( ˆΘL0 , Θ0 ) ≥ 
By Theorem 2 of (Wong & Shen, 1995), P
n,p,k ) by using the fact that h( ˆΘL0 , Θ0 ) ≤ 1.
E h2 ( ˆΘL0 , Θ0 ) = O(2
Consider a special situation when log(r(Θ0 )) ≤ ds0
2 for some constant d > 0 that is
2 and p + k − r(Θ0 ) ≤ p + k − s0
1 < p + k − s0
(cid:16)
(cid:16)
(cid:17) ≤ (p + k − s0
(cid:17)
independent of p, k . Note that s0
2 . Then
(p + k − r(Θ0 ))r(Θ0 )
(p + k − r(Θ0 ))r(Θ0 )
p + k − s0
e
2 ) log
s0
1
2
2 ) log(er(Θ0 )) ≤ 2d(p + k − s0
≤ (p + k − s0
2 )s0
2 .

s0
1 log

e

70

Simultaneous Pursuit of Sparseness and Rank Structures for Matrix Decomposition

(cid:114)

(

2

√

= O

log

1


.

Cp,k = O

(cid:18)(cid:113)
(cid:19) (cid:113)
Thus, I1 + I2 is upper bounded by
√
√
1 ·
2 + s0
log(29M c4 (l3
(p + k)s0
2 + l1 )) +
d
2
(cid:17) (cid:112)(p + k)s0
(cid:16)(cid:112)log(29 c4 (l3
√
2c−1
2 + s0
1 . The result then follows.
2 + l1 )) +
d
Let c3 = 2
5
(cid:4)
This completes the proof.
Proof of Corollary 1: If Θ0 is sparse and (cid:107)Θ0(cid:107)0 ≤ p + k − 2, then by the deﬁnition of
1 + (p + k − s0
2 ≤ (cid:107)Θ0(cid:107)0 . This implies that
(cid:1)(cid:19)
(cid:18)(cid:113)(cid:107)Θ0(cid:107)0 log (cid:0)p + k − r(Θ0 ))r(Θ0 )/(cid:107)Θ0(cid:107)0
(cid:16)(cid:112)(cid:107)Θ0(cid:107)0
(cid:17)
eﬀective degrees of freedom s0 = s0
2 )s0
(cid:18)(cid:113)(cid:107)Θ0(cid:107)0 log (cid:0)p + k − r(Θ0 ))r(Θ0 )/(cid:107)Θ0(cid:107)0
(cid:1)(cid:19)
+ O
.
x and (cid:112)x log(a/x) in x for
√
The second inequality is because of nondecreasingness of
x ≤ a/e.
(cid:18)(cid:112)
(cid:1)(cid:19)
(cid:113)
If Θ0 is low-rank, we have
1 log (cid:0)(p + k − r(Θ0 ))r(Θ0 )/s0
(p + k − r(Θ0 ))r(Θ0 ) +
s0
Cp,k = O
.
1 log (cid:0)(p + k − r(Θ0 ))r(Θ0 )/s0
(cid:1) ≤ log (cid:0)(p + k − r(Θ0 ))r(Θ0 )/e(cid:1). The result
1
Note that s0
1
(cid:16)(cid:112)(p + k − s0
(cid:17)
(cid:113)
follows.
If Θ0 is dense and of full rank, then (p + k − r(Θ0 ))r(Θ0 ) is of order O(pk). Hence Cp,k
(cid:4)
1 log( pk
2 )s0
s0
can be written as O
2 +
)
. This completes the proof.
s0
2σ2 (y − µi )T (y − µi )(cid:1) for i = 1, 2. µ1 = aT Θ and µ2 = aT Θ(cid:48) . Then
2πσ)k exp (cid:0)− 1
1
Proof of Corollary 2: It suﬃces to show the Assumption A is met. Let f (µi , y) =
√
1
(cid:90)
(f 1/2 (µ1 , y) − f 1/2 (µ2 , y))2dy
(cid:33)
(cid:32)
(cid:90)
sup
(cid:107)Θ−Θ(cid:48) (cid:107)max≤δ
− (cid:107)y − µ1+µ2
√
≤ 2 − 2
(cid:18)
(cid:19)
2
inf
exp
(cid:107)Θ−Θ(cid:48) (cid:107)max≤δ
− (cid:107)µ1 − µ2(cid:107)2
2
4σ2
≤ ((cid:107)a(cid:107)1 )2 δ2
4σ2

≤ 2 − 2
exp
inf
(cid:107)Θ−Θ(cid:48) (cid:107)max≤δ
≤ ((cid:107)a(cid:107)1 )2(cid:107)Θ − Θ(cid:48)(cid:107)2
max
4σ2

2 + (cid:107)µ1 − µ2(cid:107)2
(cid:107)2
2/2
2σ2

1
2πσ)k

(

dy

.

The second inequality follows from the invariance property of the normal distribution. Corollary 2
follows when (cid:107)a(cid:107)1 is bounded. This completes the proof.
(cid:4)

71

Yan, Ye and Shen

(cid:33)

= 2

,

(cid:19)

(cid:19)

≥ 2

≤ P

= P

K (Θ0 , Θ) =

h2 (Θ, Θ0 ) = 2

When  < 1,
P (K (Θ0 , ˆΘL0 ) ≥ 42 ) = P

(cid:32)
(cid:90)
1 − n(cid:89)
exp (cid:2) − 1
i Θ0 (cid:107)2 )(cid:3)dy
Proof of Theorem 2: After some calculations, we obtain that
√
i Θ(cid:107)2 + (cid:107)yi − aT
4σ2 ((cid:107)yi − aT
1
(cid:33)
(cid:32)
1 − n(cid:89)
exp (cid:2) − 1
2πσ)k
(
i=1
8σ2 (cid:107)aT
i (Θ − Θ0 )(cid:107)2
(cid:18)
(cid:19)
i=1
1 − exp(− 1
8σ2 (cid:107)A(Θ − Θ0 )(cid:107)2
= 2
F )
2σ2 (cid:107)A(Θ − Θ0 )(cid:107)2
1
F .
(cid:18) 1
(cid:18) 1
(cid:19)
8σ2 (cid:107)A( ˆΘL0 − Θ0 )(cid:107)2
F ≥ 2
(cid:18)
(cid:18)
(cid:19)
F ≥ − log(1 − 2
8σ2 (cid:107)A( ˆΘL0 − Θ0 )(cid:107)2
)
2
h2 ( ˆΘL0 , Θ0 ) ≥ 2(cid:17)
(cid:16)
1 − exp(− 1
8σ2 (cid:107)A( ˆΘL0 − Θ0 )(cid:107)2
2
F )
.
= P
For any  ≥ n,p,k , it follows from Theorem 1 and Corollary 2 that
(cid:17)1/2
(cid:17)1/2(cid:16)
(cid:16)
EK (Θ0 , ˆΘL0 ) ≤ EK (Θ0 , ˆΘL0 )I {K (Θ0 , ˆΘL0 ) ≤ 42} + EK (Θ0 , ˆΘL0 )I {K (Θ0 , ˆΘL0 ) > 42}
≤ 42 +
P (K 2 (Θ0 , ˆΘL0 ) > 42 )
EK 2 (Θ0 , ˆΘL0 )
.
By the triangle inequality, (cid:107)AΘ0 − A ˆΘL0 (cid:107)F − (cid:107)(cid:107)F ≤ (cid:107)AΘ0 +  − A ˆΘL0 (cid:107)F . Note that ˆΘL0 is a
global minimizer of (3). Then (cid:107)AΘ0 +  − A ˆΘL0 (cid:107)F ≤ (cid:107)(cid:107)F . Hence
σ2 (cid:107)(cid:107)2
2σ2 (cid:107)A(Θ0 − ˆΘL0 )(cid:107)2
F ≤ 2
1
F .
(cid:17)1/2
(cid:16)
P (cid:0)K 2 (Θ0 , ˆΘL0 ) > 42 (cid:1)
σ4 (cid:107)(cid:107)4
EK (Θ0 , ˆΘL0 ) ≤ 42 +
4
E
√
F
≤ 42 + 10 exp(−c1n2 + log
3nk).
The results in Theorem 2 follow by letting  = n,p,k and using the fact that log k ≤ C 2
p,k . This
(cid:4)
completes the proof.
√
Proof of Theorem 3: Without loss of generality, assume p ≥ k and n = p. When σ = O(1/
by Theorem 2, we have
(cid:32) C 2
(cid:33)
2
(cid:107) ˆΘL0 − Θ0(cid:107)2
n,p,k
F = 2σ2K (Θ0 , ˆΘL0 ) = OP (
p
(cid:33)
(cid:32) C 2
p,k
)
p2 log(
p,k
)
p2 log(

K (Θ0 , ˆΘL0 ) =

= OP

= OP

,

(26)

)

√

p
Cp,k

p2
C 2
p,k

Thus,

p),

72

Simultaneous Pursuit of Sparseness and Rank Structures for Matrix Decomposition

(cid:115)

where

Cp,k = O

(cid:18)

e

(cid:113)
2 + s0
(p + k)s0
1 +

(cid:32)(cid:112)log(p)
(p + k − r(Θ0 ))r(Θ0 )
s0
1 log
s0
1
(cid:33)
(cid:32)
(27) comes from the proof of Corollary 2 with M in Assumption A being O(
C (cid:48)
p,k log(
)
(cid:16)

(cid:107) ˆΘL0 − Θ0 (cid:107)2
F = OP

1
C (cid:48)
p,k

log(p) · [(p + k)s0
2 + s0
1 ] + s0
1 log
p2

e (p+k−r(Θ0 ))r(Θ0 )
s0
1

(cid:19)(cid:33)

.

(27)

√

p). Thus,

(cid:17)

.

(cid:4)

with

C (cid:48)
p,k =

This completes the proof.

References

Agarwal, A., Negahban, S. and Wainwright, M. (2012). Noisy matrix decomposition via convex
relaxation: optimal rates in high dimensions. The Annals of Statistics, Vol. 40, No. 2, 1171–1197.

Amit, Y., Fink, M., Srebro, N., and Ullman, S. (2007). Uncovering shared structures in multiclass
classiﬁcation. Proceedings of the 24th Annual International Conference on Machine learning, 17–
24.

Arlot, S. and Celisse, A. (2010). A survey of cross-validation procedures for model selection. Statist.
Surv., 4, 40–79.

Beck, Amir and Teboulle, Marc (2009). A fast iterative shrinkage-thresholding algorithm for linear
inverse problems. SIAM Journal on Imaging Sciences, Vol. 2, No. 1, 183-202.

Bunea, F., and She, Y. (2011). Optimal selection of reduced rank estimators of high-dimensional
matrices. Annals of Statistics, 39(2), 1282-1309.

Cai, J.F., Cand`es, E.J. and Shen, Z. (2010).A singular value thresholding algorithm for matrix
completion. Arxiv preprint arXiv:0810.3286.

Cai, T. T., Liu, W. and Luo, X. (2011). A constrained l1 minimization approach to sparse precision
matrix estimation. J. Amer. Statist. Assoc., 106, 594-607.

Candes, E., Li. X., Ma, U., and Wright, J. (2009). Robust principal component analysis. Journal of
ACM, 58(1), 1-37.

Candes, E.J. and Recht, B. (2009). Exact matrix completion via convex optimization. Found of
Comput. Math., 9, 717-772.

Chandrasekaran, V., Sanghavi, S., Parrilo, P.A., and Willsky, A.S. (2011). Rank-sparsity incoherence
for matrix decomposition, SIAM J. Optim., 21, 572-596.

Chen, B. , He, S., Li, Z. and Zhang, S. (2012). Maximum block improvement and polynomial
optimization., SIAM Journal on Optimization, 22, 87-107.

Chen, J., Liu, J., and Ye, J. (2010). Learning incoherent sparse and low-rank patterns from multiple
tasks. The Sixteenth ACM SIGKDD International Conference On Know ledge Discovery and Data
Mining. (SIGKDD 2010).

73

Yan, Ye and Shen

Efron, B. (2004). The estimation of prediction error: Covariance penalties and cross-validation.
Journal of the American Statistical Association, 99, 619-642. (with discussion).

Ganesh, A., Min, K., Wright, J. and Ma, Y. (2012). Principal component pursuit with reduced linear
measurements. International Symposium on Information Theory.

Golub, G. and Van Loan, C. (1996). Matrix Computations. Third edition. London: The Johns
Hopkins University Press.

Halko, N., Martinsson P. G. and Tropp, J. A.. (2011). Finding structure with randomness: stochastic
algorithms for constructing approximate matrix decompositions. SIAM Rev., 53(2), 217-288.

Jain, P., Meka, R. and Dhillon, I. (2010). Guaranteed rank minimization via singular value pro jec-
tion. Advances in Neural Information Processing Systems, 23, 937–945.

Kolmogorov, A.N. and Tihomirov, V.M. (1959). -entropy and -capacity of sets in function spaces.
Uspekhi Mat. Nauk. 14 3-86. [In Russian. English translation, Ameri. Math. Soc. Transl. 2, 17,
277-364.(1961)].

Lin, Z., Chen, M., Wu, L. and Ma, Y. (2009). The augmented Lagrange multiplier method for exact
recovery of corrupted low-rank matrices. UIUC Technical Report UILU-ENG-09-2215.

Liu, J., and Ye, J. (2009). Eﬃcient Euclidean pro jections in linear time. The Twenty-Sixth Interna-
tional Conference on Machine Learning.

Negahban, S. and Wainwright, M.J. (2011). Estimation of (near) low-rank matrices with noise and
high-dimensional scaling. Annals of Statistics, 39(2), 1069-1097.

Ossiander, M. (1987). A central limit theory under metric entropy with L2 bracketing. Ann. Probab.
15 897-919.

Porat, B. (1997). A Course in Digital Signal Processing, New York: John Wiley.

She, Y. (2013). Reduced rank vector generalized linear models for feature extraction. Statistics and
Its Interface, Vol 6, 197-209.

Shen, X., Pan, W., and Zhu, Y. (2012). Likelihood-based selection and sharp parameter estimation.
Journal of the American Statistical Association, 107, 223-232.

Srebro, N., Rennie, J.D.M., and Jaakkola, T.S. (2005). Maximum-margin matrix factorization. Ad-
vances in Neural Information Processing Systems, 17, 1329–1336.

Stanica, P., and Montgomery, A.P. (2001). Good lower and upper bounds on binomial coeﬃcients.
J. Ineq. in Pure. Appl. Math., 2, art 30.

Stock, J. H. and Watson, M. W. (2012). Generalized shrinkage methods for forecasting using many
predictors. Journal of Business and Economic Statistics.

Zhou, T. and Tao, D. (2011). GoDec: Randomized low-rank & sparse matrix decomposition in noisy
case. Proceedings of the 28th International Conference on Machine Learning, Bellevue, WA, USA.

Waters, A.E., Sankaranarayanan, A.C. and Baraniuk, R.G.(2011). SpaRCS: Recovering low-rank
and sparse matrices from compressive measurements. Neural Information Processing Systems,
Granada, Spain.

74

Simultaneous Pursuit of Sparseness and Rank Structures for Matrix Decomposition

Wong, W.H., and Shen, X. (1995). Probability inequalities for likelihood ratios and convergence
rates of sieve MLEs. Ann. Statist., 23, 339-362.

Wright, J., Ganesh, A., Min, K., and Ma, Y. (2013). Compressive principal component pursuit.
Information and Inference.

Wright, J., Ganesh, A., Rao, S., and Ma, Y. (2009). Robust principal component analysis: Exact
recovery of corrupted low-rank matrices via convex optimization. Advances in Neural Information
Processing Systems.

Xing, S., Zhu, Y., Shen, X., and Ye, J. (2012). Optimal exact rank minimization for noisy data. Pro-
ceeding the 18th ACM SIGKDD Conference on Know ledge Discovery and Data Mining, Beijing,
China.

Yuan, X. and Yang, J. (2013). Sparse and low-rank matrix decomposition via alternating direction
methods. Paciﬁc Journal of Optimization, 9(1), 167-180.

75

