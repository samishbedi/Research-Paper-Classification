(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:3)(cid:8)(cid:6)(cid:9)(cid:7)(cid:10)(cid:11)(cid:6)(cid:9)(cid:12)(cid:4)(cid:13)(cid:11)(cid:14)(cid:8)(cid:6)(cid:9)(cid:12)(cid:4)(cid:11)(cid:15)(cid:4)(cid:16) (cid:3)(cid:17)(cid:7)(cid:4)(cid:18)(cid:3)(cid:6)(cid:3)(cid:8)(cid:9)(cid:7)(cid:10)(cid:11)(cid:6)(cid:4)(cid:16) (cid:3)(cid:7)(cid:19) (cid:11)(cid:8)(cid:20)(cid:4)(cid:21)(cid:5)(cid:13)(cid:16) (cid:18)(cid:16) (cid:22)(cid:23)(cid:4)(cid:24)(cid:11)(cid:12)(cid:25)(cid:26)(cid:23)(cid:4)(cid:16) (cid:11)(cid:25)(cid:27)(cid:23)(cid:4)(cid:28) (cid:9)(cid:8)(cid:29)(cid:2)(cid:4)(cid:26)(cid:30)(cid:27)(cid:30) 

 

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:7)(cid:6)(cid:11)(cid:2)(cid:8)(cid:12)(cid:9)(cid:13)(cid:2)(cid:4)(cid:2)(cid:14)(cid:2)(cid:15)(cid:16)(cid:9)(cid:17)(cid:7)(cid:18)(cid:19)(cid:15)(cid:20)(cid:9)(cid:19)(cid:20)(cid:9)(cid:21)(cid:19)(cid:3)(cid:7)(cid:14)(cid:19)(cid:15)(cid:22)(cid:6)(cid:9)(cid:2)(cid:23)(cid:9)
(cid:24)(cid:25)(cid:18)(cid:26)(cid:25)(cid:14)(cid:9)(cid:27)(cid:8)(cid:25)(cid:4)(cid:22)(cid:9)(cid:24)(cid:8)(cid:2)(cid:4)(cid:7)(cid:8)(cid:6)(cid:16)(cid:9)
 
Sanjay Kumar Pal1 and Samar Sen Sarma2 
 
1Department of Computer Science & Applications, NSHM College of Management & 
Technology, Kolkata, INDIA,  
pal.sanjaykumar@gmail.com 
 
2Department of Computer Science & Engineering University of Calcutta, Kolkata, INDIA 
sssarma2001@yahoo.com 

 
Abstract 
  
Constantly growing demands of high productivity and  security of computer  systems and computer networks call  the 
interest  of  specialists  in  the  environment  of  construction  of  optimum  topologies  of  computer  mediums.  In  earliest 
phases  of  design,  the  study  of  the  topological  influence  of  the  processes  that  happen  in  computer  systems  and 
computer networks allows to obtain useful information which possesses a significant value in the subsequent design. 
It  has  always  been  tried  to  represent  the  different  computer  network  topologies  using  appropriate  graph  models. 
Graphs  have  huge  contributions  towards  the  performance  improvement  factor  of  a  network.  Some  major 
contributors  are  de-Bruijn,  Hypercube,  Mesh  and  Pascal.  They  had  been  studied  a  lot  and  different  new  features 
were  always  a  part  of  research  outcome.  As  per  the  definition  of  interconnection  network  it  is  equivalent  that  a 
suitable  graph  can  represent  the  physical  and  logical  layout  very  efficiently.  In  this  present  study  Pascal  graph  is 
researched again and a new characteristics has been discovered. From the perspective of network topologies Pascal 
graph and  its properties were  first  studied more  than  two decades back. Since  then, a numerous graph models have 
emerged with  potentials  to  be  used  as  network  topologies. This  new  property  is  guaranteed  to make  an  everlasting 
mark  towards  the  reliability  of  this  graph  to  be  used  as  a  substantial  contributor  as  a  computer  network  topology. 
This shows its credentials over so many other topologies. This study reviews the characteristics of the Pascal graph 
and the new property is established using appropriate algorithm and the results. 
 
Keywords 
  
Pascal Triangle, Pascal Graph, Pascal Matrix, Adjacency Matrix, Dependable Node of Pascal Graph(DNPG), 
Computer Network Topology. 
 
1. Introduction 
 
While searching for a class of graphs with certain desired properties to be used as computer networks, we 
have  found  graphs  that  come  close  to  being  optimal.  One  of  the  desired  properties  is  that  the  design  be 
simple  and  recursive,  so  that  when  a  new  node  is  added,  the  entire  network  does  not  have  to  be 
reconfigured. Another property is that one central vertex be adjacent to all others. The third requirement is 
that  there exists several paths between each pair of vertices (for reliability) and that some of  this paths be 
of  short  lengths,  to  reduce  communication  delay.  Finally  the  graphs  should  have  good  cohesive  and 
connectivity. Complete graph Kn satisfy all these properties, but are ruled out because of the expense.  
The  proper  introduces  a  set  of  adjacency  matrices  called  Pascal  matrices,  which  are  constructed  using 
Pascal’s  triangle  modulo  2.  We  also  define  Pascal  graphs,  the  set  of  graphs  corresponding  to 
Corresponding to Pascal matrices.     

(cid:27)(cid:30)(cid:25)(cid:31)(cid:27)(cid:26)(cid:27) (cid:10)!(cid:6)"(cid:6)(cid:25)(cid:26)(cid:30)(cid:27)(cid:30)(cid:25)(cid:26)(cid:27)(cid:30)#(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)#(cid:30)(cid:4)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:3)(cid:8)(cid:6)(cid:9)(cid:7)(cid:10)(cid:11)(cid:6)(cid:9)(cid:12)(cid:4)(cid:13)(cid:11)(cid:14)(cid:8)(cid:6)(cid:9)(cid:12)(cid:4)(cid:11)(cid:15)(cid:4)(cid:16) (cid:3)(cid:17)(cid:7)(cid:4)(cid:18)(cid:3)(cid:6)(cid:3)(cid:8)(cid:9)(cid:7)(cid:10)(cid:11)(cid:6)(cid:4)(cid:16) (cid:3)(cid:7)(cid:19) (cid:11)(cid:8)(cid:20)(cid:4)(cid:21)(cid:5)(cid:13)(cid:16) (cid:18)(cid:16) (cid:22)(cid:23)(cid:4)(cid:24)(cid:11)(cid:12)(cid:25)(cid:26)(cid:23)(cid:4)(cid:16) (cid:11)(cid:25)(cid:27)(cid:23)(cid:4)(cid:28) (cid:9)(cid:8)(cid:29)(cid:2)(cid:4)(cid:26)(cid:30)(cid:27)(cid:30) 

In  this  paper,  author  tries  in  summarized way,  to  expose  the  fundamental  problems  of  topological  design 
in computer technology [8, 9, 10, 11, 12]. The problems of multiprocessing and computer systems     have 
been formulated, and  these problems have been resolved  in  form of classes of Pascal graph with extreme 
topological  parameters  that  correspond  to  the  parameters  of  speed  of  information  transmission,  
productivity, and security with limitations in costs [10].   In  the phase of  topological design,  the computer 
network  is  represented  in  graph  [8,  10]  form  whose  vertices  correspond  to  the  nodes  of  information 
processing, and edges correspond to the communication lines. 
The ontogeny of Pascal Graph (PG) was Pascal Matrix (PM) that in turn was generated meticulously from 
Pascal’s  triangle  [1].  Scientist  have  been  putting  a  lot  of  efforts  in  ameliorating  computer  network 
properties  [4].  Wide  varieties  of  graph  model  worked  as  resource  to  their  brainstorming  contribution  in 
this field [3]. Pascal graph is one of those resources and played a significant role as soon as it exploration 
was  initiated  almost  two  decades  back.  As  a  consequence,  several  similar  graph models  emerged  in  this 
arena with laudable potentials to be used as computer network topologies [2]. The inspiration of this study 
stemmed  from  consistent  urge  to  contribute  substantially  in  this  crescendo  of  research  from  the 
perspective of computer network  topology. Here we have reincarnated Pascal Graph with another feather 
added to its credentials. We have reviewed to its characteristics and established a significant property with 
adequate  theoretical  practical  support. The  technique  and  algorithms  here  proposed  can  be  used  not  only 
in the design of computer networks, but also in the design of other network.  
 
2. Origin of Pascal Graph 
 
We  introduced  Pascal  Graph  from  historical  modern  perspective.  Blaise  Pascal  (1623  -  1662)  first 
conceptualized  Pascal’s  Triangle  around  the  middle  of  seventeenth  century  [7].  This  Pascal  triangle 
played the most important role while generating Pascal Matrix [1].  
 
2.1 Pascal Matrix  
 
An  (n  x  n)  symmetric  binary  matrix  is  called  the  Pascal  Matrix  PM(n)  of  order  n  if  its  main  diagonal 
entries  are  all  0’s  and  its  lower  (and  therefore  the  upper  also)  consists  of  the  first  (n-1)  rows  of  Pascal 
Triangle modulo  2. Where  pmi,j  denotes  the  element  of  ith    row  and  jth    column  of  the  Pascal Matrix  [1]. 
An example of Pascal graphs along with associated Pascal matrices is shown in next section.   
 
2.2 Pascal Graph  
 
An undirected graph of n vertices corresponding  to PM(n) as an adjacency matrix  is called Pascal Graph 
(n),                   where n is the order of the Pascal graph [1]. An example of Pascal graph having 5 nodes is 
shown hereunder, for better comprehension of the study. 
 
Pascal Graph (5), 
 

 

 
 
 
 
 
 
 
 
 
 
 
 

 

V1 

V3 

V5 

V2 

V4 

PG(5) 

31 

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:3)(cid:8)(cid:6)(cid:9)(cid:7)(cid:10)(cid:11)(cid:6)(cid:9)(cid:12)(cid:4)(cid:13)(cid:11)(cid:14)(cid:8)(cid:6)(cid:9)(cid:12)(cid:4)(cid:11)(cid:15)(cid:4)(cid:16) (cid:3)(cid:17)(cid:7)(cid:4)(cid:18)(cid:3)(cid:6)(cid:3)(cid:8)(cid:9)(cid:7)(cid:10)(cid:11)(cid:6)(cid:4)(cid:16) (cid:3)(cid:7)(cid:19) (cid:11)(cid:8)(cid:20)(cid:4)(cid:21)(cid:5)(cid:13)(cid:16) (cid:18)(cid:16) (cid:22)(cid:23)(cid:4)(cid:24)(cid:11)(cid:12)(cid:25)(cid:26)(cid:23)(cid:4)(cid:16) (cid:11)(cid:25)(cid:27)(cid:23)(cid:4)(cid:28) (cid:9)(cid:8)(cid:29)(cid:2)(cid:4)(cid:26)(cid:30)(cid:27)(cid:30) 

Pascal Matrix(5), 

 

 

 

PM(5) 

 
V1  V2   V3  V4   V5                      
11110
10101
11011
10101
01111

 
V1  
 
V2  
 
V3   
 
V4 
 
V5 
 
 
2. 3 Connectivity Properties of Pascal Graph 
 
There  are  certain  pragmatic  properties  that  make  Pascal  graph  a  better  choice  for  a  computer  network 
topology over many others. Some of those properties are given below:  
 
1‡" n
•  PG(n) is a subgraph of PG(n+1) 
. 
71 ££i
•  All  Pascal  Graph  PG(i)  for 
  are  planner;  all  Pascal  Graph  of  higher  order  are  non-
planner. 
•  Vertex V1 is adjacent to all other vertices in the Pascal Graph. Vertex V1 is adjacent to Vi+1 in the 
1‡i
Pascal graph for 
. 
1‡" n
•  PG(n) contains a star tree 
. 
•  PG(n) contains a Hamiltonian circuit [1,2,3,……,n-1, n,1]. 
•  PG(n) contains wn-x  (wheel of order n minus an edge). 
If k=2n +1, n is a positive integer, then Vk is adjacent to all Vi.    
• 
•  All Pascal Graph of order  3‡  are 2-connected. 
•  No two even no of vertices of a Pascal Graph are adjacent. 
2£   between  any  two  distinct  vertices  in 
•  There  are  at  least  two  edge  disjoint  path  of  length 
n£3
.  
PG(n), 
• 
If Vi is adjacent to Vj , where j is even and |i-j|>1, then I is odd and Vi is adjacent to Vj-1. 
•  Let det(PM(n)) refer to the determinant of the Pascal matrix of order n. Then, det(PM(n)) = 0, for 
4‡n
all even 
. 
•  Define e(PG(n)) to be the number of edges in PG(n), 
3‡n
•  Det(PM(n)) is even for all 
.       
 
2.4 Pascal Graph Generation Algorithm 
 
Step 1: Enter n number of vertices. 
 
Step 2: Initialized LT[n,0] = 1;  
 
Step 3: From the lower left triangle [LT(n,n)] by adding the number directly above and to the left with the 
             number  directly  above  and  to  th  right  to  find  the  new  value.  If  either  the  number  to  the  right  or 
            left is not present, substitute a zero to its place. 
 
Step 4: From the upper right triangle [UT(n,n)] using the same manner. 
 
Step 5: Convert the lower left triangle into binary values. 
 
Step 6: Convert the upper right triangle into binary values. 
 
Step 7: From the final adjacency matrix [PM(n,n)] of with LT(n,n) and UT(n,n). 
 
Step 8: Stop. 

nPGc
(
(

))

log)1

£ n
(cid:1)
(

-

(cid:2)3

. 

2

 

32 

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:3)(cid:8)(cid:6)(cid:9)(cid:7)(cid:10)(cid:11)(cid:6)(cid:9)(cid:12)(cid:4)(cid:13)(cid:11)(cid:14)(cid:8)(cid:6)(cid:9)(cid:12)(cid:4)(cid:11)(cid:15)(cid:4)(cid:16) (cid:3)(cid:17)(cid:7)(cid:4)(cid:18)(cid:3)(cid:6)(cid:3)(cid:8)(cid:9)(cid:7)(cid:10)(cid:11)(cid:6)(cid:4)(cid:16) (cid:3)(cid:7)(cid:19) (cid:11)(cid:8)(cid:20)(cid:4)(cid:21)(cid:5)(cid:13)(cid:16) (cid:18)(cid:16) (cid:22)(cid:23)(cid:4)(cid:24)(cid:11)(cid:12)(cid:25)(cid:26)(cid:23)(cid:4)(cid:16) (cid:11)(cid:25)(cid:27)(cid:23)(cid:4)(cid:28) (cid:9)(cid:8)(cid:29)(cid:2)(cid:4)(cid:26)(cid:30)(cid:27)(cid:30) 

3. Connotation of Present Work 
 
It  is  better  not  to  predestine  any  claim  before  underpinned  with  adequate  reasoning.  To  bolster  this,  the 
basic  requirement  must  be  articulated  and  hence  substantiated.  Studying  different  graph  with  respect  to 
their properties has been a basic research interest for a long time [6]. It can be said without any ambiguity 
that  there  are  certain  criteria  in  any  network  topology  that  are  given  priorities  to  judge  its  credentials. 
Those  criteria  are,  (i)  degree  of  the  network,  (ii)  diameter  of  the  network,  (iii)  scalability  of  the  network 
and  (iv)  reliability  of  the  network  [2].  There  is  a  tradeoff  between  degree  related  to  hardware  cost  and 
diameter  related  to  transmission  time  of messages within  any  Interconnection Network  [2]. Reliability  is 
the quality to  
sustain  faulty  circumstances  and  scalability  is  the  scope  of  up-gradation  without  much  alteration.  To 
prove  its  worth,  Pascal;  graph  have  been  merged  cogently  with  Hypercube  graph  to  engender  simple-
(q,n)-graph [2]. The destiny of the research is signified keeping this philosophy in mind. 
 
4. Fascinating Characteristic of Pascal Graph    
 
In this work a new property of Pascal graph has been characterized that has increased the reliability of the 
graph without perturbing  its degree, diameter as well as scalability as computer network  topology. As we 
mentioned  in  property  (iii)  in  properties  of  Pascal  graph,  vertex  v1  of  any  Pascal  graph  is  adjacent  to  all 
other  vertices  in  that  graph.  It  has  been  shown  that  if  the  index  start  from  0  instead  of  1  then  it  can  be 
proved  that  vertex  v0  in  any  Pascal  graph  is  adjacent  to  all  other  vertices  in  that  graph.  Vertex  v1  is 
0‡i
adjacent  to  v1+1  in  the  Pascal  graph  if 
[2].  Extending  the  property  (iii)  in  the  properties  of  Pascal 
0‡i
graph,  it  can  be  very well  shown  that  there  exist  other  node  vi  (
)  having  the  same  property  like  v1. 
According to property (iv), this node will never have even number as its index. 
 
4.1 Thought of Present Work 
 
1‡i
If  we  consider  a  Pascal  graph  containing  n  nodes  i.e.  PG(n)  then  by  default  vi  (
)  is  adjacent  to  all 
other nodes [1]. The exploration does not conclude with  it. We have found and recommended other node 
similar  to  v1  in  the  PG(n)  and  termed  it  as  Dependable  Node  of  Pascal  Graph  (DNP);  the  special  node, 
other than node v1, of Pascal graph PG(n) with same degree like v1. 
To  satiate  our  claim  and  to  rationalize  the  whole  thing  we  have  used  very  simplistic  approach.  The 
explanation with some suitable example is given below. 
 
(
)n
log2=
n
       
Case 1: 
log2p
(cid:3)
(cid:4)n
n
Case 2: 
 
2 log +
=
(cid:2) 1
(cid:1)
n
n
 
Case N: 
 
To  establish  the  new  property,  we  are  supposed  to  generate  the  index  I  of  DNP.  Here,  we  provide 
formulae for each case including the case 3. 
 
(
)
fi
+
=
i
where
,1
2
For Case 1: 
n +
=
fi
2 log
(cid:2)
(cid:1)
i
where
1
1
 
For Case 2: 
-
+
=
=
1
log
n
(cid:2)
(cid:1)
(cid:1)
i
and
i
2
1
2
For Case N: 
 
4.2 Experimental Result 
 
The source code has been written in  the ‘C’ programming  language  to generate  the different order Pascal 
matrix.  The  formulas  were  put  into  test  for  a meticulous  checking  of  the  existence  of  Dependable  Node 
Pascal Graph  (DNP)  in  different Pascal matrices  generated by  the  program. Some  instances  of  execution 
of the program are given in the table 1, for a better comprehension of the conjecture. Each instances given 
in  the  table  1,  the  Pascal  matrix  has  been  generated  and  printed  along  with  the  degree  of  the  particular 

where

fi

,1

1

 

+

1

 

log

n

(cid:2)

log

n

-

1

 

33 

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:3)(cid:8)(cid:6)(cid:9)(cid:7)(cid:10)(cid:11)(cid:6)(cid:9)(cid:12)(cid:4)(cid:13)(cid:11)(cid:14)(cid:8)(cid:6)(cid:9)(cid:12)(cid:4)(cid:11)(cid:15)(cid:4)(cid:16) (cid:3)(cid:17)(cid:7)(cid:4)(cid:18)(cid:3)(cid:6)(cid:3)(cid:8)(cid:9)(cid:7)(cid:10)(cid:11)(cid:6)(cid:4)(cid:16) (cid:3)(cid:7)(cid:19) (cid:11)(cid:8)(cid:20)(cid:4)(cid:21)(cid:5)(cid:13)(cid:16) (cid:18)(cid:16) (cid:22)(cid:23)(cid:4)(cid:24)(cid:11)(cid:12)(cid:25)(cid:26)(cid:23)(cid:4)(cid:16) (cid:11)(cid:25)(cid:27)(cid:23)(cid:4)(cid:28) (cid:9)(cid:8)(cid:29)(cid:2)(cid:4)(cid:26)(cid:30)(cid:27)(cid:30) 

node  and  its  index.  When  case  N  occurs,  the  Pascal  graph  is  at  at  a  precise  level  of  reliability.  Unlike 
other cases, it has got three (one more than case 1 and case 2) nodes. 
 

Table 1: 

PG(n) 

i 

DNP 

Degree 

7 
8 
9 
10 
13 
14 
15 
16 
31 
32 
33 
39 

5 
5, 9 
9 
9 
9 
9 
9 
9, 17 
19 
17, 33 
33 
33 
 

V5 
V5 , V9 
V9 
V9 
V9 
V9 
V9 
V9, V17 
V17 
V17, V33 
V33 
V33 

Conjecture 
satisfied 
8 
Case 1 
9 
Case N 
10 
Case 1 
11 
Case 2 
14 
Case 1 
15 
Case 2 
16 
Case N 
17 
Case N 
32 
Case 1 
33 
Case N 
34 
Case 2 
40 
Case 2 
5. Significance of the Work 
 
When Pascal graph merged with Hypercube to form Simple-(q,n)-graph [2]; it showed great credentials to 
be  exploited  as a  typical  computer  network  topology.  It  alone can contribute  substantially  in  this  field  of 
active research. This new discovery will earn Pascal graph high esteem through its reliability under erratic 
and exigent erroneous circumstances and make it sure that the situation does not exacerbate any more. As 
per  property  (iii)  mentioned  in  some  property  of  Pascal  graph,  it  is  very  easy  to  access  or  traverse  any 
node  from  v1  directly  in  a  single  hop.  Every  other  node  except  v1  had  to  use  either  two  hop  (the  two 
communicating nodes have even number  indices) or single hop otherwise. Everything  is fine  till v1  is not 
perturbed. If, under any faulty circumstances, v1 gets disconnected then the above mentioned property no 
longer  holds  for  Pascal  graph  and  the  reliability  is  seized  with  an  increase  in  the  minimum  number  of 
hops  required  by  the message  to  travel  from  one  node  to  another.  In  this  paper we  have  reestablished  its 
reliability  under  this  circumstances  by  finding  DNP  that  is  similar  to  v1  in  PG(n).  by  this  work  we  have 
eradicated  threat  imposed  upon  Pascal  graph  by  mollifying  the  faulty  circumstances.  The  superiority  of 
the graph is maintained very efficiently.  
As the whole work is not based on any convoluted formulae or theory, it is easy to exploit Pascal graphs’ 
high reliability, effective degree and diameter from computer network point of view. Now,  it  is a reliable 
choice  for  any  network  designer  to  opt  the  Pascal  graph when  reliability  along with  efficient  degree  and 
diameter are  the key  factors  in designing. Since  the new property  is based on very  simple  logic  it will be 
easier to incorporate the concept into the physical and logical layout of the computer network topology. 
 
6. Scope of Future work 
 
Selecting  various  graph  models  to  represent  topologies  of  computer  network  had  always  been  an  active 
research  area  for  network Scientists  [5]. The  reasonable  criteria  behind  this  selection  are  very  significant 
because  this  put  long  testing  impact  on  the  network.  How  to  make  a  graph  model  an  ideal  choice  to 
implement  a  specific  topology  would  always  impose  a  huge  challenge  to  the  upcoming  Scientists.  The 
chance of finding optimal, if not optimum, graph model with unparallel properties entirely depends on the 
institution  of  the  Scientists,  in-depth  analysis  of  the  graph  and  technical  feasibility  of  implementing 
computer  network  using  that  graph.  Finally,  it  can  be  said  very  intuitively  that  this  field  still  holds 
immense scopes for Scientists for further research work.   

 

34 

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:3)(cid:8)(cid:6)(cid:9)(cid:7)(cid:10)(cid:11)(cid:6)(cid:9)(cid:12)(cid:4)(cid:13)(cid:11)(cid:14)(cid:8)(cid:6)(cid:9)(cid:12)(cid:4)(cid:11)(cid:15)(cid:4)(cid:16) (cid:3)(cid:17)(cid:7)(cid:4)(cid:18)(cid:3)(cid:6)(cid:3)(cid:8)(cid:9)(cid:7)(cid:10)(cid:11)(cid:6)(cid:4)(cid:16) (cid:3)(cid:7)(cid:19) (cid:11)(cid:8)(cid:20)(cid:4)(cid:21)(cid:5)(cid:13)(cid:16) (cid:18)(cid:16) (cid:22)(cid:23)(cid:4)(cid:24)(cid:11)(cid:12)(cid:25)(cid:26)(cid:23)(cid:4)(cid:16) (cid:11)(cid:25)(cid:27)(cid:23)(cid:4)(cid:28) (cid:9)(cid:8)(cid:29)(cid:2)(cid:4)(cid:26)(cid:30)(cid:27)(cid:30) 

7. Conclusion 
 
A  source  code  has  been  written  in  C  programming  language  to  generate  the  Pascal  Matrix  of  different 
order.  The  formulae  were  put  into  test  for  a  meticulous  checking  of  the  existence  of  DNP  in  different 
Pascal  Matrices  generated  by  the  program.  From  the  case  3,  the  Pascal  graph  is  at  a  precise  level  of 
reliability.  The  Pascal  Matrix  has  been  generated  along  with  some  extra  details  e.g.  the  degree  of 
particular node and its index. Unlike other cases, it has got three nodes that are adjacent to all other nodes. 
 
References 
 
[1]   N.  Deo  and Michael  J.  Quinn,  “Pascal  Graphs  and their Properties”, Fibonacci Quarterly, vol.  21,  pp.  203-214, 
1983. 
 
[2]  S. Chatterjee and S. S. Sarma, “In Search of a Versatile and Flexible Graph Model to Implement  
Computer Network”, Proceeding of International Conference on Communications, Devices and  Intelligent  Systems, 
pp. 635-638, 2004. 
 
[3]   U.  Bhattacharya  and  R.  Chaki,  “A  New  Scalable  Topology  for  Multihop  Optical Network”  ASIAN,  pp.  263-
272, 2000. 
 
[4]   N.  F.  Mir,  “Analysis  of  High  Speed  Interconnection  Network”,  Journal  of   Interconnection  Networks,  vol.  3, 
pp. 49-65, 2002. 
 
[5]  E. L. Wilmer  and M. D.  Ernst,  “Graphs  induced  by Gray Codes”,  discrete Mathematics,  vol.  257,  pp.  585-598, 
November 2002. 
 
[6]   S.  S.  Sarma  and  S.  K.  Bandyopadhyay,  “Properties  and  Generation  of  Some  special   Graph”,  AMSE  Review, 
Vol. II, pp. 7-18, 1989. 
 
[7]  H. K. Rosen, “Discrete Mathematics and its Applications”, TMH Edition, ISBN:0-07-053047-5. 
 
[8]  A New Assignment Algorithm for Star Network  
Topology Design http://kre.elf.stuba.sk/~petrek/papers/dubrovnik.pdf 
 
[9]  On  Some  Aspects  of  Design  of  Cheapest  Survivable  Networks,  IJCSNS  International  Journal  of  Computer 
Science and Network Security, 210 VOL.7 No.11, November 2007. 
 
[10]  Topological  Design  of  Minimum  Cost Survivable  Computer  Communication Networks:  Bipartite  Graph 
Method, International Journal of Computer Science and Information Security, Vol. 3, No. 1, 2009. 
 
[11]  A Genetic Algorithm Approach to Optimal Topological Design of All Terminal Networks, 
http://www.eng.auburn.edu/~smithae/publications/refereed/berna.pdf. 
 
[12]  Topological 
All 
Agile 
of 
Dimensioning 
and 
Design 
http://www.tsp.ece.mcgill.ca/Networks/projects/pdf/mason_JournalCompComm06.pdf. 

Photonic Networks, 

 

35 

This is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  
 
© IEEE Personal use of this material is permitted. Permission from IEEE must be obtained 
for all other uses, in any current or future media, including reprinting/republishing this 
material for advertising or promotional purposes, creating new collective works, for resale 
or redistribution to servers or lists, or reuse of any copyrighted component of this work in 
other works. 

 

AirHopper: Bridging the Air-Gap between Isolated 
Networks and Mobile Phones using Radio Frequencies 
 
 
3
, Assaf Kachlon
 

, Yuval Elovici

, Gabi Kedma

2

 

Mordechai Guri

1

*4

 

 

Department of Information Systems Engineering, Ben-Gurion 
University 
*Telekom Innovation Laboratories at Ben-Gurion University 

Abstract 
Information  is  the  most  critical  asset  of  modern  organizations,  and 
accordingly  coveted by  adversaries. When highly  sensitive data  is  involved, 
an  organization  may  resort  to  air-gap  isolation,  in  which  there  is  no 
networking  connection  between  the  inner  network  and  the  external  world. 
While  infiltrating  an  air-gapped  network  has  been  proven  feasible  in  recent 
years  (e.g.,  Stuxnet),  data  exfiltration  from  an  air-gapped  network  is  still 
considered  to  be  one  of  the most  challenging  phases  of  an  advanced  cyber-
attack.  
In  this  paper  we  present  "AirHopper",  a  bifurcated  malware  that  bridges 
the  air-gap between  an  isolated network  and nearby  infected mobile phones 
using  FM  signals.  While  it  is  known  that  software  can  intentionally  create 
radio  emissions  from  a  video  display  unit,  this  is  the  first  time  that  mobile 
phones  are  considered  in  an  attack  model  as  the  intended  receivers  of 
maliciously  crafted  radio  signals.  We  examine  the  attack  model  and  its 
limitations,  and  discuss  implementation  considerations  such  as  stealth  and 
modulation methods.  Finally,  we  evaluate AirHopper  and  demonstrate  how 
textual  and binary data  can be  exfiltrated  from  physically  isolated  computer 
to mobile phones at a distance of 1-7 meters, with effective bandwidth of 13-
60 Bps (Bytes per second).  

                                                           
1  gurim@post.bgu.ac.il 
2  gabik@post.bgu.ac.il 
3  assafka@post.bgu.ac.il 
4  elovici@bgu.ac.il 
 
 

This is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 
1.  Introduction 
 

Modern organizations rely on their computer networks to deliver all kinds 
of  information.  Consequently,  such  networks  are  a  lucrative  target  for 
malicious  adversaries.  Defending  a  network  is  quite  a  complicated  task, 
involving  host-level  protection,  network-level  protection,  secured  gateways 
and  routers,  and  so  on.  However,  as  long  as  the  local  area  network  has  a 
connection with  the outer world  (e.g.,  the  Internet),  irrespective of  the  level 
of protection, an innovative and persistent attacker will eventually find a way 
to breach the network, to eavesdrop, and to transmit sensitive data outward. 
Accordingly, when a network is used to deliver highly sensitive data, or to 
connect  computers  that  store  or  process  such  data,  the  so  called  'air  gap' 
approach  is  often  applied.  In  an  air-gapped  network,  there  is  no  physical 
networking  connection  (wired  or  wireless)  with  the  external  world,  either 
directly  or  indirectly  through  another,  less  secure  network.  Nevertheless, 
employees  may  carry  their  mobile  phones  around  the  organization's 
workplace, possibly within the vicinity of a classified computer which is part 
of  an  air-gapped  network.  This  may  occur  despite  reasonable  security 
procedures, and does not require malicious intent on the employee's part.  
This scenario becomes relevant and increasingly common since a growing 
portion of modern mobile phones are being marketed with FM radio receivers 
[1]  [2]  [3].  With  appropriate  software,  compatible  radio  signals  can  be 
produced by a compromised computer, utilizing the electromagnetic radiation 
associated with the video display adapter. This combination, of a transmitter 
with a widely used mobile receiver, creates a potential covert channel that is 
not being monitored by ordinary security instrumentation. 
In  this paper we show how mobile phones with FM receivers can be used 
to  collect  emanating  radio  signals,  modulated  with  sensitive  information, 
from  a  compromised  computer.  Although  the  attack  model  is  somewhat 
complicated,  it  is not beyond  the capabilities of a  skilled persistent attacker. 
In reward for his or her effort,  the attacker may collect valuable  information 
from air-gapped networks. 
 
1.1   FM Receiver in Mobile Phones 
 

FM  radio  receivers  in mobile  phones,  first  introduced  around  1998,  have 
grown in popularity with the emergence of modern smartphones. Overriding 
early  hesitations, most major  carriers  and manufacturers  are  integrating  FM 
receivers in  their products. Microsoft has endorsed FM, announcing support 
in an update to Windows Phone 8 [4]. Of the nearly 700 million phones sold 
by  Nokia  in  2012  and  2013,  the  two  bestselling  models  (Lumia  520  and 
Lumia 920) now support FM radio. In the Android market, over 450 models 
support  FM  radio  [1]  [2].  FM  receivers  are  also  integrated  in  many  feature 
phones  (low-end  mobile  phones)  and  media  players  (e.g.,  iPod  nano). 

2 
 

This is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 
Additionally,  legislative  efforts  have  been  made  in  the  US  to  enforce 
integration of FM receivers in mobile phones, for use in emergency situations 
[5] . 
 

2.  Attack Model 
 

The attack model posits that hostile code can be installed and run (a) on the 
targeted  network,  consisting  of  one  computer  or  more  and  (b)  on  mobile 
devices that may reside in the vicinity of the targeted system. With the current 
popularity  of  'Bring  Your  Own  Device'  (BYOD),  mobile  phones  are  often 
carried into and out of the physical perimeter of the organization. 
The attack  is composed of  four main  steps:  (1)  inserting hostile code  into 
the  target  system;  (2)  infecting  mobile  phones  with  hostile  code;  (3) 
establishing a Command and Control (C&C) channel with the mobile phone; 
(4)  detecting  emanated  signals  and  transmitting  them  back  to  the  attacker 
(Figure 1). 
 
 

 
Figure 1: Demonstration of AirHopper. The targeted computer (A) emanates 
radio signals that are picked up by a mobile phone (B).  In this 
demonstration, the distance between (A) and (B) is 4 meters. 

 
 

3 
 

This is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 
 
2.1   Infecting the Target Machine 
 

Once the air gap barrier is breached, hostile code can infect systems within 
the targeted network. Such breaches are possible, as demonstrated by Stuxnet 
[6] [7] and Agent.btz [8]. It would be reasonable to assume that most similar 
cases  are  unpublished.    Possible  attack  vectors  are  (a)  through  removable 
media or (b) through outsourced software or hardware components. Once the 
hostile code runs on one node of the target network, it may propagate further 
and infect other nodes. 
 
2.2   Infecting the Mobile Phone 
 

Regarding mobile phones, the array of attack vectors is considerably wider 
than  those  target  computers  offer  since  these  devices  are  connected  to  the 
outer world through a host of interfaces, such as cellular networks, Bluetooth, 
Wi-Fi, and physical connections to other devices. Most probably, the attacker 
would  try  to  locate  and  infect  mobile  phones  of  those  employees  in  the 
targeted organization, who are most likely to work in the vicinity of targeted 
computers. Similar to the early stage of an Advanced Persistent Threat (APT), 
this step may utilize data mining, social networks, phishing and similar social 
engineering methods. The mobile phone can be  infected via  the  Internet,  by 
visiting  contaminated  sites,  by  email  messages  with  malicious  attachments, 
or  by  installing  malicious  applications.  Other  infection  routes  include 
physical access, SMS vulnerabilities  [9], and  interference with  the signaling 
interface [10]. 
 
2.3   Establishing the C&C Channel 
 

After  infecting  the  mobile  phone,  the  attacker  should  establish  a  C&C 
channel. This can be done via the Internet (data or Wi-Fi), or by SMS, where 
messages  can  be  sent  and  received  without  alerting  the  user.  If  suitable 
connectivity  with  the  attacker  is  limited  or  discontinuous,  the  malicious 
program  that  runs  on  the  mobile  phone  may  store  the  accumulated 
information, and  send  it back  to  the attacker when  the desired  connection  is 
available. 
 
2.4   Radio Monitoring 
 

The  mobile  phone  should  monitor  the  radio  channel  in  order  to  detect 
emanated  information. When  an  emanated  broadcast  is  detected,  the  device 
decodes it. Monitoring may be: (1) continuous as long as the mobile phone is 
active; (2) temporal, based on the work schedule of the user; (3) spatial, based 
on  the  current  location  of  the  user,  as  indicated  by  GPS,  wireless  network 

4 
 

This is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 
cells, or otherwise; or (4) initiated upon receiving a remote command through 
the C&C channel. 
 
3.  Background 
 

Generating  radio  signals  from  video  card  has  been  discussed  in  several 
works and technical papers [11] [12] [13] [14]. The main parameter required 
for transmission calculation is the pixel clock. The pixel clock is measured in 
MHz,  and  it  represents  the  frequency  at which  the  pixels  are  transmitted  to 
screen such that a full frame of pixels fits within one refresh cycle. It can be 
calculated with a simple multiplication: 
 

(Hpixel + Hsync )(Vpixel + Vsync)(Rr) = PC 

 
where  Hpixel   and  Vpixel   are  vertical  and  horizontal  resolution  (e.g.,  1024 
and 768) settings, Vsync , Hsync  are the vertical and horizontal synchronization 
length (in pixels) , Rr is the refresh rate (e.g., 60Hz) and PC is the pixel clock. 
These parameters are determined by the display standard. Generally, in both 
analog  and  digital  standards,  pixels  timing  is  mainly  determined  by  the 
display resolution and refresh rates. 
Given  the  graphic  card  standards,  it  is  possible  to  construct  an  image 
pattern  that  while  being  sent  to  the  display  will  generate  a  carrier  wave 
modulated with a data signal. The first option is to directly affect the voltage 
of  the  signal  by  changing  pixel  colors.  Amplitude  modulation  (AM)  is  the 
obvious choice for data transmission of this type. By gradually changing the 
shade of the pixels over a fixed carrier the amplitude of the carrier signal can 
be  changed.  Previous  research  has  demonstrated  intentional  generation  of 
modulated radio signals using monitors and display adapters  [15] [12] [14]  . 
The purpose of our research is to transmit FM signals which will be received 
and  interpreted  by mobile  phone  FM  receiver.  FM  signals  are  generated  by 
constructing an image of alternating sequences of black and white pixels that 
reflect  the  required  carrier  frequency modulation.  In  FM modulation, white 
and black pixels  are used  to  generate  fixed, maximal  amplitude  for  stronger 
signal generation.  
 
4.  Implementation 
 

For  the  purpose  of  testing  and  evaluating  the  realistic  applicability  of  the 
attack, we developed  the "AirHopper" malware.  In  this  section, we describe 
the  implementation  of  the  malware's  two  components;  a  transmitter 
implemented for the PC and a receiver implemented for the mobile phone.  
Reception  of  the  generated  signals  with  a  mobile  phone's  FM  receiver 
poses  new  challenges  and  opportunities.  One  of  the  main  challenges  is  the 
fact that FM-receiver chips in mobile phones are decoding the FM signals in 

5 
 

This is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 
DSP  (Digital  Signal  Processing)  chips  and  then  generating  audio  signals. 
Consequently,  the  mobile  phone's  operating  system  has  access  only  to  the 
generated  audio  output  (layer  2,  in  Figure  2)  and  not  to  carrier  waves 
themselves. Hence,  in order  to  send digital  information  to  receiver, data has 
to be modulated using audio tones (layer 3, in Figure 2). In addition, the audio 
frequencies  used  in  modulations  were  selected  given  the  FM  receiver 
characteristics and limitations, as mentioned in section 4.2.  
 
 

Figure 2: AirHopper modulation layers 

 

 
4.1   FM Audio Signal Generation 
 

In  this  section  we  describe  the  algorithm  used  by  AirHopper   to  generate 
pixels pattern (image) that cause emission of specific FM audio tone.  For the 
purpose  of  the  audio  modulation  algorithm,  we  denote  𝐹𝑐   to  be  the  carrier 
frequency  which  the  FM  receiver  must  be  tuned  to  in  order  to  receive  the 
transmission (e.g. 90 MHz). This value is limited to be twice the pixel clock 
at  most  [15].  However,  since  the  signal  generated  by  the  display  adapter  is 
nearly  square  (alternating  between  "black"  and  "white"  pixels),  it  generates 
strong harmonics which may be used to ignore this limitation and transmit on 
a frequency higher than twice the pixel clock with some loss of signal quality 
[15]. 
𝐹𝑑  denotes the data frequency (audio tone) to be modulated over the carrier. 
𝑃𝑐  denotes the value of the pixel clock previously described and 𝐻𝑝 , 𝑉𝑝  are the 
horizontal and vertical resolution parameters, plus the front and back porches 
which are periods of time in which no displayed pixel data is sent. This time 
period is used to allow for a CRT monitor time to retract its beam back to the 
top at the end of a frame and to the left at the end of a line. These are left over 
in the standard for compatibility with CRT display technology.   
The  modulation  process  outlined  by  the  following  algorithm  (Algorithm 
1),  is  loosely  based  on  the  code  from  Tempest  for  Eliza  [14].  A  modified 
variant was implemented in order to improve quality of the transmitted signal.  
Intuitively, the above algorithm decides which pixel to color by emulating 
the  carrier  frequency's  peaks.  The  resulting  image  is  made  of  alternating 
horizontal  "stripes"  consisting  of  either  a  black  and white  pixel  pattern  that 

6 
 

FM signals  (87.5-108MHz)Audio (~0-44KHz)Data (text/binary)1Layer 2Layer 3Layer Modulations(AirHopperPC)Demodulations(AirHopperMobile)FM chip/DSPThis is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 
matches  the  carrier  frequency  or  an  empty  black  stripe.  The  inner  loop 
calculates the pattern that emulates the carrier frequency 𝐹𝑐 . The width of the 
stripes, and consequently the number of stripes is determined by the value of 
𝐹𝑑  and calculated in the outer loop. Simply put, this periodic change in stripes 
modulates the audio tone 𝐹𝑑 . 
Given  a  pixel  map,  the  following  approximate  formula  can  be  used  to 
determine the generated signal frequency. 
 

(𝑌1 − 𝑌0)

(𝐻𝑝𝑖𝑥𝑒𝑙 + 𝐻𝑠𝑦𝑛𝑐 )
𝑃𝑐

≈

1
𝐹𝑑

 

 
𝑌0  and 𝑌1  are  the vertical pixel coordinates of  two consecutive pixel  rows 
that  have  the  same  pixel  pattern.  This  formula  uses  the  video  card  timing 
parameters to approximately calculate the period time of the data signal.  
                             
01 k ← 2 * Fd / PC , t ← 0 
02 all pixels ← BLACK 
03   For i ← 0 to Vp 
04     IF floor(t*k)is odd  
05       For j ← 0 to Hp 
06         IF floor(t*k) is odd  
07           pixel[j][i] ← WHITE 
08         t←t+1 
09     Else 
10       t←t+Hp 
 
Algorithm 1: Pixel map for signal tone (Fd) modulation 

            

 

 
4.2   AirHopper Transmitter - Data Modulation over Audio 
 

We  used  two  techniques  to  modulate  digital  information  over  the  audio 
signals,  Audio  Frequency-Shift  Keying  (A-FSK)  and  Dual-Tone  Multiple-
Frequency  (DTMF).  Our  evaluation  shows  that  A-FSK  is  less  sensitive  to 
interferences and can transmit to longer distances compared to DTMF. DTMF 
on  the  other  hand,  is  more  efficient  in  terms  of  bandwidth  and  binary  data 
transmissions.  
With both techniques, the data was modulated over audio frequencies. Our 
experiments  show  that  frequencies  lower  than  600Hz  suffered  from 
significant interference, and extensive reception testing showed that reception 
distance  started  to  diminish  significantly  for  signal  frequencies  greater  than 
11 kHz. We limited the transmission range accordingly in both methods. 
 

7 
 

This is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 
4.2.1  A-FSK.  With  this  data  modulation  method  over  audio  each  letter  or 
character was keyed with different audio frequency. Technically, this is done 
by  showing  an  image  on  the  entire  screen  area  to  generate  a  single,  unique 
signal  frequency,  and  keeping  the  image  static  for  a  short  time  span.  The 
receiver  is  then  responsible  for  sampling  the  demodulated  audio  signal  and 
decoding the data. Using less than 40 distinct audio frequencies, we were able 
to encode simple textual data – both alphabetical and numerical. This method 
is  very  effective  for  transmitting  short  textual  massages  such  as  identifiers, 
key-stroking,  keep-alive  messages  and  notifications.  During  our  extensive 
testing this method proved to be the most resilient to interference and had the 
greatest  reception  range  and  the best  accurate data  recovery  rate.  Following 
these  positive  results,  we  attempted  modulating  arbitrary  binary  data  using 
256 different audio tones, resulting in frequency spacing of 11000/256≈40Hz 
per  character.  Our  tests  resulted  in  higher  error  ratio  and  ambiguous 
demodulations  at  the  decoder  side.  The  primary  reason  is  that  the  digital 
signal  processing  (DSP)  component  in  FM  receivers  utilizes  adaptive  noise 
filtering,  equalization  and  other  techniques,  to  enhance  the  listening 
experience. These methods are altering adjacent audio  frequencies, unifying 
them or filtering  them as noise. As a  result of the above, we adapted DTMF 
modulation to encode binary data. 
 
4.2.2.  DTMF(16x16).  In  the  DTMF  schema,  a  combination  of  two  tones 
represents a byte of data. This is similar to using two hexadecimal digits. The 
tones  were  selected  from  a  table  of  16x16  frequency  pairs  enabling  256 
different  combinations.  The  table's  columns  are  frequencies  in  the  600Hz-
5000Hz  range,  and  the  table's  rows  are  frequencies  in  the  6000Hz-10400Hz 
range. E.g., a byte containing data equivalent to the value 134 is represented 
by a tone in the eighth row (⌊134/16⌋=8) and a tone in the sixth column (134 
mod 16=6), played simultaneously. The transmission of two modulated audio 
signals  is done by  logically splitting  the screen  into  two halves,  transmitting 
a  different  image  to  each  half.  This  resulted  in  some  loss  of  range  and 
reception quality, but the tradeoff was a much higher data transfer rate.  
 
4.3   AirHopper Receiver - Data Demodulation from Audio 
 

An essential part of  the malicious code on  the mobile phone  is  the ability 
to record the received FM audio and decode the data or forward the recording 
to the attacker. The important implementation details are described in the next 
sections. 
 
4.3.1  Android  FM  audio  output  redirection.  In  order  to  process  the  FM 
audio, its output has to be recorded or saved. Recording output from the FM 
radio  was  undocumented  in  Android  APIs  up  to  API18  (Android  4.3,  Jelly 
Bean  MR2).  By  disassembling 
the  MediaRecorder 
class 
file 
(MediaRecorder$AudioSource.smali)  from  the  Android  framework,  we 

8 
 

This is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 
found how  to enable FM  radio  recording by using AudioRecord object with 
audioSource=8 and sampleRateInHz=44100 [16]. 
 
4.3.2  Audio  sampling.  Modern  mobile  phones  support  a  maximum  audio 
capture sampling rate of 44.1 KHz. The Nyquist–Shannon sampling theorem 
states  that  perfect  reconstruction  of  a  signal  is  possible,  when  the  sampling 
frequency  is  greater  than  twice  the maximum  frequency  of  the  signal  being 
sampled. Consequently, sampling can be accomplished at 20 KHz maximum 
frequency. 20 KHz is the highest frequency generally audible by humans, thus 
making 44.1 KHz the logical choice for most audio material. Using Android’s 
recording API, the radio signals were recorded and stored in a memory buffer, 
using Pulse-Code Modulation (PCM) format at 16 bit per sample. 
 
4.3.3 Signal processing. Fast Fourier Transform (FFT) transforms the buffer 
from the time domain to the frequency domain. Since there are 1024 samples 
per  chunk,  we  generate  a  discrete  function  that  depicts  the  spectrum  of  the 
recorded  signal,  where  the  amplitude  value  of  each  index  represents  a 
frequency range of 44100/1024≈43Hz. Depending on the modulation method 
(A-FSK or DTMF)  -  the spectrum  is expected to contain one or  two distinct 
amplitudes. In DTMF modulation, there should be one frequency index with 
distinctively  high  amplitude,  in  each  half  of  the  spectrum  range  (600Hz -
5000Hz  and 6000Hz-10400Hz). The  frequency  index pair  is  then  compared 
to the DTMF frequency table to recover the transmitted byte.  
 
4.3.4 Bypassing headphone requirement. Some phone models require that 
headphones should be connected to enable the user to turn on the radio. This 
is necessary since the headphone cable is being used as an antenna for the FM 
receiver  chip. Without  an  antenna,  the  reception  quality  of  the  FM  receiver 
will  be  poor. While  it  is  technically  possible  to  receive  a  signal  without  an 
antenna,  the  headphone  cable  requirement  ensures  a  good  user  experience. 
We  found  that  this  limitation  is  implemented  by  the  vendor  at  the  software 
level, and that it can be bypassed. We disassembled the Samsung Galaxy S3 
framework  file  (/system/framework/framework2.odx)  using  baksmali  [17] 
disassembler.  In  the  service  file  (FMRadioService),  the mIsHeadsetPlugged 
variable  was  initialized  to  'true',  and  the  headphone  check  methods 
("access$302), was modified to return  'true'. This  tricks the application level 
headphone checks to pass, regardless of the true condition. 
 
4.4   Transmission Protocol 
 

AirHopper  employed  two modes of  transmissions;  raw  and  structured.  In 
raw  transmissions,  the  data  stream  is  taken  from  an  array  of  bytes  and 
transmitted sequentially. In case of signal loss or interruptions, the receiver is 
unaware  of  it.  Streaming  the  raw  data  sequentially  is  suitable  for  textual 
information  such  as  key-logging  and  text  files  or  for  short  signaling  and 

9 
 

This is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 
notifications  messages.  Such  information  is  valuable  even  when  some 
characters are missing. The structured protocol is appropriate for binary data 
transmissions when it is important to identify errors and know which part of 
the file or data was actually received. The transmission headers include an (1) 
initial  synchronization pulse,  (2)  the  sequence number of  the packet,  (3)  the 
size of the packet, (4) the data itself, and (5) a checksum. AirHopper receiver 
can easily detect which protocol  is used (raw of structured) by searching for 
synchronization pulse. 
 
5.  Hiding 
 

In  order  to  remain  hidden  while  transmitting,  AirHopper  uses  various 
techniques for hiding both visual aspects and transmitted signal. 
 
5.1   Visual Hiding 
 

This  section  explains  how  to  eliminate  the  visual  appearance  of  the 
transmitted  data  on  a  computer  screen.  Three  of  the  suggested  techniques 
utilize  standardized  means  of  communication  with  an  attached  monitor. 
HDMI,  DVI  and  VGA  monitors  and  VDUs  implement  the  Display  Data 
Channel (DDC) family of protocols.  
Display  Data  Channel  Command  Interface  (DDC/CI) 
is  a 
itself 
bidirectional communication protocol between computer monitor and VDU. 
Monitor Control Command Set  (MCCS)  is a known standard, developed by 
Video  Electronics  Standards  Association  (VESA),  that  specifics  the  set  of 
commands  for  controlling  monitors  over  the  DDC/CI  communication 
protocol. 
 
5.1.1 Transmitting when monitor is off. With DDC/CI, the transmitter can 
determine  that  a  monitor  is  off,  and  only  then  start  the  transmissions.  A 
monitor goes off  as  a  result of power-saving  settings, when  the  computer  is 
idle for predefined period. As soon as the transmitter detects that the monitor 
is  back  on,  it  stops  the  transmissions.  Note  that  since  the  VDU  constantly 
generates  signals,  and  the  monitor  cable  is  used  only  as  an  antenna,  the 
transmission is just as effective with the monitor turned off. Technically, the 
monitor  status  is  determined  by  checking  Power  Control  Status  (address 
0xe1)  of DDC/CI.  The  transmitter  continuously  samples  this  status,  polling 
ioctl (I/O control) in a dedicated thread. 0 indicates that the screen is powered 
off. 
 
 
5.1.2  Turning  the  monitor  off.  Using  DDC/CI,  the  transmission  code  can 
also  send a command  to  intentionally  turn off  the monitor. This method can 
be used in the same manner as a screen-saver; the code monitors the computer 

10 
 

This is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 
for  user  activity  and waits  for  a  period  of  idle  time.  In  case  o f  inactivity,  it 
will turn off the monitor and start data transmissions until a user activity, such 
as  a  mouse  movement  or  keyboard  press,  is  detected.  It  will  then  stop 
transmitting and turn the monitor back on, in the same way a normal screen -
saver  or  power  saving  feature  would  behave.  Like  the  previous  method, 
turning the monitor on/off is done by writing 1 or 0 to 0xe1, using ioctl. 
 
5.1.3 Switching  the monitor  to a different computer. Another method  for 
hiding  the  displayed  image  from  users  utilizes  the  popular Keyboard Video 
Mouse  (KVM)  switch.  This  switch  is  a  device  that  allows  desktop  basic 
peripherals,  keyboard  mouse  and  monitor,  to  be  shared  between  multiple 
computers.  By  clicking  a  button  on  the  switch  or  using  a  keyboard  key 
combination  the  user  switches  between  desktops.  Since  this  allows  the  user 
to comfortably use multiple desktops on one office desk, it is commonly used 
in organizations with air-gapped networks. Using DDC/CI, a transmitter can 
detect  when  the  KVM  switches  between  desktops  and  start/stop  the 
transmissions accordingly.  
 
5.1.1 Using  secondary display output. Another method  that we  tested was 
to force the VDU to transmit on a specific video output even though it did not 
detect  a  connected monitor. This  capability  can be useful when  a  computer, 
laptop  or  desktop,  has  multiple  display  outputs.  The  transmitting  code  can 
detect  which  video  output  is  currently  being  used  and  redirect  the 
transmission to an inactive output. Since in most cases only one display cable 
is connected, and transmitting without a display cable as an antenna requires 
close proximity to the receiver, this method is effective in cases when the user 
places his mobile phone on the desk right next to the laptop/desktop.  
Another scenario is when laptop/desktops are used in conference rooms with 
a  secondary  persistent  connection  to  a  projector.  The  projector  output  is 
turned  off  or  disconnected  most  of  the  time,  allowing  for  very  long 
transmission  periods with  nothing  suspicious  being  visible. Our  testing  also 
showed  that  the  long,  unshielded  cables  that  are  usually  used  to  connect  to 
projectors  are  also  ideal  for  transmission  quality  and  can  be  received  by 
phones in adjacent rooms. 
 
 
5.2   Signal Hiding 
 

Another  detection  risk  is  accidental  reception  of  the  transmitted  signal. 
Since  the  system  that  modulates  transmitted  data  uses  audible  tone 
frequencies,  tuning  a  regular  FM  receiver  to  the  carrier  frequency  used  for 
transmission, results in a noticeable, changing tone that may alert suspecting 
individuals.  In  order  to  reduce  the  risk  of  such  an  accidental  detection,  we 
extended  the  phone’s  FM  reception  band  to  76  MHz  –  108  MHz  and  the 
transmission carrier frequency to 80MHz.  

11 
 

This is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 

FM transmissions between 76 MHz to 87.5 MHz are only used in very few 
countries, such as Japan. Most FM receivers available elsewhere in the world 
do  not  allow  tuning  or  seeking  in  this  range.  We  found,  however,  that 
although the nonstandard range is actually supported by FM chips in mobile 
phones,  the upper software  layers disable  it. By  transmitting  in  the extended 
range, accidental detection can be avoided in most countries. On the receiving 
phone, we  enabled  reception of  the  extended  frequency  range by modifying 
the  Samsung  framework  utilizing  smali  [17]  disassembler.  Technically,  the 
framework  file  is  located  in  /system/framework.  We  changed  the  class 
FMRadioService  in  the  package  com.android.server  by  setting  the  value  of 
BAND_76000_108000_kHz  to  1. This modification  at  the OS  level  code  is 
unnoticeable  by  the  phone's  user.  Using  a  nonstandard  band  also  helps  to 
avoid collisions with local radio station broadcasts, and allows a wider range 
of  available  transmission  frequencies.  Another  modification  that  deters 
accidental  detection  involved  changing  the  phone's  frequency  seeking 
interval from the default 200 KHz to 50 KHz. This allows the transmitter and 
receiver code to fine-tune to carrier frequencies that many FM receivers will 
not allow for an ordinary user, hence avoiding accidental detection. 
 
6.  Evaluation 
 

In  this  section  we  evaluate  the  feasibility  and  the  effectiveness  of  the 
proposed data exfiltration method. The main efficiency measures include the 
emitted signal strength,  the effective broadcasting distance,  the transmission 
speed,  the  transmission  quality,  the  battery  consumption,  and  the  scanning 
time. The factors include the cable type, the presence of receiver antenna, the 
modulation method (A-FSK/DTMF), the carrier frequency, the signal delay, 
the  transmitted  data  size,  and  the  range  of  transmitted  frequencies.  Further 
explanation  of  the  various  efficacy  measures  and  factors,  along  with 
explanation  of  their  relevance,  is  provided  below,  following  a  short 
description of the experimentation setup. 
 
6.1   Setup 
 

The  monitor  used  was  a  Samsung  SyncMaster  SA450.  The  operating 
system was Ubuntu 13.04 (raring) 64bit, kernel 3.8.0-29-generic. 
The  mobile  phone  was  a  Samsung  GT-I9300  Galaxy  S3.  The  Android 
version was Samsung stock 4.1.2. The baseband version was I9300XXEMC2. 
Kernel version was 3.0.31-1042335. The radio chip was Silicon Labs Si4705 
FM  Radio  Receiver  ICs.  We  used  the  standard  1.22  meters  (4  ft.)  stereo 
headphones that came with the mobile phone. Note that in this mobile phone 
model, the headphones are used as the FM receiver antenna. 
Four  types  of  cables were  used  to  connect  the  computer with  the  display 
unit:  (I) standard VGA  cable, 1.52 meters  (5  ft.), 30V, 15-pin male  to male; 

12 
 

This is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 
(II) standard VGA  cable, 0.91 meters  (3  ft.), HD15, male  to  female, usually 
used as projector extension cable; (III) HDMI cable, 0.91 meters (3 ft.), 30V. 
(IV) DVI  cable,  1.82 meters  (6  ft.),  30V.  Below we  refer  to  these  cables  as 
standard  VGA,  extension  VGA,  HDMI  and  DVI,  respectively.  Table  1 
provides relevant classifications of the cable types (i.e. Shielded/Unshielded, 
Analog/Digital). 
 

Table 1: Cable Classification 

Cable 
std VGA  
ext VGA  
HDMI 
DVI 

Shielding 
Shielded 
Unshielded 
Shielded 
Shielded 

Signal 
Analog 
Analog 
Digital 
Digital 

 
6.2   Signal Strength 
 

The strength of the received signal is expected to drop as the distance from 
the  transmitter  is  increased.  We  measured  the  Received  Signal  Strength 
Indication  (RSSI)  (see  Figure  3),  as  well  as  the  dBm  (decibel-milliwatts, 
see Figure 4), across varying distances. RSSI is an indicator of signal quality 
usually exposed by a chip's API. The RSSI scale is arbitrarily selected by the 
manufacturer  (Broadcom  in  the  case  of  the  tested  mobile  phone)  and  is 
usually a logarithmic scale related to the peak voltage of the received signal. 
dBm is also a logarithmic scale based on the power of the received signal  in 
reference to one milliwatt (power is affected by both voltage and current).  
The  RSSI  measurement  was  performed  by  calling  the  getCurrentRSSI 
function  of  IFMPlayer,  an  internal  Samsung  service  used  by  the  FM  radio 
player. dBm values were measured with an external spectrum analyzer.   
The RSSI measurements appear to be correlated with the effective distance, 
where  a measured value  of 10 on  the RSSI  scale  roughly  indicates  the  limit 
of the effective distance on each of the tested cables. 

13 
 

This is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
Figure 3 :  Signal strength at varying distances (RSSI), using A-FSK. 
(Measured by mobile-phone FM receiver) 

 
 

 
 

 
Figure 4 : Signal strength at varying distances (dBm), using A-FSK. 
(Measured by an external spectrum analyzer) 

 
6.3   Effective Distance 
 
The effective distance is the maximal distance between the transmitter and 
the  receiver,  at which  the  transmission  quality  is  still  acceptable. As  can  be 
seen  in  Figure  5,  the  effective  distance  when  using  the  receiver  antenna  is 
significantly  larger with unshielded cable (extended VGA), compared  to  the 
shielded cables  (HDMI and  standard VGA). To summarize, with both cable  
types,  the  effective  distance  when  the  receiver  antenna  is  present  is  in  the 
order of 8-20 meters, which can be considered as useful for our purposes. 
Some  new  models  of  mobile  phones  are  equipped  with  built-in  FM 
antenna, which voids the need for headphones. 

14 
 

010203040506013579111315171921RSSIDistance (meters)std VGAext VGAHDMI/DVI-70-65-60-55-50-45-40-35-30-25-20                   dBmDistance (meters)std VGAext VGAHDMI/DVIThis is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 

 

 
Figure 5: Signal strength at varying distances, RSSI, using A-FSK 

6.4   Data Modulation Method 
 
The  signaling  method,  i.e.  A-FSK  or  DTMF  also  affects  the  effective 
distance. As can be seen  in Figure 6 A-FSK yields a slightly larger effective 
distance. 

 
Figure 6: Effective distance with varying cable types, with receiver antenna, 
using A-FSK and DTMF 

 
6.5   Transmission Quality 
 

The transmission quality (also referred to as the 'success rate') is defined as 
the rate of correctly received bytes per originally  transmitted bytes. The rate 

15 
 

110100100010000std VGAext VGAHDMI / DVIEffective Distance (cm)Cable typewithout antenawith antena0510152025std VGAext VGAHDMI/DVIEffective distance (meters)Cable typeA-FSKDTMFThis is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 
was  measured  against  varying  signal  delays,  i.e.  the  time  interval  (in 
milliseconds) assigned for transmitting one byte of data.  
As can be seen in Figure 7, the correlation between the transmission quality 
and  the  signal  delay  is  logarithmic,  asymptotically  approaching  100%. 
Beyond  a  signal  delay  of  70ms,  the  additional  transmission  quality  seems 
negligible. 
Settling  on  70ms  seems  reasonable,  since  a  higher  delay  would  result  in 
slower  transmission  which  does  not  yield  significant  improvement  in  the 
transmission quality. 

 
Figure 7: Transmission quality at varying signal delays, using DTMF. 

 

As  shown  in  Table  2,  the  transmission  quality  (or  success  rate)  was 
measured  for  each  cable  type,  over  varying  distances. The  signal  delay was 
70 ms, which  is  the optimal value. The measurements  are  shown  for 0.3 m, 
for 6.2 m, and for the maximal distance at which the transmission quality was 
acceptable  (or  the  effective distance). As  can be  seen,  the maximal  distance 
has different values for various cable types. 
The  values  in  Table  2  reflect  the  behavior  of  the  transmission  quality, 
which retains an almost fixed high value over an increasing distance, until  it 
hits the maximal distance (which varies among the cable types). 
 

Table 2: Transmission quality over varying distance 
using DTMF 

Cable type 

Std VGA 
Ext VGA 
HDMI 
DVI 

Transmission quality (%)  Max distance 
(meters) 
At 0.3 m  At 6.2 m  At max 
7 
99.86 
99.27 
97.73 
22.2 
99.07 
99.37 
99.88 
6.8 
99.61 
99.27 
99.21 
99.5 
99 
99 
6.8 

16 
 

50556065707580859095100      Transmission quality (%)Signal delay (ms)std VGAext VGAHDMI/DVIThis is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 
 

6.6   Data Size 
 

Effective  exfiltration  of  data  places  practical  limits  on  the  transmission 
time.  The  transmitting  program,  running  on  the  compromised  computer, 
should  hide  its  activities  and  has  limited  safe  time  intervals  where  the 
collected  data  can  be  broadcasted.  Additionally,  a  receiving  mobile  phone 
does not stay within the effective range of the transmitter for indefinite time. 
In  Table  3 we  show  the  transmission  time  of  various  types  of  files, with 
typical  sizes,  when  using  raw  data  and  when  using  structured  packets  (as 
described  in  the  'Implementation'  section).  A  70ms  delay  was  found  to  be 
optimal. The  time  required  for  transmitting  tiny-size data  (below 100 bytes) 
such  as  IP  &  MAC,  or  password,  is  below  ten  seconds,  which  seems 
acceptable.  Small-size  data  (below  10  KB)  such  as  system-information  or 
one-day  keylogging  would  require  several  minutes,  which  also  seems 
acceptable. However, documents  in  the order of 0.5 MB would  take  several 
hours  to  transmit and  this would  seem  to be unacceptable. Note  that  the  toll 
inflicted  by  using  structured  packets  compared  to  raw  data  seems  to  be 
tolerable, and justifies using this method when accurate reception is required. 
 
 
 

Table 3: Transmission time with varying data size (70ms delay) 

File (data size) 

IP & MAC (10 bytes) 
Password file (100 bytes) 
1 hour keylogging (1 KB) 
System information (2 KB) 
1 day keylogging (8 KB) 
Document (0.5MB) 

Raw data  Structured 
packets 
1 sec 
10 sec 
106 sec 
211 sec 
14.1 min 
15 hour 

<1 sec 
8 sec 
77 sec 
153 sec 
10.25 min 
10.9 hour 

 
 
 
7.  Countermeasures 
 
    Defensive countermeasures for general Tempest threats can be categorized 
into  technical vs. procedural. Countermeasures of  the technical kind  include 
(a)  physical  insulation,  (b)  software-based  reduction  of  information-bearing 
emission,  and  (c)  early  encryption  of  signals.  Procedural  countermeasures 
include  official  practices  and  standards,  along  with  legal  or  organizational 
sanctions. The relevant American and NATO standards, concerning Tempest, 
were held highly classified  throughout  the Cold War. Some of the standards 

17 
 

This is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 
eventually leaked or were released but are severely redacted [18] while more 
informative  documents, 
in  particular 
those  concerning  defensive 
countermeasures, are still classified. The prevailing standards aim at limiting 
the  level of detectable  information-bearing RF  signals over a given distance 
on  open  air,  or  its  equivalent  when  insulating  materials  are  used.  In  effect, 
certified  equipment  is  classified  by  'zones' which  refer  to  the  perimeter  that 
needs to be controlled to prevent signal reception  [19]. As a countermeasure 
against  attacks  like  the  one  in  this  research  the  ‘zones’  approach  should  be 
used to define the physical areas inside the organization, in which carrying a 
mobile phone or other radio receivers is prohibited. Another countermeasure 
is ensuring  that only properly shielded cables are used. Shielding affects  the 
effective range as shown by our experiments.   
 
8.  Related Work 
 

Anderson  [19]  provides  a  highly  informative  review  of  emission  or 
emanation  security  (Emsec):  preventing  attacks  which  use  compromising 
emanations  (CE)  consisting  of  either  conducted  or  radiated  electromagnetic 
signals.  In  1985,  van  Eck  [11]  demonstrated  how  Tempest  exploits  can  be 
produced with rather ordinary equipment and skills. Using a modified TV set, 
he managed  to  reconstruct  an  image  from  electromagnetic  signals  produced 
by Video Display Unit (VDU) at a considerable distance. During  the second 
half of the 1990s, several researches and publications related to Tempest were 
released  [12]  [13],  spurring  increased  public  interest.  This  trend  was 
amplified  by  the  Web  which  has  offered  a  glimpse  into  classified  official 
standards  related  to  Tempest  [18],  or  provided  detailed  instructions  and 
tutorials  related  to  Tempest  exploits  [14].  Kuhn  and  Anderson  [12]  [13] 
demonstrated  that many  of  the  compromising  emanations  from  a PC  can be 
manipulated by suitable  software,  in either a defensive or offensive manner. 
Thiele  [14]  [20]  offers  an  open  source  program  dubbed  'Tempest  for Eliza', 
that uses  the computer monitor to send out AM radio signals. The generated 
music  can  then  be heard  on one’s  radio. Kania  [15] provides  a  collection of 
programs for generating FM radio signals  in software, and  then transmitting 
them  using  a VGA  graphics  card.  In  our  implementation, we  used  different 
technique to generate FM signals in order to improve the signals quality.    
 
9.  Conclusion 
 

Exfiltration of data from air-gapped networks is not a trivial task. However, 
in  this  paper  we  introduce  AirHopper,  a  bifurcated  attack  pattern  by  which 
this  challenging  task  can  be  accomplished. The  core  of  the method  consists 
of  two  essential  elements:  (a)  electromagnetic  signals  emitted  from  a 
computer's monitor cable, with data intentionally modulated on those signals, 
and  (b)  an  FM  receiver  on  a  mobile  phone  that  can  collect  the  transmitted 

18 
 

This is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 
signals,  and  extract  the  modulated  data.  The  chain  of  attack  is  rather 
complicated,  but  is  not  beyond  the  level  of  skill  and  effort  employed  in 
modern  Advanced  Persistent  Threats  (APT).  Evaluation  of  the  proposed 
method includes experimental measurements of various measures such as the 
effective  transmitting distance,  cable  type,  the presence of  receiver antenna, 
etc. 
AirHopper  adds  to  an  understanding  of  electromagnetic  emission  threats, 
coupled with APT  techniques. This  research area  is not  sufficiently  covered 
in  recent  academic  literature.  We  believe  that  a  careful  professional  and 
academic discussion of this threat, ultimately serves the interest of the cyber-
defense community. 
 
References 
 
[1]   Nokia,  "Nokia  Corporation  Interim  Report  for  Q2  2013  and  January-
June 2013," 2013. 

[2]   Gartner, "Gartner Says Smartphone Sales Grew 46.5 Percent in Second 
Quarter of 2013 and Exceeded Feature Phone Sales for First Time," 14 
Available: 
August 
[Online]. 
2013. 
http://www.gartner.com/newsroom/id/2573415. 
[Accessed 
15 
September 2013]. 

[3]  

phoneArena.com, 
[Online].  Available: 
finder,"  2013. 
"phone 
http://www.phonearena.com/phones. [Accessed 8 October 2013]. 

[4]   M. Stroh, "Windows Phone Blog," Microsoft, 14 May 2013. [Online]. 
Available: 
http://blogs.windows.com/windows_phone/b/windowsphone/archive/2
013/05/14/nokia-s-first-metal-windows-phone-arrives-meet-the-sexy-
lumia-925.aspx. [Accessed 8 October 2013]. 

[5]   National  broadcaster  association,  "Equipping  Mobile  Phones  with 
Broadcast Radio Capability  for Emergency  Preparedness,"  September 
2013. 
[Online]. 
Available: 
http://www.nab.org/documents/advocacy/FMEnabledPhones/RadioEn
abledPhonesIssueSheet.pdf. [Accessed 8 October 2013]. 

[6]   Q. Z. P. R. B. T. Clark A., "An impact-aware defense against Stuxnet," 
in American Control, 2013.  

[7]  

J. Larimer, "An inside look at Stuxnet," IBM X-Force, 2010. 

[8]   W. J. L. III, Defending a New Domain, 2010.  

[9]   C. A. E.  I. K. A. Hamandi K.,  "Android SMS Malware: Vulnerability 
and Mitigation," in Advanced Information Networking and Applications 
Workshops, 2013.  

[10]   R.-P. Weinmann, "Baseband Attacks: Remote Exploitation of Memory 
Corruptions in Cellular," in Usenix, 2012.  

19 
 

This is a slightly revised version of the paper accepted by the 9 th IEEE International 
Conference on Malicious and Unwanted Software (MALCON 2014).  

 
 
[11]   W. van Eck, "Electromagnetic Radiation from Video Display Units: An 
Eavesdropping Risk?," Computers and Security 4, pp. 269-286, 1985.  

[12]   M.  G.  Kuhn  and  R.  J.  Anderson,  "Soft  Tempest:  Hidden  data 
transmission using electromagnetic emanations," in Information hiding, 
Springer-Verlag, 1998, pp. 124-142. 

[13]   M.  G.  Kuhn,  "Compromising  emanations:  Eavesdropping  risks  of 
computer  displays,"  University  of  Cambridge,  Computer  Laboratory, 
2003. 

[14]   E.  Thiele,  "Tempest 
[Online].  Available: 
for  Eliza,"  2001. 
http://www.erikyyy.de/tempest/. [Accessed 4 10 2013]. 

[15]   B. Kania, "VGASIG: FM radio transmitter using VGA graphics card," 
19 
4 
2009. 
[Online]. 
Available: 
http://bk.gnarf.org/creativity/vgasig/vgasig.pdf. [Accessed 4 10 2013]. 

[16]   Google,  "Android  API:  AudioRecord,"  02  11  2013.  [Online]. 
Available: 
http://developer.android.com/reference/android/media/AudioRecord.ht
m. [Accessed 03 11 2013]. 

[17]   "smali. An assembler/disassembler for Android's dex format," [Online]. 
Available: https://code.google.com/p/smali/. 

[18]   J.  McNamara,  "The  Complete,  Unofficial  TEMPEST  Information 
Page," 
Available: 
[Online]. 
1999. 
http://www.jammed.com/~jwa/tempest.html. [Accessed 4 10 2013]. 

[19]   R.  J.  Anderson,  "Emission  security,"  in  Security  Engineering,  2nd 
Edition, Wiley Publishing, Inc., 2008, pp. 523-546. 

[20]   E. Thiele,  "Tempest  for Eliza  (updated Readme  file)," 2004.  [Online]. 
Available: 
http://www.olifantasia.com/projects/gnuradio/mdvh/videocard_rf_out
put/readme.fm.txt. [Accessed 4 10 2013]. 

[21]   USAF,  "AFSSI  7700:  Communications  and  information  emission 
security," Secretary of the Air Force, 2007. 

 
 

20 
 

Computer Security in the Real World 
 
 
Butler W. Lampson1 
Microsoft 
 
 

lion  or  two  machines,  and  newspapers  print  extravagant 
estimates  of  the  damage  it  does,  but  these  are  minor  an-
noyances. There  is  no accurate data about  the cost of  fail-
ures  in computer  security. On  the one hand, most of  them 
are  never  made  public  for  fear  of  embarrassment.  On  the 
other,  when  a  public  incident  does  occur,  the  security  ex-
perts  and  vendors  of  antivirus  software  that  talk  to  the 
media have every incentive to greatly exaggerate its costs. 
But  money  talks.  Many  vendors  of  security  have  learned 
to  their  regret  that  although  people  complain  about  inade-
quate  security,  they  won ’t  spend  much  money,  sacrifice 
many  features,  or  put  up  with  much  inconvenience  in  or-
der  to  improve  it. This  strongly  suggests  that  bad  security 
is not really costing them much. 
Of  course,  computer  security  is  not  just  about  com-
puter  systems. Like any  security,  it  is only as  strong as  its 
weakest  link,  and  the  links  include  the  people  and  the 
physical  security  of  the  system.  Very  often  the  easiest 
way  to  break  into  a  system  is  to  bribe  an  insider.  This 
short paper, however, is limited to computer systems. 
What do we want from secure computer systems? Here 
is a reasonable goal:  
Computers  are  as  secure  as  real  world  systems,  and 
people believe it.  
Most real world systems are not very  secure by the ab-
solute  standard  suggested  above.  It ’s  easy  to  break  into 
someone ’s  house.  In  fact,  in  many  places  people  don’t 
even  bother  to  lock  their  houses,  although  in  Manhattan 
they  may  use  two  or  three  locks  on  the  front  door.  It’s 
fairly easy to steal something from a store. You need very 
little  technology  to  forge  a  credit  card,  and  it ’s  quite  safe 
to use a forged card at least a few times. 
Why  do  people  live  with  such  poor  security  in  real 
world  systems?  The  reason  is  that  security  is  not  about 
perfect defenses against determined attackers. Instead, it ’s 
about  
value,  
locks, and  
punishment.  
The  bad  guy  balances  the  value  of  what  he  gains  against 
the  risk  of  punishment,  which  is  the  cost  of  punishment 
times  the  probability  of  getting  punished.  The main  thing 

Abstract 
After  thirty  years  of  work  on  computer  security,  why 
are  almost  all  the  systems  in  service  today  extremely  vul-
nerable  to  attack?  The main  reason  is  that  security  is  ex-
pensive  to  set  up  and  a  nuisance  to  run,  so  people  judge 
from  experience  how  little  of  it  they  can  get  away  with. 
Since  there ’s  been  little  damage,  people  decide  that  they 
don ’t  need  much  security.  In  addition,  setting  it  up  is  so 
complicated  that  it ’s  hardly  ever  done  right.  While  we 
await  a  catastrophe,  simpler  setup  is  the  most  important 
step toward better security. 
In  a  distributed  system  with  no  central  management 
like the Internet, security requires a clear story about who 
is  trusted  for  each  step  in  establishing  it,  and  why.  The 
basic tool for telling this story is the  “speaks for” relation 
between  principals  that  describes  how  authority  is  dele-
gated,  that  is, who  trusts whom. The  idea  is  simple, and  it 
explains what’s going on  in any  system  I know. The many 
different ways of encoding this relation often make it hard 
to see the underlying order.  
 
 

1 

Introduction 

People  have  been  working  on  computer  system  secu-
rity  for at  least 30 years. During  this  time  there have been 
many  intellectual  successes.  Notable  among  them  are  the 
subject/object  access  matrix  model  [11],  access  control 
lists  [17],  multilevel  security  using  information  flow  [6, 
13]  and  the  star-property  [3],  public  key  cryptography 
[14],  and  cryptographic  protocols  [1].  In  spite  of  these 
successes,  it  seems  fair  to  say  that  in  an  absolute  sense, 
the  security of  the  hundreds of millions of deployed com-
puter  systems  is  terrible:  a  determined  and  competent 
attacker  could  destroy  most  of  the  information  on  almost 
any  of  these  systems,  or  steal  it  from  any  system  that  is 
connected to a network. Even worse, the attacker could do 
this to millions of systems at once. 
On  the  other  hand,  not  much  harm  is  actually  being 
done  by  attacks  on  these  insecure  systems. Once  or  twice 
a  year  an  email  virus  such  as  “I  love  you”  infects  a  mil-

                                                            
1 blampson@microsoft.com, research.microsoft.com/lampson 

 

 

1 

that  makes  real  world  systems  sufficiently  secure  is  that 
bad  guys  who  do  break  in  are  caught  and  punished  often 
enough  to  make  a  life  of  crime  unattractive.  The  purpose 
of  locks  is  not  to provide absolute  security, but  to prevent 
casual intrusion by raising the threshold for a break-in. 
Well, what ’ s wrong with perfect defenses? The answer 
is simple: they cost too much. There is a good way to pro-
tect personal belongings against determined attackers: put 
them in a  safe deposit box. After 100 years of experience, 
banks  have  learned  how  to  use  steel  and  concrete,  time 
locks,  alarms,  and  multiple  keys  to  make  these  boxes 
quite  secure.  But  they  are  both  expensive  and  inconven-
ient.  As  a  result,  people  use  them  only  for  things  that  are 
seldom needed and either expensive or hard to replace.  
Practical  security  balances  the  cost  of  protection  and 
the risk of loss, which is the cost of recovering from a loss 
times  its  probability.  Usually  the  probability  is  fairly 
small  (because  the  risk  of  punishment  is  high  enough), 
and  therefore  the  risk  of  loss  is  also  small. When  the  risk 
is less than the cost of recovering, it ’ s better to accept it as 
a  cost  of  doing  business  (or  a  cost  of  daily  living)  than  to 
pay  for  better  security.  People  and  credit  card  companies 
make these decisions every day. 
With  computers,  on  the  other  hand,  security  is  only  a 
matter  of  software,  which  is  cheap  to  manufacture,  never 
wears  out,  and  can ’ t  be  attacked with  drills  or  explosives. 
This  makes  it  easy  to  drift  into  thinking  that  computer 
security can be perfect, or nearly so. The fact that work on 
computer  security  has  been  dominated  by  the  needs  of 
national  security  has  made  this  problem  worse.  In  this 
context the stakes are much higher and there are no police 
or courts available to punish attackers, so it ’ s more impor-
tant  not  to  make  mistakes.  Furthermore,  computer  secu-
rity  has  been  regarded  as  an  offshoot  of  communication 
security,  which  is  based  on  cryptography.  Since  cryptog-
raphy can be nearly perfect,  it’ s natural  to  think  that com-
puter security can be as well. 
What’ s wrong with this reasoning? It ignores two criti-
cal facts: 
•   Secure systems are complicated, hence imperfect. 
•   Security gets in the way of other things you want. 
Software  is  complicated,  and  it ’ s  essentially  impossi-
ble  to  make  it  perfect.  Even  worse,  security  has  to  be  set 
up  by  establishing  user  accounts  and  passwords,  access 
control  lists  on  resources,  and  trust  relationships  between 
organizations.  In  a  world  of  legacy  hardware  and  soft-
ware,  networked  computers,  mobile  code,  and  constantly 
changing  relationships  between  organizations,  setup  gets 
complicated. And  it ’ s  easy  to  think  up  scenarios  in which 
you  want  precise  control  over  who  can  do  what.  Features 
put  in  to  address  such  scenarios  make  setup  even  more 
complicated. 

Security  gets  in  the  way  of  other  things  you  want.  For 
software  developers,  security  interferes  with  features  and 
with  time  to market. This  leads  to  such  things as a widely 
used  protocol  for  secure  TCP/IP  connections  that  use  the 
same key  for every session as long as  the user ’ s password 
stays  the  same  [20],  or  an  endless  stream  of  buffer-
overrun errors  in privileged programs, each one making  it 
possible for an attacker to take control of the system. 
For  users  and  administrators,  security  interferes  with 
getting  work  done  conveniently,  or  in  some  cases  at  all. 
This is more important, since there are lot more users than 
developers.  Security  setup  also  takes  time,  and  it  contrib-
utes  nothing  to  useful  output.  Furthermore,  if  the  setup  is 
too  permissive  no  one  will  notice  unless  there ’ s  an  audit 
or  an  attack.  This  leads  to  such  things  as  users  whose 
password  is  their  first  name, or a  large company  in which 
more  than  half  of  the  installed  database  servers  have  a 
blank  administrator  password  [9],  or  public  access  to  da-
tabases  of  credit  card  numbers    [22,  23],  or  e-mail  clients 
that  run  attachments  containing  arbitrary  code  with  the 
user ’ s privileges [4].  
Furthermore,  the  Internet  has  made  computer  security 
much  more  difficult  than  it  used  to  be.  In  the  good  old 
days,  a  computer  system  had  a  few  dozen  users  at  most, 
all  members  of  the  same  organization.  It  ran  programs 
written  in-house  or  by  a  few  vendors.  Information  was 
moved  from one computer to another by carrying tapes or 
disks. 
Today  half  a  billion  people  all  over  the  world  are  on 
the  Internet,  including  you.  This  poses  a  big  new  set  of 
problems. 
•   Attack  from  anywhere:  Any  one  on  the  Internet  can 
take a poke at your system. 
Sharing  with  anyone:  On  the  other  hand,  you  may 
want  to  communicate  or  share  information  with  any 
other Internet user.  
•   Automated  infection:  Your  system,  if  compromised, 
can spread the harm to many others in a few seconds.  
•   Hostile  code: Code  from many  different  sources  runs 
on  your  system,  usually without  your  knowledge  if  it 
comes  from  a Web  page.  The  code  might  be  hostile, 
but  you  can ’ t  just  isolate  it,  because  you  want  it  to 
work for you.  
•   Hostile  environment:  A  mobile  device  like  a  laptop 
may  find  itself  in  a  hostile  environment  that  attacks 
its physical security.  
•   Hostile hosts: If you own information (music or mov-
ies, for example), it gets downloaded to your custom-
ers’  systems, which may be hostile and try to steal it. 

•  

 

 

2 

1.1  Real security? 

The end result should not be surprising. We don ’ t have 
“ real”   security  that  guarantees  to  stop  bad  things  from 
happening,  and  the  main  reason  is  that  people  don’ t  buy 
it.  They  don ’ t  buy  it  because  the  danger  is  small,  and  be-
cause security is a pain.  
•   Since  the  danger  is  small,  people  prefer  to  buy  fea-
tures.  A  secure  system  has  fewer  features  because  it 
has  to  be  implemented  correctly.  This  means  that  it 
takes more  time  to  build,  so  naturally  it  lacks  the  lat-
est features.  
•   Security  is  a  pain  because  it  stops  you  from  doing 
things, and you have to do work to authenticate your-
self and to set it up.  
A  secondary  reason  we  don’ t  have  “ real”   security  is 
that  systems  are  complicated,  and  therefore  both  the  code 
and  the  setup  have  bugs  that  an  attacker  can  exploit. This 
is  the  reason  that  gets  all  the  attention,  but  it  is  not  the 
heart of the problem. 
Will  things  get  better? Certainly  if  there  are  some ma-
jor  security  catastrophes,  buyers  will  change  their  priori-
ties  and  systems  will  become  more  secure.  Short  of  that, 
the  best  we  can  do  is  to  drastically  simplify  the  parts  of 
systems that have to do with security: 
•   Users  need  to  have  at  most  three  categories  for  au-
thorization:  me,  my  group  or  company,  and  the 
world.  
•   Administrators  need  to  write  policies  that  control 
security  settings  in  a  uniform  way,  since  they  can ’ t 
deal effectively with lots of individual cases. 
•   Everyone  needs  a  uniform  way  to  do  end-to-end  au-
thentication  and  authorization  across  the  entire  Inter-
net. 
Since  people  would  rather  have  features  than  security, 
most of these things are unlikely to happen. 
 On  the  other  hand,  don ’ t  forget  that  in  the  real  world 
security  depends more  on  police  than  on  locks,  so  detect-
ing  attacks,  recovering  from  them,  and  punishing  the  bad 
guys are more important than prevention. 
Section 2.3 discusses  these points  in more detail. For a 
fuller account, see Bruce Schneier ’ s recent book [19]. 

1.2  Outline 

The  next  section  gives  an  overview  of  computer  secu-
rity,  highlighting  matters  that  are  important  in  practice. 
Section  3  explains  how  to  do  Internet-wide  end-to-end 
authentication and authorization.  

2  Overview of computer security 

Like  any  computer  system,  a  secure  system  can  be 
studied under three headings:  

Specification:   What is it supposed to do? 
Implementation:   How does it do it? 
Correctness:  
Does it really work? 
In  security  they  are  called  policy,  mechanism,  and  as-
surance,  since  it ’ s  customary  to  give  new  names  to  famil-
iar concepts. Thus we have the correspondence: 
Specification 
Policy 
Implementation  Mechanism 
Correctness 
Assurance 
Assurance  is  especially  important  for  security  because 
the  system  must  withstand  malicious  attacks,  not  just  or-
dinary  use.  Deployed  systems  with  many  happy  users 
often  have  thousands  of  bugs.  This  happens  because  the 
system  enters  very  few  of  its  possible  states  during  ordi-
nary use. Attackers, of course, try to drive the system into 
states  that  they  can  exploit,  and  since  there  are  so  many 
bugs, this is usually quite easy. 
This  section  briefly  describes  the  standard  ways  of 
thinking  about  policy  and  mechanism.  It  then  discusses 
assurance  in more  detail,  since  this  is  where  security  fail-
ures occur. 

2.1  Policy: Specifying security 

Organizations  and  people  that  use  computers  can  de-
scribe  their  needs  for  information  security  under  four ma-
jor headings [15]: 
•  
Secrecy: controlling who gets to read information. 
•  
Integrity:  controlling  how  information  changes  or 
resources are used. 
•   Availability:  providing  prompt  access  to  information 
and resources. 
•   Accountability:  knowing  who  has  had  access  to  in-
formation or resources. 
They  are  usually  trying  to  protect  some  resource 
against  danger  from  an  attacker.  The  resource  is  usually 
either  information  or money.  The most  important  dangers 
are: 

 
Vandalism or sabotage that 
integrity 
— damages information 
availability 
— disrupts service 
 
Theft 
integrity 
— of money 
secrecy 
— of information 
Loss of privacy 
secrecy 
Each  user  of  computers must  decide  what  security means 
to  them.  A  description  of  the  user ’ s  needs  for  security  is 
called a security policy. 
Most  policies  include  elements  from  all  four  catego-
ries, but the emphasis varies widely. Policies for computer 
systems  are  usually  derived  from  policies  for  security  of 
systems  that  don ’ t  involve  computers.  The  military  is 
most  concerned  with  secrecy,  ordinary  businesses  with 

 

 

3 

integrity  and  accountability,  telephone  companies  with 
availability.  Obviously  integrity  is  also  important  for  na-
tional  security:  an  intruder  should  not  be  able  to  change 
the  sailing  orders  for  a  carrier,  and  certainly  not  to  cause 
the  firing  of  a missile  or  the  arming  of  a  nuclear  weapon. 
And  secrecy  is  important  in  commercial  applications: 
financial and personnel information must not be disclosed 
to  outsiders.  Nonetheless,  the  difference  in  emphasis  re-
mains [5]. 
A  security  policy  has  both  a  positive  and  negative  as-
pect.  It  might  say,  “ Company  confidential  information 
should  be  accessible  only  to  properly  authorized  employ-
ees” . This means  two  things: properly authorized employ-
ees  should  have  access  to  the  information,  and  other  peo-
ple  should  not  have  access. When  people  talk  about  secu-
rity,  the  emphasis  is  usually  on  the  negative  aspect:  keep-
ing  out  the  bad  guy.  In  practice,  however,  the  positive 
aspect  gets  more  attention,  since  too  little  access  keeps 
people  from  getting  their  work  done,  which  draws  atten-
tion  immediately,  but  too  much  access  goes  undetected 
until  there ’ s  a  security  audit  or  an  obvious  attack,2  which 
hardly  ever  happens.  This  distinction  between  talk  and 
practice is pervasive in security. 
This paper deals mostly with integrity, treating secrecy 
as  a  dual  problem.  It  has  little  to  say  about  availability, 
which  is  a  matter  of  keeping  systems  from  crashing  and 
allocating  resources  both  fairly  and  cheaply. Most  attacks 
on  availability  work  by  overloading  systems  that  do  too 
much work in deciding whether to accept a request. 

2.2  Mechanism: Implementing security 

Of  course,  one  man ’ s  policy  is  another  man’ s  mecha-
nism.  The  informal  access  policy  in  the  previous  para-
graph  must  be  elaborated  considerably  before  it  can  be 
enforced  by  a  computer  system. Both  the  set  of  confiden-
tial  information  and  the  set  of  properly  authorized  em-
ployees  must  be  described  precisely.  We  can  view  these 
descriptions as more detailed policy, or as implementation 
of the informal policy. 
In  fact,  the  implementation  of  security  has  two  parts: 
the  code  and  the  setup  or  configuration.  The  code  is  the 
programs  in  the  trusted  computing  base.  The  setup  is  all 
the  data  that  controls  the  operations  of  these  programs: 
access  control  lists,  group  memberships,  user  passwords 
or encryption keys, etc. 
The  job  of  a  security  implementation  is  to  defend 
against vulnerabilities. These take three main forms: 
1)  Bad (buggy or hostile) programs. 

                                                            
2  The  modifier  “ obvious”   is  important;  an  undetected  attack  is  much 
more dangerous, since the attacker can repeat it. Even worse, the victims 
won ’ t  know  that  they  should  take  steps  to  recover,  such  as  changing 
compromised plans or calling the police. 

2)  Bad  (careless  or  hostile)  agents,  either  programs  or 
people,  giving  bad  instructions  to  good  but  gullible 
programs. 
3)  Bad agents tapping or spoofing communications. 
Case  (2)  can  be  cascaded  through  several  levels  of  gulli-
ble agents. Clearly agents that might get  instructions  from 
bad  agents must  be  prudent,  or  even  paranoid,  rather  than 
gullible. 
Broadly speaking, there are four defensive strategies: 
1)  Keep  everybody  out.  This  is  complete  isolation.  It 
provides  the  best  security,  but  it  keeps  you  from  us-
ing  information  or  services  from  others,  and  from 
providing  them  to  others.  This  is  impractical  for  all 
but a few applications. 
2)  Keep  the  bad  guys  out.  It ’ s  all  right  for  programs 
inside  this  defense  to  be  gullible.  Code  signing  and 
firewalls do this. 
3)  Let  the  bad  guys  in,  but  keep  them  from  doing  dam-
age.  Sandboxing  does  this,  whether  the  traditional 
kind  provided  by  an  operating  system  process,  or  the 
modern  kind  in  a  Java  virtual  machine.  Sandboxing 
typically  involves  access  control  on  resources  to  de-
fine  the  holes  in  the  sandbox.  Programs  accessible 
from  the  sandbox  must  be  paranoid;  it’ s  hard  to  get 
this right. 
4)  Catch the bad guys and prosecute them. Auditing and 
police do this. 
The  well-known  access  control  model  provides  the 
framework  for  these  strategies.  In  this  model,  a  guard3 
controls  the  access  of  requests  for  service  to  valued  re-
sources, which are usually encapsulated in objects.  

 

Principal 

Do operation

Source 

Request 

monitor  
Reference 
Guard 

Object 

Resource 

Authentication 

Authorization 

 

The  guard ’ s  job  is  to  decide  whether  the  source  of  the 
request,  called  a  principal,  is  allowed  to  do  the  operation 
on the object. To decide, it uses two kinds of information: 
authentication  information  from  the  left,  which  identifies 
the  principal  who  made  the  request,  and  authorization 
information  from  the  right,  which  says  who  is  allowed  to 
do  what  to  the  object.  As  we  shall  see  in  section  3,  there 
are many ways to make this division. 
The  reason  for  separating  the  guard  from  the  object  is 
to  keep  it  simple.  If  security  is  mixed  up  with  the  rest  of 
the object ’ s  implementation,  it’ s much  harder  to be confi-
dent  that  it ’ s  right.  The  price  paid  for  this  is  that  in  gen-
eral  the  decisions must  be made  based only  on  the  princi-
pal,  the method of  the object, and perhaps  the parameters. 
                                                            
3 a  “ reference monitor”  in the jargon 

 

 

4 

For  instance,  if  you  want  a  file  system  to  enforce  quotas 
only  for  novice  users,  there  are  only  two  ways  to  do  it 
within this model: 
1)  Have  separate  methods  for  writing  with  quotas  and 
without,  and  don ’ t  authorize  novice  users  to  write 
without quotas. 
2)  Have a separate quota object that the  file system calls 
on the user ’ s behalf. 
Of  course  security  still  depends  on  the  object  to  im-
plement  its  methods  correctly.  For  instance,  if  a  file ’ s 
read method  changes  its  data,  or  the write method  fails 
to debit the quota, or either one touches data in other files, 
the system is insecure in spite of the guard. 
Another model  is  sometimes  used  when  secrecy  in  the 
face  of  bad  programs  is  a  primary  concern:  the  informa-
tion  flow  control model  [6,  13].  This  is  roughly  a  dual  of 
the  access  control  model,  in  which  the  guard  decides 
whether information can flow to a principal. 

 

Information

Source 

monitor  
Reference 
Guard 

Principal 

Sink 

 

In  either  model,  there  are  three  basic  mechanisms  for 
implementing  security. Together, they  form the gold stan-
dard for security: 
•   Authenticating  principals,  answering  the  question 
“ Who  said  that? ”   or  “ Who  is  getting  that  informa-
tion? ” .  Usually  principals  are  people,  but  they  may 
also be groups, machines, or programs. 
•   Authorizing  access,  answering  the  question  “ Who 
can do which operations on this object? ” . 
•   Auditing  the  decisions  of  the  guard,  so  that  later  it ’ s 
possible to figure out what happened and why. 

2.3  Assurance: Making security work 

The  unavoidable  price  of  reliability  is  simplicity.
(Hoare) 
 

What does it mean to make security work? The answer 
is  based  on  the  idea  of  a  trusted  computing  base  (TCB), 
the  collection  of  hardware,  software,  and  setup  informa-
tion  on  which  the  security  of  a  system  depends.  Some 
examples may help to clarify this idea. 
•  
If  the  security  policy  for  the  machines  on  a  LAN  is 
just  that  they  can  access  the  Web  but  no  other  Inter-
net  services,  and  no  inward  access  is  allowed,  then 
the  TCB  is  just  the  firewall  (hardware,  software,  and 
setup)  that allows outgoing port 80 TCP connections, 
but  no  other  traffic.4  If  the  policy  also  says  that  no 
software  downloaded  from  the  Internet  should  run, 
                                                            
4  This  assumes  that  there  are  no  connections  to  the  Internet  except 
through the firewall. 

•  

then  the  TCB  adds  the  browser  code  and  setup  that 
disables Java and other software downloads.5 
If  the  security  policy  for  a  Unix  system  is  that  users 
can  read  system  directories,  and  read  and  write  their 
home  directories,  then  the  TCB  is  roughly  the  hard-
ware,  the  Unix  kernel,  and  any  program  that  can 
write  a  system  directory  (including  any  that  runs  as 
superuser).  This  is  quite  a  lot  of  software.  It  also  in-
cludes  /etc/passwd  and  the  permissions  on  system 
and home directories. 
The  idea  of  a  TCB  is  closely  related  to  the  end-to-end 
principle  [18] —
just  as  reliability  depends  only  on  the 
ends,  security  depends  only  on  the  TCB.  In  both  cases, 
performance and availability isn ’ t guaranteed. 
In  general,  it ’ s  not  easy  to  figure  out  what  is  in  the 
TCB  for  a  given  security  policy.  Even  writing  the  specs 
for the components is hard, as the examples may suggest. 
For  security  to  work  perfectly,  the  specs  for  all  the 
TCB  components  must  be  strong  enough  to  enforce  the 
policy,  and  each  component  has  to  satisfy  its  spec.  This 
level of assurance  has  seldom been attempted. Essentially 
always,  people  settle  for  something  much  weaker  and 
accept  that  both  the  specs  and  the  implementation will  be 
flawed.  Either  way,  it  should  be  clear  that  a  smaller  TCB 
is better. 
A  good  way  to  make  defects  in  the  TCB  less  harmful 
is  to  use  defense  in  depth,  redundant  mechanisms  for  se-
curity. For example, a system might include: 
•   Network level security, using a firewall. 
•   Operating  system  security,  using  sandboxing  to  iso-
late  programs.  This  can  be  done  by  a  base  OS  like 
Windows  2000  or Unix,  or  by  a  higher-level OS  like 
a Java VM. 
•   Application  level  security  that  checks  authorization 
directly. 
The  idea  is  that  it  will  be  hard  for  an  attacker  to  simulta-
neously  exploit  flaws  in  all  the  levels.  Defense  in  depth 
offers no guarantees, but it does seem to help in practice. 
Most  discussions  of  assurance  focus  on  the  software 
(and  occasionally  the  hardware),  as  I  have  done  so  far. 
But  the  other  important  component  of  the  TCB  is  all  the 
setup  or  configuration 
information, 
the  knobs  and 
switches that tell the software what to do. In most systems 
deployed  today  there  is  a  lot  of  this  information,  as  any-
one who has run one will know. It includes: 
1)  What  software  is  installed  with  system  privileges, 
and  perhaps  what  software  is  installed  that  will  run 
with  the  user ’ s  privileges.  “ Software ”   includes  not 

                                                            
5  This  assumes  that  the  LAN  machines  don ’ t  have  any  other  software 
that might do downloads  from  the  Internet. Enforcing  this would greatly 
expand the TCB in any standard operating system known to me. 

 

 

5 

just  binaries,  but  anything  executable,  such  as  shell 
scripts or macros. 
2)  The  database  of  users,  passwords  (or  other  authenti-
cation  data),  privileges,  and  group  memberships.  Of-
ten  services  like  SQL  servers  have  their  own  user 
database. 
3)  Network  information  such  as  lists  of  trusted  ma-
chines. 
4)  The access controls on all  the  system resources:  files, 
services  (especially  those  that  respond  to  requests 
from the network), devices, etc. 
5)  Doubtless  many  other  things  that  I  haven’ t  thought 
of. 
Although  setup  is  much  simpler  than  code,  it  is  still 
complicated,  it  is  usually  done  by  less  skilled  people,  and 
while  code  is  written  once,  setup  is  different  for  every 
installation.  So  we  should  expect  that  it ’ s  usually  wrong, 
and  many  studies  confirm  this  expectation.  The  problem 
is made worse by  the  fact  that  setup must be based on  the 
documentation  for  the  software,  which  is  usually  volumi-
nous, obscure, and incomplete at best.6 See [2] for an eye-
opening  description  of  these  effects  in  the  context  of  fi-
nancial  cryptosystems,  [16]  for  an  account  of  them  in  the 
military, and [19] for many other examples. 
The  only  solution  to  this  problem  is  to  make  security 
setup much simpler, both  for administrators and  for users. 
It’ s not practical to do this by changing the base operating 
system,  both  because  changes  there  are  hard  to  carry  out, 
and  because  some  customers  will  insist  on  the  fine-
grained control it provides. Instead, take advantage of this 
fine-grained  control  by  using  it  as  a  “ machine  language ” . 
Define  a  simple  model  for  security  with  a  small  number 
of  settings,  and  then  compile  these  into  the  innumerable 
knobs and switches of the base system. 
What form should this model take?  
Users need a very simple story, with about three levels 
of  security:  me,  my  group  or  company,  and  the  world, 
with  progressively  less  authority.  Browsers  classify  the 
network  in  this  way  today.  The  corresponding  private, 
shared, and public data  should be  in  three parts of  the  file 
system:  my  documents,  shared  documents,  and  public 
documents. This combines the security of data with where 
it  is  stored,  just  as  the  physical world  does with  its  public 
bulletin  boards,  private  houses,  locked  file  cabinets,  and 
safe deposit boxes. It ’ s familiar, there ’ s  less to set up, and 
it’ s obvious what the security of each item is.  
Everything  else  should  be  handled  by  security  policies 
that  vendors  or  administrators  provide.  In  particular,  poli-
cies  should  classify  all  programs  as  trusted  or  untrusted 
based  on  how  they  are  signed,  unless  the  user  overrides 

                                                            
6  Of  course  code  is  also  based  on  documentation  for  the  programming 
language and libraries invoked, but this is usually much better done. 

them  explicitly.  Untrusted  programs  can  be  rejected  or 
sandboxed;  if  they  are  sandboxed,  they  need  to  run  in  a 
completely separate world, with separate global state such 
as  user  and  temporary  folders,  history,  web  caches,  etc. 
There should be no communication with the trusted world 
except when the user explicitly copies something by hand. 
This is a bit inconvenient, but anything else is bound to be 
unsafe. 
Administrators  still  need  a  fairly  simple  story,  but 
they need even more  the ability  to  handle many  users and 
systems  in  a  uniform  way,  since  they  can ’ t  deal  effec-
tively  with  lots  of  individual  cases.  The  way  to  do  this  is 
to  let  them  define  so-called  security  policies7,  rules  for 
security  settings  that  are  applied  automatically  to  groups 
of machines. These should say things like: 
•   Each  user  has  read/write  access  to  their  home  folder 
on a server, and no one else has this access. 
•   A  user  is  normally  a  member  of  one  workgroup, 
which  has  access  to  group  home  folders  on  all  its 
members ’  machines and on the server. 
•   System  folders  must  contain  sets  of  files  that  form  a 
vendor-approved release. 
•   All  executable  programs  must  be  signed  by  a  trusted 
authority. 
 These  policies  should  usually  be  small  variations  on 
templates  provided  and  tested  by  vendors,  since  it ’ s  too 
hard  for most  administrators  to  invent  them  from  scratch. 
It  should  be  easy  to  turn  off  backward  compatibility  with 
old  applications  and  network  nodes,  since  administrators 
can’ t deal with the security issues it causes. 
Some  customers  will  insist  on  special  cases.  This 
means  that  useful  exception  reporting  is  essential.  It 
should  be  easy  to  report  all  the  variations  from  standard 
practice  in  a  system,  especially  variations  in  the  software 
on  a  machine,  and  all  changes  from  a  previous  set  of  ex-
ceptions.  The  reports  should  be  concise,  since  long  ones 
are sure to be ignored.  
To make  the  policies  manageable,  administrators  need 
to  define  groups  of  users  (sometimes  called  “ roles” )  and 
of resources, and then state the policies concisely in terms 
of  these  groups.  Ideally,  groups  of  resources  follow  the 
file  system  structure,  but  there  need  to  be  other  ways  to 
define them  to take account of the baroque conventions in 
existing networks, OS ’ s and applications. 
The implementation of policies is usually by compiling 
them into existing security settings. This means that exist-
ing  resource  managers  don’ t  have  to  change,  and  it  also 
allows  for  both  powerful  high-level  policies  and  efficient 

                                                            
7  This  is  a  lower-level  example  of  a  security  specification,  one  that  a 
machine can understand, by contrast with the informal, high-level exam-
ples that we saw earlier. 

 

 

6 

enforcement,  just  as  compilers  allow  both  powerful  pro-
gramming languages and efficient execution. 
Developers  need  a  type-safe  language  like  Java;  this 
will  eliminate  a  lot  of  bugs.  Unfortunately,  most  of  the 
bugs  that  hurt  security are  in  system  software  that  talks  to 
the  network,  and  it  will  be  a  while  before  system  code  is 
written that way. 
They also  need a development process  that  takes  secu-
rity  seriously, valuing designs  that make assurance easier, 
getting  them  reviewed  by  security  professionals,  and  re-
fusing to ship code with serious security flaws. 

3  End-to-end access control 

Any  problem  in  computer  science  can  be  solved  with 
(Wheeler) 
another level of indirection. 

Secure  distributed  systems  need  a  way  to  handle  au-
thentication  and  authorization  uniformly  throughout  the 
Internet.  In  this  section  we  first  explain  how  security  is 
done  locally  today,  and  then  describe  the  principles  that 
underlie a uniform end-to-end scheme. 

3.1  Local access control 

•  

Most existing systems do authentication and authoriza-
tion  locally. They  have  a  local  database  for  user  authenti-
cation (usually by passwords) and group membership, and 
a  local  database  of  authorization  information,  usually  in 
the form of an access control list (ACL) on each resource.  
In these systems access control works like this: 
It’ s  assumed  that  the  channel  on  which  the  user 
communicates with  the  system  is  secure,  that  is,  only 
the user and  the  system can  send or  receive messages 
on that channel. 
•   The  system  has  a  local  database  of  user  names  and 
passwords  (or  hashes  of  passwords).  This  also  re-
cords  which  users  are  members  of  which  groups. 
Usually  it  stores  an  internal  security  identifier  (SID) 
for each user and group as well. 
•   The user authenticates the channel by sending a pass-
word  response  (some  function  of  the  password  and  a 
challenge)  to  the  system.  This  is  called  “ logging  in” . 
After  verifying  the  response  from  the  local  database, 
the system creates a process for the user, attaches it to 
the channel, and assigns the user and group SIDs to it 
as  its  identity.  If  this  process  creates  others,  they  get 
the same SIDs, or perhaps a subset. 
•   An  executable  file  (program  image)  can  also  have  an 
identity (called setuid in Unix). This means that if a 
process  is  started  up  running  this  program,  it  gets  the 
program’ s  identity  as  well  as  the  caller ’ s,  and  it  can 
switch  between  the  two  identities.  This  switching  is 
sometimes called  “ impersonation” . 

•   Each resource object has an ACL that is a list of SIDs 
along  with  the  access  each  one  is  permitted. When  a 
process calls a method of  the  object, a guard  (usually 
the  OS  kernel)  checks  that  one  of  the  SIDs  in  the 
process’   identity  is  on  the  ACL  with  the  right  access 
permission.  An  object  can  read  its  caller ’ s  identity 
and do its own access checking if it wants to. 
Operating  systems  like  Unix  and  Windows  2000  do 
security  this  way;  they  either  rely  on  physical  security  or 
luck to secure the channel to the user, or use an encrypted 
channel  protocol  like  PPTP.  The  databases  for  both  au-
thentication  (user  names,  passwords,  SIDs,  and  groups) 
and authorization (ACLs) are strictly local. 
You  might  think  that  security  on  the  web  is  more 
global  or  distributed,  but  in  fact  web  servers  work  the 
same way. They  usually  use  SSL  to  secure  the  user  chan-
nel;  this  also  authenticates  the  server ’ s  DNS  name,  but 
users  hardly  ever  pay  any  attention.  Authorization  is 
primitive,  since  usually  the  only  protected  resources  are 
the  entire  service  and  the  private  data  that  it  keeps  for 
each  user.  Each  server  farm  has  a  separate  local  user  da-
tabase. 
There is a slight extension of this strictly local scheme, 
in  which  each  system  belongs  to  a  “ domain”   and  the  au-
thentication  database  is  stored  centrally  on  a  domain  con-
troller.  The  basic  idea  is  very  simple;  the  following  de-
scription omits many details about how the bits are routed 
and how the secure messages are formatted.  
Each  system  in  the  domain  has  a  secure  channel  to  the 
controller,  implemented  by  a  shared  key  that  encrypts 
messages  between  the  two;  this  key  is  set  up  when  the 
system  joins  the  domain.  To  log  in  a  user  the  login  sys-
tem,  instead  of  doing  the  work  itself,  does  an  RPC  to  the 
controller,  passing  in  the  user ’ s  password  response.  The 
controller does exactly what the login system did by itself 
in  the  earlier  scheme,  and  returns  the  user ’ s  identity.  It 
also  returns  a  token  that  the  login  system  can  use  later  to 
reauthenticate  the  user  to  the  controller.  SIDs  are  no 
longer local to the individual system but span the domain.  
Kerberos,  Windows  NT,  and  Passport  all  work  this 
way.  In  Kerberos  the  reauthentication  token  is  confus-
ingly called a  “ ticket-granting ticket” . 
To  authenticate  the  user  to  another  system  in  the  do-
main,  the  login  system  can  ask  the  controller  to  forward 
the  authentication  to  the  target  system  (or  it  can  forward 
the password response, an old and deprecated method). In 
Kerberos  the  domain  controller  does  this  by  sending  a 
message called a  “ ticket”  to the target on the secure chan-
nel between them.8 
                                                            
8  This  means  that  the  controller  encrypts  the  ticket  with  the  key  that  it 
shares  with  the  target.  It  actually  sends  the  ticket  to  the  login  system, 
which  passes  it  on  to  the  target,  but  the  way  the  bits  are  transmitted 
doesn ’ t  affect  the  security.  The  ticket  also  enables  a  secure  channel 

 

 

7 

Authentication to another domain works the same way, 
except  that  there  is  another  level  of  indirection  through 
the  target  domain ’ s  controller.  A  shared  key  between  the 
two  domains  secures  this  channel.  The  secure  communi-
cation  is  thus  login  system  to  login  controller  to  target 
controller  to  target.  A  further  extension  organizes  the  do-
mains  in  a  tree  and  uses  this  scheme  repeatedly,  once  for 
each domain on the path through the common ancestor.  
Unless  the  domains  trust  each  other  completely,  each 
one should have its own space of SIDs and should only be 
trusted  to  assign  its  own  SIDs  to  a  user.  Otherwise  any 
domain can assign any SID to any user, so that the Micro-
soft  subsidiary in Russia can authenticate someone as Bill 
Gates.  Unfortunately  Windows  2000  inter-domain  se-
curity  today  omits  this  precaution.  See  section  3.5  for 
more on this point. 

3.2  Distributed access control 

A  distributed  system  may  involve  systems  (and  peo-
ple)  that  belong  to  different  organizations  and  are  man-
aged  differently.  To  do  access  control  cleanly  in  such  a 
system (as opposed to the  strictly local  systems of  the  last 
section) we  need  a way  to  treat  uniformly  all  the  items  of 
information  that  contribute  to  the  decision  to  grant  or 
deny access. Consider the following example: 
Alice at Intel is part of a team working on a joint Intel-
Microsoft  project  called  Atom.  She  logs  in,  using  a  smart 
card  to  authenticate  herself,  and  connects  using  SSL  to  a 
project web  page  called  Spectra  at Microsoft.   The  web 
page grants her access because: 
1)  The  request  comes  over  an  SSL  connection  secured 
with a session key KSSL. 
2)  To  authenticate  the  SSL  connection,  Alice ’ s  smart 
card  uses  her  key  KAlice  to  sign  a  response  to  a  chal-
lenge from the Microsoft server.9 
Intel certifies that KAlice is the key for Alice@Intel. 
com. 
4)  Microsoft’ s  group database  says  that Alice@Intel. 
com is in the Atom group. 
5)  The  ACL  on  the  Spectra  page  says  that  Atom  has 
read/write access. 
For brevity, we drop the .com from now on. 
To  avoid  the  need  for  the  smart  card  to  re-authenticate 
Alice  to Microsoft,  the  card  can  authenticate  a  temporary 
key  Ktemp  on  her  login  system,  and  that  key  can  authenti-
cate the login connection. (2) is then replaced by: 

3) 

                                                                                                 
between  the  target  and  the  login  system,  by  including  a new  key  shared 
between them. 
9  Saying  that  the  card  signs  with  the  public  key  KAlice  means  that  it  en-
crypts with the corresponding private key. 

2a)  Alice ’ s smart card uses her key KAlice to sign a certifi-
cate  for  the  temporary  key  Ktemp  owned  by  the  login 
system. 
2b)  Alice ’ s  login  system  authenticates  the  SSL  connec-
tion  by  using  Ktemp  to  sign  a  response  to  a  challenge 
from the Microsoft server. 
From  this  example  we  can  see  that  many  different 
kinds of information contribute to the access control deci-
sion: 
Authenticated session keys 
User passwords or public keys 
Delegations from one system to another 
Group memberships 
ACL entries. 
We want to do a number of things with this information: 
•   Keep  track  of  how  secure  channels  are  authenticated, 
whether by passwords, smart cards, or systems. 
•   Make it  secure  for Microsoft  to accept Intel’ s authen-
tication of Alice. 
•   Handle delegation of authority  to a system,  for exam-
ple, Alice ’ s login system. 
•   Handle  authorization  via  ACLs  like  the  one  on  the 
Spectra page. 
•   Record  the  reasons  for  an  access  control  decision  so 
that it can be audited later. 

3.3  Chains of responsibility 

What  is  the common element  in all  the  steps of  the ex-
ample and all the different kinds of information? From the 
example  we  can  see  that  there  is  a  chain  of  responsibility 
running  from  the  request  at  one  end  to  the  Spectra  re-
source at the other. A link of this chain has the form 
“ Principal P speaks for principal Q about subjects T. ”  
For  example,  KSSL  speaks  for  KAlice  about  everything,  and 
Atom@Microsoft  speaks  for  Spectra  about  read  and 
write. 
The  idea  of  “ speaks  for ”   is  that  if  P  says  something 
about  T,  then  Q  says  it  too.  Put  another  way,  Q  takes  re-
sponsibility for anything that P says about T. A third way, 
P  is  a  more  powerful  principal  than  Q  (at  least  with  re-
spect  to  T)  since  P ’ s  statements  are  taken  at  least  as  seri-
ously as Q ’ s, and perhaps more seriously. 
The  notion  of  principal  is  very  general,  encompassing 
any  entity  that  we  can  imagine making  statements.  In  the 
example,  secure  channels,  people,  systems,  groups,  and 
resource objects are all principals. We can think of a prin-
cipal  as  in  some  sense  equivalent  to  the  set  of  statements 
that  it  ever  makes.  A  stronger  principal  makes  more 
statements. 
The idea of  “ about subjects T ”  is that T is some way of 
describing  a  set  of  things  that  P  (and  therefore  Q)  might 
say. You can think of T as a pattern or predicate that char-

 

 

8 

acterizes  this  set  of  statements.  In  the  example,  T  is  “ all 
subjects”   except  for  step  (5),  where  it  is  “ read  and  write 
requests” .  It ’ s  up  to  the  guard  of  the  object  that  gets  the 
request  to  figure  out  whether  the  request  is  in  T,  so  the 
interpretation  of  T ’ s  encoding  can  be  local  to  the  object. 
SPKI [8] develops this idea in some detail. 
 Q  for  short,  or  P  ⇒
We  can  write  this  P  T⇒
 Q  if  T  is 
“ all  subjects” . With  this  notation  the  chain  for  the  exam-
ple is: 
 Alice@Intel  ⇒
KSSL  ⇒
 KAlice  ⇒
 Ktemp  ⇒
Atom@Microsoft r/w⇒
 Spectra 
The  picture  below  shows  how  the  chain  of  responsibility 
is  related  to  the  various  principals.  Note  that  the  “ speaks 
for ”  arrows are quite independent of the flow of bytes.  

 

 

Intel 

Microsoft 

says 

Alice@Intel 

Atom@Microsoft 

says 

KAlice 

Alice ’s 
smart card 

Ktemp 

Alice ’s login 
system  

KSSL 

Spectra 
ACL 
says 

Spectra 
web page 

 

The  remainder  of  this  section  explains  some  of  the  de-
tails of establishing the chain of responsibility. Things can 
get  a  bit  complicated;  don’ t  lose  sight  of  the  simple  idea. 
For more details see [12, 21, 8, 10]. 

3.4  Evidence for the links 

How  do  we  establish  a  link  in  the  chain,  that  is,  a  fact 
P  ⇒
 Q?  Someone,  either  the  object’ s  guard  or  a  later 
auditor,  needs  to  see  evidence  for  the  link;  we  call  this 
entity the  “ verifier ” . The evidence has the  form  “ principal 
says  delegation” ,  where  a  delegation  is  a  statement  of  the 
form  P  T⇒
 Q.10  The  principal  is  taking  responsibility  for 
the delegation. So we need to answer three questions: 
Why do we trust the principal for this delegation? 
How do we know who says the delegation? 
Why is the principal willing to say it? 
Why  trust? The  answer  to  the  first  question  is  always 
the same: We trust Q for P  ⇒
 Q, that is, we believe it if Q 
says  it. When Q  says P  T⇒
 Q, Q  is delegating  its authority 
for  T  to  P,  because  on  the  strength  of  this  statement  any-
thing  that  P  says  about  T  will  be  taken  as  something  that 
Q  says. We  believe  the  delegation  on  the  grounds  that Q, 

                                                            
10  In  [12,  21]  this  kind  of  delegation  is  called  “ handoff” ,  and  the  word 
“ delegate”  is used in a narrower sense. 

•  

as  a  responsible  adult  or  the  computer  equivalent,  should 
be allowed to delegate its authority.  
There are also  some delegations  that we  trust  uncondi-
tionally, because  they are instances of general rules called 
“ axioms” .  There  are  no  axioms  for  delegation  from  basic 
principals  like  encryption  keys.  We  discuss  compound 
principals like Alice@Intel later. 
Who  says?  The  second  question  is: How  do  we  know 
that  Q  says  P  T⇒
 Q?  The  answer  depends  on  how  Q  is 
doing the saying.  
•  
If  Q  is  a  key,  then  “ Q  says  X ”   means  that  Q  crypto-
graphically  signs  X,  and  this  is  something  that  a  pro-
gram  can  easily  verify.  This  case  applies  for  Ktemp  ⇒
 
KAlice.  If  KAlice  signs  it,  the  verifier  believes  that  KAlice 
says  it,  and  therefore  trusts  it  by  the  delegation  rule 
above. 
If Q  is  the verifier  itself,  then P  T⇒
 Q  is probably  just 
an  entry  in  a  local  database;  this  case  applies  for  an 
ACL  entry  like  Atom  ⇒
  Spectra.  The  verifier  be-
lieves its own local data. 
These  are  the  only  ways  that  the  verifier  can  directly 
know  who  said  something:  receive  it  on  a  secure  channel 
or store it locally. 
To  verify  that  any  other  principal  says  something,  the 
verifier  needs  some  reasoning  about  “ speaks  for ” .  For  a 
key  binding  like  KAlice  ⇒
  Alice@Intel,  the  verifier 
needs  a  secure  channel  to  some  principal  that  can  speak 
for  Alice@Intel.  As  we  shall  see  later,  Intel  delegate
 
Alice@Intel. So  it’ s enough  for  the verifier  to  see KAlice 
  Alice@Intel  on  a  secure  channel  from  Intel. 
Where does this channel come from? 
The  simplest  way  is  for  the  verifier  to  simply  know 
KIntel  ⇒
  Intel,  that  is,  to  have  it  wired  in.  Then  encryp-
tion  by  KIntel  forms  the  secure  channel.  Recall  that  in  our 
example  the  verifier  is  a Microsoft  web  server.  If Micro-
soft  and  Intel  establish  a  direct  relationship,  Microsoft 
will  know  Intel ’ s  public  key  KIntel,  that  is,  know  KIntel  ⇒
 
Intel. 
Of  course, we  don ’ t want  to  install KIntel  ⇒
 Intel  ex-
plicitly on every Microsoft  server.  Instead, we  install  it  in 
some Microsoft-wide directory. All the other  servers have 
secure  channels  to  the  directory  (for  instance,  they  know 
the directory’ s public key KMSDir) and trust it uncondition-
ally  to  authenticate  principals  outside  Microsoft.  Only 
KMSDir and the delegation  
“ KMSDir  ⇒
 * except *.Microsoft.com ”   
need to be installed in each server. 
The  remaining  case  in  the  example  is  the  group  mem-
bership Alice@Intel  ⇒
 Atom@Microsoft.  Just as In-
tel  delegate
  Alice@Intel,  so  Microsoft  delegate
  Atom@ 
Microsoft.  Therefore  it ’ s  Microsoft  that  should  make 
this delegation. 

 

 

9 

⇒
⇒
⇒
⇒
Why  willing?  The  third  question  is:  Why  should  a 
principal  make  a  delegation?  The  answer  varies  greatly. 
Some  facts  are  installed  manually,  like  KIntel  ⇒
  Intel  at 
Microsoft  when  the  companies  establish  a  direct  relation-
ship,  or  the  ACL  entry  Atom  r/w⇒
  Spectra. Others  follow 
from  the  properties  of  some  algorithm.  For  instance,  if  I 
run  a  Diffie-Hellman  key  exchange  protocol  that  yields  a 
fresh shared key KDH, then as long as I don ’ t disclose KDH, 
I should be willing to say  
“ KDH  ⇒
 me, provided you are  the other end of a Dif-
fie-Hellman  run  that  yielded  KDH,  you  don’ t  disclose 
KDH  to  anyone  else,  and  you  don’ t  use  KDH  to  send 
any messages yourself. ”   
In  practice  I  do  this  simply  by  signing  KDH  ⇒
  Kme;  the 
qualifiers  are  implicit  in  running  the  Diffie-Hellman  pro-
tocol.11 
For  a  very  different  example,  consider  a  server  S  start-
ing  a  process  from  an  executable  file  SQLServer71 
.exe. If S sets up a  secure channel C from this process, it 
can  safely  assert  C  ⇒
  SQLServer71.  Of  course,  only 
someone  who  trusts  S  to  run  SQLServer71  (that  is,  be-
lieves  S  ⇒
  SQLServer71)  will  believe  S ’ s  statement. 
Normally administrators set up such delegations. 
To  be  conservative,  S  might  compute  a  cryptographic 
hash  HSQL7.1  of  the  file  and  require  a  statement  from  Mi-
crosoft  saying  “ HSQL71  ⇒
  SQLServer71 ”   before  authen-
ticating  C.  There  are  three  principals  here:  the  executable 
file, the hash, and the running SQL server. Of course only 
the last actually generates requests. 

3.5  Names 

In the last section we said without explanation that In-
tel  delegate
 Alice@Intel. Why  is  this a  good convention 
to  adopt? Well,  someone  has  to  speak  for  Alice@Intel, 
or  else  we  have  to  install  facts  about  it  manually.  Who 
should  it  be?  The  obvious  answer  is  that  the  parent  of  a 
name  should  speak  for  it,  at  least  for  the  purpose  of  dele-
gating its authority. Formally, we have the axiom P delegate
 
P/N12  for  any  principal  P  and  simple  name  N.  Using  this 
repeatedly,  P  can  delegate  from  any  path  name  that  starts 
with  P.  This  is  the  whole  point  of  hierarchical  naming: 
parents have authority over children. 
The simplest  form of this  is K delegate
 K/N, where K is a 
key.  This  means  that  every  key  is  the  root  of  a  name 
space.  This  is  simple  because  you  don ’ t  need  to  install 
anything to use it. If K is a public key, it says P  ⇒
 K/N by 
signing  a  certificate  with  this  contents.  The  certificate  is 

                                                            
11 Another way, used by SSL,  is  to  send my password on  the KDH  chan-
nel.  This  is  all  right  if  I  know  from  the  other  scheme  that  the  intended 
server  is  at  the  other  end  of  the  channel.  Otherwise  I  might  be  giving 
away my password. 
12 Alice@Intel.com  is just a variant syntax for com/Intel/Alice. 

public,  and  anyone  can  verify  the  signature  and  should 
then believe P  ⇒
 K/N. 
Unfortunately, keys don’ t have any meaning to people. 
Usually  we  will  want  to  know  KIntel  ⇒
  Intel,  or  some-
thing  like  that,  so  that  from  KIntel  says  “ KAlice  ⇒
  Al-
ice@Intel ”  we can believe it. How do we establish this? 
One way, as always, is to install KIntel  ⇒
 Intel manually; 
we  saw  in  the  previous  section  that  Microsoft  might  do 
this  if  it  establishes  a  direct  relationship  with  Intel.  The 
other is to use hierarchical naming at the next level up and 
believe  KIntel  ⇒
  Intel.com  because  Kcom  says  it  and  we 
know  Kcom  ⇒ com.  Taking  one  more  step,  we  get  to  the 
root  of  the DNS  hierarchy;  secure DNS  lets  us  take  these 
steps [7].  
This  is  fine  for  everyday  use.  Indeed,  it ’ s  exactly what 
browsers  do  when  they  rely  on  Verisign  to  authenticate 
the  DNS  names  of  web  servers,  since  they  trust  Verisign 
for any DNS name.  It puts a  lot of  trust  in Verisign or  the 
DNS root, however, and if tight security is needed, people 
will  prefer  to  establish  direct  relationships  like  the  Intel-
Microsoft one.  
Why  not  always  have  direct  relationships?  They  are  a 
nuisance to manage, since each one requires exchanging a 
key in some manual way, and making some provisions for 
changing the key in case it ’ s compromised. 
Naming is a  form of multiplexing, in which a principal 
P  is  extended  to  a  whole  family  of  sub-principals  P/N1, 
P/N2, etc.; such a  family is usually called a  “ name space ” . 
Other  kinds  of  multiplexing,  like  running  several  TCP 
connections  over  a  host-to-host  connection  secured  by 
IPSec,  can  use  the  same  “ parent  delegates  to  child ”  
scheme.  The  quoting  principals  of  [10,  12]  are  another 
example  of multiplexing.  SPKI  [8]  is  the most  highly  de-
veloped form of this view of secure naming. 

3.6  Variations 

There are many variations  in  the details of  setting up a 
chain of responsibility:  
How secure channels are implemented.  
How bytes are stored and moved around.  
Who collects the evidence.  
Whether evidence is summarized.  
How big objects are and how expressive T is. 
What compound principals exist other than names. 
We  pass  over  the  complicated  details  of  how  to  use  en-
cryption  to  implement  secure  channels.  They  don’ t  affect 
the overall  system design much, and problems in this area 
are  usually  caused  by  overoptimization  or  sloppiness  [1]. 
We  touch briefly on  the other  points, each of which could 
fill a paper. 
Handling  bytes.  The  details  of  how  to  store  and  send 
around  the  bytes  that  represent  messages  are  not  directly 

 

 

10 

⇒
⇒
⇒
relevant  to  security  as  long  as  the  bytes  are  properly  en-
crypted,  but  they make  a  big  difference  to  the  system  de-
sign  and  performance.  In  analyzing  security,  it ’ s  impor-
tant  to  separate  the  secure  channels  (usually  recognizable 
by encryption at one end and decryption at the other) from 
the  ordinary  channels.  Since  the  latter  don’ t  affect  secu-
rity,  the  flow  and  storage  of  encrypted  bytes  can  be  cho-
sen to optimize simplicity, performance, or availability. 
The  most  important  point  here  is  the  difference  be-
tween  public  and  shared  key  encryption.  Public  key  al-
lows  a  secure  off-line  broadcast  channel. You  can write  a 
certificate  on  a  tightly  secured  offline  system  and  then 
store it in an untrusted  system; any number of readers can 
fetch and verify  it. To do broadcast with  shared keys,  you 
need  a  trusted  on-line  relay  system.  There’ s  nothing 
wrong with this in principle, but it may be hard to make it 
both secure and highly available. 
Contrary to popular belief, there ’ s nothing magic about 
such  certificates.  The  best  way  to  think  of  them  is  as  se-
cure  answers  to  pre-determined  queries  (for  example, 
“ What  is  Alice ’ s  key? ” ).  You  can  get  the  same  effect  by 
querying  an  on-line  database  that  maps  users  to  keys,  as 
long  as  the  database  server  is  secure  and  you  have  a  se-
cure channel to it. 
Caching  is  another  aspect  of  where  information  is 
stored.  It  can  greatly  improve  performance,  and  it  doesn’ t 
affect  security  or  availability  as  long  as  there ’ s  always  a 
way to reload the cache if gets cleared or invalidated. 
Collecting  evidence.  The  verifier  (the  guard  that’ s 
granting  access  to  an  object)  needs  to  see  the  evidence 
from  each  link  in  the  chain  of  responsibility.  There  are 
two basic approaches to collecting this information: 
Push: The  client  gathers  the  evidence  and  hands  it  to 
the object. 
Pull:  The  object  queries  the  client  and  other  data-
bases to collect the evidence it needs. 
Most  systems  use  push  for  authentication,  the  evidence 
for the identity of the client, and pull for authorization, the 
evidence  for  who  can  do  what  to  the  object.  The  security 
tokens  in  Windows  2000  are  an  example  of  push,  and 
ACLs  an  example  of  pull.  Push may  require  the  object  to 
tell  the  client  what  sort  of  evidence  it  needs;  see  [8,  10] 
for details. 
If  the  client  is  feeble,  or  if  some  authentication  infor-
mation  such  as  group  memberships  is  stored  near  the  ob-
ject, more pull may be good. Cross-domain authentication 
in  Windows  is  a  partial  example:  the  target  domain  con-
troller, rather than the login controller, discovers member-
ship in groups that are local to the target domain. 
Summarizing  evidence.  It ’ s  possible  to  replace  sev-
eral  links of a chain  like P  ⇒
 Q  ⇒
 R with a  single  link P 
  R  signed  by  someone  who  speaks  for  R.  In  the  limit  a 
link  signed  by  the  object  summarizes  the  whole  chain; 

this  is usually called a capability. The advantages are sav-
ings  in  space  and  time  to  verify, which  are  especially  im-
portant  for  feeble  objects  such  as  computers  embedded  in 
small  devices.  The  drawbacks  are  that  it ’ s  harder  to  do 
setup and to revoke access. 
Expressing sets of statements. Traditionally an object 
groups its methods into a few sets (for example, files have 
read,  write,  and  execute  methods),  permissions  for  these 
sets  of  requests  appear  on  ACLs,  and  there’ s  no  way  to 
restrict the set of statements in other delegations. SPKI [8] 
uses  “ tags”   to  define  sets  of  statements  and  can  express 
unions  and  intersections  of  sets  in  any  delegation,  so  you 
can  say  things  like  “ Alice  ⇒
  Atom  for  reads  of  files 
named *.doc and purchase orders  less  than $5000 ” . This 
example  illustrates  the  tradeoff  between  the  size  of  ob-
jects  and  the  expressiveness  of T:  instead  of  separate  per-
missions  for each .doc  file,  there ’ s a  single one  for all of 
them. 
Compound principals. We discussed named or multi-
plexed  principals  in  section  3.5.  Here  are  some  other  ex-
amples of compound principals:  
•   Conjunctions:  Alice  and  Bob.  Both  must  make  a 
statement  for  the  conjunction  to make  it. This  is  very 
important  for  commercial  security,  where  it ’ s  called 
“ separation  of  duty”   and  is  intended  to  make  insider 
fraud harder by  forcing  two  insiders  to collude. SPKI 
has  a  generalization  to  threshold  principals  or  “ k  out 
of n ”  [8]. 
•   Disjunctions:  Alice  or  FlakyProgram.  An  object 
must  grant  access  to  both  for  this  principal  to  get  it. 
In  Windows  2000  this  is  a  “ restricted  token”   that 
makes  it  safer  for Alice  to  run a  flaky program,  since 
a  process  with  this  identity  can  only  touch  objects 
that explicitly grant access to FlakyProgram, not all 
the objects that Alice can access. 
Each  kind  of  compound  principal  has  axioms  that  de-
fine who can speak for it. See [12] for other examples. 

3.7  Auditing 

As  is  typical  for  computer  security,  we  have  focused 
on  how  end-to-end  access  control  works  and  the  wonder-
ful  things  you  can  do  with  it.  An  equally  important  prop-
erty,  though,  is  that  the  chain  of  responsibility  collects  in 
one  place,  and  in  an  explicit  form,  all  the  evidence  and 
rules  that  go  into making  an  access  control  decision. You 
can  think  of  this  as  a  proof  for  the  decision.  If  the  guard 
records  the  proof  in  a  reasonably  tamper-resistant  log,  an 
auditor can  review  it  later  to establish accountability or  to 
figure  out  whether  some  unintended  access  was  granted, 
and why. 
Since  detection  and  punishment  is  the  primary  instru-
ment of practical security, this is extremely important. 

 

 

11 

⇒
4  Conclusion 

We have outlined the basic ideas of computer security: 
secrecy, integrity, and availability, implemented by access 
control  based  on  the  gold  standard  of  authentication,  au-
thorization, and auditing. We discussed the reasons why it 
doesn’ t work very well in practice: 
•   Reliance on prevention rather than detection and pun-
ishment. 
•   Complexity  in  the code and especially  in  the  setup of 
security, which overwhelms users and administrators. 
We gave some ways to reduce this complexity. 
Then  we  explained  how  to  do  access  control  end-to-
end  in  a  system  with  no  central management,  by  building 
a  chain  of  responsibility  based  on  the  “ speaks  for ”   rela-
tion between principals. Each link in the chain is a delega-
tion  of  the  form  “ Alice@Intel  speaks  for  Atom.Mi-
crosoft  about  reads  and  writes  of  files  named  *.doc” . 
The  right  principal  (Microsoft  for  this  example)  has  to 
assert  each  link,  using  a  secure  channel.  Every  kind  of 
authentication  and  authorization  information  fits  into  this 
framework:  encrypted  channels,  user  passwords,  groups, 
setuid programs, and ACL entries. The chain of respon-
sibility  is  a  sound  basis  for  logging  and  auditing  access 
control decisions.  
Principals  with  hierarchical  names  are  especially  im-
portant. A parent can delegate for all of its children. Root-
ing  name  spaces  in  keys  avoids  any  need  for  a  globally 
trusted root. 
There are many ways to vary  the basic scheme: how to 
store  and  transmit  bytes,  how  to  collect  and  summarize 
evidence  for  links,  how  to  express  sets  of  statements,  and 
what is the structure of compound principals. 

References 

1.  Abadi and Needham, Prudent engineering practice for cryp-
tographic protocols.  IEEE Trans. Software Engineering 22, 
1  (Jan  1996),  2-15,  dlib.computer.org/ts/books/ts1996/pdf/ 
e0006.pdf  or  gatekeeper.dec.com/pub/DEC/SRC/research-
reports/abstracts/src-rr-25.html 
2.  Anderson,  Why  cryptosystems  fail.  Comm.  ACM  37,  11 
(Nov.  1994),  32-40, www.acm.org/pubs/citations/  proceed-
ings/commsec/168588/p215-anderson 
3.  Bell and LaPadula, Secure computer  systems. ESD-TR-73-
278  (Vol.  I-III)  (also  Mitre  TR-2547),  Mitre  Corporation, 
Bedford, MA, April 1974 
4.  CERT  Coordination  Center,  CERT  advisory  CA-2000-04 
Love  Letter  Worm,  www.cert.org/advisories/CA-2000-
04.html 
5.  Clark  and  Wilson,  A  comparison  of  commercial  and  mili-
tary  computer  security  policies.  IEEE  Symp.  Security  and 
Privacy (April 1987), 184-194 
6.  Denning,  A  lattice  model  of  secure  information  flow. 
Comm. ACM 19, 5 (May 1976), 236-243 

7.  Eastlake  and  Kaufman,  Domain  Name  System  Security 
Extensions,  Jan.  1997,  Internet  RFC  2065,  www.faqs.org/ 
rfcs/rfc2065.html 
8.  Ellison  et  al.,  SPKI  Certificate  Theory,  Oct.  1999,  Internet 
RFC 2693, www.faqs.org/rfcs/rfc2693.html 
9.  Gray, J., personal communication 
10.  Howell  and  Kotz,  End-to-end  authorization,  4th  Usenix 
Symp. Operating  Systems Design  and  Implementation,  San 
Diego,  Oct.  2000,  www.usenix.org/publications/library/ 
proceedings/osdi00/howell.html 
11.  Lampson,  Protection.  ACM  Operating  Systems  Rev.  8,  1 
(Jan.  1974),  18-24,  research.microsoft.com/lampson/09-
Protection/Abstract.html 
12.  Lampson  et  al,  Authentication  in  distributed  systems:  The-
ory  and  practice.  ACM  Trans.  Computer  Systems  10,  4 
(Nov.  1992),  pp  265-310,  www.acm.org/pubs/citations/ 
journals/tocs/1992-10-4/p265-lampson 
13.  Myers  and  Liskov,  A  decentralized  model  for  information 
flow  control,  Proc.  16th  ACM  Symp.  Operating  Systems 
Principles, Saint-Malo, Oct. 1997, 129-142, www.acm.org/ 
pubs/citations/proceedings/ops/268998/p129-myers 
14.  Rivest, Shamir, and Adleman. A method for obtaining digi-
tal  signatures  and  public-key  cryptosystems.  Comm.  ACM 
21,  2  (Feb.,  1978),  120-126,  theory.lcs.mit.edu/~rivest/ 
rsapaper.ps 
15.  National  Research  Council,  Computers  at  Risk:  Safe  Com-
puting  in  the  Information  Age.  National  Academy  Press, 
Washington D.C., 1991, books.nap.edu/catalog/1581.html 
16.  National  Research  Council,  Realizing  the  Potential  of  C4I: 
Fundamental  Challenges.  National  Academy  Press, Wash-
ington D.C., 1999, books.nap.edu/catalog/6457.html 
17.  Saltzer, Protection and the control of information sharing in 
Multics. Comm. ACM 17, 7 (July 1974), 388-402 
18.  Saltzer et al., End-to-end arguments in system design. ACM 
Trans.  Computer  Systems  2,  4  (Nov.  1984),  277-288, web. 
mit.edu/Saltzer/www/publications/endtoend/endtoend.pdf 
19.  Schneier, Secrets and Lies: Digital Security in a Networked 
World, Wiley, 2000. 
20.  Schneier  and  Mudge,  Cryptanalysis  of  Microsoft ’ s  point-
to-point  tunneling  protocol.  5th  ACM  Conf.  Computer  and 
Communications  Security,  San  Francisco,  1998,  132-141, 
www.acm.org/pubs/citations/proceedings/commsec/ 
288090/p132-schneier 
21.  Wobber et al., Authentication in the Taos operating system. 
ACM Trans. Computer Systems 12, 1 (Feb. 1994), pp 3-32, 
www.acm.org/pubs/citations/journals/tocs/1994-12-1/p3-
wobber 
22.  ZDNet, Stealing credit cards from babies. ZDNet News, 12 
Jan. 
2000,  www.zdnet.com/zdnn/stories/news/0,4586, 
2421377,00.html 
23.  ZDNet,  Major  online  credit  card  theft  exposed.  ZDNet 
News,  17 Mar.  2000, www.zdnet.com/zdnn/stories/news/0, 
4586,2469820,00.html 

 

 

12 

Discovering Dependencies for Network Management
Paramvir Bahl, Paul Barham, Richard Black, Ranveer Chandra, Moises Goldszmidt,
Rebecca Isaacs, Srikanth Kandula† , Lun Li‡ , John MacCormick, David A. Maltz, Richard Mor tier,
Mike Wawrzoniak¶ , Ming Zhang.
Microsoft Research; also ‡ Caltech, † MIT and, ¶ Princeton.

This paper presents the Leslie Graph, a simple yet powerful ab-
straction describing the complex dependencies between network,
host and application components in modern networked systems. It
discusses challenges in the discovery of Leslie Graphs, their uses,
and describes two alternate approaches to their discovery, sup-
ported by some initial feasibility results.

1 Introduction
It is lamentable that Leslie Lamport’s famous quote [9] “A
distributed system is one in which the failure of a com-
puter you didn’t even know existed can render your own
computer unusable ” describes a scenario familiar to almost
every computer user. As IT systems are increasingly distrib-
uted, it is not only the clients and servers themselves that can
render a computer useless for an afternoon, but any of the
many routers, links and network services also involved.
In distributed systems, the underlying problem is the ab-
sence of tools to identify the components that “can render
your own computer unusable ”: the implicit web of depen-
dencies among these components exists only in the minds
of the human experts running them. The complexity of
these dependencies quickly adds up, requiring more help
than traditional IT management software provides. Listing
the contents of a single DFS 1 directory, for example, can
involve a minimum of three hosts and eight network ser-
vices (WINS, ICMP Echo, SMB, DFS, DNS, Kerberos, ISA
key exchange, ARP). Existing management solutions focus
on network elements, topology discovery, or particular ser-
vices, but what is needed are tools to manage and improve
the user’s end-to-end experience of networked applications.
In deference to Lamport, this paper de ﬁnes the Leslie
Graph as the graph representing the dependencies between
the system components, with subgraphs representing the de-
pendencies pertaining to a particular application or activ-
ity. Nodes represent the computers, routers and services on
which user activities rely, and directed edges capture their
inter-dependencies. Different versions of a Leslie Graph can
express different granularities of dependence for an activ-
ity — for some analyses, an Leslie Graph capturing inter-
machine dependences at the granularity of IP addresses
might be sufﬁcient, while for others an Leslie Graph captur-
ing inter-service dependencies at the granularity of software
processes might be desirable.
This paper makes three contributions: (i) we de ﬁne Leslie
Graphs and discuss the challenges in ﬁnding them, ( ii) we
suggest important problems that Leslie Graphs could help

1Windows Distributed File System

solve, and (iii) we describe two ongoing projects that are
exploring different approaches to automatically infer the
Leslie Graphs.

1.1 Existing Approaches
It might seem that the Leslie Graph for an application could
easily be constructed if its designer generated rules that spell
out the application’s dependencies.
Indeed, a number of
commercial products such as MAM2 and the DSI “System
3 do just this. However, this approach has
De ﬁnition Model”
several problems: the system could evolve faster than the
rules; deployment of various forms of middlebox (e.g., ﬁre-
walls, proxies) can change the application’s dependencies
without the rule writers even being aware; and rules are un-
available for legacy systems.
Similarly, analysis of con ﬁguration ﬁles to determine the
Leslie Graph is insufﬁcient as many dependencies among
components are dynamically constructed. For example,
web browsers on enterprise networks are often con ﬁgured
to communicate through a proxy, sometimes named in the
browser preferences but frequently contacted through au-
tomatic proxy-discovery protocols that themselves rely on
resolution of well-known names.
Systems have been proposed to expose dependencies by
requiring all applications to run on a middleware platform
instrumented to track dependencies at run-time [1, 4, 7].
However, heterogeneity defeats most such efforts in prac-
tice. Networks run a plethora of platforms, operating sys-
tems, and applications, often from a wide range of vendors.
While a single vendor might instrument their software, it
is unlikely that all vendors will do so in a common fash-
ion; similarly, building all distributed applications over a
single common middleware platform is infeasible. Further-
more, many underlying services on which others depend are
legacy services and cannot easily be instrumented or ported
to run over an instrumented layer.

1.2 Challenges Finding the Leslie Graph
In contrast to the above approaches, but also without ex-
plicitly de ﬁning some notion of a Leslie Graph, others have
argued that a promising approach to inferring dependencies
is to observe externally visible behavior of system compo-
nents without parsing the contents of packets they send —the
“black-box ” approach [2, 13]. We follow this general ap-
proach, relying mainly on correlation of observed network

2 http://www.mercury.com/us/products/business-availability-
center/application-mapping/
3 http://www.microsoft.com/windowsserversystem/dsi/sdm.mspx

1

trafﬁc to infer system dependencies, and augmenting as re-
quired with other techniques such as active probing. How-
ever, there are several challenges with this approach.

False positives. The Leslie Graph is expressed in terms
of dependency between components, which requires under-
standing their causality. However, using observed trafﬁc re-
sults in measuring their correlation, which is not the same.
For example, it is perfectly possible for unrelated conversa-
tions, such as periodic background maintenance trafﬁc, to
exhibit misleading timing correlations.

False negatives (caching). Statistical correlations re-
quire a substantial number of observations in the presence of
noise (unrelated background trafﬁc). However, much crit-
ical control plane and session setup behavior occurs rela-
tively rarely. For example, it is difﬁcult to determine that
a web-browser’s use of HTTP depends on both DNS and
ARP through trafﬁc observation alone, as the services are
typically invoked once and the results cached.

Granularity. Many modern IT deployments use clusters
of servers to implement load-balancing and resilience for
critical tasks. Alternatively, in smaller systems multiple ser-
vices will be hosted as separate processes on a single server.
Thus, a vertex of the Leslie Graph might need to represent
other than a single computer: at some times an entire cluster
of computers will be appropriate, at others a single process
on a single computer.

Complexity. Enterprise networks use a wide variety of
applications [11], including complex services like authen-
tication (Active Directory, IPSEC, Kerberos, RADIUS), re-
mote ﬁle systems (AFS, DFS, NFS, SMB), web applications
(Sharepoint, Wikis), communications (VoIP, IM, email) and
utilities (printing, DHCP, ARP). The inter-dependencies be-
tween these are extensive and poorly speciﬁed.

Trust. To compute the Leslie Graph hosts must share in-
formation about their activities and are expected to do so
truthfully. In an enterprise network this trust can be estab-
lished and enforced by the company’s network policy and
administration procedures.

We restrict our subsequent discussion to discovery of
Leslie Graphs in enterprise networks precisely because the
latter two challenges, complexity and trust, make enterprise
networks both a useful and feasible place to do so. We as-
sume that we can place agents on a reasonable fraction of
the computers on the network to monitor packets sent and
received. These agents can also be used for tomography:
taking measurements and enabling probing from many van-
tage points to discover network topology and resolve ambi-
guities in the Leslie Graph.

2 Leslie Graphs and Their Uses
Studies show that ∼ 70% of enterprise IT budgets are spent
on maintenance.4 The ability to create an enterprise’s Leslie
Graph could have a major ﬁnancial impact by enabling the
following techniques for management and troubleshooting.
Fault localization. A common source of frustration for
users is when an application temporarily hangs for no read-
ily apparent reason. The hardest part of resolving such prob-
lems is often locating the problem in the ﬁrst place. Is it
in an overloaded server? A policy con ﬁguration? A failed
router or link? The Leslie Graph for an application not only
summarizes the components that are involved, but also al-
lows information from multiple clients to be combined to
pinpoint faults through tomography.
Reconﬁguration planning. A classic tale of unexpected
consequences [10] involves an old machine con ﬁgured to
backup an SQL database. Since the dependency of the pri-
mary server on this old machine for backup service was not
explicit, operators re-imaged and recycled the old machine.
Unfortunately, the primary server failed around the same
time, and the database was completely lost. Companies
are continually adding, reorganizing, or consolidating ser-
vices. Frequently, changes are disruptive to services beyond
those directly involved due to unexpected and previously
hidden interactions. Planning these changes and diagnosing
the problems that inevitably result is expensive.
Leslie Graphs can be expected to help in two ways. First,
by automatically detecting dependencies, unexpected con-
sequences can be identiﬁed in advance and planned for. Sec-
ond, Leslie Graphs allow IT departments to warn ahead of
time the users who will be affected by changes.
Helpdesk optimization. The fact that many users are ac-
tive at the same time means that failures are likely to result
in many calls to the helpdesk — initiating a new diagnostic
effort for each call would be wasteful. Knowing the depen-
dencies among components means that new reports can be
rapidly chained to the trouble ticket of a known issue: elim-
inating time spent investigating dependent issues.
It also
reduces the likelihood of inappropriate remediation such as
unnecessarily rebooting the user’s computer, and it helps to
prioritize trouble tickets by the numbers of users affected.
Anomaly detection. If Leslie Graphs are automatically
constructed based on the observed behavior of hosts, anom-
alies and changes in the graphs point to hosts that are worthy
of more detailed human investigation. For instance, differ-
ences between clients can be used to ﬁnd policy issues. If a
set of clients cannot reach a server while everything is ﬁne
for another set of clients, our algorithms will localize the
problem to the clients. The structure of the Leslie Graph
can then help guide a human to determine if the cause is a
middlebox/ﬁrewall common among the clients or a policy
(e.g., IPSEC) on the clients themselves.
4 Forrester Research, “Governing IT in the enterprise” (July
http://research.microsoft.com/events/snmsummit

2004) and

2

3 Implementation Considerations
We are exploring two different approaches to approximating
the Leslie Graph using low-level packet correlations. The
Constellation system uses a distributed approach, reactively
constructing the Leslie Graph of any node on-demand. In
contrast, the AND system, which stands for Analysis of Net-
work Dependencies, proactively maintains the approximate
Leslie Graph at a centralized inference engine. The rest of
this section describes these systems in more detail.

3.1 Constellation
In the Constellation system, local trafﬁc correlations are in-
ferred by passively monitoring packets and applying ma-
chine learning techniques. The basic premise is that a typ-
ical pattern of messages is associated with accomplishing
a given task. Therefore, it is possible to approximate the
Leslie Graph by taking the transitive closure of strongly cor-
related nodes, and furthermore we can detect or diagnose
faults by observing the absence of expected messages.
In order to explore the class of machine learning ap-
proaches that are applicable, we have formalized the prob-
lem. Space precludes a complete presentation, but the fol-
lowing three concepts are critical:
Channel. A channel represents the entities between
which messages ﬂow and thus between which an edge exists
in the Leslie Graph. For example, all packets sharing the
same source and destination address might be designated
as belonging to a single channel. Alternatively, at a ﬁner
granularity we might additionally use application protocol
to identify a channel. Channels are described as input or
output channels based on whether they represent messages
received at or transmitted by a host.
Activity pattern. We assign a value of either active or
inactive to each channel in the network over some ﬁxed time
window. A set of such assignments to channels at a node is
an activity pattern for that node, indicating whether or not a
packet was observed on each channel during the observation
time window.
Activity model. The activity model for a node is a func-
tion mapping the activity pattern of the input channels to a
vector of probabilities for each output channel being active.
The idea is that by repeatedly observing whether an out-
put channel is active for a given input activity pattern, we
can learn the activity model on a host. To do this we are
investigating a number of alternative mechanisms including
Naive Bayes Classiﬁers [8] and Noisy-OR models [12]. Our
results so far show promise, but have also highlighted some
of the inherent trade-offs for this approach. Since activity
patterns discard all packet timings and counts within the
observation window, picking a suitable duration for the win-
dow is critical. Over a very long time window we will learn
that all channels are related, whereas selecting a window
size that is too small will cause correlations to be missed.

We are tackling this problem by building activity models si-
multaneously for a range of window size and working on
good ways to combine the resulting models.
Constellation uses activity models on hosts to approxi-
mate Leslie Graphs in a completely distributed manner. The
correlation coefﬁcients in the activity model encode the co n-
ﬁdence level for a dependency between two nodes. When a
host wishes to learn its Leslie Graph for a particular service,
it queries its relevant peers to ﬁnd strong next-hop correla -
tions in their activity models for when only the input chan-
nel on which the query was sent is active. This query is
then forwarded to those peers who repeat the process, and
the resulting transitive correlations combine to give a Leslie
Graph from the point of view of the local host. When the
Leslie Graph is large this has the advantage that we can or-
der the search by “most likely ” path. Leslie Graphs are gen-
erated on-demand and give a snapshot of recent history at
each member host.
One of the challenges when combining local activity
models to form a Leslie Graph is choosing an appropriate
threshold for deciding that a correlation is strong enough to
be part of the graph. At some correlation value for a given
edge there is insufﬁcient evidence to assume a causal rela-
tionship, and so the edge should be excluded. We are cur-
rently investigating this and several other issues, including
statistical hypothesis tests for detecting both anomalous and
normal changes to an activity model.

3.2 AND
The AND system consists of a centralized inference engine
and a set of agents, one running on each desktop and server.
Each agent performs temporal correlation of the packets
sent and received by its host and makes summarized in-
formation available to the engine. The inference engine
serves as an aggregation and coordination point: assembling
the Leslie Graph for applications by combining information
from the agents; ordering agents to conduct active probing
as needed to ﬂesh out the Leslie Graph or to localize faults;
and interfacing with the human network managers.
Computing the Leslie Graph. Using the terminology of
Section 3.1, we de ﬁne a channel as a 3 tuple of [RemoteIP,
RemotePort, Protocol]. Each agent then continuously up-
dates a matrix of the frequency with which two channels are
active within a 100 ms window. 5
To construct the Leslie Graph, the inference engine polls
the agents for their matrices. Figure 1 illustrates how ag-
gregating matrices from multiple agents over a long period
of time can ﬁnd dependencies that might be obscured by
caching, since even infrequent messages to a server become
measurable when summed over many hosts. For example,
many hosts will have a matrix similar to H 3’s that shows

5We have found values from 100 ms to 1 s produce the same depen-
dency graphs on clients, but servers that are heavily loaded may cause de-
pendences to be spread over a larger time window.

3

 
Figure 1: Part of a Leslie Graph discovered by AND when clients access
some web server. The dashed line indicates a dependency found by aggre-
gating information across hosts, H 1, H 2, H 3.
a strong dependence on the web server, but no dependence
on DNS as the web server’s address has been cached. How-
ever, the matrices for H 1 and H 2 show that when these hosts
communicated with the web server they also communicated
with DNS in the same 100 ms window. If enough hosts that
communicate on channel A (e.g., the web server) also com-
municate on channel B (e.g., DNS) within the same 100 ms,
then the engine infers that any host depending on A most
likely depends on B as well and will add to the Leslie Graph
a dependency on B, as shown by the dashed line in the ﬁg-
ure. Each edge in the Leslie Graph also has a weight, which
is the probability with which it actually occurs in a transac-
tion. In Figure 1, for example, H 1 contacts the DNS server
20% of the time before it accesses the web server.
Networks that include either fail-over or load-balancing
clusters of servers (e.g., primary/secondary DNS servers,
web server clusters) are modeled by introducing a meta
node into the Leslie Graph to represent each cluster, for ex-
ample, the DNS Service node in Figure 1. Currently we use
heuristics based on DNS names, port numbers, and stem-
ming URLs to identify clusters and leave automatic detec-
tion of cluster con ﬁgurations for future work.
In addition to user machines and application servers,
AND extends the Leslie Graph by populating it with net-
work elements, such as routers, switches and physical links.
This broadens the applications of the Leslie Graph as,
e.g., link congestion faults can now be localized. We can
map the layer-2 topology by using the agents to send and
listen for ﬂooded MAC packets as in [5], and the layer-3
topology using traceroutes. Other techniques [6] could be
used if SNMP data is available.
Using the Leslie Graph. Of the scenarios described in
Section 2, our current focus is on efﬁcient fault localiza-
tion. Each agent observes the experiences of its own host
(e.g., measuring the response time between requests and
replies). When a user on the host ﬂags the experience as
bad, the agent sends a triggered experience report to the
inference engine. For example, a negative experience re-
port might be generated when a user restarts their browser
or hits a button that means “I’m unhappy now ”, or when
automated parsing identiﬁes that something wrong has hap-
pened (e.g., too many “invalid page ” HTTP return codes).

4

A small number of randomly selected positive experiences
(e.g., the time to load a web page when the user did not
complain) are sent to the engine every 300 s.
The engine batches experience reports from multiple
agents and applies Bayesian inference to ﬁnd the most plau-
sible explanation for the experience reports (i.e., the min-
imum set of faulty physical components that would afﬂict
all the hosts, routers and links with poor performance while
leaving unaffected the components experiencing acceptable
performance). Space prevents a full description, but al-
though Bayesian models typically require training, initial
results (Section 4) show the structure of the Leslie Graph
and the number of viewpoints provided by agents cause the
results to have little sensitivity to the training process.
Scalability. The use of a centralized inference engine
clearly makes it easier to aggregate information, but it raises
scalability concerns about CPU and bandwidth limitations.
On a single CPU, our system localizes faults on a Leslie
Graph of 160 nodes (see Section 4 for details on the exper-
iment setup) within 200 ms, with the time growing linearly
in the number of nodes in the Leslie Graph.
Back-of-the-envelope calculations show the bandwidth
requirements are feasible even for large enterprise networks.
Experience reports are about 100 B and are sent to the in-
ference engine every 300 s by each agent. The full co-
occurrence matrix is polled from each agent every 3600 s.
Most hosts in our network use fewer than 100 channels
(i.e., use < 100 servers), so the matrix is less than 100x100
ﬂoats. Even for an extremely large enterprise network with
O(100,000) computers and O(10,000) routers/switches, this
results in an average bandwidth of only 10 Mbps. Busy
servers have much more than 100 channels, but compres-
sion can be used if needed.

3.3 Discussion
As two separate projects, Constellation and AND are ex-
ploring two different points in the design space of ap-
proximating Leslie Graphs. While both approaches com-
pute Leslie Graphs by aggregating the activities of multi-
ple nodes, their differences highlight how the overall design
space can be broken down into three axes, namely timing,
structure, and granularity.
The ﬁrst axis in the construction of a Leslie Graph is the
time when it is constructed. Constellation constructs the
Leslie Graph reactively, while AND proactively maintains
it at the inference engine. If the Leslie Graph is constructed
reactively, it imposes little overhead on hosts. However,
since nodes log packets over a short period of time, a re-
active scheme might miss out on dependencies affected by
cached state. For example, in Figure 1, a reactive scheme
might not determine the dependency between H3 and DNS.
Furthermore, by proactively maintaining the Leslie Graph,
the inference engine can respond to faults even before they
are detected by all the users.

The second axis is the structure of the system, i.e. whether
the Leslie Graph computation is centralized or distributed.
Constellation uses a distributed approach to compute the
Leslie Graph, while AND computes it at the centralized
inference engine. A distributed (unstructured) approach is
more robust to network and machine failures that might af-
fect connectivity to the centralized server, while a central-
ized approach is simpler and easier to manage. We are im-
proving the fault tolerance of AND by implementing the in-
ference engine as a distributed cluster of machines.
The third axis is the granularity of the Leslie Graph, as
discussed in Section 1.2. The nodes in the Leslie Graph
could be a cluster of servers, a particular machine or a
process on a machine. Similarly, for network elements, a
node could be end-to-end connectivity between machines,
or all the routers and switches in the path. The analysis
on a more granular Leslie Graph will be more precise, al-
though it might add complexity to the algorithm and unnec-
essary detail to the results. Constellation represents hosts
and processes in the Leslie Graph, while AND also includes
the routers and switches.
A notable trend is the increasing popularity of peer-to-
peer applications. These are designed to achieve reliability
by dynamically changing the set of servers with which a
client communicates based on the content being exchanged
or congestion levels in the system. As a result, the Leslie
Graph is not stable across long time-periods. A system
like Constellation, with its on-demand creation of the Leslie
Graph using only recent observations, will report the set of
peers currently in use. AND, which aggregates information
across time, may show a dependency on servers no longer
in use.
Our schemes do have limitations. We do not expect our
techniques to ﬁnd servers that return incorrect answers un-
less these errors lead to performance or fail-stop problems.
For example, if a DNS server holds the wrong IP address
for a name and only one client looks up the name, our ap-
proach will help only if the Leslie Graph changes as a result.
Even if many clients lookup the wrong IP address and thus
are unable to establish a connection, our fault localization
algorithm will only point to the clients — although human
examination of the Leslie Graph would reveal the affected
clients share a DNS server. Here our tools and the structure
of the Leslie Graph may help human investigators, even if
they cannot automatically ﬁnd the root cause.

4 Initial Results
Existence of correlations. The ﬁrst step in validating
our approach is determining if there is detectable correlation
between input channels and output channels that are known
to be related, and no correlation between unrelated chan-
nels. If this were not true, then black-box techniques would
be infeasible. Figure 2 shows the results of plotting the time

5

0.12

0.1

0.08

0.06

0.04

0.02

s
n
o
i
t
a
v
r
e
s
b
o
 
f
o
 
n
o
i
t
r
o
p
o
r
P

 

EPM vs NetLogon
EPM vs uniform
NetLogon vs uniform

0.06

0.05

0.04

0.03

0.02

0.01

s
n
o
i
t
a
v
r
e
s
b
o
 
f
o
 
n
o
i
t
r
o
p
o
r
P

SMB vs EPM
EPM vs uniform
SMB vs uniform

 

0
 
103

104
106
105
Time between packets (us)

0
 
103

104
106
105
Time between packets (us)

Figure 2: Evaluation of correlation between EPM packets & NetLogon
packets (left) and EPM packets & SMB packets (right), using correlation
with random noise as a control. EPM vs. NetLogon is signiﬁcan tly dif-
ferent from the control correctly validating their correlation while EPM vs.
SMB is indistinguishable from the control.

dns/wins
prxy
prn

 1

 0.8

 0.6

 0.4

 0.2

 0

t
n
e
d
n
e
p
e
D
 
s
q
e
R
 
n
o
i
t
c
a
r
F

 10
 40
 50
 20
 30
client ID
	

	
 
 
 
Figure 3: Example of ﬁnding dependencies of 53 hosts that contact an
internal webserver. The ﬁgure on top shows the probabilisti c dependen-
cies discovered at each client. The ﬁgure below graphically represents the
typical dependencies and also illustrates a false-dependency.
difference between receiving a packet of one protocol and
sending a packet of the other protocol for three protocols:
EPM (the RPC portmapper), NetLogon and SMB. Traces
were collected for 1 hour from a busy server. The time dif-
ferences are also computed against a packet stream whose
timestamps are drawn from a uniform random distribution,
representing background noise.
The right ﬁgure shows that SMB and EPM correlate as
strongly with the random packet stream as they do with
each other, implying they are not correlated. This is cor-
rect, as SMB and EPM are unrelated protocols. The left
ﬁgure shows EPM and NetLogon have a very different dis-
tribution than the comparison with the random stream, im-
plying EPM and NetLogon are closely related — in fact,
NetLogon clients use EPM to locate the port to which they
send their requests. We obtained similar validation for many
other protocols, implying that packet-correlation techniques
are feasible.
Finding dependencies. As a ﬁrst test of AND’s tech-
nique for ﬁnding dependencies in presence of caching, we
ran the Leslie Graph generation algorithm against data from
53 hosts collected over one hour. Figure 3 shows the frac-
tion of requests made by each client to the DNS, proxy,
and PrintServer that co-occur with a request to a common
webserver, called MSweb. As we can see, most clients in-
voke DNS when making web requests, although not 100%
of the time due to caching. However, we still extract the

correct dependency. The data also show some clients are
dependent on the proxy that is normally used for external
access, even when accessing the internal web server.
In-
vestigation showed that these clients were miscon ﬁgured
with an out-of-date list of internal names, indicating how
our approach can be useful for detecting some classes of
policy/con ﬁguration faults. Two misbehaving hosts made
so many requests to the PrintServer that their dependencies
for MSWeb became abnormal, showing that even false pos-
itives can yield valuable management information.
Usefulness of the Leslie Graph. To evaluate the ability
of AND to ﬁnd and use the Leslie Graph for fault local-
ization, we have created a testbed with 23 clients that are
evenly divided between two subnets connected by a router.
Each subnet has a web server running Sharepoint (a wiki-
like application), with data for the web sites stored on a sin-
gle SQL database server on one of the subnets. Network ser-
vices (DHCP, authentication servers, DNS) are connected to
the subnet without the SQL server. Using packet-droppers,
rate-shapers, and load generators we can deterministically
create scenarios where any desired subset of the clients,
servers, router, and links appears as failed or overloaded.
We evaluated ﬁve scenarios where combinations of one
or more web servers, SQL server, routers, and links were set
to an overloaded or failed state while robots on the clients
made accesses to the web servers and each agent observed
the response times seen by its client. These scenarios have
Leslie Graphs with about 160 nodes, each of which is a
component that could potentially fail. A small portion of
the Leslie Graph for the testbed is shown in Figure 1. In all
ﬁve scenarios, our fault localization algorithms run over t he
Leslie Graph correctly determined the problematic compo-
nent. In three scenarios, the algorithm reported one more
potentially problematic candidate than the number actually
afﬂicted, but the algorithm also proposed the correct set of
active probing tests to resolve this ambiguity.

5 Related Work
Project5 [2] proposes ﬁnding performance bottlenecks in
a distributed system by using black-box approaches to
track requests as they move between servers in the system.
WAP5 [13] extends Project5 by developing a new message
correlation algorithm for determining which arriving pack-
ets trigger which outgoing packets on a host. In contrast,
this paper identiﬁes the importance and challenges of dis-
covering the Leslie Graph to support a broad range of man-
agement functions. By computing Leslie Graphs at differ-
ent granularities, our techniques can uncover dependencies
which might be overlooked by WAP5 and Project5, such as
those masked by caching. We also present several new sce-
narios where the Leslie Graph can be applied. The notion of
“Communities of Interest” (COIs) in enterprise networks is
studied by Aiello et al [3]. A COI is de ﬁned more narrowly

6

than the Leslie Graph, as a collection of interacting hosts,
and the authors do not explicitly consider the problem of
ﬁnding network dependencies.

6 Conclusion
As the web of dependencies between hosts, applications
and network elements increases in size and complexity,
building tools to automatically discover and reason about
these dependencies will be invaluable for network oper-
ators and normal users.
In this paper, we introduce the
Leslie Graph as a generic representation of this web of
dependencies. We present two complementary approaches
to computing Leslie Graphs, and highlight the differences
between them in three dimensions in the design space.
We also present several applications of Leslie Graph and
the challenges in discovering its approximation that have
not been addressed in prior work. We are now gaining
experience using the Leslie Graph, and so far have had
success ﬁnding anomalous con ﬁgurations and localizing
performance faults, which tend to be transient, hard to
debug, and annoying to users. We believe that the general
problem of discovering and using Leslie Graphs presents a
rich ﬁeld of future research.

References
[1] A.Brown, G.Kar, and A.Keller. An active approach to characteriz-
ing dynamic dependencies for problem determination in a distributed
environment. In IFIP/IEEE IM, May 2001.
[2] M. K. Aguilera, J. C. Mogul, J. L. Wiener, P. Reynolds, and A. Muthi-
tacharoen. Performance debugging for distributed systems of black
boxes. In SOSP’03, pages 74–89, Oct. 2003.
[3] W. Aiello, C. Kalmanek, P. McDaniel, S. Sen, O. Spatscheck, and
J. V. der Merwe. Analysis of communities of interest in data net-
works. In PAM’05, Mar. 2005.
[4] P. Barham, A. Donnelly, R. Isaacs, and R. Mortier. Using Magpie for
request extraction and workload modelling. In OSDI’04, Dec. 2004.
[5] R. Black, A. Donnelly, and C. Fournet. Ethernet topology discovery
without network assistance. In ICNP’04, Oct. 2004.
[6] Y. Breitbart, M. Garofalakis, B. Jai, C. Martin, R. Rastogi, and A. Sil-
berschatz. Topology discovery in heterogeneous IP networks: The
NetInventory system. IEEE/ACM ToN, 12(3), June 2004.
[7] M. Y. Chen, A. Accardi, E. K ıcıman, J. Lloyd, D. Patterson , A. Fox,
and E. Brewer. Path-based failure and evolution management.
In
NSDI’04, pages 309–322, Mar. 2004.
[8] R. Duda, P. Hart, and D.G.Stork. Pattern Classiﬁcation . Wiley-
Interscience, 2nd edition, Oct. 2000.
[9] L. Lamport. Quarterly quote. ACM SIGACT News, 34, Mar. 2003.
[10] D. Oppenheimer, A. Ganapathi, and D. A. Patterson. Why do Internet
services fail, and what can be done about it? In USENIX SITS, 2003.
[11] R. Pang, M. Allman, M. Bennett, J. Lee, V. Paxson, and B. Tierney.
A ﬁrst look at modern enterprise trafﬁc.
In
IMC’05, pages 15–28,
Oct. 2005.
[12] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of
Plausible Inference. Morgan Kaufmann, Sept. 1988.
[13] P. Reynolds, J. L. Wiener, J. C. Mogul, M. K. Aguilera, and A. Vah-
dat. WAP5: Black-box performance debugging for wide-area sys-
tems. In WWW’06, May 2006.

SECURE NETWORK-ENABLED COMMERCIAL AIRPLANE OPERATIONS: 
IT SUPPORT INFRASTRUCTURE CHALLENGES 
Richard V. Robinson1, Krishna Sampigethaya1, 2, Mingyan Li1, Scott Lintelman1,  
Radha Poovendran2, and David von Oheimb3 
 
1 Boeing Phantom Works, Bellevue, WA 98008, USA 
2 Network Security Lab (NSL), University of Washington, Seattle, WA 98195, USA 
3 Siemens Corporate Technology, 81730 München, Germany 

 

ABSTRACT 

The  next-generation  commercial  airplane  models  have 
networking  facilities  that  enable  onboard  systems  to 
communicate  between  themselves  as  well  as  with  off-
board  systems.  This  new 
feature  allows  network 
applications 
for  airplane 
realize  many  benefits 
to 
manufacturing,  operations  and  maintenance  processes. 
However,  at  the  same  time  vulnerabilities  are  introduced 
that  can  threaten  the onboard systems. Regulatory bodies 
such  as 
the  EASA  and  FAA 
recognize 
that 
the 
unprecedented  network-enabled  airplane  model  may 
impact  long-established  safety  regulations  and  guidance. 
In  this  paper,  we  focus  on  securing  a  specific  network 
application,  i.e.  the  electronic  distribution  of  airplane 
loadable  software.  The  use  of  data  networks  provides 
opportunities  for  corruption  of  safety-critical  and  business-
critical  airplane  software.  The  paper  presents  a  security 
framework  that we have proposed  for  identifying  threats  to 
the  airplane  software  distribution,  and  mitigating  them. 
Additionally,  challenges  to  securing  the  distribution,  and 
open  problems 
the  security  of  network-enabled 
in 
airplanes are discussed.  
1. 
INTRODUCTION 

The  convergence  of  rapidly  expanding  world-wide  data 
communication  infrastructures, network-centric  information 
processing,  and  commoditized  lightweight  computational 
hardware,  has  brought  the  aerospace  industry  to  the 
threshold  of  a  new  era  in  aviation:  the  age  of  a  fully 
network-enabled  or  “eEnabled”  airplane.  The  prospects  in 
commercial  aviation  are  exceedingly  optimistic  for  airline 
businesses  and  the  flying  public  alike,  as  the  eEnabled 
airplane  promises  to  provide  a  basis  for  improvements  in 
passenger amenities, schedule predictability, maintenance 
and operational efficiencies, flight safety, and other areas.  
However,  as  large-scale  airplanes  employ  more  internal 
computer  processing  and  network  facilities,  and  become 
connected  with 
network 
environments 
off-board, 
opportunities  for  information  security  attacks  open.  The 
widespread  use  of  commercial  off-the-shelf  components 
raises  the  potential  for  re-engineering  and  sabotaging 
aircraft  IT  components.  Regulatory  institutions  have  yet  to 
systematically  address 
security  needs 
information 
appropriate  to  commercial  aircraft,  such  as  the  network-
enabled  787-8  airplane  model10,11.  Indeed,  while  the 
framework  informing  safety  engineering  principles  and 
practices for airplanes and airplane software is mature and 
widely  agreed  (e.g.  RTCA  DO-178B),  no  such  framework 
exists for corresponding information security needs4. 

This  paper  describes  an  approach  and  methodology  for 
addressing  one  specific,  well-defined  aspect  of 
the 
eEnabled  airplane  security  problem,  viz.,  electronic 
distribution  of  airplane  loadable  software.  Today,  industry 
standard  mechanisms 
for 
retaining  and  distributing 
airplane  loadable  software  parts1  are  evolving  away  from 
processes  that  handle  physical  storage  media,  in  favor  of 
electronic storage and distribution via computer networks2. 
We  analyze  security  issues  that  emerge when  information 
networks are used to store and distribute airplane loadable 
software  and  describe  an  approach  to  ensuring  the 
integrity of such parts throughout their lifecycles.  

Correctness  of  certain  airplane 
loadable  software 
components,  e.g.  flight  control  computer  software,  has 
direct  safety  implications.  This  self-evident  observation  is 
addressed  at 
the  standards  and  advice 
in 
length 
mandated,  for  example  in  RTCA  DO-178B1,  for  assuring 
the  quality  of  airplane  loadable  software  during  its  design 
and  development.  The  integrity  of  safety-critical  software 
parts  must  not  be  compromised.  However,  the  use  of 
public  networks 
for  storing  and  distributing  airplane 
software  may  expose  vulnerabilities  that  can  be  exploited 
to attack the integrity of parts, potentially posing a threat to 
airplane  safety  by  reducing  safety  margins.  Furthermore, 
attackers  might  exploit  vulnerabilities 
to  compromise 
systems  in  a  manner  that  reduces  passenger  comfort  or 
confidence, 
impedes  airline  business  processes,  or 
creates  unwarranted  delays  or  expenses.  In  effect,  the 
industry’s  investment  in  the  safety  and  reliability  of 
airplane software is at risk.  

In  this  paper,  we  summarize  our  analysis  of  requirements 
for  a  generic  heterogeneous  system  for  electronic  storage 
and  distribution  of  airplane  software  (an  Airplane  Asset 
Distribution  System  or  AADS)3.  We  identify  the  security 
threats,  and  propose  countermeasures  in  the  form  of 
security  primitives  sufficient  to  address  those  threats 
comprehensively.  

The  rest  of  the  paper  is  organized  as  follows.  Section  2 
presents  an  overview  of  the  proposed  security  framework. 
We  present  the  AADS  model,  security  threats  to  AADS, 
and  requirements  for  mitigating  the  threats.  We  also 
outline  a  solution  approach  based  on  digital  signatures 
that  can  provide  end-to-end  security  for  AADS.  Section  3 
discusses various unprecedented challenges presented by 
the  secure  AADS 
to  airplane  operators.  Section  4 
discusses  open  problems  and  future directions  in  the area 
of eEnabled airplane security, and Section 5 concludes. 

2.  A SECURITY FRAMEWORK FOR AIRPLANE 
SOFTWARE DISTRIBUTION 

As  will  be  seen  in  Section  3,  these  constraints  complicate 
the design of the secure AADS by requiring tradeoffs. 

2.1.  AADS Model 

Fig.  1  shows  the  constituent  entities  in  the  AADS  model. 
As 
illustrated,  a  supplier  creates 
loadable  software 
appropriately  assured  for  safety,  and  distributes  the 
software 
to  an 
intermediate  entity, 
i.e.  the  airplane 
manufacturer,  owner  (an  airline)  or  third-party  servicer. 
The  intermediate  entity  stores  the  loadable  software  and 
distributes it to the airplane or to a next intermediate entity. 
An  attacker  may  attempt  to  corrupt  software  by  exploiting 
network  and  system  vulnerabilities  or  as  an  insider  at  an 
intermediate  entity.  Additionally,  we  consider  the  following 
constraints (C1 through C4) on the AADS system. 
(C1)  Fig.  1  illustrates  that  an  airplane  can  traverse 
– 
multiple  airports  with  different  networking  capabilities. 
Each  airport  at  which  the  airplane  receives  software 
may  employ  one  among  many  available  wireless 
standards  for  its  network,  or  may  not  have  any 
network  connectivity  whatever.  Therefore  apart  from 
interoperability,  an  airplane  is  faced  with  intermittent 
connectivity  along  its  traversed  path.  Moreover,  at 
each  traversed  airport,  the  airplane  may  need  to 
communicate with multiple off-board systems. 
(C2)  Fig.  1  also  shows  that  an  airplane  can  receive 
software  from  multiple  suppliers.  Additionally,  in  the 
presence  of  multiple  owners  and  servicers  at  each 
traversed  airport,  the  airplane  must  accept  software 
only from its owner and/or authorized servicer. 
(C3) As a business objective for the AADS, the impact 
of  security  requirements  on  the  airplane  owner  must 
not be excessively costly. 
(C4)  Changes  to  the  AADS  (e.g.  use  of  onboard 
networks  and  security  mechanisms)  with  potential 
impact  on  airplane  safety  warrant  modifications  to 
mandated airplane safety regulations and guidance. 

– 

– 

– 

We assume that the airplane operator verifies the loadable 
software  configuration  to  be  correct  after  upload  to 
onboard  systems.  Verification  may  be  enabled  by  an 
airplane configuration  list of software parts available  to  the 
operator.  We  also  assume  airplane  loadable  software 
design  is  fault-tolerant,  e.g.  multiple  instances  of  software 
exist  in  system  to  prevent  a  single  point  of  failure  during 
execution.  Moreover,  we  assume  that  redundancy  checks 
help  prevent  installing  manipulated  software  on  airplane 
LRUs.  Nevertheless,  threats  to  the  service  provided  by 
AADS emerge as discussed next. 

2.2.  Security Threats 

Data networks have vulnerabilities that can be exploited by 
attackers attempting to tamper with AADS operation. 

Airplane  safety  threats.  To  lower  safety  margins  of  an 
airplane,  attackers  can  attempt  to  manipulate  and  corrupt 
the  airplane’s  safety-critical  software  parts  (e.g.  DO-178B 
Level  A  parts)  during  distribution.  Safety  threats  are 
reduced  to  an  extent  by  the  error-detecting  and  fault-
tolerant  design  of  on-board  systems.  So  the  main  safety 
threat  is  that  coherent  intentional  manipulation  of  genuine 
parts  or  injection  of  fake  parts  by  well-informed  attackers 
could go undetected. 

Business  threats.  Late  detection  of  part  manipulation, 
tampering  with  the  AADS  administrative  messages  (i.e., 
upload  commands, 
inventory 
requests  and 
related 
responses)  which  may  lead,  for  instance,  to  false  alarms, 
and  general  denial  of  service  attacks  on  software 
distribution can all create unwarranted delays to flights and 
increase  owner  costs.  An  airplane  owner’s  business  can 
also  be  impeded  if attackers manipulate non-safety critical 

Fig. 1 - Illustration of the Airplane Asset Distribution System (AADS) model and its constraints, i.e. multiple 
suppliers delivering software to airplane, and multiple airports are traversed by airplane. 

software,  such  as  cabin  light  system  software  and  other 
DO-178B  Level  D  or  E  parts,  to  generate  visible  onboard 
system  malfunctions  and  lower  passenger  confidence  or 
convenience.  Further,  an  eavesdropper  can 
induce 
intellectual property costs by illegally distributing copyright-
protected software. 

2.3.  Security Requirements 

In  order  to  address  the  two  classes  of  threats,  the 
following security requirements must be met by the AADS. 
– 
Integrity:  Software  received  by  the  airplane  must  be 
correct,  i.e.  as  produced  at  its  supplier.  This  ensures 
that  any  manipulation  of  the  content  of  distributed 
software  is  detected.  The  part  identity  must  be 
protected  with  the  part,  such  that  it  can  be  enforced 
that  (the  right  version  of)  the  part  is  accepted  at  the 
right  destination  as  desired  by 
the  airplane 
configuration management. 
–  Authenticity:  Each  software  part  by  the  airplane  must 
be  traceable  to a  trusted source,  i.e. any  intermediate 
entity in the model and/or its supplier.  
–  Authorization:  The 
identity  and  corresponding 
privilege  (e.g.  allowed  to  send  software  part)  of 
subjects attempting  to perform critical actions must be 
verified.  This  helps  ensure  that  the  received  software 
is valid and enables traceability. 
–  Traceability: Any action related to software distribution 
must be logged and attributed to a responsible entity. 
–  Early  Detection:  The  fact  that  a  part  has  been 
tampered  with  must  be  detected  as  early  as  possible 
(that  is,  by  the  next  trusted  entity  handling  it)  to 
reduce  propagation  of  invalid  parts  and  minimize 
delays to obtain replacements 
–  Correct  Status  Reporting:  Status 
information 
concerning  asset  use,  in  particular  reports  on  the 
current  contents  of 
the  airplane  on-board  parts 
storage  and  signature  expiration  information  must  be 
correct.  This  avoids,  for  instance,  false  claims  about 
missing parts. 
–  Availability:  As  long  as  there  is  sufficient  connectivity, 
AADS  must  ensure  in-time  delivery.  Should  the 
network  be  unavailable, 
there  must  be  backup 
mechanisms to distribute software to the airplane (e.g. 
physical transfer of CD/DVDs). 
We  refer  the  reader  to  [3]  and  [5]  for  a  detailed exposition 
of  the  threats,  requirements,  and  their  adequacy  in 
meeting specific threats in AADS. 

2.4.  Security Mechanisms 

Digital  signature  with  a  timestamp  offers  a  public  key 
cryptography based mechanism for protecting integrity and 
authenticity  of  software  parts,  as  well  as  satisfying 
traceability  and  even  non-repudiation.  Further,  we  note 
that  public  key  encryption  can  serve 
to  protect 
confidentiality  of  software  parts  with  intellectual  property 
content when needed. 

We  note  that  virtual  private  networks  (VPN)  do  not  suffice 
as  a  solution  for  AADS.  A  VPN  authenticates  the  source 
integrity  and  confidentiality. 
and  protects  message 
However,  message  authentication 
is  not  provided. 
Therefore,  a  VPN  cannot  guarantee  that  software  parts 
received are authentic.  If an attacker sends a manipulated 

part over VPN, the destination will incorrectly accept it as a 
valid part as long as its integrity is verifiable. 

In order  to verify a signature,  the corresponding public key 
must be  retrieved  from a digital certificate. The destination 
system  receives  the  certificate  along  with  the  signed 
software  part.  For  verifying  the  validity  of  the  received 
certificate,  the  receiver  can  either  use  off-line  verification 
with  a  trusted  set  of  preloaded  certificates  or  verification 
with a trusted third party called Certification Authority (CA). 
The  preloaded  certificates  or  CA  public  key,  respectively, 
must  be 
transported 
to 
the  airplane  using 
integrity 
protected  out-of-band  processes.  The  CA 
forms  an 
integral  part  of  a  public-key  infrastructure  (PKI),  with 
functions  that  include  certifying  the  signing  keys,  and 
distributing  certificates  for  these  keys  as  well  as  checking 
their validity  (revocation status)8. Although  this paper does 
not consider details of PKI, in Sections 4 and 5 we discuss 
some  of  the  challenges  raised  by  PKI  and  public  key 
based solutions.  

2.5.  Security Evaluation Requirements 

We  have  developed  a  formalized  version  of  our  proposed 
security  framework  for  the  AADS5  as  a  Common  Criteria 
(CC)7  Protection  Profile.  Based  on  an  analysis  of  the 
information  value  of  safety-critical  assets  (e.g.  Level  A 
software)  and  the  nature  of  expected  threats  against  the 
security  of  those  assets,  we  have  justified  the  minimum 
Evaluation  Assurance  Level  (EAL)7  for  the  integrity  and 
authenticity  protection  by  the  AADS  as  EAL  6.  However, 
we  have  also  determined  that  handling  less  critical 
software  parts  and  the  business-related  security  aspects 
require  only  EAL  4.  Our  analysis  is  validated  by  the 
Information  Assurance  Technical  Framework  (IATF)6, 
Chapter 4, “Technical Security Measures”. 

3.  CHALLENGES TO SECURING THE AADS 

The  proposed  use  of  security  solutions  such  as  digital 
signatures  and  certificates 
Internet 
to 
is  not  new 
applications.  Financial  institutions  and  other  businesses 
engaging  in  e-commerce  are  aware  of  the  returns  from 
investing  heavily  in  security  solutions  for  their  online  data 
transactions9.  However,  the  use  of  information  security 
solutions  in  airplane  applications  is  relatively  new  to  the 
aviation  industry.  Several  unprecedented  challenges  arise 
that  must  be  addressed.  For  example,  implementing 
security 
in  applications  while  meeting 
the  unique 
restrictions  presented  by  onboard/off-board  environments 
(e.g.  the  constraints  of  AADS  listed  in  Section  2.1). 
Another  example  is  evaluation  of  the  impact  of  secure 
applications  on  airplane  manufacturers  and  owners,  e.g. 
balancing  added  operational  costs  with  expected  returns 
from  the  security  investment.  We  highlight  the  important 
challenges arising in the secure AADS. 

3.1.  Verifying Signatures at Traversed Airport 
w ithout Network Connectivity 

An  airplane  may  traverse  multiple  airports  during  its  end-
to-end  flight,  requiring  the  ability  to  handle  intermittent 
network  connectivity  along  its  trajectory  (constraint  C1  in 
Section  2.1).  Further,  at  each  airport,  airplane  systems 
may  be  required  to  connect  securely  to  multiple  off-board 

systems,  e.g.  wireless  networks  and  airline  IT  systems. 
Consequently,  any  candidate  security  solution  for  airplane 
applications  must  be  scalable  in  terms  of  total  number  of 
communicating  off-board  systems.  With  the  use  of  digital 

suppliers,  one  solution  approach  is  to  have  the  airplane 
verify  signatures  of  suppliers  on  the  parts.  Additionally,  to 
ensure  that  the  airplane  accepts  parts  only  from  its 
authorized  owner,  the  airplane  must  verify  the  owner’s 

Fig. 2 - Illustration of proposed approaches meeting the AADS constraints. The top half is a schematic of 
secure software distribution from suppliers to airplane using either preloaded certificates or proper PKI (CA) 
at airplane. The bottom half shows high-level protocols for secure software distribution with verification (end-
to-end) or without verification (hop-by-hop) of supplier signature at airplane. SignX(p) denotes signature of 
entity X on part p. Cert(X) denotes certificate of entity X. 

signatures  the  problem  reduces  to  ensuring  airplane 
systems are able  to verify certificates  from  these off-board 
systems.  Therefore, even  if backup mechanisms are used 
to  transfer  software  to  the  airplane,  the  certificates 
received  with  the  software  still  need  to  be  verified.  In  this 
paper,  we  consider 
two  extreme  approaches 
for 
verification  of  the  validity  of  certificates  received  by 
airplane  systems,  as  shown  in  the  top  right  side  of  Fig.  2, 
as well as their combination. 

One  approach  is  based  on  a  PKI  that  provides  online 
verification  to  check  certificate  validity  or  the  latest 
certificate  revocation  lists  to  the airplane over  the network. 
Obviously,  this  approach  is  limited  by  the  availability  of 
networks  at  traversed  airports.  On  the  other  end  of  the 
spectrum  is  an  approach  that  pre-loads  certificates  in  the 
airplane,  providing  offline  verification  of  signatures. 
However,  with  this  approach,  revocation  of  certificates  is 
very  limited  and  the  scalability  is  limited  by  the  number  of 
communicating  off-board  systems  and  by  the  number  of 
software suppliers, as seen next. 

3.2.  Verifying Signatures from Multiple 
Suppliers and Owners 

The  AADS  comprises  multiple  suppliers  that  produce 
software  for  a  given  airplane.    Multiple  owners  may  be 
present at any given airport  (constraint C2  in Section 2.1). 
In  order 
to  protect  software  parts  distributed 
from 

signature  on  the  parts.  As  shown  in  Fig.  2,  each  supplier 
signs  its  software  parts,  the  owner  verifies  the  owner 
signature  and  adds  its  own  signature,  and  finally  the 
airplane  verifies  owner’s  as  well  as  supplier’s  signatures. 
However,  such  an  approach  may  not  be  scalable  if  the 
airplane  uses  offline 
verification  with  preloaded 
certificates,  since  certificate  management  complexity 
increases  with  the  number  of  suppliers.  Fig.  2  also 
illustrates  that an alternative, scalable approach  is  to have 
the  owner  verify  and  distribute  re-signed  software  parts, 
while  the  airplane  verifies  only  the  owner’s  signature 
against  a  preloaded  certificate.  Unfortunately, 
this 
approach  may  increase  the  overhead  costs  at  the  owner, 
as discussed below. 

3.3.  Reducing Impact of Secure AADS at 
Owner 

The  RTCA  DO-178B  guidance  indicates  that  the  safety-
criticality  of  airplane  loadable  software  may  range  from 
Level  A,  safety-critical,  to  Level  E,  no  safety  impact1. 
However,  AADS  need  not  differentiate  software  based  on 
these  levels, rendering  the same  level of assurance for all. 
With  an  assurance  level  of  CC  EAL6  needed  for  systems 
handling  safety-critical  parts,  it  becomes  necessary  to 
evaluate 
the  entire  AADS  at 
that  assurance 
level. 
Consequently,  the  evaluation  effort,  which  involves  use  of 
formal  methods  in  security  analysis,  incurs  significant 
costs and  time7. For  the approach described above where 

the  owner  removes  supplier  signatures  and  re-signs 
software,  the  evaluation  effort  of  EAL6  is  levied  on  both 
owner and supplier. 

Property 

Intermittent 
Connectivity 

explicitly  acknowledged  the  need  to  secure  the  electronic 
distribution  of  loadable  software11.  On  the  other  hand, 
regulatory  agencies  also  understand  that  the  introduction 
of  digital  certificates  and  cryptographic  keys  in  onboard 
Multiple  
Reduced Impact  
Multiple 
Off-board 
Suppliers 
at Owner 
Systems 

Scheme 
(Signature + Verification on 
airplane) 
√ 
√ 
√ 
× 
Supplier + PKI 
√ 
× 
× 
√ 
Supplier + Preload cert 
× 
√ 
√ 
× 
Owner + PKI 
Owner + Preload cert 
× 
√* 
× 
√ 
Table 1: Comparison of proposed schemes and satisfied AADS properties.  √ - can accommodate. × - not 
guaranteed to accommodate. Scheme specifies the signature verified and verification mechanism at airplane. 
 √* - accommodated if owner verifies supplier signatures using a proper PKI on ground. 

In  order  to  reduce  the  impact  at  the  owner  (constraint  C3 
in  Section  2.1)  a  tradeoff  can  be  achieved  by  having 
owners  retain  supplier  signatures  on  the  safety-critical 
parts,  and  making  airplanes  verify  these  signatures.  This 
approach  reduces  the  security  evaluation  effort  to  a 
manageable  portion,  i.e.  the  system  signing  parts  at 
suppliers  and  the  system  verifying  parts  on  the  airplane. 
The  burden  of  rigorously  evaluating  (at  EAL6)  the  IT 
systems handling safety-critical parts at the airplane owner 
is  eliminated. Another advantage of  the approach  is  that  it 
provides  end-to-end  integrity  and  authenticity  protection 
for  safety-critical  parts.  However,  scalability  issues  with 
use  of  preloaded  certificates  discussed  above  must  be 
addressed  by  the  owner.  Moreover,  this  approach  also 
requires  compliance  and  support  from  all  the  airplane 
loadable software suppliers. 

Table  1  summarizes  the  proposed  approaches  and  the 
AADS  constraints  accommodated  by  each.  It  can  be 
observed  that  each  approach  has  its  tradeoffs.  Overall,  a 
hybrid  solution 
to  secure  AADS  while  meeting 
its 
constraints  can  be  constructed  as  follows:  have  each 
supplier  sign  all  software  parts;  ensure  owners  verify 
supplier  signatures  and  additionally  sign  the  parts;  have 
the  airplane  verify  the  owner  signature,  and  for  safety-
critical parts additionally verify the supplier signature. 

For  verification  of  the  supplier  signed  safety-critical  parts 
at  the  airplane,  use  a  PKI  that  provides  online  verification 
of  certificate  validity,  or  at  least  provides  the  most  recent 
certificate  revocation  lists  to  the  airplane.  In  the  absence 
of  network  connection,  verify  only  the  owner  signatures, 
using pre-loaded certificates. 

Determining  the  set  of  preloaded  certificates  for  verifying 
certificates  from  off-board  systems  connecting  with  the 
airplane remains as a challenging open problem. 

3.4.  Specifying Impact of Security on Airplane 
Safety Regulations and Guidance 

Consistent  with  the  constraint  C4  in  Section  2.1,  the  FAA 
recently  acknowledged 
that  when  onboard  networks 
connect  to  off-board  systems,  the  airplane  effectively 
becomes  a  node  on  the  Internet.  Existing  airworthiness 
regulations do not  include  safety standards  to address  the 
resulting  security  requirements10.  Further, 
they  have 

system  storage  clearly  affects  airplane  operator  guidance. 
We discuss one specific impact next. 

4.  OPEN PROBLEMS AND FUTURE WORK 

4.1. 

Implementing and Evaluating the AADS 

As  noted  in  [2],  Boeing  is  implementing  an  instance  of 
AADS,  called  Boeing  Electronic  Distribution  of  Software 
(BEDS)  system, 
for  secure  electronic  distribution  of 
loadable  software  and data between airplanes and ground 
systems. We are  in  the process of applying our  framework 
to BEDS  for analyzing and exhibiting  the system’s security 
properties.  The  established  CC  Protection  Profile5  will 
enable  us  to  evaluate  BEDS  against  a  specific  CC 
Security  Target  derived 
from 
the  generic  Protection 
Profile.  

4.2.  Airline PKI Requirements 

The  public  key  based  applications  of  eEnabled  airplanes 
levy 
new 
requirements 
on 
airplane 
operators. 
Consequently,  FAA  and  the  European  Aviation  Safety 
Agency  (EASA) have mandated  that operator guidance be 
suitably  modified  to  include  PKI  requirements,  such  as 
management of  certificates  and cryptographic keys.  In our 
on-going  work,  we  are  exploring  airline  PKI  needs  and 
studying  the  applicability  of  solution  approaches,  including 
preloaded  certificates  not  employing  any  trust  chain 
between  them,  and  employment  of  a  proper  PKI. We  also 
intend  to  investigate  evaluation  cost-effective  and  high-
assurance PKI models to support AADS. 

4.3.  Security of Airplane Health Management 
for eEnabled Airplanes 

An unexplored area  in eEnabled airplane  is  the security of 
airplane-generated  data  that  is  distributed  to  ground 
systems.  We  focus  on  the  airplane  health  management 
(AHM)  application2.  In  particular,  we  will  explore  the 
potential  use  of  wireless  sensor  networks  (WSNs)  to 
sense, collect, and  transfer health data, some of which will 
be  distributed  to  off-board  systems  for  analysis.  An  AHM 
WSN  can  offer  significant  advantages 
to  airplane 
operators,  including  enhancing  safety  by  real-time  health 
monitoring  of 
reducing 
flight-critical  systems,  and 
maintenance  costs  and  delays  by  early  detection  of 

[6] 

[3]  S.  Lintelman,  R.  Robinson,  M.  Li,  D.  von  Oheimb,  K. 
Sampigethaya,  and  R.  Poovendran, 
“Security 
Assurance  for  IT  Infrastructure  Supporting  Airplane 
Production,  Maintenance,  and  Operation,”  National 
Systems 
Workshop 
on 
Aviation 
Software 
http://chess.eecs.berkeley.edu/hcssas/papers/Lintelma
n-HCSS-Boeing-Position_092906_ 2.pdf 
(April 2007). 
[4]  Eric  Fleischman,  Randall  E.  Smith,  and  Nick  Multari, 
“Networked  Local  Area  Networks  (LANs)  in  Aircraft:  
Safety,  Security  and  Certification  Issues,  and  Initial 
Acceptance  Criteria  (Phases  1  and  2),”  Final  Report, 
December 2006. 
[5]  R. Robinson, D. von Oheimb, M. Li, K. Sampigethaya, 
R. Poovendran,  “Security Specification  for Distribution 
and  Storage  of  Airplane-Loadable  Software  and 
Airplane-Generated 
Data,” 
Common 
Criteria 
Protection Profile manuscript, available upon request. 
Information  Assurance 
Technical 
Framework, 
Release  3.1.  US  National  Security  Agency. 
http://www.iatf.net/framework_docs/version-3_1/. 
[7]  Common Criteria. 
http://www.commoncriteriaportal.org/  
[8]  C.  Adams  and  S.  Lloyd,  Understanding  PKI: 
Concepts, 
Standards, 
and 
Deployment 
Considerations, 2nd edition, Addison-Wesley, 2003. 
[9]  H.  Cavusoglu,  B.  Mishra,  and  S.  Raghunathan,  “The 
Effect  of  Internet  Security  Breach  Announcements  on 
Market Value of Breached Firms and Internet Security 
Developers,” 
International  Journal  of  Electronic 
Commerce, Vol. 9, No. 1, 2004, pp. 69-105. 
[10] Federal  Aviation  Administration,  14  CFR  Part  25, 
Special  Conditions:  Boeing  Model  787–8  Airplane; 
Systems  and  Data  Networks  Security—Isolation  or 
from  Unauthorized  Passenger  Domain 
Protection 
Systems  Access, 
[Docket  No.  NM364  Special 
Conditions  No.  25–07–01–SC],  Federal  Register, 
Vol. 72, No. 71, April 13, 2007, 
http://edocket.access.gpo.gov/2007/pdf/E7-7065.pdf  
[11] Federal  Aviation  Administration,  14  CFR  Part  25, 
Special  Conditions:  Boeing  Model  787–8  Airplane; 
Systems  and  Data  Networks  Security—Protection  of 
Airplane  Systems  and  Data  Networks  From 
Unauthorized  External  Access,  [Docket  No.  NM365 
Special  Conditions  No.  25–07–02–SC],  Federal 
Register, Vol. 72, No. 72, April 16, 2007, 
http://edocket.access.gpo.gov/2007/pdf/07-1838.pdf. 
[12] H.  Bai,  M.  Atiquzzaman,  and  D.  Lilja,  “Wireless 
Sensor  Network 
for  Aircraft  Health  Monitoring,” 
Broadband  Networks  (BROADNETS'04),  2004,  pp. 
748 – 750. 
[13] M.  Barbeau,  “WiMax/802.16  threat  analysis,”  ACM 
international  workshop  on  Quality  of  service  & 
security  in  wireless  and  mobile  networks,  Montreal, 
Quebec, Canada, 2005, pp. 8--15. 

onboard  system  failures12.  Another  notable  benefit  is  the 
reduction  in  system  weight  and  costs  associated  with 
onboard  wiring.  In  our  future  work,  we  will  propose  a 
security  framework  to  enable  the  beneficial  use  of  AHM 
WSN.  We  note  that  integration  of  this  framework  with  the 
one  proposed  for  AADS,  offers  end-to-end  security  for 
AHM data. 

4.4.  Security of Air Traffic Management for 
eEnabled Airplanes 

Integration  with  air  traffic  management  (ATM)  centers  is 
another  potential  application  of  eEnabled  airplanes. 
Advances  in  wireless  technologies,  such  as  WiMAX13, 
enable  broadband  point-to-point  connectivity  over  long 
distances  between  airplane  and  ATM  center.  By 
communicating  with  air 
traffic  centers,  an  eEnabled 
airplane  may  not  only  improve  air  traffic  control  efficiency 
and  reduce  flight  delays,  but  also  automate  processes 
prone  to  human  errors  (e.g.  landing  in  low  visibility 
conditions).  Based  on  the  security  framework  proposed  in 
this  paper, we will  study  the  security  of  ATM  for  eEnabled 
airplanes.  However,  unique  security  challenges  arise  due 
to  application  constraints  such  as  online  connection 
between in-flight airplanes and the traffic centers. 

5.  CONCLUSION 

This  paper  focused  on  securing  the  electronic  distribution 
of airplane  loadable software. We  identified  two classes of 
threats,  to  airplane  safety  and  to  the  business  of  airplane 
owners.  After  specifying  security 
requirements,  we 
proposed  use  of  digital  signatures  for  end-to-end  integrity 
and  authenticity  of  software  distributed  from  a  supplier  to 
an  airplane.  We  presented 
the  main  challenges 
to 
securing  the  electronic  distribution  of  airplane  software, 
and  suggested  a  suitable  architecture  that  addresses 
these  challenges.  The  results  of  our  work  have  profound 
implications 
for  security  of  other  potential  eEnabled 
airplane applications, ranging from integration with ground-
based maintenance  information  systems  for  flight  logistics 
and maintenance,  to  interoperability with  air  traffic  control. 
Identifying  criteria  that  regulatory  agencies  must  adopt  or 
recommend  with  respect  to  the  security  of  eEnabled 
airplane applications, remains an open problem. 

6.  ACKNOWLEDGEMENTS 

We  would  like  to  thank  Prof.  Peter  Hartmann  from  the 
Landshut  University  of  Applied  Sciences  for  his  insightful 
and valuable comments  that helped us  to  improve specific 
sections of this paper. 

7.  REFERENCES 
[1]  DO-178B:  Software  Considerations 
in  Airborne 
and  Equipment  Certification.  Radio 
Systems 
Technical  Commission 
for  Aeronautics 
(RTCA) 
(1992). 
[2]  G.  Bird,  M.  Christensen,  D.  Lutz,  and  P.  Scandura, 
“Use  of  integrated  vehicle  health  management  in  the 
field  of  commercial  aviation,”  NASA  ISHEM  Forum, 
2005. 

ability
abort
abramson
abstract
academic
academy
accept
acceptable
accepted
access
accessed
accessible
accessing
accidental
accomplished
according
accordingly
account
accountability
accuracy
accurate
achieve
achieved
acknowledgement
acoustic
acquire
acquired
acquisition
across
action
activation
active
activity
actual
actually
actuation
actuator
adapt
adaptation
adapted
adapter
adaptive
added
adding
addition
additional
additionally
address
addressed
addressing
adequate
adjacency
adjacent
administration
administrator
advance
advanced
advantage
affect
affected
afips
afnity
agency
agent
aggregation
agreed
agreement
aimed
aircraft
airhopper
airline
airplane
airport
akyildiz
algorithm
alice
allocated
allow
allowed
allowing
allows
almost
aloha
alone
along
already
alternating
alternative
alternatively
although
always
ambedkar
american
among
amount
ample
amplitude
analog
analysis
analyzer
anderson
android
animal
annual
anomaly
another
answer
antenna
anyone
anything
anytime
anywhere
apple
applicability
application
applied
applies
apply
approach
appropriate
approximate
approximation
april
arbitrary
architecture
arise
around
arrive
arrives
arriving
articial
article
ashwin
aspect
assemble
asset
associated
association
assume
assumed
assumes
assumption
assurance
assure
assured
atlanta
attached
attachment
attack
attacker
attempt
attempted
attempting
attention
audio
auditing
august
australia
authenticate
authenticated
authenticates
authentication
authenticity
author
authored
authority
authorization
authorized
automated
automatic
automatically
availability
available
average
aviation
avoid
avoiding
award
aware
axiom
backbone
background
backup
bacteria
balzani
bandwidth
barrier
based
basic
basis
battery
become
becomes
begin
behalf
behavior
behaviour
behind
belief
believe
belonging
benchmark
benchmarking
berkeley
besides
better
beyond
bidirectional
binary
binding
biological
biology
biomedical
black
block
board
boeing
bogus
bonfire
bottleneck
bottom
bound
boundary
brain
breach
break
broadband
broadcast
broadcasting
broken
browser
brunetti
buffer
buffering
build
building
built
business
cable
cache
cached
caching
calcium
calculated
calculation
california
called
campus
canada
candidate
capability
capable
capacity
capture
carbon
careful
carried
carrier
carry
carrying
category
cation
cause
caused
cellular
center
central
centralized
certain
certificate
certification
chain
chair
challenge
challenging
chance
change
changed
changing
channel
characteristic
characterize
charge
charles
check
checking
checksum
chemical
chemosignal
chine
choice
choose
circuit
circumstance
claim
class
classical
classified
classify
clause
clear
clearly
client
clock
close
closet
cloud
cluster
coaxial
coding
coexistence
cognitive
collect
collected
collection
college
colliding
collision
color
combination
combine
coming
command
comment
commercial
common
commonly
commun
communicate
communicating
communication
community
company
comparability
compare
compared
comparing
comparison
compatibility
complete
completely
complex
complexity
compliance
complicated
component
compound
compression
compromise
compromised
compu
computation
computational
compute
computer
computing
concentration
concept
concern
conclude
conclusion
condition
conduct
conducted
conference
configuration
congestion
conguration
conjunction
connect
connected
connecting
connection
connectivity
connects
consequence
consequently
consider
consideration
considered
considering
consist
consistent
consisting
consists
consortium
constantly
constellation
constraint
construct
constructed
constructing
construction
consumption
contact
contain
containing
contains
content
contention
context
continuous
continuously
contract
contrast
contribute
contribution
control
controllable
controlled
controller
controlling
convenient
convention
converged
convergence
convert
cooperate
coordinate
coordinated
coordination
corporation
correct
correction
correctly
correlation
correspond
correspondent
corresponding
corruption
could
council
countermeasure
coupled
course
cover
coverage
covered
create
created
creates
creating
creation
credential
credit
criterion
critical
crocker
crowther
crucial
cryptographic
cryptography
cules
cumulative
current
currently
customer
damage
damaged
danger
database
datapath
decade
december
decide
decides
decision
decode
decoded
decoder
decodes
decoding
dedicated
default
defense
defensive
deference
define
defined
degree
delay
delegate
delegation
deliver
delivered
delivery
demand
demonstrate
demonstrated
dened
denes
denotes
department
depend
dependence
dependency
dependent
depending
depends
deploy
deployed
deploying
deployment
depth
derby
derived
describe
described
describes
describing
description
design
designated
designed
designer
designing
desirable
desired
desktop
despite
destination
detail
detailed
detect
detected
detecting
detection
determination
determine
determined
determines
determining
develop
developed
developer
developing
development
deviation
device
diameter
dierent
difcult
difference
different
diffusion
digital
direct
direction
directly
directory
disconnected
discovered
discovery
discrete
discus
discussed
discussion
disease
display
disruption
distance
distinct
distinguished
distribcted
distribute
distributed
distributing
distribution
diverse
diversity
division
document
doesnt
domain
dozen
drawback
dresden
driven
dropped
duration
dynamic
dynamically
earlier
early
easier
easily
eavesdropping
ecules
edition
editor
editorial
education
eenabled
efcient
effect
effective
effectively
efficiency
efficient
effort
egashira
either
electrical
electromagnetic
electronic
electronics
element
eliminate
eliza
elsevier
email
emanation
embedded
emerge
emerging
emission
emphasis
employ
employed
employee
enable
enabled
enables
enabling
encapsulation
encode
encoded
encoder
encodes
encoding
encourage
encrypted
encryption
endpoint
endreply
energy
enforce
enforced
enforcement
engine
engineering
enhance
enhancement
enhancing
enomoto
enough
ensure
ensuring
enterprise
entire
entirely
entity
entry
environment
environmental
envisaged
envisioned
equipment
equipped
equivalent
error
especially
essential
essentially
establish
established
establishing
estimate
estimating
estimation
ethane
ether
ethernet
european
evaluate
evaluated
evaluating
evaluation
event
eventually
every
everything
evidence
evolution
exact
exactly
example
exceed
except
exchange
executable
execute
executing
execution
exfiltration
exhibit
exibility
exist
existence
existing
exists
expand
expanding
expect
expected
expensive
experience
experiment
experimental
experimentation
experimenter
explain
explained
explains
explanation
explicit
explicitly
exploit
exploited
explore
explored
expose
express
extend
extended
extending
extends
extension
extensive
external
extract
extreme
extremely
fabric
facility
factor
fading
failed
failure
fairly
false
farber
faster
fault
faulty
feasibility
feasible
feature
february
federal
ferent
fewer
field
figure
filter
filtering
finally
financial
finding
firewall
first
firstly
fixed
flakyprogram
flexibility
flight
floor
focus
focused
folder
follow
followed
following
follows
force
foreign
format
formation
formula
forward
forwarding
found
fourth
fraction
framework
fraser
freely
freitas
frequency
frequent
frequently
front
fully
function
functional
functionality
furthermore
future
gateway
general
generally
generate
generated
generates
generating
generation
generic
genetic
georgia
germany
getting
given
giving
global
going
grant
granularity
graph
graphic
great
greater
greatly
ground
group
growing
growth
guarantee
guaranteed
guard
guest
guidance
gullible
gunningberg
gured
handle
handled
handling
handoff
handover
happen
happens
hardware
hawaii
header
headphone
health
heard
heart
heavily
hence
heterogeneous
hidden
hiding
hierarchical
hierarchy
higher
highlight
highlighted
highly
history
horizontal
hosted
hostile
hosting
house
however
human
hundred
hybrid
ications
ideal
identied
identified
identify
identifying
identity
iguration
ijarcsse
illustrated
illustrates
image
imagine
immediately
immune
impact
implant
implement
implementation
implemented
implementing
implication
implies
importance
important
impossible
improve
improved
improvement
improving
inactive
include
included
includes
including
increase
increased
increasing
increasingly
indeed
independent
independently
index
india
indicate
indicated
indicates
indicator
indirect
individual
industrial
industry
infecting
inference
influence
information
infrastructure
infrastructured
initial
initiated
inner
innovation
input
inside
insider
inspired
install
installed
instance
instantaneous
instead
institute
institution
instruction
instrumentation
insurance
integrated
integration
integrity
intel
intelligent
intended
interact
interaction
interactive
interconnect
interconnected
interconnection
interest
interesting
interface
interfacing
interfere
interference
interfering
intermediate
intermittent
internal
international
internet
interoperability
interpretation
interruption
interval
intranet
introduce
introduced
introduces
introducing
introduction
investigate
investigating
invoked
involve
involved
involves
involving
ipsec
isolate
isolated
isolation
issue
jalandhar
january
jitter
johnson
journal
junction
kalice
keeping
kerberos
kernel
keyboard
keywords
kilometer
kintel
kmsdir
knowledge
known
korea
ktemp
kumar
laboratory
landfeldt
landscape
language
laptop
large
larger
latency
later
latter
layer
layout
leader
leading
learn
learning
least
legacy
length
leslie
letter
level
licensed
ligand
light
lightweight
likely
limit
limitation
limited
linear
linux
literature
little
living
loadable
loaded
local
localization
located
location
locomotion
logical
login
longer
loose
lower
lowest
machine
magazine
magnetic
mainly
maintain
maintaining
maintains
maintenance
major
making
malcon
malicious
malware
manage
managed
management
manager
managing
manet
manipulation
manner
manually
manufacturer
manufacturing
maodv
mapping
march
market
match
matching
material
mathematics
mation
matrix
matter
maximal
maximum
meaning
measure
measured
measurement
measuring
mechanical
mechanism
median
medium
meeting
megabit
melander
member
membership
membrane
memory
mentioned
ments
message
messaging
messenger
metcalfe
meter
method
methodology
metric
michael
micro
microsoft
microtubule
middle
middleware
might
military
million
millisecond
minicomputer
minimal
minimum
minute
mitrix
mitter
mncrs
mobile
mobility
model
modeled
modelled
modelling
modern
modied
modification
modified
modify
modifying
modulated
modulation
molecular
molecule
money
monitor
monitored
monitoring
moore
moreover
motion
motor
moved
movement
moving
multicast
multicasting
multimedia
multipath
multiple
multiplexing
multiprocessing
multiprocessor
munication
murthy
nakano
named
namely
naming
nanomachines
nanomechanical
nanomedicine
nanonetwork
nanonetworks
nanotechnology
nanotube
national
natural
naturally
nature
nding
nearly
necessarily
necessary
needed
negative
neighbor
neighborhood
neighboring
neither
nents
netfpga
netlogon
network
networked
networking
never
nevertheless
noise
nokia
nomadic
normal
normally
nothing
notion
novel
november
number
numerous
object
objective
obligation
observation
observed
obstacle
obtain
obtained
obtaining
obvious
obviously
occur
occurs
october
offer
offered
offering
office
often
onboard
ongoing
online
openflow
operate
operates
operating
operation
operational
operator
opportunity
optical
optimal
optimization
optimum
option
order
ordinary
organ
organelle
organism
organization
organized
origin
ornstein
others
otherwise
outcome
outgoing
outline
output
outside
outstanding
overall
overcome
overhead
overloaded
overview
owner
packe
packet
paper
paradigm
parallel
parameter
parent
participating
particle
particular
partition
pascal
passenger
passing
passive
password
patent
pathway
pattern
people
perfect
perform
performance
performed
performing
performs
perhaps
period
periodic
periodically
perkins
permission
persistent
personal
phase
pheromonal
pheromone
phone
physical
physically
piece
pipeline
pixel
place
placed
planning
platform
played
player
point
police
policy
pollution
popular
popularity
population
portion
positive
possibility
possible
possibly
potential
potentially
power
powered
powerful
practical
practice
precise
preloaded
presence
present
presented
presenting
press
prevent
previous
previously
primary
primitive
principal
principle
printer
printing
prior
privacy
private
privilege
proach
proactive
probability
probably
probe
probing
problem
problematic
procedure
proceeding
process
processed
processing
processor
produce
produced
product
production
professor
program
programmable
progress
project
projector
promise
promising
propagate
propagates
propagation
proper
properly
property
proposal
propose
proposed
proposes
protect
protected
protection
protein
protocol
proven
provide
provided
provider
provides
providing
provisioning
proxy
public
publication
published
punishment
purpose
puter
quality
query
question
quickly
quite
quota
radiation
radio
raised
random
range
ranging
rapid
rapidly
rather
ratio
reach
react
reaction
reactive
reader
realistic
realize
reason
reasonable
reasoning
receive
received
receiver
receiving
recent
recently
reception
receptor
record
recording
reduce
reduced
reduces
reducing
reduction
redundancy
refer
reference
referred
reflect
regarding
region
registration
regulation
regulatory
related
relationship
relative
relatively
relay
release
released
relevant
reliability
reliable
remains
remote
remove
repeat
repeatability
repeatedly
repeater
replace
reply
report
reported
reporting
represent
representation
represented
represents
reprinted
request
requesting
require
required
requirement
requires
requiring
research
researcher
reserved
resolution
resolve
resolved
resource
respectively
response
responsibility
responsible
responsiveness
restrict
restricted
restriction
result
resulting
retransmission
retransmissions
retransmit
return
review
reviewed
revised
revocation
ricean
richard
right
robert
robust
roughly
round
route
routed
router
routing
running
safety
sample
sampling
samsung
saodv
satellite
satisfaction
satisfied
satisfy
scalability
scalable
scale
scanning
scenario
scheduling
scheme
schneier
school
science
scientist
scope
score
scott
screen
seamless
search
second
secondary
secrecy
secret
section
secure
secured
securing
security
seems
segment
seldom
select
selected
selecting
selection
sender
sending
sends
seneviratne
sense
sensing
sensitive
sensitivity
sensor
separate
separately
separation
september
sequence
sequenced
serve
served
server
service
session
setting
setuid
setup
several
share
shared
sharing
shielded
short
showed
shown
sigcomm
signal
signaling
signature
signed
significant
significantly
signing
similar
similarly
simple
simpler
simplest
simplicity
simply
simulation
simultaneous
simultaneously
since
single
situation
slightly
slowly
small
smaller
smart
smooth
sniffer
society
software
solution
solved
someone
something
sometimes
sophisticated
source
space
spain
spand
speak
speaks
special
specialized
specic
specically
specication
specie
specied
specif
specific
specifically
specification
specified
specify
specifying
spectral
spectrum
speed
spread
square
stable
stage
stale
standard
standardized
stanford
start
started
state
statement
static
station
statistic
statistical
status
still
stimulus
storage
store
stored
storing
story
strategy
stream
strength
strictly
stripe
strong
structure
structured
studied
study
studying
stuxnet
subject
subsequent
subset
substance
substantial
substantially
success
successful
successfully
sufcient
sufficient
suggest
suggested
suggests
suitable
suited
summarize
summarized
supplier
supply
support
supported
supporting
survey
sustained
sweden
switch
switching
symposium
synchronization
synchronous
system
table
taken
taking
target
targeted
technical
technically
technique
technology
telecommunication
telephone
temperature
tempest
temporal
temporary
terminal
terminology
testbed
testbeds
tested
testing
textual
thanks
thentication
theoretical
theory
thereby
therefore
therne
thing
think
third
though
thought
threat
three
threshold
throughout
throughput
ticket
timing
tional
tions
today
together
token
topic
topological
topology
total
touch
towards
trace
track
tradeoff
traditional
trafc
traff
traffic
trans
transaction
transceiver
transceivers
transducer
transfer
transferred
transient
translation
transmission
transmit
transmitted
transmitter
transmitting
transport
travel
traverse
traversed
treat
treatment
trend
triangle
trigger
trivial
troller
trouble
truncated
trust
trusted
tuning
tunnel
tunneling
tures
turned
turner
tween
twice
typical
typically
unable
uncontrollable
underlying
understand
understanding
understood
undetected
unexpected
unfortunately
uniform
unique
university
unless
unlike
unlikely
unrelated
unused
unwanted
update
updated
upper
uptime
usage
useful
usenix
using
usually
utility
vaidya
valid
validity
valuable
value
variability
variable
variation
varies
variety
various
varying
vector
vendor
verification
verified
verifier
verifies
verify
verifying
version
vertex
vertical
vesicle
vgaext
vicinity
video
viewed
violated
violation
virtual
visible
visiting
vlans
voice
voltage
volume
vulnerability
waiting
walden
washington
water
waypoint
weight
whenever
whereas
whether
whilst
white
whole
whose
widely
wider
widespread
wiley
willard
willing
window
windowing
wired
wireless
wireline
wiring
within
without
working
workload
workshop
world
worse
would
write
written
wrong
yield
zdnet
zhang
zigbee
Computer Networks 55 (2011) 1455–1458

Contents lists available at ScienceDirect

Computer Networks

j o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / c o m n e t

Editorial
Recent Advances in Network Convergence

Recent years have witnessed an increasing trend to-
wards the convergence of networks, services, and applica-
tions.
In contrast to a telecommunications landscape
where different services are delivered through separate ac-
cess technologies, each tailored to a speciﬁc application,
network convergence aims at providing a uniﬁed network
platform for the efﬁcient coexistence of previously distinct
media accessible through common interfaces on single de-
vices. The convergence trend is further stressed by users’
increasing demand for ubiquitous access to any service
anytime, anywhere and on any device. This, however, im-
plies that one device should be able to connect with a mul-
titude of specialized services, access technologies, and
possibly different network operators as illustrated in Fig. 1.
As network convergence involves many different par-
ties, its technical and economical success depends on the
ability to agree on a common set of standard interfaces.
Moreover, the convergence of different access technologies
along with users’ increasing mobility and demand for con-
nectivity at all times at high data rates raise the need for
new monitoring, control and management approaches
essential for providing converged services with quality-
of-experience (QoE) assurance. At the same time, network
convergence is expected to serve as an enabler for new
business models for network operators and service provid-
ers, a means to reduce operations and management costs,
and a platform for faster deployment of new multimedia
applications. Convergence attracted even more interest
with the advent of Cloud Computing in the recent few
years where a converged view of infrastructure is emerg-
ing in which all resources whether computing, storage or
networking are viewed as being part of shareable resource
pools that can be controlled and managed using the same
systems. The converged infrastructure is envisioned to
encompass remote datacenters, dense small-cell wireless
access integrated with optical backhaul where the tradi-
tional point-to-point backhauling may be partly replaced
by multi-point technologies, and deployment of a Future
Internet architecture. The realization of this vision how-
ever faces many technical challenges that need to be ad-
dressed in order to ensure failsafe network operations, to
efﬁciently support user mobility, and to guarantee QoE.
This special issue has as its main purposes the presen-
tation of recent developments in the area of network

1389-1286/$ - see front matter Ó 2011 Published by Elsevier B.V.
doi:10.1016/j.comnet.2011.04.002

convergence, and providing a view of the ﬁeld’s state-of-
the-art today. It is composed of eleven papers selected
from 50 submissions. The contributions span IP multime-
dia subsystems
(IMS), media independent handover
(MIH), ﬁxed-mobile convergence, convergence of different
wireless standards, vertical handover, quality-of-service
(QoS) and QoE, and convergence in the network core.
QoE is the ultimate measure of user satisfaction. In the
paper ‘‘QoE assurance in converged networks.’’, R. Stan-
kiewicz and A. Jajszczyk discuss several quality measures,
speciﬁcally QoS, Grade-of-Service (GoS), and Quality-of-
Resilience (QoR), and their impact on QoE assurance in
converged networking environments. The authors focus
on convergence between ﬁxed and wireless as well as con-
vergence between heterogeneous wireless technologies,
and elaborate on the requirements for any service, any-
where, anytime and on any device.
The IP multimedia subsystem (IMS), standardized by
3GPP, plays a key role in network convergence and pro-
vides a platform for converged voice, video, and data-
applications over ﬁxed and mobile networks based on IP.
In the paper ‘‘TRIM: An architecture for transparent IMS-
based mobility,’’ I. Vidal, A. de la Oliva, J. Garcia-Reinoso
and I. Soto consider IP mobility in IMS networks and show
how a new architecture called TRIM extends mobile termi-
nals such that seamless IMS-based mobility can be pro-
vided transparently to end-user applications. TRIM
support for mobility is evaluated on the basis of an IMS
test-bed with 3G and WLAN access networks.
The transition towards new mobile communication
standards is characterized by the co-existence of different
wireless standards such as GSM and UMTS in addition to
3GPP LTE and IEEE 802.16 during the roll-out of 4G net-
works. This problem is addressed in the paper ‘‘Reinforce-
ment learning for joint radio resource management in LTE-
UMTS scenarios.’’ by N. Vucevic, J. Pérez-Romero, O. Sal-
lent, and R. Agustı´ . In particular, the authors consider the
convergence of 3G and 4G networks using the example
of co-existing UMTS and LTE technologies and present a
dynamic joint radio resource management algorithm span-
ning both technologies. Based on a reinforcement learning
technique, the algorithm determines the most appropriate
Radio Access Technology for user sessions in a multi-
service scenario under dynamic conditions. The low

1456

Editorial / Computer Networks 55 (2011) 1455–1458

Fig. 1. Typical wireless and ﬁxed networks involved in network convergence.

complexity of the proposed algorithm makes it a likely
candidate for implementation in real-time systems.
Co-existence and convergence of different mobile access
technologies implies the need for seamless handover be-
tween these technologies commonly referred to as vertical
handover. IEEE 802.21 provides a standard framework for
vertical handover and deﬁnes the mechanisms of media
independent handover. In the paper ‘‘Context-aware media
independent information server for optimized seamless
handover procedures.’’ P. Neves, J. Soares, S. Sargento, H.
Pires, and F. Fontes consider vertical handover between
IEEE 802.11 (WiFi), IEEE 802.16 (WiMax), 3GPP UMTS,
HSPA, and LTE. Their proposal extends IEEE 802.21 media
independent handover by introducing a context manage-
ment server used to dynamically acquire and maintain
information from the network and the mobile terminals
such as the number of and requirements of currently run-
ning services, established radio bearers, user preferences,
and others. The context information is then exploited to
optimize the handover process, e.g., the handover duration.
On a similar topic, the paper ‘‘Autonomic personalized
handover decisions for mobile services in heterogeneous
wireless networks.’’ by J.-M. Kang, J. Strassner, S.-S. Seo,
J.W.-K. Hong focuses on personalized and automated verti-
cal handover decisions in heterogeneous mobile networks.
In this proposal, the handover is not solely based on the
signal strength but considers two additional metrics: the
access-point-acceptance
and
access-point-satisfaction.
The former is a vector of individual channel quality mea-
sures such as delay and power consumption, and the latter
combines these individual measures based on the user’s
needs. Accordingly, the access point which maximizes

these metrics and thereby user satisfaction is selected.
The proposal is evaluated through simulations and com-
pared to other decision metrics.
While previous contributions consider the co-existence
of different mobile communication standards, the paper
‘‘Vertical handovers among different wireless technologies
in a UMTS radio access-based integrated architecture’’ by N.
Vulic, S.M. Heemstra de Groot, I.G. Niemegeers extends the
problem to the co-existence of mobile access technologies
and broadcast systems. In particular, the authors focus on
the integration of UMTS, IEEE 802.11, IEEE 802.16, and
DVB-H at the radio access level. The authors describe the
necessary handover mechanisms and conduct performance
evaluations for different vertical handover scenarios.
Heterogeneous wired/wireless networks rely on com-
plex wired mesh backbones with gateway nodes connect-
ing to the Internet. These gateway nodes act as converging
points for heterogeneous network trafﬁc heading to and
from the Internet and can turn into bottlenecks once their
resources are exhausted. In this context, the paper ‘‘Re-
source competition in a converged heterogeneous net-
working ecosystem.’’ by A. Jamalipour, F. Javadi, and K.S.
Munasinghe asks the important question of how a system
can maintain stability over a period of time in a resource
diminishing environment, provided the resources are fairly
allocated at the time of admission. To answer this ques-
tion, the authors conduct an ecologically inspired analysis
where the converged heterogeneous network is modeled
as an ecosystem of diverse networks with limited resource
constraints and competing communication sessions. Based
on the result of their analysis and the key observation
of competitive exclusion between sessions, the authors

Editorial / Computer Networks 55 (2011) 1455–1458

1457

propose a novel admission control policy to allow fair co-
existence among sessions.
In the paper ‘‘Evaluation of two integrated signaling
schemes for the Ultra Flat Architecture using SIP, IEEE
802.21, and HIP/PMIP protocols.’’ Z. Faigl, L. Bokor, P.M.
Neves, K. Daoud, and P. Herbelin discuss a new network
architecture, called UFA (Ultra Flat Architecture), which
distributes some of the core network (e.g., 3GPP) functions
to the edge (close to the base stations) in order to better
scale to the ever growing trafﬁc demand. Speciﬁcally the
paper extends UFA with support for non-SIP based applica-
tions for more ﬂexibility in converged networks as well as
legacy Internet applications including IMS compliant
applications.
The heterogeneous landscape of wireless access tech-
nologies does not only imply the necessity of co-existence
but also provides the possibility to integrate different ac-
cess technologies using the same infrastructure. This not
only relates to the radio access but also to the core net-
work, which may be shared by different operators. In
‘‘On the role of infrastructure sharing for mobile network
operators in emerging markets.’’ D.-E. Meddour, T. Ras-
heed, and Y. Gourhant discuss the different options of
infrastructure sharing their implications and potential
savings of operational expenditures (OPEX) as well as cap-
ital expenditures (CAPEX) under technical, practical and
regulatory constraints.
The clustering and ﬁxed allocation of the available spec-
trum over the years has resulted in an inefﬁcient use of the
scarce spectral resources. To alleviate this problem, dy-
namic spectrum management techniques such as cognitive
radio have received a lot of attention recently. B. Ishibashi,
N. Bouabdallah, and R. Boutaba in their paper ‘‘QoS Capac-
ity of Virtual Wireless Networks’’, introduce the concept of
Virtual Wireless Networks (VWNs) created and operated
without obtaining any dedicated spectrum resources of
their own. VWNs exploit residual bandwidth left underuti-
lized by licensed users. However, it is unclear whether
VWNs can provide QoS guarantees since the residual re-
sources can be reclaimed anytime by the primary users.
The authors extend a classical wireless system with
cognitive radio capabilities and use Markov chain-based
analysis and simulations to demonstrate that VWNs are
indeed capable of providing QoS and allowing for new ser-
vices provided by virtual wireless operators.
In the last paper presented in this special issue ‘‘SWISH:
Secure WiFi Sharing.’’, D. Leroy, G. Detal, J. Cathalo, M.
Manulis, F. Koeune, and O. Bonaventure present an efﬁ-
cient and fully deployed solution, SWISH, enabling secure
WiFi sharing with visiting mobile users. The key motiva-
tion behind the development of SWISH is to enable mobile
users to connect to the Internet via a nearby foreign net-
work without jeopardizing the security of the visited net-
work. There exist
indeed many WiFi networks with
unused resources, which visiting mobile users can use to
connect to the Internet if the owners of these networks
were not reluctant due to security concerns. Using crypto-
graphic authentication of the involved parties (mobile
user, her/his home network and the visited WiFi network),
the authors demonstrate how WiFi networks can be fully
secured with minimal impact on their performance which

is demonstrated by deploying and testing their solution in
a production campus network.
In presenting these diverse research efforts on converg-
ing communication systems and services, we hope to
provide some possible directions to future network con-
vergence research and to excite future research explora-
tions in this promising ﬁeld. It is also our hope that the
selected papers have shown to you, the readers, the
breadth and depth of the challenges we face and the exis-
tence of many open research issues.
We express our gratitude to the Editor-in-Chief, Dr.
Harry Rudin, for giving us the opportunity to put together
this special issue, and for his continuous support through-
out this project from establishing a successful call for pa-
pers to the time-consuming editorial work he put on
each accepted paper. We also express our thanks to the
many authors who responded to the call for papers. We
are particularly grateful to the authors of the four invited
papers for kindly agreeing to write papers: A. Jajszczyk,
A. Jamalipour, D.-E. Meddour, J. W.-K. Hong and their co-
authors. All the papers went through a rigorous review
process and we are grateful to the many obliging reviewers
whose timely efforts were essential for the selection of the
papers. As much as we enjoyed working on this special is-
sue, we hope you will enjoy reading it.

Peter Rost received his Ph.D. degree in elec-
trical engineering from Technische Universität
Dresden, Dresden, Germany, in 2009 and his
M.Sc. degree in information technology from
University of Stuttgart, Stuttgart, Germany, in
2005.
From 1999 to 2002, he was with Fraunhofer
Institute for Beam and Material Technolo-
gies, Dresden, Germany where he developed
augmented and virtual reality based con-
trolling systems. From 2002 to 2005 he was
with IBM Deutschland Entwicklung GmbH,
Böblingen, Germany, where he contributed to the IBM Tivoli system. In
June 2005 he joined the Vodafone Chair of Prof. Gerhard Fettweis at
Technische Universität Dresden and focused on different aspects of
relaying in the context of mobile communications systems. He was also
a leading member of the organizing team of IEEE ICC 2009, prepared
multiple successful EU FP6 and FP7 proposals, and was actively con-
tributing to the European research project WINNER since 2005 as task
leader. Since April 2010 Peter is member of the Mobile and Wireless
Networks group at NEC Laboratories Europe, where he is working in BU
projects as well as the EU FP7 project FLAVIA.
Currently, he is conference coordinator of the ﬁrst IEEE online conference
(2010 IEEE Online Conference on Green Communications). He published
30 + contributions to books, journals, conferences, and public project
reports, and he is author of multiple patents and patent applications.

Raouf Boutaba received the M.Sc. and Ph.D.
degrees in computer science from the Uni-
versity Pierre & Marie Curie, Paris, in 1990
and 1994, respectively. He is currently a pro-
fessor of computer science at the University of
Waterloo (Canada) and a distinguished visit-
ing professor in the division of IT convergence
engineering at POSTECH (Korea). His research
interests include network, resource and ser-
vice management in wired, and wireless net-
works. He is the founding editor in chief of the
IEEE Transactions on Network and Service
Management (2007–2010) and on the editorial boards of other journals.
He has received several best paper awards and other recognitions such as
the Premier’s Research Excellence Award, the IEEE Hal Sobol Award in

1458

Editorial / Computer Networks 55 (2011) 1455–1458

2007, the Fred W. Ellersick Paper Prize in 2008, the Joe LociCero award
and the Dan Stokesbury in 2009.

Klaus Doppler received his Ph.D. degree in
Electrical Engineering from Aalto University
School of Science and Technology, Helsinki,
Finland,
in 2010 and his M.Sc. degree in
Electrical Engineering from Graz University of
Technology, Austria, in 2003.
He joined Nokia Research Center in 2002.
Currently he is leading the Wireless Systems
team of the Radio Systems Laboratory in
Berkeley, California. He has been leading and
contributing to research activities on the
design and integration of novel radio concepts
into cellular systems, including device-to-device communication, (coop-
erative) relaying and multiband operation. He was among the top ten
inventors in Nokia in the years 2008–2009 and received a Nokia top
inventor award in the years 2007–2008.He has been the proof-of-concept
subtask leader within the relaying task of the European research project
WINNER II and he has been leading the device-to-device communication
and network coding task within WINNER+. He has authored and co-
authored 25 + conference and journal articles, two book chapters and
more than 40 pending patent applications.

Ashwin Gumaste is currently the James R.
Isaac Chair and faculty member
in the
Department of Computer Science and Engi-
neering at the Indian Institute of Technology
(IIT) Bombay. He is currently also a consultant
to Nokia Siemens Networks, Munich where he
works on optical access
standardization
efforts. He was a Visiting Scientist with the
Massachusetts Institute of Technology (MIT),
Cambridge, USA in the Research Laboratory
for Electronics from 2008 to 2010. He was
previously with Fujitsu Laboratories (USA) Inc
in the Photonics Networking Laboratory (2001–05). He has also worked in
Fujitsu Network Communications R&D (in Richardson TX) and prior to

that with Cisco Systems in the Optical Networking Group (ONG). His
work on light-trails has been widely referred, deployed and recognized by
both industry and academia. His recent work on Omnipresent Ethernet
has been adopted by tier-1 service providers. Ashwin has 20 granted US
patents and over 30 pending patent applications. Ashwin has published
about 120 papers in referred conferences and journals. He has also
authored three books in broadband networks called DWDM Network
Designs and Engineering Solutions (a networking bestseller), First-Mile
Access Networks and Enabling Technologies and Broadband Services:
User Needs, Business Models and Technologies for John Wiley. Owing to
his many research achievements and contributions, Ashwin was awarded
the Government of India’s DAE-SRC Outstanding Research Investigator
Award in 2010 as well as the Indian National Academy of Engineering’s
(INAE) Young Engineer Award (2010). He has served Program Chair, Co-
chair, Publicity chair and workshop chair for IEEE conferences and as
Program Committee member for IEEE ICC, Globecom, OFC, ICCCN, Grid-
nets etc. Ashwin is also a guest editor for IEEE Communications Magazine,
IEEE Network and the founding Editor of the IEEE ComSoc ONTC’s
newsletter Prism. He is the Chair of theIEEE Communication Society’s
Technical Committee on High Speed Networks (TCHSN) 2011–2013. He
has been with IIT Bombay since 2005 where he convenes Networking
Laboratory (GNL): www.cse.iitb.ac.in/gnl. The Gigabit Networking Labo-
ratory has secured over 8 million USD in funding since its inception and
has been involved in 4 major technology transfers to the industry. Ashwin
can be reached through www.ashwin.name.

P. Rost
R. Boutaba
K. Doppler
A. Gumaste

Network Performance Monitoring for Applications Using EXPAND.

Bjö rn Landfeldt*, Aruna Seneviratne*, Bob Melander**, Per Gunningberg**

*Dept. of Electrical Engineering and Telecommunications
The University of New South Wales
Sydney 2052 Australia
bjornl@ee.unsw.edu.au
a.seneviratne@unsw.edu.au

**Information Technology, Dept. of Computer Systems
Uppsala University
P.O. Box 325, S-75105
Uppsala Sweden
[melander, perg]@docs.uu.se

Keywords: Network monitoring, wireless links, dynamic window generation, passive monitoring and active
probing

Abstract:
As  computing  becomes  more  interactive  and  as  networked  applications  need  special  system  support,  it
has  become  increasingly  important  to manage  network  resources.  An  important  part  of  the management
lies  within  monitoring  network  behaviour.  It  is  important  for  many  applications  to  be  able  to  estimate
system performance in order to select appropriate encoding schemes, adaptation, buffering sizes etc. This
paper  discusses  monitoring  within  wired  and  wireless  networks  and  the  type  of  monitoring  information
needed  to support different applications. We  suggest a hybrid active/passive monitoring approach with a
dynamic time window mechanism and interchangeable filters to extract requested information. The paper
also shows our initial experimental results and presents our conclusions.

B. Landfeldt A. Seneviratne, B. Melander, P. Gunningberg

Page 1

1  Introduction
A  range of new  applications  for use with  the  Internet has been  introduced  in  the  past  few  years.
Many  of  these  are  interactive  and  use  data  with  real  time  characteristics.  This  has  lead  to  a
diversification  in  the  way  the  Internet  is  used,  and  therefore  changed  the  requirements  of  the
network.
In  parallel,  there  has  been  a  rapid  growth  in  cellular  and  other  mobile  communication  systems
such  as  wireless  LANs,  and  infra  red  access  nodes.  Consequently,  it  is  clear  that  the  natural
evolution  is  to  combine  the  two  and  provide  a  wireless  Internet,  thus  providing  access  to
information  “a t anytime from anywhere” . The wireless Internet will comprise of a fixed IP based
core  network  and  multiple  overlaid  wireless  access  networks.  This  will  provide  users  with
simultaneous  access  to  several  different  networks  [14].  In  the  ideal  case,  this  will  provide  the
capability  of  matching  the  application  and  user  requirements  to  network  characteristics.  For
example,  a  user  can  interactively  request  a  file  download  over  a  low  latency  cellular  network,
and let the bulk data transfer occur over a high speed satellite link.
In  the  wired  Internet,  resources  can  be  reserved  to  satisfy  the  needs  of  applications.  There  are
several existing technologies capable of doing this, such as ATM, Intserv [15] and Diffserv [16].
However,  resource  availability  cannot be guaranteed  in  the wireless  Internet  since wireless  links
are  unreliable  by  nature.  The  resource  availability  is  not  only  governed  by  the  maximum  link
speed  and  the  traffic  load,  but  is  also  dependant  on  external  factors  such  as  channel  fading  and
noise interference. In this environment, it is important for applications to be able to determine the
status of the network  in order  to adapt  to  the current conditions. For example, a video player can
select  encoding  scheme  according  to  the  available  bandwidth.  It  is  therefore  important  for  these
applications  to  have  access  to  a  good  monitoring  tool  to  determine  the  characteristics  of  the
network.
A  number  of  network-monitoring  tools  have  been  proposed  recently  [2].  However,  they  have
been  primarily  aimed  at  fixed  Internet  environments.  Consequently,  they  are  not  suited  for  the
emerging  wireless  Internet  environment.  In  this  paper,  we  discuss  the  requirements  for
performing network monitoring  in wireless environments and suggest a framework  that provides
flexibility  in  statistical  processing  and  presentation  of  monitoring  information  to  enable  the
choice  of  network  and  the  configuration  of  applications.  The  rest  of  this  paper  is  organised  as
follows.  Section  2  presents  related  work  and  section  3  discusses  the  requirements  applications
will  have  on  a monitoring  tool.  Section  4  then  discusses  general  aspects  of  network  monitoring
and section 5 examines the relevance of the proposed monitoring framework. Section 6 describes
our experimental set-up and results and finally in section 7 we present our conclusions and future
work.

2  Related work
A  good  survey  of  network  monitoring  techniques  and  the  available  tools  can  be  found  in  [2].
These  monitoring  schemes  can  be  divided  into  two  categories,  namely  active  and  passive
monitoring.

2.1  Active monitoring schemes
Active  monitoring  schemes  operate  by  inserting  probing  traffic  into  the  network.  These  probes
are  used  to  extract  information  about  network  characteristics  such  as  available  and  bottleneck
bandwidth, round  trip delay etc. Bottleneck bandwidth  is  the bandwidth of  the  lowest bandwidth
link of the entire transmission path when  it  is  idle. Available bandwidth  is  the unused bandwidth
taken  over  the  path  and  it  varies  in  time  depending  on  cross  traffic.  The  available  bandwidth  is
always less than or equal to the bottleneck bandwidth.

B. Landfeldt A. Seneviratne, B. Melander, P. Gunningberg

Page 2

Bottleneck bandwidth can be detected by a  technique called back-to-back or packet-pair probing
[6]. This  technique measures  the  time  space  in-between  two  arriving  packets  that  are  sent  back-
to-back.  If  there are no other packets  in-between  the  two probing packets,  the  space corresponds
to  the  time  it  takes  for  the  second  packet  to  queue  until  the  first  packet  is  transmitted  on  the
slowest  link  in  the  transmission  path.  Therefore,  it  is  proportional  to  the  bottleneck  bandwidth.
Several  monitoring  tools  use  this  technique  [5,6,7,11].  For  example,  Pathchar  [7]  uses  this
method  in  combination  with  traceroute  [3],  in  order  to  derive  per-hop  information  about  the
network.
The  framework  presented  in  [5]  consists  of  two  parts.  The  first  part,  b-probe,  similarly  to  the
other  tools  above,  only  provides  information  about  the  bottleneck  bandwidth  using  packet-pair.
The  second  part,  c-probe,  provides  an  estimate  of  the  available  bandwidth  of  a  network.  This  is
done  by  sending  a  “ short  train”  (several  packets  send  back-to-back)  of  ICMP  echo  packets  and
measuring  the  time  in-between  the  first  and  last  packet,  that  will  eventually  be  received  at  the
sender side.
A  more  common  way  of  estimating  the  available  bandwidth  is  to  simulate  a  bulk  data  transfer.
Ttcp  [8]  makes  an  estimation  of  the  available  bandwidth  based  on  data  from  a  TCP  bulk  data
transfer. Treno [9] uses ICMP echo packets with flow and congestion control in order to simulate
a  TCP  bulk  data  transfer,  and  from  that  derive  the  available  bandwidth.  Compared  to  probing
that measures  the distance between packets, both  these  schemes  introduce  significant  traffic  into
the  network.  It  should  also  be  noted  that  these  methods  measure  the  average  bandwidth  over  a
period, and that the instantaneous bandwidth may vary significantly during this period.
The  advantage  of  using  the  bottleneck  bandwidth  is  that  the  value  stays  fixed  as  long  as  the
routing  path  is  not  changed.  The  disadvantage  is  that  the  bottleneck  value,  though  stable,  does
not  reflect  the  actual  load  on  a  link  and  thus,  may  be  an  over-optimistic  metric.  Furthermore,
bottleneck bandwidth estimation does not detect  any degradation  in  throughput  that might occur
due  to  heavy  CPU  load  in  a  server.  We  therefore  believe  that  the  available  bandwidth  is  more
useful  for  applications.  Bottleneck  bandwidth  can  be  used  for  routing  purposes,  choice  of
network  and  for  obtaining  background  statistical  information  that  can  be  used  for  estimating
bandwidth.
Active  monitoring  provides  an  accurate  method  of  determining  network  characteristics.
However,  the monitoring  traffic competes with application data  flows  for network  resources and
active  probing  is  therefore  not  scalable,  especially  for  resource  scarce  environments.
Furthermore,  the delays associated with obtaining necessary results may be  large,  thus making  it
unsuitable  for  use  with  reactive  QoS  managers  such  as  USA.  The  manager  reacts  to  user
dissatisfaction  and  need  to  present  configuration  suggestions  to  the  user  within  an  “ac ceptable ”
period of time.

2.2  Passive Monitoring
Passive  schemes  overcome  the  disadvantages  of  active  schemes  associated  with  overheads  and
delay  by  monitoring  streams  in  progress  and  from  them,  deriving  the  current  network  status,
rather  than  introducing  monitoring  traffic.  SPAND  [1]  is  a  monitoring  scheme  that  is  based  on
the  principles  of  passive  monitoring.  It  extends  the  basic  passive  monitoring  by  providing
facilities  for  sharing  of  measurement  results  among  hosts  in  order  to  increase  the  accuracy.
SPAND  operates  through  a  centralised  server  to  which  applications  may  report  the  measured
parameters  of  a  flow.  This  server  is  accessible  by  any  application  to  obtain  an  estimate  of  the
bandwidth  to  a  certain  host. The  tool  relies  on  the  assumption  that  hosts within  a  certain  region
are  likely  to have  the  same path  to a  remote host and hence  can  share  information. Furthermore,

B. Landfeldt A. Seneviratne, B. Melander, P. Gunningberg

Page 3

it assumes that network performance  is stable so  that reported results at  the server are valid for a
“ reasonable”  period of time.
Although  the  last assumption  is valid  in  the current  fixed  Internet environments  as  shown  in  [4],
it  is not valid  in  a wireless  Internet  environment. The  performance  of  a wireless  access  network
not  only  depends  on  the  current  load,  but  it  also  depends  on  other  factors  such  as  multi-path
fading,  other  forms  of  interference,  and  the  transmitted  power.  These  factors  in  turn  depend  on
the geographic  location of  the mobile host. Consequently,  sharing measurement  results collected
by  mobile  hosts  are  likely  to  lead  to  unreliable  estimates.  Therefore,  shared  passive  monitoring
cannot be directly used in the wireless Internet environment.

3  Monitoring for applications
A  network-monitoring  tool  should  be  versatile,  in  its  ability  to  present  information,  thus
supporting  different  applications  as  described  earlier.  Figure  1  shows  an  available  bandwidth
measurement,  which  illustrates  the  need  for  flexibility  when  presenting  the  results  to  the
application.

h
t
d
i
w
d
n
a
B

6

5

4

3

2

1

0

1 9

7
1

5
2

3
3

1
4

9
4

7
5

5
6

3
7

1
8

Bandwidth

Mean

Lower limit

3
1
1

1
2
1

9
2
1

7
3
1

5
4
1

3
5
1

1
6
1

9
6
1

7
7
1

5
8
1

3
9
1

9
8

7
9

5
0
1
Time

Figure 1. Example of varying available bandwidth

The  figure  shows  bandwidth  varying  with  a  “ bounded  noise ”  factor,  the  mean  value  of  the
bandwidth  and  an  effective  “ lower  limit ”  which  is  the  minimum  measured  bandwidth.  Assume
that  the  video  conferencing  tool  in  our  example  wants  an  estimate  of  the  available  bandwidth.
The mean  value might  be  a  good  estimate  if  the  application  is  adaptive  and  can  compensate  for
the  noise,  for  example  by  using  a  play-out  buffer.  If  not,  the  “ lower  limit ”  e stimates  the  least
available  bandwidth  the  application  will  be  expected  to  obtain  from  the  network  and  the
encoding scheme can be selected depending on this limit.
In order to highlight the need for versatility, table 1 gives a non-exhaustive list of the information
different  applications  may  require  from  a  network-monitoring  tool.  A  user  running  an  FTP
session may want to estimate the time of large file transfers and the associated monetary cost and
battery  usage.  The  mean  available  bandwidth  is  sufficient  for  this.  A  file  transfer  is  elastic  and
therefore  information  about  delay  and  jitter  become  irrelevant.  In  contrast,  when  using  a  web
browser,  the  response  time  becomes  one  of  the  significant  characteristics,  therefore  the
monitoring should indicate the expected packet delay.

B. Landfeldt A. Seneviratne, B. Melander, P. Gunningberg

Page 4

Type of application

Bandwidth Delay

Jitter MTU

Elastic

(FTP)

Yes

No

No

No

Elastic Interactive

Yes

Yes

No

No

(HTTP, Telnet)

Real Time

(Stored Video)

Yes

No

Yes

Yes

Real Time Interactive

Yes

Yes

Yes

Yes

(Video Conference)

Table 1. Example of information required by different application types

As  shown  in  table  1,  stored  video  players  need  different  information,  e.g.  mean  available
bandwidth  for  choosing  the  appropriate  encoding  and  information  about  jitter  in  order  to
determine  the  play-out  buffer  size.  Video  applications  may  want  to  know  the  path  Maximum
Transmission Unit, MTU in order to be able to use Application Layer Framing, ALF [10].

4  Hybrid Passive and Active Monitoring Framework
It  is  clear  from  the  above  discussion  that  neither  active,  nor  passive  monitoring  can  be  directly
used  for monitoring  a wireless  Internet. However,  it  is possible  to use a hybrid  scheme  that uses
passive  monitoring  for  the  wired  segments,  and  active  monitoring  for  the  wireless  segments
coupled  with  a  dynamic  windowing  mechanism  which  accounts  to  the  rapid  fluctuations.  The
proposed  scheme  is  such  a  hybrid  scheme,  which  provides  a  universal  monitoring  tool  for  the
emerging wireless Internet environment.

4.1  Hybrid Monitoring
In  the  proposed  scheme,  as  shown  in  Figure  2,  the  wired  and  wireless  parts  are  treated
independently.  The  passive  component  uses  an  extended  SPAND  server  -  EXPAND.  The
EXPAND  server  thus provides  support  for  active monitoring  of  the wireless  access  networks  as
described below, in addition to the passive monitoring of the fixed network segment.

Wireless Internet
Active Probing

Wired Internet
Shared Passive Monitoring

Wireless
Access Node

EXPAND

Mobile
Client

Internet

Filter

Data

Figure 2. The hybrid passive and active monitoring approach

B. Landfeldt A. Seneviratne, B. Melander, P. Gunningberg

Page 5

 The  hosts  attached  to  the  wireless  segments  will  send  active  probe  packets  to  the  EXPAND
server.  These  probe  packets  will  request  information  from  the  EXPAND  server  about  the
connection via  the wired  segment  to  the destination. The EXPAND  server will  respond with  the
requested  information.  The  request-response  packet  pair  in  turn  will  be  used  by  the  requesting
host as active probes  to determine  the  characteristics of  the wireless  access network. Finally,  the
requesting  host  will  use  the  results  form  the  EXPAND  server  about  the  fixed  segment  of  the
network,  and  the  information  about  the  wireless  segment  of  the  network,  gathered  from  the
request-response packet pair, to estimate the end-to-end characteristics of the connection.

4.2  Dynamic Windowing
The  raw measurement provided by  the  split-monitoring  scheme  in  itself,  is  insufficient  to obtain
a good estimate of the network characteristics. Firstly, in order  to obtain a good estimate  the  raw
data  needs  to  be  filtered  to  suit  the  application  requesting  the  data.  Secondly,  as  the
characteristics  of  the  wireless  access  segment  will  vary  significantly,  it  will  not  be  possible  to
use  simple  statistical  techniques.  Therefore,  the  proposed  scheme  uses  a  dynamic  windowing
mechanism  to  filter  the  raw  data  obtained  through  the  split-monitoring  scheme.  The  dynamic
windowing  scheme  exploits  the  fact  that  the  Internet  displays  quasi-stable  characteristics  [4].
This  will  be  true  even  for  the  wireless  networks  as  these  networks  will  be  used  to  access
information  from  stationary  locations,  rather  than  whilst  moving,  i.e.  from  a  meeting  room,  a
laboratory or a client ’ s premises. The emerging wireless LAN environment contains mechanisms
for  reducing  the  data  rate  when  the  signal  quality  drops,  thus  giving  different  link  speeds  at
different  locations  within  the  network  [12,  13].  The  quasi-stable  behaviour  also  applies  to
cellular networks. Thus, within  these networks,  the data  rate  stays  stable within a  region  and  the
only difference between the wireless and wired segments will be the period of stability.
Considering  this,  the proposed dynamic windowing  scheme  attempts  to determine  a  step  change
in the network characteristics, and then uses a simple statistical analysis of the data from the step
in order to determine the characteristics. This is schematically shown in Figure 3.

W
B

1800

1600

1400

1200

1000

800

600

400

200

0

ty-t1

tx-t0

t1

ty

t0

tx

1 4 7

0
1

3
1

6
1

9
1

2
2

5
2

8
2

1
3

4
3

7
3

0
4

3
4

6
4

9
4

2
5

5
5

8
5

1
6

4
6

7
6

0
7

3
7

6
7

9
7

2
8

5
8

8
8

1
9

4
9

7
9

0
0
1

Time

Figure 3. Dynamic window data regions

At  time  t =  tx, 0 <  tx <  t1,  the data  set used  for calculation  is between  tx and  t0. At  time  ty;  t1  <  ty  ,
i.e. when the step change has been detected, the data set used for the calculation is between ty and
t1 and the data between t0 and t1 is discarded.

B. Landfeldt A. Seneviratne, B. Melander, P. Gunningberg

Page 6

Considering  the above,  the windowing  scheme consists of  two parts. A mechanism  for detecting
step  changes  in  instantaneous  measured  data  and  a  statistical  filtering  mechanism.  The  step
changes are determined by  the standard deviation of  the data normalised by  the mean value. The
standard  deviation  indicates  the  variability  of  the  measurements  and  will  therefore  reflect  the
step  change. The  normalisation  is  done  so  that  the  variability  can  be  quantified.  In  order  to  find
the step, we define a threshold value d, and compare  it against  the cumulative standard deviation
to mean ratio, i.e.
)
(
(cid:229) -
1
n
2
x
=-
i
1
1
i
(cid:229)
1
n
n
=
1
i
The  statistical  filtering  mechanism  then  filters  the  data  as  described  below  and  produces  an
approximation of the requested parameters, i.e. RTT, bandwidth or jitter.

A step change is found if 

 is true.

x

i

x

d

£

n

4.3  Filtering
The  filtering  phase  should  be  versatile  to  support  the  need  of  different  applications.  USA  will
need  a  good  estimate  of  the  available  bandwidth  and  delay.  Other  applications  might  need
different  information. For example, a video player might want  to know  jitter and mean  available
bandwidth  values  in  order  to  determine  the  size  of  the  play-out  buffer.  Interactive  applications
might want to know delay and an estimate of the periods with the lowest bandwidth for selecting
an  appropriate  encoding  scheme,  since  a  play-out  buffer  cannot  be  used  with  an  interactive
application.  Therefore,  in  the  proposed  scheme  the  filters  are  interchangeable,  and  specified  by
the requesting application as a part of the signalling while probing the wireless access network.
The  filters  will  approximate  the  data  set  within  the  region  found  by  the  dynamic  windowing
scheme.  The  region  will  typically  consist  of  a  trend  and  some  noise.  Since  the  traffic  over  the
Internet  is not yet  fully modelled,  it  is  impossible  to determine which  approximation  is  the most
suitable.  Initially  we  propose  to  approximate  the  data  set  with  a  curve,  using  the  least  square
method in order to take advantage of the extra information the trend provides.

5  Experimental results

L a p t o p   c o m p u t e r

W a v e L A N

L i n u x   P C

E t h e r n e t

R o u t e r   A u s t r a l i a

I n t e r n e t

E X P A N D

R o u t e r   S w e d e n

S e r v e r

Figure 4. The experimental test-bed

B. Landfeldt A. Seneviratne, B. Melander, P. Gunningberg

Page 7

To  verify  the  viability  of  the  above  framework  we  conducted  several  tests  within  the
experimental test-bed shown in Figure 4.
The  WaveLAN  network  covered  one  floor  of  the  Electrical  Engineering  building  at  the
University  of New  South Wales,  and  provided  varying  SNR  at  different  locations.  The wireless
tests  consisted  of  a  person  moving  with  a  laptop  within  the  wireless  LAN,  measuring  the
available  bandwidth  to  the  PC  from  fixed  locations.  The  wired  tests  were  carried  out  over  the
Internet between the PC in Australia and the server in Sweden

5.1  Wireless experiments
Our  wireless  tests  aimed  to  investigate  the  traffic  behaviour  over  the  wireless  link.  Figure  5
shows  a  typical  plot  of  available  bandwidth  measured  over  the  wireless  LAN  while  moving
around.  In  the  figure  the  client  moved  towards  the  cell  boundary  where  the  bandwidth  dropped
significantly.  The  client  then  turned  and  moved  towards  the  centre  again  and  the  bandwidth
rapidly  increased  to  the  maximum  value.  In  our  measurements,  we  could  consistently  see  this
behaviour.

Figure 5. Bandwidth measurements while moving in a wireless LAN.

Because of  the stable behaviour within a cell and rapid changes at  the boundaries,  the need  for a
windowing scheme is accentuated. The stable behaviour also means historical data is useful even
within a wireless network.

5.2  Dynamic window experiments
We  conducted  a  series  of  measurements  between  Australia  and  Sweden  in  order  to  investigate
the viability of  the dynamic window  scheme. Figure 6  shows one measurement of  the  round  trip
time  (RTT)  measured  between  Australia  and  Sweden  during  4  hours.  The  following  example
demonstrates the performance improvement of the dynamic window scheme.
 As  can  be  seen  the measurements  vary  significantly  with  time.  There  is  a  significant  change  at
around  8  am  when  most  people  come  in  to  work  and  start  to  read  mail,  newspapers  etc.  The
round  trip  time  reaches  a  peak  at  around  8.30  am  and  starts  to  decay  slowly  until  9  am  after
which it drops down to its previous level.
The figure shows RTT estimations using three different methods, mean value,  least square  linear
approximation  and  least  square  linear  approximation  using  the  dynamic  window  scheme.  In
order to find the edges of  the dynamic windows we used  the mean  to standard deviation formula
described above and compared it to a d-value of 0.15. Determining the appropriate d-value is still

B. Landfeldt A. Seneviratne, B. Melander, P. Gunningberg

Page 8

an open research issue. For now, our experiences show that a value between 0.15 and 0.20 makes
the scheme perform well.

)
s
m
(
 
e
m
i
T

1400

1200

1000

800

600

400

200

0

Region
extracted in
Figure 8

9 am

8 am

RTT
Leas t Square Dynamic  W indow
Leas t Square Full W indow
Mean Full W indow

1

2
1

3
2

4
3

5
4

6
5

7
6

8
7

9
8

0
0
1

1
1
1

2
2
1

3
3
1

4
4
1

5
5
1

6
7
8
9
6
7
8
9
1
1
1
1
Measurement

0
1
2

1
2
2

2
3
2

3
4
2

4
5
2

5
6
2

6
7
2

7
8
2

8
9
2

9
0
3

0
2
3

1
3
3

2
4
3

3
5
3

Figure 6. RTT estimation between Australia and Sweden using three different methods

Cumulative Error Dynam ic window

Cumulative Error Least Square

Cumulative Error Mean 

50000

45000

40000

35000

30000

r
o
r
r
E

25000

20000

15000

10000

5000

0

1

2
1

3
2

4
3

5
4

6
5

7
6

8
7

9
8

0
0
1

1
1
1

2
2
1

3
3
1

4
4
1

5
5
1

6
7
8
9
6
7
8
9
1
1
1
1
Measurement

0
1
2

1
2
2

2
3
2

3
4
2

4
5
2

5
6
2

6
7
2

7
8
2

8
9
2

9
0
3

0
2
3

1
3
3

2
4
3

3
5
3

Figure 7 Cumulative Error of estimated and actual RTT values

B. Landfeldt A. Seneviratne, B. Melander, P. Gunningberg

Page 9

The d-value of 0.15 results in the dynamic windowing scheme finding four different regions with
values 1-21, 22-141, 142-241  and  242-359. The  figure  clearly  shows  that  the  least  square  linear
approximation with dynamic windowing estimates the RTT better than the other two methods. In
order  to  highlight  the  performance  difference  of  the  three  methods,  Figure  7  shows  the
cumulative  error  between  estimated  and  actual  RTT  values.  Again,  the  least  square  linear
approximation with dynamic windowing performs much better than the other two methods.

5.3  Filtering experiments
We  have  made  preliminary  experiments  with  different  methods  for  filtering  the  data  after  we
have  applied  the  dynamic  window  scheme.  From  our  measurements,  we  have  found  that  the
traffic  over  the  fixed  Internet  show  quasi-stable  properties  as  stated  previously.  We  have  also
found  that  within  the  stable  periods  there  are  often  slowly  growing  or  decaying  trends.  These
trends  provide  additional  information  to  a  weighed  value,  when  estimating  parameters  such  as
RTT and bandwidth. We therefore ran experiments using different methods to see how well  they
captured  these  trends.  Our  measurements  show  that  a  median  or  mean  calculation  wrongly
estimates the value when there is a trend in the data set.

)
s
m
(
 
e
m
i
T

1400

1200

1000

800

600

400

200

0

Subplot, RTT

Mean

Least Square

Median

1 3 5 7 9

1
1

3
1

5
1

7
1

9
1

1
2

3
2

5
7
9
2
2
2
Measurement

1
3

3
3

5
3

7
3

9
3

1
4

3
4

5
4

7
4

9
4

1
5

Figure 8. RTT subplot

As  an  example,  Figure  8  shows  the  region  144-197  extracted  from  the  data  in  Figure  6.  The
figure  also  shows  the mean, median  and  least  squares  linear  estimation of  the RTT  value.  It  can
be seen  in  the figure how  the median and mean calculations underestimate  the  trend whereas  the
linear  approximation  better  captures  it.  Until  the  Internet  traffic  has  been  mathematically
modelled properly,  it  is  impossible  to determine  the best method of estimation. We can however
conclude  that  mean  and  median  values  are  not  suitable  for  estimating  values  such  as  RTT  and
bandwidth over a period since they do not accurately capture the trends.

B. Landfeldt A. Seneviratne, B. Melander, P. Gunningberg

Page 10

6 
 Conclusion and future work
In  this paper, we discussed how network monitoring  should be  carried out  in  a mixture of wired
and  wireless  environments.  The  purpose  of  this  discussion  is  to  form  guidelines  for  an  overall
framework  for  network  monitoring  taking  into  account  the  different  needs  of  different
applications.  We  have  proposed  a  split  network  monitoring  approach  with  passive/active
monitoring. Central to this framework is the method of dynamic windowing to find a region with
a  relevant  set  of  data  on  which  to  conduct  statistical  computations. We  have  also  discussed  the
need  for  flexible  filtering  of  the  monitoring  information  in  order  to  derive  the  different
information  needed  by  different  applications.  Finally,  we  have  conducted  initial  experiments  to
verify our framework and to show the advantages of the dynamic window scheme.
Our  experimental  results  show  that  using  the  dynamic  windowing  scheme  gave  consistently
better  results  than  using  instantaneous  measurements  or  fixed  windows.  Instantaneous
measurements  are  not  reliable  when  there  is  a  large  fluctuation  in  the  measurements,  and  it  is
impossible  to  determine  a  correct  size  for  the  fixed  window. We  also  believe  that  the  dynamic
windowing  scheme  is  sufficient  to  determine  the  state  of  the  network  and  to  extract  appropriate
regions.  We  therefore  conclude  that  the  scheme  is  a  universal  tool  that  applies  to  all  network
types.
In the future, we intend to focus on statistical computations of monitoring results. Firstly, we will
investigate  possible  ways  of  determining  the  optimal  time  window  size  for  filtering  transient
events  while  still  reacting  on  sustained  bandwidth  shifts.  The  combination  of  statistical
properties  and  application  requirements  will  be  of  primary  interest.  Secondly,  we  will  work  on
statistical  methods  for  use  in  different  filters  in  order  to  provide  accurate  and  for  applications
appropriate  estimates  of  the  performance  parameters.  Thirdly,  we  will  work  on  implementation
issues  such  as  a  protocol  between  a  terminal  and  a  SPAND  server  for  specifying  filter  options
etc.  Finally,  we  intend  to  implement  a  prototype  of  the  framework  in  order  to  evaluate  its
usefulness.
Acknowledgements
The  authors  would  like  to  acknowledge  Ericsson  Radio  Systems  AB,  Sweden  and  Ericsson
Australia for their financial support.
References
[1]
S. Seshan, M. Stemm and R. Katz. “ SPAND:Shared Passive Network Performance
Discovery” . In Proc 1st Usenix Symposium on Internet Technologies and Systems
(USITS '97) Monterey, CA December 1997.

[2]

http://heplibw3.slac.stanford.edu/xorg/nmtf/nmtf-tools.html.

[3] W. R. Stevens, TCP/IP Illustrated Volume 1 Addison-Wesley, Reading MA, Nov 1994.

[4]

[5]

[6]

V. Paxon. “ Measurements and Analysis of End-to-End Internet Dynamics. PhD thesis, U.
C. Berkeley, May 1996.

R. L. Carter and M. E. Crovella.  “ Measuring bottleneck-link speed in packet switched
networks ” . Technical Report BU-CS-96-007, Computer Science Department, Boston
University, March 1996.

S. Keshav. “ Packet-Pair Flow Control ” . IEEE/ACM Transactions on Networking, Feb
1995.

B. Landfeldt A. Seneviratne, B. Melander, P. Gunningberg

Page 11

[7]

[8]

[9]

Pathchar –  A Tool to Infer Characteristics in Internet Paths. ftp://ee.lbl.gov/pathchar,
1997.

IEEE RFC 1470 June 1993

M. Mathis and J. Mahdavi “ Diagnosing Internet Congestion with a Transport Layer
Performance Tool ” . In Proc. INET  ’ 96, Montreal Canada, June 1996.

[10] D. Clark and D. Tannenhouse  “ Architectural considerations for a new generation of
protocols ” . In SIGCOM ’ 90 Philadelphia Pennsylvania September 1990.

[11] K. Lai and M. Baker, "Measuring Bandwidth." Proceedings of IEEE Infocom' 99, March
1999.

[12] www.3com.com/promotions/wireless

[13] www.wavelan.com/products

[14] R. Katz and E. Brewer,  “ The Case for Wireless Overlaid Networks ” , SPIE Multimedia
and Networking Conference (MMNC ’ 96) San Jose January 1996.

[15] R. Braden et al. “ Integrated Services in the Internet Architecture: an Overview ” , RFC
1633, ISI, MIT and PARC, June 1994.

[16] K. Nichols, V. Jacobson and L. Zhang,  “ A two-bit Differential Services Architecture for
the Internet", IETF Internet draft November 1997.

B. Landfeldt A. Seneviratne, B. Melander, P. Gunningberg

Page 12

E the rne t : D is t r ibu ted Pac ke t Sw i tch ing fo r
Lo ca l Compu te r Ne two rks

by Robe r t M. Me tca l fe and Da v id R. Bogg s

CSL ·75 ·7 May 1975 , rep r in ted Feb rua r y 1980

Abs t rac t : E therne t is a b ranch ing broadcas t commun ica t ion system for ca r ry ing d ig i ta l da ta
packets among loca l ly d is t r ibu ted compu t ing stat ions.
The packe t
t ranspo r t mechan ism
provided by E the rne t has been used to bu i ld systems wh ich can be viewed as e i the r loca l
loose ly coup led mu lt iprocessors.
compu te r ne two rks o r

An Ethernet's sha red commun ica t ion facility, its Ether, is a passive b roadcas t med ium w i th no
centra l control. Coo rd ina t ion o f access to the E the r fo r packe t b roadcas ts is d is t r ibu ted
among the con tend ing transm i t t ing s ta t ions using con t ro l led stat ist ica l a rb i t ra t ion . Sw i tch ing
o f packets to the i r des t ina t ions on the Ether is d is tr ibu ted among the rece iv ing s ta t ions us ing
packe t address recogn i t ion .

Design pr inc ip les and imp lementat ion are descr ibed based on exper ience .with an opera t ing
E the rne t o f1 00 nodes a long a k i lome ter o f coax ia l cable.
A model
fo r est imat ing
performance unde r heavy loads and a packet p ro toco l fo r error-con tro l led commun ica t ion are
inc luded for comp leteness.

A version of th is pape r appeared in Commun ica t ions o f the ACM, vol. 19 no. 7, Ju ly 1976.

CR Ca tego r ies : 3.81, 4.32, 6.35

Key wo rds and ph rases : compu te r networks, packe t sw i tch ing , mu l t iprocess ing , d is t r ibu ted
con t ro l , d is tr ibu ted compu t ing , b roadcas t commun ica t ion , stat ist ica l a rb i t ra t ion

XEROX
PALO ALTO RESEARCH CENTER
3333 Coyo te H i l l Road I Pa lo A l to I Ca l i fo rn ia 94304

ETI-IER~ET: DISTRIBCTED PACKET S\VITCH:Ii\G FOR LOCAL CO.\1PCTER NETWORKS

1

1. Background

One can characterize distributed computing as a spectrum o f activities varying in the degree o f
decentralization., with one extreme being remote computer networking and the other extreme being
multiprocessing. Remote computer networking is the loose interconnection o f previously isolated,
widely separated, and rather large computing systems. Multiprocessing is the construction o f
previously monolithic and serial computing systems from increasingly numerous and smaller pieces
computing in parallel. Near the middle o f this spectrum is local networking, the interconnection o f
computers
to gain the
resource sharing o f computer networking and the parallelism o f
multiprocessing.
The separation between computers and the associated bit rate o f their communication can be
used to divide .the distributed computing spectrum into b road activities. The product o f separation
and bit rate, now about 1 gigabit-meter per second (1 Gbmps),
is an indication o f the limit o f
current communication technology and can be expected to increase with time.

Activity

Separation

Bit Rate

Remote Networks >10 km

< .1 Mbps

Local Networks

10-.1 km

.1-10 Mbps

Multiprocessors

< .1 km

>10 Mbps

1.1 Remote COlnputer Networking

terminal-computer communication,
Computer networking evolved from telecommunications,
where the object was to connect remote terminals to a central computing facility. As the need for
computer-computer
interconnection
grew,
computers
themselves were
used
to
provide
communication [Baran, 1964; Rustin., 1972; Abramson, 1975]. Communication using computers as
packet switches [Roberts, 1970; Heart, 1970, 1973; Metcalfe 1973b] and communications among
computers for
resource sharing [Crocker, 1972; Thomas, 1973] were both advanced by the
development o f the Arpa Computer Network.
The Aloha Network at the University o f Hawaii was originally developed to apply packet radio
techniques for communication between a central computer and i ts terminals scattered among the
Hawaiian Islands [Abramson, 1970, 1975]. Many o f the terminals are now minicomputers
communicating among themselves using the Aloha Network's Menehune as a packet switch. The
Menehune and an Arpanet Imp are now connected providing terminals on the Aloha Network
access to computing resources on the U .S. ma~nland.

2

ETHER~ET: D ISTR IBLTED PACKET S'VITCHI~G FOR LOCAL CO : \1PL IER NETWORKS

Just as computer networks have grown across continents and oceans to interconnect major
computing facilities around the world, they are now growing down corridors and between buildings
to interconnect minicomputer~ in offices and laboratories [AshenhuFSt, 1975; Willard, 1973; Fraser,
1975~ Farber, 1973, 1975].

1.2 l~lulliprocessing

Multiprocessing first took the' form o f connecting an I /O controller to a large central computer;
IBM'S ASP is a classic example [Rustin, 1972]. Next, multiple central processors were connected to a
common memory to provide more power for compute-bound applications [Thornton, 1970]. Fo r
certain o f these applications, more exotic multiprocessor architectures such as Illiac .1V were
introduced [Barnes, 1968].
More recently minicomputers have been connected in multiprocessor configurations for
economy, reliability, and increased system modularity [Wuli: 1972; Ornstein, 1975]. The trend has
been toward decentralization for reliability; loosely coupled multiprocessor systems depend less on
shared central memory and more on thin wires for interprocess communication with increased
isolation [Metcalfe, 1972a, 1973b]. With the continued thinning o f interprocessor
component
cor:nmunication for reliability and the development o f distributable applications, multiprocessing is
form o f distributed computing.
gradually approaching a local

1.3 Local Computer Networking

local networks such as Mitre's Mitrix, Bell
Ethernet shares many objectives with other
Telephone Laboratory's Spider, and V.C. Irvine's Distributed Computing System (nes) [Willard,
1973; Fraser, 1975; Farber, 1973, 1975]. Prototypes o f all four local networking schemes operate at
rates between one and three megabits per second. Mitrix and Spider have a central
bit
minicomputer for switching and bandwidth allocation while DCS and Ethernet use distributed
control. Spider and Des use a ring communication path, Mitrix uses off-the-shelf CATV technology
to implement two one-way busses, and our experimental Ethernet uses a branching tw'o-way passive
bus. Differences among these systems are due to differences among their intended applications,
differences among the cost constraints under which trade-offs were made, an d differences o f opinion
among researchers.
Before going into a detailed description o f Ethernet, we offer the following ,overview (see
Figure 1).

ETI-fER:\ET: DISTRIBL~TED P.-\CKET SW ITCH I : \G FOR LOCAL CO~1PL·TER NET \VORKS

3

Term inator

t----.... Repeater

Controller

Interface

E ther segment # 1

Fig.l. A two-segment Ethernet.

4

ETHER~ET: D ISTR IBCTE 'D PACKET SWITCHI!'\G FOR LOCAL COMPCTERNETWORKS

2. System Summary

Ethernet is a system for local communication among computing stations. Ou r . experimental
Ethernet uses. tapped coaxial cables to carry variable-length digital data packets among.' for example,
personal minicomputers, printing facilities, large file storage "devices, magnetic tape backup stations,
larger central computers, and longer-haul communication equipment.
The shared communication facility,. a branching Ether, is passive. A station's Ethernet interface
~onnects bit-serially through an interface cable to a transceiver which in t um taps into the passing
Ether. A packet is broadcast onto the Ether, is heard by all stations, .and is copied from the Ether
by destinations which select it according to the packet's leading address bits. This is broadcast
packet sVt'itching and should be distinguished from store-and-forward packet switching in which
routing is perfonned" by intermediate processing elements. To handle the demands o f growth, an
Ethernet can be extended using packet repeaters for signal regeneration, packet filters for traffic
localization, and packet gateways for
internetwork address extension.
Control is completely distributed among stations with packet transmissions coordinated through
statistical arbitration. Transmissions initiated by a station defer to any which may already be in
progress. Once started, i f interference with other packets is detected, a transmission is aborted and
rescheduled by its source station. After a certain period o f interference-free tr,ansmission, a packet
is heard by all stations and will run to completion without interference. Ethernet controllers in
colliding stations each generate random retransmission intervals to avoid repeated collisions. The
mean o f a packet's retransmission intervals is adjusted as a function o f collision history to keep
Ether utilization near
the optimum with changing network load.
Even when transmitted without source-detected interference, a packet may still ,no t reach its
destination without error; thus, packets are delivered only with high probability. Stations requiring a
residual error rate lower than. that provided by the bare Ethernet packet transport mechanism must
follow mutually agreed upon packet protocols.

3. Design Principles

Our object is to design a communication system which can grow smoothly to accommodate
several buildings full o f personal computers and the facilities needed for
their support.
Like the computing stations to be connected, the communication system must be inexpensive.
We choose to distribute control o f the communications facility among the communicating computers
to eliminate the reliability problems o f an active central controller, to avoid creating a bottleneck in
a system rich in parallelism, and to reduce the fIXed costs which make small systems uneconomical.
Ethernet design started with the basic idea o f packet collision and retransmission d.eveloped in
the Aloha Network [Abramson, 1970]. We expected that, like the Aloha Network, Etherllets would
carry bursty traffic so that conventional, synchronous time-division multiplexing (STDM) would be
inefficient [Roberts, 19io; Abramson, 1970, 1975; Metcalfe, 1973b]. We saw promise in the Aloha

ETHER~ET: DISTRIBUTED P.~CKET SWITCHIXG FOR LOCAL CO~1Pl;lER NETWORKS

5

o\\'n~

approach to distributed control o f radio channel multiplexing and hoped that it could be applied
effectively with media suited to local computer communication. With several innovations o f our
the promise .is
realized.
Ethernet is named for the historicallunliniferous ether through which electromagnetic radiations
were once alleged to propagate. Like an Aloha radio transmitter, an Ethernet transmitter broadcasts
completely-addressed transmitter-synchronous bit sequences called packets onto the Ether and hopes
that they are heard by the intended receivers. The Ether is a logically passiye medium for the
propagation o f digital signals and can be constructed using any number o f media including coaxial
tcables,
twisted pairs, and optical
fibers.

3.1 Topology

We cannot afford the redundant connections and dynamic routing o f store-and-forward packet
switching to assure reliable communication, so we choose to achieve reliability through simplicity.
We choose to make the shared communication facility passive so that
the failure o f an active
element will tend to affect the communications o f only a single station. The layout and changing
needs o f office and laboratory buildings leads us to pick a network· topology with the potential for
convenient
incremental extention and reconfiguration with minimal service disruption.
I t i s a l r e e so tha t the E ther can
The topology o f the Ethernet is that o f an unrooted tree.
branch at the entrance to a building's corridor, yet avoid multipath interference. There must be
only one path through the Ether between any source and destination; i f more than one path were to
exist, a transmission would interfere with itself: repeatedly arriving at its intended destination having
travelled by paths o f different length. The Ether is unrooted because it can be extended from any
o f its points in any dire~tion. Any station wishing to join an Ethernet taps into the E ther at the
nearest convenient point.
Looking at the relationship o f interconnection and control, we see. that Ethernet is·the dual o f a
star network. Rather than distributed interconnection through many separate links and central
control in a switching node, as in a star network, the Ethernet has central interconnection through
the Ether and distributed control among its stations.
Unlike an Aloha Network which is a star network with an outgoing broadcast channel and an
incoming multi-access channel, an Ethernet supports many-to-many communication with a single
broadcast multi-access channel.

3.1 Control

Sharing o f the E ther is controlled in such a way that it is not· only possible. bu t probable that
two or more .stations will attempt to transmit a packet at roughly the same time.
.Packets which
overlap in time on the Ether are said to collide; they interfere so as to be unrecognizable by a
receiver. A station recovers from a detected collision by abandoning the attempt and retransmitting

6

ETHER~ET: DISTRIBCTED PACKET SWITCIII~G FOR LOCAL COMPCTER NETWORKS

Arbitration o f conflicting

the packet after
some dynamically"'chosen random time period.
transmission demands is both distributed and statistical.
When the Ether is largely unused; a station transmits its packets ,at will,
the packets are
received without error, and all
the rate o f packet
is well. As more stations begin to transmit,
intetference increases.
Ethernet controllers in each station are built
to adjust
the mean
retransmission interval in proportion to the frequency o f collisions; sharing o f the Ether among
competing station-station transmissions is thereby kept near the optimum [Metcalfe, 1973a, 1973b].
A degree o f cooperation among the stations is required to share the Ether equitably.
In
demanding applications certain stations might usefully take transmission priority through some
systematic violation o f equity rules. A station could usurp the Ether by not adjusting its
retransmission interval with increasing traffic or by sending very large packets. Both practices are
now prohibited by low ... level software in each staqon.

3.3 Addressing

Each packet has a source and destination, both o f which are identified in the packet's header.
A packet placed on the E ther eventually propagates to all stations. Any station can copy a packet
from the Ether into its local memory, bu t normally only an active destination station matching 'its
address in the packet's header will do so as the packet passes. By convention, a zero destination
address is a wildcard and matches all addresses; a packet with a destination o f zero is called a
broadcast packet.

3.4 Reliability

Packets may be lost due to interference with other packets,
An Ethernet is probabilistic.
impulse noise on the Ether, an inactive receiver at a packet's intended destination, o r purposeful
discard. Protocols used to communicate through an Ethernet must assume that packets will be
received correctly at
intended destinations only with high probability.
An Ethernet gives its best efforts to transmit packets successfully, bu t ·it is the responsibility ,of
processes in the source and destination stations to take the precautions necessary to assure reliable
communication o f the quality they themselves desire [Metcalfe, 1972a, 1973b]. Recognizing the
costliness and dangers o f promising "error"'free" communication, we refrain from guaranteeing
reliable delivery o f any single packet to get both economy o f transmission and high reliability
averaged over many packets
[Metcalfe, 1973b].
Removing the responsibility for
r~liable
communication from the packet t ranspon mechanism allows us to tailor reliability to the application
and to place error recovery where it will do the most good. This policy becomes more important as
Ethernets are interconnected in a hierarchy o f networks through which packets must travel farther
and suffer greater
risks.

ETHER~ET: DISTRIBCTED PACKET SWITCHING FOR LOCAL CO~1PUTER NETWORKS

7

3.5 AIechanisnzs

A station connects to the Ether \vith a lap and a transceiver. A tap is a device for phyically
connecting to the Ether while disturbing its transmission characteristics as little as possible. The
design o f the transceiver must be an exercise in paranoia. Precautions mus t be taken to insure that
likely failures in the transceiver o r station do not result in pollution o f the Ether.
In particular,
to disconnect'· from the Ether.
from the transceiver should cause it
removing power
Five mechanisms are provided in our experimental Ethernet for reducing the probability and
cost o f losing a packet. These are (1) carrier detection, (2) interference detection, (3) packet error
filtering, and (5) collision consensus enforcement.
truncated packet
(4)
detection,

3.5.1 Carrier Detection. As a packet's bits are place~ on the Ether by a station, they are phase
encoded, (like bits on a magnetic tape) which guarantees· that there is a t least one transition. on the
Ether during each bit time. The passing o f a packet on the Ether can therefore be detected by
listening for its transi:ions. Tc us·e . l radio analogy, we speak o f the presence o f ca"ier as a packet
passes a transceiver. Because a station can sense the carrier o f a passing packet, it can delay sending
one o f its own until the detected packet passes safely. The Aloha Network does not have carrier
detection and consequently suffers a substantially higher collision rate. Without carrier detection,
efficient use o f the Ether would decrease with increasing packet length.
In Section 6 below, we
show that with carrier detection, Ether efficiency increases with increasing packet
length.
With carrier detection we are able to implement deference: no 'station will start transmitting
while hearing carrier. With deference comes acquisition: once a packet transmission has been in
progress for an Ether end-to-end propagation time, all stations are hearing carrier and are deferring;
the Ether has been acquired and the transmission will complete without an interfering collision.
With carrier detection, collisions should occur only when two or mo re stations find the Ether
silent and begin transmitting simultaneously: within an Ether end-to-end propagation time. This
will almost always happen immediately after a packet
transmission during which two o r more
stations were deferring.
Because stations do no t now randomize after deferring, when the
the waiting stations pile on together, collide, randomize, and retransmit.
transmission terminates,

Interference is
3.5.2
Each transceiver has an interference detector.
Interference Detection.
indi~ated when the transceiver notices a difference between the value o f the bit it is receiving from
the Ether and the value o f the bit
is attempting to transmit.
it
Interference detection has three advantages. First, a station detecting a collision knows that its
packet has been damaged. The packet can be scheduled for retransmission immediately, avoiding a
interference periods on the Ether are limited to a
long acknowledgement
Second,
timeout.
maximum o f one round trip time. Colliding packets in the the Aloha Netw~rk run to completion,
bu t the truncated packets resulting from Ethernet collisions waste only a small fraction o f a packet
time on the Ether. Third, the frequency o f detected interference is used to estimate E the r traffic for
adjusting retransmission intervals and optimizing channel efficiency.

8

ETHER~ET:' DISTRIBCTED PACKET SWITCHI~G FOR LOCAL CO~1PLTER NETWORKS

3.5.3 Packet Error Detection. As a packet is placed on the Ether, a checksum is computed and
appended. As the packet is read from the Ether, the checksum is recomputed. Packets which do
not carry a ~onsistent checksum are discarded.
In this way transmission errors, impulse noise errors~
and errors due to undetected interference are caught at a packet's destination.

3.5.4 Truncated Packet Filtering.
Interference detection and deference cause most collisions to
in truncated packets o f only a few bits; colliding stations detect
interference and 'abort
result
transmission within an Ether round-trip time. To reduce the processing lOad that the rejection o f
such obviously damaged packets would place on listening station software, truncated packets are
in hardware.
filtered out

its transmission is
3.5.5 Collision Consensus Enforcement. When a station determines that
experiencing interference, i t momentarily jams the Ether to insure that all other participants in the
collision will detect interference and, because o f deference, will be forced to a bo r t Without this
collzsion consensus enforcement nlechanism, it is possible that the transmittil1g station which would
otherwise be the last to detect a collision might not do so as the other interfering transmissions
successively abort and stop interfering. Although the packet may look good to that last transmitter,
different path lengths between the colliding transmitters and the· intended receiver will cause the
packet
to arrive damaged.

4. Implementation

Our choices o f 1 kilometer, 3 megabits per second, and 256 stations for the parameters o f an
experimental Ethernet were
based on characteristics o f
the
locally-distributed computer
communication environment .and ou r assessments o f what would be marginally achievable;
they
were certainly no t hard restrictions essential
to the Ethernet concept.
We expected that a reason~ble maximum network size would be on the order o f 1 kilometer o f
cable. We used this working number to choose among Ethers o f varying signal attenuation and to
design transceivers w i th appropriate power and sensitivity.
The dominant station on our experimental Ethernet is a minicomputer for which 3 megabits
per second is a convenient data transfer rate. By keeping the peak rate well below that o f the
,we reduce the need for expensive special-purpose packet
computer's path to main memory,
buffering in ou r Ethernet interfaces. By keeping the peak rate as high as is convenient, we provide
for larger numbers o f stations and more ambitious multiprocessing communications applications.
To expedite low-level packet handling among 256 stations, we allocate the first 8-bit byte o f the
packet to be the destination address field and the second byte to be the source address field (see
Figure 2).
256 is a number small enough to allow each station to get an adequate share o f the
available bandwidth and approaches the limit o f what· we can achieve with current techniques for
tapping cables. 256 is only a convenient number for the lowest level o f protocol; higher levels can

ETHER~ET: DISTRIBUTED PACKET S\VITCHIXG FOR LOCAL COMPUTER NETWORKS

9

accomodate ex tended address spaces with additional
interpret
them.
Ou r experimental E therne t
implementation has four major parts:
interfaces, and controllers (see Figure 1).

fields inside the packet and software to

the· Ether,

transceivers,

4.1 Ether

We chose to implement ou r experimental Ether using low-loss coaxial cable with off-the-shelf
CATV taps and connectors.
I t is possible to mix E the rs on a single E therne t ; we use a smaller(cid:173)
diameter coax for convenient connection within station clusters and a larger-diameter coax for low(cid:173)
loss runs between clusters. The cost o f coaxial cable E t h e r is insignificant relative to the cost o f the
distributed computing systems suppor ted by Ethernet..

4.2 Transceivers

transceivers can drive a kilometer o f coaxial cable E the r tapped by 256
Ou r experimental
Th e transceivers c an endure (Le., work after)
stations transmitting a t 3 megabits per second.
imp rope r termination o f the Ether, and simultaneous dr ive by all 256
sustained direct shorting,
stations; they can tolerate (Le., work during) ground differentials and everyday electrical noise, from
typewriters o r electric drills, encoun tered when stations are separated by as much as a kilometer.
An E therne t transceiver attaches directly to the E th e r which passes by in the ceiling o r und e r
is powered and controlled through 5 twisted pairs in an interface cable carrying
the floor.
I t
transmit data, receive data, interference detect, and powe r supply voltages. When unpowered, the
transceiver disconnects itself electrically from the Ether. Here is where ou r fight for reliability is
won o r lost; a b roken transceiver can, bu t should not, b r ing down an en t i re Ethernet. A watchdog
timer circuit in each transceiver attempts to prevent pollution o f the E th e r by shu t t ing down the
ou tpu t stage i f i t acts suspiciously. Fo r transceiver simplicity we use the Ether's base frequency
band, bu t an E therne t could b e bu i l t
to use any suitably sized b and o f a frequency division
multiplexed Ether.
Even though ou r experimental transceivers are very s imp le and c an tolerate only limited signal
attenuation, they have proven qu i te adequa te and reliable. A more sophisticated transceiver design
m igh t permit. passive branch ing o f the E ther and wider station separation.

4 .3 In ter f«e

. An E the rne t interface serla1izes and deseria11zesthe parallel da ta u s ed -by Its station. There are
. a numbe r .o f different stations on ou r Ethernet; an interface mu s t b e bu i l t :for each kind.

10

I:~TI--IER::\ET: DISTRIBCTED PACKET SWITCHI:\G FOR LOCAL C,O~1PUTER NETWORKS

Each interface is equipped with the hardware necessary to compute a 16-bit cyclic redundancy
checksum (cRe) on serial data as it
is transmitted and received. This checksum only protects
against errors in the Ether and specifically not errors in the p'arallelportions o f the interface
hardware or station. Higher-level software checksums are recommended for applications in which·a
higher degree o f reliability is required.
A transmitting interface uses a packet buffer address and word coun t to serialize and phase
encode a variable number o f 16-bit words which are taken from the station's memory and passed to
the transceiver, preceded by a start bit (called SYNC in Figure 2) and followed by the CRC. A
receiving interface uses the appearance o f carrier to detect the start o f a packet and uses the SYNC
bit to aquire bit phase. As long as carrier stays on,
the interface decodes and deserializes the
incoming bit stream depositing 16-bit words in a packet buffer in the station's main memory. When
carrier goes away, the interface checks tha t an integral number o f 16-bit words has been received
and that the eRC is correct. The last word received is assumed to be the eRe and is no t copied into
the packet huffer.
These interfaces ordinarily include hardware for accepting only those packets with appropriate
addresses in their headers. Hardware address filtering helps a station avoid burdensome software
is very busy carrying traffic intended for other stations.
packet processing when the Ether

Accessible to software

s
Dest
Source
y
N Address Address
c

Data

Checksum

1 bit

8 bits

8 bits

__ __ _______y~---------.",..J.--~y--~
o - 4000 b its
16 b its

Fig. 2. Ethernet packet layout.

4.4 Controller

An Ethernet controller is the station-specific low-level firmware o r software for getting packets
onto and out o f the Ether. When a source-detected collision occurs,
i t is the source controller's
'responsibility to generate a new random retransmission interval based on the updated, collision
count. We have studied a number o f algorithms for controlling retransmission rates in stations to
.
-
maintain Ether efficiency [Metcalfe, 1973a, 1974]. The more practical o f the'se algorithms esti.mate
traffic load using recent collision history.

ETHER:\ET: DISTRIBl~TED PACKET SWITCHING FOR LOCAL COl\1PUTER, NETWORKS

11

Retransmission intervals are multiples o f a slot,
the maximum time between starting a
transmission and detecting a collision, one ·end-to-end round trip delay. An Ethernet controller
begins transmission o f each new packet with a mean retransmission interval o f one slot. Each time
a transmission attempt ends in collision, the controller delays for an interval o f random length with
a mean twice that o f the previous interval, defers to any passing packet, and then attempts
This heuristic approximates an algorithm we have called Binary Exponential
retransmission.
Backoff (see Figure 3) [Metcalfe, 1974].

Zero load es t ima te
for new packe t

Generate random
numbe r

yes

E r ro r

Increase load
estimate

Weight by load
es t ima te

Decremen t we igh ted
random numbe r

co l l is ion

Transm i t when
E ther s i len t

t imeou t

ok

Done

Fig. 3. Collision control algorithm.

12

ETHER~ET: D lSTR IBCTED PACKET SW ITCH l i \G FOR LOCAL CO~1PUTER NETWORKS

When the network is unloaded and collisions are rare, the mean seldom departs from one and
retransmissions are p romp t As the traffic load increases, more collisions are experienced, a backlog
o f packets builds up in the stations, retransmission intervals increase, and retransmission traffic
backs off to sustain channel efficiency.

5. Growth

5.1 Signal Cover

One can expand an Ethernet just so far by adding transceivers and Ether. At some point, the
transceivers and Ether will be unable to carry the required signals. The signal cover can be
extended with a simple unbuffered packet repeater.
In ou r experimental Ethernet where, because· o f
transceiver simplicity the Ether : an n o t be branched passively, a simple repeater may joL'l any
number o f Ether segments to enrich the topology while extending the signal cover.
We operate an experimental two-segment packet repeater, but hope to avoid relying on them.
In branching the Ether and extending its signal cover,
there is a trade-off between using
sophisticated. transceivers and using repeaters. With increase4 power and sensitivity, transceivers
become more expensive and less reliable. The introduction o f repeaters into an Ethernet makes the
centrally interconnecting Ether active. The failure o f a transceiver will sever the communications o f
its owner;
the failure o f a repeater partitions the Ether severing many communications.

5.2 Traffic Cover

One can expand an Ethernet ju s t so far by adding Ether and packet repeaters. At some point
the Ether will be so busy that additional stations will jus t divide more finely the already inadequate
bandwidth: The traffic cover can be extended with an unbuffered tra~c-filtering repeater or packet
filter, which passes packets from one Ether segment to another only i f the destination station is
located on Ule new segment. A packet
filter also extends the signal cover.

5.3 Address Cover

One can expand an Ethernet just so far by adding Ether, repeaters, and traffic filters. A t some
point there will be too many stations to be addressed with the Ethernet's 8-bit addresses. The
address cover can be extended with packet gateways and the software addressing conventions they
implement [Cerf, 1974]. Addresses can be expanded in two directions: down into the station by
adding fields to identify destination ports or processes within a station, and up into the internetwork
by adding fields to identify destination stations on remote networks. A gateway also extends the
traffic and signal covers.

ETHER ! \ET : D ISTR IBL IED ,PACKET S\VITCIIING FOR LOCALCO~1PUTER NETWORKS

13

There can be only one repeater or packet filter connecting two Ether segments; a packet
repeated onto a segment ·by multiple repeaters would interfere with itself. However, there ,is no
limit to the number o f gateways connecting two segments; a gateway only repeats packets addressed
to itself as an intermediary. Failure o f the single repeater connecting two segments partitions the
network; failure o f a gateway need not partition the net i f there are paths through' other gateways
between the segments.

6. Performance

Here is a simple set o f formulas with which to characterize the performance expected o f an
Ethernet when it· is heavily loaded. More elaborate analyses and several detailed simulations have
been done, bu t the following simple model has proven very useful in understanding the Ethernet's
distributed contention scheme, even when it
is loaded beyond expectations [Abramson, 1970;
Metcalfe, 1973a, 1973b, 1974; Roberts, 1973; Murthy, 1975].
We develop a simple model o f the performance o f a loaded Ethernet by examining alternating
Ether time periods. The first, called a transmission interval, is that during which the Ether has been
acquired for a successful packet transmission. The second, called a contention interval,
is that
composed o f the retransmission slots o f Section 4.4, during which stations attempt to acquire control
o f the Ether. Because the model's Ethemets are loaded and because stations defer to passing
the slots are synchronized by the tail o f the preceding
packets before starting transmission,
acquisition interval. A slot will be empty when no station chooses to attempt transmission in it and
it will contain a collision·i f more than one station attempts to transmit. When a slot contains only
one attempted transmission,
then the Ether has been acquired for the duration o f a packet, the
contention interval ends, and a transmission interval begins.
Let P be the number o f bits in an Ethernet packet. Let C be the peak capacity in bits per
second, carried on the Ether. Le t T be the time in seconds o f a slot, the number o f seconds it takes
there are Q stations
to detect a collision after starting a transmission.
Let us assume that
continuously queued to transmit a packet; either the acquiring station has a new packet immediately
after a successful acquisition o r another station comes ready. Note that Q also happens to give the
total offered load on the network which for this analysis is always 1 or greater. We assume that a
queued station. attempts to transmit
in the current slot with probability l /Q , or delays with
this is known to be the optimum statistical decision rule, approximated in
probability 1-(1 /Q);
Ethernet stations by means o f ou r load-estimating retransmission control algorithms [Metcalfe,
1973a, 1973b].

14·

ETI-IER~ET: D ISTR IBL IED PACKET S \ \'ITCHING FOR LOCAL COl\1PCTER NETWORKS

6.1 Acquisition Probability

We now compute A, the probability that exactly one station attempts a transmission in a slot
and therefore acquires the Ether. A is Q* ( l /Q )*« I - ( l IQ » (Q - l » ; . there are Q ways in which one
station can choose to transmit . (with probability ( I I Q» while Q - l stations choose to wait (with
probability 1-(11Q».
Simplifying,

A = (1-(I/Q»(Q-1).

6.2 Wa it ing Time

We now compute W, the mean number o f slots o f waiting in a contention interval before a
successful acquisition o f the Ether by a station's transmission. The probability o f waiting no time at
all
the probability that one· and only one station chooses to transmit in the first slot
is just A,
fJllowing a tr3~smission. The probabi!ity o f waiting 1 slot is A*(I- A); tile probability o f waiting i
slots is A*« l - A ) i ) .
The mean o f this geometric distribution is

w= ( I -A ) IA .

6.3 Efficiency

that fraction o f time the Ether is carrying good packets, the efficiency.
We now compute E,
The Ether's time is divided between transmission intervals and contention intervals. A packet
transmission takes P IC seconds. The mean time to acquisition is Ul*T. Therefore, by ou r simple
model,

E = (P /C ) /«P IC )+ (U I *n ) .

Table I presents representative performance figures (Le., E) for our experimental E therne t with
the indicated packet sizes and number o f continuously queued stations. The efficiency figures given
do not account for inevitable reductions due to headers and control packets nor for losses due to
imprecise control o f the retransmission parameter I I Q;
the former is straightforwardly p rotoc 01(cid:173)
dependent and the latter requires analysis beyond the scope o f this paper. Again, we feel that all o f
the Ethemets in the table are overloaded; normally loaded Ethemets will usually have a Q ·much
less than 1 and exhibit behavior not covered by this model.
For OUf calculations, we use a C o f 3 megabits pe r second and aT o f 16 microseconds. The
slot duration, T ,mu s t be long enough to allow a collision to be detected o r a t least twice the Ether's
round trip time. We limit in software the maximum length o f ou r packets to be near 4000 bits to
keep the latency o f network access down and to perm i t efficient use o f station packet buffer storage.

ETHERXET: D ISTR IBCTED PACKET S\VITCHING FOR LOCAL CO~1PlJTER NETWORKS

15

Table I "E the rne t Efficiency.

Q

P=4096 P=1024 P=512 P=48

1

2

3

4

5

10

32

64

1.0000

1.0000

1.0000

1.0000

0.9884

0.9552

0.9143

0.5000

0.9857

0.9447

0.8951

0.4444

0.9842

0.9396

0.8862

0.4219

0.9834

0.9367

0.8810

0.4096

0.9818

0.9310

0.8709

0.3874

0.9807

0.9272

0.8642

0.3737

0.9805

0.9263

0.8627

0.3708

128

0.9804

0.9259

0.8620

0.3693

256

0.9803

0.9257

0.8616

0.3686

Fo r packets whose size is above 4000 bits, the efficiency· o f our experimental Ethernet stays well
F o r ·packets with. a ,size···approximating that o f a slot, Ethernet· efficiency
above 95 percent.
approaches l i e ,
the asymptotic efficiency o f a. slotted Aloha network [Roberts, 1973].

7. Protocol

There is more to the construction o f a viable packet communication system than simply
providing the mechanisms for packet transport. Methods' for error correction, flow control, process
naming, security, and accounting must also be provided through higher level protocols implemented
on top o f the Ether control protocol decribed in Sections 3 and 4 above [Ceri: 1974; Crocker, 1972;
Metcalfe, 1973b; Farber, 1975; Rowe, 1975; Walden, 1972]. Ether control includes packet framing,
error detection, addressing and multi-access control; like other line control procedures, Ethernet is
used to support numerous network and multiprocessor architectures [SDLC" 1974; SNA, 1975].
Here i s a b r ie f description o f one simple error-controlling packet protocol. The EF fP (Ethernet
File Transfer Protocol) is o f interest both because i t is relatively easy to understand and implement
correctly and because it has dutifully carried many valuable files during the development o f more
general and efficient protocols.

16

ETHERNET: DISTRIBL'TED PACKET SWITCHI~G 'FOR LOCAL COMPUTER NETWORKS

7.1 General Terminolog)"

In discussing packet protocols" we 'use the following generally. useful terminology. A packet is
said to have a source and a destination. A flow o f data is said to have a sender and a receiver,
recognizing that to support a flow o f data some p~ckets (typically acknowledgments) will be sourced
at the receiver and destined for the sender. A connection is said to have a listener and an initiator
It is very useful to treat these as orthogonal
and a service is said to have a server and a user.
descriptors o f the participants in a communication. O f course, a server is usually a listener and the
source o f data-bearing packets is usually the sender.

7.2 EFTP

The first 16 bits o f all Ethernet packets contain its interface-interpretable destination and source
station addresses, a byte each, in that order (see Figure 2). By software convention, the second '16
bits o f all Ethernet packets cOl1tain_ the packet type.' Different protocols use disjoint sets o f packet
types. The EF fP uses 5 packet types: data, ack, abort, end, and endreply. Following the 16-bit type
word o f an EF fP packet are 16 bits o f sequence number, 16 bits o f length, optionally some 16-bit
data words, and finally a 16-bit software checksum word (see Figure 4). The Ethernet's hardware
checksum is present only on the Ether and is not counted at
this level o f protocol.

Destination

Sou rce

Packet Type

Sequence Number

Leng th (in words)

~

~

Data (words)

~ h

Data
Packets
On ly

. So f tware Checksum

1-·-(.-.......- - - - - 16 b its - - - - - - -1 _ ....1

Fig. 4. EF fP packet layout.

ETHER~ET: DISTRIBCTED PACKET SWITCHI~G FOR LOCAL C01\1PUTER NETWORKS"

17

It· should b e obvious that little care has been taken to cram certain fields into jus t the right
number o f bits. The ., emphasis here is on simplicity and ease o f programming.
' Despite th'is
disclaimer, we do feel that it is more advisable to err on the side o f spacious fields; try as you may,
one field or another will always turn ou t
to be too small.
The software checksum word is used to lower the probability o f an undetected error.
It serves
not only as a backup for the experimental E theme f s serial hardware 16-bit cyclic redundancy
checksum (in Figure 2), bu t also for protection against failures in parallel da ta paths within stations
which are not checked by the CRee The checksum used by the EF fP is a 1's complement add an d
cycle over the entire packet, including header and con ten t data. The checksum can be ignored at
the user's peril a t either end; the sender may pu t all l ' s (an impossible value) into the checksum
that no checksum was computed.
word to indicate to the receiver

7.2.1 Data Transfer. The 16-bit words o f a file are carried from sending station to receiving station
in data packets consecutively numbered from O. Each .data packet is retransmitted periodically by
the sender until an ack packet with a matching sequence number is re turned from the receiver. Th e
receiver ignores all damaged packets, packets from a station other than the sender, and packets
whose sequence number does no t match either the expected one or the one preceding. When a
packet has the expected sequence numbe.r, the packet is acked, its data is accepted as pa r t o f the
file, and the sequence number is incremented. When a packet arrives with a sequence number on e
less than that. expected, it is .acknowledged and discarded; the presumption is that i t s ack was lost
and needs retransmission (Metcalfe, 1973b).

7.2.2 End. When all the data has been transmitted, an end packet is sent with the next consecutive
sequence number and then the sender waits for a matching endreply. Having accepted an end
packet in sequence, the data receiver responds with a matching endreply an d then dallys for some
reasonably long period o f time (10 seconds). Upon getting the endreply,
the sending station
transmits an echoing endreply and is free to go o f f with the assurance that
the file has been
transferred successfully. The dallying receiver then gets the echoed endreply and i t too goes o f f
assured.
The comparatively complex end-dally sequence is intended to make i t practically certain tha t
the sender and receiver o f a file will agree on whether the file has been transmitted correctly.
I f the
end packet is lost, the data sender simply retransmits i t as it would any packet with an overdue
I f the endreply from the data receiver is lost, the data sender will time o u t in
acknowledgement.
the same way and retransmit the end packet which will in tum be acknowledged by the dallying
I f the echoed endreply is lost, the dallying receiver will be inconvenienced having to wait
receiver.
for it, bu t when it has timed out, the receiver can nevertheless be assured o f suc-cessful transfer o f
the file because the end packet has bee-n received.
.At any time during all o f this, either side is free to decide communication has failed and ju s t
give up; it is considered polite to send an abort packe t to end the communication promptly In the

, 18

ETHERNET: DISTRIBUTED PACKET SWITCHING FOR LOCAL COMPUTER NET\VORKS

event

-of: '. say, a user-initiated abort or a file system error.

7.23 "EFTP Shortcomings. The EF fP has been very useful, but its shortcomings are many_ First,
the protocol provides only for
file ·transfer from station to station in a single network and
specifically not from process· to process within stations either on the .same network or through a
gateway. Seconq, process rendezvous is degenerate in that there are no mechanisms for finding
processes by name or. for convenient handling o f multiple users by a single server. Third, there is
no real flow control.
I f data arrives at a receiver unable to accept it into its buffers, the data can
simply be thrown away with complete assurance that it will be retransmitted eventually. There is no
\yay for a receiver to quench the flow o f such wasted transmissions or to expedite retransmission~
Fourth, data is transmitted in integral numbers of 16-bit words belonging to unnamed files and thus
the EF fP is either terribly restrictive or demands some nested file transfer formats internal to its data
words. And fifth, functional generality is lost because the receiver is also the listener and server.

8. Conclusion

leads us to conclude tha t our emphasis on
Our experience with an operating Ethernet
distributed control was well placed. By keeping the shared components o f the communication
system to a minimum and passive, we have achieved a very high level o f reliability.
Installation,and
maintenance o f our experimental Ethernet has been more than satisfactory. The flexibility o f
station interconnection provided by'broadcast packet switching has encouraged the development o f
numerous computer networking and multiprocessing applications.

9. Acknowledgements

Our colleagues at the Xerox Palo Alto Research Center, especi~ly Tat C. Lam, Butler W.
Lampson, John F. Shoch, and Charles P. Thacker, have contributed in many ways to the evolution
o f Ethernet ideas and to the construction. o f the experimental system without which ,such ideas
would be just so much speculation.

ETHER:\ET: I)ISTRIBCTED PA~CKET S'VITCHING FOR LOCAL COMPUTER NETWORKS

19

10. References

[Abramson, 1970]
N. Abramson, "The AlohaSystem~', AFIPS Conference Proceedings, voL 37, Fall 1970.
- - [Abramson, 1975]
N. Abramson, F.F. Kuo, Computer-Comlnunication Networks, Prentice-Hall, 1975.

[Ashenhurst, 1975]
R.L. Ashenhurst, R.H. Vonderohe, "A Hierarchical Network", Datamation, February 1975.
[Baran, 1964]
P. Baran, On Distributed Comnlunications, Rand Corporation Memo RM-3420-PR, August
1964.

[Barnes, 1968]
G.H. Barnes, R.M. Brown, M. Kato, D.J. Kuck, D.L. Slotnick, R.A. Stokes, "The Illiac IV
IEEE Transactions, C-17, vol. 8, August 1968.
Computer",

[Binder, 1975]
R. Binder, N. Abramson, F. Kuo, A. Okinaka, D. Wax, "Aloha Packet Broadcasting-A
Retrospect", Proceedings o f the National Computer Conference, May 1975.
[Cerf, 1974]
V.G. Cert: R.E. Kahn,
for Packet Network Intercommunication",
"A Protocol
Transactions on Conlmunications, vol. cOM-22, no. 5, May 1974.
[Computer, 1974a]
"The Shrinking World: Computer Networks and Communications", Computer, IEEE Computer
Society, February 1974.

IEEE

[Computer, 1974b]
"Distributed-Function Computer Architectures", Computer,
1974.

IEEE Computer Society, March

[Crocker, 1972]
S.D. Crocker, J.F. Heafner, R.M. Metcalfe, and I.B. Postel, "Function-Orie·nted Protocols for
the ARPA Computer Network", AF IP S Conference Proceedings, vol. 40, May 1972. Reprinted in
Advances in COlnputer Communications, edited by W.W. Chu, Artech House Inc., 1974.
Reprinted in Computer Communications, edited by P.E. Green and R.W. Lucky, IEEE press,
1975.

[Crowther, 1975]
W.R. Crowther, F.E. Heart, A.A. McKenzie, J.M. McQuillan, and D.C. Walden, "Issues in
Packet-Switching Network Design", Proceedings o f the National Computer Conference, May
1975.

[Farber, 1973]
I
D J . Farber, et ai, "The Distributed Computing System", ProceediJgs o f the 7th Annual IEEE
I
Computer Society International Conference, February 1973.

i

[Farber, 1975)
D.J. Farber, "A Ring Network", Datamation, February 1975.

20

ETHERi'\ET: DISTRIBUTED PACKET SWITCHING FOR LOCAL COMP1JTER NETWORKS

[Fraser, 1975]
A.G. Fraser,

i tA Virtual Channel Network", Datamalion, Febru~ 1975.

(Heart, 1970]
F.E. Hean, R.E. Kahn, S.M. Ornstein, W.R. Crowther, D.C. Walden, "The Interface Message
Processor for the Arpa Computer Network", AFIPS Conference Proceedings, vol. 36, May 1970.

[HeaIt 1973]
F.E. Heart, S.M. Ornstein, W.R. Crowther, and W.B. Barker, "A New Minicomputer(cid:173)
Multiprocessor for the Arpa Network", D IPS Conference Proceedings, vol. 42, June '1973.
Reprinted in Advances in Computer Communications, edited by W.W. Chu, Artech House Inc.,
1974.
[Kahn, 1975]
R.R. Kahn, "The Organization o f Computer Resources into a Packet Radio. Network",
Proceedings o f the National Computer Conference, May 1975.

[Metcalfe, 1972a]
R.M. Metcalfe, "Strategies for
Interprocess Communication in a Distributed Computing
System", Proceedings o f the Symposium on Compute,.Communications Networks and Teletraffic,
Polytechnic Press, New York, 1972.
[Metcalfe, 1972b]
R.M. Metcalfe, "Strategies for Operating Systems in Computer Networks", Proceedings o f the
ACM National Conference, August 1972.

[Metcalfe, 1973a]
R.M. Metcalfe, "Steady-State Analysis of a Slotted and Controlled Aloha System w i th
Blocking", Proceedings o f the Sixth Hawaii Conference on System Sciences, January 1973.
Reprinted in the Sigcomm Review, January 1975.
[Metcalfe, 1973b]
R.M. Metcalfe, Packet Communication, Harvard PhD Thesis, Massachusetts Institute o f
Technology Project Mac TR-114, December 1973.

[Metcalfe, 1974]
R.M. Metcalfe, "Distributed Algorithms for a Broadcast Queue",
talk given at Stanford
University in November 1974 and at the University of California at Berkeley in Febraury 1975,
paper in preparation.
[Murthy, 1975]
P. Murthy, "Analysis o f a Carrier-Sense Random-Access System with Random Packet Lengths",
Aloha System Technical Report B75-17, University o f Hawaii, May 1975.
[Ornstein, 1975]
S.M. Ornstein, W.R. Crowther, M.F. Kraley, R.D. Bressler, A. Michel, and F.E. Heart,
"Pluribus-A Reliable Multiprocessor", Proceedings o f the National Computer Conference, May
1975.

[Retz, 1975J
D.L. Retz, "Operating·System Design Considerations for the Packet Switching Environment",
Proceedings o f the National Computer Conference, May 1975.

.t

ETHERNET: DISTRIBD'TED PACKET SWITCHING FOR LOCAL COMPUTER NETWORKS

21

[Robens, 1970]
L. Roberts, B. Wessler, "Computer Network Development to Achieve Resource Sharing", AFIPS
Conference Proceedings, vol. 36, May 1970.
[Roberts, 1973]
L.G. Roberts, "Capture Effects on Aloha Channels", Proceedings o f the Sixth Hawaii
Conference on S}'stem Sciences, January 1973.
[Rowe, 1975]
L.A. Rowe, "The Distributed Computing Operating System", Technical Report Number 66,
Department o f Information and Computer Science, University o f California, Irvine, June 1975.
[Rustin, 1972]
R. Rustin, Editor, Computer Networks, Proceedings o f the Courant Computer Science
Symposium 3, December 1970, Prentice-Hall Series in Automatic Computation, 1972.
[sDLe, 1974]
IBM Synchronous Data Link Control-General Information, IBM Systems De,velopment Division,
Publications Center, Department E01, P.O. Box 12195, Research Triangle Park, North Carolina
27709, 1974.

[SNA, 1975]
IBM System Network Architectur~General Infromation,
IBM Systems Development Division,
Publications Center, Department EOI, P.O. Box 12195, Research Triangle Park, North Carolina
27709, 1975.

[Thomas, 1973]
R.H. Thomas, "A Resource Sharing Executive for the Arpanet", AFIPS Conference Proceedings,
October 1973.

[Thornton, 1970]
J.E. Thornton, Design o f a Computer: the Control Data 6600, Scott Foresman and Company,
1970.

[Walden, 1972]
D.C. Walden, "A System for Interprocess Communication in 'a Resource Sharing Computer
Network", Communications o f the ACM, vol. 15, no. 4, April 1972.
[Willard, 1973]
D.O. Willard, "Mitrix: A Sophisticated Digital Cable Communications System", Proceedings o f
the National Telecommunications Conference, November 1973.
[Wult: 1972]
W. Wulf, R. Levin, "C .mmp -A Multi-~ini-Processor", AFIPS Conference Proceedings, Fall
~.972.

International Journal of Information and Education Technology, Vol. 3, No. 1, February 2013

 
 

 
A Review of Routing Protocols for Mobile  
 
Ad-Hoc NETworks (MANET)  

 
Alex Hinds, Michael Ngulube, Shaoying Zhu, and Hussain Al-Aqrabi 

 
 

 
 

Abstract—The  increase  in  availability  and  popularity  of 
mobile wireless  devices  has  lead  researchers  to  develop  a  wide 
variety  of Mobile  Ad-hoc  NETworking  (MANET)  protocols  to 
exploit  the  unique  communication  opportunities  presented  by 
these  devices.  Devices  are  able  to  communicate  directly  using 
the  wireless  spectrum  in  a  peer-to-peer  fashion,  and  route 
messages  through  intermediate  nodes,  however  the  nature  of 
wireless  shared  communication  and  mobile  devices  result  in 
many routing and security challenges which must be addressed 
before  deploying  a  MANET.  In  this  paper  we  investigate  the 
range  of  MANET  routing  protocols  available  and  discuss  the 
functionalities  of  several  ranging  from  early  protocols  such  as 
DSDV  to more  advanced  such  as MAODV,  our  protocol  study 
focuses  upon  works  by  Perkins  in  developing  and  improving 
 
MANET  routing.  A  range  of  literature  relating  to  the  field  of 
MANET routing was identified and reviewed, we also reviewed 
literature on the topic of securing AODV based MANETs as this 
may  be  the  most  popular  MANET  protocol.  The  literature 
review  identified  a  number  of  trends  within  research  papers 
such  as  exclusive  use  of  the  random  waypoint  mobility  model, 
excluding  key  metrics  from  simulation  results  and  not 
 
comparing protocol performance against available alternatives.  
 
 

 
Index Terms—AODV, MANET, routing protocols. 

 

 
 

 
I.  INTRODUCTION 
 

Wireless  technologies  such  as  Bluetooth  or  the  802.11 
standards  enable  mobile  devices  to  establish  a  Mobile 
Ad-hoc  Network  (MANET)  by  connecting  dynamically 
through  the  wireless  medium  without  any  centralised 
structure  [1].  MANETs  offer  several  advantages  over 
traditional  networks  including  reduced  infrastructure  costs, 
ease  of  establishment  and  fault  tolerance,  as  routing  is 
performed  individually  by  nodes  using  other  intermediate 
network  nodes  to  forward  packets  [2],  this  multi-hopping 
reduces the chance of bottlenecks, however the key MANET 
 
attraction is greater mobility compared with wired solutions.  
There are a number of issues which affect the reliability of 
 
Ad-hoc  networks  and  limit  their  viability  for  different 
scenarios;  lack  of  centralised  structure  within  MANET 
requires that each individual node must act as a router and is 
responsible for performing packet routing  tasks;  this  is done 
using  one  or  more  common  routing  protocols  across  the 
 
MANET [3]. Performing routing tasks requires memory and 
computation power, however mobile devices feature physical 
size  and  weight  limitations  essential  for  their  mobility,  this 

 

 
 
Manuscript  received  September  6,  2012;  revised  December  12,  2012. 
 
This work was supported by the University of Derby  
The  authors  are  with  the  University  of  Derby,  Derbyshire,  DE22  1GB, 
UK 
(e-mail:  A.Hinds1@unimail.derby.ac.uk, 
eppiemike@aol.com, 
 
s.y.zhu@derby.ac.uk, h.al-aqrabi@derby.ac.uk‎).  
 
 

DOI: 10.7763/IJIET.2013.V3.223

1

 

reduces  the  available  memory  and  computational  resources 
as well as limiting battery power.  
MANETs  containing  more  nodes 
require  greater 
processing  power,  memory  and  bandwidth  to  maintain 
accurate routing information; this introduces traffic overhead 
into the network as nodes communicate routing information, 
this in turn uses more battery power. 
Wireless 
technologies  use  a  shared  communication 
medium;  this  causes  interference  which  degrades  network 
performance  when  multiple  nodes  attempt  to  transmit 
 
simultaneously. 
as  Distributed 
such 
Techniques 
Coordination Function (DCF) are used to limit the impact of 
channel  contention  upon  network  performance,  DCF  uses 
carrier  sense  multiple  access  with  collision  avoidance 
(CSMA/CA) and channel switching to reduce interference [4] 
however larger MANETs feature more interference. 
The  mobility  of  nodes  is  also  a  major  factor  within 
MANETs due to limited wireless transmission range; this can 
cause the network topology to change unpredictably as nodes 
enter  and  leave  the  network  [5].  Node  mobility  can  cause 
broken  routing  links  which  force  nodes  to  recalculate  their 
routing information; this consumes processing time, memory, 
device  power  and  generates  traffic  backlogs  and  additional 
overhead traffic on the network [6]. 
Security  of  MANETs  is  another  major  deployment 
concern;  due  to  the  mobility  and  wireless  nature  of  the 
network malicious  nodes  can  enter  the  network  at  any  time, 
the security of the nodes and the data transmitted needs to be 
considered [7]. 
Due  to  these  issues  ad-hoc  networks  are  not  appropriate 
for  most  general  usage  of  mobile  devices,  where  internet 
access  is  the  key  requirement;  in  these  situations  wireless 
devices  typically  connect  into  the  wired  infrastructures 
through access points  (AP)  to  reduce  the unreliability of  the 
wireless domain [8].  
However  Ad-Hoc  networks  show  great  potential  in 
situations where  internet  access  is  not  a  key  requirement  or 
infrastructure  is  not  available;  including  disaster  or  military 
 
scenarios  or  in  low  power  wireless  sensor  networks  or 
vehicles  which  only  need  to  communicate  with  each  other 
[9].   
This paper is structured as follows; Section II discusses the 
core requirements of a MANET routing protocol, Section III 
discusses  MANET 
routing  principles,  Section 
IV 
 
investigates  some of  the  earliest MANET  routing protocols; 
DSR and DSDV as well as the impact of mobility models on 
 
simulations.  Section  V  focuses  upon  the  AODV  MANET 
 
routing  protocol,  Section VI  highlights  improvements made 
to  AODV 
in 
the  form  of  multicasting,  section  VII 
investigates security systems designed to AODV and Section 

 
 

International Journal of Information and Education Technology, Vol. 3, No. 1, February 2013

 
 

VIII concludes the paper and proposes future work.  
 

II.  LITERATURE REVIEW 

 

We  have  identified  several  pieces  of  key  literature  in  the 
 
field  of MANET  routing  protocols which  highlight  existing 
protocols as well as the current thinking within the field and 
 
 
the directions researchers are moving in the future.  
Reference  [3] proposes  that  an  effective MANET  routing 
protocol  must  be  equipped  to  deal  with  the  dynamic  and 
unpredictable  topology  changes  associated  with  mobile 
nodes,  whilst  also  being  aware  of  the  limited  wireless 
bandwidth and device power considerations which may lead 
to  reductions  in  transmission  range  or  throughput.  This  is 
expanded  upon by  [1] who  propose  that  in  addition  to  these 
core requirements; MANET routing protocols  should also be 
decentralized,  self-healing  and  self-organising  and  able  to 
exploit multi-hopping and load balancing, these requirements 
ensure  MANET  routing  protocols  ability 
to  operate 
autonomously. 
 

III.  MANET ROUTING PRINCIPLES  

The  first  pieces  of  literature we will  discuss  are  a  pair  of 
 
survey  papers  by  [1],  [8],  these  two  survey  papers  gather 
together information on the wide variety of MANET routing 
 
 
protocols  which  researchers  have  developed  to  meet  the 
challenges  of  MANET  routing,  many  of  which  feature 
different  methods  of  managing  the  issues  associated  with 
mobility.  
Reference [8] performed an extensive research survey into 
the  available  routing  protocols  and  attempted  to  categorise 
them  by  the  features  they  exhibit  and  provide  details  on  the 
core  protocols  of  each  category.  This  is  similar  to  work 
undertaken  by  [1] who  took  a  similar  approach  in  grouping 
routing  protocols  using 
the  categories;  geographical, 
multi-path,  hierarchical,  geo-cast  and  power  aware  routing 
protocols.  The  two  survey  papers  both  find  that  every 
protocol identified also fit into the core categories of; reactive, 
proactive  or  hybrid  routing  protocols  in  additional  to  any 
other characteristics they exhibit.  

A.  Proactive Routing  

Proactive protocols rely upon maintaining routing tables of 
known destinations, this reduces the amount of control traffic 
overhead  that  proactive  routing  generates  because  packets 
 
are  forwarded  immediately  using  known  routes,  however 
routing tables must be kept up-to-date; this uses memory and 
nodes periodically send update messages to neighbours, even 
when no traffic is present, wasting bandwidth [10]. Proactive 
routing  is  unsuitable  for  highly  dynamic  networks  because 
routing  tables  must  be  updated  with  each  topology  change, 
this leads to increased control message overheads which can 
degrade network performance at high loads [11]. 

B.  Reactive Routing  

Reactive Protocols use  a  route discovery process  to  flood 
the network with route query requests when a packet needs to 
be  routed  using  source  routing  or  distance  vector  routing. 

 
Source  routing  uses  data  packet  headers  containing  routing 
information‎ meaning‎ nodes‎ don’t‎ need‎ routing‎ tables;‎
however  this  has  high  network  overhead.  Distance  vector 
routing  uses  next  hop  and  destination  addresses  to  route 
packets, this requires nodes to store active routes information 
until no longer required or an active route timeout occurs, this 
prevents  stale  routes  [10].  Flooding  is  a  reliable  method  of 
disseminating information over the network, however it uses 
bandwidth  and  creates  network  overhead,  reactive  routing 
broadcasts routing requests whenever a packet needs routing, 
this  can  cause  delays  in  packet  transmission  as  routes  are 
calculated,  but  features  very  little  control  traffic  overhead 
and  has  typically  lower  memory  usage  than  proactive 
alternatives, this increases the scalability of the protocol [1].  

C.  Hybrid Routing  

Hybrid protocols combine features from both reactive and 
proactive  routing  protocols,  typically  attempting  to  exploit 
 
the  reduced  control  traffic  overhead  from  proactive  systems 
 
whilst  reducing  the  route  discovery  delays  of  reactive 
systems by maintaining some form of routing table [10]. 
The  two  survey  papers  [1],  [8]  successfully  collect 
information  from  a  wide  range  of  literature  and  provide 
detailed  and  extensive  reference  material  for  attempting  to 
deploy  a MANET, both papers  reach  the  conclusion  that  no 
single  MANET  routing  protocol  is  best  for  every  situation 
meaning  analysis  of 
the  network  and  environmental 
requirements  is  essential  for  selecting  an  effective  protocol.  
Whilst these papers contain functionality details for many of 
the  protocols  available,  performance  information  for  the 
different  protocols  is  very  limited  and  no  details  of  any 
testing  methodologies  is  provided,  because  of  this  the 
validity of some claims made cannot be verified.  
 
 

IV.  EARLY MANET ROUTING PROTOCOLS  

The  next  piece  of  literature  is  a  protocol  performance 
 
comparison  by 
[12]  which  compares 
the  proactive 
Destination  Sequenced  Distance  Vector  (DSDV)  protocol 
 
 
and  the  reactive  Dynamic  Source  Routing  (DSR)  protocol; 
these  protocols  were  developed  in  1994  and  were  amongst 
the  earliest  MANET  routing  protocols  identified  using  the 
previous survey papers.   

A.  Destination Sequenced Distance Vector (DSDV) 

The proactive DSDV protocol was proposed by [13] and is 
based  upon  the  Bellman-Ford  algorithm  to  calculate  the 
shortest number of hops to the destination [11]. Each DSDV 
 
node  maintains  a  routing  table  which  stores;  destinations, 
next hop  addresses and number of hops as well  as  sequence 
numbers;  routing  table  updates  are  sent  periodically  as 
incremental  dumps  limited  to  a  size  of  1  packet  containing 
only new information [12].  
DSDV compensates for mobility using sequence numbers 
and  routing  table  updates,  if  a  route  update  with  a  higher 
sequence number is received it will replace the existing route 
thereby  reducing  the  chance of  routing  loops, when  a major 
 
topology change is detected a full routing table dump will be 
performed,  this  can  add  significant  overhead  to  the  network 
in dynamic scenarios [13]. 

2

 
 

International Journal of Information and Education Technology, Vol. 3, No. 1, February 2013

 
 

B.  Dynamic Source Routing (DSR) 

A.  Ad-Hoc on-Demand Distance Vector (AODV) 

The  reactive  DSR  Protocol  was  developed  by  [14], 
operation  of  the  DSR  protocol  is  broken  into  two  stages; 
route  discovery  phase  and  route  maintenance  phase,  these 
phases are triggered on demand when a packet needs routing. 
 
Route discovery phase floods the network with route requests 
if a suitable route is not available in the route [12]. 
DSR uses a source routing strategy to generate a complete 
route to the destination, this will then be stored temporarily in 
nodes  route  cache  [15].  DSR  addresses  mobility  issues 
through  the  use  of  packet  acknowledgements;  failure  to 
receive  an  acknowledgement  causes  packets  to  be  buffered 
and  route  error  messages  to  be  sent  to  all  upstream  nodes. 
Route  error  messages  trigger  the  route  maintenance  phase 
which  removes  incorrect  routes  from  the  route  cache  and 
undertakes a new route discovery phase [14]. 

C.  Mobility Models  

Reference  [12]  compares  the  performance  of  DSR  and 
DSDV using simulations against 4 different mobility models; 
these  are  mathematic  models  which  control  the  motion  of 
nodes  around  the  simulation;  this  allows  researchers  to 
 
measure  the  effect  of  mobility  upon  the  routing  protocols 
 
performance.  Various  mobility  models  are  used  to  simulate 
different situations such as high speed vehicular networks or 
lower mobility ad-hoc conference users, however research by 
[15]  reveals  that  many  studies  perform  protocol  evaluation 
almost  exclusively  using  the  random  waypoint  mobility 
model. This  research  is  supported  by  findings  from  [2] who 
claim  that  the  random  waypoint  model  is  the  most  widely 
used mobility model, however discrepancies were  identified 
between  the  models  behaviour  and  real  world  scenarios 
where users  typically move  in groups, due  to  this  the model 
 
may not be appropriate for exclusive testing.  
Reference  [12]  performs  simulations  against  multiple 
mobility  models  using  networks  of  varying  sizes  up  to  100 
nodes;  this  increases  the  accuracy  and  reliability  of  the  data 
and reveals network performance under different conditions, 
the study revealed that DSR gave greater network throughput 
than DSDV in all tests. These findings cannot be considered 
conclusive  evidence  of  DSRs  superiority  because  the  study 
only  collected  network  throughput metrics;  this  information 
alone does not give an accurate representation of the network 
performance;  collection  of  other  metrics  such  as  packet 
delivery  ratio  or  end-to-end  delay  should  be  considered  as 
these are important metrics for evaluating performance.  
 

V.  SECOND GENERATION MANET ROUTING PROTOCOL – 
AODV  

Researchers  learned  many  lessons  from  early  MANET 
 
protocols such as DSR and DSDV, these lead to proposals for 
new  protocols  to  improve  performance,  one  of  the  most 
 
 
significant contributions to MANET routing was the Ad-hoc 
  
On-demand  Distance  Vector  (AODV)  protocol  which  was 
designed by [16] as an improvement upon previous work on 
the DSDV protocol with [13]. Reference [17] has produced a 
paper  discussing  the  protocols  functionality  and  testing  it 
against a number of criteria.   

AODV  utilises  sequence  numbers  and  routing  beacons 
from DSDV but  performs  route  discovery  using  on-demand 
 
route  requests  (RREQ);  the  same  process  as  the  DSR 
protocol  [17].  AODV  is  different  to  DSR  in  that  it  uses 
 
distance vector routing; this requires every node in the route 
to maintain a  temporary routing  table  for  the duration of  the 
communication.  AODV  has  improved  upon  the  DSR  route 
request  process  using  an  expanding  ring  search  mechanism 
based  upon  incrementing  time-to-live  (TTL)  to  prevent 
excessive RREQ  flooding  [2]. Nodes within  an  active  route 
record  the  senders  address,  sequence  numbers  and  source  / 
destination  IP  address  within  their  routing  tables,  this 
information  is  used  by  route  reply  (RREP)  to  construct 
reverse paths [11].  
AODV deals with node mobility using sequence numbers 
to identify and discard outdated routes, this is combined with 
route  error  (RERR)  messages  which  are  sent  when  broken 
links  are  detected,  RERR  packets  travel  upstream  to  the 
 
source informing nodes to delete the broken links and trigger 
new route discovery if alternative routes are not available [4]. 
Reference  [17]  discusses  the  core  principles  of  the 
protocol  but  provide  no  real  insight  into  possible  directions 
the protocol could take in the future,  the network simulation 
collects  data  on  a  number  of  important  metrics;  dropped 
packets,  transmission  and  receiving  throughput  (UDP  and 
TCP),  delay,  send  time  vs.  delay,  jitter  and  round  trip  time. 
These  metrics  are  all  important  for  quality  of  service 
considerations and useful indicators of network performance, 
however  the simulations are  run only using AODV protocol 
so no direct comparison between alternative protocols can be 
made,  the  simulation  topology  also  uses  a  uniform  random 
waypoint  mobility  model  of  16  nodes  which  as  discussed 
previously  in  Section  IV.  C  is  not  an  ideal  testing 
environment.  

B.  Expanding upon AODV  – Multicasting 

The  AODV  protocol  is  considered  by  some  researchers 
[17]  to  be  the  most  popular  MANET  routing  protocol,  this 
has lead to many variants and improvements being proposed 
by researchers to address some of the many issues of wireless 
 
MANETs.  
One  of  these  issues  was  the  lack  of  multicast  support  in 
early MANET routing protocols, including DSR, DSDV and 
AODV,  this  functionality  is  useful  for  communicating  with 
multiple  nodes  and  increased  available  routing  knowledge 
whilst  reducing  control  traffic  overheads  [18].  In  order  to 
address  this  issue  [18]  proposed  the  Multicast  Ad-hoc 
On-demand  Distance  Vector  (MAODV)  routing  protocol, 
this  protocol  builds  directly  upon  their  previous  work  on 
AODV  by  adding  support  for  multicast  operation  to  the 
protocol.  
The next piece of  literature  in our  review  is an evaluation 
of  the MAODV  protocol  produced  by  [19]  who  discuss  the 
technical  aspects  of  the  protocol  and  provides  a  number  of 
simulations  to  evaluate  the  performance  of  the  protocol  in 
scenarios such as long and short lived communications.   

1)  Multicast ad-hoc on-demand distance vector 
(MAODV) 

The  MAODV  protocol  shares  the  same  underlying 

3

 
 

International Journal of Information and Education Technology, Vol. 3, No. 1, February 2013

 
 

architecture as the AODV protocol with some modifications 
and  the  addition  of  Multicast  Activations  (MACT)  and 
Group  Hello  (GRPH)  messages,  each  node  also  maintains 
 
separate  unicast  and  multicast  routing  tables  [20].  When 
MAODV broadcasts RREQ messages onto the network they 
now support multiple destination IP addresses, each of these 
IP  addresses  will  reply  with  RREP  packets  as  per  AODV 
behaviour however upon receipt of a RREP packet the source 
will  send  a  MACT  to  the  destination  node  activating  a 
multicast  route.  Multicast  paths  are  added  to  a  multicast 
delivery  tree which  is  stored on  the  source;  this  tree  records 
all multicast destinations and allows the node to learn unicast 
destinations  from  the  tree without  broadcasting RREQ  [18]. 
The first node to join a multicast group becomes the leader of 
that  group  responsible  for  group  maintenance,  this  is  done 
using  by  broadcasting  GRPH  messages  which  contain  the 
leaders IP, these GRPH messages are used to synchronise the 
multicast  group  using  incrementing  sequence  numbers  [19]. 
Should  a  tree  group  member  become  disconnected  it  will 
attempt  to  reconnect  to  the  existing  tree  using  the  leader  IP 
and  re-synchronise  before  attempting  to  create  a  new  tree, 
this reduces network overhead.  
Reference  [19]  have  performed  a  wide  range  of 
simulations to test the performance of the MAODV protocol 
however a key limitation of their work is that they only used 
random  waypoint  mobility  model  in  testing,  as  discussed 
previously this mobility model alone has several  limitations. 
The simulations also  failed  to collect a number of  important 
performance metrics  such  as  network  throughput  and  didn't 
perform  any  performance  comparisons  with  other  multicast 
protocols  available  such  as  Lightweight  Adaptive Multicast 
(LAM) which were discussed in the literature.  
 

VI.  ISSUES OF AODV – SECURITY  

One  of  the  major  concerns  about  deploying  MANETs  is 
security; wireless networks have increased vulnerability to a 
wide  variety  of  security  threats  such  as  eavesdropping  and 
 
packet tampering compared to traditional wired networks [7]. 
The  original  AODV  protocol 
included  no  security 
 
 
 
 
mechanisms  meaning  that  it  is  vulnerable  to  attacks  which 
target  the  network  routing  protocol  functions  such  as 
sequence number or hop count manipulation [21]. In order to 
address this issue researchers developed a number of security 
and  authentication  schemes  for  MANETs  as  well  as 
extensions  of  AODV  designed  to  increase  security,  such  as 
Security-aware  Ad-hoc  On-demand  Distance  Vector 
(SAODV)  and  Adaptive  Secure  Ad-hoc  On-demand 
Distance Vector (A-SAODV). These protocols feature digital 
signing  of  routing  traffic  and  data  to  ensure  integrity  and 
authenticity.  

A.  Security-Aware Ad-Hoc on-Demand Distance Vector 
Routing Protocol (SAODV) 

We reviewed literature produced by [22] which performed 
a  comparison  of  three  routing  protocols;  AODV,  SAODV 
  
and  A-SAODV.  Security  issues  which  these  protocols 
address 
include  Message 
tampering  attacks,  Message 
 
dropping  attack  and  Message  replay,  also  known  as 
the wormhole  attack.  In  an  effort  to  guard  against  these 

attacks,  AODV  security  protocols  need  the  ability  to 
authenticate  and  confirm  the  identity  of  a  source. Protocols 
also  need  to  authenticate  the  neighbour  transmitting  the 
packet; message integrity must also be checked to ensure that 
messages  in  transit  have  not  been  modified  through 
accidental or malicious activity. Protocols need the ability to 
ensure  that nodes wishing  to  access network  resources have 
the  appropriate  access  rights  [22].  The  literature  includes 
performance  simulations  for  the  AODV,  SAODV  and 
A-SAODV  protocols  in  a  free-attack  scenario  where 
simulated  threats  attack  the  network.  However  the  AODV 
protocol features no security mechanisms meaning this is not 
a fair comparison; the results for AODV should only be used 
as  a  benchmark  for  comparison.  Simulations  collected  a 
number of important metrics but were only performed using a 
random waypoint mobility model with very high node speeds 
of  40m/s  limiting  the  applicability  of  the  results  in  a  real 
world scenario as not many networks feature such high node 
speeds. 
 

VII.  CONCLUSION 

In  this  paper  we  have  identified  and  reviewed  a  range  of 
literature  on  the  topic  of  MANET  routing  protocols,  our 
 
initial work discussed a pair of survey papers from which we 
identified  early  reactive  and  proactive  MANET  routing 
 
 
protocols. Our  review  focuses  upon  protocols  developed  by 
Perkins, namely  the Destination Sequenced Distance Vector 
(DSDV)  and Ad-hoc On-demand  Distance  Vector  (AODV) 
which researchers claim is the most popular MANET routing 
protocol.  Due  to  the  popularity  of  the  AODV  protocol  a 
number of variations and improvements on the core protocol 
have been proposed by researchers to address specific issues 
with the protocol.  
We  investigate  the  evolution  of  the  AODV  protocol  by 
reviewing  works  based  upon 
the  Multicast  Ad-hoc 
On-demand Distance Vector (MAODV), developed by  [18], 
this  protocol  adds  multicasting  support  to  the  core  AODV 
protocol.  A  number  of  researchers  highlighted  the  lack  of 
security mechanisms within the original AODV protocol as a 
major  concern  for  deployment  of  a  MANET.  We  reviewed 
literature  relating  to  the  security  of  the AODV  protocol  and 
proposed  modifications  with  the  aim  of  addressing  the 
security  issues  raised,  one  example  is  the  Security-aware 
Ad-hoc On-demand Distance (SAODV).  
A  common  theme  across  many  of  the  papers  we  have 
reviewed is the exclusive usage of random waypoint mobility 
model for simulations despite several researchers identifying 
limitations  with  this  approach  to  testing.  The  collections  of 
metrics  from  simulations  is  another  area  which  was 
highlighted  in  several  of  the  reviewed  papers,  researchers 
focus  upon  very  specific  metric  collection  but  exclude 
collection  of  core  metrics  such  as  network  throughput  or 
delay which are essential for understanding the performance 
of  a  protocol.  This  is  also  true  in  the  case  of  simulations 
which  perform  testing of protocols  in  isolation;  this  reduces 
the  applicable  value  of  the  results  because  they  cannot  be 
directly compared to available alternatives.  
Areas  for  future  work  include  reviewing  literature  which 
addresses  some  of  the  issues  with MANET  and  the  AODV 

4

 
 

International Journal of Information and Education Technology, Vol. 3, No. 1, February 2013

 
 

protocol  in  particular  which  were  identified  within  the 
literature  we  have  discussed  such  as;  power  aware  routing, 
Mobility  aware  routing,  hierarchical  routing,  reliability 
focused routing.  

REFERENCES 

 

 

 

 

 

[1]  E.  Alotaibi  and‎ B.‎ Mukherjee,‎ “A  survey  on  routing  algorithms  for 
wireless  Ad-Hoc  and  mesh  networks,”‎ Computer  Networks:  The 
International  Journal  of  Computer  and  Telecommunications 
Networking, vol. 56,  no. 2, pp. 940–965, October 2011.  
[2]  M.  Zhang  and  P. H.  J. Chong,‎ “Performance Comparison  of  Flat  and 
 
Cluster-Based  Hierarchical  Ad  Hoc  Routing  with  Entity  and  Group 
Mobility,”‎ in  Proc.  of  IEEE  Communications  Society  conference  on 
 
Wireless  Communications  &  Networking,  Budapest,  Hungary,  2009, 
pp. 2450-2455. 
[3]  R.  O.  Schmidt  and  M.  A.  S.‎ Trentin,‎ “MANETs  Routing  Protocols 
  
 
 
 
 
Evaluation  in  a  Scenario  with  High  Mobility:  MANET  Routing 
 
 
 
 
Protocols  Performance  and  Behaviour,”  Network  Operations  and 
Management  Symposium,  2008. NOMS  2008.  IEEE,  Salvador, Bahia, 
 
pp.883-886, 2008. 
 
 
 
[4]  X.  Hu,  J.  K.  Wang,  C.  R.  Wang,  and  C.  Wang,  “Is  mobility  always 
 
harmful  to  routing  protocol  performance  of  MANETs?”  in  Proc.  of 
 
International  Conference  on  Cyber-Enabled  Distributed  Computing 
and Knowledge Discovery, pp. 108-112, 2010. 
 
[5]  Y.  Khamayseh,  O.  M.  Darwish,  and  S.  A.‎ Wedian,‎ “MA-AODV: 
Mobility Aware  Routing  Protocols‎ for‎Mobile‎ Ad‎ hoc‎ Networks,”  in 
 
Proc.  of  Fourth  International  Conference  on  Systems  and  Networks 
Communications IEEE,  pp. 25-29, 2009. 
[6]  W.  Wang  and‎ C.‎ Amza,‎ “Motion-based  Routing  for  Opportunistic 
Ad-hoc Networks,”‎in Proc. of 14th ACM international conference on 
 
Modeling,  analysis  and  simulation  of  wireless  and  mobile  systems, 
October 31–November 4, 2011, pp. 169-178.  
[7]  R.  Akbani,  T.  Korkmaz,  and  G  .V.  S.‎ Raju,‎ “HEAP:  A  packet 
authentication scheme for mobile ad hoc networks,” Ad Hoc Networks, 
 
vol. 6, no. 7, pp. 1134–1150, 2008. 
 
[8]  A. Boukerche et al., “Routing protocols in ad hoc networks: A survey,” 
Computer  Networks:  The  International  Journal  of  Computer  and 
Telecommunications Networking, vol. 55, no. 13. pp. 3032–3080, May 
2011.  
 
[9]  B.  Malarkodi,  P.  Gopal,  and  B.  Venkataramani,  “Performance 
 
evaluation  of  AD-hoc  networks  with  different  multicast  routing 
 
 
 
 
protocols  and  mobility  models,”  in  Proc.  of  2009  International 
 
Conference  on  Advances  in  Recent  Technologies  in  Communication 
and Computing IEEE, India, 27-28 Oct., 2009, pp. 81-84.  
 
 
[10]  H.  Amri,  M.  Abolhasan,  and  T.  Wysocki,  “Scalability  of  MANET 
routing  protocols  for  heterogeneous  and  homogenous  networks ,” 
Computers  and  Electrical  Engineering,  vol.  36,  no.  4,  pp.  752–765, 
2010. 
[11]  C. Liu  and‎ S.‎Chang,‎ “The  study  of  effectiveness  for  ad-hoc wireless 
network,”  in  Proc.  of  ICIS  2009  2nd  International  Conference  on 
 
 
Interaction  Sciences:  Information  Technology,  Culture  and  Human , 
Seoul, Korea, 24-26 Nov., 2009, pp. 412-417. 
 
[12]  B.‎ Divecha,‎ A.‎ Abraham,‎ C.‎ Grosan,‎ and‎ S.‎ Sanyal,‎ “Analysis  of 
Dynamic Source Routing and Destination-Sequenced Distance-Vector 
 
Protocols‎ for‎ Different‎ Mobility‎ models,”‎ in  Proc.  of  First  Asia 
International  Conference  on  Modelling  &  Simulation ,  Phuket, 
Thailand, 27-30 March, 2007, pp. 224-229. 
 
[13]  C.‎Perkins‎ and‎P.‎Bhagwat,‎ “Highly‎Dynamic‎Destination-Sequenced 
 
 
Distance-Vector Routing (DSDV) for Mobile Computers,”‎in Proc. of 
Sigcomm conference on Communications architectures, protocols and 
applications, London, England, UK, 1994, pp. 234-244. 
[14]  D. B. Johnson and D. A.‎Maltz,‎“Dynamic Source Routing in Ad Hoc 
 
Wireless  Networks,”  Mobile  Computing,  T.  Imielinski  and  H.  Korth, 
Ed. Kluwer Academic Publishers, 1996, vol. 5, pp. 153-181. 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

[15]  F.  Maan  and‎ N.‎ Mazhar,‎ “MANET  Routing  Protocols  vs  Mobility 
 
 
Models:  A  Performance  Evaluation,”‎ in  Proc.  of  Third  International 
 
 
Conference on Ubiquitous and Future Networks IEEE, Dalian, China, 
 
 
June 15-17, 2011, pp. 179-184. 
[16]  C. E. Perkins and E. M.‎Royer,‎“Ad-hoc On-Demand Distance Vector 
 
Routing,”‎ in  Proc.  of  the  2nd  IEEE  workshop  on  mobile  computing 
 
systems and applications, 1997, pp. 1-11. 
[17]  M.  Morshed,  H.  Rahman,  R.  R.  Mazumder,  and  K.  A.  M.  Lutfullah, 
 
 
 
“Simulation  and  Analysis  of  Ad-hoc  On-demand  Distance  Vector 
Routing  Protocol,”  in  Proc.  of  ICIS,  November  24-26,  2009  Seoul, 
Korea, pp. 610-614. 
[18]  C.  E.  Perkins  and  E.  M.‎ Royer,‎ “Multicast  operation  of  the  ad-hoc 
on-demand  distance‎ vector‎ routing‎ protocol,”‎ in  Proc.  of  5th  annual 
 
ACM/IEEE  international  conference  on  Mobile  computing  and 
networking, Seattle, Washington, USA, August 15-20, pp. 207-218. 
[19]  W.  A.  Mobaideen,  H.  M.  Mimi,  F.  A.  Masoud,  and  E.  Qaddoura, 
 
 
 
“Performance  evaluation  of  multicast  ad  hoc  on-demand  distance 
 
vector  protocol,”‎ Computer  Communications,  vol.  30,  no.  9,  pp. 
1931–1941, 2007. 
[20]  D. Dharmaraju, M. Karir, J. S. Baras, and‎S.‎Bas,‎“An Implementation 
Study  of  Multicast  Extensions‎ of‎ AODV,”‎ in  Proc.  of  International 
Symposium  on  Performance  Evaluation  of  Computer  and 
Telecommunication Systems, Montreal, Canada, July 20-24, 2003, pp. 
122-130.  
[21]  M. Mohammadizadeh, A. Moyaghar, and M. Safi, “SEAODV: Secure 
Efficient AODV Routing Protocol for MANETs Networks,” in Proc. of 
2nd  International  Conference  on  Interaction  Sciences:  Information 
 
 
Technology,  Culture  and  Human,  Seoul,  Korea,  November  24-26, 
2009, pp. 940-944. 
[22]  M.  A.  Jaafar  and  Z.  A.  Zukarnain,  “Performance  Comparisons  of 
 
AODV, Secure AODV and Adaptive Secure AODV Routing Protocols 
in  Free  Attack  Simulation  Environment,”  European  Journal  of 
 
Scientific  Research,  ISSN  1450-216X,  vol.  32,  no.  3,  pp.  430-443, 
2009. 

 

 

 

Alex  Hinds  is  a  full-time  student  studying  for  his Msc 
Advanced  Computer  Networks  at  Derby  University, 
Derby,  UK.  He  received  his  Bsc  Computer  Networks 
from  the  University  of  Derby  in  2011.  His  current 
research  interests  are  cloud  computing  and  mobile  ad 
hoc networking. 

 

 
 
 
Michael  Ngulube  was  born  in  Gweru,  Zimbabwe,  in 
 
1960.  He  obtained  his  BSc  of  Computer  Networks, 
University  of  Derby,  UK,  in  2010.  Currently,he  is  
studying  Masters  in  Advanced  Computer  Networks, 
University  of  Derby,  UK.  He  holds  career  certification 
from  Cisco,  CCNA  and  Student  member  of 
the 
Institution  of  Engineers  and  Technology,  in  2011.  
Wireless networking is my interest of study. 

 

Shao  Ying  Zhu  is  a  senior  lecturer  in  the  School  of 
 
Computing and Mathematics at the University of Derby. 
 
She  received  her  PhD  degree  from  the  Color  Imaging 
Institute  at  the  University  of  Derby  in  2002.  She  has 
published  a  large  number  of  conference  papers  and 
journal articles on a range of research areas such as color 
research,  image  processing,  e-learning  and  networking. 
Her current research interests are in e-learning, network 
security, mobile computing and wireless networks. 

 

 
 
 
 
 

 

 
 

 
 

 
 
 
 

 
 

 

5

Mobile Networks and Applications 3 (1998) 319–334

319

Mobile networking in the Internet

Charles E. Perkins
Sun Microsystems, Inc., 15 Network Circle, Menlo Park, CA 94025, USA

Computers capable of attaching to the Internet from many places are likely to grow in popularity until they dominate the population
of the Internet. Consequently, protocol research has shifted into high gear to develop appropriate network protocols for supporting
mobility. This introductory article attempts to outline some of the many promising and interesting research directions. The papers in
this special issue indicate the diversity of viewpoints within the research community, and it is part of the purpose of this introduction to
frame their place within the overall research area.

1. Introduction

This issue of Mobile Networking and Applications
presents research papers probing the effects of mobility on
the Internet. As one might expect, given the diverse na-
ture of protocols employed by Internet addressable devices,
there are a wide range of effects. In fact, there are so many
different aspects to Internet mobility that no single journal
issue or book could possibly describe all of them. Thus,
we will have to be content with presenting a representative
selection of articles that, in their diversity, give a good hint
at the larger picture. At the same time, these articles pro-
vide new directions and lead the way towards solving the
interesting and new problems raised by mobility.
It is the purpose of this introductory article to brieﬂy
mention a larger cross-section of the fresh ideas and propos-
als for solutions of the problems raised by mobile network-
ing, than could be represented by articles for publication in
this journal issue. Thus, this paper will touch on current
topics in many areas of networking. From cryptography to
routing, from billing to expanded techniques for automatic
conﬁguration, mobility changes the way we think about
computing, and invalidates some of the design assumptions
upon which current network protocols and products have
been built.
The impetus for all this change is the burgeoning mar-
ket for mobile and portable computers and computing de-
vices [62]. Besides the growing number of laptop comput-
ers, there are numerous other devices gaining popularity,
so-called personal digital assistants (PDAs) that can handle
messaging, calendars, personal information management,
reminders, and address book and telephone directory func-
tions. The role for such devices seems certain to grow as
more computing power and communications capability can
be included.
Wireless communications has been another growth area
affecting the system design of mobile computers [15]. From
the beginnings of the Internet, protocol designers have
been fascinated by the attractions of wireless communica-
tions [30], but the lack of bandwidth and the expense of the
equipment has prevented any widespread deployment. As

(cid:211) Baltzer Science Publishers BV

increased bandwidth becomes available and more informa-
tion resources become available by way of the Internet, the
push for inclusion of wireless capabilities in laptop com-
puters will become unstoppable. Adding travel computers
to automobiles will provide new opportunities for making
productive use of the Internet, as well as enabling new
applications for increased road safety.
If the appropriate
wireless signposts are added to the automotive transporta-
tion infrastructure, wireless Internet computing could pos-
sibly help realize the age-old dream of automatic piloting
on long car trips. Reports of road closures and trafﬁc con-
gestion, or even food preferences, could be automatically
taken into account, when the automatic pilot is planning the
best routes.
Before those dreams come true, however, a lot of work
has to be done. Developing the new network protocols is
the theme of this special issue, and this article intends to
provide an overview of the variety of network protocols and
associated technologies at all levels that must be considered
when providing solutions for mobile computer users.

2. Overview

In this paper, we organize the description of mobile net-
working generally according to a classical layered model
of network functions [14]. Each layer, from physical to
application, is affected in various ways in the new operat-
ing environments encountered by mobile computer users.
Although the reduced size and weight of mobile computers
has some effect on their system architecture, these effects
are not dominant because of the terriﬁc advances in sys-
tem miniaturization, display technologies, and communica-
tions. There are many mobile computers envisioned that
will not have hard disks, and many without keyboards, but
these more restricted devices are not principal drivers for
the mobile networking techniques explored in this special
issue, or within this paper. Conversely, many of the tech-
niques and protocols developed for more general purpose
mobile computers can be adapted as needed for the special
or restricted case. A good model, therefore, for the kinds

320

C.E. Perkins / Mobile networking in the Internet
(cid:15) TDMA;
(cid:15) CDMA;
(cid:15) Short range radio.
There are a number of variations for each of the above
channel types.
For the purposes of higher-level protocols, each chan-
nel encoding scheme can just as well be considered as a
new physical medium. Operations within the lowest pro-
tocol layers serve the function of manipulation of various
interface registers to set up the physical layer encoding and
channelization.
The new wireless media becoming available are among
the primary drivers for the interest in mobile computing.
Thus, it is appropriate to understand the nature of wire-
less communications, and the contrast between wireless and
wired media.
For wired media, there is typically:
(cid:15) Well deﬁned broadcast range;
(cid:15) Low bit error rate;
(cid:15) High bandwidth;
(cid:15) Symmetric connectivity.
For wireless media, there is typically:
(cid:15) Point-to-point communication only, or vague and poorly
controllable boundaries for broadcast range;
(cid:15) Variable (time and distance dependent) bit error rate;
(cid:15) Low to medium bandwidth;
(cid:15) Possibly asymmetric connectivity.
These characteristics make protocol design for wireless
communications systems challenging. For instance, one re-
sult of the way wireless broadcast works (when it is avail-
able at all) is that eavesdropping is more difﬁcult to detect
and prevent.

of mobile computers under consideration is a laptop com-
puter with sufﬁcient disk storage and any of a variety of
network interfaces. The variation in the capabilities of the
communication devices is one of the main differentiators
between mobile computers.
Besides minimal weight and size, there are other hard-
ware implications when designing for mobile computing.
Clearly, battery powered operation is highly desirable, and
improvements in battery life continue to extend the feasi-
bility of tetherless computing. On the other hand, the pro-
liferation of mobile laptop computers is driving the creation
of friendlier computing environments, to attract the profes-
sionals who are among the people most likely to own and
operate them. Advances in operating system design for in-
termittently powered I/O devices are being made in order
to further reduce power demands and extend battery life.
Wireless and mobility are not the same, but they are
features which are quite synergistic. It is possible to have
wireless computers that do not move, just as it is possi-
ble to move wired computers from place to place. Clearly,
however, the possibility for wireless data communications
creates an irresistible urge to ﬁnd ways to support mobility
and network access at the same time. As a rule, wireless
dominates the design space at the lower levels in the con-
text of mobile computing, because at the lower levels the
differences between physical media are most visible. At
the network layer and above, mobility dominates. These
design parameters require variability in essential protocol
elements in ways not envisioned by the designers of exist-
ing network protocols.
As mobile computers become smaller and cheaper, it
becomes more feasible to use them as commodity devices
without any personality, much as one might treat a pad of
paper. In this scenario, it becomes important to temporar-
ily allow the notepad computer to operate on behalf of the
user, and to have all the authorization proper for that user.
This can be easily done by allowing the mobile computer to
acquire authorization rights and capabilities from informa-
tion encoded on a smart card owned by the user. It’s easier
and more convenient to carry around a smart card in one’s
wallet or purse, as long as a suitable computer is available
when needed that can acquire the rights and privileges of
the cardholder.

3. Physical layer considerations

At the physical layer, the main objective is to detect the
signals between the two endpoints of a communications
link. While physical layer considerations are among the
most interesting, they do not form the focus of this article
or journal issue. Many different media and channel coding
schemes have been proposed, for instance:
(cid:15) Directional infrared;
(cid:15) Diffuse infrared;
(cid:15) Analog cellular telephone;

4. Link layer considerations

A great deal of attention has been paid to methods for
establishing links between mobile computers and base sta-
tions or access points. One typical method is the creation of
telephone links; the popularity of this method rests largely
on the widespread availability of the physical media which
can be used. Cellular telephones using various technologies
can provide good coverage within the United States, parts
of Asia, and Europe, although no single technology so far
provides sufﬁcient breadth of coverage.
The following operations are among those sometimes
included at the link layer:
(cid:15) Handoffs;
(cid:15) Compression;
(cid:15) Encryption;
(cid:15) Elimination of the hidden terminal problem;

C.E. Perkins / Mobile networking in the Internet

321

(cid:15) Retransmission of garbled data;
(cid:15) Power control;
(cid:15) Neighbor discovery;
(cid:15) Address resolution;
(cid:15) Adaptive error correction.
The next subsections describe these and their importance
for wireless communications systems.

4.1. Handoffs

Central to the concept of seamless mobility is the process
of establishing links at each new connection point. When-
ever this process requires the transfer of state information
from the old connection point (e.g., base station) to the new
one, a handoff has to occur. There are numerous methods
for performing handoffs, as numerous as the kinds of state
information that has been designed for mobile nodes, as
well as the kinds of network entities that maintain the state
information. Often, authentication has to be performed to
ascertain the identity of the mobile node.

4.2. Compression

Compression is often desirable because it reduces band-
width requirements, and that can be very important for
many low-speed wireless media. However, use of compres-
sion at the link layer is problematic in some circumstances,
because the best compression is almost always achievable at
higher level protocol levels, especially the application layer
– for instance, using techniques described in WebExpress,
described in this issue. Compared to the link layer, the
application is much more likely to be able to aggregate
larger data objects into an efﬁcient coding scheme, because
the link layer only has access to the bit stream, not to the
sequence of data objects being transmitted.
Unfortunately, attempting compression at two different
protocol levels is typically less efﬁcient than performing
it at only level, and can have the effect of increasing the
amount of data to be transmitted. Consequently, whenever
higher-level protocols use encryption, the link layer should
be inhibited from attempting any further compression. The
dichotomy between the need for use of compression for
naive applications, and the need to inhibit compression at
the link layer for more intelligent applications, indicates
that any lower-level compression features must be control-
lable by higher-level protocols. This is only one of several
situations where link layer operations must be made visible
(perhaps on a packet-by-packet basis) to higher level proto-
cols. As a result, more sophisticated control strategies are
often needed for use by higher level protocols.

4.3. Security

Whereas the constrained bandwidth of wireless technolo-
gies suggests the use of compression, it is the open propaga-
tion of wireless signals throughout the range of the transmit-
ter that suggests applying security techniques to the wireless

signal before transmission. A number of encrypting link
layer devices and products have been introduced, especially
for use in military applications. Link-layer security intro-
duces further requirements for control of features by appli-
cations for reasons entirely different than were important for
controlling the use of compression. Among the link layer
parameters that may need to be speciﬁed or controlled are:
(cid:15) Whether security features are to be used at all;
(cid:15) The key to be used for encryption;
(cid:15) The encryption algorithm (and mode);
(cid:15) Whether the data must be encrypted for privacy, or
merely authenticated.
The use of security features at the link layer has the ef-
fect of requiring additional processing, which uses more
power and which can signiﬁcantly degrade transmission
speed on high-speed wireless links. Even when the trans-
mitter can handle encryption at high speeds, the receiver
must decrypt at the same speed. Fortunately, there are en-
cryption algorithms [52] which allow relatively speedy de-
cryption by the mobile wireless receiver, which may have
limited power or processing capabilities.

4.4. Hidden terminals

The overall wireless bandwidth available to all mobile
computer users can be improved by increasing the number
of cells (where a cell is considered to be the range of cover-
age of a base station connecting the mobile node to the rest
of the network), reusing the frequencies in each cell, and
reducing the number of mobile computers per cell. Reduc-
ing the number of mobile computers per cell is typically
accomplished by making the cells smaller, so that the ac-
cess points in the cell are within range of fewer wireless
computers. The way that frequencies are used in each cell
has to be managed carefully so that neighboring cells do
not interfere with each other.
Having multiple computers in a cell can give rise to the
hidden terminal problem [6] illustrated in ﬁgure 1, a difﬁ-
culty encountered in the use of wireless communications.
In the ﬁgure, two laptop computers with radio links to an
access point AP (say, a base station) may try to communi-
cate with the access point simultaneously. Each computer
can hear the access point, and cannot directly detect any
interference on the wireless medium. Nevertheless, the ac-
cess point will likely be unable to receive the transmission
from either laptop.
A number of solutions to this problem have been pro-
posed and developed, including MACA [26], W/MACA
[26], IEEE 802.11 [26], and FAMA [26]. Typically, a
sender asks to transmit its data (e.g., by transmitting a RTS,
or request to send), and then waits until the intended re-
ceiver grants permission (e.g., CTS, or clear to send). The
intended receiver, then, does not issue the CTS while it is
receiving data from some other sender. It is better to have
a separate channel for transmitting RTS that will not inter-
fere with data packets. In any case, the sender should make

322

C.E. Perkins / Mobile networking in the Internet

multiple use. For instance, when voice and data are carried
on the same channel, TCP can experience retransmission
anomalies, as analyzed in detail in the paper by Sudhir Ra-
makrishna et al. in this issue. Hybrid schemes are possible
whereby high-level detection of data loss or corruption can
trigger the utilization of retransmission modes by the link
layer protocol.

4.6. Neighbor discovery

Central to any link layer operation is the process of
neighbor discovery, by which a wireless node may deter-
mine which other nodes are within range of transmissions
made using the particular physical medium and/or channel
of interest. Sometimes a particular kind of neighbor is re-
quired, such as a base station, and in those cases the neigh-
bor discovery mechanism must take into account marker in-
formation included in advertisements from the distinguished
neighbor. In other cases, all neighbors might be of interest
to the node, and topological connectivity information will
be exchanged between the neighboring nodes.
Neighborhood information can be dynamic, changing as
the nodes move relative to each other. Thus, neighbor dis-
covery algorithms typically operate periodically, and the
period (rate of repetition) deﬁnes in some sense the respon-
sivity of the collection of nodes. The period should be
chosen so that the neighborhood typically undergoes only
incremental change during the time of a single repetition.
When the rate of motion is too great to sustain the control
trafﬁc needed for neighbor discovery, it probably no longer
makes much sense to model the local physical medium con-
necting two nodes as a link to be established for future
communication. Instead, the physical medium becomes es-
sentially a way to relay broadcast messages, and ﬂooding
is then the only way to get data to a particular destination.
The link layer neighbor discovery algorithms may also
maintain network-layer (e.g., IP) address information for
later use. When such address information is cached for
better performance, node mobility has to be reckoned with
since the cached information can become stale [48]. At
the link layer, cached information which identiﬁes a neigh-
bor node becomes invalid when the node is no longer in
the neighborhood. Additional protocol operations may be
needed to cover for the node during the time it is away
from the neighborhood, if indeed the node is ever expected
to return.

4.7. Power control

As has been noted, additional bandwidth can be made
available to mobile nodes if the same wireless medium can
be used simultaneously by units which are out of physical
range of each other (e.g., by making the cell size smaller).
Maximizing the availability of a medium by spatial re-use,
then, is an important consideration in wireless system de-
sign. The power used to transmit a wireless signal is typ-
ically the dominant factor in determining the range for its

Figure 1. Hidden terminal problem.

sure not to send RTS packets too often, compared with rea-
sonable transmission times for data packets that might be
coming to the intended receiver from other transmitters.

4.5. Retransmission

As mentioned above, wireless media often ﬁnd appli-
cation in situations where error-free transmission cannot
be guaranteed. Problems arise when the wireless stations
begin to move apart from each other, introducing fading
effects as the received signal power decreases. When sig-
nal power becomes about the same as power in the channel
from other sources (co-channel interference), data errors
occur. Noise can overwhelm the received signal power for
other reasons. For instance, a wireless receiver can move
through an area which has some obstacle preventing the
reception of signal from a transmitter, but soon afterwards
might emerge from behind the obstacle. Alternatively, a
noise source can traverse the area between the transmitter
and the receiver. All of these can disrupt the ﬂow of data
between wireless nodes, and cause failures at higher level
protocols. In order to combat temporary corruption or loss
of signal, and the consequent bit errors detected by wireless
receivers, the link layer can be designed to supply acknowl-
edgements for packets received, or to transmit requests for
retransmissions when packet losses are detected. If left for
correction by higher-level protocols, the delays introduced
by timeouts and increased processing requirements cause
substantial degradation of the performance of the wireless
link, which can even affect the user ’s satisfaction with inter-
active applications. Link-layer retransmission can occur on
a time scale shorter than what is possible using retransmis-
sion schemes in higher level protocols (e.g., TCP), and thus
reduced latency for the data stream will be experienced.
Whether or not this is a good idea depends on the ad-
ditional bandwidth required to transmit the sequence and
identiﬁcation information for each packet at the link layer,
as well as the complex interaction between the link layer
and retransmissions performed at higher levels. Additional
complications arise when the channel is multiplexed for

C.E. Perkins / Mobile networking in the Internet

323

reception. Consequently, wireless systems should control
the amount of power used to transmit their data.
In ad-
dition to increasing frequency re-use, reducing power may
increase battery life. On the other hand, reduced range for
signals from mobile nodes increases the probability of loss
of signal.
Link layer protocols can control the transmission power
used by wireless communications adapters to balance these
two needs. If, for instance, a mobile node determines the
amount of power needed to contact an essential subset of
its neighboring nodes, it can set its power level accord-
ingly. Determining the essential subset is, of course, not
always very easy. In some cases, particularly when cluster-
ing algorithms are used in ad hoc networking [7,20,21,28],
some simplifying assumptions are made so that the process
is more manageable.

4.8. Error correction

As error conditions on a wireless link get better or worse,
the number of bits employed for error correction could be
decreased or increased to enable error-free reception. When
the bit-error rate is relatively high, it is better to enable er-
rors to be ﬁxed directly rather than requesting retransmis-
sion of packets as discussed above. However, predicting the
number of error-correction bits needed to assure error-free
reception is not easy. Overestimation wastes a signiﬁcant
fraction of the bandwidth on correction bits that are never
used, and underestimation causes more retransmissions to
be necessary. Even so, when the error rate is somewhat
predictable, this technique often effectively improves the
data rate available over wireless links.

5. Network layer considerations

The Internet Protocol (IP) [50] offers a convenient de-
sign point for introducing the necessary protocol operations
for supporting node mobility. By now, the network layer
operations for mobility support are well understood, and are
speciﬁed in Mobile IP (RFC 2002 [46,48]), a freely avail-
able standard. To understand Mobile IP, it is ﬁrst necessary
to understand IP. For the purposes of this paper, and the
other papers in this special issue, IP may be considered to
offer the following functions:
(cid:15) Identifying each network;
(cid:15) Identifying each node on a network;
(cid:15) Forwarding packets to the correct next hop when they
arrive at an intermediate node (router) which is not the
ﬁnal destination;
(cid:15) Fragmentation and reassembly as needed;
(cid:15) Triggering mechanisms for resolving IP addresses into
lower-level (link layer, or MAC) addresses;
(cid:15) Generating appropriate control and status information
for handling exceptional link conditions.

For the purposes of handling node mobility, the forward-
ing function is the main thing in IP that needs change. Mi-
nor modiﬁcations are also needed to the means by which er-
ror information is propagated through the network. Lastly,
resolving the IP address of a mobile node to a MAC address
by way of ARP (the Address Resolution Protocol [49])
presents a delicate design problem because ARP results are
usually cached, and the cached information goes stale as
soon as the mobile node moves away.
IP traditionally makes next-hop decisions based solely
on the IP address of the destination; these decisions are
not necessarily affected by the mobility of the source of
the packet. To support the mobility of the destination node
using Mobile IP, IP is modiﬁed to tunnel packets to a mobile
node at its current point of attachment to the Internet, as
part of the forwarding process. By this mechanism, packets
arriving at the mobile node’s home agent are then no longer
conﬁned to the network identiﬁed by the mobile node’s IP
address. The new and important additions to IP for handling
node mobility all revolve around the care-of address, which
is the IP address used to identify the mobile node’s current
point of attachment (not the mobile node itself). The care-
of address does not affect the IP address used to identify
the mobile node to the rest of the Internet.
Mobile IP can be understood as three interrelated oper-
ations involving the care-of address:
(cid:15) Advertising it at the new point of attachment;
(cid:15) Registration, or storing it for future use at the mobile
node’s home agent;
(cid:15) Use by the home agent, to tunnel data trafﬁc from the
home network to the network indicated by the care-of
address.

The association between a mobile node’s IP address and the
care-of address it acquires as it moves about is known as a
binding; the binding carries along with it information spec-
ifying how long the association is allowed to be considered
valid.
The papers by Chikarmane et al., and Montenegro and
Gupta in this issue provide additional background on Mo-
bile IP. Other good references may be found in [8,40,45,
46,54].

5.1. Reducing registration frequency

One criticism that has been lodged against the base Mo-
bile IP protocol is the need for possibly frequent reregistra-
tion as the mobile node moves about from place to place.
Such reregistrations can cause dropped packets if the mo-
bile node is far away from its home network and route
optimization is not in use by the foreign agents. Moreover,
in the situation where, say, thousands of mobile nodes are
reregistering upon emergence from a densely traveled ma-
jor tunnel for automobile trafﬁc, the control trafﬁc from the
registration protocol may overwhelm local resources.
For this reason, there has been interest in ﬁnding ways to
enable local processing for Mobile IP messages by the for-

324

C.E. Perkins / Mobile networking in the Internet

eign agents, to reduce network trafﬁc, possibly the number
of registration attempts, the registration time, and conse-
quently the time for which pending registration state in-
formation has to be maintained. One method is to allow
the mobile node to use a multicast address for its care-of
address. Then, any foreign agent belonging to the associ-
ated multicast group will receive all packets for the mobile
node; the designated foreign agent serving the mobile node
will actually deliver the decapsulated datagrams to the mo-
bile node. There is some additional control protocol to
allow one of the foreign agents to be designated as the one
currently serving the mobile node, and to allow new for-
eign agents to assume the designated function as needed
when the mobile node moves. If the foreign agents were
organized as an anycast group [38] the packet would only
have to be delivered to one of the foreign agents. That for-
eign agent would then have to forward the packet to to the
designated foreign agent, with correspondingly higher re-
quirements for transmitting control information, but greatly
reduced storage requirements for most of the foreign agents
in the anycast group compared to the case or the multicast
group.
Another idea is to arrange the foreign agents into a hier-
archy. Then when the mobile node moves, it can restrict its
registration messages to stay within the hierarchy as long
as it can determine that its new point of attachment is in
the same hierarchy as its previous point of attachment. The
common ancestor is the nearest foreign agent that can han-
dle the reregistration, and no further ancestors need to be
aware of the mobile node’s movement. The particular case
when an administrative domain has a “gateway” foreign
agent with many subordinate foreign agents may initially
be a popular design point.

5.2. Route optimization

Aside from the basic operations provided by Mobile IP,
extended operations allow for mobile-aware correspondent
nodes to send their data directly to the mobile node instead
of going through the home agent. This route optimiza-
tion [34,47] is accomplished by sending the mobile node’s
care-of address to correspondent nodes, in so-called binding
updates. Therefore, this technique can only work for such
nodes that are able to process the protocol messages con-
taining the necessary information; today’s product comput-
ers cannot. Route optimization messages have almost the
same need for security that registration messages do in base
Mobile IP, since bogus binding updates sent to correspon-
dent nodes allow the same sort of malicious trafﬁc redirec-
tion that bogus registrations sent to a home agent would
allow. Privacy considerations dictate that the dissemina-
tion of binding updates be controllable by the mobile node,
since they carry information describing the mobile node’s
current location.

5.3. Smooth handoffs

Recent investigations have considered the advisability
of buffering at the foreign agent, as part of a process of
smooth handoffs. The paper in this issue by C ´aceres and
Padmanabhan is one of the ﬁrst published in this area, and
shows that substantial speedups can be obtained with min-
imal buffering strategies. Additional improvements can be
obtained by integrating buffers with regionalized registra-
tions. Handing off the buffered packets can be made secure
by establishing security relationships using the binding up-
date mechanism speciﬁed for use with smooth handoffs in
route optimization.
Application of route optimization is also of particular
interest. If foreign agents are enabled to maintain binding
cache information for a mobile node, then they can im-
prove the robustness of communications with that mobile
node even after the mobile node moves away to a new
point of attachment. When a foreign agent knows the mo-
bile node’s new care-of address, it can forward all packets
for the mobile node to that new care-of address. For ex-
ample, this would help with packets in ﬂight sent to the
mobile node during the time it is trying to complete its
registration process, which might otherwise be lost. Note
that this smooth handoff is even more important when there
are correspondent nodes that are maintaining binding cache
information for the mobile node acquired by use of route
optimization protocol messages.
Smooth handoff is expected to need binding cache infor-
mation only for some hundreds of milliseconds, the amount
of time it takes for mobile nodes to complete a new regis-
tration and to update correspondent nodes with new bind-
ing cache entries. After this time, the previous foreign
agent can drop the binding cache entry for the mobile
node. Moreover, establishing the binding cache entry has
reduced (but nontrivial) security requirements. Replay at-
tacks would generally be ineffective, since the cached infor-
mation has such a short lifetime and a foreign agent would
not accept a new binding for any mobile node not already
in its visitor list.
Providing any security at all for binding updates sent
to a foreign agent by a mobile node may be problematic,
because the mobile node and the foreign agent are not ex-
pected to have any security relationship before the time of
the mobile node’s registration. There are a number of meth-
ods deﬁned by which a mobile node and foreign agent can
establish the necessary security relationship. The meth-
ods deﬁned attempt to use existing security relationships
whenever available, but allow use of Difﬁe–Hellman key
exchange as a last resort. The possibility of a man-in-the-
middle attack, which frequently plagues Difﬁe–Hellman ex-
change protocols, is controlled by using the home agent as
a Key Distribution Center (KDC) and allowing it to au-
thenticate the extension containing the newly created key
for the new security extension between the mobile node
and the foreign agent.

C.E. Perkins / Mobile networking in the Internet

325

5.4. Source routing

Many early approaches to Mobile IP attempted to make
use of IP’s loose source route (LSR) option. This seems
an attractive possibility, because packets sent to a mobile
node can be delivered directly to the mobile node by a
foreign agent if the foreign agent is speciﬁed as part of the
loose source route. Moreover, if the mobile node sends
a packet to a correspondent node and includes the care-
of address in the source route, the correspondent node can
use the source route to return packets to the mobile node,
achieving a cheap form of route optimization. Since IP
speciﬁes that higher-level protocols should reverse source
routes, such source routing approaches accomplish mobile
networking without creating any new protocol.
However, the gains offered by source routing approaches
are, unfortunately, only illusory. In the ﬁrst place, as with
any such remote redirection as indicated by source routes
requiring reversal by the receiver, authentication is required,
and nodes reversing source routes do not typically perform
any such authentication operations. Thus, malicious nodes
could impersonate mobile nodes by sending bogus source
routes. Because of the opportunity for foul play, most In-
ternet routers do not forward source routed trafﬁc, so that
the whole approach is, in practice, unworkable. Moreover,
even if the routers were conﬁgured to handle source routes,
and the end nodes were conﬁgured to require authentication
before reversing source routes, the performance penalty at
the routers proves unacceptable for handling source routes.
All of these factors combine to exclude source routing ap-
proaches from consideration as a solution for mobile net-
working in today’s Internet.

5.5. Mobile IPv6

IP version 6 (IPv6) [16,23] is a new network layer pro-
tocol designed to increase the address space available for
nodes within the Internet, and to improve routability for
packets using IPv6 addresses. As part of the design process,
many deﬁciencies of the current version of IP (also called
IPv4) have been ﬁxed. Support for mobile networking has
been laid out as a mandatory requirement for IPv6 [11],
and the design for Mobile IP has been modiﬁed to take
advantage of IPv6’s superior capabilities.
All IPv6 nodes are able to autoconﬁgure an IPv6 address
appropriate for their current point of attachment to the In-
ternet [35,60]; moreover there are plenty of IPv6 addresses
available, so foreign agents are no longer needed to support
mobility. Furthermore, since all IPv6 nodes are required to
support authentication and privacy protection at the network
layer, binding updates can be supplied in a secure fashion
to the correspondent nodes that need them. This means
that route optimization ﬁts naturally within the framework
offered by IPv6, and does not have to be done as an up-
grade to a huge installed base as with IPv4. Since future
Internet nodes are expected to be capable of mobility [24],
this represents a signiﬁcant reduction in the network load
to be sustained by the IPv6 Internet.

In order to send packets to the mobile node, a routing
header (the IPv6 equivalent of source routing) is used by
any sender that has the mobile node’s care-of address. On
the other hand, whenever a packet arrives at the home agent
instead of going directly to the mobile node, it can be as-
sumed that the sender does not have the care-of address of
the mobile node. In this case, the home agent does not in-
sert a source route to complete the delivery of the packet to
the mobile node. Instead, the home agent is required to use
encapsulation. Thus, the mobile node can tell whenever it
needs to send a binding update to any of its correspondents.
Moreover, when the mobile node moves to a new care-of
address, it assumes that each of its active correspondent
nodes should receive a new binding update. The mobile
node can ﬁnd active correspondent nodes by checking its
TCP protocol control blocks; but this only works for TCP
trafﬁc.

5.6. Vertical IP

Recent experiments at University of California at Berke-
ley (UCB) have shown the feasibility of using Mobile IP to
assist mobile nodes when switching between heterogeneous
physical media. This is important in many applications,
for instance when a mobile node moves from a high-speed
wireless LAN in an ofﬁce environment, to a wide-area wire-
less connection, as with cellular telephones. The main con-
siderations are handling discovery mechanisms in the dis-
parate media, and making policy decisions about when it is
best to change from one medium to another. For instance,
one would like to maintain a high-speed and cost-free con-
nection to the local wireless LAN as long as possible, until
the error rate becomes too high for comfort, and corre-
spondingly to switch back to the wireless LAN as soon
as possible upon re-entering the campus or ofﬁce environ-
ment where it is available. Other considerations such as
security, proxy availability, route selection, or latency may
also come into play. A good example of the work in this
area is the paper in this issue by M. Stemm and R. Katz.

5.7. Multicast

Multicast protocols have, in the past, not been designed
for the case of mobile nodes. In Mobile IP, a mobile node
can pretend to be on its home network and receive tunneled
packets, joining multicast groups through the tunnel. It can
also attempt to join local multicast groups on the foreign
network, but this leads to possibly poor performance in re-
constructing the multicast routing tree after each movement,
and possibly violates some of the implied semantics of mul-
ticast. These design points and many others are explored
in the paper by Chikarmane et al. in this issue.

5.8. Tunneling

Mobile IP depends upon tunneling [22,41,42]. But, tun-
neling also plays a part in other protocol operations of in-
terest to mobile nodes. For instance, access to enterprise

326

C.E. Perkins / Mobile networking in the Internet

computing resources for mobile users often depends upon
establishing a tunnel through the ﬁrewall protecting the en-
terprise computing environment from malicious abuse by
external Internet attackers.
In fact, there seems to be a
gradual convergence of efforts in the areas of mobile net-
working, virtual private networks (VPNs), and dial-up ac-
cess to local or remote points of attachment to the Internet.
One relatively new effort in this area is the Tunnel Es-
tablishment Protocol (TEP) [12], which takes as its initial
design point the fact that Mobile IP is, among other things,
a way to establish a tunnel between two points. For Mobile
IP, the tunnel endpoints are the home agent and the care-of
address, but this can be generalized. In fact, the previous
ideas developed for hierarchical foreign agents (see sec-
tion 5.1) carry over to TEP, and help motivate a way to
establish multi-segment tunnels across multi-level security
domains.
When Mobile IP was speciﬁed, IP-within-IP seemed to
be the most suitable candidate for a default tunneling al-
gorithm. Recent developments call for re-examination of
that decision. Now, newer tunneling protocols such as
L2TP [37] are receiving widespread deployment, and this
author believes that they may represent another opportu-
nity for offering the beneﬁts of mobile computing to a new
population of mobile users.

5.9. Network address translation

Network address translation is becoming a feature with
wide deployment within the Internet. The basic idea is
that a collection of nodes can use private IP addresses in
a network which is attached to the global Internet by way
of a network address translation (NAT) [55] unit, which
“hides” the other nodes’ IP addresses. As data traverses the
NAT unit towards the nodes using the private addresses, the
network layer (IP) addresses in the IP header are translated
from externally known IP addresses to the privately known
addresses of the other nodes.
This technique spells trouble for Mobile IP, because a
care-of address on the “inside” of the NAT unit does not
make sense to a home agent on the “outside”. Until the
NAT boxes can be programmed in detail about how to trans-
late tunnel addresses, and the addresses inside Mobile IP
Registration Requests and Replies, it seems unlikely that
Mobile IP can work across NAT boundaries. This is not
at all trivial to do, considering Mobile IP’s need for au-
thenticating the registration messages; changing any of the
internal ﬁelds would destroy the authentication data. More-
over, since NAT (typically) depends on port numbers, and
IP-within-IP does not have a port number to use, there is
a basic design incompatibility. To overcome this problem,
the NAT device should probably also be the foreign agent.

other commonly available transport control protocols have
not been investigated nearly as often. TCP provides for
congestion control, reliable delivery, and sequenced recep-
tion of datagrams by the destination.
Providing for mobility by modifying TCP cannot be con-
sidered as a complete solution for mobile networking. In
fact, modifying the User Datagram Protocol (UDP) to sup-
port mobility does not make very much sense, because UDP
doesn’t keep track of any state relevant to the source or des-
tination nodes. Neither the mobile node’s IP address nor
anything else about it is used by UDP to identify the state
of the data communication, so nothing can be done by UDP
to help improve the forward progress of communications to
or from a mobile node. RTP is not as widely deployed as
TCP or UDP, and makes up only a tiny percentage of the
total data ﬂowing in the Internet, so that there has been
much less consideration given to the transmission of data
by mobile nodes using RTP.
TCP, however, offers many interesting possibilities.
Careful coordination between the mobile node and TCP
running at a base station can provide the following bene-
ﬁts:
(cid:15) Reduced retransmission delays;
(cid:15) Smooth handoffs;
(cid:15) Improved throughput.
For data streams to or from a mobile node, which ﬂow
through a base station, several investigators have proposed
breaking the data stream into two parts which are handled
separately; both substreams can be terminated at the base
station. Some approaches [3,4,63] suffer from the problem
of providing TCP ACKs to correspondent nodes, for pack-
ets that are never actually delivered to the mobile node.
This violates the well-understood end-to-end semantics of
TCP, and requires very careful handling, or perhaps even
making modiﬁcations to application software.
Going a step further, it is possible to equip TCP at the
base station with the power to transfer internal state related
to the mobile node to a new base station, whenever the
mobile node moves from place to place. Providing for
mobility in this way shares some features in common with
Mobile IP. In the ﬁrst place, it is often presumed that only
the mobile node or the base station can be modiﬁed to
provide the mobility support. As the mobile node moves
to a new point of attachment to the Internet, it must notify
the previous intermediate TCP connection point about its
new location, so that all necessary TCP control information
can be modiﬁed or transferred. This can be considered as a
variation on Mobile IP’s registration procedure, and carries
with it all the same requirements for authentication (and, in
certain applications, privacy).

6.1. Snooping

6. Transport layer considerations

Supporting mobility at the transport layer usually means
modifying the Transmission Control Protocol (TCP) [1];

When a base station is the last hop from the wired Inter-
net to a wireless mobile node, the base station can improve
end-to-end performance by retransmitting lost TCP packets

C.E. Perkins / Mobile networking in the Internet

327

only over the wireless link, while still maintaining TCP’s
end-to-end semantics. These retransmissions are invisible
to the remote connection endpoint, and are comparable in
effect to retransmissions performed at the link layer 4.5.
As the base station delivers packets over the wireless link
to the mobile node, it buffers the packets until the mo-
bile node sends the expected TCP acknowledgement.
If
the acknowledgment does not come (in time substantially
shorter than the end-to-end round-trip-time (RTT)), or, if
the mobile node sends another TCP acknowledgement (a
dupack), the base station can retransmit the lost packet and
avoid end-to-end timeouts and retransmissions. This ap-
proach of snooping and buffering offers big performance
improvements [5].
Unfortunately, recent security protocols preclude inspec-
tion of the relevant packet contents (e.g., TCP sequence
numbers) by base stations. It seems unlikely that the mo-
bile computer user would wish to share its privacy keys
with every base station (or foreign agent) that it establishes
a connection with. There do not appear to be any simple
approaches to this problem, so that performance increases
available from snoopy base stations will be lost for the du-
ration of encrypted data transmissions.

6.3. Asymmetry

Satellite communications with mobile nodes can provide
an important type of wireless connectivity to the Internet.
In many cases, the communications path is then asymmet-
ric, for mobile nodes that do not transmit data back to the
satellite. The mobile node might use a telephone or other
land line to maintain end-to-end connectivity with other
Internet nodes, relying on the satellite link only for down-
loading bulk data (for instance, video information). The
data rate available on the satellite downlink is typically far
greater than the reverse link from the mobile node back to
the Internet. Thus, both the data rate and the routing path
are different.
TCP was not constructed to work well with such asym-
metric data rates. When the asymmetry is too great, the
mobile node cannot supply ACKs back to the source of
TCP data fast enough, and the supply of data to the down-
link operates at far below capacity. Most solutions to this
problem require changes to the Internet node providing data
to the mobile node [2]. As with route optimization, solu-
tions in this class will take a long time to deploy, and will
probably only happen as satellite communications become
important for the general operation of the Internet.

6.2. Errors vs congestion

6.4. Resource reservation

TCP, as commonly implemented, offers advanced fea-
tures for controlling Internet congestion. The primary ob-
servation about such control algorithms, is that control traf-
ﬁc has to be minimized or nonexistent after congestion oc-
curs, because there is a high probability that any control
packets would be dropped, and besides that they add to
the congestion anyway. TCP’s slow start [56] performs
as needed to reduce congestion, by ﬁrst throttling the data
transmission of the connection, and then slowly building
back up to an efﬁcient transmission rate.
The problem comes when errors are mistaken as evi-
dence of congestion. Packets which are lost or garbled
will effectively not be delivered to TCP, and may trigger
the slow-start mechanism. This is bad, because packets
with corrupted data should be retransmitted right away, and
should not cause such a slowdown in the data rate. Thus,
the effect of errors due to wireless media is magniﬁed by
slow-start. Poor interactivity and reduced throughput are
the likely results. It would be better if TCP could be mod-
iﬁed to detect whether a lost packet was the result of con-
gestion or instead was lost because of bit errors [9,10,32].
One theory suggests that packets lost due to congestion
tend to be lost in long contiguous sequences, and that packet
loss because of bit corruption occurs more randomly, inter-
mingled with error-free packet reception. It remains to be
seen whether TCP can be modiﬁed to make the determina-
tion about causes for errors, and whether the determination
can be exact enough to produce improvements in the over-
all response of TCP packet-loss algorithms in real Internet
operational environments.

Mobile IP uses tunnels as part of the path for packets to
be delivered to the mobile node, and that affects the ﬂexibil-
ity of paths reserved for multimedia data between Internet
nodes [59]. Once the tunnel is established, it is not so easy
way for another Internet endpoint to make sure the interme-
diate points of the tunnel are willing to offer the necessary
resources for a new multimedia data stream. This is espe-
cially true because the correspondent node may not even be
aware that it is communicating with a node that is mobile.
Worse yet, the tunnel from the home network to the
mobile node is re-established every time the mobile node
moves to a new point of attachment to the Internet. One
solution to this problem has the mobile node establishing
paths with sufﬁcient resources to a possibly large set of
attachment points [58]. When the node arrives at a par-
ticular point of attachment, the path to that attachment
point becomes active, so that the data can still be deliv-
ered effectively. The downside of this approach is that
many resources are reserved which may never be used, and
the reserved resources remain unused even though they are
available for other uses.
This same problem occurs for multicast delivery of data
to mobile nodes, although in that case there are already
protocol methods in place for pruning the multicast routing
tree so that data need not be delivered to a previous point
of attachment of the mobile node.
For a connectionless algorithm for reserving bandwidth
and making it available to applications needing a well-
deﬁned ﬂow availability, see the paper in this issue by
Murthy and Garcia-Luna-Aceves, which leads the way to
some interesting new research directions.

328

7. Middleware

C.E. Perkins / Mobile networking in the Internet

For the purposes of this paper, we will deﬁne middle-
ware to be the software which does not directly handle
application protocol needs, but on the other hand fulﬁlls, in
a generic way, an intermediate or ancillary role in providing
network services or environment to network applications.
Nomadic computer users, by deﬁnition, change their lo-
cality and thus need to periodically re-establish their link
and connectivity to the Internet. Since the parameters of
such connectivity typically depend upon the characteristics
of the current point of attachment, nomadic users require
that their connectivity be parameterized by those relevant
characteristics. This introduces many problems that are not
very well satisﬁed by existing solutions for network con-
nectivity.
For instance, Mobile IP can be understood as a protocol
to allow parameterization of the IP address of the mobile
node’s current point of attachment (i.e., to allow for vari-
able care-of addresses). But, Mobile IP is not invoked by
application software, and usually is considered to operate
at the network layer; thus it is not middleware. DHCP
(see section 7.2), on the other hand, could (theoretically)
be invoked by applications to obtain application-speciﬁc
parameters, like server IP addresses that can be used by
the client application to initiate a transaction. Thus, DHCP
could be considered as middleware.
In this section we list a few potential candidates for mid-
dleware functions that are likely to become more important
as computers become more mobile.

7.1. Service location

When a nomadic user arrives at a new computing en-
vironment, it is likely, and probably typical, that the user
will be unaware of basic conﬁguration details about local
network services. For instance, there may be a dozen local
printers, each with varying capabilities, and each possibly
useful at various times to the nomadic user. It would be
nice if the user could resolve service needs automatically,
dynamically, and based only on the nature of those needs,
independent of local naming conventions or network topol-
ogy.
This ability to dynamically resolve service needs, which
is a matter of convenience now, is likely to become a ne-
cessity in the service-oriented network of the future. There
is likely to be an increased emphasis on accessing data
across the network, as the Internet becomes more fully
deployed. Consequently, when the network is viewed as
a universal (and robust) resource, applications will begin
to make use of network resources as a matter of course,
much as Web applications now often assume multimedia
capabilities which were quite rare and expensive ten years
ago. If a typical computer hosts applications which together
make use of dozens or hundreds of disparate network re-
sources and services, then typical users are quite unlikely
to be willing to reconﬁgure these applications at each new

Figure 2. SLP agent model.

point of attachment. The number and diversity of network
services will make manual conﬁguration obsolete, and the
ease and speed of network reattachment offered by wireless
communications will make even hard-coded proﬁle-based
reconﬁguration seem quite awkward.
Service Location Protocol (SLP) [61] enables simple ser-
vice requests from user agents to be resolved by receiving
service replies which contain URLs from service agents.
The user agents act on behalf of the application needing
service, and the service agent acts on behalf of the network-
attached service. The protocol for user agents and service
agents is lightweight and places minimal load on the com-
munications medium, as appropriate for typical nomadic
computing platforms.
User agents can obtain the necessary service handles
directly from service agents, or alternatively they can query
a nearby Directory Agent (DA) for the information. These
relationships are illustrated in ﬁgure 2, where the printer is
shown represented by a service agent. In the conﬁguration
shown, the User Agent discovers the Service Agent using
DHCP [39].
SLP offers other features for convenience and scalability
not relevant to this article.

7.2. DHCP and dynamic DNS

The Dynamic Host Conﬁguration Protocol (DHCP)1 is
likely to play a prominent role in the deployment of future
mobile computers. DHCP fulﬁlls the basic requirement for
allocation of an IP address to a node which needs to begin
communications at its new point of attachment. Today,
DHCP is not typically employed by mobile computers, but
is seeing use with portable computers. When a computer
is attached to a LAN, for instance, it can call DHCP to
get its IP address, along with a default router, the domain
name server for the local network, and various other bits of
useful information. This works for mobile computers, too,

1 R. Droms, Dynamic Host Conﬁguration Protocol, RFC 2131 (March
1997).

C.E. Perkins / Mobile networking in the Internet

329

but each time the connection is made the mobile computer
typically needs to be restarted.
Even if the computer could work without restarting,
there are severe difﬁculties with establishing connections
to a mobile computer that relies only on DHCP for its
network attachment. For one thing, most communications
with the mobile node start with its domain name (often,
its Fully Qualiﬁed Domain Name (FQDN)). Each new IP
address would require updating the IP address resolution
for that mobile node’s domain name unless, all commu-
nications with the mobile node are to be initiated by the
mobile node. On the other hand, updating DNS is an op-
eration that can be performed only with very tight security.
If a bogus update were ever accepted for the mobile node’s
domain name resolution, all communications depending on
that resolution would be disrupted and possibly hijacked.
Such security operations are tricky and are only now be-
coming standardized [17,18].
Moreover, there remains a problem with DNS caching.
Whenever the resolution of a mobile node’s domain name
is cached at an intermediate name server, that cache will
be stale as soon as the mobile node moves to a new point
of attachment. Thus, as more and more mobile nodes are
deployed, misusing DNS for this purpose will cause a pro-
portionate increase in the already huge amount of trafﬁc
taken up for name resolution. Combatting the problem by
disallowing DNS resolutions to be cached only adds to an
already worrisome problem in today’s Internet.
Supposing that the appropriate security measures can be
taken, supplying new resolution information for each new
point where the mobile node attaches to the Internet does
not preserve existing communications. And, as wireless
cell sizes decrease (see section 3), this will be viewed as
increasingly inconvenient until ﬁnally it is just unaccept-
able.
If, on the other hand, the mobile node uses Mobile IP,
and is equipped to use DHCP as a mechanism for obtaining
a (co-located) care-of address, it can maintain its existing
home address resolution for its FQDN. This allows simpli-
ﬁed communication with the mobile node at all times, as
well as enabling the node to preserve its ongoing commu-
nications at each new point of attachment. In this mode of
operation, the mobile node can also make use of the de-
fault router conﬁguration delivered to it by DHCP. Since
no beacons may be expected from any foreign agent, the
mobile node with a co-located care-of address may be de-
signed to substitute pings to the default router instead of
detection of agent advertisements. DHCP can also be used
to get information about SLP directory agents at the same
time that the care-of address and default router information
is obtained, as illustrated in ﬁgure 2.

7.3. PPP

Just as a mobile node which employs Mobile IP can use
an IP address acquired from the local DHCP server as its
care-of address, it can also use the IP address allocated to it

by establishing a PPP (dialup) connection as a care-of ad-
dress. In this situation, the mobile node will use the NAS
(Network Access Server) as its default router. Furthermore,
it is possible that the mobile node, by using recently de-
ﬁned PPP extensions [53], can detect whether or not the
NAS is in fact a foreign agent, and thus able to perform
decapsulation for the mobile node.
Mobile nodes using PPP now face a stringent require-
ment for managing authentication (e.g., by using CHAP),
so that the guest network can charge for local network re-
sources used. This authentication process stands in the way
of real mobile networking, and relegates the PPP users to
intermittent connectivity, which can be called portable com-
puting. Automatic ways to perform billing will be needed
before PPP-based approaches can offer seamless mobil-
ity. On the other hand, situations where connectivity is
achieved by discrete dial-up operations do not present the
user with the occasion to expect continuous connectivity
anyway.

7.4. Adaptivity

As indicated previously, a mobile-aware application run-
ning on a mobile node might proﬁtably be designed to take
advantage of information about the link conditions or other
information about the mobile node’s point of attachment.
For a simple example, if the wireless connection offers
only a small amount of bandwidth, it would be fruitless
for an application to attempt to acquire large volumes of
graphic presentation data from the network. Instead, high-
bandwidth graphics should in those circumstances be de-
ferred or eliminated entirely from the data stream. This de-
termination should be made dynamically if possible, since
transient conditions cause great variability in the available
bandwidth to the application. Furthermore, smart wireless
adapters are able to trade off bandwidth for error correction.
While this tradeoff is likely to improve bandwidth availabil-
ity overall, it also directly illustrates the point that wireless
bandwidth should not be considered a constant over the life
of a particular link between a mobile node and the rest of
the Internet.
Bandwidth, however, is not the only wireless parameter
of possible interest to applications. Cost can also be a big
factor, and can determine whether to spool results for later
processing or printing. In many circumstances, bandwidth
is less important than controllable delay bounds. Further-
more, applications may wish to adapt to changing security
factors or make variations in other policy selections.
Well-known methods of adapting to changing condi-
tions make sense when designing mobile-aware applica-
tions, depending upon the form in which the information is
made available. For instance, if the wireless communica-
tion channel parameters of interest are stored in a system
data area, the application may be designed to poll the sys-
tem data at some time granularity. If, on the other hand,
the operating system supports event notiﬁcations as well
as the ability to deﬁne events based on parameter values

330

C.E. Perkins / Mobile networking in the Internet

for the wireless communication attributes, a more efﬁcient,
event-driven system can be designed. One likely strategy
for constructing such events uses the idea of deﬁning high-
water and low-water marks for each appropriate parameter,
with the necessary amount of hysteresis built in to avoid
unnecessary ﬂurries of borderline events. Both system data
and event notiﬁcations are useful for many applications, so
having one should not preclude the other.

7.5. MNCRS

The Mobile Network Computer Reference Speciﬁcation
(MNCRS) is under development by a consortium of com-
panies interested in enhancing the marketability of network
computers, where a network computer is supposed to be
dependent upon external services for its operation. Thus,
a network computer (NC) platform is typically associated
with a department or enterprise server that is specially con-
ﬁgured to serve that kind of computer; moreover, the net-
work computer is considered to belong to a large population
of similar computers, each of which using the same server
or a similarly conﬁgured server. The main differentiation
between different instances of a network computer might
result from automatic interpretation of a customized user
proﬁle, or from speciﬁc interactions with the user.
Schematically, MNCRS looks like a Java API on top,
and some mandatory, standardized networking protocols on
the bottom. This is intended to guarantee device interoper-
ability as well as application portability. The speciﬁcation
applies to four classes of devices:

1. Professional assistant (e.g., a laptop);

2. Information access device (e.g., for calendaring);

3. Basic messaging, paging, and telephony devices.

is organized into four working

Some parts of the speciﬁcation may not apply to all device
classes.
The MNCRS effort
groups:
(cid:15) Data synchronization;
(cid:15) Mobile communications;
(cid:15) Booting and automatic conﬁguration;
(cid:15) Operation for devices in classes 2 and 3.
Data synchronization, derived from ideas developed at
CMU [51], is required to allow mobile wireless nodes to
work independently whenever their network connection is
down, either intermittently or for a protracted period of
time. With reasonable care, the mobile node can cache
enough programs and data from a server, so that the oper-
ation of the mobile node proceeds independently from the
server until connection is re-established. Then, data from
the mobile node can be applied as updates to ﬁles resident
on the server, and ﬁles on the server that have changed
can be resent to the mobile node. Simultaneous inconsis-
tent updates have to be taken care of; with good engineering

design, the problems can be minimized, but the mobile user
will sometimes have to decide manually to resolve conﬂicts.
The mobile communications group has the most inter-
esting job, from the standpoint of the topics covered in
this special issue. That group has the charter to specify
mobile networking protocols, an API application adaptivity
to changing network conditions, tunneling protocols, use
of network computers with Network Address Translation
(NAT) devices, and a messaging API useful (for instance)
by class 3 devices. The network model visible to MNCRS
applications should enable direct communications between
network computers and servers, as well as indirect com-
munications (transparently or explicitly) through proxy de-
vices. Again, network interoperability between NCs, and
between NCs and servers, will rely as much as possible
on network protocols standardized or under consideration
for standardization within the IETF. Examples of protocols
that are speciﬁed as part of MNCRS include HTTP, HTTPS,
Mobile IP, DHCP, IPSec, Service Location Protocol (SLP),
and Secure Sockets Layer (SSL), as well as infrastructure
favorites like IP, TCP, UDP, and DNS.
The working group which is considering Booting and
Automatic Conﬁguration has a difﬁcult task, given to-
day’s infrastructure and the complications of today’s In-
ternet. Some of the protocols just mentioned, for instance
DHCP and SLP, will go a long way towards enabling au-
toconﬁguration Java offers features to help, for instance
the Java Naming and Directory Interface (JNDI), which al-
lows a uniﬁed interface to directory services like SLP and
the Lightweight Directory Access protocol (LDAP), even
though the sorts of directory entries storied in the heteroge-
neous directories might be handled quite distinctly. Along
with autoconﬁguration, MNCRS speciﬁes ﬂexible boot se-
quences, and offers power-saving services by way of Java
APIs. These are currently important considerations for all
mobile devices.

7.6. Environment management

As wireless mobile nodes become more common, and
people are more likely to carry data processing and comput-
ing power with them in various forms, the computing nodes
themselves may be provided with additional environmental
services to support their improved operation. Such envi-
ronmental services may suggest or indicate the selection of
appropriate user proﬁles. For instance, it would be nice if
telephones (cellular or otherwise) did not ring in conference
rooms. If user data is being displayed on a clip-area of a
common white board, the user may wish to disable output
from certain applications for privacy reasons. Privacy con-
cerns may cause the user to run applications in different
modes when roaming away from home, even if just for the
reason that wireless communications seem more vulnera-
ble to eavesdropping than do wireline communications on
building LANs.
In fact, the kinds of environmental considerations that
may come into play at a mobile node’s new point of at-

C.E. Perkins / Mobile networking in the Internet

331

tachment to the Internet are probably so varied and difﬁ-
cult to classify that we cannot make a very good analysis
of future developments in this area. Perhaps protocols such
as ACAP (Application Conﬁguration Access Protocol [36])
will help us organize our directions. On the other hand,
other kinds of environmental interactions are less related
to user application proﬁles, and more related to the user ’s
constantly changing need for information. For instance, it
doesn’t make much sense to route the user ’s data to a local
printer unless the user has some way to physically access
that printer. Similarly, making positional determinations by
GPS or by local building coordinates are both useful at
times, depending upon the execution environment of the
application. Moreover, such application adaptivity is not
naturally modeled as a need for a particular service or net-
work parameter, so that DHCP or SLP is not likely to be
of much help.
Designing mobile-aware applications to make use of
such environment management functions will be the subject
of much future interest.

7.7. Policy determination

Mobile nodes will eventually be equipped with policy
engines to enable applications to make the right decisions
along many dimensions. Some examples have been men-
tioned previously, including choice of graphic resolution,
cost management, and avoiding embarrassment. Nodes us-
ing co-located addresses may be faced with a choice of
which IP address to use, based on high-level considera-
tions [13,64]. If a mobile user is accessing data from a ser-
vice that does not care about the IP address of the source,
the mobile node should more likely use its care-of address
to avoid having trafﬁc go back through the home network.
Likewise, for TCP sessions which are unlikely to last very
long, Mobile IP may offer little advantage. Such sessions
should also be identiﬁed internally by the care-of address
instead of with the mobile node’s home address.
Appropriate application designs are difﬁcult to build in
a modular fashion with today’s tools. What is needed
is a better way for various system modules to determine
which policies should be put into effect at a certain mo-
ment, depending upon environmental considerations, link
conditions, time of day, the user ’s work (or recreational)
purposes, and many other factors. While not exclusively
arising from user mobility, it is mobility that brings these
design needs into sharp focus. Changes in a user ’s com-
puting context naturally correlate very well with changes
in the user ’s physical location.

7.8. Proxies

Not only will mobile nodes rely on standardized mid-
dleware components to simplify their design and operation,
they may well also rely on standardized network compo-
nents to perform specialized services such as voice recogni-
tion, protocol translation, or to provide access to specialized

hardware. These network components are called proxies,
and a great deal of recent interest has been focussed on
ways to ofﬂoad functions from the mobile node to suitable
proxies [27]. Use of proxies (possibly in conjunction with
so-called intelligent agents) offers the following advantages
to mobile nodes:
(cid:15) Power savings;
(cid:15) Reduced storage requirement;
(cid:15) Access to specialized hardware;
(cid:15) Reduced mobile platform maintenance;
(cid:15) Performance (speed) improvements;
(cid:15) Disconnected operations.
On the other hand, reliance on external entities such
as proxies introduces a new requirement for compatibility.
Software on the mobile node could be difﬁcult to upgrade
unless the proxy software is also upgraded to match. Up-
grading a proxy that serves dozens or hundreds of mobile
nodes could be quite tricky. Moreover, the proxy represents
another required stopover for data en route to the mobile
node. As the mobile node moves away from its proxy, it
will see gradual performance reductions. This performance
loss may indicate a need to enable the mobile node to switch
proxies; such a switching operation is new and has not been
fully investigated. Just switching to a closer replica of a
distant database is already tricky [57]. Managing additional
dynamic program behavior could prove infeasible.

8. Security

Security is an increasing concern in the design of mobile
networking protocols and systems. As seen in the discus-
sion about Mobile IP, authentication is critical to authoriz-
ing operations indicating the mobile node’s new point of
attachment. As another example, we have seen how the
link layer can be augmented to supply encryption; the need
for encryption is increased because of the frequently un-
trustworthy nature of the mobile computer ’s surroundings.
Privacy takes on added importance, when the mobile user
does not wish to divulge his or her current whereabouts.
Modern approaches to authentication and encryption use
cryptographic approaches. The algorithmic results are made
unforgeable by including secret keys (possibly with some
additional unique data, such as a timestamp, to avoid match-
ing any previously authenticated data) along with the data
to be authenticated or hidden. Distribution of the secret key
is a difﬁcult problem in today’s Internet [31].
Other security measures common in today’s Internet af-
fect mobile networking. Firewalls, which are installed to
protect an enterprise computing environment from exter-
nal intrusion and/or disruption, make it more difﬁcult for
mobile workers to make use of their ofﬁce computing en-
vironment. Border routers that enforce forwarding policies
based on the source address of packets [19] (as opposed
to the traditional reliance only on the destination address),

332

C.E. Perkins / Mobile networking in the Internet

make it difﬁcult for mobile nodes to use their home address
in foreign domains. This ingress ﬁltering can force even
further detours in the routing path between a mobile node
and its correspondent nodes [13].
Authentication algorithms typically rely on the posses-
sion of a secret key to establish the identity of the sender.
For instance, in Mobile IP, the originator of the Registra-
tion Request implicitly claims to be a mobile node under
the care of the destination home agent. This claim is veri-
ﬁed if the authentication data has been correctly calculated.
Instead of such an implicit claim, authentication could also
be done using other more explicit claims of identity, such as
the node’s FQDN. Using some identiﬁcation other than the
IP address of the mobile node might have the beneﬁcial ef-
fect of enabling mobile networking across NAT boundaries,
for instance.
See the paper in this issue by Gupta and Montenegro
for further discussion of some of the issues in this area,
along with an effective design for ﬁrewall traversal. It is
to be hoped that as more experience is gained with secure
mobile networking, useful techniques will become wide-
spread enough to offer standardized products to be readily
available.

9. Ad hoc networking

Suppose for the moment that the needs for wireless ser-
vices and connectivity could be supplied to a population of
mobile users while they are within range of foreign agents
or base stations connected to the Internet. Next, imagine
that the same users met together at a conference which did
not offer wireless connectivity to the Internet. These users
might still need to communicate data ﬁles to each other,
browse each other ’s Web pages, transact electronic mail, or
use any of the many network applications which have mo-
tivated the tremendous growth of the Internet. They would
ﬁnd that their mobile networking software was useless with-
out the needed infrastructure, and might even seriously get
in the way.
These users need a way to deliver packets between wire-
less stations without infrastructure routers. If all the wire-
less nodes are within range of each other, this is not dif-
ﬁcult. Mobility poses no problem, unless two nodes that
need to communicate have moved out of range from each
other. Otherwise, any necessary routing functions must be
performed by the mobile nodes themselves. Intermediate
mobile nodes could cooperate to forward data from source
to destination.
Ad hoc networking is a name given to the creation
of such dynamic and multi-hop networks that are created
by the mobile nodes as needed for their communication
purposes. The mobile nodes can do this in many ways.
Most solutions involve running routing protocols on the
mobile nodes. Routing protocols have the advantage that
they are inherently multi-hop. Their dynamic behavior re-
quires careful attention, however, because the typical rate

of change in an ad hoc network is likely to be substantially
greater than that for the topology of the Internet, for which
most routing protocols are engineered. There are numerous
routing protocols proposed and in use within the Internet
today, and each of them could be potentially modiﬁed and
applied to the creation of ad hoc networks.
The two main kinds of routing algorithms in use today
are link state algorithms, which provide each node with a
complete representation of the network topology, and dis-
tance vector algorithms. Examples of distance vector rout-
ing algorithms modiﬁed to work in ad hoc networks include
DSDV [43] and AODV [44]. Link-state routing algorithms
(e.g., OSPF [33]) have also been modiﬁed to provide rea-
sonable response for communications between any two mo-
bile nodes, even if they have not been in communication
recently.
Instead of running routing protocols, and thus treating
the ad hoc network as an intranet, mobile nodes can treat
it instead as an incompletely connected physical medium.
In such an approach, all IP addresses are considered to be
part of the same communications medium, but the multihop
nature of the medium requires the cooperation of various
mobile nodes to keep it together. Viewed in this way, the
process of ﬁnding a path to a destination can be handled by
extending ARP to return the layer-2 address of the next hop
towards the destination. The Dynamic Source Routing [25]
method enables source routes to be returned to the ARP
requestor, and extends the domain of applicability further
to handle asymmetric wireless connectivity.
Other ideas which have been applied to the construc-
tion of ad hoc networks include formation of hierarchies,
tracking signal strengths to select the most robust data path,
and maintaining multiple routes for improved handling of
preferential service needs or bounded delay paths. The
IETF Mobile Ad Hoc Network (manet) working group
is attempting to establish standards for creation of ad hoc
networks, and all of the above techniques are receiving at-
tention.
Ad hoc networking presents interesting challenges for
traditional client/server applications. For one example, con-
sider whether DNS might be accomplished in an ad hoc
network. First, there is no clearly deﬁned way for the ad
hoc nodes to discover which node or nodes are offering
domain name resolution. Even without that problem, the
non-hierarchical nature of ad hoc addressing does not map
well into the standard hierarchical domain conventions of
DNS. The trouble is the deﬁning characteristic of the ad
hoc network, which is that the IP addresses of the nodes
are assumed to be unrelated to each other. If, instead, the
IP nodes somehow acquire IP addresses dynamically and
perform some sort of aggregation, their relative movement
would soon make the initial aggregation ineffective. Be-
sides that, it is difﬁcult anyway to cause the nodes to dy-
namically select IP addresses which are unique across the
ad hoc network. Various client/server applications present
other difﬁculties.

C.E. Perkins / Mobile networking in the Internet

333

As it turns out, there are nontrivial issues surrounding
the simultaneous use of ad hoc networks with Mobile IP.
Users would naturally expect that both should be useful
together; a foreign agent attached to an ad hoc network
should provide Internet connectivity to every node in the
ad hoc network. On the other hand, manipulation of the
route table by Mobile IP is not completely consistent with
the way ad hoc routing protocols may wish to do route
table management [29]. Furthermore, the rules for Mobile
IP need to be adjusted or else interpreted correctly so that
the agent advertisements can be delivered to every mobile
node in the ad hoc network.
Use of multicast, RSVP, and other quality of service
issues are not yet widely discussed in the context of ad hoc
networks. As basic protocols become available these topics
will assume additional importance, not least because of the
obvious military applications for ad hoc networks.

10. Conclusion

Mobile computing opens the door to a fresh examination
of practically every area of network protocol engineering.
The areas discussed in this article, and the articles in this
special issue, are only a sampling of the kinds of new re-
search results being reported.
It is my sincere hope that
this special issue will pique the interest of new researchers,
and provide a better overall understanding of the problem
areas needing more attention and new solutions.

Acknowledgement

I would like to express my gratitude especially to all
the authors of this issue, for their patience in handling the
requests of the guest editors, and for their exciting contri-
butions to this issue and to the ﬁeld. Thanks also to the
referees for their reviews, without which the editorial de-
cisions could not have been made. Lastly, thanks to Nitin
Vaidya and those who have helped with the preparation of
this article.

References

[1] Transmission Control Protocol, RFC 793 (September 1981).
[2] M. Allman and D. Glover, Enhancing TCP over satellite channels,
draft-ietf-tcpsat-stand-mech-05.txt (August 1998) (work in progress).
[3] A. Bakre and B.R. Badrinath, Implementation and performance eval-
uation of indirect TCP, IEEE Trans. Computers (March 1997) 260–
278.
[4] A. Bakre and B.R. Badrinath, Handoff and systems support for
indirect TCP/IP,
in: Proc. 2nd USENIX Symposium on Mobile
and Location-Independent Computing, Ann Arbor, Michigan, USA
(April 10–11, 1995) pp. 11–24.
[5] H. Balakrishnan, S. Seshan, E. Amir and R.H. Katz, Improving
TCP/IP performance over wireless networks, in: Proc. 1st ACM
Conference on Mobile Computing and Networking (Mobicom) (No-
vember 1995).
[6] D.F. Bantz and F.J. Bauchot, Wireless LAN design alternatives, IEEE
Network (March 1994) 43–53.

[7] M. Bergamo, R.R. Hain, K. Kasera, D. Li, R. Ramanathan and
M. Steenstrup, System design speciﬁcation for mobile multimedia
wireless networks (MMWN) (draft), Technical report, BBN Systems
and Technologies (October 1996).
[8] P. Bhagwat, C. Perkins and S.K. Tripathi, Network layer mobility: an
architecture and survey, IEEE Personal Communications Magazine
3(3) (June 1996) 54–64.
[9] S. Biaz and N.H. Vaidya, Distinguishing congestion losses from
wireless transmission losses, in: IEEE 7th Int. Conf. on Computer
Communications and Networks (October 1998).
[10] S. Biaz and N.H. Vaidya, Sender-based heuristics for distinguishing
congestion losses and wireless transmission losses. Technical report,
Computer Science Department, Texas A&M University, College Sta-
tion (June 1998) (under preparation).
[11] S. Bradner and A. Mankin, The recommendation for the IP next
generation protocol, RFC 1752 (January 1995).
[12] P. Calhoun and C. Perkins, Tunnel establishment protocol (TEP),
draft-ietf-mobileip-calhoun-tep-00.txt
(December 1997)
(work in
progress).
[13] S. Cheshire and M. Baker, Internet mobility 4x4, in: Proc. ACM
SIGCOMM Conference on Applications, Technologies, Architectures,
and Protocols for Computer Communications, New York (August
26–30, 1996). ACM SIGCOMM Computer Communication Review,
26(4) pp. 318–329.
[14] D.E. Comer, Principles, Protocols, and Architecture. Internetwork-
ing with TCP/IP, Vol. 1, 3rd edn. (Prentice Hall, Englewood Cliffs,
N.J., 1995).
[15] D. Cox, Wireless network access for personal communications, IEEE
Comm. Magazine 30(12) (December 1992) 96–116.
[16] S. Deering and R. Hinden, Internet protocol, version 6 (IPv6) spec-
iﬁcation, RFC 1883 (December 1995).
[17] D. Eastlake, Secure domain name system dynamic update, RFC 2137
(April 1997).
[18] D.E. Eastlake and C.W. Kaufman, Domain name system protocol
security extensions, draft-ietf-dnssec-secext-09.txt (January 1996)
(work in progress).
[19] P. Ferguson and D. Senie, Ingress ﬁltering in the Internet, RFC 2267
(January 1998).
[20] M. Gerla and J.T.-C. Tsai, Multicluster, mobile, multimedia radio
network, ACM J. Wireless Networks 1(3) (July 1995).
[21] Z.J. Haas and M.R. Pearlman, The zone routing protocol (ZRP)
for ad hoc networks, draft-zone-routing-protocol-00.txt (November
1997) (work in progress).
[22] S. Hanks, T. Li, D. Farinacci and P. Traina, Generic routing encap-
sulation (GRE), RFC 1701 (October 1994).
[23] R. Hinden and S. Deering, IP version 6 addressing architecture, RFC
1884 (December 1995).
[24] C. Huitema, IPv6 – The New Internet Protocol (Prentice Hall PTR,
Upper Saddle River, NJ, USA, 1996).
[25] D.B. Johnson and D.A. Maltz, Dynamic Source Routing in Ad Hoc
Wireless Networks (Kluwer Academic, 1996) pp. 153–181.
[26] P. Karn, MACA: A new channel access method for packet radio, in:
ARRL/CRRL Amateur Radio 9th Computer Conference (September
1990).
[27] R.H. Katz, Adaptation and mobility in wireless information systems,
IEEE Personal Commun. Magazine 1(1) (1994) 6–17.
[28] P. Krishna, N.H. Vaidya, M. Chatterjee and D.K. Pradhan, A cluster-
based approach for routing in dynamic networks, ACM Computer
Commun. Review (March 1997) 372–378.
[29] H. Lei and C.E. Perkins, Ad hoc networking with mobile IP, in:
Proc. 2nd European Personal Mobile Communications Conference
(October 1997) pp. 197–202.
[30] B.M. Leiner, D.L. Nielson and F.A. Tobagi, in: Proc. IEEE Special
issue on “ Packet Radio Networks”, Issues in Packet Radio Network
Design 75(1) (1987) 6–20.
[31] D. Maughan, M. Schertler, M. Schneider and J. Turner, Internet
security association and key management protocol (ISAKMP), draft-
ietf-ipsec-isakmp-10.txt (July 1998) (work in progress).

334

C.E. Perkins / Mobile networking in the Internet

[32] G. Montenegro and S. Dawkins, Wireless networking for
the
MNCRS, draft-montenegro-mncrs-00.txt (August 1998) (work in
progress).
[33] J. Moy, OSPF version 2. Request for comments (draft standard)
2178,
Internet Engineering Task Force (July 1997)
(Obsoletes
RFC1583).
[34] A. Myles, D. Johnson and C. Perkins, A mobile host protocol sup-
porting route optimization and authentication, IEEE J. Selected Areas
in Commun. 13(5) (June 1995) 839–849.
[35] T. Narten, E. Nordmark and W. Simpson, Neighbor discovery for IP
version 6 (IPv6), RFC 1970 (August 1996).
[36] C. Newman and J.G. Myers, ACAP – application conﬁguration ac-
cess protocol, RFC 2244 (November 1997).
[37] W. Palter, T. Kolar, G. Pall, M. Littlewood, A. Valencia, K. Hamzeh,
W. Verthein, J. Taarud and W.M. Townsley, Layer two tunneling pro-
tocol ‘L2TP’, draft-ietf-pppext-l2tp-08.txt (November 1997) (work
in progress).
[38] C. Partridge, T. Mendez and W. Milliken, Host anycasting service.
Request for Comments (Informational) 1546, Internet Engineering
Task Force (November 1993).
[39] C. Perkins, DHCP options for service location protocol, draft-ietf-
dhc-slp-04.txt (November 1997) (work in progress).
[40] C.E. Perkins, Mobile IP, Int. J. Commun. Systems 11(1) (March
1998) 3–20.
[41] C. Perkins, IP encapsulation within IP, RFC 2003 (May 1996).
[42] C. Perkins, Minimal encapsulation within IP, RFC 2004 (May 1996).
[43] C. Perkins and P. Bhagwat, Routing over multi-hop wireless network
of mobile computers, SIGCOMM’94: Computer Commun. Review
24(4) (October 1994) 234–244.
[44] C.E. Perkins, Ad hoc on demand distance vector (AODV) routing,
draft-ietf-manet-aodv-02.txt (November 1998) (work in progress).
[45] C.E. Perkins, Mobile IP: Design Principles and Practice (Addison–
Wesley, Reading, Massachusetts, 1998).
[46] C.E. Perkins, Mobile networking through mobile IP, IEEE Internet
Computing Magazine 2(1) (January 1998) 58–69.
[47] C.E. Perkins and D.B. Johnson, Route optimization in mobile-
IP, draft-ietf-mobileip-optim-07.txt
(November 1997)
(work in
progress).
[48] C. Perkins, ed., IP mobility support, RFC 2002 (October 1996).
[49] D.C. Plummer, An ethernet address resolution protocol: or con-
verting network protocol addresses to 48bit ethernet addresses for
transmission on ethernet hardware, RFC 826 (November 1982).
[50] J.B. Postel, ed., Internet protocol, RFC 791 (September 1981).
[51] M. Satyanarayanan, J.J. Kistler, P. Kumar, M.E. Okasaki, E.H. Siegel
and D.C. Steere, Coda: a highly available ﬁle system for a distributed
workstation environment, IEEE Trans. Computers 39(4) (April 1990)
447–459.
[52] B. Schneier, Applied Cryptography: Protocols, Algorithms, and
Source Code in C (Wiley, New York, 1994).
[53] J. Solomon and S. Glass, Mobile-IPv4 conﬁguration option for PPP
IPCP, RFC 2290 (February 1998).
[54] J. Solomon, Mobile IP: The Internet Unplugged (Prentice Hall, En-
glewood Cliffs, 1998).

[55] P. Srisuresh and K. Egevang, Traditional IP network address trans-
lator (traditional NAT), draft-ietf-nat-traditional-00.txt (July 1998)
(work in progress).
[56] W. Stevens, TCP slow start, congestion avoidance, fast retransmit,
and fast recovery algorithms, RFC 2001 (January 1997).
[57] C. Tait and D. Duchamp, Service interface and replica consistency
algorithm for mobile ﬁle system clients, in: 1st Int. Conf. Parallel
and Distributed Information System (1991).
[58] A.K. Talukdar, B.R. Badrinath and A. Acharya, On accommodating
mobile hosts in an integrated services packet network, in: Proc.
Infocom’97 (April 1997).
[59] A. Terzis, J. Krawczyk, J. Wroclawski and L. Zhang, RSVP op-
eration over IP tunnels, draft-ietf-rsvp-tunnel-01.txt (August 1998)
(work in progress).
[60] S. Thomson and T. Narten, IPv6 stateless address autoconﬁguration,
RFC 1971 (August 1996).
[61] J. Veizades, E. Guttman, C. Perkins and S. Kaplan, Service location
protocol, RFC 2165 (July 1997).
[62] M. Weiser, Some computer science issues in ubiquitous computing,
Commun. ACM 36(7) (July 1993).
[63] R. Yavatkar and N. Bhagwat, Improving end-to-end performance of
TCP over mobile internetworks, in: Workshop on Mobile Computing
Systems and Applications (December 1994).
[64] X. Zhao, C. Castelluccia and M. Baker, Flexible network support for
mobility, in: 4th ACM Int’l Conf. Mobile Computing and Networking
(Mobicom’98) (1998).

Charles E. Perkins is a Senior Staff Engineer at
Sun Microsystems, developing Service Location
Protocol and investigating dynamic conﬁguration
protocols for mobile networking. He is an edi-
tor for ACM/IEEE Transactions on Networking,
for ACM/Baltzer/URSI Wireless Networks in the
area of wireless networking, and for ACM/Baltzer
Mobile Networks and Applications. He is serv-
ing as document editor for the mobile-IP work-
ing group of the Internet Engineering Task Force
(IETF), and is author or co-author of standards-track documents in the
mobileip, svrloc, dhc (Dynamic Host Conﬁguration) and IPng working
groups. Charles is also associate editor for Mobile Communications and
Computing Review, the ofﬁcial publication of ACM SIGMOBILE. He is
serving on the Internet Architecture Board (IAB) of the IETF. Charles
has authored a book on Mobile IP, and has published a number of papers
in the areas of mobile networking, ad-hoc networking, route optimization
for mobile networking, resource discovery, and automatic conﬁguration
for mobile computers. Charles has served on various committees for the
National Research Council, and is currently the chairperson of the No-
madicity Working Team of the Cross-Industry Working Team (XIWT).
Charles holds a B.A. in mathematics and a M.E.E. degree from Rice Uni-
versity, and a M.A. in mathematics from Columbia University. He is a
member of ISOC, ACM, IEEE, and the IETF.
E-mail: cperkins@eng.sun.com

Computer Networks 52 (2008) 2260–2279

Contents lists available at ScienceDirect

Computer Networks

j o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / c o m n e t

Nanonetworks: A new communication paradigm
Ian F. Akyildiz a,*, Fernando Brunetti b,1, Cristina Blázquez c,2
a Broadband Wireless Networking (BWN) Laboratory, School of Electrical and Computer Engineering, Georgia Institute of Technology,
250 14th Street, Atlanta, GA 30332, USA
b Bioengineering Group, Institute of Industrial Automation, Spanish Research Council (CSIC), Madrid, Spain
c Departament d’Arquitectura de Computadors Universitat Politècnica de Catalunya, Barcelona, Spain

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 1 February 2008
Received in revised form 1 April 2008
Accepted 15 May 2008
Available online 6 April 2008

Keywords:
Nanonetworks
Nanocommunication
Molecular communication

Nanotechnologies promise new solutions for several applications in biomedical, industrial
and military ﬁelds. At nano-scale, a nano-machine can be considered as the most basic func-
tional unit. Nano-machines are tiny components consisting of an arranged set of molecules,
which are able to perform very simple tasks. Nanonetworks. i.e., the interconnection of
nano-machines are expected to expand the capabilities of single nano-machines by allow-
ing them to cooperate and share information. Traditional communication technologies are
not suitable for nanonetworks mainly due to the size and power consumption of transceiv-
ers, receivers and other components. The use of molecules, instead of electromagnetic or
acoustic waves, to encode and transmit the information represents a new communication
paradigm that demands novel solutions such as molecular transceivers, channel models
or protocols for nanonetworks. In this paper, ﬁrst the state-of-the-art in nano-machines,
including architectural aspects, expected features of future nano-machines, and current
developments are presented for a better understanding of nanonetwork scenarios. More-
over, nanonetworks features and components are explained and compared with traditional
communication networks. Also some interesting and important applications for nanonet-
works are highlighted to motivate the communication needs between the nano-machines.
Furthermore, nanonetworks for short-range communication based on calcium signaling
and molecular motors as well as for long-range communication based on pheromones are
explained in detail. Finally, open research challenges, such as the development of network
components, molecular communication theory, and the development of new architectures
and protocols, are presented which need to be solved in order to pave the way for the devel-
opment and deployment of nanonetworks within the next couple of decades.
Ó 2008 Elsevier B.V. All rights reserved.

1. Introduction

The concepts in nanotechnology was ﬁrst pointed out
by the 1965 nobel laureate physicist Richard Feynman in
his famous speech entitled ‘‘There’s Plenty of Room at the
Bottom” in December 1959. The main focus of his speech
was about the ﬁeld of miniaturization and how he be-

* Corresponding author. Tel.: +1 404 894 5141; fax: +1 404 894 7883.
E-mail addresses: ian@ece.gatech.edu (I.F. Akyildiz), brunetti@iai.csic.
es (F. Brunetti), cristina@ece.gatech.edu (C. Blázquez).
1 This work was conducted during his stay at BWN Lab in 2007–2008.
2 This work was conducted during her stay at BWN Lab in 2007–2008.

1389-1286/$ - see front matter Ó 2008 Elsevier B.V. All rights reserved.
doi:10.1016/j.comnet.2008.04.001

lieved humans would create increasingly tinier and pow-
erful devices in the future. The term ‘‘nanotechnology”
was ﬁrst deﬁned by [59] 15 years later as: ‘‘Nanotechnol-
ogy mainly consists of the processing of, separation, con-
solidation, and deformation of materials by one atom or
by one molecule”. In the 1980s, the basic idea of this def-
inition was explored in much more depth by [26] who
took the Feynman concept of a billion tiny factories and
added the idea that they could replicate themselves, via
computer control instead of control by a human operator.
Activity surrounding nanotechnology began to slowly in-
crease and the advancements really began to accelerate
in the early 2000s.

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

2261

Nanotechnology enables the miniaturization and fabri-
cation of devices in a scale ranging from 1 to 100 nanome-
ters. At this scale, a nano-machine can be considered as the
most basic functional unit. Nano-machines are tiny com-
ponents consisting of an arranged set of molecules which
are able to perform very simple computation, sensing
and/or actuation tasks [57]. Nano-machines can be further
used as building blocks for the development of more com-
plex systems such as nano-robots and computing devices
such as nano-processors, nano-memory or nano-clocks.
Nano-machines can be interconnected to execute collab-
orative tasks in a distributed manner. Resulting nanonet-
works are envisaged to expand the capabilities and
applications of single nano-machines in the following ways:

 Nano-machines such as chemical sensors, nano-valves,
nano-switches, or molecular elevators [4], cannot exe-
cute complex tasks by themselves. The exchange of
information and commands between networked nano-
machines will allow them to work in a cooperative
and synchronous manner to perform more complex
tasks
such as
in-body drug delivery or disease
treatments.
 The workspace of a single nano-machine is extremely
limited. Nanonetworks will allow dense deployments
of interconnected nano-machines. Thus, larger applica-
tion scenarios will be enabled, such as monitoring and
control of chemical agents in ambient air.
 In some application scenarios, nano-machines will be
deployed over large areas, ranging from meters to kilo-
meters. In these scenarios, the control of a speciﬁc
nano-machine is extremely difﬁcult due to its small size.
Nanonetworks will enable the interaction with remote
nano-machines by means of broadcasting and multihop
communication mechanisms.

Communication between nano-machines can be real-
ized through nanomechanical, acoustic, electromagnetic
and chemical or molecular communication means [31].
Nanomechanical communication is deﬁned as the trans-
mission of
information through mechanical contact
between the transmitter and the receiver. In acoustic com-
munication, the transmitted message is encoded using
acoustic energy, i.e., pressure variations. Electromagnetic
communication is based on the modulation of electromag-
netic waves to transmit information. Molecular communica-
tion can be formally deﬁned as the use of molecules as
messages between transmitters and receivers.
Molecular communication is the most promising ap-
proach for nano-networking based on the following
advantages:

 Due to the size and principles of traditional acoustic
transducers and radiofrequency transceivers, their inte-
gration at molecular or nano-scale is not feasible [31].
By contrast, molecular transceivers are intrinsically con-
ceived at nano-scale. These are nano-machines which
are able to emit and receive molecules.
 In nanomechanical communication, transmitters and
receivers need to be in direct contact. This is not a
restriction for molecular communication over large

areas, where transmitters and receivers can be remotely
located as long as the transmitted molecules reach the
intended receiver.

In the recent literature, the term ‘‘nanonetworks” re-
fers to electronic components and their interconnection
within a single chip on a nano-scale [12]. This concept
is also known as Network on Chip (NoC). This term is
also referred to as the network-like interconnection of
nanomaterials as well, e.g., carbon nanotubes arrays
[38,15]. In this paper, we use the term ‘‘nanonetworks”
strictly for the interconnection of nano-machines based
on molecular communication.
This paper follows the bio-inspired approach to explore,
from a telecommunication point of view, the potential of
molecular communication for nanonetworks. First, in Sec-
tion 2, we present the nano-machines including the state-
of-the-art in research and current approaches for their
development. In Section 4, we explain nanonetwork fea-
tures and their advantages and disadvantages over tradi-
tional communication networks. In Section 3, we explain
potential applications of nanonetworks. We explore exist-
ing biological models and techniques for molecular com-
munication for short and long-ranges in Sections 5–7,
respectively. Finally, we outline open research issues and
contrast them with traditional communication network
challenges in Section 8. We conclude the paper in Section
9.

2. Overview of Nano-machines

A nano-machine is deﬁned as ‘‘an artiﬁcial eutactic
mechanical device that relies on nanometer-scale compo-
nents” [24]. Also the term ‘‘molecular machine” is deﬁned
as ‘‘a mechanical device that performs a useful function using
components of nanometer-scale and deﬁned molecular struc-
ture; includes both artiﬁcial nano-machines and naturally
occurring devices found in biological systems”.
In general terms, we deﬁne a nano-machine as ‘‘a de-
vice, consisting of nano-scale components, able to perform a
speciﬁc task at nano-level, such as communicating, comput-
ing, data storing, sensing and/or actuation”. The tasks per-
formed by one nano-machine are very simple and
restricted to its close environment due to its low complex-
ity and small size.
There are three different approaches for the develop-
ment of nano-machines as depicted in Fig. 1. In the top-
down approach, nano-machines are developed by means
of downscaling current microelectronic and micro-elec-
tro-mechanical technologies without atomic level control.
In the bottom-up approach, the design of nano-machines
is realized from molecular components, which assemble
themselves chemically by principles of molecular recogni-
tion arranging molecule by molecule. Recently, a third ap-
proach called bio-hybrid is proposed for the development
of nano-machines [63]. This approach is based on the use
of existing biological nano-machines, such as molecular
motors, as components or models for the development of
new nano-machines.
In Fig. 1, different systems are
mapped according to their origin, biological or man-made

2262

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

Humans

Insects

Nature

Bacteria

Cells

Cell
organells

Bio-hybrid

Molecules

Robots                           Micro-robots           Nano-robots NANOMACHINES
Top-down

Man-made

Computers             Micro-electonics       Nano-electronics
MEMS
NEMS
m                                mm                              μm                              nm
Scale

Fig. 1. Approaches for the development of nano-machines.

Bottom-up

systems, and to their size, ranging from nanometers to me-
ters. In the future, nano-machines will be obtained follow-
ing any of these three approaches. However, the existence
of successful biological nano-machines, which are highly
optimized in terms of architecture, power consumption
and communication, motivate their use as models or build-
ing blocks for new developments.

2.1. Development of nano-machines

2.1.1. Top-down approach
Recently, newest manufacturing processes, such as the
45 nm lithographic process, have made the integration of
nano-scale electronic components in a single device possi-
ble [39]. The top-down approach is focused on the develop-
ment of nano-scale objects by downscaling current
existing micro-scale level device components. To achieve
this goal, advanced manufacturing techniques, such as elec-
tron beam lithography [61] and micro-contact printing [42],
are used. Resulting devices keep the architecture of pre-
existing micro-scale components such as microelectronic
devices and micro-electro-mechanical systems (MEMS).
Nano-machines, such as nano-electromechanical sys-
tems (NEMS) components, are being developed using this
approach [19,34,45]. However, the fabrication and assem-
bly of these nano-machines is still at an early stage. So
far, only simple mechanical structures, such as nano-gears
[66], can be created following this approach.

2.1.2. Bottom-up approach
In the bottom-up approach, nano-machines are devel-
oped using individual molecules as the building blocks.
Recently, many nano-machines,
such as molecular
differential gears and pumps [51], have been theoretically
designed using a discrete number of molecules. Manufac-
turing technologies able to assemble nano-machines mol-
ecule by molecule do not exist, but once they do; nano-
machines could be efﬁciently created by the precise and
controlled arrangement of molecules. This process is called
molecular manufacturing [24].
Molecular manufacturing could be developed from cur-
rent technologies in couple decades if adequate resources
are invested. Current development of nano-machines using
this bottom-up approach, such as molecular switches [5]

and molecular shuttles [6], are based on self-assembly
molecular properties [7].

2.1.3. Bio-hybrid approach
Several biological structures found in living organisms
can be considered as nano-machines. Most of these bio-
logical nano-machines can be found in cells. Biological
nano-machines in cells include: nano-biosensors, nano-
actuators, biological data storing components, tools and
control units [25]. Expected features of
future nano-
machines are already present in a living cell, which can
be deﬁned as a self-replicating collection of nano-machines
[63]. Several biological nano-machines are interconnected
in order to perform more complex tasks such as cell
division. The resulting nanonetwork is based on molecular
signaling. This communication technique is also used for
inter-cell communication allowing multiple cells to coop-
erate to achieve a common objective such as the control
of hormonal activities or immune system responses in
humans.
The bio-hybrid approach proposes the use of these bio-
logical nano-machines as models to develop new nano-
machines or to use them as building blocks integrating
them into more complex systems such as nano-robots. Fol-
lowing this approach, the use of a biological nano-motor to
power a nano-device has been reported in [56]. Another
example in line with this approach is the use of bacteria
as controlled propulsion mechanisms for the transport of
micro-scale objects [10].

2.2. Expected features of nano-machines

The main constraint in the development of nano-ma-
chines is the lack of tools which are able to handle and
assemble molecular structures in a precise way. However,
given the rapid advances in manufacturing technologies,
efﬁcient fabrication of more complex nano-machines will
be possible in the near future. They are expected to include
most of the functionalities of existing devices at micro-
scale. In addition, nano-machines will present novel fea-
tures enabled by molecular properties of the materials that
can be exploited at nano-scale. The most important and ex-
pected features of future nano-machines can be described
as follows:

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

2263

 Nano-machines will be intrinsically self-contained. This
means that each nano-machine will contain a set of
instructions or code to realize the intended task. These
instructions or sequence of operations can be embedded
in the molecular structure of nano-machines, or can be
read from another molecular structure in which the
instruction set is stored.
 Self-assembly is deﬁned as the process in which several
disordered elements form an organized structure with-
out external intervention, as a result of local interactions
between them. At nano-level, self-assembly is naturally
driven by molecular afﬁnities between two different ele-
ments. Self-assembly will leverage the development of
nano-machines and will allow them to interact with
external molecules in an autonomous way.
 Self-replication is deﬁned as the process in which a
device makes a copy of itself using external elements.
This potential process will enable the creation of large
number of nano-machines to realize macroscopic tasks
in an inexpensive way [43]. Similar to the ﬁrst feature,
self-replication implies that the nano-machine contains
the instructions to create a copy of itself.
 Locomotion is the ability to move from one place to
another. Nano-machines are aimed to accomplish spe-
ciﬁc tasks, which are usually described by a spatial-tem-
poral actuation. This means that a nano-machine should
be located in the right place at the right time to accom-
plish the task. However, no single nano-machine is able
to move towards a previously identiﬁed target. More
complex systems could use embedded nano-sensors
and nano-propellers to detect and follow speciﬁc traces
of the target. Locomotion will enable the use of nano-
machines in applications where mobile actors are
needed, e.g., nano-robots for disease treatments [18].
 Communication between nano-machines is needed to
allow them to realize more complex tasks in a coopera-
tive manner. At this level, as explained in Section 1, the
most promising technique is based on molecular com-
munication. Further advances in nano-sensors and
nano-actuators are expected to enable the integration
of molecular transceivers into nano-machines.

2.3. Nano-machine architecture

A nano-machine could consist of one or more compo-
nents, resulting in different levels of complexity, which
could be from simple molecular switches to nano-robots
[18]. The most complete nano-machines will include the
following architecture components:

(1) Control unit. It is aimed at executing the instructions
to perform the intended tasks. To achieve this goal, it
can control all the other components of the nano-
machine. The control unit could include a storage
unit, in which the information of the nano-machine
is saved.
(2) Communication unit. It consists of a transceiver able
to transmit and receive messages at nano-level,
e.g., molecules.
(3) Reproduction unit. The function of this unit is to fab-
ricate each component of the nano-machine using

external elements, and then assemble them to repli-
cate the nano-machine. This unit is provided with all
the instructions needed to realize this task.
(4) Power unit. This unit is aimed at powering all the
components of the nano-machine. The unit will be
able to scavenge energy from external sources such
as light, temperature and store it for a later distribu-
tion and consumption.
(5) Sensor and actuators. Similar to the communication
unit, these components act as an interface between
the environment and the nano-machine. Several sen-
sors and/or actuators can be included in a nano-
machine, e.g., temperature sensors, chemical sensors,
clamps, pumps, motor or locomotion mechanisms.

Currently such complex nano-machines cannot be built.
However, there exist systems found in the nature, such as
living cells, with similar architectures. According to the
bio-hybrid approach these biological models, i.e., the cells,
can be used to learn and understand the principles govern-
ing the operation of nano-machines and their interactions.
This knowledge is expected to contribute to the develop-
ment of new bio-inspired nano-machines and systems for
speciﬁc purposes. In Fig. 2, we show a component mapping

Microrobot node

Sensors

4

Actuators

5

Transceiver
3

Processing and Control
Unit
1

Storage Unit

2

Location
System

Power Unit

Energy Scavenger

6
7

Nanonetwork 
node
(Cell inspired)

1
Nucleus

ADN
2

Vacuoles
7

Gap 
Junctions
3

Mitochondrion
6

Receptors
4

Flagellum
5

Fig. 2. Functional architecture mapping between nano-machines of a
micro or nano-robot, and nano-machines found in cells.

2264

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

between a generic architecture of a nano-machine and a
living cell, including its biological nano-machines. Similar
to the architecture of a nano-machine, a cell contains the
following components:

(1) Control unit. The nucleus can be considered as the
control unit of the cell. It contains all the instruc-
tions to realize the intended cell functions.
(2) Communication. The gap junctions and hormonal and
pheromonal receptors, located on the cell mem-
brane, act as molecular transceivers for inter-cell
communication.
(3) Reproduction. Several nano-machines are involved in
the reproduction process of the cell such as the cen-
trosome and some molecular motors. The code of
the nano-machine is stored in molecular sequences,
which are duplicated before the cell division. Each
resulting cell will contain a copy of the original
DNA sequence.
(4) Power unit. Cells can include different nano-
machines for power generation. One of them is the
mitochondrion that generates most of the chemical
substances, which are used as energy in many cellu-
lar processes. Another interesting nano-machine is
the chloroplast, which converts sunlight into chemi-
cal fuel.
(5) Sensors and actuators. Cells can include several sen-
sors and actuators such as the Transient Receptor
Potential channels for tastes and the ﬂagellum of
the bacteria for locomotion. The chloroplast of the
plants can also be considered as an actuator since
it transforms water to oxygen that is later released
to the environment.

A bio-hybrid approach cannot only be used to develop
novel nano-machines, but also to understand their interac-
tions in larger systems such as cells. These interactions,
exclusively enabled by molecular communication tech-
niques, are essential since this is the only way to explore
their capabilities to achieve more complex tasks in a coop-
erative manner.
In this paper, we expand the bio-hybrid approach be-
yond the models for novel nano-machines in order to study
and develop molecular communication techniques for
their interconnection.

3. Nanonetworks applications

The potential applications of nanonetworks are unlim-
ited. We classify them in four groups: biomedical, environ-
mental,
industrial and military applications. However,
since nanotechnologies have a key role in the manufactur-
ing process of several devices, nanonetworks could be used
extensively in many other ﬁelds such as consumer elec-
tronics, life style and home appliances among others.

3.1. Biomedical applications

The most direct applications of nano-machines and
nanonetworks are in the biomedical ﬁeld. Biological mod-

els inspire and encourage the use of nanotechnologies to
interact with organs and tissues. The advantages provided
by nanonetworks are clearly in terms of size, biocompati-
bility and biostability, enabled by the control of system
components at molecular level. These are some of the
envisaged applications:

 Immune system support. The immune system is com-
posed by several nano-machines that protect organism
against diseases. These nano-machines are a collection
of nano, micro and macro systems, including sensors
and actuators, acting in a coordinated way to identify
and control
foreign and pathogen elements. Nano-
machines can be used to help the detection and elimina-
tion procedures. They could realize tasks of localization
and response to malicious agents and cells, such as can-
cer cells [20,32], resulting in a less aggressive and inva-
sive treatments compared to the existing ones.
 Bio-hybrid implants. These are aimed at supporting or
replacing components such as organs, nervous tracks
or lost
tissues [24,30]. Nanonetworks can provide
friendly interfaces between the implant and the envi-
ronment. Restoration of central nervous system tracks
is a possible application of bio-hybrid implants.
 Drug delivery systems. These are another speciﬁc type of
implants. For instance, these systems could help to com-
pensate metabolism diseases such as diabetes. In this
sense, nano-sensors and smart glucose reservoirs or pro-
ducers can work in a cooperative manner to support reg-
ulating mechanisms [29]. Drug delivery systems could
also help to mitigate the effects of neurodegenerative
diseases by delivering neurotransmitters or speciﬁc
drugs [64].
 Health monitoring. Oxygen and cholesterol level, hor-
monal disorders, and early diagnosis are some examples
of possible applications that can take advantage of in-
body nano-sensor networks [22,30]. The information
retrieved by these systems must be accessible by rele-
vant actors. Thus, nanonetworks must provide the
proper level of connectivity to deliver the sensed
information.
 Genetic engineering. Manipulation and modiﬁcation of
nano-structures such as molecular sequences and genes
can be achieved by nano-machines. The use of nanonet-
works will allow expanding the potential applications in
genetic engineering.

3.2. Industrial and consumer goods applications

Nanonetworks will be used not only the intra-body but
also in industries. Nanonetworks can help with the devel-
opment of new materials, manufacturing processes and
quality control procedures. More speciﬁcally, these appli-
cations have already been proposed:

 Food and water quality control. Similar to health monitor-
ing applications, food and ﬂuids quality control can take
advantage of nanonetworks. Nano-sensor networks can
help detecting small bacteria and toxic components that
can affect to the product quality and cannot be detected

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

2265

using traditional sensing technologies [3]. Advanced
self-powered nano-sensor networks can be used to detect
very small amount of chemical or biological agents
installed in water supplies across the country.
 Functionalized materials and fabrics. Nanonetworks can
be included in advanced fabrics and materials to get
new and improved functionalities. Antimicrobial and
stain-repeller textiles are being developed using nano-
functionalized materials [60]. For instance, nano-actua-
tors can help to improve the airﬂow in smart fabrics.
These nano-actuators can communicate to nano-sensors
to control the proper reaction based on the external
conditions.

3.3. Military applications

Nanotechnologies can also have several applications in
the military ﬁeld. While in the applications pointed out be-
fore, the range covered by nanonetworks is short, in mili-
tary ﬁeld the deployment range of nanonetworks can be
widely variable depending on the application. Battleﬁeld
monitoring and actuation demand a dense deployment of
nanonetworks over large areas, while systems aimed at
monitoring soldier performance are deployed in smaller
areas, i.e., human body. Among the military applications,
we can mention:

 Nuclear, biological and chemical (NBC) defenses. This is a
classic application for large area deployment. Nanonet-
works can be deployed over the battleﬁeld or targeted
areas to detect aggressive chemical and biological
agents and coordinate the defensive response [55]. The
overall system can be compounded of nano-sensors
and nano-actuators, which would detect and control
the hostile agents. Nano-sensor networks could also be
deployed into cargo containers to detect the unautho-
rized entrance of chemical, biological or radiological
materials.
 Nano-functionalized equipments. These applications are
similar to those found in consumer goods ﬁeld, but it
is focused on military equipment. Advanced camouﬂage
as well as army uniforms can take advantage of nanon-
etworks. For instance, equipments can be manufactured
using advanced materials containing nanonetworks that
self-regulate the temperature underneath soldiers
clothes [27] and even detect whether the soldier has
been injured.

3.4. Environmental applications

Since nanoneworks are inspired in biological systems
found in nature, they can also be applied in environmental
ﬁelds achieving several goals that could not be solved with
current technologies. Some environmental applications
are:

 Biodegradation. There is an existing and growing prob-
lem with garbage handling around the world. In this
line, nanonetworks could help with the biodegradation

process in the garbage dumps. Nanonetworks can help
the biodegradation process by sensing and tagging dif-
ferent materials that can be later located and processed
by smart nano-actuators.
 Animals and biodiversity control. Nanonetworks can be
also used in natural environments to control several
species. For instance, as seen in nature, nanonetworks
using pheromones as messages can trigger certain
behaviors on animals. As a result it is possible to interact
with those animals and also to control their presence in
particular areas.
 Air pollution control. Similar to the quality control appli-
cations, air can be monitored using nanonetworks.
Moreover, nano-ﬁlters can be developed to improve
the air quality by removing harmful substances con-
tained in it [35].

4. Communication among nano-machines

Among all of the expected features of future nano-ma-
chines,
the communication capabilities are also very
important. This is the only feature that enables them to
work in a synchronous, supervised and cooperative man-
ner to pursuit a common objective.
Nano-machines communication can include the two
following bidirectional scenarios:

(1) Communication between a nano-machine and a lar-
ger system such as electronic micro-devices, and
(2) Communication between two or more nano-
machines.

Different communication technologies, such as electro-
magnetic, acoustic, nanomechanical or molecular; have
been proposed for each scenario in [31].
Communication based on electromagnetic waves is the
most common technique to interconnect microelectronic
devices. These waves can propagate with minimal losses
along wires or through air. However, given the size of
nano-machines, wiring a large quantity of them is unfeasi-
ble. As an alternative, wireless solutions could be used. To
establish a bidirectional wireless communication, a radio-
frequency transceiver should be integrated in the nano-
machine. Nano-scale antennas could be developed for very
high frequency communication. However, due to the size
and current complexity of the transceivers, they still can-
not be easily integrated into nano-machines. In addition,
if the integration were possible, the output power of the
nano-transceiver would not be enough to establish a bidi-
rectional communication channel [31]. As a consequence,
electromagnetic communication could be used to transmit
information from a micro-device to a nano-machine, but
not from nano-machines to micro-devices, nor among
nano-machines.
At nano-level, acoustic communication is mainly based
on the transmission of ultrasonic waves. Similar to the
communication based on electromagnetic waves,
the
acoustic communication implies the integration of ultra-
sonic transducers in the nano-machines. These transducers
should be capable to sense the rapid variations of pressure
produced by ultrasonic waves and to emit acoustic signals

2266

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

accordingly. Again, the size of these transducers represents
the major barrier in their integration in the nano-
machines.
In nanomechanical communication, the information is
transmitted through hard junctions between linked de-
vices at nano-level. The main drawback of this type of com-
munication is that a physical contact between the
transmitter and the receiver is required. Moreover, this
coupling should be precise enough to guarantee that the
desired mechanical transceivers are aligned correctly. This
communication technique is not suitable in many applica-
tion scenarios where nano-machines are deployed over
large areas without any direct contact between them. In
addition, without precise navigation systems in nano-ma-
chines, their positioning for a correct nanomechanical
communication is a major barrier.
Molecular communication is deﬁned as the transmission
and reception of information encoded in molecules. Molec-
ular communication is a new and interdisciplinary ﬁeld
that spans nano, bio and communication technologies
[46]. Unlike previous communication techniques, the inte-
gration process of molecular transceivers in nano-ma-
chines is more feasible due to the size and natural
domain of molecular transceivers, i.e., nano-scale frame-
work. These transceivers are nano-machines which are
able to react to speciﬁc molecules, and to release others
as a response to an internal command.
Molecular communication can be used to interconnect
multiple nano-machines, resulting in nanonetworks as de-
ﬁned in Section 1. Nanonetworks expand the capabilities of
single nano-machines in the following terms:

 More complex objectives can be achieved if multiple
nano-machines cooperate. Nanonetworks enable this
cooperation by providing mechanisms to exchange
information between different nano-machines such as
molecular motors or nano-switches.
 Single nano-machines can only perform tasks at nano-
level, and therefore their workspace is very limited.
However, if a large number of nano-machines are inter-
connected, they can pursuit macro-scale objectives, and
work over larger areas, such as treatment of cancer
tumors or air pollution monitoring.
 If multiple nano-machines are deployed over large
areas, the interaction with a speciﬁc nano-machine is
extremely difﬁcult due to its small size. This interaction
includes procedures such as nano-machines activation/
deactivation, conﬁguration of parameters, data acquisi-
tion or actuation commands. Nanonetworks will
enable this interaction by providing the infrastructure
and mechanism to broadcast the information over
these areas.
In addition, two nano-machines could
interact indirectly by using other nano-machines as
repeaters.

Besides expanding capabilities of single nano-machines,
nanonetworks represent a potential solution for some
applications where available communication networks
and micro-devices are not suitable. Compared to current
communication network technologies, nanonetworks have
the following advantages:

 The reduced size of nano-machines and the resulting
nanonetwork components can be an advantage in many
applications where the dimension of the involved sys-
tems is critic. For instance,
in the biomedical ﬁeld,
nano-machines can be used for intra-body applications
allowing nanonetworks to lead to nano-invasive and
more selective treatments.
 Biocompatibility is deﬁned as the quality of a device to
operate accordingly in biological environments without
affecting them negatively. In some biomedical applica-
tions, many electronic devices have to cope with hostile
environments as well as many organisms
reject
implants and drugs. Nanotechnologies can be used to
enhance the compatibility between nano-machines
and natural organs or tissues by means of more friendly
materials and interfaces. For instance, bio-hybrid nano-
machines compound by biological elements can interact
with natural processes without any side effect. In addi-
tion, nano-machines and molecular messages may also
be programmed to deactivate after completing the
nanonetwork task preventing removal procedures.
Nanotechnologies, allow the control of materials at
molecular level. Using these materials, we can design
nanonetworks nodes according to speciﬁc environmen-
tal conditions improving the biocompatibility of the
system.
 Chemical reactions are highly efﬁcient in terms of energy
consumption [47]. These reactions will power the nanon-
etworks nodes and processes. Chemical reactions can
also represent complex computation and decision pro-
cesses, which in traditional communication could mean
multiple operations.

4.1. Nanonetworks versus Traditional communication
networks

Nanonetworks are not a simple extension of traditional
communication networks at the nano-scale. They are a
complete new communication paradigm, in which most
of the communication processes are inspired by biological
systems found in nature. Nanonetworks have the following
differences with traditional communication networks:

 In nanonetworks, the message is encoded using molecules;
while in traditional communication networks, the infor-
mation is encoded in electromagnetic, acoustic or opti-
cal signals. Two different and complementary coding
techniques can be considered to represent the informa-
tion in nanonetworks. The ﬁrst one uses temporal
sequences to encode the information, such as the tem-
poral concentration of speciﬁc molecules in the med-
ium. According to the level of the concentration, i.e.,
the number of molecules per volume, the receptor
decodes the received message. For instance, this tech-
nique is used by the Central Nervous System to propa-
gate the neural
impulses. This technique can be
considered similar to those used in traditional networks
where time-varying sequences transport the informa-
tion. The second technique, hereinafter called molecular
encoding, uses internal parameters of the molecules to

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

2267

encode the information such as the chemical structure,
relative positioning of molecular elements or polariza-
tion [46]. In this case, the receiver must be able to detect
these speciﬁc molecules to decode the information. This
technique is similar to the use of encrypted packets in
communication networks, in which only the intended
receiver is capable to read the information. In nature,
molecular encoding is used in pheromonal communica-
tion, where only members of the transmitter specie can
decode the transmitted message.
 The propagation speed of signals used in traditional com-
munication networks, such as electromagnetic or acous-
tic waves,
is much faster than the propagation of
molecular messages. In nanonetworks, the information,
i.e., molecules, has to be physically transported from
the transmitter to the receiver. In addition, molecules
can be subject to random diffusion processes and envi-
ronmental conditions, such as temperature, which can
affect the propagation of the molecular messages.
 In traditional
is
communication networks, noise
described as an undesired signal overlapped with the
signals transporting the information. In nanonetworks,
according to the coding techniques, two different types
of noise can affect the messages. First, as occurs in tradi-
tional communication systems, noise can be overlapped
with molecular signals such as concentration level of
molecules. This means that another source emits the
same molecules used to encode the message, therefore
they affect the concentration sensed by the receiver. In
nanonetworks, noise can also be understood as an unde-
sired reaction occurring between information molecules
and other molecules present in the environment. These
reactions can modify the structure of the information
molecules and therefore the receiver would not be able
to detect the transmitted message.
 Text, voice and video are usually transmitted over
traditional communication networks. By contrast,
in
nanonetworks, since the message is a molecule, the
transmitted information is more related to phenomena,
chemical states and processes [11].
 In nanonetworks, most of the processes are chemically
driven resulting in low power consumption. In tradi-
tional communication networks the communication
processes consume electrical power that is obtained
from batteries or from external sources such as electro-
magnetic induction.

We summarize nanonetworks and traditional commu-
nication network features in Table 1 [37]. Most of the exist-

ing communication networks knowledge is not suitable for
nanonetworks due to their particular features. Nanonet-
works require innovative networking solutions according
to the characteristics of the network components and the
molecular communication processes.

4.2. Nanonetwork components

The ﬁrst models of nanonetworks are based on those
used in Information and Communication Technologies
(ICT) for telecommunication networks. In Fig. 3, we show
the general concepts of nanonetworks versus existing tele-
communication systems. Nanonetworks components are
functionally similar to those found in traditional networks.
In nanonetworks, we can identify ﬁve different compo-
nents: the transmitter node, the receiver node, the mes-
sages,
the carrier, and the medium. Each of
these
components affects the overall communication process,
which includes the following steps as Fig. 3b depicts:

encodes

onto

the message

(1) The
transmitter
molecules.
(2) The transmitter inserts the message into the med-
ium by releasing the molecules to the environment
or attaching them to molecular carriers.
(3) The message propagates from the transmitter to the
receiver.
(4) The receiver detects the message.
(5) The receiver decodes the molecular message into
useful information such as reaction, data storing,
actuation commands, etc.

4.2.1. Transmitters and receivers
The transmitter role is played by a nano-machine such
as a modiﬁed living cell, a biomedical implant, or a nano-
robot. In most of traditional communication networks,
the nodes encode the information by modulating electro-
magnetic signals. In nanonetworks the nodes encode the
information by modifying molecules by means of chemical
reactions. Nano-machines also play the receiver role. The
receiver should be able to extract the message from the
medium. Since nano-machines are very simple, and the
message is basically a molecule, handling this message
could not be a trivial task. The message contains useful
information that is going to be used by the receiver. At
molecular level, this can mean to forward the message,
to store it, or to react to it. Nanonetwork nodes transmit
and receive molecules. These molecules are a ﬁnite source.

Table 1
Main differences between traditional communication networks and nanonetworks enabled by molecular communication [37]

Communication

Communication carrier
Signal type
Propagation speed
Medium conditions

Noise
Encoded information
Other features

Traditional

Electromagnetic waves
Electronic and optical (Electromagnetic)
Light (3  108 m/s)
Wired: Almost immune
Wireless: Affect communication
Electromagnetic ﬁelds and signals
Voice, text and video
High energy consumption

Molecular

Molecules
Chemical
Extremely low
Affect communication

Particles and molecules in medium
Phenomena, chemical states or processes
Low energy consumption

2268

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

RX

a

b

Propagation 
speed (Light)

TX

Electromagnetic
waves

Matter
(Molecules)

TX
Nano-machine

Encoder

RX

RX
Nano-machine

Noise

Sensitive to
obstacles

Low propagation 
speed
(Random process)

Decoder

RX
Nano-machine

Fig. 3. Overview of (a) traditional communication systems and (b) nanonetworks: energy transmission vs. molecular transmission.

This has two consequences. First, the transmitter nano-ma-
chines should be able to obtain, from an external source,
raw molecules and store them for a later use as messages.
Second, the receiver nano-machines should be able to buf-
fer a limited number of molecules, and also include release
mechanisms to empty this molecular buffer to be ready for
the next messages.

4.2.2. Message
In traditional computer networks, the information is
represented using a binary system and the transmitted
message is usually a set of bits. In molecular communica-
tion, the message is a molecule. This molecular message
will have three important characteristics [31]. First, it will
present a predeﬁned external structure that will allow an
easy recognition at the receiver. Second, it will be inactive.
This means that molecular messages will not be prone to
react to other molecules in the medium. Finally, molecular
messages should easily be eliminated without any side ef-
fect once they are decoded at the receiver nano-machine.
In [37], some molecular features such as polarity, motion,
magnetization, and structures, are suggested for informa-
tion coding. A potential candidate for molecular messaging
is the partially ﬂuorinated polyethylene proposed in [24].

This molecule can be compounded by a sequence of hydro-
gen and ﬂour atoms, resulting in a binary sequence able to
store digital data [9].

4.2.3. Carrier
In classical communication paradigm, a carrier is used to
transport the message. The carrier is used because of its sig-
nal feature advantages, especially in terms of propagation.
In traditional computer networks, the carriers also allow
creating multiple communication channels. In nanonet-
works, the carriers are particular molecules which are able
to transport chemosignals or molecular structures contain-
ing the information. The aim of the molecular carriers is to
enhance the propagation features of single information
molecules, to create more reliable communication by pro-
tecting molecules from external noise sources, and ﬁnally
to enable the creation of multiple independent channels
using the same medium. The use of molecules as informa-
tion carriers in molecular communication was observed in
biological systems. According to these observations, two
potential types of carriers have been identiﬁed: the molecu-
lar motors and the calcium ions [50]. Molecular motors, e.g.,
kinesin, dynein and myosin, are proteins that can generate
movements using chemical energy. These protein motors

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

2269

can transport a data packet, i.e., molecule, from the trans-
mitter to the receiver as described in [49]. The second type
of carrier consists of calcium ions (Ca2+). The transmitter can
modulate the concentration of these ions in amplitude and
frequency to encode the information [17].

4.2.4. Medium
Wireless communication networks have been devel-
oped for almost every type of media found in nature such
as airborne, waterborne, and underground. Many wireless
transmission technologies have also been applied success-
fully to communicate with implantable devices. The propa-
gation of typical network signals, such as acoustic, optical
or electromagnetic, can be affected by the medium condi-
tions. In molecular communication, the medium can be
wet or dry, e.g., in-body or environmental monitoring nan-
onetworks, and the propagation is even more dependent on
the medium conditions. The speed of the medium, which is
faster than the propagation speed of the molecules, can af-
fect the communication between nano-machines. In addi-
tion, physical obstacles on the signal pathway can impede
the propagation of the molecules, which can go through so-
lid objects. These radical changes oblige the review of many
common communication concepts such radiation, range, as
well as the development of new propagation models.

5. Short-range communication using molecular motors

First nanonetworks models are inspired by molecular
communication schemes observed in biological systems.
Biological nanonetworks are used for intra-cell, inter-cell
and intra-specie communication. Intra-cell and inter-cell
communication are referred as short-range techniques
due to the size of living cells and their internal compo-
nents. Thus, in the framework of nanonetworks, we classify
as short-range the communication process that takes place
in a range from nm to few mm. Complementarily, we clas-
sify as long-range communication, the interconnection of
two nano-machines in a range of mm to km as occurs in
pheromonal communication.
Most of the intra-cell communications are based on
molecular motors. Molecular motors, e.g., dynein, are pro-
teins or protein complexes that transform chemical energy
into mechanical work at a nano-scale [16]. Their use as
information shuttles or communication carriers for nano-
machines within a short-range has been widely proposed
[21,46,54]. These molecular motors can be found in
eukaryotic cells in living organisms.
Inside the cells, molecular motors are aimed at trans-
porting essential particles among organelles during differ-
ent stages of the cell life cycle. They travel or move along
molecular rails called microtubules. These microtubules
are widely deployed setting a complete railway network
for intra-cell substance transportation. The microtubules
go from the centrosome, i.e., cell organelle, outwards the
cell membrane resembling a star topology in communica-
tion networks.
Speciﬁc molecular motors able to carry other molecules
use these intracellular molecular rails. Depending on the
cargo, molecular motors move at a speed up to 400 mm/

day. The ability to move molecules makes molecular mo-
tors a feasible way to transport information packets, i.e.,
molecules, from the transmitter to the receiver nano-ma-
chine. As an additional advantage, molecular motors and
microtubules can be used as a bio-hybrid communication
interface between man-made nano-machines and biologi-
cal structures. For instance, nano-machines could use this
communication interface to interact with cell organelles
to achieve speciﬁc tasks, such as drug delivery.
The deployment of the microtubules, i.e., network infra-
structure, to support the movement of the molecular mo-
tors is a key issue. Two different solutions could be used
for the deployment of the network infrastructure. The ﬁrst
one is based on natural mechanisms found in cells, in
which microtubules can be developed by means of molec-
ular self-assembly [62]. The second solution is based on the
development of lithographic tracks of molecular motors to
interconnect nano-machines. These tracks, which are made
of molecular motors attached to a surface, can move struc-
tures similar to the microtubules from one point to another
[36]. In this solution, the information molecule is not at-
tached to the molecular motor but to the microtubule or
a structure with similar characteristics [48].
The concept behind these two solutions is the same. In
both solutions, the movement created by molecular mo-
tors is used to transport the information molecules. Fur-
ther description of
this molecular
communication
technique will consider the scenario according to the ﬁrst
solution. The scenario is assumed to include system com-
ponents such as pre-deployed microtubules between
nano-machines and traveling molecular motors. These
components, shown in Fig. 4, should present speciﬁc fea-
tures to support all of the communication processes be-
tween the nano-machines.

5.1. Communication features

Molecular communication enabled by molecular mo-
tors takes place in aqueous medium. The environment
should include the necessary components at biologically
appropriate conditions [28], such as temperature, humid-
ity, medium viscosity and pH. Due to the organic and
chemical nature of the involved nano-machines and infor-
mation packets, the nanonetwork is highly sensitive to
these conditions and the communication process can be
negatively affected by sudden variations of the environ-
mental conditions.
In molecular communication based on molecular mo-
tors, the communication process includes one transmitter
and only one receiver. When the communication process
occurs, the information molecule is altered and the en-
coded information is lost. Therefore the molecule informa-
tion restricts the communication process to one receiver.
The outcome is similar to a point-to-point physical com-
munication link in traditional communication networks.
To implement more complex communication schemes
including multihoping techniques, the receiver nano-ma-
chines should be able to amplify, decode and redirect the
information.
On the transmitter side, the information molecules
are loaded on molecular motors, which transport the

2270

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

Molecular rail

TX

Encoder

Molecular motor

RX

Decoder
De de

Fig. 4. System components in molecular motors communication systems.

information along the microtubules to the receiver.
The packets can be encapsulated in vesicles. A vesicle is a
ﬂuid or an air-ﬁlled cavity that can store or digest cellular
products [1]. The objective if this encapsulation of the
information is twofold [48]. First,
it allows enhancing
the compatibility between the information molecule and
the molecular motor, enabling the use of diverse types
of molecules
as
information packets.
Second,
the
encapsulation protects the information molecules avoiding
them to react with antagonistic receptors present in the
medium.
The network infrastructure should be deployed prior
to the beginning of the communication process. Several
microtubules could be deployed to interconnect one
node, i.e., nano-machine, with many others. Depending
on the nanonetwork infrastructure, a transmitter nano-
machine will be able to use unicasting or multicasting
mechanisms. To implement the ﬁrst mechanism, the
transmitter should be able to select a speciﬁc molecular
rail to transmit the information molecule to the intended
receiver. To achieve a multicast transmission, the trans-
mitter could release several molecules, containing each
one the same data, which are self-assembled onto molec-
ular motors traveling on different molecular rails and
hence different destinations. The propagation of molecu-
lar motors along a microtubule is unidirectional. The
polarity of
the microtubule indicates the movement
direction of speciﬁc molecular motors, e.g., kinesin moves
towards the (+) end of the microtubule, and dynein to-
wards the ( ) end. Thus, bidirectional communication
links can be obtained using different molecular motors
or a pair of opposite microtubules between two nano-
machines.

5.2. Communication process using molecular motors

Molecular motors are used to carry vesicles containing
information molecules that are transmitted from the trans-
mitter to the receiver. To facilitate the reception, the trans-
mitter uses protein tags that bind to speciﬁc receptors on
receiver nano-machines. Once the infrastructure is de-
ployed between the transmitter and receiver nano-ma-
chines, the process includes the following tasks [46]:

 Encoding. This task involves the generation of the infor-
mation molecules. When an external stimulus is applied
to transmitter nano-machines,
they generate these
information molecules. The encoding process consists
of selecting the right molecules that represent the infor-
mation or the reaction to be invoked at the receiver.
Thus, it is possible to control the reaction at the receiver
by selecting the proper stimulus applied to the transmit-
ter nano-machine.
 Transmission. This is a key task in this scenario and fur-
ther research is needed to establish the process to attach
the information packet to carrier molecules in an accu-
rate way. The feasibility of this process is based on
ligand-receptor binding process [41]. A ligand molecule,
usually a protein, can bind to another larger molecule
referred as receptor according to the afﬁnity between
them. In chemical terms, afﬁnity is deﬁned as the trend
of dissimilar elements to form chemical compounds
based on their electronic properties. The transmitter
nano-machine should guarantee the high afﬁnity
between the information molecules and the molecular
motors. If this afﬁnity exists, the information molecule
can bind to the molecular motors to start the propaga-
tion process. Encapsulation techniques can be used to
ensure the afﬁnity between the information molecule
and the molecular motors. Vesicles are biological cap-
sules with high afﬁnity with molecular motors, there-
fore, they can be used as standard interfaces between
different information molecules and molecular motors.
Other biological process such as cell ﬁssion, reproduc-
tion or pore formation could also be used to release
information packet to the medium [48].
 Propagation. This part of the process refers to the move-
ment of the information molecules through the medium.
Microtubules or molecular rails interconnecting nano-
machines restrict the movement to themselves. As a
result, molecules move in the direction of these rails
instead of diffusing or moving randomly through the
medium. In this scenario, the propagation is similar to
propagation of electrical signals through conducting
materials. The difference lies on the propagation speed,
which will depend on the information molecule, the
molecular motors and the molecular rail. The natural

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

2271

domain for molecular motors is intra-cell communica-
tion. However, new advances in biomolecular science
can enable inter-cell and bio-hybrid scenarios, as
reported in [36].
 Reception. It is the process in which molecular motors
containing information molecules arrive at the receiver
nano-machine. The information molecules are detached
from the molecular motors. This is achieved thanks to
the afﬁnity between the protein tags or the molecular
encapsulation and receptors at the receiver. A receiver
can have several reception properties in order to detach
different information molecules from the molecular
rails. Similar to the transmission, the extraction of
information from a vesicle can also be done through
different cellular processes such as fusion or pore forma-
tion [58].
 Decoding. The receiver nano-machine invokes
the
desired reaction according to the received information
molecule. These biological reactions depend on the
information molecule sent by the transmitter nano-
machine. A nano-machine can be equipped with several
receptors. Each receptor would be able to react to a spe-
ciﬁc information molecule. The decoding can be highly
associated to these speciﬁc receptors. For instance, each
receptor can be mapped to a speciﬁc action of the
receiver nano-machine. Another option is to decode
the information molecules inside the nano-machines.
In this case the nano-machine could have one receptor
and the different messages should use a common inter-
face such as vesicles. As we can see, molecular motors
just provide a means to transport the information from
one point to another. Nano-machines will have to
include all the mechanisms to support traditional net-
work tasks such as message coding/decoding, multi-
hopping and routing.

6. Short-range communication using calcium signaling

Inter-cell communication based on calcium signaling is
one of the most well-known molecular communication
techniques. It is responsible for many coordinated cellular
tasks such as fertilization, contraction or secretion [11].
Calcium signaling is used in two different deployment sce-
narios. It can be used to exchange information among cells
physically located one next to each or among cells de-
ployed separately without any physical contact. Depending
on the scenario, the propagation of the chemosignals in-
cludes different messengers such as ATP substance, cal-
cium ions (Ca2+) or
inositol 1,4,5-triphosphate (IP3)
among others.
The description of the sequential propagation of the
chemosignals driven by different messengers is known as
the chemosignal pathway. One messenger transports the
information until certain point of the pathway where the
information is transferred to another messenger. The infor-
mation transfer from one messenger to another continues
until the information reaches its destination. This multi-
messenger propagation scheme presents two advantages.
First, the signal can be ampliﬁed at different levels of the
chemosignal pathway. The ampliﬁcation is usually done
when the information is transferred among messengers.

Second, we could use a set of messengers, i.e., chemical
agents, to obtain different signal transduction and decod-
ing results. The combination of different messengers could
lead to different results and could be used to adapt the
propagation parameters to a speciﬁc environment. The li-
gand-receptor binding principle is one of the most impor-
tant processes participating in the information transfer
among messengers. This process consists in the bounding
of two molecules resulting in a local reaction, which in turn
can trigger other processes.
If cells are deployed separately in the chemosignal
pathway, as it is shown in Fig. 5b, the information mole-
cule, i.e., the ligand, binds to a molecular receptor located
on the external membrane of the cell. This binding gener-
ates an inner cell signal, which can be decoded by cell com-
ponents. In this case, the ligand is considered as the ﬁrst
messenger while the inner signal molecules are considered
as second messengers of the chemosignal pathway. First
messengers are molecules that transport the information
outside the cell. Complementarily,
second messengers
transport the information inside the cells. Membrane
receptors convert the ﬁrst messenger signals into second
messenger signals by chemical reactions.
When cells are located next to each other, as it is shown
in Fig. 5a, they can be connected through gap junctions.
These biological gates allow different molecules and ions
to pass freely between cells [1]. In this deployment sce-
nario, the signal travels along cells using second messen-
gers, such IP3 and without
the intervention of ﬁrst
messengers. The IP3 is a messenger that provokes the re-
lease of calcium ions by cell organelles. Thus, the diffusion
of IP3 through gap junctions propagates a Ca2+ wave along
the interconnected cells.
Several cellular components participate in the ﬂux reg-
ulation of Ca2+ that makes signal propagation possible. The
storage of Ca2+ inside the cell is enabled by certain proteins
of the cytoplasm and organelles such as the endoplasmatic
reticulum that can act as Ca2+ buffers or reservoirs. Extra-
cellular mechanisms, such as ion channel located on the
cell membrane, are also involved in this process in order
to maintain this ﬁnite resource.
Calcium signaling provides a mean to interconnect
neighboring cells. Unlike communication based on molec-
ular motors and molecular rails, as explained in Section 5,
calcium signaling provides more ﬂexible transmission
schemes. Using calcium signaling, all the surrounding
nano-machines can receive a message sent by a unique
transmitter. This communication technique is similar to
broadcast networks in which all the transceivers located
in the transmission range of the transmitter can sense
the electromagnetic signal.

6.1. Communication features

Calcium signaling can be used for short-range commu-
nication among several nano-machines [50]. Similar to
natural models, nano-machines should be near each other.
Similar to the natural models, the propagation of the
information can
be
performed
in
two
different
schemes depending on the deployment of the nano-
machines:

2272

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

TX

Encoder

Calcium
Signalin

Gap
Junctio

TX

Encoder

Calcium
Signaling

Diffusion
Process

RX

Decoder

RX

Decoder

Fig. 5. Signal propagation in calcium signaling communication systems by (a) gap junctions signal forwarding and (b) by diffusion.

 Direct access.
If nano-machines are physically con-
nected, Ca2+ signals travel from one nano-machine to
the next one through the gates, as shown in Fig. 5a.
These gates should work similarly to the gap junctions
allowing the ﬂux of
ions and molecules from one
nano-machine to the interior of another nano-
machine.
 Indirect access. If the nano-machines are not in direct
contact, the transmitter nano-machine should be able
to release the information molecules to the medium
as ﬁrst messenger in the chemosignal pathway. Infor-
mation molecules will move through the medium fol-
lowing a diffusion process. They will generate a
calcium signal
inside the receiver nano-machine.
Transmitters can encode the information varying the
concentration of ﬁrst messengers
shown in
as
Fig. 5b. Biological systems encode the information on
frequency and amplitude of concentration changes,
usually referred to as Ca2+ oscillation and Ca2+ spikes
[11].

These two propagation schemes enable the formation of
networks supporting multicast or broadcasting transmis-
sion mechanisms. The overall communication system
could work as follows: transmitter and receiver nano-ma-
chines are connected to each other through a signaling net-
work consisting of interconnected nodes, which propagate
the information using Ca2+ signals.
Due to the close interaction between nano-machines,
the propagation of the information is not affected by the
natural speed of the medium. However, the channel is
not expected to be noiseless. Ca2+ can bind easily to
charged particles that can be found in the network
medium. A nanonetwork could use several different mes-
sengers to design the chemosignal pathway according to
the working conditions. Nanonetworks should also be
aware of potential communication process running in the
environment. One of the potential drawbacks on using cal-
cium signaling is the interference that can be caused in
biological process
such as muscle contractions or
neurosignals.

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

2273

6.2. Communication process within calcium signaling

For a better description of the communication process,
we use two different network scenarios as shown in
Fig. 5a and b, respectively. The direct access scenario con-
tains one transmitter nano-machine and many other cells
or similar nano-machines that are able to propagate the
signal, i.e., the message. These components are physically
located one next to each other and they are interconnected
through functional gates or gap junctions. The receiver
nano-machine, to which the message is addressed, can be
any of these nano-machines members of the network.
The indirect access scenario includes nano-machines that
are deployed separately and where the signals propagate
along the medium where the nano-machine are deployed.
In these two scenarios, the communication process
based on calcium signaling includes the following steps:

 Encoding. This task refers to the generation of the infor-
mation molecules. If neighboring nano-machines are
used for the propagation of the signal as occurs in direct
access scenario, the transmitter nano-machine encodes
the information using Ca2+. The information is precisely
encoded in amplitude and frequency of the function
describing the concentration of Ca2+ signal. These
encoding methods are known as amplitude modulation
(AM) and frequency modulation (FM). In the indirect
access case, the transmitter should encode the informa-
tion in the molecule to be used as a ﬁrst messenger. The
ﬁrst messengers could also be encoded using AM and
FM techniques. An external stimulus can be used to start
the generation encoding process. For instance, it has
been reported that some mechanical stimulus applied
to cells provoke the generation of IP3 substance inside
the cell. The presence of IP3 unleashes the release of
Ca2+.
 Transmission. This task involves the signaling initiation.
In direct access scheme, transmitter nano-machines
stimulate neighboring cells and consequently the signal-
ing process starts. The signaling generates the initiation
of propagation of Ca2+ waves. IP3, which was generated
previously in the transmitter nano-machine, starts ﬂow-
ing into neighboring cells through the gates or gap junc-
tions. Thus, neighboring nano-machines would release
more Ca2+ driven by the presence on IP3 substance. In
the indirect access case, transmitters may initiate sig-
naling by releasing substances to the environment. Sim-
ilar processes to cell ﬁssion or pore formation could be
used by nano-machines to release the information mol-
ecules to the medium.
 Signal propagation. When nano-machines use direct
access, IP3 transmitted to neighboring cells or nano-
machines induces the release of Ca2+ from the IP3-sensi-
tive Ca2+ stores. The diffusion of the IP3 substance con-
tinues to new neighboring nano-machines inducing
again the release of Ca2+ on these nano-machines. This
chain reaction provokes an increase of the Ca2+ concen-
tration and as a result, the Ca2+ wave propagates across
the networked nodes affected by IP3. This propagation
can be controlled varying the permeability of the gates
or gap junctions. When nano-machines use indirect

access, the propagation of ﬁrst messengers containing
the information can be described by using diffusion or
Brownian motion models. When these information mol-
ecules bind to the receptors of the receiver nano-
machines, they are translated into Ca2+ internal signals.
The indirect access is much more sensitive to the med-
ium conditions than the direct access. Transmitter
nano-machines should consider the medium conditions
such as wind, ﬂow, temperature and noise to ensure that
the information molecules will arrive to the intended
receiver.
 Reception. When using direct reception, receiver nano-
machine establishes gap junctions with the neighboring
cells and perceives the Ca2+ concentration from inside of
these cells. Once the message is received, receiver nano-
machine can stop the IP3 propagation by closing the
gates or gap junctions connecting with other nano-
machines. In the case of indirect reception, the receptor
plays the most import role. They have to translate the
information molecule into internal Ca2+ signals. A
nano-machine could be equipped with several receptors
capable to detect different information molecules. This
can be used to establish several parallel communication
channels among different nano-machines.
 Decoding. The receiver nano-machine reacts to the inter-
nal Ca2+ concentration. The concentration depends on
the inﬂux of IP3 and Ca2+ that arrives to the receiver or
on the bindings occurred between information mole-
cules and nano-machines receptors. The Ca2+ signal
can be encoded in amplitude and frequency enabling
the activation of different processes. The basic difference
between the receiver and those nano-machines or cells
participating in the signal propagation is that the recei-
ver can decode the message, while other nano-machines
just receive the signal and forward it. Since calcium sig-
naling can be considered as a broadcast scheme, multi-
ple receivers can decode the same message.

7. Long-range communication using pheromones

Long-range communication is referred to as the com-
munication process in which the distance between trans-
mitter
from
ranges
nano-machines
receiver
and
millimeters up to kilometers. Control and communication
of nano-machines in long-range communication can be
useful in many applications, such as the military ﬁeld or
environmental monitoring.
Nanonetworks in long-range communication scenario
are also inspired in biological systems found in nature.
Ants, butterﬂies, bees and many mammals use molecular
messages, i.e., pheromones, to communicate with mem-
bers of the same species. Pheromones can be deﬁned as
molecular compounds containing information that can
only be decoded by speciﬁc receivers and can invoke cer-
tain reactions in them. Pheromonal communication is used
to build complex networks including micro and macro-
scale mobile actors. For instance, in ants colony, the com-
munication between members is based on pheromones
and a whole set of social behavior primitives is encoded
in these nano-messages. Another example is the long-dis-
tance communication between butterﬂies using phero-

2274

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

mones where molecular messages can reach the range of a
few kilometers [65].

7.1. Communication features

Communication using pheromones is similar to the
short-range techniques described in Section 5 and 6. The
communication is based on the release of molecules that
can be detected by a receiver. In long-range nanonetworks
the channel cannot be modeled as a deterministic neither a
physical link between transmitter and receiver. Thus, con-
cepts like diffusion, ﬂows and molecules concentration in
the medium play a key role in the communication process.
Regarding the medium, molecular communication using
pheromones can be used in dry [14] and wet [40] environ-
ments. Once the molecules are released to the medium,
they can be affected by several factors, such as antagonist
agents, medium ﬂow and temperature, and dispersion. All
those factors can be considered as sources of noise, similar
to those found in traditional communication channels, and
they can compromise the transmission reliability.
A key feature in long-range communication using pher-
omones is the coding system. While in traditional commu-
nication networks the message is encoded using a binary
system, the information in long-range nanonetworks is en-
coded on the molecule itself. Since messages consist of mol-
ecules, there is a huge quantity of possible combinations
that can be used to transmit data. Moreover, messages
can be compounded by several different molecules allow-
ing even more combinations to encode the information.
The reception of the transmitted molecules is realized
by molecular receptors located on the receiver, as depicted
in Fig. 6. This phenomenon is based on the ligand-receptor
binding process as occurs within calcium signaling trig-
gered by ﬁrst messengers. A ligand is a molecule that inter-
acts with a protein, by speciﬁcally binding to this one. In
molecular communication using pheromones, the receptor
proteins can be considered as the receiver nano-machine
antenna or transducer, which transforms energy contained
in the message into a reaction at the receiver.
Nanonetworks based on pheromonal communication
are good examples of scalable molecular communication
systems. In biological communication systems based on
pheromones, the information is encoded on molecules at
nano-scale, although the transmitter and receiver nano-

machines, i.e., animals, can be considered as macro-sys-
tems. However, the communication is still based on
nano-transceivers and nano-messages and therefore is in
line with the deﬁnition of nanonetworks. In these biologi-
cal systems, complex neural networks and hormonal sys-
tems act as the interface among these macro entities, e.g.,
brain, and the nano communication system.

7.2. Long-range pheromonal communication process

The communication process includes ﬁve tasks similar
to those found in short-range communication techniques.
Currently, there are still no artiﬁcial nano-machines capa-
ble of executing the tasks described in this section but they
are expected to be available in the near future. The objec-
tive of the following description is to identify the existing
biological communication processes and to map them into
those communication processes found in traditional com-
munication networks. This knowledge mapping can help
to understand the biological communication mechanisms
and to develop solutions for future nano-machines.

 Encoding. This process includes the selection of the spe-
ciﬁc pheromones or appropriate molecules to transmit
the information and produce the reaction at
the
intended receiver. On the biological systems found in
nature, animals release speciﬁc pheromones to invoke
certain reactions at the receivers. For instance, the
female Douglas-ﬁr beetle releases pheromones to attract
the male to the host tree. When the male detects these
pheromones, it transmits an acoustic signal to stop the
sexual pheromones production and also releases Doug-
las-ﬁr anti-odors to avoid the detection of the host by
other males. Another example of coding through mole-
cules can be found in colonies of bees. Honeybees use
up to 15 known glands in their molecular communica-
tion process. The released message is not a single com-
ponent but
rather a complex molecular structure
compound by different chemicals. Different pheromones
can be used as sexual, alarm, marking, attraction or
aggregation and orientation messages.
 Transmission. This process consists of releasing the
selected pheromones during the encoding process to
the medium. Since pheromones can be released to
the medium through physiological ﬂuids, molecular

Scale

o
r
c
i
m

o
n
a
n

Micro-system

Micro-system

Encoder

TX
Nano-machine

Pheromones
(the message)

Receptors

RX
Nano-machine

Fig. 6. Conceptual diagram of a pheromonal communication. Biological models provide a useful example of molecular communication scalability.

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

2275

messages can be in liquids or gases. The transmission of
information is deﬁned as a voluntary action of the trans-
mitter. It is important to emphasize the voluntary char-
acter of the transmission, otherwise it is not considered
as a communication message. For instance, odors can
contain information and can trigger speciﬁc behaviors
at the receivers, but usually they are not transmitted
voluntarily and therefore they cannot be considered as
an example for nanonetwork communication.
 Propagation. The propagation of pheromones, from the
transmitter to the receiver nano-machine, can be mod-
eled using diffusion processes where each molecule is
subjected to Brownian motion. The diffusion of mole-
cules or propagation is very sensitive to environmental
conditions such as temperature, viscosity and pressure.
Antagonist molecules present in the medium can also
affect the propagation negatively by modifying the infor-
mation molecules before they reach the destination. In
biology, the reception of pheromonal messages is stud-
ied by measuring the concentration level of pheromones
near the intended receiver [13]. These communication
models based on concentration level and diffusion mod-
els, can help to determine some parameters, such as the
sensitivity needed in the receptor to detect the molecular
messages. The model can also help to develop channel-
multiplexing mechanisms and to assess the channel
capacity according to the propagation characteristics.
 Reception. Once the information message reaches the
receiver, it can use receptor proteins to detach the mol-
ecule from the carrier. These receptors can be located on
the physical surface of the receiver. Receptors are pro-
teins with a high molecular afﬁnity to pheromonal mes-
sages, so that they can bind to them. The ligand-receptor
binding process also depends on the environmental con-
ditions. Special structures can be included in nano-
machines for this task, e.g., the vomeronasal organ that
acts as the receiver for pheromonal communication in
most mammals [14]. This biological organ is a blind-
ended mucus-ﬁlled tube, located in the nasal septum
[23]. The vomeronasal sensory neurons (VSNs) trans-
form these received molecules into bioelectric signals
that are transmitted to the brain and later decoded into
a speciﬁc reaction.
 Decoding. This process is referred to the interpretation of
the information transmitted or the reaction invoked by
the received message. For instance, in some insects the
reception organs includes many different receptors that
can be sensitive to different molecules or messages. The
molecular receptor of the fruit ﬂy is its antennae and the
maxillary palps that include 1300 olfactory receptor
neurons (ORNs). These ORNs, which are connected to
the brain, can decode 40 different odors. The decoding
system is embedded in the reception organs and can
be expressed as spatial-temporal activation patterns of
the receptors.

8. Research challenges in nanonetworks

The interest in nanotechnologies is growing while more
applications are being proposed. The potential impact of

these technologies leverages the continuous development
of new tools, such as simulators [18], scanning probe
microscopes [44], or lithography machines able to pattern
nano-metric structures.
Over the last years, novel molecular manufacturing
techniques are enabling the development of more ad-
vanced nano-machines. Using nanonetworks, these nano-
machines can be interconnected to realize more complex
tasks in a coordinated and complementary manner. First
developments in nanonetworks are inspired by biological
systems based on molecular communication.
Recently, two molecular communication schemes have
been proposed based on natural models. These schemes
are based on inter and intra-cell molecular communication
techniques [57]. Additionally, in this paper we propose a
third communication scheme based on pheromones. These
three communication schemes are examples for short or
long-range molecular communication observed in nature.
They are proposed as basic models for the development
of future nanonetworks.
Nanonetworks are not only related to the molecular
communication techniques, but also to nano-machines,
which are able to communicate at this level. The nanonet-
works development roadmap includes several stages, in
which an interdisciplinary scientiﬁc approach is needed
to address all the posted research challenges.

8.1. Development of nano-machines, testbeds and simulation
tools

One of the ﬁrst phases in the nanonetworks roadmap is
aimed at developing nano-machines including molecular
transceivers. Latest efforts on nanotechnologies enabled
the creation of functional nano-structures that introduce
switching, memory, and light-emitting behaviors [8].
These are still very simple but they will lead to more com-
plex nano-machines, which will be capable of performing
speciﬁc communication processes. Future nano-machines
are expected to operate autonomously, and also to include
self-assembly, self-replication, locomotion and communi-
cation capabilities as described in Section 4.
Despite the current existence of simulation tools for
molecular assembly, and biological and genetic systems,
there is none for nanonetworks up to now. Simulation
tools should allow the use of different nanonetwork topol-
ogies and molecular communication schemes such as cal-
cium signaling or networks based on molecular motors. It
should also include the medium parameters that affect
the propagation of the information molecules, and allow
the selection of different carriers
to transport
the
information.
Simple nano-machines or molecular transducers can be
considered for the development of ﬁrst nanonetworks
testbeds for molecular communication solutions. This is
the case of robotic micro-noses, which could be used to
measure concentration level of certain molecules in the
environment [33], and therefore act as molecular receiv-
ers. Using molecular releasers as transmitters, and sys-
tems like the robotic noses, developments in signal
modulation, channel models, and medium access could
be tested.

2276

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

8.2. Theoretical approach: basis for a new communication
paradigm

A typical communication process includes the following
phases:

 The encoding phase in which the transmitter forms the
information molecule.
 The transmission or release of these molecules to the
environment.
 The propagation of the information molecules trough
the medium.
 The reception of the information molecules.
 The decoding of the received information molecules.

First nanonetworks are based on many biological com-
munication processes. However, the communication tasks
occurring among these biological components are still
not completely understood.
When talking about nanonetworks, the ﬁrst questions
that arise are regarding the information encoding and later
decoding. Based on the three bio-inspired communication
schemes presented in Sections 5–7, we can identify two
different methods to encode the information in molecular
communication. The ﬁrst method encodes the information
in the information molecule itself, e.g., structure or polar-
ity. Depending on the molecular structure the message will
bind only to speciﬁc receptors, resulting in a channel de-
ﬁned by these information molecules and proper receptors.
Thus, the communication system could use different mol-
ecules and receptors to encode the information as spa-
tial-temporal activation patterns of the receptors, e.g.,
decoding of odors in fruit ﬂies. The second method encodes
the information in the ﬂuctuations of the concentration of
ions, similar to amplitude and frequency modulation
methods used in traditional telecommunication systems.
How nano-machines decode these different ﬂuctuations
is still uncertain although the activation of some cellular
processes according to these variations has already been
proven [11].
The transmission and reception of information mole-
cules is more related to the nano-machines, more speciﬁ-
cally with the molecular transceiver. There are many
open questions regarding the transmission and reception
such as (i) how to acquire new molecules and modify them
to encode the information, (ii) how to manage the received
molecules, and (iii) how to control the binding processes to
release the information molecules to the medium. In [52],
models for ﬂux and concentration detectors are presented.
Such models demonstrate the dependence of the binding
process on the medium conditions. This kind of models
can help to understand the interaction between the recep-
tors and the information signal, and to assess some trans-
ceivers features such as the molecule release rate or the
sensitivity of the receptor.
The propagation of the communication signals in nan-
onetworks is totally different than in classical communica-
tion networks. Nanonetworks use molecules instead of
electromagnetic or acoustic waves. The propagation of
these molecules, which is subject to different medium
parameters such as viscosity, temperature or pH, can be

described using a particle diffusion model. Uncontrolled
medium conditions such as rain, obstacles, wind or tide,
can also affect the communication pathway. Prior to the
development of communication mechanisms or protocols,
it is important to develop, analyze and understand the
channel model and the molecular signal propagation fea-
tures. Brownian motion models were successfully used to
describe the free movement of particles in ﬂuids [53].
Using similar models,
the communication based on
diffusion of pheromones in ant colonies has been described
in [13]. These models can help to develop the channel and
propagation models for nanonetworks. Transmitter and re-
ceiver binding features can be added to these propagation
models to assess the molecular channel capacity [2].
Similar models, but including molecular propagation prin-
ciples, can be used to explore more complex encoding
techniques for molecular transmissions, such as AM and
FM. If molecular motors are used as carriers, the propaga-
tion is restricted to the nanonetwork infrastructure and
therefore the communication process can be considered
to be more deterministic. The signal propagation speed
using this physical channel as well as the reliability of
the transmission using molecular rails still need to be
measured.

8.3. Knowledge transfer: architectures and protocols

Once the basic nanonetwork components are built, the
transmission is controlled and the propagation is under-
stood, advanced networking knowledge can be applied to
design and realize more complex nanonetworks. The
development of the nanonetworks can be done using a lay-
ered architecture including medium access protocols, rout-
ing schemes and application interfaces.
A medium access control (MAC) protocol is needed to
deﬁne and apply mechanisms to ensure a fair use of
transmission channel. In biological molecular communi-
cations, most of the channel access schemes are based
on code division. In nanonetworks, many molecular cod-
ing schemes could be used to transmit the information
while these molecules do not interfere with each other
and do not affect the medium, e.g., changing the viscos-
ity. An example of biological channel access based on dif-
ferent codes can be found in pheromonal communication.
In this example, each communication channel uses differ-
ent information molecules that can only be decoded by
intended receivers, resulting in multiple communication
channels over the same medium, one per species. An-
other medium access technique used by biological sys-
tems is based on the modulation in frequency (FM) and
amplitude (AM) of the molecular signal. According to
these last
the communication channels
techniques,
among nano-machines can be established using different
frequencies or signal amplitudes. Each frequency and
amplitude channel can be decoded by speciﬁc nano-
machines.
For more complex networks, an addressing scheme is
needed. The packet should include the information about
its origin and destination to allow bidirectional and multi-
hop communications. How to include this information in
an information molecule is still an open issue. However,

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

2277

according to natural models of nanonetworks, the informa-
tion is usually encoded and broadcast without including a
speciﬁc address of the transmitter nano-machine. In bio-
logical nanonetworks it is assumed that only the autho-
rized transmitters can have access to the channel and
send the encoded message. When this message arrives at
the receptor, it reacts according to the molecular informa-
tion ignoring its origin. Thus, these nanonetworks can be
considered as data centric networks. This behavior can be
observed in pheromonal communication, or in living cell
networks using calcium signaling. In these examples, the
reactions at the receiver are triggered by the molecular sig-
nal itself, which does not contain any speciﬁc data about
the origin. This principle is also the basis for drugs delivery
and the use of supplementary hormones in healthcare. By
contrast, the destination address is crucial in data centric
networks. Nano-machines participating in the propagation
pathway should be able to route a packet towards the right
receptor based on the destination address. Addressing and
routing schemes are not trivial, and solutions will be devel-
oped according to the level of complexity of nano-ma-
chines participating in the propagation of the molecular
signals. In natural models encoded signals are broadcast
in a medium where the intended receivers are located. This
simple mechanism is used to reach the intended receivers
of the network.
Nanonetworks are developed for a speciﬁc purpose or
application. The objectives are described in a main control-
ler or are distributed in the internal code of the participat-
ing nano-machines. All the interfacing aspects between the
controller of a nano-machine, in which the application
tasks are described, and the nanonetwork must be ex-
plored. Standard interfaces should be deﬁned including
protocols and primitives to start a transmission using
nano-machines and translate the received molecules into
information relevant to the application. These interfaces
should be available for macro-systems participating in
the nanonetworks, such as monitoring computer networks
in biomedical or military ﬁelds.
Communication reliability is another key issue in nan-
onetworks. Some applications demand reliable mecha-
nisms to monitor or interrupt selective transmissions or
on-going processes. Some molecular
communication
schemes are subject to random processes; as a result, the
reception of a transmitted message by the right receiver
cannot be guaranteed. The communication reliability
should be investigated by considering random communi-
cation features and particular infrastructures in molecular
communication.

9. Conclusions

The development of nanotechnologies will continue
and will have a great impact in almost every ﬁeld. The
use and control of these technologies will be a major
advantage in economics, homeland security, sustainable
growth and healthcare. Intrinsic technological burdens will
limit the use of more advanced and smart materials, sen-

sors, actuators and devices at nano-scale, if they are not
able to communicate to cooperate to perform more com-
plex tasks. This need for a communication network will
be more plausible with the increased complexity of devel-
oped nano-devices.
Molecular communication seems to provide efﬁcient
mechanisms for networking of nano-machines. It repre-
sents a complete new communication paradigm in which
the information is encoded into molecules. In this paper
the term ‘‘nanonetwork” was deﬁned as the interconnec-
tion of multiple nano-machines using molecular communi-
cation. Nanonetworks demand innovative solutions to
create reliable molecular communication channels among
nano-machines. First developments are bio-inspired by
existing biological nanonetworks. At nano-level, many
components and communication process has been studied
from a biological or chemical point of view.
Despite being a novel communication paradigm that re-
quires an interdisciplinary approach, information and com-
munication technologies (ICT) are called to be a key
contributor for the evolution of the nanonetworks. Net-
work architectures, channel models, nano-machines and
transceivers architectures, medium access control and
routing protocols are some of the contributions that are ex-
pected from the ICT ﬁeld.

Acknowledgement

The authors would like to thank Eylem Ekici, Mehmet C.
Vuran, Linda Jiang Xie, Tommaso Melodia, and Dario
Pompili for their valuable comments.

References

[1] B. Alberts, Molecular Biology of the Cell, 4th Bk& Cdr edition, Ed.
Garland Science, 2002.
[2] G. Alfano, D. Miorandi, On information transmission among
nanomachines, in: Proceedings of the First International Conference
on Nano-Networks (NanoNet’06), September 2006.
[3] J.W. Aylott, Optical nanosensors – an enabling technology for
intracelular measurements, Analyst 128 (2003) 309–312.
[4] J.D. Badjic, V. Balzani, A. Credi, S. Silvi, J.F. Stoddar, A molecular
elevator, Science 303 (March) (2004) 1845–1849.
[5] R. Ballardini, V. Balzani, A. Credi, M. Gandolﬁ, M. Venturi, Artiﬁcial
molecular-level machines: which energy to make them work?,
Accounts of Chemical Research 34 (May) (2001) 445–455
[6] V. Balzani, M. Gómez-López,
J.F. Stoddart, Molecular machines,
Accounts of Chemical Research 31 (May) (1998) 405–414.
[7] V. Balzani, A. Credi, S. Silvi, M. Venturi, Artiﬁcial nanomachines based
on interlocked molecular species: recent advances, Chemical Society
Reviews 35 (2006) 1135–1149.
[8] J. Batteas, C. Chidsey, C. Kagan, T. Seideman, Building electronic
functions into nanoscale molecular architectures, National Science
Foundation, Technical Report, June 2007.
[9] C. Bauschlicher, A. Ricca, R. Merkle, Chemical storage of data,
Nanotechnology 8 (March) (1997) 1–5.
[10] B. Behkam, M. Sitti, Bacterial ﬂagella-based propulsion and on/off
motion control of microscale objects, Applied Physics Letters 90
(January) (2007).
[11] M.J. Berridge, The AM and FM of calcium signalling, Nature 386
(April) (1997) 759–780.
[12] T. Bjerregaard, S. Mahadevan, A survey of research and practices of
network-on-chip, ACM Computing Surveys 38 (2006) 1–50.
[13] W.H. Bossert, E. Wilson, The analysis of olfactory communication
among animals, Journal of Theoretical Biology 5 (November) (1963)
443–469.

2278

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

[14] P.A. Brennan, E.B. Keverne, Something in the air? New insights into
mammalian pheromones, Current Biology 14 (January) (2004) R81–
R84.
[15] S.F. Bush, Y. Li, Nano-communications: A new ﬁeld? An exploration
into a carbon nanotube communication network, General Electric,
Technical Report, February 2006.
[16] C. Bustamante, Y. Chelma, N. Forde, D. Izhaky, Mechanical processes
in biochemistry, Annual Review of Biochemistry 73 (July) (2004)
705–748.
[17] E. Carafoli, Calcium signaling: a tale for all seasons, Proceedings of
the National Academy of Sciences of the United States of America 99
(3) (2002) 1115–1122.
[18] A. Cavalcanti, B. Shirinzadeh, R.A. Freitas, T. Hogg, Nanorobot
architecture for medical target identiﬁcation, Nanotechnology 19
(1) (2008) 1–12.
[19] C.-Y. Chang, The highlights in the nano world, Proceedings of the
IEEE 91 (November) (2003) 1756–1764.
[20] C. Chen, Y. Haik, J. Chatterjee, Development of nanotechnology for
biomedical
applications,
in:
Proceedings
of
the
Emerging
Information Technology Conference, August 2005.
[21] J. Dennis, J. Howard, V. Vogel, Molecular shuttles: directed motion of
microtubules along nanoscale kinesin tracks, Nanotechnology 10
(September) (1999) 232–236.
[22] T. Donaldson, 24th century medicine, Cryonics (December) (1988)
16–34.
[23] K. Doving, D. Trotier, Structure and function of the vomeronasal
organ, Journal of Experimental Biology 21 (November) (1998) 2913–
2925.
[24] E. Drexler, Nanosystems: Molecular Machinery, Manufacturing, and
Computation, John Wiley and Sons Inc., 1992.
[25] E. Drexler, C. Peterson, G. Pergami, Unbounding the future: The
nanotechnology revolution, Foresight Institute, Technical Report,
1991.
[26] E. Drexler, Molecular engineering: Assemblers and future space
hardware, American Astronautical Society: AAS-86-415, 1986.
[27] M. Endo, T. Hayashi, Y.A. Kim, H. Muramatsu, Development and
applications of carbon nanotubes,
Japanese Journal of Applied
Physics 46 (June) (2006) 4883–4892.
[28] A. Enomoto, M. Moore, T. Nakano, R. Egashira, T. Suda, A. Kayasuga,
H. Kojima, H. Sakibara, K. Oiw, A molecular communication system
using a network of cytoskeletal ﬁlaments, in: Proceedings of the
2006 NSTI Nanotechnology Conference, May 2006.
[29] R.A. Freitas, Pharmacytes: An ideal vehicle for targeted drug delivery,
Journal of Nanoscience and Nanotechnology 6 (September) (2006)
2769–2775.
[30] R. Freitas, What is nanomedicine?, Nanomedicine: Nanotechnology,
Biology and Medicine 1 (November) (2004) 2–9
[31] R.A. Freitas, Nanomedicine, Volume I: Basic Capabilities. Landes
Biosience, 1999.
[32] R.A. Freitas, Nanotechnology, nanomedicine and nanosurgery,
International Journal of Surgery 3 (November) (2005) 243–246.
[33] J. Gardner, P. Bartlett, Electronic Noses: Principles and Applications,
Oxford University Press, 1999.
[34] H. Goldstein, The race to the bottom, IEEE Spectrum (March) (2005)
32–39.
J. Fu, R.B. Schoch, Molecular sieving using nanoﬁlters:
[35] J. Han,
past, present and future, Lab on a Chip 8 (January)
(2008)
23–33.
[36] Y. Hiratsuka, T. Tada, K. Oiwa, T. Kanayama, T.Q.P. Uyeda, Controlling
the direction of kinesin-driven microtubule movements along
microlithographic tracks, Biophysical
Journal 81 (September)
(2001) 1555–1561.
[37] S. Hiyama, Y. Moritani, T. Suda, R. Egashira, A. Enomoto, M. Moore, T.
Nakano, Molecular communication,
in: Proceedings of the NSTI
Nanotechnology Conference, October 2005.
[38] Y. Homma, Y. Kobayashi, T. Ogino, T. Yamashita, Growth of
suspended
carbon
nanotube
networks
on
100-nm-scale
silicon pillars, Applied Physics Letters 81 (September)
(2002)
2261–2263.
[39] Intel, ‘‘Technology@intel magazine,” October 2006.
[40] P. Karlson, A. Butenandt, Pheromones (ectohormones) in insects,
Annual Review of Entomology 4 (April) (1959) 49–58.
[41] I. Koltz, D. Hunston, Mathematical models for ligand-receptor
binding. real sites, ghost sites, Journal of Biological Chemistry 259
(16) (1984) 10060–10062. August.
[42] H.H. Lee, E. Menard,
J.A.R.N.G. Tassi, G.B. Blanchet, Large area
microcontact printing presses for plastic electronics, Materials
Research Society Bulletin 846 (2005) 731–736.

[43] R. Merkle, Self-replicating systems and molecular manufacturing,
Journal of the British Interplanetary Society 45 (1992) 407–413.
[44] E. Meyer, H.J. Hug, R. Bennewitz, in: E. Meyer (Ed.), Scanning Probe
Microscopy: The Lab on a Tip, Springer, 2004.
[45] M. Meyyappan, J. Li, J. Li, A. Cassell, Nanotechnology: An overview
and integration with MEMS,
in: Proceedings of the 19th IEEE
International Conference on Micro Electro Mechanical Systems
(MEMS’06), January 2006, pp. 1–3.
[46] M. Moore, A. Enomoto, T. Nakano, R. Egashira, T. Suda, A. Kayasuga,
H. Kojima, H. Sakakibara, K. Oiwa, A design of a molecular
communication system for nanomachines using molecular motors,
in Proceedings of the Fourth Annual IEEE International Conference
on Pervasive Computing and Communications (PerCom’06), March,
2006.
[47] M. Moore, A. Enomoto, T. Nakano, Y. Okaie, T. Suda, Interfacing with
nanomachines through molecular communication, in: Proceedings
of
the IEEE International Conference on Systems, Man and
Cybernetics, October 2007, pp. 18–23.
[48] Y. Moritani, S. Hiyama, T. Suda, Molecular communication among
nanomachines
using
vesicles,
in:
Proceedings
of NSTI
Nanotechnology Conference, May 2006.
[49] Y. Moritani, S. Hiyama, T. Suda, Molecular communication for health
care applications,
in: Proceedings of
the Fourth Annual
IEEE
International
and
Computing
Pervasive
on
Conference
Communications (PerCom’06), March 2006.
[50] T. Nakano, T. Suda, M. Moore, R. Egashira, A. Enomoto, K. Arima,
Molecular communication for nanomachines using intercellular
calcium signaling, in: Proceedings of the Fifth IEEE Conference on
Nanotechnology, June 2005, pp. 478–481.
[51] C. Peterson, Taking technology to the molecular level, IEEE Computer
33 (January) (2000) 46–53.
[52] J.-P. Rospars, V. Krivan, P. Lánky, Perireceptor and receptor events in
olfaction. Comparison of concentration and ﬂux detectors: a
modeling study, Chemical Senses 25 (June) (2000) 293–311.
[53] D. Selmeczi, S. Mosler, P. Hagedorn, N. Lasen, H. Flyvbjerg, Cell
motility as persistent random motion: theories from experiments,
Biophysical Journal 89 (June) (2005) 912–931.
[54] V. Serreli, C. Lee, E.R. Kay, D.A. Leigh, A molecular information
ratchet, Nature 445 (February) (2007) 523–527.
[55] R. Smalley, M.S. Dresselhaus, G. Dresselhaus, P. Avouris, in: M.S.
Dresselhaus
(Ed.), Carbon Nanotubes:
Synthesis,
Structure,
Properties and Applications, Springer, 2001.
[56] R.K. Soong, G.D. Bachand, H.P. Neves, A.G. Olkhovets, H.G. Craighead,
C.D. Montemagno, Powering an inorganic nanodevice with a
biomolecular motor, Science 24 (November)
(2000) 1555–
1558.
[57] T. Suda, M. Moore, T. Nakano, R. Egashira, A. Enomoto, Exploratory
research on molecular communication between nanomachines, in:
Proceedings of Genetic and Evolutionary Computation Conference
(GECCO’05), June 2005.
[58] T. Tanaka, R. Sano, Y. Yamashita, M. Yamazaki, Shape changes and
vesicle ﬁssion of giant unilamellar vesicles of liquid-ordered phase
membrane induced by lysophosphatidylcholine, Langmuir 20
(August) (2004) 9526–9534.
[59] N. Taniguchi, On the basic concept of nano-technology,
in:
Proceeding of
International Conference on Production
the
Engineering, 1974.
[60] D. Tessier, I. Radu, M. Filteau, Antimicrobial fabrics coated with
nano-sized silver salt crystals, NSTI Nanotech 1 (May) (2005) 762–
764.
[61] A. Tseng, K. Chen, C. Chen, K. Ma, Electron beam lithography
in nanoscale fabrication: recent development, IEEE Transactions
on Electronics Packaging Manufacturing 26 (April) (2003) 141–
149.
[62] U.
P. Wadswort,
Ferenz,
N.
Fagerstrom,
C.
Tulu,
Molecular requirements for kinetochore-associated microtubule
formation in mammalian cells, Current Biology 16 (March) (2006)
536–541.
[63] G.M. Whitesides, The once and future nanomachine, Scientiﬁc
American (September) (2001) 78–83.
[64] B. Wowk, Cell repair technology, Cryonics, vol. July 1988, pp. 21–30,
May.
[65] T.D. Wyatt, Pheromones and animal behaviour: communication by
smell and taste, in: T.D. Wyatt (Ed.), Cambridge University Press,
2003. September.
[66] Y.J. Yun, C.S. Ah, S. Kim, W.S. Yun, B.C. Park, D.H. Ha, Manipulation of
freestanding Au nanogears using an atomic force microscope,
Nanotechnology 18 (November) (2007) 505304. 5 pp.

I.F. Akyildiz et al. / Computer Networks 52 (2008) 2260–2279

2279

Ian F. Akyildiz (M’86-SM’89-F’96) received
the B.S., M.S., and Ph.D. degrees in Computer
Engineering from the University of Erlangen-
Nuernberg, Germany, in 1978, 1981 and 1984,
respectively.
Currently, he is the Ken Byers Distinguished
Chair Professor with the School of Electrical
and Computer Engineering, Georgia Institute
of Technology, Atlanta, and Director of
Broadband Wireless Networking Laboratory.
He
of Computer
an Editor-in-Chief
is
Networks Journal (Elsevier) as well as the
founding Editor-in-Chief of the AdHoc Net-
work Journal (Elsevier) and the Physical Communication Journal (Else-
vier). His current research interests are nanonetworks, cognitive radio
networks, wireless mesh networks and next generation sensor networks.
He received the ‘‘Don Federico Santa Maria Medal” for his services to the
Universidad of Federico Santa Maria, in 1986. From 1989 to 1998, he
served as a National Lecturer for ACM and received the ACM Outstanding
Distinguished Lecturer Award in 1994. He received the 1997 IEEE Leonard
G. Abraham Prize Award (IEEE Communications Society) for his paper
entitled ‘‘Multimedia Group Synchronization Protocols for Integrated
Services Architectures” published in the IEEE Journal of Selected Areas in
Communications (JSAC) in January 1996. He received the 2002 IEEE Harry
M. Goode Memorial Award (IEEE Computer Society) with the citation ‘‘for
signiﬁcant and pioneering contributions to advanced architectures and
protocols for gíreles and satellite networking”. He received the 2003 IEEE
Best Tutorial Award (IEEE Communication Society) for his paper entitled
‘‘A Survey on Sensor Networks,” published in IEEE Communications
Magazine, in August 2002. He also received the 2003 ACM Sigmobile
Outstanding Contribution Award with the citation ‘‘for pioneering con-
tributions in the area of mobility and resource management for gı´ reles
communication networks”. He received the 2004 Georgia Tech Faculty
Research Author Award for his ‘‘outstanding record of publications of
papers between 1999 and 2003”. He also received the 2005 Distinguished

Faculty Achievement Award from School of ECE, Georgia Tech. He has been
a Fellow of the Association for Computing Machinery (ACM) since 1996.

Fernando Brunetti was born in Asunción,
Paraguay. In 2004, he received his Electrical
Engineering diploma from the Universidad
Católica de Asunción, in Paraguay. In 2005, he
received a second M.S. degree from the School
of Informatics of the Universidad Politécnica
de Madrid (UPM), Spain. Currently, he is
afﬁliated to the Bioengineering Group of the
Industrial Automation Institute,
Spanish
Research Council (CSIC) and is a Ph.D. candi-
date in the School of Telecommunications
Engineering of the UPM. He has been a visit-
ing researcher at the Broadband Wireless
Networking Laboratory, Georgia Institute of Technology, Atlanta, from
October 2007 to March 2008. His current research interests are nanon-
etworks, body area networks, and inertial sensing.

Cristina Blázquez was born in Barcelona
(Spain) in 1983. She is pursuing her M.S.
degree in Telecommunication Engineering
from the ETSETB-Telecom Barcelona, Univer-
sitat Politècnica de Catalunya (UPC), Spain.
From August 2007 she is working at the
Broadband Wireless Networking Laboratory,
School of Electrical and Computer Engineering
in Georgia Institute of Technology, Atlanta.

OpenFlow: Enabling Innovation in Campus Networks

Hari Balakrishnan
Tom Anderson
Nick McKeown
MIT
University of Washington
Stanford University
Jennifer Rexford
Larry Peterson
Guru Parulkar
Princeton University
Stanford University
Princeton University
Jonathan Turner
Scott Shenker
Washington University in
University of California,
Berkeley
St. Louis
This ar ticle is an editorial note submitted to CCR. It has NOT been peer reviewed.
Authors take full responsibility for this ar ticle’s technical content.
Comments can be posted through CCR Online.

ABSTRACT
This whitepaper proposes OpenFlow: a way for researchers
to run experimental protocols in the networks they use ev-
ery day. OpenFlow is based on an Ethernet switch, with
an internal ﬂow-table, and a standardized interface to add
and remove ﬂow entries. Our goal is to encourage network-
ing vendors to add OpenFlow to their switch products for
deployment in college campus backbones and wiring closets.
We believe that OpenFlow is a pragmatic compromise: on
one hand, it allows researchers to run experiments on hetero-
geneous switches in a uniform way at line-rate and with high
port-density; while on the other hand, vendors do not need
to expose the internal workings of their switches. In addition
to allowing researchers to evaluate their ideas in real-world
traﬃc settings, OpenFlow could serve as a useful campus
component in proposed large-scale testbeds like GENI. Two
buildings at Stanford University will soon run OpenFlow
networks, using commercial Ethernet switches and routers.
We will work to encourage deployment at other schools; and
We encourage you to consider deploying OpenFlow in your
university network too.

Categories and Subject Descriptors
C.2 [Internetworking]: Routers

General Terms
Experimentation, Design

Keywords
Ethernet switch, virtualization, ﬂow-based

1. THE NEED FOR PROGRAMMABLE
NETWORKS
Networks have become part of the critical infrastructure
of our businesses, homes and schools. This success has been
both a blessing and a curse for networking researchers; their
work is more relevant, but their chance of making an im-
pact is more remote. The reduction in real-world impact of
any given network innovation is because the enormous in-
stalled base of equipment and protocols, and the reluctance

to experiment with production traﬃc, which have created an
exceedingly high barrier to entry for new ideas. Today, there
is almost no practical way to experiment with new network
protocols (e.g., new routing protocols, or alternatives to IP)
in suﬃciently realistic settings (e.g., at scale carrying real
traﬃc) to gain the conﬁdence needed for their widespread
deployment. The result is that most new ideas from the net-
working research community go untried and untested; hence
the commonly held belief that the network infrastructure has
“ossiﬁed”.
Having recognized the problem, the networking commu-
nity is hard at work developing programmable networks,
such as GENI [1] a proposed nationwide research facility
for experimenting with new network architectures and dis-
tributed systems. These programmable networks call for
programmable switches and routers that (using virtualiza-
tion) can process packets for multiple isolated experimen-
tal networks simultaneously. For example, in GENI it is
envisaged that a researcher will be allocated a slice of re-
sources across the whole network, consisting of a portion
of network links, packet processing elements (e.g. routers)
and end-hosts; researchers program their slices to behave as
they wish. A slice could extend across the backbone, into
access networks, into college campuses, industrial research
labs, and include wiring closets, wireless networks, and sen-
sor networks.
Virtualized programmable networks could lower the bar-
rier to entry for new ideas, increasing the rate of innovation
in the network infrastructure. But the plans for nationwide
facilities are ambitious (and costly), and it will take years
for them to be deployed.
This whitepaper focuses on a shorter-term question closer
to home: As researchers, how can we run experiments in
our campus networks? If we can ﬁgure out how, we can
start soon and extend the technique to other campuses to
beneﬁt the whole community.
To meet this challenge, several questions need answering,
including: In the early days, how will college network admin-
istrators get comfortable putting experimental equipment
(switches, routers, access points, etc.) into their network?
How will researchers control a portion of their local net-
work in a way that does not disrupt others who depend on
it? And exactly what functionality is needed in network

ACM SIGCOMM Computer Communication Review69Volume 38, Number 2, April 2008switches to enable experiments? Our goal here is to propose
a new switch feature that can help extend programmability
into the wiring closet of college campuses.
One approach - that we do not take - is to persuade
commercial “name-brand” equipment vendors to provide an
open, programmable, virtualized platform on their switches
and routers so that researchers can deploy new protocols,
while network administrators can take comfort that the
equipment is well supported. This outcome is very unlikely
in the short-term. Commercial switches and routers do not
typically provide an open software platform, let alone pro-
vide a means to virtualize either their hardware or software.
The practice of commercial networking is that the standard-
ized external interfaces are narrow (i.e., just packet forward-
ing), and all of the switch’s internal ﬂexibility is hidden. The
internals diﬀer from vendor to vendor, with no standard
platform for researchers to experiment with new ideas. Fur-
ther, network equipment vendors are understandably ner-
vous about opening up interfaces inside their boxes: they
have spent years deploying and tuning fragile distributed
protocols and algorithms, and they fear that new experi-
ments will bring networks crashing down. And, of course,
open platforms lower the barrier-to-entry for new competi-
tors.
A few open software platforms already exist, but do not
have the performance or port-density we need. The simplest
example is a PC with several network interfaces and an op-
erating system. All well-known operating systems support
routing of packets between interfaces, and open-source im-
plementations of routing protocols exist (e.g., as part of the
Linux distribution, or from XORP [2]); and in most cases it
is possible to modify the operating system to process packets
in almost any manner (e.g., using Click [3]). The problem,
of course, is performance: A PC can neither support the
number of ports needed for a college wiring closet (a fanout
of 100+ ports is needed per box), nor the packet-processing
performance (wiring closet switches process over 100Gbits/s
of data, whereas a typical PC struggles to exceed 1Gbit/s;
and the gap between the two is widening).
Existing platforms with specialized hardware for line-rate
processing are not quite suitable for college wiring clos-
ets either. For example, an ATCA-based virtualized pro-
grammable router called the Supercharged PlanetLab Plat-
form [4] is under development at Washington University,
and can use network processors to process packets from
many interfaces simultaneously at line-rate. This approach
is promising in the long-term, but for the time being is tar-
geted at large switching centers and is too expensive for
widespread deployment in college wiring closets. At the
other extreme is NetFPGA [5] targeted for use in teaching
and research labs. NetFPGA is a low-cost PCI card with
a user-programmable FPGA for processing packets, and 4-
ports of Gigabit Ethernet. NetFPGA is limited to just four
network interfaces—insuﬃcient for use in a wiring closet.
Thus, the commercial solutions are too closed and inﬂex-
ible, and the research solutions either have insuﬃcient per-
formance or fanout, or are too expensive. It seems unlikely
that the research solutions, with their complete generality,
can overcome their performance or cost limitations. A more
promising approach is to compromise on generality and to
seek a degree of switch ﬂexibility that is:
(cid:129) Amenable to high-performance and low-cost imple-
mentations.

Scope of OpenFlow Switch Specification

OpenFlow
Switch

sw

Secure
Secure
Channel
Channel

hw

Flow
Flow
Table
Table

Controller

OpenFlow
Protocol

SSL

PC

Figure 1: Idealized OpenFlow Switch. The Flow
Table is controlled by a remote controller via the
Secure Channel.

(cid:129) Capable of supporting a broad range of research.
(cid:129) Assured to isolate experimental traﬃc from production
traﬃc.
(cid:129) Consistent with vendors’ need for closed platforms.

This paper describes the OpenFlow Switch—a speciﬁca-
tion that is an initial attempt to meet these four goals.

2. THE OPENFLOW SWITCH
The basic idea is simple: we exploit the fact that most
modern Ethernet switches and routers contain ﬂow-tables
(typically built from TCAMs) that run at line-rate to im-
plement ﬁrewalls, NAT, QoS, and to collect statistics. While
each vendor’s ﬂow-table is diﬀerent, we’ve identiﬁed an in-
teresting common set of functions that run in many switches
and routers. OpenFlow exploits this common set of func-
tions.
OpenFlow provides an open protocol to program the ﬂow-
table in diﬀerent switches and routers. A network admin-
istrator can partition traﬃc into production and research
ﬂows. Researchers can control their own ﬂows - by choosing
the routes their packets follow and the processing they re-
ceive. In this way, researchers can try new routing protocols,
security models, addressing schemes, and even alternatives
to IP. On the same network, the production traﬃc is isolated
and processed in the same way as today.
The datapath of an OpenFlow Switch consists of a Flow
Table, and an action associated with each ﬂow entry. The
set of actions supported by an OpenFlow Switch is exten-
sible, but below we describe a minimum requirement for
all switches. For high-performance and low-cost the data-
path must have a carefully prescribed degree of ﬂexibility.
This means forgoing the ability to specify arbitrary handling
of each packet and seeking a more limited, but still useful,
range of actions. Therefore, later in the paper, deﬁne a basic
required set of actions for all OpenFlow switches.

ACM SIGCOMM Computer Communication Review70Volume 38, Number 2, April 2008An OpenFlow Switch consists of at least three parts: (1)
A Flow Table, with an action associated with each ﬂow en-
try, to tell the switch how to process the ﬂow, (2) A Secure
Channel that connects the switch to a remote control pro-
cess (called the control ler), allowing commands and packets
to be sent between a controller and the switch using (3) The
OpenFlow Protocol, which provides an open and standard
way for a controller to communicate with a switch. By speci-
fying a standard interface (the OpenFlow Protocol) through
which entries in the Flow Table can be deﬁned externally,
the OpenFlow Switch avoids the need for researchers to pro-
gram the switch.
It is useful to categorize switches into dedicated OpenFlow
switches that do not support normal Layer 2 and Layer 3
processing, and OpenFlow-enabled general purpose com-
mercial Ethernet switches and routers, to which the Open-
Flow Protocol and interfaces have been added as a new fea-
ture.

Dedicated OpenFlow switches. A dedicated OpenFlow
Switch is a dumb datapath element that forwards packets
between ports, as deﬁned by a remote control process. Fig-
ure 1 shows an example of an OpenFlow Switch.
In this context, ﬂows are broadly deﬁned, and are limited
only by the capabilities of the particular implementation of
the Flow Table. For example, a ﬂow could be a TCP con-
nection, or all packets from a particular MAC address or
IP address, or all packets with the same VLAN tag, or all
packets from the same switch port. For experiments involv-
ing non-IPv4 packets, a ﬂow could be deﬁned as all packets
matching a speciﬁc (but non-standard) header.
Each ﬂow-entry has a simple action associated with it;
the three basic ones (that all dedicated OpenFlow switches
must support) are:

1. Forward this ﬂow’s packets to a given port (or ports).
This allows packets to be routed through the network.
In most switches this is expected to take place at line-
rate.

2. Encapsulate and forward this ﬂow’s packets to a con-
troller. Packet is delivered to Secure Channel, where
it is encapsulated and sent to a controller. Typically
used for the ﬁrst packet in a new ﬂow, so a controller
can decide if the ﬂow should be added to the Flow
Table. Or in some experiments, it could be used to
forward all packets to a controller for processing.

3. Drop this ﬂow’s packets. Can be used for security, to
curb denial of service attacks, or to reduce spurious
broadcast discovery traﬃc from end-hosts.

An entry in the Flow-Table has three ﬁelds: (1) A packet
header that deﬁnes the ﬂow, (2) The action, which deﬁnes
how the packets should be processed, and (3) Statistics,
which keep track of the number of packets and bytes for
each ﬂow, and the time since the last packet matched the
ﬂow (to help with the removal of inactive ﬂows).
In the ﬁrst generation “Type 0” switches, the ﬂow header
is a 10-tuple shown in Table 1. A TCP ﬂow could be spec-
iﬁed by all ten ﬁelds, whereas an IP ﬂow might not include
the transport ports in its deﬁnition. Each header ﬁeld can
be a wildcard to allow for aggregation of ﬂows, such as ﬂows
in which only the VLAN ID is deﬁned would apply to all
traﬃc on a particular VLAN.

In
Port

VLAN
ID

Ethernet
SA DA Type

IP
SA DA Proto

TCP
Src Dst

Table 1: The header ﬁelds matched in a “Type 0”
OpenFlow switch.

The detailed requirements of an OpenFlow Switch are de-
ﬁned by the OpenFlow Switch Speciﬁcation [6].

OpenFlow-enabled switches.
commercial
Some
switches, routers and access points will be enhanced with
the OpenFlow feature by adding the Flow Table, Secure
Channel and OpenFlow Protocol (we list some examples in
Section 5). Typically, the Flow Table will re-use existing
hardware, such as a TCAM; the Secure Channel and Proto-
col will be ported to run on the switch’s operating system.
Figure 2 shows a network of OpenFlow-enabled commercial
switches and access points.
In this example, all the Flow
Tables are managed by the same controller; the OpenFlow
Protocol allows a switch to be controlled by two or more
controllers for increased performance or robustness.
Our goal is to enable experiments to take place in an ex-
isting production network alongside regular traﬃc and ap-
plications. Therefore, to win the conﬁdence of network ad-
ministrators, OpenFlow-enabled switches must isolate ex-
perimental traﬃc (processed by the Flow Table) from pro-
duction traﬃc that is to be processed by the normal Layer 2
and Layer 3 pipeline of the switch. There are two ways to
achieve this separation. One is to add a fourth action:

4. Forward this ﬂow’s packets through the switch’s nor-
mal processing pipeline.

The other is to deﬁne separate sets of VLANs for experi-
mental and production traﬃc. Both approaches allow nor-
mal production traﬃc that isn’t part of an experiment to be
processed in the usual way by the switch. All OpenFlow-
enabled switches are required to support one approach or
the other; some will support both.

Additional features. If a switch supports the header for-
mats and the four basic actions mentioned above (and de-
tailed in the OpenFlow Switch Speciﬁcation), then we call it
a “Type 0” switch. We expect that many switches will sup-
port additional actions, for example to rewrite portions of
the packet header (e.g., for NAT, or to obfuscate addresses
on intermediate links), and to map packets to a priority
class. Likewise, some Flow Tables will be able to match on
arbitrary ﬁelds in the packet header, enabling experiments
with new non-IP protocols. As a particular set of features
emerges, we will deﬁne a “Type 1” switch.

Controllers. A controller adds and removes ﬂow-entries
from the Flow Table on behalf of experiments. For example,
a static controller might be a simple application running
on a PC to statically establish ﬂows to interconnect a set
of test computers for the duration of an experiment.
In
this case the ﬂows resemble VLANs in current networks—
providing a simple mechanism to isolate experimental traﬃc
from the production network. Viewed this way, OpenFlow
is a generalization of VLANs.
One can also imagine more sophisticated controllers that
dynamically add/remove ﬂows as an experiment progresses.
In one usage model, a researcher might control the complete

ACM SIGCOMM Computer Communication Review71Volume 38, Number 2, April 2008Server room 

OpenFlow
Access Point

Controller

PC

OpenFlow

OpenFlow

OpenFlow

OpenFlow-enabled
Commercial Switch

Normal
Software

Normal
Datapath

Secure
Secure
Channel
Channel

Flow
Flow
Table
Table

Figure 2: Example of a network of OpenFlow-
enabled commercial switches and routers.

network of OpenFlow Switches and be free to decide how all
ﬂows are processed. A more sophisticated controller might
support multiple researchers, each with diﬀerent accounts
and permissions, enabling them to run multiple indepen-
dent experiments on diﬀerent sets of ﬂows. Flows identiﬁed
as under the control of a particular researcher (e.g., by a
policy table running in a controller) could be delivered to a
researcher’s user-level control program which then decides if
a new ﬂow-entry should be added to the network of switches.

3. USING OPENFLOW
As a simple example of how an OpenFlow Switch might be
used imagine that Amy (a researcher) invented Amy-OSPF
as a new routing protocol to replace OSPF. She wants to
try her protocol in a network of OpenFlow Switches, with-
out changing any end-host software. Amy-OSPF will run in
a controller; each time a new application ﬂow starts Amy-
OSPF picks a route through a series of OpenFlow Switches,
and adds a ﬂow- entry in each switch along the path. In her
experiment, Amy decides to use Amy-OSPF for the traﬃc
entering the OpenFlow network from her own desktop PC—
so she doesn’t disrupt the network for others. To do this,
she deﬁnes one ﬂow to be all the traﬃc entering the Open-
Flow switch through the switch port her PC is connected to,
and adds a ﬂow-entry with the action “Encapsulate and for-
ward all packets to a controller”. When her packets reach
a controller, her new protocol chooses a route and adds a
new ﬂow-entry (for the application ﬂow) to every switch
along the chosen path. When subsequent packets arrive at
a switch, they are processed quickly (and at line-rate) by
the Flow Table.
There are legitimate questions to ask about the perfor-

mance, reliability and scalability of a controller that dynam-
ically adds and removes ﬂows as an experiment progresses:
Can such a centralized controller be fast enough to process
new ﬂows and program the Flow Switches? What happens
when a controller fails? To some extent these questions were
addressed in the context of the Ethane prototype, which
used simple ﬂow switches and a central controller [7]. Pre-
liminary results suggested that an Ethane controller based
on a low-cost desktop PC could process over 10,000 new
ﬂows per second — enough for a large college campus. Of
course, the rate at which new ﬂows can be processed will de-
pend on the complexity of the processing required by the re-
searcher’s experiment. But it gives us conﬁdence that mean-
ingful experiments can be run. Scalability and redundancy
are possible by making a controller (and the experiments)
stateless, allowing simple load-balancing over multiple sep-
arate devices.

3.1 Experiments in a Production Network
Chances are, Amy is testing her new protocol in a network
used by lots of other people. We therefore want the network
to have two additional properties:

1. Packets belonging to users other than Amy should be
routed using a standard and tested routing protocol
running in the switch or router from a “name-brand”
vendor.

2. Amy should only be able to add ﬂow entries for her
traﬃc, or for any traﬃc her network administrator has
allowed her to control.

Property 1 is achieved by OpenFlow-enabled switches.
In Amy’s experiment, the default action for all packets
that don’t come from Amy’s PC could be to forward them
through the normal processing pipeline. Amy’s own packets
would be forwarded directly to the outgoing port, without
being processed by the normal pipeline.
Property 2 depends on the controller. The controller
should be seen as a platform that enables researchers to im-
plement various experiments, and the restrictions of Prop-
erty 2 can be achieved with the appropriate use of permis-
sions or other ways to limit the powers of individual re-
searchers to control ﬂow entries. The exact nature of these
permission-like mechanisms will depend on how the con-
troller is implemented. We expect that a variety of con-
trollers will emerge. As an example of a concrete realization
of a controller, some of the authors are working on a con-
troller called NOX as a follow-on to the Ethane work [8].
A quite diﬀerent controller might emerge by extending the
GENI management software to OpenFlow networks.
3.2 More Examples
As with any experimental platform, the set of experiments
will exceed those we can think of up-front — most experi-
ments in OpenFlow networks are yet to be thought of. Here,
for illustration, we oﬀer some examples of how OpenFlow-
enabled networks could be used to experiment with new net-
work applications and architectures.

Example 1: Network Management and Access Con-
trol. We’ll use Ethane as our ﬁrst example [7] as it was
the research that inspired OpenFlow. In fact, an OpenFlow

ACM SIGCOMM Computer Communication Review72Volume 38, Number 2, April 2008Switch can be thought of as a generalization of Ethane’s
datapath switch. Ethane used a speciﬁc implementation of
a controller, suited for network management and control,
that manages the admittance and routing of ﬂows. The ba-
sic idea of Ethane is to allow network managers to deﬁne a
network-wide policy in the central controller, which is en-
forced directly by making admission control decisions for
each new ﬂow. A controller checks a new ﬂow against a set
of rules, such as “Guests can communicate using HTTP, but
only via a web proxy” or “VoIP phones are not allowed to
communicate with laptops.” A controller associates pack-
ets with their senders by managing all the bindings between
names and addresses — it essentially takes over DNS, DHCP
and authenticates all users when they join, keeping track of
which switch port (or access point) they are connected to.
One could envisage an extension to Ethane in which a policy
dictates that particular ﬂows are sent to a user’s process in
a controller, hence allowing researcher-speciﬁc processing to
be performed in the network.

Example 2: VLANs. OpenFlow can easily provide users
with their own isolated network, just as VLANs do. The
simplest approach is to statically declare a set of ﬂows which
specify the ports accessible by traﬃc on a given VLAN ID.
Traﬃc identiﬁed as coming from a single user (for example,
originating from speciﬁc switch ports or MAC addresses) is
tagged by the switches (via an action) with the appropriate
VLAN ID.
A more dynamic approach might use a controller to man-
age authentication of users and use the knowledge of the
users’ locations for tagging traﬃc at runtime.

Example 3: Mobile wireless VOIP clients. For this
example consider an experiment of a new call- handoﬀ
mechanism for WiFi-enabled phones.
In the experiment
VOIP clients establish a new connection over the OpenFlow-
enabled network. A controller is implemented to track the
location of clients, re-routing connections — by reprogram-
ming the Flow Tables — as users move through the network,
allowing seamless handoﬀ from one access point to another.

Example 4: A non-IP network. So far, our examples
have assumed an IP network, but OpenFlow doesn’t require
packets to be of any one format — so long as the Flow
Table is able to match on the packet header. This would
allow experiments using new naming, addressing and rout-
ing schemes. There are several ways an OpenFlow-enabled
switch can support non-IP traﬃc. For example, ﬂows could
be identiﬁed using their Ethernet header (MAC src and dst
addresses), a new EtherType value, or at the IP level, by a
new IP Version number. More generally, we hope that fu-
ture switches will allow a controller to create a generic mask
(oﬀset + value + mask), allowing packets to be processed
in a researcher-speciﬁed way.

Example 5: Processing packets rather than ﬂows.
The examples above are for experiments involving ﬂows —
where a controller makes decisions when the ﬂow starts.
There are, of course,
interesting experiments to be per-
formed that require every packet to be processed. For ex-
ample, an intrusion detection system that inspects every
packet, an explicit congestion control mechanism, or when
modifying the contents of packets, such as when converting
packets from one protocol format to another.

Controller

PC

OpenFlow-enabled
Commercial Switch

Normal
Software

Normal
Datapath

Secure
Secure
Channel
Channel

Flow
Flow
Table
Table

Laboratory

NetFPGA

Figure 3: Example of processing packets through an
external line-rate packet-processing device, such as
a programmable NetFPGA router.

There are two basic ways to process packets in an
OpenFlow-enabled network. First, and simplest, is to force
all of a ﬂow’s packets to pass through a controller. To do
this, a controller doesn’t add a new ﬂow entry into the Flow
Switch — it just allows the switch to default to forward-
ing every packet to a controller. This has the advantage of
ﬂexibility, at the cost of performance.
It might provide a
useful way to test the functionality of a new protocol, but
is unlikely to be of much interest for deployment in a large
network.
The second way to process packets is to route them to
a programmable switch that does packet processing — for
example, a NetFPGA-based programmable router. The ad-
vantage is that the packets can be processed at line-rate in
a user-deﬁnable way; Figure 3 shows an example of how this
could be done, in which the OpenFlow-enabled switch op-
erates essentially as a patch-panel to allow the packets to
reach the NetFPGA. In some cases, the NetFPGA board (a
PCI board that plugs into a Linux PC) might be placed in
the wiring closet alongside the OpenFlow-enabled switch, or
(more likely) in a laboratory.

4. THE OPENFLOW CONSORTIUM
The OpenFlow Consortium aims to popularize OpenFlow
and maintain the OpenFlow Switch Speciﬁcation. The Con-
sortium is a group of researchers and network administra-
tors at universities and colleges who believe their research
mission will be enhanced if OpenFlow-enabled switches are
installed in their network.
Membership is open and free for anyone at a school,
college, university, or government agency worldwide. The
OpenFlow Consortium welcomes individual members who
are not employed by companies that manufacture or sell
Ethernet switches, routers or wireless access points (because
we want to keep the consortium free of vendor inﬂuence). To
join, send email to join@OpenFlowSwitch.org.

ACM SIGCOMM Computer Communication Review73Volume 38, Number 2, April 2008The Consortium web-site 1 contains the OpenFlow Switch
Speciﬁcation, a list of consortium members, and reference
implementations of OpenFlow switches.
Licensing Model: The OpenFlow Switch Speciﬁcation
is free for all commercial and non-commercial use. (The ex-
act wording is on the web-site.) Commercial switches and
routers claiming to be “OpenFlow-enabled” must conform
to the requirements of an OpenFlow Type 0 Switch, as de-
ﬁned in the OpenFlow Switch Speciﬁcation. OpenFlow is a
trademark of Stanford University, and will be protected on
behalf of the Consortium.

5. DEPLOYING OPENFLOW SWITCHES
We believe there is an interesting market opportunity
for network equipment vendors to sell OpenFlow-enabled
switches to the research community. Every building in thou-
sands of colleges and universities contains wiring closets
with Ethernet switches and routers, and with wireless ac-
cess points spread across campus.
We are actively working with several switch and router
manufacturers who are adding the OpenFlow feature to their
products by implementing a Flow Table in existing hard-
ware; i.e. no hardware change is needed. The switches run
the Secure Channel software on their existing processor.
We have found network equipment vendors to be very
open to the idea of adding the OpenFlow feature. Most ven-
dors would like to support the research community without
having to expose the internal workings of their products.
We are deploying large OpenFlow networks in the Com-
puter Science and Electrical Engineering departments at
Stanford University. The networks in two buildings will
be replaced by switches running OpenFlow. Eventually, all
traﬃc will run over the OpenFlow network, with produc-
tion traﬃc and experimental traﬃc being isolated on dif-
ferent VLANs under the control of network administrators.
Researchers will control their own traﬃc, and be able to
add/remove ﬂow-entries.
We also expect many diﬀerent OpenFlow Switches to be
developed by the research community. The OpenFlow web-
site contains “Type 0” reference designs for several diﬀerent
platforms: Linux (software), OpenWRT (software, for ac-
cess points), and NetFPGA (hardware, 4-ports of 1GE). As
more reference designs are created by the community we will
post them. We encourage developers to test their switches
against the reference designs.
All reference implementations of OpenFlow switches
posted on the web site will be open-source and free for com-
mercial and non-commercial use.2

6. CONCLUSION
We believe that OpenFlow is a pragmatic compromise
that allows researchers to run experiments on heterogeneous
switches and routers in a uniform way, without the need for
vendors to expose the internal workings of their products,
or researchers to write vendor-speciﬁc control software.
If we are successful in deploying OpenFlow networks in
our campusses, we hope that OpenFlow will gradually catch-
on in other universities, increasing the number of networks
that support experiments. We hope that a new generation
of control software emerges, allowing researchers to re-use
controllers and experiments, and build on the work of oth-
ers. And over time, we hope that the islands of OpenFlow
networks at diﬀerent universities will be interconnected by
tunnels and overlay networks, and perhaps by new Open-
Flow networks running in the backbone networks that con-
nect universities to each other.

7. REFERENCES
[1] Global Environment for Network Innovations. Web site
http://geni.net.
[2] Mark Handley Orion Hodson Eddie Kohler. “XORP:
An Open Platform for Network Research,” ACM
SIGCOMM Hot Topics in Networking, 2002.
[3] Eddie Kohler, Robert Morris, Benjie Chen, John
Jannotti, and M. Frans Kaashoek. “The Click modular
router,” ACM Transactions on Computer Systems
18(3), August 2000, pages 263-297.
[4] J. Turner, P. Crowley, J. Dehart, A. Freestone, B.
Heller, F. Kuhms, S. Kumar, J. Lockwood, J. Lu,
M.Wilson, C. Wiseman, D. Zar. “Supercharging
PlanetLab - High Performance, Multi-Application,
Overlay Network Platform,” ACM SIGCOMM ’07,
August 2007, Kyoto, Japan.
[5] NetFPGA: Programmable Networking Hardware. Web
site http://netfpga.org.
[6] The OpenFlow Switch Speciﬁcation. Available at
http://OpenFlowSwitch.org.
[7] Martin Casado, Michael J. Freedman, Justin Pettit,
Jianying Luo, Nick McKeown, Scott Shenker. “Ethane:
Taking Control of the Enterprise,” ACM SIGCOMM
’07, August 2007, Kyoto, Japan.
[8] Natasha Gude, Teemu Koponen, Justin Pettit, Ben
Pfaﬀ, Martin Casadao, Nick McKeown, Scott Shenker,
“NOX: Towards an Operating System for Networks,”
In submission. Also:
http://nicira.com/docs/nox-nodis.pdf.

1 http://www.OpenFlowSwitch.org
2 Some platforms may limit the license terms of software
running on them. For example, a reference implementation
on Linux may be limited by the Linux GPL.

ACM SIGCOMM Computer Communication Review74Volume 38, Number 2, April 2008Service Level Agreements on IP Networks.  
Dinesh C. Verma 
IBM T. J Watson Research Center 
PO Box 704, Yorktown Heights, NY-10598, USA 
Email: dverma@us.ibm.com 

 
Abstract: This paper provides an overview of  service-level agreements  in  IP networks.  It 
looks  at  the  typical  components  of  a  service-level  agreement,  and  identifies  three 
common approaches that are used to satisfy service level agreements in IP networks. The 
implications  of  using  the  approaches  in  the  context  of  a  network  service  provider,  a 
hosting service provider, and an enterprise are examined. While most providers currently 
offer  a  static  insurance  approach  towards  supporting  service  level  agreements,  the 
schemes that can lead to more dynamic approaches are identified.  
Keywords: SLA, Service Level Agreements, Network Management.   

1. Introduction 
A  Service  Level  Agreement  (SLA)  is  a  formal  definition  of  the  relationship  that  exists 
between  a  service  provider  and  its  customer.  A  SLA  can  be  defined  and  used  in  the 
context  of  any  industry,  and  is  used  to  specify  what  the  customer  could  expect  from  the 
provider,  the  obligations  of  the  customer  as  well  as  the  provider,  performance, 
availability  and  security  objectives  of  the  service,  as  well  as  the  procedures  to  be 
followed  to  ensure  compliance  with  the  SLA.  Service  level  agreements  are  often  used 
when  corporations  outsource  functions  considered  outside  the  scope  of  their  own  core 
competencies  to  third  party  service  providers.  The  operation  and  maintenance  of 
computer  networks  is  outsourced  by  many  companies  to  third-party  network  providers, 
making SLA support an important subject in the context of computer networks. 
 
This paper  looks at  the different approaches used  for supporting service  level agreements 
in  computer  networks,  specifically  in  the  context  of  networks  based  on  the  Internet 
Protocol.  In  the next section of  this paper, we  look at  the  typical components of a service 
level  agreement.  This  is  followed  in  Section  3  by  a  description  of  some  common 
environments  of  IP  networks  where  service  level  agreements  play  an  important  role. 
Section  4  examines  the  different  approaches  that  are  used  by  different  organizations  in 
order  to  meet  the  obligations  of  their  service  level  agreements  in  these  environments. 
Finally, we  summarize  the  status  of  service  level  agreement  support  in  IP  networks,  and 
identify areas for further research.  

2. Typical Components of SLAs 

A service level agreement would typically contain the following information:  
•  A  description  of  the  nature  of  service  to  be  provided:  It  includes  the  type  of 
service to be provided, and any qualifications of the type of service to be provided. 
In  the  context  of  IP  network  connectivity,  the  type  of  service  may  specify  the 

maintenance  of  network  connectivity, or  it may  include  additional  functions  such 
as  operation  and  maintenance  of  domain  name  servers,  dynamic  host 
configuration protocol servers, etc.  
•  The  expected  performance  level  of  the  service,  specifically  its  reliability  and 
responsiveness:  Reliability includes availability requirements; when is the service 
available,  and  what  are  the  bounds  on  service  outages  that  may  be  expected.  
Responsiveness  includes how  soon  the  service would be performed  in  the normal 
course of operations.  
•  The procedure  for reporting problems with  the service: This  includes  information 
about  the  person  to  be  contacted  for  problem  resolution,  the  format  in  which 
complaints  have  to  be  filed,  and  the  steps  to  be  undertaken  in  order  to  quickly 
resolve  the problem. The agreement would also  typically describe a  time-limit by 
which  a  reported  problem  would  be  responded  to  (someone  would  start  to  work 
on the problem) as well as how soon the problem would be resolved. 
•  The  time-frame  for  response  and  problem  resolution:  This  specifies  a  time-limit 
by  which  someone  would  start  investigating  a  problem  that  was  reported.  The 
start  of  the  investigation  is  typically  marked  by  a  representative  of  the  supplier 
contacting  the  customer  who  reported  the  problem  initially.  There  may  also  be  a 
time  limit  by  which  the  problem  would  be  resolved.  A  SLA  may  specify  that  a 
failed link would be recommissioned within 24 hours.  
•  The  process  for  monitoring  and  reporting  the  service  level  :  This  outlines  how 
performance  levels  are   monitored  and  reported,  i.e., who will do  the monitoring, 
what  types  of  statistics will  be  collected,  how  often would  they  be  collected,  and 
how  past/current  statistics  may  be  accessed.  Some  network  providers  may  allow 
the  customer  to  directly  access  part  of  the  network  through  a  network 
management tool. The customer would be typically provided access to monitoring 
and statistics information, but may not be allowed to modify the configurations or 
operation of the network.   
•  The  consequences  for  the  service  provider  not  meeting  its  obligations:    It  is 
customary  to  extend  some  credits  to  the  customers when  the  service  expectations 
are  not  met.  Other  consequences  of  not  meeting  the  obligation  may  include  the 
ability of the customer to terminate its relationship, or to ask for reimbursement of 
part  of  the  revenues  lost  due  to  loss  of  service. The  consequences  of  not meeting 
the  SLA  may  vary  depending  on  the  nature  of  the  relationship  between  the 
customer and the supplier.  
•  Escape  clauses  and  constraints:  Escape  clauses  are  conditions  under  which  the 
service  level does not apply, or under which  it would be considered unreasonable 
to  meet  the  requisite  service  level  agreements,  e.g.  when  the  service  provider’s 
equipment have been damaged in flood, fire or war. They often also impose some 
constraints  on  the  behavior  by  the  customer.  A  network  operator  may  void  the 

service  level agreement  if  the customer  is attempting  to breach  the  security of  the 
network. 
Not  all  of  the  components  of  a  SLA  may  be  present  in  all  contracts,  but  a  good  SLA 
would  provide  an  overview  of  the  different  items  that  can  go  wrong  with  the  provided 
service, and attempt to cover those situations as part of the SLA agreement.  

 

3. IP Network Environments 
Within  the  context  of  an  IP  networks,  SLAs  are  typically  provided  for  three  common 
types  of  operating  environments.  Each  of  these  environments  consists  of  service 
providers  offering  a  different  type  of  service  to  their  customers.  The  three  common 
services provided  in  IP networks are network connectivity services, hosting services, and 
integrated connectivity and hosting services. 

3.1 Network Connectivity Services  
Network connectivity  services are provided by  several  telecommunications companies  to 
large  enterprises.    They  provide  the  access  links  to  the  different  sites  of  its  customers, 
enabling customer sites to be connected to each other as an intranet, or provide the access 
to  the  global  Internet  from  the  customer’s  site.  Customer  networ ks  are  attached  to  the 
provider  network  via  access  routers  that  are  present  at  the  access  points.  A  typical 
scenario  is  shown  in  Figure  1.  For  each  customer,  the  network  operator  has  defined 
performance and availability limits in the appropriate SLA signed with the customer.  
 

Customer Networks

Access Routers

ISP Network

Internet

Figure 1

 

 
 
In this environment, typical clauses related to performance and availability may look like:   
 

•  The  average  delay  measured  monthly  across  the  ISP  network  between  any  two 
access routers of the customer should be less than 200 ms. 

•  The average delay across the ISP network between any access router in New York 
City to any access router of the customer within the US would be less than 200 ms.  
•  The  average  delay  across  the  ISP  network  on  the  transcontinental  link  between 
New York City access router of  the customer and  the London access router of  the 
customer would be less than 250 ms.  
•  The  customer  will  not  have  unscheduled  connectivity  disruption  across  the  ISP 
network  between  any  two  access  routers  exceeding  5  minutes.  Connectivity 
disruption  would  be  defined  as  the  loss  of  100%  of  packets  as  measured  by 
pinging an access router from a machine connected to another access router. 

 
By  offering  better  performance,  availability  and  responsive  customer  service  to  its 
customers,  a  network  provider  would  be  better  able  to  compete  with  its  rivals.  The 
resources within  the  network  are  provisioned  so  as  to meet  the  desired  performance  and 
availability  objectives,  thereby  reducing  the  operational  cost  without  impacting  the 
satisfaction of the customers receiving the connectivity service.  
 

3.2 Hosting Services 
Hosting  services  are offered by  operators  that host  and  support  different  types of  servers 
on  behalf  of  their  customers. The most  common  case  of  these  providers  are web-hosting 
companies,  e.g.  Verio  (www.verio.com)  or  SBC  web-hosting  (www.webhosting.com), 
that  manage  and  provide  servers  to  operate  the  web-sites  for  individual  companies. 
Hosting  operators  provide  a  variety  of  services  to  their  customers,  and  includes 
companies  that  simply  provide  cages  in  well-connected  locations  where  customers  may 
place  their  servers,  companies  that  provide  cages  and maintain  the  uptime  of  the  servers 
in  the  cages,  as  well  as  companies  that  take  responsibility  for  running  and  ensuring  the 
operation  of  the  entire  application  hosted  at  the  site.  The  last mode  of  providing  hosting 
services,  in which  the  provider  is  responsible  for  the  overall  operation  of  the  application 
being  hosted,  is  gaining  ground  as being more  attractive  to both  the  service provider  and 
the customer.  

Internet

Access
Router

Firewall
Load
Balancer

Server

Server

Server

Hosting Site

Figure 2

 
The  typical  scenario  for  hosting  services  is  shown  in  Figure  2.  The  SLAs  offered  by 
hosting  providers  deal  with  the  uptime  and  performance  of  the  servers  that  are  being 
hosted.  These  operators  are  only  able  to  control  the  server  side  of  the  total 

 

communication.  In  most  cases,  they  have  no  control  over  the  client  side  of  the 
communication,  nor  over  the  performance  of  the  network  (usually  the  Internet).    As  a 
result,  Service  Provider  SLAs  usually  specify  the  amount  of  sustained  throughput  or 
connection  request  rates  that needs  to be  supported  for  a  specific  server. This determines 
the  aggregated  number  of  requests  that  must  be  handled  by  the  server  with  acceptable 
performance.  In  the  hosting  environment,  typical  performance  and  availability  clauses 
that are provided to the customer may look like: 
 
 
•  The  hosted  server  will  not  be  unavailable  for  a  contiguous  period  exceeding  5 
minutes  in  any  24 hour period. Unavailability  is defined  as  the  ability  to ping  the 
server  from a machine with network connectivity  to  the hosting provider’ s  access 
router.  
•  The  hosted  server  will  be  able  to  handle  inbound  traffic  of  30,000  web-requests 
per day.  
•  The  hosted  application  will  be  provided  access  to  the  Internet  at  a  bandwidth  of 
45 Mbps or more.     
•  The  service  provider  will  ensure  that  there  are  at  least  5  servers  available  and 
running the application at all times.  

 
A  large  hosting  service  provider  may  host  multiple  customers  at  the  same  site,  and  thus 
would  be  responsible  for  ensuring  that  the  performance  of  one  customer's  server  is  not 
adversely affected by requests directed to other customers.  

3.3 Integrated Services 
 
A  third  type  of  service  provides  a  consolidated  service  in  which  the  service  provider 
controls the network as well as the hosting infrastructure. Such a service is often provided 
by  an  enterprise  information  technology  (IT)  department  that  operates  and maintains  the 
intranet  of  an  enterprise  and  the  various  applications  that  run  within  that  environment. 
The  customers  of  the  IT  department  are  the  other  departments  of  the  enterprise.  Often 
times,  the  IT  department  is  in  charge  of  clients  as  well  as  servers  in  the  network,  and 
needs to control end-to-end performance of the different applications.  
In an integrated environment, customers would often expect performance and availability 
on  the operation of  the entire distributed  system. Some examples of performance  clauses 
that one may see within an enterprise IT context would look like: 
 

•  The  time  to  perform  an  employee  lookup  on  the  corporate  directory  would  not 
exceed 500 milliseconds.  
•  The  average  performance  of  a  standard  synthetic  web-based  transaction,  as 
reported by probes located at selected locations, will not exceed 100 milliseconds.  
•  Unscheduled  downtime  of  the  mail  server  will  not  exceed  a  30  minute  period 
during the normal business day of 9 AM to 5 PM.  

 
Another  type  of  integrated  service  is  offered  by  hosting  service  providers  which  would 
provide  an  integrated  hosting  and  connectivity  service  to  the  provider. As  an  example,  a 
networking  service  provider  like  AT&T  also  operates  data-centers,  and  could  offer  a 

consolidated  service  including  network  connectivity  and  data  center  hosting  to  its 
customers.  An  alternate  way  to  provide  an  integrated  service  may  be  outsourcing  some 
the hosting part or  the networking part  to another  company. As an  example,  IBM Global 
Services  may  offer  an  integrated  service  to  its  customers,  and  obtain  networking 
connectivity by outsourcing it to AT&T.  
 
In  all  of  the  above  operating  environments,  the  nature  of  the  service  being  provided  and 
the  performance/availability  objectives,  and  the  mechanism  used  to  monitor  the 
performance  level would  the  service are different. However,  the other components of  the 
service level agreements would tend to be relatively similar in all of those environments.  
 
The  procedure  for  reporting  problems with  the  service would  typically  describe whether 
the problem should be reported by calling a help-desk, or whether it can be reported via a 
web-based  interface.  The  help-desk  personnel  or  the web-interface may  attempt  to  solve 
the  problem  using  a  set  of  known  procedures,  or  open  a  problem  ticket  for  support 
personnel  to  try  to  solve  the  problem.  The  time-frame  for  response  and  problem 
resolution  clause  would  be  similar  across  the  different  environments,  and  would  dictate 
how  soon action would be  taken on  the problem  ticket. The escape clauses as well as  the 
penalties  to  be  paid  for  not  monitoring  the  service  would  also  be  expressed  in  similar 
terms.  
 

4. Approaches to SLA support 
 
Supporting  the  appropriate  level  of  performance  and  availability  specified  in  a  service 
level  agreement  is  an  important  aspect  of  the  operation  of  an  enterprise.  Since  the 
inability  to  meet  service  level  agreements  can  often  result  in  monetary  damages,  the 
provider  of  services,  regardless  of  the  specific  type  of  service  being  offered,  attempts  to 
meet the service level agreements to the best of their ability. 
 
A  service provider may  sign SLAs with difference performance objectives with different 
customers.  The  network  operator  needs  to  identify  the  type  of  packets  coming  into  the 
network,  so  that  they  can  be  dealt  with  appropriate  urgency.  Once  the  SLA  has  been 
agreed upon, the network operator needs to monitor the performance of the network.  The 
SLA would detemine which network performance metrics ought  to be monitored, as well 
as the operating ranges of the performance metrics.  
 
The  creation  and  filing  of  periodic  reports  is  an  important  step  in  the  process  of 
supporting  SLAs.  The  reports  on  monitored  performance  must  be  available  for 
examination by  the  customer. A  side-benefit of  storing  reports would be  that  the historic 
information can be used to extrapolate trends in network traffic, and thus be used as input 
to the service provisioning process.  
 
If  monitoring  indicates  that  all  the  SLAs  are  being  satisfied,  there  is  no  need  for  any 
further  action.  However,  one  may  want  to  check  if  it  would  be  possible  to  satisfy  the 
same  SLA  constraints  with  a  possibly  cheaper  or  simpler  configuration.  If  so,  the 

operational constraints of  the network might need  to be changed. A worse  case would be 
when  the SLA objectives are not being met.  In  this case,  the network configuration must 
be  changed,  through  the  service  provisioning  process,  so  that  the  objectives  can  be 
successfully met.  
 
As a last step of the customization process, one must examine if the agreed  SLAs can be 
satisfied.  If  experience  shows  that  the  SLAs  can  not  be met,  one may want  to  revise  the 
performance  objectives  to  those  that  are  feasible  to  meet.  One  can  also  revise  SLA 
objectives  to  become  more  stringent,  if  that  is  likely  to  attract  new  customers  and  new 
streams of revenues. 
 
Three  common  approaches  are  used  to  support  and  manage  service  level  agreements 
within  the  three  IP  environments  described  in  Section  3.  The  first  approach  takes  the 
model  of  an  insurance  company  towards  monitoring  and  supporting  SLAs.  The  second 
approach  uses  configuration  and  provisioning  techniques  to  support  SLAs  within  the 
network  and  the  third  approach  takes  a  more  dynamic  and  adaptive  approach  towards 
supporting service level agreements.  
 

Insurance Approach:  
In  the  insurance  approach  towards  supporting  SLAs,  the  service  provider  makes  its  best 
attempt  to  satisfy  the  performance,  availability  and  responsiveness  objectives  that  are 
specified  in  the  service  level  agreement  according  to  its  normal  operating  procedures. 
Generally,  the  same  level  of  service  is  offered  to  all  of  the  customers.    The  service 
provider  keeps  on  monitoring  its  service  to  check  how  well  it  is  complying  with  the 
objectives  set within  the  contract. The performance  and  availability parameters  specified 
within the SLA are specified so that they are not likely to be violated during the course of 
normal operation of  the  system. When  the  limits are violated,  the  service provider would 
pay  the  penalty  charges  specified  in  the  contract  to  the  customers.  Thus,  the  service 
provider is computing the financial risk associated with providing a given service level to 
a  new  customer,  and  revises  the  terms  offered  to  customers  when  the  financial  risk 
associated with  the  SLA  violation  is  unacceptable. The  computation  of  such  risk  is what 
most  companies  in  the  insurance  business  do,  and  this  approach  can  be  viewed  as  self-
insurance of a service provider against the risk of violating the service level objectives. It 
would  not  be  unreasonable  in  the  future  to  see  service  providers  take  out  explicit 
insurance policies against the possible violation of their service level objectives.  
 
In  other  words,  the  insurance  approach  towards  supporting  SLAs  can  be  summarized  as 
performing the following steps: 
 

 1.  Identify  service  objectives  (performance,  availability,  responsiveness)  to  be 
offered for the service.  
 2.  Monitor agreed-upon service objectives.  
 3.    Issue  SLA  reports,  possibly  including  periodic  meetings  with  the  customers  to 
discuss status of SLA compliance. 
4.  Issue appropriate credits to the customers if service levels are not being satisfied. 

5. Periodically, modify service  level objectives so  that  the probability of violating  the 
objectives and the associated financial impact is acceptable.  

 
A  description  of  a  network  architectures  that monitors  SLA  compliance  can  be  found  in 
[8].   

Provisioning Approach: 
In  the  provisioning  approach  towards  satisfying  service  level  agreements,  the  service 
provider  typically  signs  different  types  of  service  objectives  with  different  customers. 
The  service  provider  would  allocate  the  resources  within  the  environment  differently  to 
each  customer  in  order  to  be  able  to  support  the  service  level  objectives  for  each  of  the 
individual  customer.  The  determination  of  the  configuration  of  the  system  is  the  most 
crucial  step  towards  the  support of  service  level  objectives  for  each  individual  customer. 
Beyond this step, the service provider follows the same approach towards monitoring and 
payment  of  credits/penalties  to  the  customers  as  in  the  insurance  approach  described 
earlier. The provisioning approach can be summarized as the following steps 
 

1.  Identify  service  objectives  (performance,  availability,  responsiveness)  to  be 
provided to each customer. 
2. Determine the right system configuration to be used for each of the customers.    
3.  Monitor agreed-upon service objectives.  
4.    Issue  SLA  reports,  possibly  including  periodic  meetings  with  the  customers  to 
discuss status of SLA compliance. 
5.  Issue appropriate credits to the customers if service levels are not being satisfied. 

 

Adaptive Approach: 
The  third  approach  towards  satisfying  service  level  agreements  adds  on  an  additional 
aspect  of  adaptive  configuration  to  the  provisioning  approach.  In  this  approach,  the 
service  provider  would  dynamically  modify  the  configuration  of  the  system  used  to 
support  the  customer  when  monitoring  indicates  that  the  service  objectives  provided  to 
the customer are in the danger of being violated. This step reduces the probability that the 
service will  actually  be  violated,  but  does  not  eliminate  it  altogether. The  steps  involved 
in the adaptive approach are the following: 
 

1.  Identify  service  objectives  (performance,  availability,  responsiveness)  to  be 
provided to each customer. 
2. Determine the right system configuration to be used for each of the customers.    
3.  Monitor agreed-upon service objectives.  
4.  If  monitoring  indicates  possible  violation  of  objectives,  reconfigure  customer 
configuration to better server the service objectives.  
5.    Issue  SLA  reports,  possibly  including  periodic  meetings  with  the  customers  to 
discuss status of SLA compliance. 
6.  Issue appropriate credits to the customers if service levels are not being satisfied. 

 

We  can  now  map  the  above  three  approaches  to  each  of  the  three  types  of  services 
discussed in Section 3. 
 

4.1 Network Connectivity Services 
 
In  the  context  of  networking  SLAs,  the  insurance  approach  to  support  service  level 
agreements  is  the  most  prevalent  one  in  the  industry.  Service  providers  (ISP)  such  as 
UUNet  provide  assurances  on  the  availability  and  responsiveness  of  the  connectivity 
services  they  offer  to  their  enterprise  customers.  The  availability  and  responsiveness  can 
be  defined  in  a  variety  of ways,  the most  common metrics  being  the  delay  between  two 
access routers and the loss-rate between a pair of access routers.  
 
In  an  insurance  approach  towards  supporting  service  level  agreements,  the  ISP  will 
assure  an  upper  bound  on  the  delay  and  loss-rate  between  any  pair  of  access  routers, 
averaged across  all of  the access  routers and over a  reasonable duration of  time. The  ISP 
would  install  a monitoring  scheme  to measure  the  delay  and  loss-rate  among  the  access-
points.  Common  ways  of  measuring  the  performance  include  the  use  of  IP  pings  to 
collect  the delays between points  in  the network, as  exemplified  by  the data  collected by 
the  Surveyor  project  (http://www.advanced.org/surveyor/)  and  the  collection  of  network 
delays as measured by the protocol exchanges of NTP, the network time protocol. UUnet 
measures  monthly  averages  of  access  point  latencies,  offering  to  return  parts  of  service 
charges if the performance guarantees are not being satisfied [1].  
 
As  we  move  from  the  insurance  approach  to  the  provisioning  approach  of  SLAs, 
networking  connectivity  providers  would  need  to  offer  different  levels  of  service  and 
performance  to  different  customers.  For  a  networking  services  provider,  it  would  mean 
offering  a  service with  a  lower  latency  and  loss  rate  to  some  customers  in comparison  to 
others.  The  terms  of  SLAs  are  customized  for  different  customers.  In  the  context  of 
network connectivity, it means that a service provider would need to provide for different 
latencies  and/or  loss-rates  on  the  links  that  interconnect  different  customers  on  the  same 
link. A  customer  running  latency-critical  applications,  e.g. Voice over  IP  calls, on  the  IP 
network  may  desire  a  tighter  assurance  on  the  network  latency  than  one  running 
traditional computer applications. A monthly average for round trip delays is not likely to 
be  useful  for  VoIP  applications,  which  typically  require  an  absolute  maximum  delay 
bound  in  the  order  of  hundreds  of  milliseconds.  In  order  to  support  the  different 
requirements  of  different  customers,  network  service  providers  would  need  to  use 
techniques like Differentiated Services [2] or traffic engineering using MPLS [3] in order 
to  plan  the  networks  so  that  they  would  meet  the  requisite  performance  targets.  Such 
provisioning  of  systems  can  be  done  currently  using  capacity  planning  tools  and  circuit 
establishment schemes that typically tend to be manual.  
 
Future  technologies  on  the  horizon,  e.g.  dynamic  bandwidth  provisioning  in  optical 
networks  [4]  offers  the  ability  for  the  carrier  to  do  such  provisioning  in  an  automated 
manner. That would enable the network operator  to dynamically provide more bandwidth 
on  its circuits as  traffic carried on  specific  segments of  the network  increase. This would 

allow  the  network  connective  service  providers  to  move  towards  the  adaptive  approach 
for  providing  service  level  agreements.  Some  of  the  algorithms  to  support  an  adaptive 
approach can be found in [5].     
 

4.2 Hosting Services.  
 
In the context of hosting services, the service provider is mostly concerned with operating 
a  set  of  network  connected machines  that  are  hosting  an  application,  e.g.  a web-site  or  a 
mail-server.  The  main  aspects  of  service  level  agreements  that  are  provided  in  this 
context  deal  with  the  uptime  and  availability  of  the  service.  A  typical  SLA  in  this 
environment  would  offer  the  customer  an  assurance  that  service  will  not  be  down  for 
more  than a  specific period of  time. Generally,  if  the  service  is disrupted  for more  than a 
specific  amount  of  time,  the  provider  would  issue  credits  to  the  customer.  Some  sample 
SLAs that correspond to  the hosting environment can be seen in [9] and [10]. The uptime 
and  availability  constraints  provided  by  the  hosting  service  providers  tend  to  follow  the 
insurance model for supporting SLAs. The same  level of uptime and reliability assurance 
is  provided  to  all  of  the  customers,  and  when  unexpected  events  cause  the  performance 
objectives not to be met, credits are issued to the customer.  
  
Performance  based  service  level  agreements  are  typically  not  found  in  the  context  of 
hosting services. This  is because  the performance of an application depends upon several 
factors  that  are no within  the  control of  the hosting  service provider. The hosting  service 
provider  has  little  control  over  the  network  latency  connecting  potential  users  to  the 
service,  or  on  the  development  of  the  actual  application  itself.  As  a  result,  the  hosting 
service provider  is  not  able  to  influence  the performance of  the overall  application  itself. 
As a result, many hosting services provider only offer their customer co-location services, 
i.e.  the ability  to place machines  in a physical  facility,  and offer guarantees  related  to  the 
availability  of  the  network  connectivity  to  their  machines.  Other  hosting  services 
providers could also take over the administration of the servers, and their services include 
the  task  of  provisioning  the  appropriate  configuration  of  machines  needed  to  run  the 
applications. 
 
Co-location services are a type of hosting in which the provider is offering physical space 
(including  power  supply  and  network  access)  to  its  customer.  In  the  context  of  co-
location  services,  the  only  difference  in  contract  that  the  provider  can  offer  to  different 
customers would be in terms of the physical bandwidth available on the access link to the 
Internet.  The  provider  could  obtain  different  physical  access  links  for  the  different 
customers. Providers tend to follow an insurance model for such a physical access.  
 
When more  than  one  customer  is  preset  at  one  of  service  provider’s  premises,  it  is more 
economical  to  get  a  common  access  link  and  share  it  among  all  the  different  customers. 
The access  link could be controlled by  a  rate control device which provides  the ability  to 
reserve different  levels  of bandwidth  to different  set of machine  addresses. Such devices 
are  available  from  many  vendors,  both  small  and  large  in  the  industry.  The  rate-control 
device  is generally partitioned  statically  to provide  for  the different  rates negotiated with 

the different customers located at the service provider’s premi ses. The static provisioning 
of  a  shared  access  link  to  provide  different  levels  of  network  bandwidth  access  allows  a 
service provider to support the provisioning approach for supporting SLAs.   
 
With  any  shared  resource,  the  static  partitioning  into  shares  of  different  customers 
independent of usage,  is not  the optimal use of  the  shared  resource. For  the best usage of 
the  shared  resource,  the  network  resource  must  be  partitioned  dynamically  so  that  the 
resource  is  used  in  the  optimal  manner.  The  rate-control  algorithm  used  for  regulating 
access  link  bandwidth  can  thus  be  crucial  in  influencing  the  right  sharing  of  bandwidth 
among  the  different  customers.  Rate  control  algorithms  which  implement  only  a  leaky-
bucket  model  for  sharing  bandwidth  would  note  be  able  to  share  the  bandwidth  among 
the  different  users,  while  rate  control  algorithms  that  allow  the  use  of  excess  shared 
bandwidth  among  the  heavy  demand  users  would  tend  to  use  the  bandwidth  more 
effectively.   This allows  the  service provider  to move  to an  adaptive  approach  to  support 
SLAs. 
 
If  the  SLAs  signed  between  the  hosting  services  provider  and  the  customer  specify  a 
fixed  rate  of  access  bandwidth,  then  the  service  provider  would  not  be  changing  the 
parameters  at  the  rate  control  device  during  run-time.  However,  if  the  performance 
parameters  in  the  SLA  contract  specify  other  parameters  such  as  the  packet  loss  rate  in 
the  access  network,  then  the  service  provider  has  to  ensure  that  adequate  bandwidth  is 
allocated  to meet  the current demands of  the customers.  In  that case,  the service provider 
may  want  to  monitor  the  usage  of  the  access  link  and  periodically  update  the  shares 
allocated  to different  customers  in order  to keep  the  shared allocated  to each customer  in 
proportion to their current usage. 
 
Some hosting providers offer  services which manages  the  servers and applications of  the 
customer.  Most  hosted  systems  tend  to  follow  a  tiered  configuration,  in  which  the 
application  system  consists  of  multiple  tiers,  each  tier  consisting  of  multiple  servers 
performing  the  same  task.  As  an  example,  most  web-based  applications  tend  to  be 
implemented  in  three  tiers,  the  first  tier  consisting  of  HTTP  servers  (e.g.  Apache),  the 
second  tier  consisting  of  application  servers  (e.g.  IBM Websphere Application  Server  or 
tomcat),  and  the  third  tier  consisting  of  the  database  server. At  each  tier,  a  load  balancer 
may  be  used  to  distributed work  among  a  number  of  different machines.  The  number  of 
servers used at various  tiers has a strong  influence  in determining  the responsiveness and 
availability of the managed service.  
 
Hosting  service providers  that offer management  and provisioning of  servers  can use  the 
shared  resources  to  manage  the  performance  and  responsiveness  of  applications  that  are 
hosted  at  their  sites.  In  this  environment,  the  hosting  service  provider  can  follow  an 
adaptive  approach  to  support  SLAs,  dynamically  modifying  the  number  of  servers 
provided  to  each of  the  customers  in order  to meet  its performance objectives. However, 
adjusting  the  number  of  servers  would  require  the  service  provider  to  maintain  a  shared 
pool of servers from which it would be able to carve out the appropriate configuration for 
each  customer  as  needed.  The  creation  of  a  shared  pool  of  servers  and  providing 
customers  free  servers  to  access  from  that  pool  is  the  corner-stone  of  utility  based 

computing  initiatives  by  hosting  services  providers  such  as  HP  [6]  and  the  Oceano 
project [7].  
 

4.3 Integrated Services.  
 
Integrated  services  are  usually  offered  by  the  IT  (information  technology)  department  of 
an enterprise and have the unique ability to control the hosting environment as well as the 
network which connects the clients to the servers.  
 
SLAs offered by the IT department of most large corporations tend to follow an insurance 
approach.  The  IT  department  would  offer  appropriate  SLAs  and  provision  the  network 
connectivity within  the enterprise  intranet and  the  servers  running an  application  so as  to 
provide the desired level of application performance.  
 
To migrate  from  the  insurance  approach  of  supporting  service  levels  to  the  provisioning 
or  the reactive model,  the  integrated service provider has  the ability  to use a combination 
of  the  techniques  from  the networking  services provider  area  as well  as  from  the hosting 
services  provider  area. These  techniques  used  for  this  purpose would  include  the  task  of 
maintaining  a  shared  set  of  resources,  and  then  trying  to  adapt  the  set  of  resources 
allocated to each application/customer in a manner best suited to support the service level 
agreements  that  are  in  place.  A  combination  of  the  adaptation  techniques  within  the 
network and ones at hosted application sites can be used.  
  

5. Conclusions and Future Areas of work. 
 
In  this  paper,  we  have  provided  an  overview  of  the  different  techniques  and  approaches 
that  can  be  used  to  support  the  notion  of  service  level  agreements  in  IP  networks.  We 
have  identified  the different  types of agreements  that are commonly used  in  IP networks, 
and  examined  the  different  models  used  in  supporting  the  agreements  in  each  of  those 
contexts.  Most  of  the  industry  currently  tends  to  follow  an  insurance  model  for 
supporting  service  level  agreements,  although  we  would  expect  them  to  move  towards 
the more dynamic schemes in the near future.  
 

References 
 
[1]  UUNET  Service  Level  Agreements  Specifications,  Available  at  URL  
http://global.mci.com/us/enterprise/customer/sla/servicessupported/index.xml.  
 
[2] S. Blake, D. Black, M. Carlson, E. Davies, Z. Wang and W. Weiss, "An Architecture 
for  Differentiated  Services", 
Internet  RFC  2475,  December  1998.  URL 
http://www.ietf.org/rfc/rfc2475.txt. 
 

[3]  J.  Boyle,  V.  Gill,  A.  Hannan,  D.  Cooper,  D.  Awduche,  B.  Christian  and  W.  Lai. 
"Applicability  Statement  for  Traffic  Engineering  with  MPLS",  Internet  RFC  3346, 
August 2002. URL http://www.ietf.org/rfc/rfc3346.txt. 
 
[4]  S.  Sengupta  and  R.  Ramamurthy,  "From  Network  Design  to  Dynamic  Provisioning 
and  Restoration  in  Optical  Cross-Connect  Mesh  Networks:  An  Architectural  and 
Algorithmic Overview", IEEE Network Magazine, July/August 2001, pp 46-54. 
 
[5]  E.  Bouillet,  D.  Mitra  and  K.  Ramakrishnan,  "The  Structure  and  Management  of 
Service  Level  Agreement 
in  Networks,"  IEEE  Journal  on  Selected  Areas 
in 
Communications, Vol.20, NO.4, May 2002, pp 691-699. 
 
 [6] V. Turner, "HP Utility Data Center", White Paper available at  
http://www.hp.com/large/infrastructure/utilitycomputing/images/IDCWhitePaper.pdf 
 
[7]  K.  Appleby,  S.  Fakhouri,  L.  Fong,  G.  Goldszmidt  and  M.  Kalantar,  "Oceano:  SLA 
based  Management  of  a  Computing  Utility",  IFIP/IEEE  International  Symposium  on 
Integrated Network Management, May 2001, pp. 855-868. 
 
[8]  E.  Kim,  J.  Song,  and  C.  Hong,  "An  integrated  CNM  architecture  for  multi-layer 
networks  with  simple  SLA  monitoring  and  reporting  mechanism",  Proceedings  of  IEEE 
Network Operations and Management Symposium, (NOMS) 2000, pp. 993 -994. 
 
[9]  Rackspace  Fanatical  Support  Service  Level  Agreement,  available  at  URL 
http://www.rackspace.com/infrastructure/service_levels.php.  
 
[10] Verio Service Level Agreements, available at URL http://verio.com/about/legal/sla/.  



  

1 
Microsoft  

 

                                                           1

Abstract 
  
1  Introduction 

locks, and  

'
seldom needed and either expensive or hard to replace.  

'
'

'
'
What'

* * 

'

'
complicated. 

'

'
user'
disks. 
problems. * 

* 

* 

* 

'

* 

* 

ers'

1.1 

'"real"

''

* 

* 

' "real"

* 

*  't 

* 

net. 
most of these things are unlikely to happen. '

'
1.2  Outline 

2 

'
2.1 

*  Secrecy* 

Integrity

*  Availabilityand resources. 

*  Accountability

are: 

---- integrity availability 

----

'

'

"
ees"ees 

should ple 

' 2

2.2 

' '

1)  programs. 
                                                           
2 "obvious"

won'

2) 

peopleprograms. 

3)  communications. 

gullible. 
1) 

2)  '
3) 

'
4) 

3 

  Reference 

monitor   Object 

Resource 

Principal 

Guard 

Authentication 
Source 

Authorization 
Request 

 
'

principal

authentication

authorization 

' ''
                                                           
3 " "

1)  '
2)  '

's 
read write

informa  Reference 

monitor   Principal 

Sink 

Information

Guard Source   

*  Authenticating" " "

tion?"

*  Authorizing "Who ". 

*  Auditing 's 

2.3 

  (Hoare) 

* 

4
                                                           4

5 

* 

cludes /etc/passwd 

--

''

* * 

*  directly. 

setup
1) 

' "Software"

                                                           5 '

2) 

database. 3) 
chines. 4) 

5)  'of. 

'
6

It'

" ". 

Users

' 'it'
                                                           6

unsafe. Administrators

'
policies7

* 

* 

members'* 

*  authority. 

'

can'

groups "roles"

'

'

                                                           7

Developers
3 

(Wheeler) 

3.1 

*  It'

* 

* 

" ". 

* 

setuid 

program' '

"impersonation". 

* 

process' '

'
tabase. 

"domain"

'

'

" ". 

"ticket"8 
                                                           
8

doesn'

'
3.2 

Atom
Spectra
1) 

KSSL. 2)  '

KAlice 9 
3)  KAlice Alice@Intel. 

com. 4)  Microsoft'

Alice@Intel. 
com Atom5) 

Spectra Atom

.com

key Ktemp

                                                                                              
9 KAlice

' KAlice

Ktempsystem. 

'

Ktemp

sion: 

* 

*  '

*  '

* 

Spectra* 

3.3 

Spectra  re"Principal P Q T." 

KSSL KAlice
Atom@Microsoft Spectrawrite. 

" " Pabout 
T Q Q

P T
P Q

T P'

Q'

statements.  "

T" T

P Q
T

T "all subjects" "
requests" '

T
T'

P  T\Theta  Q  P \Theta  Q T" "

KSSL \Theta  Ktemp \Theta  KAlice \Theta  Alice@Intel \Theta  
Atom@Microsoft r/w\Theta  Spectra 

"speaks for"

 

says 

Spectra ACL 

KSSL 

says 
says 

Alice's  system  Spectra 

Ktemp KAlice 

Alice@Intel  Atom@Microsoft 

Microsoft Intel 

 
'
3.4 
P \Theta  Q '

"verifier" "principal "
form  P  T\Theta  Q.10

trust

says
willing

Q P \Theta  Q Q 
Q P  T\Theta  Q, Qfor 
T P

P T
Q Q, 

                                                           10 "handoff"
"delegate"

"axioms"

Alice@Intel
that  Q P  T\Theta  Q Q

*  If Q "Q X" Q

X

Ktemp \Theta  
KAlice KAlice KAlice 

above. *  If 

Q P  T\Theta  Q

Atom  \Theta   Spectra

directly 

" "
KAlice  \Theta   Alice@Intel

for  Alice@Intel Intel  delegate\Theta   
Alice@Intel ' KAlice \Theta  

Alice@Intel Intel. 

KIntel \Theta  Intel

KIntel

' KIntel KIntel  \Theta  
Intel.  '

KIntel \Theta  Intel

' KMSDir
KMSDir"

KMSDir \Theta  *  *.Microsoft.com"  

bership Alice@Intel \Theta  Atom@Microsoft Intel  delegate\Theta    Alice@Intel Microsoft  delegate\Theta    Atom@ 
Microsoft '

KIntel \Theta  Intel
Atom r/w\Theta  Spectra

KDH ' KDH, 
"KDH \Theta 

KDH '
KDH ' KDH"  

KDH  \Theta   Kme
tocol.11 

S
SQLServer71 
.exe S C from this process, it 

C  \Theta   SQLServer71

S SQLServer71lieves 
S  \Theta   SQLServer71 S'

Shash 
HSQL7.1 "

HSQL71 \Theta  SQLServer71"ticating 
C

3.5  Names 

Intel delegate\Theta   Alice@Intel

Alice@Intel, 

P delegate\Theta   
P/N12 P Nrepeatedly, 

P can delegate from any path name that starts with 
P

K delegate\Theta   K/N K

'
K P \Theta  K/N

                                                           11 K

DH

12 Alice@Intel.com   com/Intel/Alice. 

P \Theta  K/N.  '

KIntel  \Theta   Intel
KIntel "KAlice  \Theta   Alice@Intel"

KIntel \Theta  Intel

believe KIntel \Theta  Intel.com Kcomknow 

Kcom  \Theta com

'

'
P P/N1, 
P/N2 " ". 

" " 

3.6  Variations 

How big objects are and how expressive T

'

'
'

'
'
" ' "

't '
's 

Push
Pull

'
P \Theta  Q \Theta  R P \Theta  
R R

'

'
uses  "tags"

"Alice  \Theta   Atomnamed 
*.doc "

T
.doc 'them. 

*  Conjunctions: 

'" "

"kof 
n"  *  Disjunctions:   

FlakyProgram

" "

FlakyProgram

 

3.7  Auditing 

proof

4  Conclusion 
doesn'* 

ishment. * 

" "
"Alice@Intel Atom.Microsoft *.doc". 

setuid

References 
1.   22, 

reports/abstracts/src-rr-25.html 2. 

  37ceedings/commsec/168588/p215-anderson 3. 

4. 

04.html 5. 

Privacy 6. 

 19  

7.  Extensions

rfcs/rfc2065.html 8. 

9. 10. 

proceedings/osdi00/howell.html 11. 

ACM    8

Protection/Abstract.html 12. 

  10
journals/tocs/1992-10-4/p265-lampson 13. 

Principlespubs/citations/proceedings/ops/268998/p129-myers 
14. 

21rsapaper.ps 
15. 

16. 
17.  Multics.  17
18.  ACM  2

mit.edu/Saltzer/www/publications/endtoend/endtoend.pdf 19. 
Schneier, World

20.  '

www.acm.org/pubs/citations/proceedings/commsec/ 288090/p132-schneier 
21.  ACM

 12www.acm.org/pubs/citations/journals/tocs/1994-12-1/p3-

wobber 22. 

2421377,00.html 23. 
4586,2469820,00.html                      Volume 2, Issue 4, April 2012                   ISSN: 2277 128X 
International Journal of Advanced Research in 
 Computer Science and Software Engineering 
                                                   Research Paper 
                             Available online at: www.ijarcsse.com 
Research Issues in Wireless Networks 
                                    
    Raj Kumar Singh1                                                            Dr.A.K.Jain2                                                          
             Professor & Head  
             M.Tech. (2nd yr)                                         
                             Department of Instrumentation & Control                                              Department of Instrumentation & Control                                                 
        Dr. B.R. Ambedkar National Institute of Technology              Dr. B.R. Ambedkar National Institute of Technology 
     
            Jalandhar, Punjab (India)       
 
 
        Jalandhar, Punjab (India)                                                                   
                                         rajmtechnit@gmail.com               
               jainak@nitj.ac.in 
 
 
 

Abstract— The arrival of wireless technology has reduced the human efforts for accessing data at various loca tions by replacing wired 
infrastructure with wireless infrastructure and also providing access to devices having mobility.Since wireless devices need to be small 
and  bandwidth  constrained,  some  of  the  key  challenges  in  wireless  networks  are  Signal  fading,  mobility,  data  rate  enhancements, 
minimizing size and cost, user security and (Quality of service) QoS. This paper is intended to provide the reader with an overview of 
the Research Issues and Challenges in wireless networks. 
Keywords— Wireless Local Area Networks (WLANs), IEEE 802.11, Quality of Service (QoS). 
 

I.  INTRODUCTION 

II.  TAXONOMY OF WIRELESS NETWORKS 

The  explosive  growth  in  wireless  networks  over  the  last 
few years resembles the rapid growth of the internet within the 
last  decade.  Wireless  communication  continues  to  enjoy 
exponential growth  in  the  cellular  telephony, wireless  internet 
and  wireless  home  networking  arenas.  With  advent  of 
Wireless LAN (WLAN) technology, computer networks could 
achieve  connectivity  with  a  useable  amount  of  bandwidth 
without  being  networked  via  a  wall  socket.  New  generations 
of  handheld  devices  allowed  users  access  to  stored  data  even 
when they travel. Users could set their laptops down anywhere 
and  instantly  be  granted  access  to  all  networking  resources. 
This  was,  and  is,  the  vision  of  wireless  networks,  and  what 
they are capable of delivering. Today, while wireless networks 
[1]  have  seen widespread  adoption  in  the  home  user  markets, 
widely  reported  and  easily  exploited  holes  in  the  standard 
security  system  have  stunted  wireless  deployment  rate  in 
enterprise  environments.  Over  time,  it  became  apparent  that 
some  form  of  security was  required  to  prevent  outsiders  from 
exploiting  the  connected  resources.  We  believe  that  the 
current  wireless  access  points  present  a  larger  security 
problem  than  the  early  lntemet connections.  As more wireless 
technology  is  wireless  technology,  this  will  be  a  good 
stepping-stone  for  providing  a  good  secure  solution  to  any 
wireless solution.  
The  rest  of  this  paper  is  organized  as  follows;  firstly  we 
present  the  taxonomy  of  wireless  networks  and  giving  the 
discussion  of  the  two  operating  modes  of  the  IEEE  802.11  in 
second  section.  We  then  provide  a  brief  overview  about 
Research  Challenges  and  Issues  of  Wireless  Networks  in 
section  third.  And  finally  the  section  fourth  gives  the 
conclusion of the whole paper. 

 
The  distinguishing  feature  of  wireless  networks  is  that 
packets  (segments)  are  transmitted  with  the  presence  of 
wireless  links.  A  device  can  send  messages  in  a  wireless 
network  via  the  wireless  medium,  air,  to  another  device 
provided  that  the  receiver  is  within  the  transmission  range  of 
the  sender.  This  adds  flexibility  to  how  a  wireless  network  is 
formed and structured. Besides, it supports device mobility. 

                           IEEE 802.11 

 

IEEE  802.11  is  a  basic  standard  for  Wireless  Local  Area 
Network (WLAN) communication. IEEE 802.11 standard was 
first introduced in 1997. It was envisioned for home and office 
environments for wireless local area connectivity and supports 
three  types  of  transmission  technologies  namely  i)  Infrared 
(IR),  ii)  Frequency  Hopping  Spread  Spectrum  (FHSS),  iii) 
Direct  Sequence  Spread  Spectrum  (DSSS).  In  1999  two  other 
transmission 
technologies  were 
included  Orthogonal 
Frequency  Division  Multiplexing  (OFDM)  and  High  Rate 
Direct  Sequence  Spread  Spectrum  (HR-DSSS).  The  second 
OFDM  modulation  scheme  was  introduced  in  2001  for  high 
data rates [2]. The standard introduces two operating modes of 
wireless networks, namely, the infrastructure networks and the 
ad hoc networks. 
 

A.  Infrastructured Networks 

 
The infrastructure operating mode (Fig 1) is a network with 
an  Access  Point  (AP),  in  which  all  STAs  must  be  associated 

© 2012, IJARCSSE All Rights Reserved                                                                                                                     Page | 115 

 
 

Volume 2, Issue 4, April 2012                                                                                                                               www.ijarcsse.com 

with  an  AP  to  access  the  network.  STAs  communicate  with 
each  other  through  the  AP.  An  infrastructured  one  with 
planned,  permanent  network  device  installations.  It  can  be  set 
up with a fixed topology, to which a wireless host can connect 
via  a  fixed  point,  known  as  a  base  station  or  an  access  point. 
The  latter  is  connected  to  the  backbone  network,  often  via  a 
wired  link.  Cellular  networks  [3]  and  most  of  the  wireless 
local  area  networks  (WLANs)  [4]  operate  as  the  static 
infrastructured  networks.  All  wireless  hosts  within 
the 
transmission coverage of the base station can connect to it and 
use it to communicate with the backbone network. This means 
that  all  communications  initiated  from  or  destined  to  a 
wireless  host  have  to  pass  through  the  base  station  to  which 
the  host  connects  directly.  In  addition,  an  infra-structured 
network is also be established with a quasi-static or a dynamic 
topology.  A  satellite  network  [5]  belongs  to  this  category.  It 
has  a  space  segment  and  a  ground  segment.  The  space 
segment  comprises  of  satellites.  The  ground  segment  has  a 
number of base stations, also known as gateway stations (GSs), 
through which all communications via long-haul satellite links 
take  place.  The  base  station,  or  access  point,  is  a  critical 
element for communication.  
 
 

directly whenever  the  receiver  is  in  its  transmission  coverage. 
If a wireless host would  like  to  send messages  to another host 
which is not in the coverage region, it will first relay them to a 
host  in  its  transmission  range. The host  functions as a  relay  to 
forward  the messages on  its way  to  the destination. The major 
advantage  of  this  configuration  is  flexibility.  An  ad-  hoc 
network  can  be  built  easily,  without  the  need  of  any  preset, 
fixed  infrastructure.  In  addition,  an  ad  hoc  network  is 
generally  more  robust  than  an  infrastructured  network  as  it 
does  not  have  any  critical  device  to  maintain  the  network 
connectivity.  In  other words,  it  is  unlikely  an  ad  hoc  network 
will be partitioned due to the failure of a wireless host, but the 
malfunction  of  a  base  station  may  partition  an  infrastructure 
network,  blocking  the  communication  between  all  wireless 
hosts  connecting  to  the  failed  base  station  and  all  other  hosts 
in the network. However, there are some drawbacks for ad hoc 
networks.  First,  it  is  much  more  difficult  and  complex  to 
perform  routing  in  ad  hoc  networks  because  of  frequent 
changes in the network topology due to host mobility.  
 
 

 

Fig. 1 Infrastructure wireless network 

 

 
To  maintain  an  ongoing  connection  when  a  mobile  host 
moves  away  from  the  coverage  of  its  base  station,  a  terminal 
handoff  occurs  such  that  a  mobile  host  hands  over  its  proxy 
for  communication  from  one  base  station  to  another  one. 
Whenever  the  coverages  of  several  neighboring  base  stations 
overlap with  each  other,  a mobile  host may  connect  to  one  of 
the reachable base stations based on certain criteria. 
 

B.  Ad Hoc Networks 

 
The  second  operating  mode,  the  independent  mode  or  the 
ad hoc mode (Fig 2) is used if there are no Access Points (APs) 
in  the network.  In  this mode, Stations  (STAs) form an Ad hoc 
network directly with each other.An ad hoc network, such as a 
packet  radio  network,  is  one  without  a  fixed  topology.  A 
wireless  host  can  freely  communicate  with  another  host 

 

Fig. 2 Ad Hoc wireless network 

 

 
Second,  it  is  more  difficult  to  control  or  coordinate  proper 
operation  of  an  ad  hoc  network,  since  each wireless  host may 
have  its  own  algorithms  to  perform  activities  such  as  time 
synchronization,  power  management,  and  packet  scheduling. 
In  an  infra  structured  network,  these  algorithms  are  often 
implemented  in  and  thus  harmonized  by  the  base  stations  or 
access points. 
 

III. RESEARCH  CHALLENGES OF WIRELESS 
NETWORKS 

 
Since  wireless  devices  need  to  be  small  and  wireless 
networks are bandwidth limited, some of the key challenges in 
wireless  networks  are  data  rate  enhancements,  minimizing 
size, cost,  low power networking, user security and Quality of 
Service (QoS). 
 

A.  Signal Fading 

 
Unlike  wired  media,  signals  transmitted  over  a  wireless 
medium  may  be  distorted  or  weakened  because  they  are 

© 2012, IJARCSSE All Rights Reserved                                                                                                                     Page | 116 
 

Volume 2, Issue 4, April 2012                                                                                                                               www.ijarcsse.com 

propagated  over  an  open,  unprotected,  and  ever  changing 
medium  with  irregular  boundary.  Besides,  the  same  signal 
may  disperse  and  travel  on  different  paths  due  to  reflection, 
diffraction, and scattering caused by obstacles before it arrives 
at  the  receiver.  The  dispersed  signals  on  different  paths  may 
take different times to reach the destination.Thus, the resultant 
signal  after  summing  up  all  dispersed  signals  may  have  been 
significantly distorted and attenuated when compared with  the 
transmitted  signal.  The  receiver  may  not  recognize  the  signal 
and  hence  the  transmitted  data  cannot  be  received.  This 
unreliable  nature  of  the  wireless  medium  causes  a  substantial 
number of packet losses. 
 

B.  Mobility 

 
Without  the  constraints  imposed  by  the  wired  connections 
among  devices,  all  devices  in  a  wireless  network  are  free  to 
move.  To  support  mobility,  an  ongoing  connection  should  be 
kept  alive  as  a  user  roams  around.  In  an  infrastructured 
network, a handoff occurs when a mobile host moves from the 
coverage  of  a  base  station  or  access  point  to  that  of  another 
one.  A  protocol  is  therefore  required  to  ensure  seamless 
transition  during  a  handoff.  This  includes  deciding  when  a 
handoff  should  occur  and  how  data  is  routed  during  the 
handoff  process.  In  some  occasions,  packets  are  lost  during  a 
handoff.  In  an  ad  hoc  network,  the  topology  changes  when  a 
mobile  host  moves.  This  means  that,  for  an  ongoing  data 
communication,  the  transmission  route  may  need  to  be 
recomputed  to,  cater  for  the  topological  changes.  Since  an  ad 
hoc  network  may  consist  of  a  large  number  of  mobile  hosts, 
this  imposes  a  significant  challenge  on  the  design  of  an 
effective  and  efficient  routing  protocol  that  can  work  well  in 
an environment with frequent topological changes. 
 

C.  Power and Energy   

 
A  mobile  device  is  generally  handy,  small  in  size,  and 
dedicated  to  perform  a  certain  set  of  functions;  its  power 
source  may  not  be  able  to  deliver  power  as  much  as  the  one 
installed  in  a  fixed  device.When  a  device  is  allowed  to  move 
freely,  it  would  generally  be  hard  to  receive  a  continuous 
supply of power. To  conserve  energy,  a mobile  device  should 
be  able  to  operate  in  an  effective  and  efficient manner.  To  be 
specific,  it  should  be  able  to  transmit  and  receive  in  an 
intelligent  manner  so  as 
the  number  of 
to  minimize 
transmissions  and  receptions  for  certain  communication 
operations [7]. 
 

D.  Data Rate 

 
Improving  the  current  data  rates  to  support  future  high 
speed  applications  is  essential,  especially,  if  multimedia 
service  are  to  be  provided.  Data  rate  is  a  function  of  various 
factors  such  as  the  data  compression  algorithm,  interference 
mitigation  through  error-resilient  coding,  power  control,  and 

the  data  transfer  protocol.  Therefore,  it  is  imperative  that 
manufacturers  implement  a  well  thought  out  design  that 
considers  these  factors  in  order  to  achieve  higher  data  rates. 
Data  compression  plays  a  major  role  when  multimedia 
applications such as video conferencing are to be supported by 
a  wireless  network.  Currently,  compression  standards  such  as 
MPEG-4 produce compression ratios of the order of 75 to 100. 
The  challenge  now  is  to  improve  these  data  compression 
algorithms  to  produce  high  quality  audio  and  video  even  at 
these  compression  rates.  Unfortunately,  highly  compressed 
multimedia  data  is  more  sensitive  to  network  errors  and 
interference  and  this  necessitates  the  use  of  algorithms  to 
protect  sensitive  data  from  being  corrupted.  Efficient  error 
control  algorithms  with  low  overhead  must  be  explored. 
Another  way  to  enhance  the  data  rates  would  be  to  employ 
intelligent  data  transfer  protocols  that  adapt  to  the  time-
varying network and traffic characteristics. 
 

E.  Security  

 
Security  is a big concern  in wireless networking, especially 
in m-commerce  and  e-commerce  applications  [8]. Mobility of 
users  increases  the  security  concerns  in  a  wireless  network. 
Current  wireless  networks  employ  authentication  and  data 
encryption  techniques  on  the  air  interface  to  provide  security 
to  its  users.  The  IEEE  801.11  standard  [2]  describes  wired 
equivalent  privacy 
(WEP) 
that  defines  a  method 
to 
authenticate  users  and  encrypt  data  between  the  PC  card  and 
the  wireless  LAN  access  point.  In  large  enterprises,  an  IP 
network  level  security  [9]  solution  could  ensure  that  the 
corporate  network  and  proprietary  data  are  safe.  Virtual 
private  network  (VPN)  is  an  option  to  make  access  to  fixed 
access  networks  reliable.  Since  hackers  are  getting  smarter,  it 
is  imperative  that  wireless  security  features  must  be  updated 
constantly [10]. 
 

F.  (Quality of Service) QoS  

 
Quality  of  Service  is  a  measure  of  network  performance 
that  reflects  the  network's  transmission  quality  and  service 
availability.  For  each  flow  of  network  traffic,  QoS  can  be 
characterized  by  four  parameters:  Reliability,  Delay,  Jitter, 
and Bandwidth. 
 
There  are  several  important  issues  related  to  QoS  in 
wireless  networks  that  do  not  get  addressed  in  the  wireline 
environment. These issues arise because wireless networks are 
inherently different from wireline networks. Several important 
wireless  network  characteristics  include  handoff,  dynamic 
connections, and actuating  transport QoS [11].The  traffic QoS 
parameters  (throughput,  delay  and  loss  rate)  are  not  sufficient 
in  a  wireless  environment.  In  a  wireline  environment,  the 
application  layer  can  normally  be  assured  that  once  a 
connection  is  established  it  will  continue  to  exist  until  it  is 
closed. 
In  a  wireless  environment,  connections  may 
temporarily  break  during  a  process  termed  handoff  [12].  It  is 

© 2012, IJARCSSE All Rights Reserved                                                                                                                     Page | 117 
 

Volume 2, Issue 4, April 2012                                                                                                                               www.ijarcsse.com 

unlikely  that  handoff  can  take  place  without  at  least  a  short 
connection  interruption.  Applications  running  in  a  wireless 
environment  must  be  able 
to  recover  from 
temporary 
interruptions,  and  should  specify  the  maximum  connection 
interruption  time  that  they  can  tolerate.  The  application  could 
specify  such  a  time  via  a  large  loss  rate;  however,  this would 
overload  the  meaning  of  loss  rate.  Loss  rate  should  only 
reflect  losses due  to buffer  overflow or  transmission  errors. A 
maximum  frequency  of  connection  interruption  is  another 
performance  parameter  that  would  be  valuable  in  a  wireless 
network.  Some  applications  may  request  a  low  interruption 
frequency  so  that  the  QoS  perceived  by  the  user  remains 
satisfactory.  For  example,  an  application  may  wish  to 
guarantee  that  a  voice  connection  will  not  be  broken  more 
than  once  per  minute.  A  low  interruption  frequency  implies 
that handoffs do not occur  too often. Applications may accept 
a  larger  maximum  connection  interruption  time  in  exchange 
for a low interruption frequency. For example, it may be more 
desirable to have infrequent long breaks in a video connection, 
rather than frequent smaller breaks.  
 

IV. CONCLUSION 

 
This  paper  identifies  and  describes  the  various  research 
issues and challenges available in the wireless domain. We fist 
presented  an  overview  of  the  taxonomy  of  wireless  network.  
We presented an overview of a comprehensive  list of research 
issues  and  challenges  of  the  wireless  network  like  signal 
fading  problem  ,  mobility  problem  ,power  and  energy  ,  data 
rate  enhancement,  security  and  the  quality  of  service  issues 
problems  of  the  wireless  networks  .In  addition  the  popularity 
of  wireless  networks  growing  at  a  exponential  rate,  the  data 
rate  enhancements,  minimizing  size,  cost, 
low  power 
networking,  user  security  and  the  best  requirement  to  obtain 
the required QoS problems becomes more challenging. 
 
In  conclusion,  wireless  networks  are  rapidly  becoming 
popular,  and  user  demand  for  useful  wireless  applications  is 
increasing. By  successfully  addressing  the  issues  presented  in 
this paper, end users will not be disappointed. 
 

REFRENCES 

 
[1]  http://en.wikipedia.org/wiki/Wireless_network 
[2] 
IEEE  802.11-1999,  IEEE  Standard  for  Local  and 
Metropolitan  Area  Networks  Specific  Requirements  – 
Part  11: Wireless  LAN Medium  Access  Control  (MAC) 
and Physical Layer (PHY) Specifications, June 12, 1999. 
[3]  V.O.K.  Li  and  X.  Qiu,  ―Personal  Communication 
Systems (PCS),‖ Proc. IEEE, vol. 83, no. 9, Sept. 1995,  
[4]  J.H.Schiller, Mobile Communications, 2nd  ed., Addison-
Wesley, 2003. 
[5]  Y.  Hu  and  V.O.K.  Li,  ―Satellite-Based  Internet:  A 
Tutorial,‖  IEEE  Commun.  Mag.,  vol.  39,  no.  3,  Mar. 
2001, pp. 154–62. 

[6]  A.Gupta, I. Wormsbecker, and C. Williamson, 
―Experimental Evaluation of TCP Performance in Multi-
Hop Wireless Ad Hoc Networks,‖ Proc. IEEE 
MASCOTS 2004, Volendam, The Netherlands, 4–8 Oct. 
2004, pp. 3–11. 
[7]  H.  Singh  and  S.  Singh,  ―Energy  Consumption  of  TCP 
Reno,  Newreno,  and  SACK  in  Multi-Hop  Wireless 
Networks,‖  ACM  SIGMETRICS  Perf.  Evaluation  Rev., 
vol. 30, no. 1, June 2002, pp. 206-216. 
[8]  Chip  Craig  J.  Mathias  Principal,  Farpoint  Group 
COMNET  2003  ―Wireless  Security:  Critical  Issues  and 
Solutions‖ 29 January 2003 
[9]  Sandra  Kay  Miller  ―Facing  the  Challenge  of  Wireless 
Security‖ July 2001 
[10]  T.  Karygiannis  and  L.  Owens.  Wireless  Network 
Security:802.11,  Bluetooth  and  Handheld  Devices.  In 
NIST Special Publication 800-48, November 2002. 
[11]  Paulo  Salvador,  Ant´onio  Nogueira,  Rui  Valadas 
―Predicting  QoS  Characteristics  on  Wireless  Networks‖ 
25 June 2007.  
[12]  Jim  Kurose  ―Open  issues  and  challenges  in  providing 
quality  of  service  in  high-speed  networks‖  Computer 
Communication Review, 23(1):6-15, January 1993. 

AUTHOR BIOGRAPHIES 
 
 Raj Kumar Singh was born at Bareilly, Uttar Pradesh, India on 24th 
October,  1989.  Currently,  He  is  pursuing  his  M.Tech  degree  in 
Instrumentation  and  Control  Engineering 
from  NIT  Jalandhar.  He  did  his  B.Tech  in 
Electronics  and  Communication  Engineering 
from  M.J.P.  Rohilkhand  University  Bareilly 
Uttar Pradesh India in 2009. He has published 
several  papers  in  national  conferences  and 
international journals on wireless and robotics. 
research 
His  area  of 
interest 
includes 
modelling 
and 
simulation  of  wireless 
networks,  robotics,  brain  computer  interfacing  and  biomedical 
applications. 
 
A.K.Jain  received  his  B.E  and  M.E  both  from  IIT,  Roorkee, 
(erstwhile  University  of  Roorkee,  Roorkee)  India  in  1981  and  1987 
respectively  and  received  his  Ph.D.  degree  on 
Quality  of  Service  in  High  Speed  Networks 
from  the  Dr.  B.  R.  Ambedkar  National 
Institute  of  Technology,  Jalandhar,  India  in 
2009.He  has  published  over 
twenty-five 
research  papers  in  national  and  international 
journals/conferences.  He  is  presently  working 
as  Professor  and  Head  in  the  Department  of 
Instrumentation  and  Control  Engineering,  Dr.  B.  R.  Ambedkar 
National  Institute  of  Technology,  Jalandhar,  India.  He  is  guiding 
Ph.D  and M.Tech  students  in  the  area  of Wireless Networks.  Before 
joining  N.I.T,  Jalandhar,  he  has  served  at  TIET  Patiala,  IET 
Lucknow,  and  NIT  Hamirpur  (Erstwhile  REC  Hamirpur)  in  various 
capacities.  His  research  interests  include  quality  of  service  in 
wireless  networks,  medium  access  protocols  for  mobile  computing, 
and mesh networks. Dr. Jain is life member of ISTE India. 
 

© 2012, IJARCSSE All Rights Reserved                                                                                                                     Page | 118 
 

Volume 2, Issue 4, April 2012                                                                                                                               www.ijarcsse.com 

 

© 2012, IJARCSSE All Rights Reserved                                                                                                                     Page | 119 
 

Benchmarking  computers  and  computer 
networks 

Stefan Bouckaert, Jono Vanhie -Van Gerwen, Ingrid Moerman - IBBT, Ghent University, Belgium 
Stephen C Phillips, IT Innovation Centre, UK 
Jerker W ilander, FIREstation 
Shafqat Ur Rehman, Walid Dabbous, Thierry Turletti, INRIA, France 

Introduction 

The  benchmarking  concept  is  not  new  in  the  f ield  of  computing  or  computer  networking. W ith 
“benchmarking  tools”, one usually  refers  to a program or set of programs, used  to evaluate  the 
performance  of  a  solution  under  certa in  reference  conditions,  relative  to  the  performance  of 
another  solution.  Since  the  1970s,  benchmarking  techniques  have  been  used  to  measure  the 
performance  of  computers  and  computer  networks.  Benchmarking  of  applications  and  virtual 
machines  in  an  Infrastructure-as-a-Service  (IaaS)  context  is  being  researched  in  a  BonFIRE 
experiment and  the benchmarking of wired and wireless computer networks  is put forward as a 
research topic  in the research projects CREW  and OneLab2. In this paper, we elaborate on the 
interpretation  of  the  term  “benchmarking”  in  these  projects,  and  answer  why  research  on 
benchmarking  is  still  relevant  today.  After  presenting  a  high -level  generic  benchmarking 
architecture, 
the  possibilities  of  benchmarking  are 
illustrated 
through 
two  examp les: 
benchmarking cloud services and benchmarking cognitive radio solutions.  

Definition and benchmarking aspects 

In  the scope of the performance analysis of computer systems, we def ine  benchmarking as  the 
act of measuring and evaluating computational perfo rmance, networking protocols, devices and 
networks,  under  reference  conditions,  relative  to  a  reference  evaluation.  The  goal  of  this 
benchmarking  process  is  to  enable  fair  comparison  between  different  solutions,  or  between 
subsequent  developments  of  a  Sys tem  Under  Test  (SUT).  These  measurements  include 
primary performance metrics,  collected directly from  the SUT  (e.g. application  throughput, node 
power  consumption),  and  in  case  of  wireless  networks  also  secondary  performance  metrics, 
characterizing  the  environment  in which  the  SUT  is  operating  (e.g.  interference  characteristics, 
channel  occupancy).    The primary  and  secondary  performance metrics may  be  complemented 
by techno-economic metrics, such as device cost or operational complexity. 
 
The following terminology will be used throughout this paper: 
 
A  benchmark  contains  a  full  set  of  specif ications  needed  to  enable  the  evaluation  of  the 
performance  of  a  certain  aspect  of  a  SUT.  These  specif ications  include  (i)  a  scenario,  (ii) 

performance  evaluation  criteria¸  (iii)  performance  evaluation  metrics,  and  (iv)  a benchmarking 
score.  Depending  on  the  context,  the  term  benchmark   may  further  also  indicate  a  specif ic 
experiment  that  is executed according to the benchmark; e.g.: to  run a  “throughput benchmark” 
v.s.  “the  throughput benchmark  is designed  to fairly compare end -to-end  throughput between  x 
and y under condition z”. 
 
A  scenario  is  a  detailed  description  of  the  set -up  of  the  experiment  needed  to  run  the 
benchmark. For example,  it may contain a network  topology , protocol conf iguration parameters 
or  traffic  traces.  The  criteria  describe  the  high-level  focus  of  the  output  of  the  benchmark. 
Example  criteria  are  energy  eff iciency  or  robustness  to  failing  links.  A  metric  is  a  quantitative 
measure  of  a  specif ic  quality  of  a  SUT.  Metrics  are  determined  according  to  a  specif ic 
methodology.  
 
Although  benchmarking,  in  its  strictest  sense,  is  limited  to  measuring  performance,  several 
additional  aspects  are  important  to  make  benchmarking  a  meaningful  research  activity. 
Therefore, the following aspects are to be considered when def ining benchmarks;   
 
Comparability  should be a  fundamental  property  of  any  benchmark;  comparability means  that 
two  independently executed benchmarks  can  be meaningfully  compared  to  each other. One  o f 
the factors  inf luencing the comparability  is  repeatability:  running an  identical benchmark on an 
identical  solution  at  different  moments  in  time  should  result  in  a  (close  to)  identical  result. 
Furthermore,  well-def ined  experimentation  methodologies  are  a  key  factor  in  achieving 
comparability. 
 
Ideally, benchmarking scores should not only be comparable to other scores obtained using the 
same  testbed, but also with scores obtained from different  testbeds with similar capabilities but 
potentially  running  different  operating  systems,  or  based  on  different  types  of  hardware.  The 
success of a specif ic benchmark may very well depend on whether this  interoperability aspect 
is satisf ied or not.  
 
Test  infrastructures  may  be  equipped  with  benchmarking  functionality.   In  this  case,  the 
configurability  of  the  testbed  environment  is  crucial:  in  wired  networks,  a  benchmark  may 
require  a  specif ic  topology  and  links of  specif ic quality;  in wireless networks,  a  trace  containing 
reference background  traffic may need to be pla yed back during  the execution of a benchmark.  
If  such  benchmarking  scenarios  are  considered,  obviously,  the  testbed  needs  to  support  these 
conf iguration settings. 

A benchmarking framework 

Def ining benchmarks  is one challenge; executing a benchmark  in an e xperimenter-friendly way, 
sharing  benchmarks  with  the  research  community,  and/or  building  a  sustainable  framework  is 
equally  important. Figure 1 presents a conceptual drawing that captures the building blocks of a 
benchmarking framework that can be implemented on top of testbed infrastructures. 
 

Figure 1: high level overview of a benchmarking framework 

 

 
The  building  blocks  below  the  dash -dotted  line  indicate  testbed  specif ic  building  blocks,  while 
the  building  blocks  above  this  line  are  of  generic  nature.   As  indicated  in  the  previous  section, 
the benchmarking scenario contains the conf iguration of the experiment, the metrics that will be 
determined, and the way for determining the benchmarking score.   
 
Every  testbed  implementing  the  benchmark  will  then  imp lement  the  benchmark  scenario  in  a 
testbed-specif ic  way  via  the  testbed  control  functionality.  The  testbed  control  functionality  is 
responsible  for  the  conf iguration  of  the  SUT,  but  also  -mainly  in  wireless  networks-  for  the 
conf iguration  of  the  environmen t.  As  an  example,  it  is  possible  that  an  experimenter  wants  to 
benchmark  the  reliability  of  a  wireless  system  in  terms  of  end -to-end  packet  loss,  with  a 
predef ined  number  of  wireless  nodes  generating  interference  by  sending  predef ined  packet 
sequences.  These  interfering  nodes  are  considered  to  be  part  of  the  wireless  environment 
rather than the SUT, since they are not the subject of the performance evaluation.  
 
The  monitoring  block  is  responsible  for  gathering  raw  data  while  the  experiment  is  running; 
monitoring  functions may  collect  data  from  the  SUT  as well  as  from  the  environment. Different 
testbeds may collect data  in different ways, be  it by storing values  in databases, or for example 

in comma separated value (CSV) log f iles. Note that in this representation, the testbed control is 
responsible for conf iguring the monitoring functions that are active during an experiment.  
 

 

Figure 2: determining a protocol reliability score based on data   
collected from the environment and the SUT 

 
In  a  next  step,  the  raw data  is  converted  according  to  the  specif ications of  the metrics  that are 
unambiguously  def ined  by  the  benchmark.  Thus,  while  the  raw  monitoring  data  may  still  be 
testbed dependent, the metric(s) are not.  
 
Figure  2  illustrates  the  relation  between  data  and  metrics;  Raw  data  can  be  collected  from  an 
experimental set-up  in different ways:  (1) dedicated solutions may be added  in  the environment 
of  the  SUT,  to  collect  performance  data.  For  example,  during  an  experiment  involving wireless 
networks,  a  spectrum  analyser  may  be  used  to  collect  power  spectral  density  (PSD)  values  in 
certain  parts  of  the  frequency  spectrum  to  characterize  the  occupation  of  the  wireless 
environment.  In  this  example,  the  measured  PSD  values  do  not  necessarily  characterize  the 
SUT  itself,  but  may  be  of  interest  to  the  experimenter  to  explain  anomalies  in  test  results, 
caused by external  interferers such as microwave ovens.  (2) Certain characteristics of the SUT 
may  be  measured  without  interfering  with  the  operation  of  the  SUT  itself.  F or  example,  a 
wireless  sniffer  or  Ethernet  packet  sniffer  may  be  used  to  capture  packets  sent  by  a  SUT.  (3) 
Other  types  of  data  characterizing  the  SUT  may  only  be  available  when  an  experimenter  has 
access to  the  internal operation of a SUT. For example, bu ffer overf low statistics of a particular 
node in a network can only be accessed by directly interfacing with that node itself.   
 
In  a  next  step,  from  this  raw  data,  no matter what  the  source  or  exact  type of  it  and  no matter 
how  it  is  stored  in  a  particula r  testbed,  unambiguous  metrics  are  derived.  Providing 

unambiguous metrics  is only possible by providing clear and agreed upon def initions of how the 
metrics  are  determined  from  the  raw  data,  and  by  ensuring  that  the  raw  data  itself  was 
measured according to standardized methodologies. An example metric  is end -to-end  link  layer 
packet  loss between  two  nodes  during  an experiment, which  is  def ined  as  the  difference  in  the 
number  of  link  layer  packets  sent  by  the  sending  node  to  a  receiving  node  and  the  number  of 
these  packets  that  are  received  at  the  link  layer  by  the  receiving  node,  relative  to  the  total 
number  of  link  layer  packets  sent.  In  this  case,  the  raw  data  could  be  link  layer  packet  traces, 
and the metric is a single number. 
 
The  metrics  may  be  used  d irectly  by  an  experimenter  to  evaluate  the  performance  of  the 
solution. However,  in the proposed benchmarking approach, an additional  level of abstraction  is 
introduced:  one  or  multiple  metrics  can  be  combined  to  form  one  or  multiple  benchmarking 
scores.  Each  benchmarking  score  may  shed  light  on  the  performance  of  the  SUT  related  to  a 
specif ic criterion. Example criteria are energy efficiency or reliability. In the example of Figure 2, 
a  reliability  benchmarking  score  is  determined  based  on  (wireless)  packe t  loss  and  wireless 
medium  availability  before,  during  and  after  an  experiment.  An  example  outcome  could  be  a 
“high”  or  ”good”  reliability  benchmarking  score  in  case  packet  loss  is  close  to  zero  when  the 
relevant  part  of  the  RF  spectrum  is  already  heavily  loaded.  In  a  next  experiment,  the  reliability 
score  could  be  relatively  lower,  even  when  fewer  packets  are  lost,  because  the  wireless 
spectrum is almost unoccupied at the moment that the experiment takes place.   
 
Obviously,  determining  how  to  combine  metrics  into  a  particular  benchmarking  score  is  a  non -
trivial  task.  However,  it  is  worth  the  effort  as  by  converting  a  set  of  metrics  to  a  benchmarking 
score, additional performance analysis features are enabled;  
 
Firstly,  when  scheduling  a  single  experiment  (i .e.  one  f ixed  set  of  conf iguration  parameters  for 
the  SUT),  benchmark  scores  provide  no  additional  information  compared  to  the  performance 
metrics  -  in  fact  the  benchmarking  score  hides  performance  details.  However,  a  thorough 
performance  analysis may  requ ire  tens,  hundreds  or  even  thousands  of  tests  to be  scheduled. 
In  this  case,  studying  the  individual  metrics  is  a  lot  of  work.  If  a  benchmarking  score  is  then 
def ined  in  such  way  that  it  combines  all  relevant  metrics  for  the  criteria  under  evaluation,  the 
benchmark  scores  greatly  help  to  quickly  classify  the  experiments  relative  to  each  other. 
Provided 
the 
requirements  are  satisf ied, 
interoperability 
the  comparability  and 
that 
benchmarking  scores  can  then  be  used  to  quickly  and  fairly  compare  the  performanc e  of 
different  solutions,  or  of  subsequent  developments  of  a  single  solution.  Moreover,  the 
performance of a single solution in varying environments can be determined and compared.  
 
Secondly,  under  the  assumption  that  meaningful  benchmarking  scores  can  be  def ined, 
performance  comparison  of  different  solutions  may  by  possible  by  non -experts,  since  no 
detailed  understanding  is  required  of  the  specif ic  metrics.  Furthermore,  the  benchmarking 
scores  can  be  used  as  a  way  to  reconf igure  experiments,  in  order  to  au tomate  performance 
evaluation.  This  is  indicated  by  the  dashed  arrows  in  Figure  1:  Based  on  the  benchmarking 
score, a computer program can detect  the  influence of a particular experiment conf iguration on 
the  performance.  The  program  could  then  propose  a  new  conf iguration  (for  example,  run  the 

same experiment again, but with a single parameter such as  “beacon  interval” set  to a different 
value),  re-evaluate  the  score,  and  auto -optimize  a  particular  SUT.  In  those  zones  with  a  large 
benchmark  score  variation,  a  f iner  grained  tuning  of  the  conf iguration  parameters  could  be 
scheduled.  Eventually,  a  user  could  def ine  ranges  for  different  parameters  of  the  SUT  to  be 
varied,  and  then  let  the  benchmarking  framework  and  testbed  take  care  of  intelligently 
scheduling additional tests, f inally presenting a user with an easy -to-understand overview of the 
performance of the SUT. 
 

Bonfire Case: Cloud Benchmarks 

Context 

Today,  different  Infrastructure -as-a-Service  (IaaS)  providers  describe  their  infrastructure 
offerings  in  different  ways  and  don’t  necessarily  provide  very  much  information,  if  at  all,  about 
the  infrastructure  being  offered.    For  instance,  Amazon  EC2  describes  (and  prices)  their 
infrastructure  in  terms  of  Amazon  EC2  Compute  Units  (ECU).    A  machine  providing  the 
capability  of  one  ECU  is  said  to  be  equivalent  to  a  1.0 -1.2  GHz  2007  Opteron  or  2007  Xeon 
processor.    Given  the  limited  and  heterogeneous  information  provided  by  IaaS  providers,  how 
can  anyone  know  what  resources  they  will  need  to  execute  their  application   with  a  particular 
quality  of  service  (QoS)?    If  the  application  is  already  adapted  for  the  IaaS  provider’s  system 
then  it may  be possible  to  just  try  the  application out  and measure  its  performance,  scaling  the 
deployment  as  required.    But  what  if  the  app lication  is  not  yet  adapted,  or  what  if  you  want  to 
choose between several IaaS providers? 
 
Providing  tools  to  help  choose  the  appropriate  IaaS  provider  and  predict  application 
performance  lies  within  the  realm  of  the  Platform -as-a-Service  (PaaS)  provider  as  part  of  the 
wider  role  of  helping  the  application  provider  develop,  deploy and manage  their  application.  To 
create Software-as-a-Service (SaaS) either an existing application requires adaptation or a new 
application  is  created.  In  either  case,  PaaS  tools  help  in  the  process  of  creating  software  to 
execute  on  an  IaaS  provider  (see  Figure  1  below).    The  major  PaaS  providers  today  (Google 
App  Engine  and  Microsoft  Azure)  take  the  role  of  IaaS  provider  as  well,  but  this  needn’t 
necessarily  be  the  case.    In  the  future  the  IaaS  market  may  be  more  developed  and  we  will 
need  a  standard  way  to  compare  providers.    This  is  one  case  where  benchmarks  and 
application models can help (see below). 
 

 
Figure 3: Application modelling is a service in the Platform as a Service layer. 

 
The  BonFIRE  project  has  an  embedded  experiment  researching  how  to  measure  the 
performance  of  the  virtual  machines  deployed  by  IaaS  providers.    Rather  than  just  executing 
standard  benchmark  tests  on  the  virtual  infrastructure,  the  experiment  is  wo rking  at  a  higher 
level  of  abstraction  by  using  so  called  “dwarf”  benchmarks  -  an  approach  f irst  introduced  by 
Colella  in  2004  [1],  which  has  been  further  developed  at  UC  Berkeley  [2].    Dwarf  benchmarks 
are  intended  to  capture  known  computational  patterns  and  may  map  more  directly  onto 
application performance. 

Application Modelling 

This  paper  has  discussed  a  general  framework  for  collecting  performance  data  and  has 
described  the  complexity  and  advantages  of  converting  this  performance  data  into  simpler 
benchmark  scores.    However,  that  is  not  the  end  of  the  story:  How  can  benchmark  scores  be 
used to make decisions? 
 
To  give  a  simple  example:  If  you  had  a  computer  application  which  purely  did  f loating  point 
operations  (no  IO,  no  network)  and  you  had  to  choose  which  computer  to  buy  to  run  your 
application  then  you  could  look  up  the  SPECfp  scores  [3]  for  the  CPUs  in  the  candidate 
computers and buy the fastest computer (in this limited sense) for your budget.  
 
This  simple  case  is  unrealistic.    Applications  do  not  map  onto  benchmark  scores  in  such  a 
straightforward  way.    Given  an  application  and  a  hardware  conf iguration  you  need  to 

understand  a  certain  amount of  detail  of  your application’s  behaviour  to know what  benchmark 
or benchmarks are most appropriate to use in your analysis. 
 
To  predict application  performance,  an application model  is  required  that  can  relate application 
workload, hardware characteristics and application performance.  A description of the hardware 
in  terms  of  Amazon  ECUs  or  similar  opaque  terms  from  other  providers  is  not  suff icient  to 
produce a  predictive model.   Would  it  help  to  know  in  detail  the  hardware  being offered  by  the 
IaaS provider?  Possibly, but IaaS providers are not generally going to give their customers this 
information as it is conf idential.   
 
The  IRMOS  project  [4]  has  constructed  some  performance  models  of  applications  taking  into 
account  the  detail  of  the  hardware,  and  whilst  they  can  be  predictive,  these  models  are  time -
consuming  to  create  and  require  expert  knowledge.    Ins tead  we  need  a  middle  ground.    The 
BonFIRE experiment previously mentioned  is researching whether virtual hardware descriptions 
couched  in  terms of dwarf benchmark scores can make application models simpler and yet still 
suff iciently predictive to help in a variety of ways: 
 

●  Making better provisioning decisions: Deploying the  infrastructure resources required for 
a given application QoS rather than over-provisioning. 
●  Making  better  application  scheduling  decisions:  Knowing  the  application  runtime  with  a 
good reliability permits more intelligent scheduling.  
●  Determining  the  optimal  application  conf iguration:  The  performance  of  complex 
applications  and  business  or 
industrial  data  processing  workf lows  with  many 
components  can  be  greatly  affected  by  their  conf iguration  such  as  buffer  sizes  and 
number of threads. 
●  Tracking  uncertainty  in  business  processes:  Many  processes  are  non -deterministic, 
predicting the likelihood of completing tasks allows for the management of risk.  

 
Application  modelling  is  a  broad  f ield  covering  many  modelling  techniques  including  discrete 
event simulations, Bayesian belief networks, artif icial neural networks and f inite state machines.  
All these models are underpinned by the description of the system that the application is running 
on, making benchmarking an important asset today for business and economic reasons.  

CREW Case: Benchmarking Cognitive Networks 

Context 

During the past decades, an explosive emergence of wireless communication  technologies and 
standards  could  be  witnessed.  Today,  many  people  use  multiple  wireless  communication 
devices  on  a  daily  basis,  and  expect  an  ever  increasing  wireless  bandwidth.    In  addition, 
wireless communication is increasingly being used for machine -to-machine communication. The 
increasing number of wireless application and wireless devices goes hand  in hand with  a  rising 
number  of  devices  coexisting  in  the  same  environment,  which  compete  for  the  same  (scarce) 
spectral  resources.  Both  in  licensed  and  unlicensed  radio  frequency  bands,  the  coexistence 

issue  gets  increasingly  problematic.  Researchers  investigating  Cognitive  Radio  and  Cognitive 
Networking  protocols  and  algorithms,  seek  to  resolve  the  coexistence  problems  by  optimizing 
the use of the wireless spectrum, either by  improving coordination between d evices operating  in 
the  same  frequency  band,  or  by  attempting  to  opportunistically  use  spectrum  in  other 
underutilized (licensed) bands. 
While a  large part of the  research on wireless networks  is still based on  theoretical models and 
simulation  results,  there  is  also  an  increasing  awareness  in  the  wireless  research  community 
that  experimentally-supported  research  may  help  to  identify  issues  that  cannot  be  discovered 
through  theory  or  simulation  only.  This  observation  is  ref lected  in  the  topics  of  internatio nal 
conferences  that  increasingly  welcome  experimentally -driven  research  and  the  interest  of  the 
European Commission in experimental facilities for future Internet research.  
 
The main target of the CREW  (Cognitive Radio Experimentation World) project is to  establish an 
open federated test platform, which facilitates such experimentally -driven research in the f ield of 
advanced  spectrum  sensing,  cognitive  radio  and  cognitive  networking  strategies.  The  CREW  
platform  is  based  on  four  existing  wireless  testbeds,   augmented  with  state-of-the-art  cognitive 
spectrum  sensing  platforms.  The 
individual 
testbeds  are  based  on  different  wireless 
technologies  and  include  heterogeneous  ISM,  heterogeneous  licensed,  cellular,  and  wireless 
sensor components. 
 
If  the CREW  platform  is  to become an established  infrastructure where  researchers are able  to 
evaluate  their  cognitive  solutions  in  an  eff icient  and  reliable  way,  providing  a  reproducible  test 
environment  is  essential.  W ithin  the  CREW   work  package  on  benchmarking,  the  goal  is  to 
design a benchmarking  framework  that will  facilitate  such a  reproducible  test environment.  The 
platform  will  offer  reliable  performance  indicators  for  cognitive  radio  solutions  and  will  enable 
researchers to compare their solutions against existing solutions. 

Example benchmark for cognitive networks  

One of  the  problems with experimental  validation  in wireless  networks  in  general,  is  that  every 
environment  and  every  device  has  its  own  characteristics,  making  it  diff icult  to  repeat 
experiments  or  compare  results  with  results  obtained  by  other  researchers.  Moreover,  if 
different  research  teams  measure  the  quality  of  their  solutions  in  different  set -ups  (topology, 
background traffic, sources of interference), using different metrics or determine those metr ics in 
a  different  way,  the  signif icance  of  experimental  results  may  be  low.  Comparing  the 
performance of wireless networking protocols  in general or cognitive protocols more specif ically 
is  thus  a  challenging  task.  Therefore,  creating  a  reproducible  test  environment  for  wireless 
networking  is the primary goal of the benchmarking framework  in the CREW  project. One of the 
major CREW  targets  is to characterize the wireless environment and to analyse  its  inf luence on 
the  SUT.  By  using  benchmarking,  a  strict  experimentation  methodology  is  enforced  that  will 
increase  the  relevance  of  experimental  results,  while  also  speeding  up  the  experimentation 
process. 
 
As an example, one of the planned CREW  use -cases evaluates a Zigbee-based sensor network 
(e.g.  used  for  temperature monitoring)  in  an emulated  home  environment which  also houses  a 

W i-Fi access point and multiple W i-Fi clients. The plan  is to emulate a home environment  inside 
an  interference-free  testbed  building,  by  configuring  a  set  of  W i -Fi  based  network  nodes  to 
transmit  specif ic  packet  traces,  which  are  modelled  according  to  measured  packet  traces  in  a 
real home environment.   In this emulated home environment,  the Zigbee -based sensor network 
is  evaluated  using a  reliability  benchmarking  score ;  a  f irst  time  as a  “standard”  sensor  network 
solution  (reference  evaluation),  and  next  after  augmenting  the  sensor  network  with  cognitive 
interference-avoidance  protocols  (novel  cognitive  solution  -  SUT).    In  this  cognitive  case,  the 
sensor  devices  use  their  built-in  RF  scanning  functionality  to  determine  the  occupation  degree 
of the different channels in the ISM band, and adjust their transmitting and receiving channels to 
optimize the coexistence with the W i-Fi network. 
 
During  the  experiment,  advanced  external  sensing  engines  are  used  to  collect  spectral 
measurements spatially and  temporally. These spectral measurements are used  to monitor  the 
characteristics  of  the  wireless  networks  under  evaluation  and  to  detect  potential  unwanted 
interferers  that  are  not  part  of  the  experiment.  The  benchmarking  framework monitors whether 
the primary metrics are signif icantly  inf luenced by external  interference.  If so,  the experiment  is 
not  relevant  and  is  therefore  postponed  or  cancelled.  During  the  benchmark  both  SUT  and 
environment are evaluated using primary and secondary metrics. The benchmarking framework 
is  further  responsible for emulating  the environment by  replaying pre -modelled  traces, so  that  it 
approximates the home environment as specif ied in the benchmark def inition  
 
The  used  benchmarking  system  for  this  use  case  maps  entirely  on  the  framework  given  in 
Figure 1: 
 

●  Benchmark  scenario:  a  full  description  of  the  Zigbee  nodes  behaviour  in  the  home 
environment,  together with  the home environment model  (traff ic models for W iFi ac cess 
point and clients). It also contains the reference benchmark score and used primary and 
secondary metrics to evaluate the scenario. 
●  Experiment config:  is responsible for executing the benchmark on the Zigbee nodes and 
conf iguring  the  W i-Fi  nodes  (W iFi  traces)  so  that  they  emulate  the  wireless  home 
environment interference. 
●  Metrics:  evaluate  the  Packet  Error  Rates  (PER)  and  throughput  on  application  level 
between  the  Zigbee  nodes  as  the  primary  metrics.  Spectrum  information  from  the 
specialised  sensing  devices  is  gathered  as  the  secondary metric.  The  feedback  loop  to 
the experiment conf iguration  is used to evaluate the validity of the experiment results by 
comparing the measured environment to the modelled environment.  
●  Benchmark  score:  a  single  score  distilled  by  inverting  the  PER  and  comparing  the 
achieved throughput with the throughput of the reference benchmark.  

 
The  use  of  a  benchmarking  framework  to  realize  the  above  plans  leads  to  several  interesting 
research  tracks,  to  be  tackled  in  the  CREW   projec t,  including:  (i)  how  to  determine  and  create 
realistic and useful  reference scenarios? This  research question  is not only  to be solved for  the 
case  of  experimenting  with  cognitive  networks  in  the  ISM  bands,  emulating  home  and  off ice 
environments, but also  for a wider  range of cognitive  radio cases  in other  (licensed) bands and 

in  other wireless  scenarios.  (ii) How will  reproducibility  exactly  be def ined?   When working with 
wireless  networks  and  experimenting  over  the  air,  100%  exact  repeatability  is  not  fe asible  in 
every  wireless  testbed  environment  because  of  external  interference.  As  a  result,  the 
“reproducibility”  concept  may  have  to  be  redef ined,  by  setting  acceptable  margins  in  which 
results  may  vary.  Even  if  perfect  reproducibility  is  not  realistic  in   a  wireless  experimental 
environment,  it  is  very  important  to  be  able  to  characterize  the  wireless  environment  using 
advanced  sensing  techniques,  and  to  analyse  the  impact  of  external  interferers  on  the 
performance of  the SUT  (iii) Common data formats are  to be defined to guarantee compatibility 
of  benchmarks  across  multiple  testing  environments.  Both  input  formats  for  a  full  experiment 
description as  output  formats  for  data gathered  from  experiments  (e.g.  spectrum  sensing  data) 
are  to  be  def ined.  (iv)  Def inition  of  metrics  and  determining  the  methodologies  to  acquire  the 
measurements.  
 
 

OneLab2 Case: Benchmarking in Wireless Networks 

Context 

Benchmarking  in  wireless  networks  is  of  great  interest  to  the  networking  research  community 
because  it  enables  fair  comparison  and  can  lead  to  peer-verif iable  research.  Fair  comparison 
means  systems  are  evaluated  under  reference  conditions.  However,  it  is  non -trivial  to  ensure 
same  spatial  and  temporal  conditions.  This  is  because  wireless  channels  are  unpredictable 
(random), error-prone and could vary over very short time scale (order of microseconds). Also  it 
is  impossible  to  ensure  innocuous  co -existence  of  collocated  wireless  networks.  The  lack  of 
control  over  some aspects  of  the wireless experimentation has been  a ma jor hurdle  in  the way 
of  comparable  performance  analysis.  However,  some  of  the  network  parameters  can  be 
conf igured  to  f ixed  values.  We  consider  all  the  conf igurable  network  parameters  to  be 
controllable  and  the  rest  uncontrollable.  Uncontrollable  paramete rs  include  station  workload 
(memory/cpu usage, etc.), network traff ic load, multipath fading, path loss (attenuation), channel 
interference,  etc.  Controllable  parameters  entail  scenario  conf igurations  (topology,  traff ic, 
wireless  card  conf igurations).  Cont rollable  parameters  can  also  include  meta -data  such  as 
system hardware/software specif ications. 
   
Simulation  has  been  the  pre -dominant  technique  for  performance  evaluation  of  networks.  It 
provisions  full  control  over  the  evaluation  process  with  a  certain  degree  of  repeatability. 
However,  the f idelity of underlying simplistic wireless propagation models  is not up  to  the mark. 
Emulation 
improves 
the  realism  by  combining  aspects  of  real -world  such  as  real 
hardware/software  and  simulations  such  as  control  and  repeatability.  However,  emulation 
employs  virtual/wired  links  instead  of  real  wireless  links  and  important  channel  characteristics 
such as path loss, packet errors, etc., are controlled and, therefore, not real.    
 
Experiments  in the  real environment are the only way  to provide wireless f idelity but complicate 
apples to apples comparison because channel conditions are highly volatile. Radio propagation 

characteristics  such  as  path  loss  and  multipath  fading  are  highly  dependent  on  the  physical 
environment,  geometric  relationship  between  the  objects,  dielectric  properties  of  the  objects, 
movements  of  nodes/objects  and  the  type  and  orientation  of  antennas  used.    Furthermore, 
interference  from  wireless-enabled  devices  operating  in  the  2.4  GHz  band  such  as  IPhone s, 
IPads,  game  controllers,  microwaves,  cordless  phones,  etc.,  and  exogenous W iFi  networks  is 
unavoidable. W ireless  technologies  are also evolving  rapidly. Diversity enabled MIMO wireless 
cards and automatic power/rate adaptation schemes make  it hard to st udy the  impact of certain 
parameter(s)  in  isolation.    In  addition,  commodity  hardware  and open -source wireless  software 
such  as  drivers  and  sniffers  may  exhibit  anomalous  behavior  and  therefore  require  sanity 
checks and calibration. 
  
Some  of  the  pitfalls  encountered  in  wireless  experimentation  are  outlined  here.  These  pitfalls 
also  provide  an  insight  into  the  challenges  in  the  way  of  benchmarking  in  wireless  networks. 
Channel  characterization  is  a  crucial  f irst  step  but  characteristics  such  as  multipath  fa ding  are 
tightly  coupled  to  the  location,  orientation  of  communication  pairs  and  movement  of  objects  in 
the  environment.  It  is  hard  to  ensure  the  desirable  accuracy  until  and  unless  a  machine -
controlled mechanism is provisioned for the accurate positioning  of nodes. One way to judge the 
similarity  of  two  set  ups  in  terms  of  distance  and  location,  is  to  use  path  loss  and  Ricean  K 
factor.    But  these  estimates  are  inf luenced  by  noise  f loor  calibrations  and  antenna  diversity.  
They are easy  to  ignore by an unsuspecting experimenter. Empirical estimation of K  factor and 
path loss can also be inaccurate if packet loss at the sniffers is not calibrated.   
 
In  order  to  overcome  these  challenges,  we  require  a  measurement  (experimentation) 
infrastructure  (tested),  experiment  control  tools,  data  management  and  analysis  tools  (as 
envisaged  in Figure 1) aided by a comprehensive benchmarking methodology.  In  OneLab2 [5], 
we  worked  on  realizing  fair  comparison  of  networking  protocols.  As  it  is  not  feasible  to  repeat 
experiments, our methodology,  testbed  and  tools  facilitate  running  of  large  number  of  runs and 
then  clustering  of  runs  based  on  similarity  of  conditions  (both  controllable  and  uncontrollable 
parameters).  We can, then, compare the performance for runs which belong  to the same group 
in subsequent experimentation campaigns. 

Wireless Channel Characterization 

In  wireless  networks,  channel  characterization  is  a  f irst  natural  step  towards  benchmarking.  It    
enables  researchers  to  investigate  the  inf luence  of  environment   on  wireless  network 
performance  and  allows  them  to  understand  how  well  a  protocol  or  an  application  performs  in 
different wireless environments.  
A mapping of our benchmarking system to the framework proposed  in Figure 1 is as follows: 
  Benchmark  Scenario:  It  consists  of  a  description  of  controllable  and  non -controllable 
parameters of both stations and  the environment. This  includes workload specif ications, 
topology, and spectrum analyzer and sniffer settings.   
  Experiment  Conf iguration:  It  consists  of  scheduling  and  execution  tools.  The  tools 
control  the workf low of each experiment  run and make  it easier  to  repeat an experiment 
arbitrary number of times.  

  Metrics: Evaluate  the Packet Error Rate  (PER) and Packet Loss Ratio at the MAC  layer 
as  the  primary  metrics.  Spectrum  information  from  the  spectrum  analyzer,  Ricean  K 
factor and SNR, Bit Error Rate (BER) as the secondary metrics.  
  Benchmark  Score:      We  use  the  packet  loss  as  an  indicator  of  performance  and 
compare  it  with  the  packet  loss  of  the  reference  benchmark.  Packet  error  gives 
additional information about packet loss incurred because of CRC errors.  
The  above  practice  is  well-supported  by  our  experimentation  methodology  which  has  been 
tailored  to  the  requirements  of  benchmarking  in wireless  networks.    The  methodology  enables 
us  to  repeat  an  experiment  fairly  large  number  of  times.  In  future ,  it  might  also  be  possible  to 
repeat  an  experiment  until  the  desired  level  of  conf idence  is  achieved  for  a  given  conf idence 
interval.  To  enable  comparability,  we  propose  to  cluster  runs  based  on  experiment  conditions.  
Exploratory  tests  can  be  performed  to  investigate  anomalies  and  perform  calibrations.  Basic 
sanity  checks  can  be  performed  to  verify  accuracy  of  software/hardware  tools.  For  example, 
performance  of packet  generator/sniffer, antenna  diversity,  time  synchronization,  etc.  Empirical 
estimates of the metrics such as Ricean K factor are verif ied against the theoretical models. 
 
Figure 4 demonstrates  the variations  in Ricean K factor  caused by human  traff ic and  its  impact 
on the percentage packet loss. Test case 1 corresponds to the case  when there is human traffic 
in  the  vicinity.  On  the  contrary,  there  is  no  human  traff ic  during  Test  case  2.  Ricean  K  factor 
def ines one  the uncontrollable  characteristics of  the wireless  channel. By  clustering, we will  be 
able  to  group  experiments  into  clusters  according  to  criteria  such  as  high,  medium  and  low 
human  traff ic. This will allow us  to perform apples  to apples comparison with future experiment 
campaigns.  This  is  especially  useful  for  a  wireless  experimentation  campaign  conducted  over 
an extended period of time.  
 

Figure 4: Ricean K factor and corresponding percentage packet loss as measured during two 
test cases. There is human traff ic during test case 1 and no human traff ic during test case 2.  

 

Conclusions 

While the benchmarking concept  itself  is not new, new challenges  in research domains such as 
cloud  computing  or  cognitive  networking  have  renewed  the  interest  of  the  research  community 
in  benchmarking,  and multiple  European  FIRE  research projects  are  putting  effort  in  designing 
benchmarking  solutions,  or  in  closely  related  research  topics  such  as  measurements, 
performance evaluation, monitoring, or methodology design.  
 
This  paper  clarif ies  what  is  meant  by  benchmarking  computers  and  computer  networks  in  the 
scope of some of these FIRE projects and  indicates several benchmarking challenges still to be 
tackled  using  the  examples  of  application  modelling  and  cognitive  radio  benchmarks.  It  is  not 
the ambition of this wh itepaper to present a detailed survey of benchmarking or to fully detail the 
different  benchmarking  approaches  followed  in  these  projects.  However,  we  hope  that  the 
presented  terminology  and  high -level  architecture  can  be  a  base  for  further  discussions  and  
may help to identify similarities and differences between the research done in different projects.   
 

Acknowledgements 

The  research  leading  to  this  white  paper  has  received  funding  from  the  European  Union's 
Seventh  Framework  Programme  (FP7/2007 -2013)  under  grant  agreements  n°258301  (CREW  
project), n°257386 (BonFIRE project) and n°224263 (OneLab2 project). 

References 

[1] P. Colella, Defining Sof tware Requirements for Scientific Computing, 2004. 
[2] K. Asanov ic et al., "The Landscape of  Parallel Computing Research: A View from Berkeley," Electrical 
Engineering and Computer Sciences, University of California at Berkeley, 2006.  
K. Asanov ic et al., "A View of the Parallel Computing Landscape," Communications of the ACM, vol. 52, 
no. 10, pp. 56-67, 2009. 
[3]  SPEC. (2011) Standard Performance Evaluation Corporation. [Online]. 
http://www.spec.org/index.html  
[4] EC FP7-ICT IRMOS Project. [Online]. http://www.irmosproject.eu/ 
[5] EC FP7-WP9 Benchmarking. [Online]. http://www.onelab.eu/index.php 
  
 

