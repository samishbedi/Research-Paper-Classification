260

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 13, NO. 3, MARCH 2002

Performance-Effective and Low-Complexity
Task Scheduling for Heterogeneous Computing

Ha luk Topcuog lu, Member, IEEE, Sa l im Har iri, Member, IEEE Computer Soc iety, and
Min-You Wu, Sen ior Member, IEEE

AbstractÐEfficient application scheduling is critical for achieving high performance in heterogeneous computing environments. The
application scheduling problem has been shown to be NP-complete in general cases as well as in several restricted cases. Because of
its key importance, this problem has been extensively studied and various algorithms have been proposed in the literature which are
mainly for systems with homogeneous processors. Although there are a few algorithms in the literature for heterogeneous processors,
they usually require significantly high scheduling costs and they may not deliver good quality schedules with lower costs. In this paper,
we present two novel scheduling algorithms for a bounded number of heterogeneous processors with an objective to simultaneously
meet high performance and fast scheduling time, which are called the Heterogeneous Earliest-Finish-Time (HEFT) algorithm
and the Critical-Path-on-a-Processor (CPOP) algorithm. The HEFT algorithm selects the task with the highest upward rank
value at each step and assigns the selected task to the processor, which minimizes its earliest finish time with an insertion-based
approach. On the other hand, the CPOP algorithm uses the summation of upward and downward rank values for prioritizing
tasks. Another difference is in the processor selection phase, which schedules the critical tasks onto the processor that minimizes
the total execution time of the critical tasks. In order to provide a robust and unbiased comparison with the related work, a
parametric graph generator was designed to generate weighted directed acyclic graphs with various characteristics. The
comparison study, based on both randomly generated graphs and the graphs of some real applications, shows that our
scheduling algorithms significantly surpass previous approaches in terms of both quality and cost of schedules, which are mainly
presented with schedule length ratio, speedup, frequency of best results, and average scheduling time metrics.

Index TermsÐDAG scheduling, task graphs, heterogeneous systems, list scheduling, mapping.

æ

1 INTRODUCTION
D IVERSE sets of resources interconnected with a high-
speed network provide a new computing platform,
called the heterogeneous computing system, which can
support executing computationally intensive parallel and
distributed applications. A heterogeneous computing
system requires compile-time and runtime support for
executing applications. The efficient scheduling of
the
tasks of an application on the available resources is one of
the key factors for achieving high performance.
The general
task scheduling problem includes the
problem of assigning the tasks of an application to suitable
processors and the problem of ordering task executions on
each resource. When the characteristics of an application
which includes execution times of tasks, the data size of
communication between tasks, and task dependencies are
known a priori, it is represented with a static model.
In the general form of a static task scheduling problem,
an application represented by a directed acyclic graph

. H. Topcuoglu is with the Computer Engineering Department, Marmara
University, Goztepe Kampusu, 81040, Istanbul, Turkey.
E-mail: haluk@eng.marmara.edu.tr.
. S. Hariri is with the Department of Electrical and Computer Engineering,
University of Arizona, Tucson, AZ 85721-0104.
E-mail: hariri@ece.arizona.edu.
. M.-Y. Wu is with the Department of Electrical and Computer Engineering,
University of New Mexico, Albuquerque, NM 87131-1356.
E-mail: wu@eece.unm.edu.

Manuscript received 28 Aug. 2000; revised 12 July 2001; accepted 6 Sept.
2001.
For information on obtaining reprints of this article, please send e-mail to:
tpds@computer.org, and reference IEEECS Log Number 112783.

(DAG) in which nodes represent application tasks and
edges represent intertask data dependencies. Each node
label shows computation cost (expected computation time)
of the task and each edge label shows intertask commu-
nication cost
(expected communication time) between
tasks. The objective function of this problem is to map
tasks onto processors and order their executions so that
task-precedence requirements are satisfied and a mini-
mum overall completion time is obtained. The task
scheduling problem is NP-complete in the general case
[1], as well as some restricted cases [2], such as scheduling
tasks with one or two time units to two processors and
scheduling unit-time tasks to an arbitrary number of
processors.
Because of its key importance on performance, the task
scheduling problem in general has been extensively studied
and various heuristics were proposed in the literature [3],
[4], [5], [6], [7], [8], [9], [10], [11], [13], [12], [16], [17], [18],
[20], [22], [23], [27], [30]. These heuristics are classified into a
variety of categories (such as list-scheduling algorithms,
clustering algorithms, duplication-based algorithm, guided
random search methods) and they are mainly for systems
with homogeneous processors.
In a list scheduling algorithm [3], [4], [6], [7], [18], [22], an
ordered list of tasks is constructed by assigning priority for
each task. Tasks are selected in the order of their priorities
and each selected task is scheduled to a processor which
minimizes a predefined cost function. The algorithms in
this category provide good quality of schedules and their
performance is comparable with the other categories at a

1045-9219/02/$17.00 ß 2002 IEEE

TOPCUOGLU ET AL.: PERFORMANCE-EFFECTIVE AND LOW-COMPLEXITY TASK SCHEDULING FOR HETEROGENEOUS COMPUTING

261

lower scheduling time [21], [26]. The clustering algorithms
[3], [12], [19], [25] are, in general for an unbounded number
of processors, so they may not be directly applicable. A
clustering algorithm requires a second phase (a scheduling
module)
to merge the task clusters generated by the
algorithm onto a bounded number of processors and to
order the task executions within each processor [24].
Similarly, task duplication-based heuristics are not practical
because of their significantly high time complexity. As an
example, the time complexity of the BTDH Algorithm [30]
and the DSH Algorithm [18] are Ov4 ; the complexity of the
CPFD Algorithm [9] is Oe  v2  for scheduling v tasks
connected with e edges on a set of homogeneous processors.
Genetic Algorithms [5], [8], [11], [13], [17], [31] (GAs) are
of the most widely studied guided random search techni-
ques for the task scheduling problem. Although they
provide good quality of schedules, their execution times
are significantly higher than the other alternatives. It was
shown that the improvement of the GA-based solution to
the second best solution was not more than 10 percent and
the GA-based approach required around a minute to
produce a solution, while the other heuristics required an
execution of a few seconds [31]. Additionally, extensive
tests are required to find optimal values for the set of
control parameters used in GA-based solutions.
The task scheduling problem has also been studied by a
few research groups for the heterogeneous systems [6], [7],
[8], [10], [11], [13], [14]. These algorithms may require
assigning a set of control parameters and some of them
confront with the substantially high scheduling costs [6],
[8], [11], [13]. Some of them partition the tasks in a DAG into
levels such that there will be no dependency between tasks
in the same level [10], [14]. This level-by-level scheduling
technique considers the tasks only in the current level (that
is, a subset of ready tasks) at any time, which may not
perform well because of not considering all ready tasks.
Additionally, the study given in [14] presents a dynamic
remapper that requires an initial schedule of a given DAG
and then improves its performance using three variants of
an algorithm, which is out of the scope of this paper.
In this paper, we propose two new static scheduling
algorithms for a bounded number of
fully connected
heterogeneous processors:
the Heterogeneous Earliest-
Finish-Time (HEFT) algorithm and the Critical-Path-on-a-
Processor (CPOP) algorithm. Although the static-schedul-
ing for heterogeneous systems is offline,
in order to
provide a practical solution,
the scheduling time (or
running time) of an algorithm is the key constraint.
Therefore, the motivation behind these algorithms is to
deliver good-quality of schedules (or outputs with better
scheduling lengths) with lower costs (i.e., lower schedul-
ing times). The HEFT Algorithm selects the task with the
highest upward rank (defined in Section 4.1) at each step.
The selected task is then assigned to the processor which
minimizes its earliest finish time with an insertion-based
approach. The upward rank of a task is the length of the
critical path (i.e., the longest path) from the task to an exit
task,
including the computation cost of
the task. The
CPOP algorithm selects the task with the highest (upward
rank + downward rank) value at each step. The algorithm

targets scheduling of all critical tasks (i.e., tasks on the
critical path of the DAG) onto a single processor, which
minimizes the total execution time of the critical tasks. If
the selected task is noncritical, the processors selection
phase is based on earliest execution time with insertion-
based scheduling, as in the HEFT Algorithm.
this research work, a parametric graph
As part of
generator has been designed to generate weighted directed
acyclic graphs for the performance study of the scheduling
algorithms. The graph generator targets the generation of
many types of DAGs using several input parameters that
provide an unbiased comparison of task-scheduling algo-
rithms. The comparison study in this paper is based on both
randomly generated task graphs and the task graphs of real
applications, including the Gaussian Elimination Algorithm
[3], [28], FFT Algorithm [29], [30], and a molecular dynamic
code given in [19]. The comparison study shows that our
algorithms significantly surpass previous approaches in
terms of both performance metrics (schedule length ratio,
speedup, efficiency, and number of occurrences giving best
results) and a cost metric (scheduling time to deliver an
output schedule).
The remainder of this paper is organized as follows: In
the next section, we define the research problem and the
related terminology. In Section 3, we provide a taxonomy of
task-scheduling algorithms and the related work in
scheduling for heterogeneous systems. Section 4 introduces
our scheduling algorithms (the HEFT and the CPOP
Algorithms). Section 5 presents a comparison study of our
algorithms with the related work, which is based on
randomly generated task graphs and task graphs of
several real applications.
In Section 6, we introduce
several extensions to the HEFT algorithm. The summary
of the research presented and planned future work is
given in Section 7.

2 TASK-SCHEDULING PROBLEM

A scheduling system model consists of an application, a
target computing environment, and a performance criteria
for scheduling. An application is represented by a directed
acyclic graph, G  V ; E , where V is the set of v tasks
and E is the set of e edges between the tasks. (Task and
node terms are interchangeably used in the paper.) Each
edge i; j 2 E represents the precedence constraint such
that task ni should complete its execution before task nj
starts. Data is a v  v matrix of communication data, where
datai;k is the amount of data required to be transmitted from
task ni to task nk .
In a given task graph, a task without any parent is
called an entry task and a task without any child is called
an exit task. Some of the task scheduling algorithms may
require single-entry and single-exit task graphs. If there is
more than one exit (entry) task, they are connected to a
zero-cost pseudo exit (entry) task with zero-cost edges, which
does not affect the schedule.
We assume that
the target computing environment
consists of a set Q of q heterogeneous processors
connected in a fully connected topology in which all
interprocessor communications are assumed to perform
without contention. In our model, it is also assumed that

262

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 13, NO. 3, MARCH 2002

computation can be overlapped with communication.
Additionally, task executions of a given application are
assumed to be nonpreemptive. W is a v  q computation
cost matrix in which each wi;j gives the estimated execution
time to complete task ni on processor pj . Before scheduling,
the tasks are labeled with the average execution costs. The
average execution cost of a task ni is defined as
Xq
j1

wi 

wi;j =q:

1

The data transfer rates between processors are stored in
matrix B of size q  q. The communication startup costs of
processors are given in a q-dimensional vector L. The
the edge i; k, which is for
communication cost of
transferring data from task ni (scheduled on pm ) to task
nk (scheduled on pn ), is defined by
ci;k  Lm  datai;k
Bm;n

2

:

When both ni and nk are scheduled on the same
processor, ci;k becomes zero since we assume that the
intraprocessor communication cost is negligible when it is
compared with the interprocessor communication cost.
Before scheduling, average communication costs are used
to label the edges. The average communication cost of an
edge i; k is defined by
ci;k  L  datai;k
B

3

;

where B is the average transfer rate among the processors
in the domain and L is the average communication
startup time.
Before presenting the objective function, it is necessary to
define the EST and EFT attributes, which are derived from a
given partial schedule. EST ni ; pj  and EF T ni ; pj  are the
earliest execution start time and the earliest execution finish
time of task ni on processor pj , respectively. For the entry
task nentry ,

EST nentry ; pj   0:
4
For the other tasks in the graph, the EFT and EST values
are computed recursively, starting from the entry task, as
shown in (5) and (6), respectively. In order to compute the
EFT of a task ni , all immediate predecessor tasks of ni must


have been scheduled.
EST ni ; pj   max availj; max
AF T nm   cm;i 
;
nm 2predni 
5

6
EF T ni ; pj   wi;j  EST ni ; pj  ;
where predni  is the set of immediate predecessor tasks of
task ni and availj is the earliest time at which processor pj
is ready for task execution. If nk is the last assigned task on
processor pj , then availj is the time that processor pj
completed the execution of the task nk and it is ready to
execute another task when we have a noninsertion-based
scheduling policy. The inner max block in the EST equation
returns the ready time, i.e., the time when all data needed by
ni has arrived at processor pj .

After a task nm is scheduled on a processor pj , the earliest
start time and the earliest finish time of nm on processor pj
is equal to the actual start time, AST nm , and the actual
finish time, AF T nm , of task nm , respectively. After all
tasks in a graph are scheduled, the schedule length (i.e.,
overall completion time) will be the actual finish time of the
exit task nexit . If there are multiple exit tasks and the
convention of inserting a pseudo exit task is not applied, the
schedule length (which is also called makespan) is defined as
makespan  maxfAF T nexit g:
7
The objective function of the task-scheduling problem is to
determine the assignment of tasks of a given application to
processors such that its schedule length is minimized.

3 RELATED WORK

Static task-scheduling algorithms can be classified into two
main groups (see Fig. 1), heuristic-based and guided
random-search-based algorithms. The former can be further
classified into three groups:
list scheduling heuristics,
clustering heuristics, and task duplication heuristics.
List Scheduling Heuristics. A list-scheduling heuristic
maintains a list of all tasks of a given graph according to
their priorities. It has two phases: the task prioritizing (or task
selection) phase for selecting the highest-priority ready task
and the processor selection phase for selecting a suitable
processor that minimizes a predefined cost function (which
can be the execution start time). Some of the examples are
the Modified Critical Path (MCP)
[3], Dynamic Level
Scheduling [6], Mapping Heuristic (MH) [7], Insertion-
Scheduling Heuristic [18], Earliest Time First (ETF) [22],
and Dynamic Critical Path (DCP) [4] algorithms. Most of
the list-scheduling algorithms are for a bounded number
of fully connected homogeneous processors. List-schedul-
ing heuristics are generally more practical and provide
better performance results at a lower scheduling time than
the other groups.
Clustering Heuristics. An algorithm in this group maps
the tasks in a given graph to an unlimited number of
clusters. At each step, the selected tasks for clustering can
be any task, not necessarily a ready task. Each iteration
refines the previous clustering by merging some clusters. If
two tasks are assigned to the same cluster, they will be
executed on the same processor. A clustering heuristic
requires additional steps to generate a final schedule: a
cluster merging step for merging the clusters so that the
remaining number of clusters equa l
the number of
processors, a cluster mapping step for mapping the clusters
on the available processors, and a task ordering step for
ordering the mapped tasks within each processor [24].
Some examples in this group are the Dominant Sequence
Clustering (DSC)
[12], Linear Clustering Method [19],
Mobility Directed [3], and Clustering and Scheduling
System (CASS) [25].
Task Duplication Heuristics. The idea behind duplica-
tion-based scheduling algorithms is to schedule a task
graph by mapping some of its tasks redundantly, which
reduces the interprocess communication overhead [9], [18],
[27], [30]. Duplication-based algorithms differ according to
the selection strategy of the tasks for duplication. The

TOPCUOGLU ET AL.: PERFORMANCE-EFFECTIVE AND LOW-COMPLEXITY TASK SCHEDULING FOR HETEROGENEOUS COMPUTING

263

Fig. 1. Classification of static task-scheduling algorithms.

algorithms in this group are usually for an unbounded
number of identical processors and they have much higher
complexity values than the algorithms in the other groups.
Guided Random Search Techniques. Guided random
search techniques (or randomized search techniques) use
random choice to guide themselves through the problem
space, which is not the same as performing merely random
walks as in the random search methods. These techniques
combine the knowledge gained from previous search
results with some randomizing features to generate new
results. Genetic algorithms (GAs) [5], [8], [11], [13], [17] are
the most popular and widely used techniques for several
flavors of the task scheduling problem. GAs generate good
quality of output schedules; however,
their scheduling
times are usually much higher than the heuristic-based
techniques [31]. Additionally, several control parameters in
a genetic algorithm should be determined appropriately.
The optimal set of control parameters used for scheduling
a task graph may not give the best results for another
task graph. In addition to GAs, simulated annealing [11],
[15] and local search method [16],
[20] are the other
methods in this group.

3.1 Task-Scheduling Heuristics for Heterogeneous
Environments
This section presents the reported task-scheduling heuristics
that support heterogeneous processors, which are the
Dynamic Level Scheduling Algorithm [6], the Levelized-
Min Time Algorithm [10], and the Mapping Heuristic
Algorithm [7].
Dynamic-Level Scheduling (DLS) Algorithm. At each
the algorithm selects the (ready node, available
step,
processor) pair that maximizes the value of the dynamic
level which is equal to DLni ; pj   ranks
u ni    EST ni ; pj .
The computation cost of a task is the median value of the
computation costs of the task on the processors. In this
algorithm, upward rank calculation does not consider the
communication costs. For heterogeneous environments, a

new term added for the difference between the task's
median execution time on all processors and its execution
time on the current processor. The general DLS algorithm
has an Ov3  q time complexity, where v is the number of
tasks and q is the number of processors.
Mapping Heuristic (MH). In this algorithm, the compu-
tation cost of a task on a processor is computed by the
number of instructions to be executed in the task divided by
the speed of
the processor. However,
in setting the
computation costs of tasks and the communication costs
of edges before scheduling, similar processing elements
(i.e., homogeneous processors) are assumed; the hetero-
geneity comes into the picture during the scheduling
process.
This algorithm uses static upward ranks to assign
priorities. (The authors also experimented by adding the
communication delay to the rank values.) In this algorithm,
the ready time of a processor for a task is the time when the
processor has finished its last assigned task and is ready to
execute a new one. The MH algorithm does not schedule a
task to an idle time slot that is between two tasks already
scheduled. The time complexity, when contention is con-
sidered, is equal to Ov2  q3  for v tasks and q processors;
otherwise, it is equal to Ov2  q.
Levelized-Min Time (LMT) Algorithm. It is a two-phase
algorithm. The first phase groups the tasks that can be
executed in parallel using the level attribute. The second
phase assigns each task to the fastest available processor. A
task in a lower level has higher priority than a task in a
higher level. Within the same level, the task with the highest
computation cost has the highest priority. Each task is
assigned to a processor that minimizes the sum of the task's
computation cost and the total communication costs with
tasks in the previous levels. For a fully connected graph,
the time complexity is Ov2  q2  when there are v tasks
and q processors.

264

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 13, NO. 3, MARCH 2002

Fig. 2. The HEFT algorithm.

4 TASK-SCHEDULING ALGORITHMS
Before introducing the details of HEFT and CPOP algorithms,
we introduce the graph attributes used for setting the task
priorities.

4.1 Graph Attributes Used by HEFT and CPOP
Algorithms
Tasks are ordered in our algorithms by their scheduling
priorities that are based on upward and downward
ranking. The upward rank of a task ni
is recursively
defined by
8
ci;j  ranku nj ;
ranku ni   wi  max
nj 2succni 
where succni  is the set of immediate successors of task ni ,
ci;j is the average communication cost of edge i; j, and wi
is the average computation cost of task ni . Since the rank is
computed recursively by traversing the task graph upward,
starting from the exit task, it is called upward rank. For the
exit task nexit , the upward rank value is equal to
9
ranku nexit   wexit :
Basically, ranku ni  is the length of the critical path from
task ni to the exit task, including the computation cost of
task ni . There are algorithms in the literature which
compute the rank value using computation costs only,
which is called static upward rank, ranks
u .
Similarly, the downward rank of a task ni is recursively
	

defined by
rankd ni   max
nj 2predni  rankd nj   wj  cj;i
10
;
where predni  is the set of immediate predecessors of
task ni . The downward ranks are computed recursively
by traversing the task graph downward starting from the
entry task of the graph. For the entry task nentry , the
downward rank value is equal to zero. Basically, rankd ni 
is the longest distance from the entry task to task ni ,
excluding the computation cost of the task itself.

4.2 The Heterogeneous-Earliest-Finish-Time (HEFT)
Algorithm
The HEFT algorithm (Fig. 2) is an application scheduling
algorithm for a bounded number of heterogeneous
processors, which has two major phases: a task prioritizing
for computing the priorities of all
tasks and a
phase
processor selection phase for selecting the tasks in the order
of their priorities and scheduling each selected task on its
ªbestº processor, which minimizes the task's finish time.

Task Prioritizing Phase. This phase requires the priority
of each task to be set with the upward rank value, ranku ,
which is based on mean computation and mean communica-
tion costs. The task list is generated by sorting the tasks by
decreasing order of ranku . Tie-breaking is done randomly.
There can be alternative policies for tie-breaking, such as
selecting the task whose immediate successor task(s) has
higher upward ranks. Since these alternate policies increase
the time complexity, we prefer a random selection strategy.
It can be easily shown that the decreasing order of ranku
values provides a topological order of tasks, which is a
linear order that preserve the precedence constraints.
Processor Selection Phase. For most of the task schedul-
ing algorithms, the earliest available time of a processor pj
for a task execution is the time when pj completes the
execution of its last assigned task. However, the HEFT
algorithm has an insertion-based policy which considers the
possible insertion of a task in an earliest idle time slot
between two already-scheduled tasks on a processor. The
length of an idle time-slot,
i.e., the difference between
execution start time and finish time of two tasks that were
consecutively scheduled on the same processor, should be
at least capable of computation cost of the task to be
scheduled. Additionally, scheduling on this idle time slot
should preserve precedence constraints.
In the HEFT Algorithm, the search of an appropriate idle
time slot of a task ni on a processor pj starts at the time
equal to the ready time of ni on pj , i.e., the time when all
tha t were sent by ni 's immed iate
input data of ni
predecessor tasks have arrived at processor pj . The search
continues until finding the first idle time slot that is capable of
holding the computation cost of task ni . The HEFT algorithm
has an Oe  q time complexity for e edges and q processors.
For a dense graph when the number of edges is
proportional to Ov2  (v is the number of tasks), the time
complexity is on the order of Ov2  p.
As an illustration, Fig. 4a presents the schedules obtained
by the HEFT algorithm for the sample DAG of Fig. 3. The
schedule length, which is equal to 80, is shorter than the
schedule lengths of
the
the related work; specifically,
schedule lengths of DLS, MH, and LMT Algorithms are
91, 91, and 95, respectively. The first column in Table 1 gives
upward rank values for the given task graph . The
scheduling order of the tasks with respect to the HEFT
Algorithm is fn1 ; n3 ; n4 ; n2 ; n5 ; n6 ; n9 ; n7 ; n8 ; n10g.
4.3 The Critical-Path-on-a-Processor (CPOP)
Algorithm
Although our second algorithm,
the CPOP algorithm
shown in Fig. 5, has the task prioritizing and processor

TOPCUOGLU ET AL.: PERFORMANCE-EFFECTIVE AND LOW-COMPLEXITY TASK SCHEDULING FOR HETEROGENEOUS COMPUTING

265

Fig. 3. A sample task graph with 10 tasks.

Fig. 4. Scheduling of task graph in Fig. 3 with the HEFT and CPOP algorithms. (a) HEFT Algorithm (schedule length = 80). (b) CPOP Algorithm
(schedule length = 86).

selection phases as in the HEFT algorithm, it uses a different
attribute for setting the task priorities and a different
strategy for determining the ªbestº processor for each
selected task.
Task Prioritizing Phase. In this phase, upward rank
(ranku ) and downward rank (rankd ) values for all tasks are
computed using mean computation and mean communica-
tion costs (Steps 1-3). The CPOP algorithm uses the critical

path of a given application graph. The length of this path,
jCP j, is the sum of the computation costs of the tasks on the
path and intertask communication costs along the path. The
sum of computation costs on the critical path of a graph is
basically the lower bound for the schedule lengths generated
by the task scheduling algorithms.
The priority of each task is assigned with the summation
of upward and downward ranks. The critical path length is

266

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 13, NO. 3, MARCH 2002

TABLE 1
Values of Attributes Used in HEFT and CPOP Algorithms
for Task Graph in Fig. 3

equal to the entry task's priority (Step 5). Initially, the entry
task is the selected task and marked as a critical path task.
An immediate successor (of the selected task) that has the
highest priority value is selected and it is marked as a
critical path task. This process is repeated until the exit node
is reached (Steps 6-12). For tie-breaking, the first immediate
successor which has the highest priority is selected.
We maintain a priority queue (with the key of
ranku  rankd )
to contain all ready tasks at any given
instant. A binary heap was used to implement the priority
queue, which has time complexity of Olog v for insertion
and deletion of a task and O1 for retrieving the task with
the highest priority. At each step, the task with the highest
ranku  rankd value is selected from the priority queue.
Processor Selection Phase. The critical-path processor,
pCP , is the one that minimizes the cumulative computation
costs of the tasks on the critical path (Step 13). If the selected

task is on the critical path, then it is scheduled on the
critical-path processor; otherwise,
it
is assigned to a
processor which minimizes the earliest execution finish
time of the task. Both cases consider an insertion-based
scheduling policy. The time-complexity of
the CPOP
algorithm is equal to Oe  p. Fig. 4b shows the schedule
obtained by the CPOP algorithm for Fig. 3, which has a
schedule length of 86. Based on the values in Table 1, the
critical path in Fig. 3 is fn1 ; n2 ; n9 ; n10 g. If all critical path
tasks are scheduled on P 1, P 2, or P 3, the path length will be
66, 54, or 63, respectively. P 2 is selected as the critical path
processor. The scheduling order of the tasks with respect to
CPOP algorithm is fn1 ; n2 ; n3 ; n7 ; n4 ; n5 ; n9 ; n6 ; n8 ; n10 g.

5 EXPERIMENTAL RESULTS AND DISCUSSION
In this section, we present the comparative evaluation of
our algorithms and the related work given in Section 3.1.
For this purpose, we consider two sets of graphs as the
workload for testing the algorithms: randomly generated
application graphs and the graphs that represent some of
the numerical real world problems. First, we present the
metrics used for performance evaluation, which is followed
by two sections on experimental results.

5.1 Comparison Metrics
The comparisons of
the algorithms are based on the
following four metrics:

.

Schedule Length Ratio (SLR). The main perfor-
mance measure of a scheduling algorithm on a
graph is the schedule length (makespan) of its output
schedule. Since a large set of task graphs with
different properties is used,
is necessary to
it
normalize the schedule length to a lower bound,

Fig. 5. The CPOP algorithm.

TOPCUOGLU ET AL.: PERFORMANCE-EFFECTIVE AND LOW-COMPLEXITY TASK SCHEDULING FOR HETEROGENEOUS COMPUTING

267

.

:

11

SLR 

which is called the Schedule Length Ratio (SLR). The
SLR value of an algorithm on a graph is defined by
P
makespan
minpj 2Q fwi;j g :
ni 2CPM IN
The denominator is the summation of the minimum
computation costs of tasks on the CPM IN . (For an
unscheduled DAG, if the computation cost of each
node ni is set with the minimum value, then the
critical path will be based on minimum computation
costs, which is represented as CPM IN .) The SLR of a
graph (using any algorithm) cannot be less than one
since the denominator is the lower bound. The task-
scheduling algorithm that gives the lowest SLR of a
graph is the best algorithm with respect to perfor-
mance. Average SLR values over several task graphs
are used in our experiments.
Speedup. The speedup value for a given graph is
computed by dividing the sequential execution time
(i.e., cumulative computation costs of the tasks in the
graph) by the parallel execution time (i.e.,
the
makespan of the output schedule). The sequential
execution time is computed by assigning all tasks to
a single processor that minimizes the cumulative of
Speedup  minpj 2Q fP
the computation costs.
ni 2V wi;j g
makespan
If the sum of the computation costs is maximized,
it results in a higher speedup, but ends up with
the same ranking of
the scheduling algorithms.
Efficiency, the ratio of the speedup value to the
number of processors used, is another comparison
metric used for application graphs of real world
problems given in Section 5.3.
. Number of Occurrences of Better Quality of
Schedules. The number of times that each algorithm
produced better, worse, and equal quality of sche-
dules compared to every other algorithm is counted in
the experiments.
. Running Time of the Algorithms. The running time
(or the scheduling time) of an algorithm is its
execution time for obtaining the output schedule
of a given task graph. This metric basically gives
the average cost of each algorithm. Among the
algorithms that give comparable SLR values, the
one with the minimum running time is the most
practical implementation. The minimization of SLR
by checking all possible task-processor pairs can
conflict with the minimization in the running time.

12

5.2 Randomly Generated Application Graphs
In our study, we first considered the randomly generated
application graphs. A random graph generator was
implemented to generate weighted application DAGs
with various characteristics that depend on several input
parameters given below. Our simulation-based framework
allows assigning sets of values to the parameters used by
random graph generator. This framework first executes the
random graph generator program to construct the applica-
tion DAGs, which is followed by the execution of the
scheduling algorithms to generate output schedules, and,

finally, it computes the performance metrics based on the
schedules.

5.2.1 Random Graph Generator
Our random graph generator requires the following input
parameters to build weighted DAGs.
. Number of tasks in the graph, v.
Shape parameter of the graph, . We assume that
.

the height (depth) of a DAG is randomly generated
p
from a uniform distribution with a mean value equal
v
 . (The height is equal to the smallest integral
to
va lue not
less than the rea l va lue generated
randomly.) The width for each level is randomly
equal to   
p
selected from a uniform distribution with mean
. A dense graph (a shorter graph
v
with high parallelism) can be generated by selecting
 >> 1:0; if  << 1:0, it will generate a longer graph
with a low parallelism degree.
. Out degree of a node, out degree.
. Communication to computation ratio, (CCR). It is the
ratio of the average communication cost
to the
average computation cost. If a DAG's CCR value is
very low, it can be considered as a computation-
intensive application.
Range percentage of computation costs on proces-
sors, . It is basically the heterogeneity factor for
processor speeds. A high percentage value causes a
significant difference in a task's computation cost
among the processors and a low percentage indi-
cates that the expected execution time of a task is
almost equal on any given processor in the system.
The average computation cost of each task ni in the
graph, i.e., wi , is selected randomly from a uniform
distribution with range  0 ; 2  wDAG , where wDAG
is the average computation cost of the given graph,
which is set randomly in the algorithm. Then, the
computation cost of each task ni on each processor pj
in the system is randomly set from the following




range:
 wi;j  wi  1  
wi  1   
2
2

13

.

:

In each experiment, the values of these parameters are
assigned from the corresponding sets given below. A
parameter should be assigned by all values given in its set
in a single experiment and, in case of any change on these
values, it is written explicitly in the paper. Note that the last
value in the out_degree set is the number of nodes in the
graph which generate fully connected graphs for the
experiments.
SETV  f20; 40; 60; 80; 100g,
.
SETCCR  f0:1; 0:5; 1:0; 5:0; 10:0g,
.
SET  f0:5; 1:0; 2:0g,
.
SETout degree  f1; 2; 3; 4; 5; vg,
.
SET  f0:1; 0:25; 0:5; 0:75; 1:0g.
.
These combinations give 2,250 different DAG types.
Since 25 random DAGs were generated for each DAG type,
the total number of DAGs used in our experiments was
around 56K . Assigning several
input parameters and
selecting each parameter from a large set cause the

268

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 13, NO. 3, MARCH 2002

Fig. 6. (a) Average SLR and (b) average speedup with respect to graph size.

generation of diverse DAGs with various characteristics.
Experiments based on diverse DAGs prevent biasing
toward a particular scheduling algorithm.

5.2.2 Performance Results
The performance of the algorithms were compared with
respect to various graph characteristics. The first set of
experiments compares the performance and cost of the
algorithms with respect to various graph sizes (see Figs. 6
and 7) . The SLR-based performance ranking of
the
algorithms is {HEFT, CPOP, DLS, MH, LMT}. (It should
be noted that each ranking in this paper starts with the best
algorithm and ends with the worst one with respect to
the given comparison metric.) The average SLR value of
HEFT on all generated graphs is better than the CPOP
algorithm by 7 percent, the DLS algorithm by 8 percent,
the MH algorithm by 16 percent, and the LMT algorithm
by 52 percent. The average speedup ranking of
the
algorithms is {HEFT, DLS, (CPOP=MH), LMT} (Fig. 6b).
Based on these experiments, the HEFT algorithm outper-
forms the other algorithms for any graph size in terms of SLR
and speedup. The CPOP algorithm outperforms the related
work in terms of average SLR; for various graph sizes, it
cannot give higher speedup values than the DLS algorithm.
With respect to average running times (see Fig. 7), The HEFT
algorithm is the fastest and the DLS algorithm is the
slowest one. On average, the HEFT algorithm is faster
than the CPOP algorithm by 10 percent, the MH algorithm
by 32 percent, the DLS algorithm by 84 percent, and the
LMT algorithm by 48 percent.
The next experiment
to the graph
is with respect
structure. When  (the shape parameter of the graph) is
equal to 0:5, i.e., the generated graphs have greater depths
with a low degree of parallelism,
it is shown that the
performance of the HEFT algorithm is better than that of
the CPOP algorithm by 8 percent, the MH algorithm by
12 percent, the DLS algorithm by 16 percent, and the
LMT algorithm by 40 percent. When  is equal to 1:0, the
average SLR value of the HEFT algorithm is better than that
of the CPOP algorithm by 7 percent, the MH algorithm
by 14 percent, the DLS algorithm by 7 percent, and the
LMT algorithm by 34 percent. When  is equal to 2:0,
the HEFT algorithm is better than the CPOP algorithm by

6 percent, the MH algorithm by 15 percent, the DLS algorithm
by 8 percent, and the LMT algorithm by 31 percent. For all
three different graph structures, the HEFT algorithm gives
the best performance.
Quality of schedules generated by the algorithms with
respect to various CCR values was compared in another
experiment. The performance ranking of the algorithms
when CCR  1:0 is {HEFT, DLS, MH, CPOP, LMT}. When
CCR > 1:0, the performance ranking changes to {HEFT,
CPOP, DLS, MH, LMT}. The CPOP algorithm gives better
results for graphs with higher CCRs than the graphs with
lower CCRs. Clustering of the critical path on the fastest
processor results in better quality of schedules for the
graphs in which average communication cost is greater than
average computation cost.
times that each scheduling
Finally,
the number of
algorithm in the experiments produced better, worse, or
equal schedule length compared to every other algorithm
was counted for the 56250 DAGs used. Each cell in Table 2
indicates the comparison results of the algorithm on the left
with the algorithm on the top. The ªcombinedº column
shows the percentage of graphs in which the algorithm on
the left gives a better, equal, or worse performance than all
other algorithms combined. The ranking of the algorithms,

Fig. 7. Average running time of algorithms with respect to graph size.

TOPCUOGLU ET AL.: PERFORMANCE-EFFECTIVE AND LOW-COMPLEXITY TASK SCHEDULING FOR HETEROGENEOUS COMPUTING

269

TABLE 2
Pair-Wise Comparison of the Scheduling Algorithms

based on occurrences of best results, is {HEFT, DLS, CPOP,
MH, LMT}. However, the ranking with respect to average
SLR values was: {HEFT, CPOP, DLS, MH, LMT}. Although
the DLS algorithm outperforms the CPOP algorithm in
terms of the number of occurrences of best results, the
CPOP algorithm has shown slightly better average SLR
value than the DLS algorithm.

5.3 Application Graphs of Real World Problems
In addition to randomly generated task graphs, we also
three real wor ld
cons idered app licat ion graphs of
problems: Gauss elimination algorithm [3],
[28], Fast
Fourier Transformation [29],
[30], and a molecular
dynamics code given in [19].

5.3.1 Gaussian Elimination
Fig. 8a gives the sequential program for the Gaussian
elimination algorithm [3], [28]. The data-flow graph of the
algorithm for the special case of m  5, where m is the
dimension of the matrix, is given in Fig. 8b. Each Tk;k
represents a pivot column operation and each Tk;j repre-
sents an update operation. In Fig. 8b, the critical path is
T1;1T1;2T2;2T2;3T3;3T3;4T4;4T4;5 , which is the path with the
maximum number of tasks.
For the experiments of Gauss elimination application, the
same CCR and range percentage values (given in Section 5.2)
were used. Since the structure of the application graph is
known, we do not need the other parameters, such as the
number of tasks, out_degree, and shape parameters. A
new parameter, matrix size (m), is used in place of v (the
number of tasks in the graph). The total number of tasks in a
Gaussian elimination graph is equal to m2m 2
.
2
Fig. 9a gives the average SLR values of the algorithms at
various matrix sizes from 5 to 20, with an increment of one,
when the number of processors is equal
to five. The
smallest size graph in this experiment has 14 tasks and
the largest one has 209 tasks. The performances of the
HEFT and DLS algorithms are the best of all. Increasing
the matrix size causes more tasks not to be on the critical
path, which results in an increase in the makespan for
each algorithm.

For the efficiency comparison, the number of processors
used in our experiments is varied from 2 to 16, increment-
ing by the power of 2; the CCR and range percentage
parameters have the same set of values. Fig. 9b gives
efficiency comparison for Gaussian elimination graphs
when the matrix size is 50. The HEFT and DLS algorithms
have better efficiency than the other algorithms. When the
number of processors is increased beyond eight,
the
HEFT algorithm outperforms the DLS algorithm in terms
of efficiency. Since the matrix size is fixed, an increase in
the number of processors decreases the makespan for each
algorithm. As part of this experiment, we compared the
running time of the algorithms with respect to the various
numbers of processors (by keeping the matrix size fixed).
The results indicate that the DLS algorithm is the slowest
algorithm among them, although it performs as well as
the HEFT algorithm. As an example, when the matrix size
is 50 for 16 processors, the DLS algorithm takes 16.2 times
longer than the HEFT algorithm to schedule a given
graph. When the performance and cost
results are
considered together,
the HEFT algorithm is the most
efficient and practical algorithm among them.

5.3.2 Fast Fourier Transformation
The recursive, one-dimensional FFT Algorithm [29], [30]
and its task graph (when there are four data points) is given
in Fig. 10. In this figure, A is an array of size m which holds
the coefficients of the polynomial and array Y is the
output of the algorithm. The algorithm consists of two
parts: recursive calls (lines 3-4) and the butterfly opera-
tion (lines 6-7). The task graph in Fig. 10b can be divided
into two partsÐthe tasks above the dashed line are the
recursive call
tasks and the ones below the line are
butterfly operation tasks. For an input vector of size m,
tasks and m  log2 m
there are 2  m   1 recursive call
(We assume that m  2k
butterfly operation tasks.
for
some integer k). Each path from the start task to any of
the exit tasks in an FFT task graph is a critical path since
the computation costs of tasks in any level are equal and the
communication costs of all edges between two consecutive
levels are equal.

270

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 13, NO. 3, MARCH 2002

Fig. 8. (a) Gaussian elimination algorithm, (b) task graph for matrix of size 5.

Fig. 9. (a) Average SLR and (b) efficiency comparison for the Gaussian elimination graph.

For the FFT-related experiments, only the CCR and range
percentage parameters, among the parameters given in
Section 5.2, were used, as in the Gauss elimination
application. The number of data points in FFT is another
parameter in our simulations, which varies from 2 to 32
incrementing by the power of 2. Fig. 11a shows the average
SLR values for FFT graphs at various sizes of input points.
One can observe that the HEFT algorithm outperforms the
other algorithms in most of the cases. Fig. 11b presents the
efficiency values obtained for each of
the algorithms
with respect
to various numbers of processors with
graphs of 64 data points. The number of processors used
varied from two to 32, incrementing by the power of 2.
The HEFT and DLS algorithms give the most efficient
schedules in all cases.
When the running times of the algorithms for schedul-
ing FFT graphs are compared with respect to both the
number of data points and the number of processors used

(see Fig. 12), one can observe that the DLS algorithm is
the highest cost algorithm. Note that
the number of
processors is equal to six in Fig. 12a and the number of
input points is equal to 64 in Fig. 12b.

5.3.3 Molecular Dynamics Code
Fig. 13 is the task graph of a modified molecular dynamic
code given in [19] . This application is part of our
performance evaluation since it has an irregular task graph.
Since the number of tasks is fixed in the application and the
structure of the graph is known, only the values of CCR and
range percentage parameters (in Section 5.2) are used in our
experiments. Fig. 14a shows the performance of
the
algorithms with respect to five different CCR values when
the number of processors is equal to six. On the average, the
SLR ranking is {HEFT, DLS, CPOP, MH, LMT}. The
efficiency comparison of the scheduling algorithms is given
in Fig. 14b, in which the number of processors is varied

TOPCUOGLU ET AL.: PERFORMANCE-EFFECTIVE AND LOW-COMPLEXITY TASK SCHEDULING FOR HETEROGENEOUS COMPUTING

271

Fig. 10. (a) FFT algorithm, (b) the generated DAG of FFT with four points.

Fig. 11. (a) Average SLR and (b) efficiency comparison for the FFT graph.

Fig. 12. Running times of scheduling algorithms for the FFT graph.

from two to seven with an increment of 1. Since there are at
most seven tasks in any level in Fig. 13, the number of
processors in the experiments is bounded up to seven
processors. It was also observed that the DLS and LMT

algorithms take a running time almost three times longer
than the other three algorithms (HEFT, CPOP, and MH).
When these results are combined, the HEFT algorithm is the
most practical and efficient algorithm for this application.

272

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 13, NO. 3, MARCH 2002

Fig. 13. The task graph of the molecular dynamics code [19].

6 ALTERNATE POLICIES FOR THE PHASES OF THE
HEFT ALGORITHM

We proposed three substitute policies (shown as A1, A2,
and A3)
for the task prioritizing phase of
the HEFT
algorithm, which is based on upward rank in the experi-
ments. In A1, the priority value is equal to the summation of
the upward and downward ranks. In A2, the right part of
the  sign gives the latest execution finish time of an
immediate predecessor task of ni , which is already
scheduled. A3 is similar to A2, except that it considers the
communication cost.
In the experiments,
it has been
observed that the original priority policy gives better results
than these alternates by 6 percent.
. A1: priorityni   ranku ni   rankd ni ,
nj 2predni  AF T nj ,
. A2: priorityni   ranku ni   max
. A3:
priorityni   ranku ni   max
fAF T nj   cj;i g:
nj 2predni 

Another extension that may improve performance is to
take immediate child tasks into account. As an example, the
HEFT algorithm generates an output schedule of length 8
for the task graph given in Fig. 15;
if task A and its
immediate child (task B) are scheduled on the same
processor, which minimizes the earliest finish time of task
B, the schedule length decreases to 7. To consider this

extension, we can modify the processor selection phase of
the HEFT algorithm as follows: For each selected task, one
of its immediate child tasks is marked as the critical-child
based on one of the three policies given below. If the other
immediate predecessors of the critical child are already
scheduled, then the selected task and its critical child are
scheduled on the same processor that minimizes the earliest
finish time of the critical child; otherwise, the selected task
is scheduled to the processor that minimizes its earliest
finish time, as in the HEFT algorithm. The three critical
child selection policies (B1, B2, and B3) use either commu-
nication cost or upward rank or both.
B1: critical childni   maxnc 2succni  ci;c ,
B2: critical childni   maxnc 2succni  ranku nc ,
B3:

.
.
.

critical childni   max
nc 2succni 

franku nc   ci;c g:

The original HEFT algorithm outperforms these alter-
nates for small CCR graphs. For high CCR graphs, some
benefit has been observed by taking critical child tasks into
account during processor selection. When 3:0  CCR < 6:0,
B1 policy slightly outperforms the original HEFT algorithm.
If CCR  6:0, B2 policy outperforms the original algorithm
and others alternates by 4 percent.

TOPCUOGLU ET AL.: PERFORMANCE-EFFECTIVE AND LOW-COMPLEXITY TASK SCHEDULING FOR HETEROGENEOUS COMPUTING

273

Fig. 14. (a) Average SLR and (b) efficiency comparison for the task graph of a molecular dynamics code.

Fig. 15. Scheduling of a task graph with the (a) HEFT algorithm and (b) an alternative method.

7 CONCLUSIONS

In this paper, we presented two new algorithms, called
the HEFT algorithm and the CPOP algorithm,
for
scheduling application graphs onto a system of hetero-
geneous processors. Based on the experimental study
using a large set (56K) of randomly generated application
graphs with various characteristics and application graphs
of several real world problems (such as Gaussian elimina-
tion, FFT, and a molecular dynamics code), the HEFT
algorithm significantly outperformed the other algorithms
in terms of both performance and cost metrics, including
average schedule length ratio, speedup, frequency of best
results, and average running time. Because of its robust
performance, low running time, and the ability to give stable
performance over a wide range of graph structures, the HEFT
algorithm is a viable solution for the DAG scheduling
problem on heterogeneous systems. Based on our perfor-
mance evaluation study, we also observed that the CPOP
algorithm has given either better performance and better
running time results than existing algorithms or comparable
results with them.
Several alternative policies were studied for task prior-
itizing and processor selection phases of the HEFT algo-
rithm. A new method was introduced for the processor

selection phase which tries to minimize the earliest finish
time of the critical-child task of each selected task.
One planned future research is to analytically investigate
the trade-off between the quality of schedules of
the
algorithms, i.e., average makespan values, and the number
of processors available. This extension may come up with
some bounds on the degradation of makespan given that
the number of processors available may not be sufficient.
We plan to extend the HEFT Algorithm for rescheduling
tasks in response to changes in processor and network
loads. Although our algorithms assume a fully connected
network, it is also planned to extent these algorithms for
arbitrary-connected networks by considering the link
contention.

[2]

REFERENCES
[1] M.R. Gary and D.S. Johnson, Computers and Intractability: A Guide
to the Theory of NP-Completeness. W.H. Freeman and Co., 1979.
J.D. Ullman, ªNP-Complete Scheduling Problems,º J. Computer
and Systems Sciences, vol. 10, pp. 384-393, 1975.
[3] M. Wu and D. Gajski, ªHypertool: A Programming Aid for
Message Passing Systems,º IEEE Trans. Parallel and Distributed
Systems, vol. 1, pp. 330-343, July 1990.
[4] Y. Kwok and I. Ahmad, ªDynamic Critical-Path Scheduling: An
Effective Technique for Allocating Task Graphs to Multi-
processors,º IEEE Trans. Parallel and Distributed Systems, vol. 7,
no. 5, pp. 506-521, May 1996.

274

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 13, NO. 3, MARCH 2002

[9]

[5] E.S.H. Hou, N. Ansari, and H. Ren, ªA Genetic Algorithm for
Multiprocessor Scheduling,º IEEE Trans. Parallel and Distributed
Systems vol. 5, no. 2, pp. 113-120, Feb. 1994.
[6] G.C. Sih and E.A. Lee, ªA Compile-Time Scheduling Heuristic for
Interconnection-Constrained Heterogeneous Processor Architec-
tures,º IEEE Trans. Parallel and Distributed Systems, vol. 4, no. 2,
pp. 175-186, Feb. 1993.
[7] H. El-Rewini and T.G. Lewis, ªScheduling Parallel Program Tasks
onto Arbitrary Target Machines,º J. Parallel and Distributed
Computing, vol. 9, pp. 138-153, 1990.
[8] H. Singh and A. Youssef, ªMapping and Scheduling Hetero-
geneous Task Graphs Using Genetic Algorithms,º Proc. Hetero-
geneous Computing Workshop, pp. 86-97, 1996.
I. Ahmad and Y. Kwok, ªA New Approach to Scheduling Parallel
Programs Using Task Duplication,º Proc.
Int'l Conf. Parallel
Processing, vol. 2, pp. 47-51, 1994.
[10] M. Iverson, F. Ozguner, and G. Follen, ªParallelizing Existing
Applications in a Distributed Heterogeneous Environment,º Proc.
Heterogeneous Computing Workshop, pp. 93-100, 1995.
[11] P. Shroff, D.W. Watson, N.S. Flann, and R. Freund, ªGenetic
Simulated Annealing for Scheduling Data-Dependent Tasks in
Heterogeneous Environments,º Proc. Heterogeneous Computing
Workshop, pp. 98-104, 1996.
[12] T. Yang and A. Gerasoulis, ªDSC: Scheduling Parallel Tasks on an
Unbounded Number of Processors,º IEEE Trans. Parallel and
Distributed Systems, vol. 5, no. 9, pp. 951-967, Sept. 1994.
[13] L. Wang, H.J. Siegel, and V.P. Roychowdhury, ªA Genetic-
Algorithm-Based Approach for Task Matching and Scheduling
in Heterogeneous Computing Environments,º Proc. Heterogeneous
Computing Workshop, 1996.
[14] M. Maheswaran and H.J. Siegel, ªA Dynamic Matching and
Scheduling Algorithm for Heterogeneous Computing Systems,º
Proc. Heterogeneous Computing Workshop, pp. 57-69, 1998.
[15] L. Tao, B. Narahari, and Y.C. Zhao, ªHeuristics for Mapping
Parallel Computations to Heterogeneous Parallel Architectures,º
Proc. Heterogeneous Computing Workshop, 1993.
[16] M. Wu, W. Shu, and J. Gu, ªLocal Search for DAG Scheduling and
Task Assignment,º Proc. 1997 Int'l Conf. Parallel Processing,
pp. 174-180, 1997.
[17] R.C. Correa, A. Ferreria, and P. Rebreyend, ªIntegrating List
Heuristics into Genetic Algorithms for Multiprocessor Schedul-
ing,º Proc. Eighth IEEE Symp. Parallel and Distributed Processing
(SPDP '96), Oct. 1996.
[18] B. Kruatrachue and T.G. Lewis, ªGrain Size Determination for
Parallel Processing,º IEEE Software, pp. 23-32, Jan. 1988.
[19] S.J. Kim and J.C. Browne, ªA General Approach to Mapping of
Parallel Computation upon Multiprocessor Architectures,º Proc.
Int'l Conf. Parallel Processing, vol. 2, pp. 1-8, 1988.
[20] Y. Kwok, I. Ahmad, and J. Gu, ªFAST: A Low-Complexity
Algorithm for Efficient Scheduling of DAGs on Parallel Proces-
sors,º Proc. Int'l Conf. Parallel Processing, vol. 2, pp. 150-157, 1996.
[21] Y. Kwok and I. Ahmad, ªBenchmarking the Task Graph
Scheduling Algorithms,º Proc. First Merged Int'l Parallel Pocessing
Symp./Symp. Parallel and Distributed Processing Conf., pp. 531-537,
1998.
J.J. Hwang, Y.C. Chow, F.D. Anger, and C.Y. Lee, ªScheduling
Precedence Graphs in Systems with Interprocessor Communica-
tion Costs,º SIAM J. Computing, vol. 18, no. 2, pp. 244257, 1989.
[23] H. El-Rewini, H.H. Ali, and T. Lewis, ªTask Scheduling in
Multiprocessor Systems,º Computer, pp. 27-37, Dec. 1995.
J. Liou and M.A. Palis, ªA Comparison of General Approaches to
Multiprocessor Scheduling,º Proc. Int'l Parallel Processing Symp.,
pp. 152-156, 1997.
J. Liou and M.A. Palis, ªAn Efficient Clustering Heuristic for
Scheduling DAGs on Multiprocessors,º Proc. Symp. Parallel and
Distributed Processing, 1996.
[26] A. Radulescu, A.J.C. van Gemund, and H. Lin, ªLLB: A Fast and
Effective Scheduling Algorithm for Distributed-Memory Sys-
tems,º Proc. Second Merged Int'l Parallel Processing Symp./Symp.
Parallel and Distributed Processing Conf., 1999.
[27] G. Park, B. Shirazi, and J. Marquis, ªDFRN: A New Approach for
Duplication Based Scheduling for Distributed Memory Multi-
processor Systems,º Proc. Int'l Conf. Parallel Processing, pp. 157-
166, 1997.
[28] M. Cosnard, M. Marrakchi, Y. Robert, and D. Trystram, ªParallel
Gaussian Elimination on an MIMD Computer,º Parallel Comput-
ing, vol. 6, pp. 275-295, 1988.

[24]

[22]

[25]

[29] T.H. Cormen, C.E. Leiserson, and R.L. Rivest, Introduction to
Algorithms. MIT Press, 1990.
[30] Y. Chung and S. Ranka, ªApplications and Performance Analysis
of a Compile-Time Optimization Approach for List Scheduling
Algorithms on Distributed Memory Multiprocessors,º Proc. Super-
computing, pp. 512-521, Nov. 1992.
[31] T. Braun, H.J. Siegel, N. Beck, L.L. Boloni, M. Maheswaran, A.I.
Reuther, J.P. Robertson, M.D. Theys, B. Yao, D. Hengsen, and R.F.
Freund, ªA Comparison Study of Static Mapping Heuristics for a
Class of Meta-Tasks on Heterogeneous Computing Systems,º
Proc. Heterogeneous Computing Workshop, pp. 15-29, 1999.

Haluk Topcuoglu received the BS and MS
degrees in computer engineering from Bogazici
University, Istanbul, Turkey, in 1991 and 1993,
respectively. He received the PhD degree in
computer science from Syracuse University,
New York, in 1999. He is currently an assistant
professor in the Computer Engineering Depart-
ment, Marmara University, Turkey. His research
interests include task scheduling techniques in
heterogeneous environments, cluster comput-
ing, parallel and distributed programming, web technologies, and
genetic algorithms. He is a member of the IEEE, the IEEE Computer
Society and the ACM.

Salim Hariri received the MSc degree from
Ohio State University in 1982 and the PhD
degree in computer engineering from the Uni-
versity of Southern California in 1986. He is a
professor in the Electrical and Computer En-
gineering Department at the University of Ar-
izona and the d i rec to r o f
the Cen te r
for
Advanced TeleSysMatics (CAT): Next-Genera-
tion Network-Centric Systems. He is the editor in
chief
for Cluster Computing: The Journal of
Networks, Software Tools, and Applications. His current
research
focuses on high performance distributed computing, agent-based
proactive and intelligent network management systems, design and
analysis of high speed networks, and developing software design tools
for high performance computing and communication systems and
applications. He has coauthored more than 200 journal and conference
research papers and is the author of
the book, High Performance
Distributed Computing: Network, Architecture and Programming, to be
published by Prentice Hall, in 2002. He is a member of the IEEE
Computer Society.

Min-You Wu received the MS degree from the
Graduate School of Academia Sinica, Beijing,
China, and the PhD degree from Santa Clara
University, California. He is an associate pro-
fessor
in the Department of Electrical and
Computer Engineering at the University of New
Mexico. He has held various positions at the
University of
Il lino is at Urbana-Champaign,
University of California at Irvine, Yale University,
Syracuse University, the State University of New
York at Buffalo, and the University of Central Florida. His research
interests include parallel and distributed systems, compilers for parallel
computers, programming tools, VLSI design, and multimedia systems.
He has published more than 90 journal and conference papers in the
above areas and edited two special
issues on parallel operating
systems. He is a senior member of the IEEE and a member of ACM.
He is listed in International Who's Who of Information Technology and
Who's Who in America.

. For more information on this or any computing topic, please visit
our Digital Library at http://computer.org/publications/dlib.

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 22, NO. 6,

JUNE 2011

931

Performance Analysis of Cloud Computing
Services for Many-Tasks Scientific Computing

A lexandru Iosup, Member, IEEE, S imon Ostermann, M. Nez ih Y ig itbas i, Member, IEEE,
Radu Prodan, Member, IEEE, Thomas Fahr inger, Member, IEEE, and D ick H.J. Epema, Member, IEEE

Abstract—Cloud computing is an emerging commercial infrastructure paradigm that promises to eliminate the need for maintaining
expensive computing facilities by companies and institutes alike. Through the use of virtualization and resource time sharing, clouds
serve with a single set of physical resources a large user base with different needs. Thus, clouds have the potential to provide to their
owners the benefits of an economy of scale and, at the same time, become an alternative for scientists to clusters, grids, and parallel
production environments. However, the current commercial clouds have been built to support web and small database workloads,
which are very different from typical scientific computing workloads. Moreover, the use of virtualization and resource time sharing may
introduce significant performance penalties for the demanding scientific computing workloads. In this work, we analyze the
performance of cloud computing services for scientific computing workloads. We quantify the presence in real scientific computing
workloads of Many-Task Computing (MTC) users, that is, of users who employ loosely coupled applications comprising many tasks to
achieve their scientific goals. Then, we perform an empirical evaluation of the performance of four commercial cloud computing
services including Amazon EC2, which is currently the largest commercial cloud. Last, we compare through trace-based simulation the
performance characteristics and cost models of clouds and other scientific computing platforms, for general and MTC-based scientific
computing workloads. Our results indicate that the current clouds need an order of magnitude in performance improvement to be
useful to the scientific community, and show which improvements should be considered first to address this discrepancy between offer
and demand.

Index Terms—Distributed systems, distributed applications, performance evaluation, metrics/measurement, performance measures.

Ç

1 INTRODUCTION
S CIENTIFIC computing requires an ever-increasing number
of resources to deliver results for ever-growing problem
sizes in a reasonable time frame. In the last decade, while
the largest research projects were able to afford (access to)
expensive supercomputers, many projects were forced to
opt for cheaper resources such as commodity clusters and
grids. Cloud computing proposes an alternative in which
resources are no longer hosted by the researchers’ compu-
tational facilities, but are leased from big data centers only
when needed. Despite the existence of several cloud
computing offerings by vendors such as Amazon [1] and
GoGrid [2], the potential of clouds for scientific computing
remains largely unexplored. To address this issue, in this
paper we present a performance analysis of cloud comput-
ing services for many-task scientific computing.

. A. Iosup, M.N. Yigitbasi, and D.H.J. Epema are with the Parallel and
Distributed Systems Group, Faculty EEMCS, Delft University of
Technology, Mekelweg 4, Delft 2628CD, The Netherlands.
E-mail: {A.Iosup, M.N.Yigitbasi}@tudelft.nl, D.H.J.Epema@ewi.tudelft.nl.
. S. Ostermann, R. Prodan, and T. Fahringer are with the Distributed and
Parallel Systems Group, Institute for Computer Science, University of
Innsbruck, Technikerstr. 21a, Innsbruck A-6020, Austria.
Email: Simon.Ostermann@dps.uibk.ac.at,
{Radu.Prodan, Thomas.Fahringer}@uibk.ac.at.

Manuscript received 21 Dec. 2009; revised 2 July 2010; accepted 19 Aug.
2010; published online 16 Feb. 2011.
Recommended for acceptance by I. Raicu, I.T. Foster, and Y. Zhao.
For information on obtaining reprints of this article, please send e-mail to:
tpds@computer.org, and reference IEEECS Log Number
TPDSSI-2009-12-0653.
Digital Object Identifier no. 10.1109/TPDS.2011.66.

The cloud computing paradigm holds great promise for
the performance-hungry scientific computing community:
Clouds can be a cheap alternative to supercomputers and
specialized clusters, a much more reliable platform than
grids, and a much more scalable platform than the largest of
commodity clusters. Clouds also promise to “scale by credit
card,” that is, to scale up instantly and temporarily within
the limitations imposed only by the available financial
resources, as opposed to the physical limitations of adding
nodes to clusters or even supercomputers and to the
administrative burden of over provisioning resources.
Moreover, clouds promise good support for bags-of-tasks
(BoTs), which currently constitute the dominant grid
application type [3]. However, clouds also raise important
challenges in many aspects of scientific computing, includ-
ing performance, which is the focus of this work.
There are three main differences between scientific
computing workloads and the initial target workload of
clouds: in required system size, in performance demand,
and in the job execution model. Size wise, top scientific
computing facilities comprise very large systems, with the
top ten entries in the Top500 Supercomputers List together
totaling about one million cores, while cloud computing
services were designed to replace the small-to-medium size
enterprise data centers. Performance wise, scientific work-
loads often require High-Performance Computing (HPC) or
High-Throughput Computing (HTC) capabilities. Recently,
the scientific computing community has started to focus on
Many-Task Computing (MTC) [4], that is, on high-perfor-
mance execution of loosely coupled applications compris-
ing many (possibly interrelated)
tasks. With MTC, a

1045-9219/11/$26.00 ß 2011 IEEE

Published by the IEEE Computer Society

932

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 22, NO. 6,

JUNE 2011

paradigm at the intersection of HPC and HTC, it is possible
to demand systems to operate at high utilizations, similar to
those of current production grids (over 80 percent [5]) and
Parallel Production Infrastructures (PPIs) (over 60 percent
[6]), and much higher than those of the systems that clouds
originally intended to replace (servers with 10-20 percent
utilization). The job execution model of scientific computing
platforms is based on the exclusive, space-shared usage of
resources. In contrast, most clouds time-share resources and
use virtualization to abstract away from the actual hard-
ware, thus increasing the concurrency of users but poten-
tially lowering the attainable performance.
These three main differences between scientific comput-
ing workloads and the target workloads of clouds raise an
important research question: Is the performance of clouds
sufficient for MTC-based scientific computing?, or, in other
words, Can current clouds execute MTC-based scientific work-
loads with similar performance (that is, for traditional perfor-
mance metrics [7]) and at lower cost? Though early attempts to
characterize clouds and other virtualized services exist [8],
[9], [10], [11], [12], this question remains largely unexplored.
Our main contribution toward answering it is threefold:

1. We investigate the presence of a (proto-)MTC
component in scientific computing workloads and
quantify the presence of these users in scientific
computing environments.
2. We evaluate with well-known microbenchmarks
and application kernels the performance of four
commercial cloud computing services that can be
used for scientific computing, among which the
Amazon Elastic Compute Cloud (EC2), the largest
commercial computing cloud in production.
3. We compare the performance of clouds with that of
scientific computing alternatives such as grids and
parallel production infrastructures. Our comparison
uses trace-based simulation and the empirical perfor-
mance results of our cloud performance evaluation.
The remainder of the article is organized as follows: In
Section 2, we give a general introduction to the use of cloud
computing services for scientific computing, and select four
exemplary clouds for use in our investigation. Then, in
Section 3 we focus on finding the MTC component in
existing scientific computing workloads, and in Section 4
we evaluate empirically the performance of four commer-
cial clouds. In Section 5, we compare the performance of
clouds and of other scientific computing environments.
Last, we compare our investigation with related work in
Section 6, and we present our conclusion and potential
future research topics in Section 7.

2 CLOUD COMPUTING SERVICES FOR SCIENTIFIC
COMPUTING

In this section, we provide a background to analyzing the
performance of cloud computing services for scientific
computing. We first describe the main characteristics of the
common scientific computing workloads, based on pre-
vious work on analyzing and modeling of workload traces
taken from PPIs [6] and grids [5], [13]. Then, we introduce
the cloud computing services that can be used for scientific

computing, and select
four commercial clouds whose
performance we will evaluate empirically.

2.1 Scientific Computing
Job structure and source. PPI workloads are dominated by
parallel jobs [6], while grid workloads are dominated by
small bags-of-tasks [3] and sometimes by small workflows
[14], [15] comprising mostly sequential tasks. Source wise, it
is common for PPI grid workloads to be dominated by a
small number of users. We consider users that submit many
tasks, often grouped into the same submission as BoTs, as
proto-MTC users, in that they will be most likely to migrate
to systems that provide good performance for MTC work-
load execution. We focus in Section 3 on a more rigorous
definition of MTC workloads, and on demonstrating their
presence in recent scientific workloads.
Bottleneck resources. Overall, scientific computing
workloads are highly heterogeneous, and can have either
one of CPU, I/O, memory, and network as the bottleneck
resource. Thus, in Section 4 we investigate the performance
of these individual resources.
Job parallelism. A large majority of the parallel jobs
found in published PPI [16] and grid [13] traces have up to
128 processors [5], [6]. Moreover, the average scientific
cluster size was found to be around 32 nodes [17] and to be
stable over the past five years [18]. Thus, in Section 4 we
look at the the performance of executing parallel applica-
tions of up to 128 processors.

2.2 Four Selected Clouds: Amazon EC2, GoGrid,
ElasticHosts, and Mosso
We identify three categories of cloud computing services
[19], [20]: Infrastructure-as-a-Service (IaaS), that is, raw
infrastructure and associated middleware, Platform-as-a-
Service (PaaS), that is, APIs for developing applications on
an abstract platform, and Software-as-a-Service (SaaS), that
is, support for running software services remotely. Many
clouds already exist, but not all provide virtualization, or
even computing services. The scientific community has not
yet started to adopt PaaS or SaaS solutions, mainly to avoid
porting legacy applications and for lack of the needed
scientific computing services, respectively. Thus, in this
study we are focusing only on IaaS providers. We also focus
only on public clouds, that is, clouds that are not restricted
within an enterprise; such clouds can be used by our target
audience, scientists.
Based on our recent survey of the cloud computing
providers [21], we have selected for this work four IaaS
clouds. The reason for this selection is threefold. First, not
all the clouds on the market are still accepting clients;
FlexiScale puts new customers on a waiting list for over two
weeks due to system overload. Second, not all the clouds on
the market are large enough to accommodate requests for
even 16 or 32 coallocated resources. Third, our selection
already covers a wide range of quantitative and qualitative
cloud characteristics, as summarized in Table 1 and our
cloud survey [21], respectively. We describe in the follow-
ing Amazon EC2; the other three, GoGrid (GG), Elasti-
cHosts (EH), and Mosso, are IaaS clouds with provisioning,
billing, and availability and performance guarantees similar
to Amazon EC2’s.

IOSUP ET AL.: PERFORMANCE ANALYSIS OF CLOUD COMPUTING SERVICES FOR MANY-TASKS SCIENTIFIC COMPUTING

933

TABLE 1
The Resource Characteristics for the Instance Types Offered by
the Four Selected Clouds

TABLE 2
The Characteristics of the Workload Traces

The Amazon Elastic Computing Cloud is an IaaS cloud
computing service that opens Amazon’s large computing
infrastructure to its users. The service is elastic in the sense
that it enables the user to extend or shrink its infrastructure by
launching or terminating new virtual machines (instances).
The user can use any of the instance types currently available
on offer, the characteristics and cost of the five instance types
available in June 2009 are summarized in Table 1. An ECU is
the equivalent CPU power of a 1.0-1.2 GHz 2007 Opteron or
Xeon processor. The theoretical peak performance can be
computed for different instances from the ECU definition: a
1.1 GHz 2007 Opteron can perform 4 flops per cycle at full
pipeline, which means at peak performance one ECU equals
4.4 gigaflops per second (GFLOPS).
To create an infrastructure from EC2 resources, the user
specifies the instance type and the VM image; the user can
specify any VM image previously registered with Amazon,
including Amazon’s or the user’s own. Once the VM image
has been transparently deployed on a physical machine (the
resource status is running), the instance is booted; at the end
of the boot process the resource status becomes installed.
The installed resource can be used as a regular computing
node immediately after the booting process has finished,
via an ssh connection. A maximum of 20 instances can be
used concurrently by regular users by default; an applica-
tion can be made to increase this limit, but the process
involves an Amazon representative. Amazon EC2 abides by
a Service Level Agreement (SLA) in which the user is
compensated if the resources are not available for acquisi-
tion at least 99.95 percent of the time. The security of the
Amazon services has been investigated elsewhere [10].

3 MTC PRESENCE IN SCIENTIFIC COMPUTING
WORKLOADS

An important assumption of this work is that the existing
scientific workloads already include Many Task Computing
users,
that
is, of users that employ loosely coupled
applications comprising many tasks to achieve their
scientific goals. In this section, we verify this assumption

through a detailed investigation of workload traces taken
from real scientific computing environments.

3.1 Method and Experimental Setup
MTC workloads may comprise tens of
thousands to
hundreds of thousands of tasks and BoTs [4], and a typical
period may be one year or the whole trace. Our method for
identifying proto-MTC users—users with a pronounced
MTC-like workload, which are potential MTC users in the
future—in existing system workloads is based on the
identification of users with many submitted tasks and/or
bags-of-tasks in the workload traces taken from real
scientific computing infrastructures. We define an MTC
user to be a user that has submitted at least J jobs and at
least B bags-of-tasks. The user part of our definition serves
as a coupling between jobs, under the assumption that a
user submits jobs for execution toward an arbitrary but
meaningful goal. The jobs part ensures that we focus on
high-volume users; these users are likely to need new
scheduling techniques for good system performance. The
bag-of-tasks part ensures that
task submission occurs
within a short period of time; this submission pattern raises
new challenges in the area of task scheduling and manage-
ment [4]. Ideally, it should be possible to use a unique pair
of values for J and B across different systems.
To investigate the presence of an MTC component in
existing scientific computing infrastructures we analyze
ten workload traces. Table 2 summarizes the character-
istics of the ten traces; see [13], [16] for more details about
each trace. The ID of the trace indicates the system from
which it was taken. The traces have been collected from a
wide variety of grids and parallel production environ-
ments. The traces precede the existence of MTC tools;
thus, the presence of an MTC component in these traces
indicates the existence of proto-MTC users, who will be
likely to use today’s MTC-friendly environments.
To identify MTC users, we first formulate the identifica-
tion criterion by selecting values for J , B. If B  1, we first
identify the BoTs in the trace using the method that we
introduced in our previous work [22], that is, we use the
BoT identification information when it is present in the trace,
and identify BoTs as groups of tasks submitted by the same
user at and during short time intervals, otherwise. (We have
investigated the effect of the time frame in the identification
of BoTs in our previous work [22].) Then, we eliminate the
users that have not submitted at least B BoTs. Last, from the

934

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 22, NO. 6,

JUNE 2011

Fig. 1. Number of MTC users for the DAS-2 trace (a), and the San Diego Supercomputer Center (SDSC) SP2 trace (b) when considering only the
submitted BoT count criterion (left), and only submitted task count criterion (right).

remaining users we select the users that have submitted at
least J tasks.

select and use this complex criterion for the remainder of
this work.

3.2 Results
The number of MTC users decreases quickly with the
increase of J and B. Fig. 1 shows the results for our analysis
where we use the number of submitted BoTs (left), and the
number of submitted tasks (right) as criteria for identifying
MTC users for the DAS-2 (top) and SDSC SP2 (bottom)
traces. As expected, the number of MTC users identified in
the workload traces decreases as the number of submitted
BoTs/tasks increases. The number of MTC users identified
in the trace decreases much faster in the SDSC trace than in
the DAS-2 trace with the increase of the number of BoTs/
tasks. In addition, since there are not many MTC users for
large number of BoTs/tasks in PPI, we see evidence that
there is more MTC activity in grids than in PPI.
Expectedly, there is more MTC-like activity in grids
than in PPIs. To compare the MTC-like activity of grids and
PPIs we analyze for each trace the percentage of MTC jobs
from the total number of jobs, and the percentage of CPU
time consumed by MTC jobs from the total CPU time
consumption recorded in the trace. Table 3 presents the
results for various simple and complex criteria for all traces.
We use “number of BoTs submitted  100” and “number of
jobs submitted  1;000” as the simple criteria, and “number
of BoTs submitted  1;000 & number of tasks submitted
 10;000” as the complex criterion. Even for the simple
criteria, we observe that for PPIs, except for the LANL-O2K
trace, there are no MTC jobs for large values of B (the
number of BoTs). As the number of BoTs and tasks
increases, the percentage of MTC jobs and their consumed
CPU time decrease for both PPI and grids, as expected.
However, for the Grid3 and GLOW traces the MTC activity
is highly present even for large values of J and B. It turns
out that the complex criterion additionally selects mostly
users who submit many single-node tasks (not shown).
Since this type of proto-MTC workload has the potential to
execute well
in any environment,
including clouds, we

4 CLOUD PERFORMANCE EVALUATION

In this section, we present an empirical performance
evaluation of cloud computing services. Toward this end,
we run microbenchmarks and application kernels typical
for scientific computing on cloud computing resources, and
compare whenever possible the obtained results to the
theoretical peak performance and/or the performance of
other scientific computing systems.

4.1 Method
Our method stems from the traditional system benchmark-
ing. Saavedra and Smith [23] have shown that benchmark-
ing the performance of various system components with a
wide variety of microbenchmarks and application kernels
can provide a first order estimate of
that system’s
performance. Similarly, in this section we evaluate various
components of the four clouds introduced in Section 2.2.

TABLE 3
The Percentage of MTC Jobs, and the CPU Time Consumed by
These Jobs from the Total Number of Jobs and Consumed CPU
Time for All Traces, with Various Simple and Complex Criteria
for Identifying MTC Users

CPUT stands for Total CPU Time.

IOSUP ET AL.: PERFORMANCE ANALYSIS OF CLOUD COMPUTING SERVICES FOR MANY-TASKS SCIENTIFIC COMPUTING

935

TABLE 4
The Benchmarks Used for Cloud Performance Evaluation

TABLE 5
The VM Images Used in Our Experiments

B, FLOP, U, and PS stand for bytes, floating point operations, updates,
and per second, respectively.

However, our method is not a straightforward application
of Saavedra and Smith’s method. Instead, we add a cloud-
specific component, select several benchmarks for a
comprehensive platform-independent evaluation, and focus
on metrics specific to large-scale systems (such as efficiency
and variability).
Cloud-specific evaluation. An attractive promise of
clouds is that
they can always provide resources on
demand, without additional waiting time [20]. However,
since the load of other large-scale systems varies over time
due to submission patterns [5], [6] we want to investigate if
large clouds can indeed bypass this problem. To this end,
one or more instances of
the same instance type are
repeatedly acquired and released during a few minutes;
the resource acquisition requests follow a Poisson process
with arrival rate  ¼ 1 s 1 .
Infrastructure-agnostic evaluation. There currently is no
single accepted benchmark for scientific computing at large-
scale. To address this issue, we use several traditional
benchmark suites comprising microbenchmarks and (scien-
tific) application kernels. We further design two types of
test workloads: SI-run one or more single-process jobs on a
single instance (possibly with multiple cores), and MI-run a
single multiprocess job on multiple instances. The SI
workloads execute in turn one of the LMbench [33], Bonnie
[34], and CacheBench [35] benchmark suites. The MI work-
loads execute the HPC Challenge Benchmark (HPCC) [28]
scientific computing benchmark suite. The characteristics of
the used benchmarks and the mapping to the test work-
loads are summarized in Table 4; we refer to the bench-
marks’ references for more details.
Performance metrics. We use the performance metrics
defined by the benchmarks presented in Table 4. We also
define and use the HPL efficiency of a virtual cluster based
on the instance type T as the ratio between the HPL
benchmark performance of the real cluster and the peak
theoretical performance of a same-sized T -cluster, ex-
pressed as a percentage. Job execution at large-scale often
leads to performance variability. To address this problem,
in this work we report not only the average performance,
but also the variability of the results.

4.2 Experimental Setup
We now describe the experimental setup in which we use
the performance evaluation method presented earlier.
Performance Analysis Tool. We have recently [36]
extended the GrenchMark [37]
large-scale distributed
testing framework with new features which allow it to test

cloud computing infrastructures. The framework was
already able to generate and submit both real and synthetic
workloads to grids, clusters, clouds, and other large-scale
distributed environments. For this work, we have added to
GrenchMark the ability to execute and analyze the bench-
marks described in the previous section.
Environment. We perform our measurements on homo-
geneous virtual environments built from virtual resources
belonging to one of the instance types described in Table 1;
the used VM images are summarized in Table 5. The
experimental environments comprise from 1 to 128 cores.
Except for the use of internal IP addresses, described below,
we have used in a l l our exper iments the standard
configurations provided by the cloud. Due to our choice
of benchmarks, our Single-Job results can be readily
compared with the benchmarking results made public for
many other scientific computing systems, and in particular
by the HPCC effort [38].
MPI library and network. The VM images used for the
HPCC benchmarks also have a working preconfigured MPI
based on the mpich2-1.0.5 [39] implementation. For the
MI (parallel) experiments, the network selection can be
critical for achieving good results. Amazon EC2 and GoGrid,
the two clouds for which we have performed MI experiments,
use internal IP addresses (IPs), that is, the IPs accessible only
within the cloud, to optimize the data transfers between
closely-located instances. (This also allows the clouds to
better shape the traffic and to reduce the number of Internet-
accessible IPs, and in turn to reduce the cloud’s operational
costs.) EC2 and GoGrid give strong incentives to their
customers to use internal IP addresses, in that the network
traffic between internal IPs is free, while the traffic to or from
the Internet IPs is not. We have used only the internal IP
addresses in our experiments with MI workloads.
Optimizations, tuning. The benchmarks were compiled
using GNU C/C++ 4.1 with the -O3 -funroll-loops
command-line arguments. We did not use any additional
architecture- or instance-dependent optimizations. For the
HPL benchmark, the performance results depend on two
main factors: the Basic Linear Algebra Subprogram (BLAS)
[40]
library, and the problem size. We used in our
experiments the GotoBLAS [41] library, which is one of
the best portable solutions freely available to scientists.
Searching for the problem size that can deliver peak
performance is an extensive (and costly) process. Instead,
we used a free analytical tool [42] to find for each system the
problem sizes that can deliver results close to the peak
performance; based on the tool advice we have used values
from 13,000 to 110,000 for N,
the size (order) of
the
coefficient matrix A [28], [43].

936

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 22, NO. 6,

JUNE 2011

TABLE 6
Statistics for Single Resource Allocation/Release

Fig. 2. Resource acquisition and release overheads for acquiring single
EC2 instances.

4.3 Results
The experimental results of the Amazon EC2 performance
evaluation are presented in the following.

4.3.1 Resource Acquisition and Release
We study two resource acquisition and release scenarios:
for single instances, and for multiple instances allocated at
once.
Single instances. We first repeat 20 times for each instance
type a resource acquisition followed by a release as soon as
the resource status becomes installed (see Section 2.2). Fig. 2
shows the overheads associated with resource acquisition
and release in EC2. The total resource acquisition time (Total)
is the sum of the Install and Boot times. The Release time is the
time taken to release the resource back to EC2; after it is
released the resource stops being charged by Amazon. The
c1: instances are surprisingly easy to obtain; in contrast, the
m1: instances have for the resource acquisition time higher
expectation (63-90 s compared to around 63 s) and variability
(much larger boxes). With the exception of the occasional
outlier, both the VM Boot and Release times are stable and
represent about a quarter of Total each. Table 6 presents basic
statistics for single resource allocation and release. Overall,
Amazon EC2 has one order of magnitude lower single
resource allocation and release durations than GoGrid.
From the EC2 resources, the m1.small and m1.large
instances have higher average allocation duration, and
exhibit outliers comparable to those encountered for GoGrid.
The resource acquisition time of GoGrid resources is
highly variable; here, GoGrid behaves similarly to grids [5]
and unlike the promise of clouds.
Multiple instances. We investigate next
the perfor-
mance of requesting the acquisition of multiple resources
(2, 4, 8, 16, and 20) at the same time; a scenario common for
creating homogeneous virtual clusters. When resources
are requested in bulk, we record acquisition and release
times for each resource in the request, separately. Fig. 3
shows the basic statistical properties of the times recorded
for c1.xlarge instances. The expectation and the
variance are both higher for multiple instances than for
a single instance.

4.3.2 Single-Machine Benchmarks
In this set of experiments, we measure the raw performance
of the CPU, I/O, and memory hierarchy using the Single-
Instance benchmarks listed in Section 4.1. We run each
benchmark 10 times and report the average results.
Compute performance. We assess the computational
performance of each instance type using the entire
LMbench suite. The performance of int and int64 opera-
tions, and of the float and double-precision float operations
is depicted in Figs. 4a and 4b, respectively. The GOPS
recorded for the floating point and double-precision float
operations is six to eight
times lower than the theoretical
maximum of ECU (4.4 GOPS). A potential reason for this
situation is the overrun or thrashing of the memory caches
by the working sets of other applications sharing the same
physical machines; a study independent from ours [44]
identifies the working set size as a key parameter to
consider when placing and migrating applications on
virtualized servers. This situation occurs especially when
the physical machines are shared among users that are
unaware of each other; a previous study [45] has found that
even instances of the same user may be located on the same
physical machine. The performance evaluation results also
indicate that the double-precision float performance of the
c1: instances, arguably the most important for scientific
computing, is mixed: excellent addition but poor multi-
plication capabilities. Thus, as many scientific computing
applications use heavily both of these operations, the user is
faced with the difficult problem of selecting between two

Fig. 3. Single-instance resource acquisition and release overheads
when acquiring multiple c1.xlarge instances at the same time.

IOSUP ET AL.: PERFORMANCE ANALYSIS OF CLOUD COMPUTING SERVICES FOR MANY-TASKS SCIENTIFIC COMPUTING

937

Fig. 4. LMbench results (a) for the EC2 instances, and (b) for the other instances. Each row depicts the performance of 32 and 64-bit integer
operations in giga-operations per second (GOPS) (left), and of floating operations with single and double precision (right).

EC2 and GoGrid instances when running the Single-Job-
Multimachine benchmarks. For these tests we execute 5 times
the HPCC benchmark on homogeneous clusters of 1-16 (1-8)
instances on EC2 (GoGrid), and present the average results.
HPL performance. The performance achieved for the HPL
benchmark on var ious v ir tua l c lus ters based on the
m1.small and c1.xlarge instance types is depicted in
Fig. 5. For the m1.small resources one node was able to
achieve a performance of 1.96 GFLOPS, which is 44.54 percent
from the peak performance advertised by Amazon. Then, the
performance increased to up to 27.8 GFLOPS for 16 nodes,
while the efficiency decreased slowly to 39.4 percent.
The results for a single c1.xlarge instance are better: the
achieved 49.97 GFLOPS represent 56.78 percent from the
advertised peak performance. However, while the perfor-
mance scales when running up to 16 instances to 425.82
GFLOPS, the efficiency decreases to only 30.24 percent. The
HPL performance loss from one to 16 instances can, therefore,
be expressed as 53.26 percent which results in rather bad
qualification for HPC applications and their need for fast

TABLE 7
The I/O of Clouds versus 2002 [25] and 2007 [26] Systems

wrong choices. Finally, several double and float operations
take longer on c1.medium than on m1.small. For the
other instances, EH: and Mosso: instances have similar
performance for both integer and floating point operations.
GG: instances have the best float and double-precision
performance, and good performance for integer operations,
which suggests the existence of better hardware support for
these operations on these instances.
I/O performance. We assess in two steps the I/O
performance of each instance type with the Bonnie bench-
marking suite. The first step is to determine the smallest file
size that invalidates the memory-based I/O cache, by
running the Bonnie suite for thirteen file sizes in the range
1,024 Kilo-binary byte (KiB) to 40 GiB. The results of this
preliminary step have been summarized in a technical report
[46, pp. 11-12]; we only summarize them here. For all
instance types, a performance drop begins with the 100 MiB
test file and ends at 2 GiB, indicating a capacity of the
memory-based disk cache of 4-5 GiB (twice 2 GiB). Thus, the
results obtained for the file sizes above 5 GiB correspond to
the real I/O performance of the system; lower file sizes
would be served by the system with a combination of
memory and disk operations. We analyze the I/O perfor-
mance obtained for files sizes above 5 GiB in the second step;
Table 7 summarizes the results. We find that the I/O
performance indicated by Amazon EC2 (see Table 1)
corresponds to the achieved performance for random I/O
operations (column ’Rand. Input’ in Table 7). The  :xlarge
instance types have the best I/O performance from all
instance types. For the sequential operations more typical to
scientific computing all Amazon EC2 instance types have in
general better performance when compared with similar modern
commodity systems, such as the systems described in the last
three rows in Table 7; EC2 may be using better hardware,
which is affordable due to economies of scale [20].

4.3.3 Multimachine Benchmarks
In this set of experiments, we measure the performance
delivered by homogeneous clusters formed with Amazon

938

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 22, NO. 6,

JUNE 2011

internode communication. We have obtained similar results
the GG.large and GG.xlarge instances, as shown in Fig. 5.
For GG.large instances, the efficiency decreases quicker
than for EC2 instances, down to 47.33 percent while achieving
45.44 GFLOPS on eight instances. The GG.xlarge per-
formed even poorer in our tests. We further investigate the
performance of the HPL benchmark for different instance
types; Table 8 summarizes the results. The efficiency results
presented in Fig. 5 and Table 8 place clouds below existing
environments for scientific computing, for which the achieved
performance is 60-70 percent of the theoretical peak even for
demanding real applications [47], [48], [49].
HPCC performance. To obtain the performance of virtual
EC2 and GoGrid clusters we run the HPCC benchmarks on
unit clusters comprising a single instance, and on 128-core
clusters comprising 16 c1.xlarge instances. Table 9 sum-
marizes the obtained results and, for comparison, results
published by HPCC for four modern and similarly-sized
HPC clusters [38]. For HPL, only the performance of the
c1.xlarge is comparable to that of an HPC system.
However, for STREAM, and RandomAccess the performance
of the EC2 clusters is similar or better than the performance of
the HPC clusters. We attribute this mixed behavior mainly to

TABLE 8
HPL Performance and Cost Comparison for Various EC2 and
GoGrid Instance Types

the network characteristics: first, the EC2 platform has much
higher latency, which has an important negative impact on
the performance of the HPL benchmark; second, the network
is shared among different users, a situation which often leads
to severe performance degradation [50]. In particular, this
relatively low network performance means that the ratio
between the theoretical peak performance and achieved HPL
performance increases with the number of instances, making
the virtual EC2 clusters poorly scalable. Thus, for scientific
computing applications similar to HPL the virtual EC2
clusters can lead to an order of magnitude lower performance

Fig. 5. The HPL (LINPACK) performance of virtual clusters formed with EC2 m1.small, EC2 c1.xlarge, GoGrid large, and GoGrid xlarge
instances. (left) Throughput. (right) Efficiency.

TABLE 9
The HPCC Performance for Various Platforms

HPCC-x is the system with the HPCC ID x [38]. The machines HPCC-224 and HPCC-227, and HPCC-286 and HPCC-289 are of brand TopSpin/
Cisco and by Intel Endeavor, respectively. Smaller values are better for the Latency column and worse for the other columns.

IOSUP ET AL.: PERFORMANCE ANALYSIS OF CLOUD COMPUTING SERVICES FOR MANY-TASKS SCIENTIFIC COMPUTING

939

call these infrastructures source environments. Then, we
compare these characteristics with those of a cloud execution.
System model. We define two performance models of
clouds, which differ by the factor that jobs are slowed
down. The cloud with source-like performance is a theoretical
cloud environment that comprises the same resources as the
source environment. In this cloud model, the runtimes of
jobs executed in the cloud are equal to those recorded in the
source environment’s workload traces (no slowdown). This
model is akin to having a grid being converted into a cloud
of identical performance and thus it is useful for assessing
the theoretical performance of future and more mature
clouds. However, as we have shown in Section 4, in real
clouds performance is below the theoretical peak, and for
parallel
jobs the achieved efficiency is lower than that
achieved in grids. Thus, we introduce the second model, the
clouds with real performance, in which the runtimes of jobs
executed in the cloud are extended by a factor, which we
call
the slowdown factor, derived from the empirical
evaluation presented in Section 4. The system equivalence
between clouds and source environments is assumed in this
model, and ensured in practice by the complete system
virtualization [53] employed by all the clouds investigated
in this work.
Job execution model. For job execution, we assume
exclusive resource use:
for each job in the trace,
the
necessary resources are acquired from the cloud, then
released after the job has been executed. We relax this
assumption in Section 5.3.4.
System workloads. To compare the performance of
clouds with other infrastructures, we use both complete
workloads, and MTC workloads extracted from the com-
plete workloads using the method described in Section 3.1.
Finally, we evaluate the performance and the cost of
executing MTC workloads in clouds with real performance
for various slowdown factors.
Performance metrics. We measure the performance of all
environments using the three traditional metrics [7]: wait time
(WT), response time (ReT), and bounded slowdown (BSD)—the
ratio between the job response time in the real versus an
exclusively—used environment, with a bound that elimi-
nates the bias toward short jobs. The BSD is expressed as
BSD ¼ maxð1; ReT =maxð10; ReT   W T ÞÞ, where 10 is the
bound that eliminates the bias of jobs with runtime below
10 s. We compute for each job the three metrics and report for
a complete workload the average values for these metrics,
AWT, AReT, and ABSD, respectively.
Cost metrics. We report for the two cloud models the
total cost of workload execution, defined as the number of
instance-hours used to complete all the jobs in the work-
load. This value can be converted directly into the cost for
executing the whole workload for $/CPU hour and similar
pricing models, such as Amazon EC2’s.

5.2 Experimental Setup
System setup. We use the DGSIM simulator [18] to analyze
the performance of cloud environments. We have extended
DGSIM with the two cloud models, and used it to simulate
the execution of real scientific computing workloads on
cloud computing infrastructures. To model the slowdown
of jobs when using clouds with real performance, we have

F ig. 6. Per formance stab i l i ty of c loud instance types w i th the
CacheBench benchmark with Rd-Mod-Wr operations.

for large system sizes (1,024 cores and higher). An alternative
explanation may be the working set size of HPL, which would
agree with the findings of another study on resource
virtualization [44]. The performance of the GoGrid clusters
with the single core instances is as expected, but we observe
scalability problems with the 3 core GG.xlarge instances. In
comparison with previously reported results, the DGEMM
performance of m1.large (c1.medium) instances is similar
to that of Altix4700 (ICE) [29], and the memory bandwidth of
Cray X1 (2003) is several times faster than that of the fastest
cloud resource currently available [30].

4.3.4 Performance Stability
An important question related to clouds is Is the performance
stable? (Are our results repeatable?) Previous work on
virtualization has shown that many virtualization packages
deliver the same performance under identical tests for
virtual machines running in an isolated environment [51].
However, it is unclear if this holds for virtual machines
running in a large-scale cloud (shared) environment.
To get a first picture of the side effects caused by the
sharing with other users the same physical resource, we have
assessed the stability of different clouds by running the
LMBench (computation and OS) and CacheBench (I/O)
benchmarks multiple times on the same type of virtual
resources. For these experiments we have used m1.xlarge,
GG.xlarge, EH.small, and Mosso.large resources.
Fig. 6 summarizes the results for one example benchmark
from the CacheBench suite, Rd-Mod-Wr. The GG.large and
EH.small types have important differences between the
min, mean, and max performance even for medium working
set sizes, such as 1010B. The best performer in terms of
computation, GG.xlarge, is unstable; this makes cloud
vendor selection an even more difficult problem. We have
performed a longer-term investigation in other work [52].

5 CLOUDS VERSUS OTHER SCIENTIFIC COMPUTING
INFRASTRUCTURES

In this section, we present a comparison between clouds
and other scientific computing infrastructures using both
complete workloads, and MTC workloads extracted from
the complete workloads.

5.1 Method
We use trace-based simulation to compare clouds with
scientific computing infrastructures. To this end, we first
extract the performance characteristics from long-term
workload traces of scientific computing infrastructures; we

940

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 22, NO. 6,

JUNE 2011

TABLE 10
The Results of the Comparison between Workload Execution in Source Environments (Grids, PPIs, etc.) and in Clouds

The “-” sign denotes missing data in the original traces. For the two Cloud models AW T ¼ 80 s (see text). The total cost for the two Cloud models is
expressed in millions of CPU hours.

used different slowdown factors. Specifically, single-pro-
cessor jobs are slowed down by a factor of 7, which is the
ave rage pe r fo rmance ra t io be tween theo re t ica l and
achieved performance analyzed in Section 4.3.2, and
parallel
jobs are slowed down by a factor up to 10
depending on the job size, due to the HPL performance
degradation with job size described in Section 4.3.3. In
Section 5.3.3, we also present the results of our performance
evaluation by using various slowdown factors with the
cloud real performance model.
Workload setup. We use as input workload the ten
workload traces described in Section 3. The traces Grid3
and LCG do not include the job waiting time information;
only for these two traces we set the waiting time for all jobs
to zero, which favors these two grids in comparison with
clouds. The wait time of jobs executed in the cloud (also
their AWT) is set to the resource acquisition and release
time obtained from real measurements (see Section 4.3.1).
Performance analysis tools. We use the Grid Workloads
Archive tools [13] to extract the performance metrics from
the workload traces of grids and PPIs. We extend these tools
to also analyze cloud performance metrics such as cost.

5.3 Results
Our experiments follow four main aspects: performance for
complete and MTC-only workloads, the effect of cloud
performance changes on performance and cost metrics, and
the performance-cost-security trade-off. We present
the
experimental results for each main aspect, in turn.

5.3.1 Complete Workloads
We compare the execution in source environments (grids,
PPIs, etc.) and in clouds of
the ten workload traces
described in Table 2. Table 10 summarizes the results of
this comparison, on which we comment below.
An order of magnitude better performance is needed
for clouds to be useful for daily scientific computing. The
performance of the cloud with real performance model is
insufficient to make a strong case for clouds replacing
grids and PPIs as a scientific computing infrastructure.
The response time of these clouds is higher than that of
the source environment by a factor of 4-10. In contrast, the
response time of the clouds with source-like performance
is much better, leading in general to significant gains (up to
80 percent faster average job response time) and at worst to

1 percent higher AWT (for traces of Grid3 and LCG, which
are assumed conservatively to always have zero waiting
time1). We conclude that if clouds would offer an order of
magnitude higher performance than the performance
observed in this study, they would form an attractive
alternative for scientific computing, not considering costs.
Price wise, clouds are reasonably cheap for scientific
computing, if the usage and funding scenarios allow it (but
usually they do not). Looking at costs, and assuming the
external operational costs in the cloud to be zero, one million
EC2-hours equate to $100,000. Thus, to execute the total
workload of RAL over one year (12 months) would cost
$4,029,000 on Amazon EC2. Similarly, the total workload of
DAS-2 over one year and a half (18 months) would cost
$166,000 on Amazon EC2. Both these sums are much lower
than the cost of these infrastructures, which includes
resource acquisition, operation, and maintenance. To better
understand the meaning of these sums, consider the scenario
(disadvantageous for the clouds) in which the original
systems would have been sized to accommodate strictly the
average system load, and the operation and maintenance
costs would have been zero. Even in this scenario using
Amazon EC2 is cheaper. We attribute this difference to the
economy of scale discussed in a recent study [20]: the price of
the basic operations in a very large data center can be an order
of magnitude lower than in a grid or data center of regular
size. However, despite the apparent cost saving it is not clear
that the transition to clouds would have been possible for
either of these grids. Under the current performance
exhibited by clouds, the use of EC2 would have resulted in
response times three to four times higher than in the original
system, which would have been in conflict with the mission
of RAL as a production environment. A similar concern can
be formulated for DAS-2. Moreover, DAS-2 is specifically
targeting research in computer science, and its community
would not have been satisfied to use commodity resources
instead of a state-of-the-art environment comprising among
others high-performance lambda networks; other new
resource types, such as GPUs and Cell processors, are
currently available in grids but not in clouds. Looking at
the funding scenario, it is not clear if finance could have been

1. Although we realize the Grid3 and LCG grids do not have zero
waiting time, we follow a conservative approach in which we favor grids
against clouds, as the latter are the new technology.

IOSUP ET AL.: PERFORMANCE ANALYSIS OF CLOUD COMPUTING SERVICES FOR MANY-TASKS SCIENTIFIC COMPUTING

941

TABLE 11
The Results of the Comparison between Workload Execution in Source Environments (Grids, PPIs, etc.)
and in Clouds with only the MTC Part Extracted from the Complete Workloads

The LCG, CTC SP2, SDSC SP2, and SDSC DS traces are not presented, as they do not have enough MTC users (the criterion is described in text).

secured for virtual resources; one of the main outcomes of the
long-running EGEE project is the creation of a European Grid
infrastructure. Related concerns have been formulated else-
where [20].
Clouds are now a viable alternative for short deadlines.
A low and steady job wait time leads to much lower
(bounded) slowdown for any cloud model, when compared
to the source environment. The average bounded slowdown
(ABSD, see Section 5.1) observed in real grids and PPIs is for
our traces between 11 and over 500!, but below 3.5 and even
1.5 for the cloud models with low and with high utilization.
The meaning of the ABSD metric is application specific, and
the actual ABSD value may seem to overemphasize the
difference between grids and clouds. However, the pre-
sence of high and unpredictable wait times even for short
jobs, captured here by the high ABSD values, is one of the
major concerns in adopting shared infrastructures such as
grids [5], [54]. We conclude that cloud is already a viable
alternative for scientific computing projects with tight
deadline and few short-running jobs remaining,
if the
project has the needed funds.

5.3.2 MTC Part of the Complete Workloads
We evaluate the performance of clouds using only the MTC
workloads extracted from the complete workloads using
the method described in Section 3.1. We assume that a user
is an MTC user if B  1;000 and J  10;000; T is considered
to be the duration of
the workload trace. Table 11
summarizes the results of our evaluation. The results are
similar to the results obtained for complete workloads, in
the previous section. We observe that the response time of
clouds with real performance is higher than that of grids/
PPIs by a factor of 2-5. Hence, although the cost of using

clouds seems reasonable, significant performance improve-
ment is needed for clouds to be a viable alternative to
grids/PPIs for MTC-based scientific computing. In addi-
tion, similar to results for complete workloads, we observe
low and steady wait times hence lower ABSD, and reduced
time to solution which makes clouds attractive for MTC-
based scientific computing.

5.3.3 The Effect of the Cloud Slowdown Factor on
Performance and Cost
The slowdown factor is the factor by which the job runtime
changes between the source environment and the cloud (see
Section 5.1). In previous sections, we have used a slowdown
factor of 7 for sequential jobs, and 10 for parallel jobs for
modeling clouds with real performance. We now evaluate
the performance of clouds with real performance using only
the MTC workloads with various slowdown factors for both
sequential and parallel jobs. Similar to previous section,
when extracting the MTC workload from complete work-
loads we assume that a user is an MTC user if B  1;000
and J  10;000.
Fig. 7 shows the average response time and cost of
clouds with real performance with various slowdown
factors for sequential (Fig. 7a) and parallel (Fig. 7b) jobs
using the DAS-2 trace. As the slowdown factor increases,
we observe a steady but slow increase in cost and response
time for both sequential and parallel jobs. This is expected:
the higher the response time, the longer a cloud resource is
used, increasing the total cost. The sequential jobs dominate
the workload both in number of jobs and in consumed CPU
time, and their average response time increases linearly
improving the
thus,
with the performance slowdown;

Fig. 7. Performance and cost of using cloud resources for MTC workloads with various slowdown factors for sequential jobs (a), and parallel jobs
(b) using the DAS-2 trace.

942

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 22, NO. 6,

JUNE 2011

TABLE 12
Relative Strategy Performance: Resource Bulk Allocation (S2)
versus Resource Acquisition and Release per Job (S1)

Only performance differences above 5 percent are shown.

performance of clouds for sequential jobs should be the first
priority of cloud providers.

5.3.4 Performance and Security versus Cost
Currently, clouds lease resources but do not offer a resource
management service that can use the leased resources. Thus,
the cloud adopter may use any of the resource management
middleware from grids and PPIs; for a review of grid
middleware we refer to our recent work [55]. We have
already introduced the basic concepts of cloud resource
management in Section 4.2, and explored the potential of a
cloud resource management strategy (strategy S1) for which
resources are acquired and released for each submitted job
in Section 5. This strategy has good security and resource
setup flexibility, but may incur high time and cost over-
heads, as resources that could otherwise have been reused
are released as soon as the job completes. As an alternative,
we investigate now the potential of a cloud resource
management strategy in which resources are allocated in
bulk for all users, and released only when there is no job left
to be served (strategy S2). To compare these two cloud
resource management strategies, we use the experimental
setup described in Section 5.2; Table 12 shows the obtained
results. The maximum relative cost difference between the
strategies is for these traces around 30 percent (the DAS-2
trace); in three cases, around 10 percent of the total cost is to
be gained. Given these cost differences, we raise as a future
research problem optimizing the application execution as a cost-
performance-security trade-off.

6 RELATED WORK

In this section, we review related work from three areas:
clouds, virtualization, and system performance evaluation.
Our work also comprises the first characterization of the
MTC component in existing scientific computing workloads.
Performance evaluation of clouds and virtualized
environments. There has been a recent spur of research
activity in assessing the performance of virtualized re-
sources, in cloud computing environments [9], [10], [11],
[56], [57] and in general [8], [24], [51], [58], [59], [60], [61]. In
contrast to this body of previous work, ours is different in
scope: we perform extensive measurements using general
purpose and high-performance computing benchmarks to
compare several clouds, and we compare clouds with other
environments based on real long-term scientific computing
traces. Our study is also much broader in size: we perform
in this work an evaluation using over 25 individual
benchmarks on over 10 cloud instance types, which is an
order of magnitude larger than previous work (though size
does not simply add to quality).
Performance studies using general purpose benchmarks
have shown that the overhead incurred by virtualization
can be below 5 percent for computation [24], [51] and below
15 percent
for networking [24] ,
[58] . Similarly ,
the

performance loss due to virtualization for parallel I/O
and web server I/O has been shown to be below 30 [62]
and 10 percent [63], [64], respectively. In contrast to these,
our work shows that virtualized resources obtained from
public clouds can have a much lower performance than the
theoretical peak.
Recently, much interest for the use of virtualization has
been shown by the HPC community, spurred by two
seminal studies [8], [65] that find virtualization overhead to
be negligible for compute-intensive HPC kernels and
applications such as the NAS NPB benchmarks; other
studies have investigated virtualization performance for
specific HPC application domains [61], [66], or for mixtures
of Web and HPC workloads running on virtualized
(shared) resources [67]. Our work differs significantly from
these previous approaches in target (clouds as black boxes
versus owned and controllable infrastructure) and in size.
For clouds, the study of performance and cost of executing a
scientific workflow, Montage, in clouds [9] investigates cost
performance trade-offs between clouds and grids, but uses
a single application on a single cloud, and the application
itself
is remote from the mainstream HPC scientific
community. Also close to our work is the seminal study
of Amazon S3 [10], which also includes a performance
evaluation of file transfers between Amazon EC2 and S3.
Our work complements this study by analyzing the
performance of Amazon EC2, the other major Amazon
cloud service; we also test more clouds and use scientific
workloads. Several small-scale performance studies of
Amazon EC2 have been recently conducted: the study of
Amazon EC2 performance using the NPB benchmark suite
[11] or selected HPC benchmarks [68], the early compara-
tive study of Eucalyptus and EC2 performance [56], the
study of file transfer performance between Amazon EC2
and S3 [69], etc. An early comparative study of
the
DawningCloud and several operational models [12] extends
the comparison method employed for Eucalyptus [56], but
uses job emulation instead of job execution. Our perfor-
mance evaluation results extend and complement these
previous findings, and gives more insights into the
performance of EC2 and other clouds.
Other (early) performance evaluation. Much work has
been put into the evaluation of novel supercomputers [27],
[29], [30], [31], [47], [48] and nontraditional systems [5], [32],
[37], [49], [70] for scientific computing. We share much of
the used methodology with previous work; we see this as
an advantage in that our results are readily comparable
with existing results. The two main differences between this
body of previous work and ours are that we focus on a
different platform (that is, clouds) and that we target a
broader scientific computing community (e.g., also users of
grids and small clusters).
Other cloud work. Recent work [12], [71] has considered
running mixtures of MTC with other workloads in cloud-
like environments. For this direction of research, our
findings can be seen as further motivation and source of
realistic setup parameters.

7 CONCLUSION AND FUTURE WORK

With the emergence of cloud computing as a paradigm in
which scientific computing can done exclusively on
resources leased only when needed from big data centers,

IOSUP ET AL.: PERFORMANCE ANALYSIS OF CLOUD COMPUTING SERVICES FOR MANY-TASKS SCIENTIFIC COMPUTING

943

e-scientists are faced with a new platform option. However,
the initial target workloads of clouds does not match the
characteristics of MTC-based scientific computing work-
loads. Thus, in this paper we seek to answer the research
question Is the performance of clouds sufficient for MTC-based
scientific computing? To this end, we first investigate the
in existing scientific
presence of an MTC component
computing workloads, and find that
this presence is
significant both in number of
jobs and in resources
consumed. Then, we perform an empirical performance
evaluation of four public computing clouds,
including
Amazon EC2, one of
the largest commercial clouds
currently in production. Our main finding here is that the
compute performance of the tested clouds is low. Last, we
compare the performance and cost of clouds with those of
scientific computing alternatives such as grids and parallel
production infrastructures. We find that, while current
cloud computing services are insufficient
for scientific
computing at large, they may still be a good solution for
the scientists who need resources instantly and temporarily.
We will extend this work with additional analysis of the
other services offered by clouds, and in particular storage and
network; how do they respond to to the combined stress of
workloads with different characteristics and requirements
that the diverse population of cloud users are supposed to
incur in the future? We will also extend the performance
evaluation with other real and synthetic applications, toward
creating a performance database for the scientific community.
Standing the test of time. The usefulness of our
empirical evaluation part of this work (Section 4.3) may
be reduced with the commercialization of new cloud
services. For example, since mid-July 2010 a new commer-
cial compute service by Amazon, the Cluster Compute
instances,
is targeted at HPC users. The increase in
performance for this new service versus the Amazon
instances tested in our work can be up to a factor of 8.5
[72], which is similar to the performance gap found by our
performance evaluation. The difference in performance for
the Cluster Compute instances cannot be explained only by
the superior resource performance—the compute perfor-
mance of the Cluster Compute instances is, for example,
only a factor of 1.5 times better than that of the best-
performing instance tested in our study. Another possible
contributor may be that
the new instance type offers
dedicated infrastructure (that is, compute and network
resources). Thus, these cloud instances are operated in a
“shared-nothing” mode; historically, such clusters tend to
have low utilization [73], which in turn threatens to cancel
out the commercial benefits. Our performance evaluation
results remain representative for clouds that multiplex their
resources among their users, at least until an isolation
technology is able to limit access to compute, memory,
network, and I/O resources with low overhead; recent yet
early attempts in this direction, such as the Linux containers
[74], are promising. Our performance evaluation may also
be indicative, as a cross-section analysis of the offerings
available on the market, for the differences between the
cloud operators present on the market at any given time.

ACKNOWLEDGMENTS

This work is partially funded by the European Union under
grant agreement number 261585/SHIWA Project.

[4]

REFERENCES
[1] Amazon, Inc., “Amazon Elastic Compute Cloud (Amazon EC2),”
http://aws.amazon.com/ec2/, Dec. 2008.
[2] GoGrid, “GoGrid Cloud-Server Hosting,” http://www.gogrid.
com, Dec. 2008.
[3] A. Iosup, O.O. Sonmez, S. Anoep, and D.H.J. Epema, “The
Performance of Bags-of-Tasks in Large-Scale Distributed Sys-
tems,” Proc. ACM Int’l Symp. High Performance Distributed
Computing (HPDC), pp. 97-108, 2008.
I. Raicu, Z. Zhang, M. Wilde, I.T. Foster, P.H. Beckman, K. Iskra,
and B. Clifford, “Toward Loosely Coupled Programming on
Petascale Systems,” Proc. ACM Conf. Supercomputing (SC), p. 22,
2008.
[5] A. Iosup, C. Dumitrescu, D.H.J. Epema, H. Li, and L. Wolters,
“How Are Real Grids Used? The Analysis of Four Grid Traces and
Its Implications,” Proc. IEEE Seventh Int’l Conf. Grid Computing,
pp. 262-269, 2006.
[6] U. Lublin, D.G. Feitelson, “Workload on Parallel Supercomputers:
Modeling Characteristics of Rigid Jobs,” J. Parallel and Distributed
Computing, vol. 63, no. 11, pp. 1105-1122, 2003.
[7] D.G. Feitelson, L. Rudolph, U. Schwiegelshohn, K.C. Sevcik, and
P. Wong, “Theory and Practice in Parallel Job Scheduling,” Proc.
Job Scheduling Strategies for Parallel Processing (JSSPP), pp. 1-34,
1997.
[8] L. Youseff, R. Wolski, B.C. Gorda, and C. Krintz, “Para-
virtualization for HPC Systems,” Proc .
ISPA Workshops ,
pp. 474-486, 2006.
[9] E. Deelman, G. Singh, M. Livny, J.B. Berriman, and J. Good, “The
Cost of Doing Science on the Cloud: The Montage Example,” Proc.
IEEE/ACM Supercomputing (SC), p. 50, 2008.
[10] M.R. Palankar, A.
Iamnitchi, M. Ripeanu, and S. Garfinkel,
“Amazon S3 for Science Grids: A Viable Solution?” Proc. DADC
’08: ACM Int’l Workshop Data-Aware Distributed Computing, pp. 55-
64, 2008.
[11] E. Walker, “Benchmarking Amazon EC2 for HP Scientific
Computing,” Login, vol. 33, no. 5, pp. 18-23, Nov. 2008.
[12] L. Wang, J. Zhan, W. Shi, Y. Liang, L. Yuan, “In Cloud, Do Mtc or
Htc Service Providers Benefit from the Economies of Scale?” Proc.
Second Workshop Many-Task Computing on Grids and Supercomputers
(SC-MTAGS), 2009.
[13] A. Iosup, H. Li, M. Jan, S. Anoep, C. Dumitrescu, L. Wolters, and
D. Epema, “The Grid Workloads Archive,” Future Generation
Computer Systems, vol. 24, no. 7, pp. 672-686, 2008.
[14] D. Thain, J. Bent, A.C. Arpaci-Dusseau, R.H. Arpaci-Dusseau, and
M. Livny, “Pipeline and Batch Sharing in Grid Workloads,” Proc.
IEEE 12th Int’l Symp. High Performance Distributed Computing
(HPDC), pp. 152-161, 2003.
[15] S. Ostermann, A. Iosup, R. Prodan, T. Fahringer, and D.H.J.
Epema, “On the Characteristics of Grid Workflows,” Proc. Work-
shop Integrated Research in Grid Computing (CGIW), pp. 431-442,
2008.
[16] The Parallel Workloads Archive Team, “The Parallel Workloads
A r c h i v e L o g s , ” h t t p : / /www . c s . h u j i . a c . i l / l a b s / p a r a l l e l /
workload/logs.html, Jan. 2009.
[17] Y.-S. Kee, H. Casanova, and A.A. Chien, “Realistic Modeling and
Svnthesis of Resources for Computational Grids,” Proc. ACM/IEEE
Conf. Supercomputing (SC), p. 54, 2004.
[18] A. Iosup, O.O. Sonmez, and D.H.J. Epema, “DGSim: Comparing
Grid Resource Management Architectures through Trace-Based
Simulation,” Proc. 14th Int’l Euro-Par Conf. Parallel Processing,
pp. 13-25, 2008.
[19] L. Youseff, M. Butrico, and D. DaSilva, “Towards a Unified
Ontology of Cloud Computing,” Proc. Grid Computing Environ-
ments Workshop (GCE ’08), Nov. 2008.
[20] M. Armbrust, A. Fox, R. Griffith, A.D. Joseph, R.H. Katz, A.
Konwinski, G. Lee, D.A. Patterson, A. Rabkin, I. Stoica, and M.
Zaharia, “Above the Clouds: A Berkeley View of Cloud Comput-
ing,” Technical Report UCB/EECS-2009-28, EECS Dept, Univ. of
California, Berkeley, Feb. 2009.
[21] R. Prodan and S. Ostermann, “A Survey and Taxonomy of
Infrastructure as a Service and Web Hosting Cloud Providers,”
Proc. Int’l Conf. Grid Computing, pp. 1-10, 2009.
[22] A.
Jan, O.O. Sonmez, and D.H.J. Epema, “The
Iosup, M.
Characteristics and Performance of Groups of Jobs in Grids,”
Proc. Euro-Par, pp. 382-393, 2007.

944

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 22, NO. 6,

JUNE 2011

[27]

[23] R.H. Saavedra, A.J. Smith, “Analysis of Benchmark Characteristics
and Benchmark Performance Prediction,” ACM Trans. Computer
Systems, vol. 14, no. 4, pp. 344-384, 1996.
[24] P. Barham, B. Dragovic, K. Fraser, S. Hand, T.L. Harris, A. Ho, R.
Neugebauer, I. Pratt, and A. Warfield, “Xen and the Art of
Virtualization,” Proc. 19th ACM Symp. Operating Systems Principles
(SOSP), pp. 164-177, 2003.
[25] A. Kowalski, “Bonnie—File System Benchmarks,” technical
r ep o r t ,
J e f f e r s o n L a b , h t tp :/ / c c . j l a b . o r g/d o c s/ s c i c omp/
benchmark/bonnie.html, Oct. 2002.
[26] M. Babcock, “XEN Benchmarks, Tech. Rep.,” mikebabcock.ca/
linux/xen/, Aug. 2007.
J.S. Vetter, S.R. Alam, T.H.D. Jr, M.R. Fahey, P.C. Roth, P.H.
Worley, “Early Evaluation of the Cray XT3,” Proc. 20th Int’l Conf.
Parallel and Distributed Processing Symp. (IPDPS), 2006.
[28] P. Luszczek, D.H. Bailey, J. Dongarra, J. Kepner, R.F. Lucas, R.
Rabenseifner, and D. Takahashi, “S12—The HPC Challenge
(HPCC) Benchmark Suite,” Proc. ACM Supercomputing Conf. High
Performance Networking and Computing (SC), p. 213, 2006.
[29] S. Saini, D. Talcott, D.C. Jespersen, M.J. Djomehri, H. Jin, and
R. Biswas, “Scientific Application-Based Performance Compar-
ison of SGI Altix 4700, IBM POWER5+, and SGI ICE 8200
Supercomputers,” Proc. IEEE/ACM Conf. Supercomputing (SC),
p. 7, 2008.
[30] T.H. Dunigan, M.R. Fahey, J.B.W. III, and P.H. Worley, “Early
Evaluation of the Cray X1,” Proc. IEEE/ACM Conf. Supercomputing
(SC), p. 18, 2003.
[31] S.R. Alam, R.F. Barrett, M. Bast, M.R. Fahey, J.A. Kuehn, C.
McCurdy, J. Rogers, P.C. Roth, R. Sankaran, J.S. Vetter, P.H.
Worley, and W. Yu, “Early Evaluation of IBM BlueGene/P,” Proc.
ACM Conf. Supercomputing (SC), p. 23, 2008.
[32] R. Biswas, M.J. Djomehri, R. Hood, H. Jin, C.C. Kiris, and S. Saini,
“An Application-Based Performance Characterization of
the
Columbia Supercluster,” Proc. IEEE Conf. Supercomputing (SC),
p. 26, 2005.
[33] L. McVoy and C. Staelin, “LMbench—Tools for Performance
Analysis,” http://www.bitmover.com/lmbench/, Dec. 2008.
[34] T. Bray, “Bonnie,” 1996, http://www.textuality.com/bonnie/,
Dec. 2008.
[35] P.J. Mucci and K.S. London, “Low Level Architectural Character-
ization Benchmarks for Parallel Computers,” Technical Report
UT-CS-98-394, Univ. of Tennessee, 1998.
[36] N. Yigitbasi, A. Iosup, S. Ostermann, and D. Epema, “C-Meter: A
Framework for Performance Analysis of Computing Clouds,”
Proc. IEEE Ninth Int’l Symp. Cluster Computing and the GRID
(CCGRID ’09), pp. 472-477, 2009.
[37] A. Iosup and D.H.J. Epema, “GrenchMark: A Framework for
Analyzing, Testing, and Comparing Grids,” Proc. IEEE Sixth Int’l
Symp. Cluster Computing and the GRID (CCGrid), pp. 313-320, 2006.
[38] The HPCC Team, “HPCChallenge Results,” http://icl.cs.utk.edu/
hpcc/hpcc_results.cgi, Jan. 2009.
J. Worringen and K. Scholtyssik, “MP-MPICH: User Documenta-
tion & Technical Notes,” June 2002.
J. Dongarra et al., “Basic Linear Algebra Subprograms Technical
Forum Standard,” Int’l.
J. High Performance Applications and
Supercomputing, vol. 16, no. 1, pp. 1-111, 2002.
[41] K. Goto, R.A.v.d. Geijn, “Anatomy of High-Performance Matrix
Multiplication,” ACM Trans. Math. Software, vol. 34, no. 3, pp. 1-25,
2008.
[42] Advanced Clustering Tech., “Linpack Problem Size Analyzer,”
http://www.advancedclustering.com/, Dec. 2008.
J. Dongarra, P. Luszczek, A. Petitet, “The Linpack Benchmark:
Past, Present and Future,” Concurrency and Computation: Practice
and Experience, vol. 15, no. 9, pp. 803-820, 2003.
[44] A. Verma, P. Ahuja, and A. Neogi, “Power-Aware Dynamic
Placement of Hpc Applications,” Proc. ACM 22nd Ann. Int’l Conf.
Supercomputing (ICS), pp. 175-184, 2008.
[45] M. Zaharia, A. Konwinski, A.D. Joseph, R.H. Katz, and I. Stoica,
“Improving Mapreduce Performance in Heterogeneous Environ-
ments,” Proc. Eighth USENIX Conf. Operating Systems Design and
Implementation (OSDI), pp. 29-42, 2008.
[46] S. Ostermann, A. Iosup, N.M. Yigitbasi, R. Prodan, T. Fahringer,
and D. Epema, “An Early Performance Analysis of Cloud
Computing Services for Scientific Computing,” Technical Report
PDS-2008-006, TU Delft, http://www.eecs.berkeley.edu/Pubs/
TechRpts/2009/EECS-2009-28.html, Dec. 2008.

[40]

[39]

[43]

[47] F. Petrini, D.J. Kerbyson, and S. Pakin, “The Case of the Missing
Supercomputer Performance: Achieving Optimal Performance on
the 8,192 Processors of ASCI Q,” Proc. IEEE/ACM Conf. Super-
computing (SC), p. 55, 2003.
[48] D.J. Kerbyson, A. Hoisie, H.J. Wasserman, “A Performance
Comparison between the Earth Simulator and Other Terascale
Systems on a Characteristic ASCI Workload,” Concurrency—
Practice and Experience, vol. 17, no. 10, pp. 1219-1238, 2005.
[49] F. Petrini, G. Fossum, J. Ferna´ ndez, A.L. Varbanescu, M. Kistler,
and M. Perrone, “Multicore Surprises: Lessons Learned from
Optimizing Sweep3D on the Cell Broadband Engine,” Proc. IEEE
Int’l Parallel and Distributed Processing Symp. (IPDPS), pp. 1-10,
2007.
[50] R.H. Arpaci-Dusseau, A.C. Arpaci-Dusseau, A. Vahdat, L.T. Liu,
T.E. Anderson, and D.A. Patterson, “The Interaction of Parallel
and Sequential Workloads on a Network of Workstations,” Proc.
ACM SIGMETRICS Joint Int’l Conf. Measurement and Modeling of
Computer Systems, pp. 267-278, 1995.
[51] B. Clark, T. Deshane, E. Dow, S. Evanchik, M. Finlayson, J. Herne,
and J.N. Matthews, “Xen and the Art of Repeated Research,” Proc.
USENIX Ann. Technical Conf. (ATC), pp. 135-144, 2004.
[52] A. Iosup, N.M. Yigitbasi, and D. Epema, “On the Performance
Variability of Production Cloud Services,” Technical Report PDS-
2010-002, TU Delft, http://pds.twi.tudelft.nl/reports/2010/PDS-
2010-002.pdf. Jan. 2010.
[53] T. Killalea, “Meet the Virts,” Queue, vol. 6, no. 1, pp. 14-18, 2008.
[54] D. Nurmi, R. Wolski, and J. Brevik, “Varq: Virtual Advance
Reservations for Queues,” Proc. ACM 17th Int’l Symp. High
Performance Distributed Computing (HPDC), pp. 75-86, 2008.
[55] A. Iosup, D.H.J. Epema, T. Tannenbaum, M. Farrellee, and M.
Livny, “Inter-Operating Grids through Delegated Matchmaking,”
Proc. ACM/IEEE Conf. Supercomputing (SC), p. 13, 2007.
[56] D. Nurmi, R. Wolski, C. Grzegorczyk, G. Obertelli, S. Soman, L.
Youseff, and D. Zagorodnov, “The Eucalyptus Open-Source
Cloud-Computing System,” Technical Report 2008-10, uCSD,
http://eucalyptus.cs.ucsb.edu/, 2008.
[57] B. Que´ tier, V. Ne´ ri, F. Cappello, “Scalability Comparison of Four
Host Virtualization Tools,” J. Grid Computing, vol. 5, pp. 83-98,
2007.
[58] A. Menon, J.R. Santos, Y. Turner, G.J. Janakiraman, and W.
Zwaenepoel, “Diagnosing Performance Overheads in the Xen
Virtual Machine Environment,” Proc. ACM First Int’l Conf. Virtual
Execution Environments (VEE), pp. 13-23, 2005.
[59] N. Sotomayor, K. Keahey, and I. Foster, “Overhead Matters: A
Model for Virtual Resource Management,” Proc. IEEE First Int’l
Workshop Virtualization Technology in Distributed Technology
(VTDC), pp. 4-11, 2006.
[60] A.B. Nagarajan, F. Mueller, C. Engelmann, and S.L. Scott,
“Proactive Fault Tolerance for HPC with Xen Virtualization,”
Proc. ACM 21st Ann. Int’l Conf. Supercomputing (ICS), pp. 23-32,
2007.
[61] L. Youseff, K. Seymour, H. You, J. Dongarra, and R. Wolski, “The
Impact of Paravirtualized Memory Hierarchy on Linear Algebra
Computational Kernels and Software,” Proc. ACM 17th Int’l Symp.
High Performance Distributed Computing (HPDC), pp. 141-152, 2008.
[62] W. Yu and J .S . Vetter, “Xen-Based HPC: A Parallel
I/O
Perspective,” Proc. IEEE Eighth Int’l Symp. Cluster Computing and
the Grid (CCGrid), pp. 154-161, 2008.
[63] L. Cherkasova and R. Gardner, “Measuring CPU Overhead for I/
O Processing in the Xen Virtual Machine Monitor,” Proc. USENIX
Ann. Technical Conf. (ATC), pp. 387-390, 2005.
[64] U.F. Minhas, J. Yadav, A. Aboulnaga, and K. Salem, “Database
Systems on Virtual Machines: How Much Do You Lose?” Proc.
IEEE 24th Int’l Conf. Data Eng. (ICDE) Workshops, pp. 35-41, 2008.
[65] W. Huang, J. Liu, B. Abali, and D.K. Panda, “A Case for High
Performance Computing with Virtual Machines,” Proc. ACM 20th
Ann. Int’l Conf. Supercomputing (ICS), pp. 125-134, 2006.
[66] L. Gilbert, J. Tseng, R. Newman, S. Iqbal, R. Pepper, O. Celebioglu,
J. Hsieh, and M. Cobban, “Performance Implications of Virtuali-
zation and Hyper-Threading on High Energy Physics Applica-
tions in a Grid Environment,” Proc. IEEE 19th Int’l Parallel and
Distributed Processing Symp. (IPDPS), 2005.
J. Zhan, L. Wang, B. Tu, Y. Li, P. Wang, W. Zhou, and D. Meng,
“Phoenix Cloud: Consolidating Different Computing Loads on
Shared Cluster System for Large Organization,” Proc. First
Workshop Cloud Computing and Its Application (CCA ’08) Posters,
pp. 7-11, 2008.

[67]

IOSUP ET AL.: PERFORMANCE ANALYSIS OF CLOUD COMPUTING SERVICES FOR MANY-TASKS SCIENTIFIC COMPUTING

945

[68] C. Evangelinos and C.N. Hill, “Cloud Computing for Parallel
Scientific Hpc Applications: Feasibility of Running Coupled
Atmosphere-Ocean Climate Models on Amazons ec2,” Proc. First
Workshop Cloud Computing and Its Application (CCA ’08), pp. 1-6,
2008.
[69] M.-E. Bgin, B. Jones, J. Casey, E. Laure, F. Grey, C. Loomis, and R.
Kubli, “Comparative Study: Grids and Clouds, Evolution or
Revolution?,” CERN’’ EGEE-II Report, https://edms.cern.ch/
file/925013/3/EGEE-Grid-Cloud.pdf, June 2008.
[70] S. Williams, J. Shalf, L. Oliker, S. Kamil, P. Husbands, and K.A.
Yelick, “The Potential of
the Cell Processor for Scientific
Computing,” Proc. ACM Conf. Computing Frontiers, pp. 9-20, 2006.
[71] B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A.D. Joseph,
R.H. Katz, S. Shenker, and I. Stoica, “Mesos: A Platform for Fine-
Grained Resource Sharing in the Data Center,” Technical Report
UCB/EECS-2010-87 , UCBerkeley , http://www.eecs.berkeley.
edu/Pubs/TechRpts/2010/EECS-2010-87.html, May 2010.
[72] L. Vu, “Berkeley Lab Contributes Expertise to New Amazon Web
S e r v i c e s O f f e r i n g , ”
h t t p : / /www . l b l . g o v / c s /A r c h i v e /
news071310.html, July 2010.
J.P. Jones and B. Nitzberg, “Scheduling for Parallel Supercomput-
ing: A Historical Perspective of Achievable Utilization,” Proc. Job
Scheduling Strategies for Parallel Processing (JSSPP), pp. 1-16, 1999.
lxc Linux Containers Team, “Linux Containers Overview,”
http://lxc.sourceforge.net/, Aug. 2010.

[73]

[74]

Alexandru Iosup received the PhD degree in
compu ter sc ience in 2009 from the De l f t
University of Technology (TU Delft), the Nether-
lands. He is currently an assistant professor with
the Parallel and Distributed Systems Group at
TU Delft. He was a visiting scholar at U.Wis-
consin-Madison and U.California-Berkeley in the
summers of 2006 and 2010, respectively. He is
the cofounder of the Grid Workloads, the Peer-
to-Peer Trace, and the Failure Trace Archives,
which provide open access to workload and resource operation traces
from large-scale distributed computing environments. He is the author of
more than 50 scientific publications and has received several awards
and distinctions, including best paper awards at IEEE CCGrid 2010,
Euro-Par 2009, and IEEE P2P 2006. His research interests are in the
area of distributed computing (keywords: massively multiplayer online
games, grid and cloud computing, peer-to-peer systems, scheduling,
performance evaluation, workload characterization). He is a member of
the IEEE and the IEEE Computer Society.

Simon Ostermann received the Bakk.techn.
and Dipl.-Ing. degrees from the University of
Innsbruck, Austria, in 2006 and 2008, respec-
tively. Since 2008, he is following a doctoral
track in computer science with the Distributed
and Parallel Systems Group at the Institute for
Computer Science, University of Innsbruck. He
is the author of more than 10 journal and
conference publications. His research interests
are in the areas of resource management and
scheduling in the area of grid and cloud computing.

M. Nezih Yigitbasi received the BSc and MSc
degrees from the computer engineering Depart-
ment of
the Istanbu l Techn ica l Un iversi ty,
Turkey, in 2006 and 2008, respectively. Since
September 2008, he is following a doctoral track
in computer science within the Parallel and
Distributed Systems Group, Delft University of
Technology. His research interests are in the
resource management, scheduling,
areas of
design and performance evaluation of
large-
scale distributed systems, in particular grids and clouds. He is a member
of the IEEE.

Radu Prodan received the PhD degree from the
Vienna University of Technology, Vienna, Aus-
tria,
in 2004. He is currently an assistant
professor at the Institute of Computer Science,
University of Innsbruck, Austria. His research in
the area of parallel and distributed systems
comprise programming methods, compiler tech-
nology, performance analysis, and scheduling.
He participated in several national and European
projects. He is currently coordinating three
Austrian projects and was workpackage Leader in the IST-034601
(edutain@grid) and 26185 (SHIWA) projects. He is the author of more
than 70 journal and conference publications and one book. He was the
recipient of an IEEE Best Paper Award. He is a member of the IEEE.

Thomas Fahringer received the PhD degree in
1993 from the Vienna University of Technology.
Between 1990 and 1998, he worked as an
assistant professor at the University of Vienna,
where he was promo ted as an assoc ia te
professor in 1998. Since 2003, he has been a
full professor of computer science in the Institute
of Computer Science, University of Innsbruck,
Austria. He coordinated the IST-034601 edu-
tain@grid project and was involved in numerous
other Austrian and international European projects. He is the author of
more than 100 papers, including three books. He was the recipient of
two best paper awards from the ACM and the IEEE. His main research
interests include’ software architectures, programming paradigms,
compiler technology, performance analysis, and prediction for parallel
and distributed systems. He is a member of the IEEE.

Dick H.J. Epema received the MSc and PhD
degrees in mathematics from Leiden University,
Leiden,
in 1979 and 1983,
the Netherlands,
respectively. Since 1984, he has been with the
Department of Computer Science of Delft Uni-
versity of Technology, where he is currently an
associate professor in the Parallel and Distrib-
uted Systems Group. During 1987-1988, the fall
of 1991, and the summer of 1998, he was a
visiting scientist at the IBM T.J. Watson Re-
search Center in New York. In the fall of 1992, he was a visiting
professor at the Catholic University of Leuven, Belgium, and in the fall of
2009 he spent a sabbatical at UCSB. He has coauthored more than
70 papers in peer-reviewed conferences and journals, and was a
general cochair of Euro-Par 2009 and IEEE P2P 2010. His research
interests are in the areas of performance analysis, distributed systems,
peer-to-peer systems, and grids. He is a member of the IEEE.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 1,

JANUARY 2013

131

Scalable and Secure Sharing of Personal
Health Records in Cloud Computing Using
Attribute-Based Encryption

M ing L i, Member, IEEE, Shucheng Yu, Member, IEEE, Yao Zheng, Student Member, IEEE,
Ku i Ren, Sen ior Member, IEEE, and Wen j ing Lou, Senior Member, IEEE

Abstract—Personal health record (PHR) is an emerging patient-centric model of health information exchange, which is often
outsourced to be stored at a third party, such as cloud providers. However, there have been wide privacy concerns as personal health
information could be exposed to those third party servers and to unauthorized parties. To assure the patients’ control over access to
their own PHRs, it is a promising method to encrypt the PHRs before outsourcing. Yet, issues such as risks of privacy exposure,
scalability in key management, flexible access, and efficient user revocation, have remained the most important challenges toward
achieving fine-grained, cryptographically enforced data access control. In this paper, we propose a novel patient-centric framework
and a suite of mechanisms for data access control to PHRs stored in semitrusted servers. To achieve fine-grained and scalable data
access control for PHRs, we leverage attribute-based encryption (ABE) techniques to encrypt each patient’s PHR file. Different from
previous works in secure data outsourcing, we focus on the multiple data owner scenario, and divide the users in the PHR system into
multiple security domains that greatly reduces the key management complexity for owners and users. A high degree of patient privacy
is guaranteed simultaneously by exploiting multiauthority ABE. Our scheme also enables dynamic modification of access policies or file
attributes, supports efficient on-demand user/attribute revocation and break-glass access under emergency scenarios. Extensive
analytical and experimental results are presented which show the security, scalability, and efficiency of our proposed scheme.

Index Terms—Personal health records, cloud computing, data privacy, fine-grained access control, attribute-based encryption

Ç

1 INTRODUCTION
IN recent years, personal health record (PHR) has emerged
as a patient-centric model of health information exchange.
A PHR service allows a patient to create, manage, and
control her personal health data in one place through the
web, which has made the storage, retrieval, and sharing of
the medical
information more efficient. Especially, each
patient is promised the full control of her medical records
and can share her health data with a wide range of users,
including healthcare providers, family members or friends.
Due to the high cost of building and maintaining specialized
data centers, many PHR services are outsourced to or
provided by third-party service providers, for example,
Microsoft HealthVault.1 Recently, architectures of storing
PHRs in cloud computing have been proposed in [2], [3].

1. http://www.healthvault.com/.

. M. Li is with the Department of CS, Utah State University, 4205 Old
Main Hill, Logan, UT 84322. E-mail: ming.li@usu.edu.
. S. Yu is with the Department of CS, University of Arkansas at Little Rock,
2801 S. University Ave., Little Rock, AR 72204. E-mail: sxyu1@ualr.edu.
. Y. Zheng and W. Lou are with the Department of CS, Virginia Tech, 7054
Haycock Road, Falls Church, VA 24061.
E-mail: {zhengyao, wjlou}@vt.edu.
. K. Ren is with the Department of Computer Science and Engineering,
University at Buffalo, 338 Davis Hall, Buffalo, NY 14260.
E-mail: kuiren@buffalo.edu.

Manuscript received 19 Sept. 2011; revised 7 Jan. 2012; accepted 23 Feb. 2012;
published online 9 Mar. 2012.
Recommended for acceptance by A. Nayak.
For information on obtaining reprints of this article, please send e-mail to:
tpds@computer.org, and reference IEEECS Log Number TPDS-2011-09-0676.
Digital Object Identifier no. 10.1109/TPDS.2012.97.

While it is exciting to have convenient PHR services for
everyone, there are many security and privacy risks which
could impede its wide adoption. The main concern is about
whether the patients could actually control the sharing of
their sensitive personal health information (PHI), especially
when they are stored on a third-party server which people
may not fully trust. On the one hand, although there exist
healthcare regulations such as HIPAA which is recently
amended to incorporate business associates [4], cloud
providers are usually not covered entities [5]. On the other
hand, due to the high value of the sensitive PHI, the third-
party storage servers are often the targets of various
malicious behaviors which may lead to exposure of the
PHI. As a famous incident, a Department of Veterans Affairs
database containing sensitive PHI of 26.5 million military
veterans, including their social security numbers and health
problems was stolen by an employee who took the data
home without authorization [6]. To ensure patient-centric
privacy control over their own PHRs, it is essential to have
fine-grained data access control mechanisms that work with
semitrusted servers.
A feasible and promising approach would be to encrypt
the data before outsourcing. Basically, the PHR owner herself
should decide how to encrypt her files and to allow which set
of users to obtain access to each file. A PHR file should only
be available to the users who are given the corresponding
decryption key, while remain confidential to the rest of users.
Furthermore, the patient shall always retain the right to not
only grant, but also revoke access privileges when they feel it
is necessary [7]. However, the goal of patient-centric privacy

1045-9219/13/$31.00 ß 2013 IEEE

Published by the IEEE Computer Society

132

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 1,

JANUARY 2013

is often in conflict with scalability in a PHR system. The
authorized users may either need to access the PHR for
personal use or professional purposes. Examples of the
former are family member and friends, while the latter can be
medical doctors, pharmacists, and researchers, etc. We refer
to the two categories of users as personal and professional
users, respectively. The latter has potentially large scale;
should each owner herself be directly responsible for
managing all the professional users, she will easily be
overwhelmed by the key management overhead. In addition,
since those users’ access requests are generally unpredict-
able, it is difficult for an owner to determine a list of them. On
the other hand, different from the single data owner scenario
considered in most of the existing works [8], [9], in a PHR
system, there are multiple owners who may encrypt according
to their own ways, possibly using different sets of crypto-
graphic keys. Letting each user obtain keys from every owner
whose PHR she wants to read would limit the accessibility
since patients are not always online. An alternative is to
employ a central authority (CA) to do the key management
on behalf of all PHR owners, but this requires too much trust
on a single authority (i.e., cause the key escrow problem).
In this paper, we endeavor to study the patient-centric,
secure sharing of PHRs stored on semitrusted servers, and
focus on addressing the complicated and challenging key
management issues. In order to protect the personal health
data stored on a semitrusted server, we adopt attribute-
based encryption (ABE) as the main encryption primitive.
Using ABE, access policies are expressed based on the
attributes of users or data, which enables a patient to
selectively share her PHR among a set of users by encrypting
the file under a set of attributes, without the need to know a
complete list of users. The complexities per encryption, key
generation, and decryption are only linear with the number
of attributes involved. However, to integrate ABE into a
large-scale PHR system,
important issues such as key
management scalability, dynamic policy updates, and
efficient on-demand revocation are nontrivial to solve, and
remain largely open up-to-date. To this end, we make the
following main contributions:

1. We propose a novel ABE-based framework for
patient-centric secure sharing of PHRs in cloud
computing environments, under the multiowner
settings. To address the key management challenges,
we conceptually divide the users in the system into
two types of domains, namely public and personal
domains (PSDs). In particular, the majority profes-
sional users are managed distributively by attribute
authorities in the former, while each owner only
needs to manage the keys of a small number of users
in her personal domain. In this way, our framework
can simultaneously handle different types of PHR
sharing applications’ requirements, while incurring
minimal key management overhead for both owners
and users in the system. In addition, the framework
enforces write access control, handles dynamic policy
updates, and provides break-glass access to PHRs
under emergence scenarios.
In the public domain, we use multiauthority ABE
(MA-ABE) to improve the security and avoid key

2.

escrow problem. Each attribute authority (AA) in it
governs a disjoint subset of user role attributes, while
none of them alone is able to control the security of
the whole system. We propose mechanisms for key
distribution and encryption so that PHR owners can
specify personalized fine-grained role-based access
policies during file encryption. In the personal
domain, owners directly assign access privileges for
personal users and encrypt a PHR file under its data
attributes. Furthermore, we enhance MA-ABE by
putting forward an efficient and on-demand user/
attribute revocation scheme, and prove its security
under standard security assumptions. In this way,
patients have full privacy control over their PHRs.
3. We provide a thorough analysis of the complexity
and scalability of our proposed secure PHR sharing
solution, in terms of multiple metrics in computa-
tion, communication, storage, and key management.
We also compare our scheme to several previous
ones in complexity, scalability and security. Further-
more, we demonstrate the efficiency of our scheme
by implementing it on a modern workstation and
performing experiments/simulations.
Compared with the preliminary version of this paper [1],
there are several main additional contributions: 1) We clarify
and extend our usage of MA-ABE in the public domain, and
formally show how and which types of user-defined file
access policies are realized. 2) We clarify the proposed
revocable MA-ABE scheme, and provide a formal security
proof for it. 3) We carry out both real-world experiments and
simulations to evaluate the performance of the proposed
solution in this paper.

2 RELATED WORK

This paper is mostly related to works in cryptographically
enforced access control for outsourced data and attribute
based encryption. To realize fine-grained access control, the
traditional public key encryption (PKE)-based schemes [8],
[10] either incur high key management overhead, or require
encrypting multiple copies of a file using different users’
keys. To improve upon the scalability of the above solutions,
one-to-many encryption methods such as ABE can be used.
In Goyal et al.’s seminal paper on ABE [11], data are
encrypted under a set of attributes so that multiple users
who possess proper keys can decrypt. This potentially makes
encryption and key management more efficient [12]. A
fundamental property of ABE is preventing against user
collusion. In addition, the encryptor is not required to know
the ACL.

2.1 ABE for Fine-Grained Data Access Control
A number of works used ABE to realize fine-grained access
control for outsourced data [13], [14], [9], [15]. Especially,
there has been an increasing interest in applying ABE to
secure electronic healthcare records (EHRs). Recently,
Narayan et al. proposed an attribute-based infrastructure
for EHR systems, where each patient’s EHR files are
encrypted using a broadcast variant of CP-ABE [16] that
allows direct revocation. However, the ciphertext length
grows linearly with the number of unrevoked users. In [17], a

LI ET AL.: SCALABLE AND SECURE SHARING OF PERSONAL HEALTH RECORDS IN CLOUD COMPUTING USING ATTRIBUTE-BASED...

133

variant of ABE that allows delegation of access rights is
proposed for encrypted EHRs. Ibraimi et al. [18] applied
ciphertext policy ABE (CP-ABE) [19] to manage the sharing
of PHRs, and introduced the concept of social/professional
domains. In [20], Akinyele et al. investigated using ABE to
generate self-protecting EMRs, which can either be stored on
cloud servers or cellphones so that EMR could be accessed
when the health provider is offline.
However, there are several common drawbacks of the
above works. First, they usually assume the use of a single
trusted authority (TA) in the system. This not only may
create a load bottleneck, but also suffers from the key escrow
problem since the TA can access all the encrypted files,
opening the door for potential privacy exposure. In addition,
it is not practical to delegate all attribute management tasks
to one TA, including certifying all users’ attributes or roles
and generating secret keys. In fact, different organizations
usually form their own (sub)domains and become suitable
authorities to define and certify different sets of attributes
belonging to their (sub)domains (i.e., divide and rule). For
example, a professional association would be responsible for
certifying medical specialties, while a regional health
provider would certify the job ranks of its staffs. Second,
there still lacks an efficient and on-demand user revocation
mechanism for ABE with the support for dynamic policy
updates/changes, which are essential parts of secure PHR
shar ing . Finally , most of
the existing works do not
differentiate between the personal and public domains
(PUDs), which have different attribute definitions, key
management requirements, and scalability issues. Our idea
of conceptually dividing the system into two types of
domains is similar with that
in [18]; however, a key
difference is in [18] a single TA is still assumed to govern
the whole professional domain.
Recently, Yu et al. (YWRL) applied key-policy ABE to
secure outsourced data in the cloud [9], [15], where a single
data owner can encrypt her data and share with multiple
authorized users, by distributing keys to them that contain
attribute-based access privileges. They also propose a
method for the data owner to revoke a user efficiently by
delegating the updates of affected ciphertexts and user
secret keys to the cloud server. Since the key update
operations can be aggregated over time,
their scheme
achieves low amortized overhead. However, in the YWRL
scheme, the data owner is also a TA at the same time. It
would be inefficient to be applied to a PHR system with
multiple data owners and users, because then each user
would receive many keys from multiple owners, even if
the keys contain the same sets of attributes. On the other
hand, Chase and Chow [21] proposed a multiple-authority
ABE (CC MA-ABE) solution in which multiple TAs, each
governing a different subset of
the system’s users’
attributes, generate user secret keys collectively. A user
needs to obtain one part of her key from each TA. This
scheme prevents against collusion among at most N   2
TAs, in addition to user collusion resistance. However, it is
not clear how to realize efficient user revocation.
In
addition, since CC MA-ABE embeds the access policy in
users’ keys rather than the ciphertext, a direct application
of it to a PHR system is nonintuitive, as it is not clear how
to allow data owners to specify their file access policies.
We give detailed overviews to the YWRL scheme and CC

TABLE 1
Frequently Used Notations

MA-ABE scheme in the supplementary material, which can
be found on the Computer Society Digital Library at
http://doi.ieeecomputersociety.org/10.1109/TPDS.2012.97.

2.2 Revocable ABE
It is a well-known challenging problem to revoke users/
attributes efficiently and on-demand in ABE. Traditionally,
this is often done by the authority broadcasting periodic key
updates to unrevoked users frequently [13], [22], which does
not achieve complete backward/forward security and is less
efficient. Recently, [23] and [24] proposed two CP-ABE
schemes with immediate attribute revocation capability,
instead of periodical revocation. However, they were not
designed for MA-ABE.
In addition, Ruj et al. [25] proposed an alternative solution
for the same problem in our paper using Lewko and Waters’s
(LW) decentralized ABE scheme [26]. The main advantage of
their solution is, each user can obtain secret keys from any
subset of the TAs in the system, in contrast to the CC MA-
ABE. The LW ABE scheme enjoys better policy expressive-
ness, and it is extended by [25] to support user revocation.
On the downside, the communication overhead of key
revocation is still high, as it requires a data owner to transmit
an updated ciphertext component to every nonrevoked user.
They also do not differentiate personal and public domains.
In this paper, we bridge the above gaps by proposing a
unified security framework for patient-centric sharing of
PHRs in a multidomain, multiauthority PHR system with
many users. The framework captures application-level
requirements of both public and personal use of a patient’s
PHRs, and distributes users’ trust to multiple authorities that
better reflects reality. We also propose a suite of access
control mechanisms by uniquely combining the technical
strengths of both CC MA-ABE [21] and the YWRL ABE
scheme [9]. Using our scheme, patients can choose and
enforce their own access policy for each PHR file, and can
revoke a user without involving high overhead. We also
implement part of our solution in a prototype PHR system.

3 FRAMEWORK FOR PATIENT-CENTRIC, SECURE
AND SCALABLE PHR SHARING

In this section, we describe our novel patient-centric secure
data sharing framework for cloud-based PHR systems. The
main notations are summarized in Table 1.

3.1 Problem Definition
We consider a PHR system where there are multiple PHR
owners and PHR users. The owners refer to patients who

134

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 1,

JANUARY 2013

have full control over their own PHR data, i.e., they can
create, manage, and delete it. There is a central server
belonging to the PHR service provider that stores all the
owners’ PHRs. The users may come from various aspects; for
example, a friend, a caregiver or a researcher. Users access
the PHR documents through the server in order to read or
write to someone’s PHR, and a user can simultaneously have
access to multiple owners’ data.
A typical PHR system uses standard data formats. For
example, continuity-of-care (CCR) (based on XML data
structure), which is widely used in representative PHR
systems including Indivo [27], an open-source PHR system
adopted by Boston Children’s Hospital. Due to the nature of
XML, the PHR files are logically organized by their categories
in a hierarchical way [8], [20].

3.1.1 Security Model
In this paper, we consider the server to be semitrusted, i.e.,
honest but curious as those in [28] and [15]. That means
the server will try to find out as much secret information
in the stored PHR files as possible, but they will honestly
follow the protocol in general. On the other hand, some
users will also try to access the files beyond their
privileges. For example, a pharmacy may want to obtain
the prescriptions of patients for marketing and boosting its
profits. To do so, they may collude with other users, or
even with the server. In addition, we assume each party in
our system is preloaded with a public/private key pair,
and entity authentication can be done by traditional
challenge-response protocols.

3.1.2 Requirements
To achieve “patient-centric” PHR sharing, a core requirement
is that each patient can control who are authorized to access
to her own PHR documents. Especially, user-controlled
read/write access and revocation are the two core security
objectives for any electronic health record system, pointed
out by Mandl et al. [7] in as early as 2001. The security and
performance requirements are summarized as follows:

. Data confidentiality. Unauthorized users (including
the server) who do not possess enough attributes
satisfying the access policy or do not have proper
key access privileges should be prevented from
decrypting a PHR document, even under user
collusion. Fine-grained access control should be
enforced, meaning different users are authorized to
read different sets of documents.
. On-demand revocation. Whenever a user’s attribute is
no longer valid, the user should not be able to access
future PHR files using that attribute. This is usually
called attribute revocation, and the corresponding
security property is forward secrecy [23]. There is
also user revocation, where all of a user’s access
privileges are revoked.
. Write access control. We shall prevent the unauthor-
ized contributors to gain write-access to owners’
PHRs, while the legitimate contributors should access
the server with accountability.
The data access policies should be flexible,
i.e.,
dynamic changes to the predefined policies shall be

.

.

allowed, especially the PHRs should be accessible
under emergency scenarios.
Scalability, efficiency, and usability. The PHR system
should support users from both the personal domain
and public domains. Since the set of users from the
public domain may be large in size and unpredict-
able, the system should be highly scalable, in terms
of complexity in key management, communication,
computation and storage. Additionally, the owners’
efforts in managing users and keys should be
minimized to enjoy usability.

3.2 Overview of Our Framework
The main goal of our framework is to provide secure
patient-centric PHR access and efficient key management at
the same time. The key idea is to divide the system into
multiple security domains (namely, public domains and
personal domains) according to the different users’ data
access requirements. The PUDs consist of users who make
access based on their professional roles, such as doctors,
nurses, and medical researchers. In practice, a PUD can be
mapped to an independent sector in the society, such as the
health care, government, or insurance sector. For each PSD,
its users are personally associated with a data owner (such
as family members or close friends), and they make accesses
to PHRs based on access rights assigned by the owner.
In both types of security domains, we utilize ABE to
realize cryptographically enforced, patient-centric PHR
access. Especially, in a PUD multiauthority ABE is used, in
which there are multiple “attribute authorities” (AAs), each
governing a disjoint subset of attributes. Role attributes are
defined for PUDs, representing the professional role or
obligations of a PUD user. Users in PUDs obtain their
attribute-based secret keys from the AAs, without directly
interacting with the owners. To control access from PUD
users, owners are free to specify role-based fine-grained
access policies for her PHR files, while do not need to know
the list of authorized users when doing encryption. Since the
PUDs contain the majority of users, it greatly reduces the key
management overhead for both the owners and users.
Each data owner (e.g., patient) is a trusted authority of her
own PSD, who uses a KP-ABE system to manage the secret
keys and access rights of users in her PSD. Since the users are
personally known by the PHR owner, to realize patient-
centric access, the owner is at the best position to grant user
access privileges on a case-by-case basis. For PSD, data
attributes are defined which refer to the intrinsic properties of
the PHR data, such as the category of a PHR file. For the
purpose of PSD access, each PHR file is labeled with its data
attributes, while the key size is only linear with the number
of file categories a user can access. Since the number of users
in a PSD is often small, it reduces the burden for the owner.
When encrypting the data for PSD, all that the owner needs
to know is the intrinsic data properties.
The multidomain approach best models different user
types and access requirements in a PHR system. The use of
ABE makes the encrypted PHRs self-protective, i.e., they
can be accessed by only authorized users even when storing
on a semitrusted server, and when the owner is not online.
In addition, efficient and on-demand user revocation is
made possible via our ABE enhancements.

LI ET AL.: SCALABLE AND SECURE SHARING OF PERSONAL HEALTH RECORDS IN CLOUD COMPUTING USING ATTRIBUTE-BASED...

135

Fig. 1. The proposed framework for patient-centric, secure and scalable
PHR sharing on semitrusted storage under multiowner settings.

3.3 Details of the Proposed Framework
In our framework, there are multiple SDs, multiple owners,
multiple AAs, and multiple users. In addition, two ABE
systems are involved: for each PSD the YWRL’s revocable
KP-ABE scheme [9] is adopted; for each PUD, our proposed
revocable MA-ABE scheme (described in Section 4) is used.
The framework is illustrated in Fig. 1. We term the users
hav ing read and wr i te access as da ta readers and
contributors, respectively.
System setup and key distribution. The system first
defines a common universe of data attributes shared by
every PSD, such as “basic profile,” “medical history,”
“allergies,” and “prescriptions.” An emergency attribute is
also defined for break-glass access. Each PHR owner’s client
application generates its corresponding public/master keys.
The public keys can be published via user’s profile in an
online healthcare social-network (HSN) (which could be
part of the PHR service; e.g., the Indivo system [27]). There
are two ways for distributing secret keys. First, when first
using the PHR service, a PHR owner can specify the access
privilege of a data reader in her PSD, and let her application
generate and distribute corresponding key to the latter, in a
way resembling invitations in GoogleDoc. Second, a reader
in PSD could obtain the secret key by sending a request
(indicating which types of files she wants to access) to the
PHR owner via HSN, and the owner will grant her a subset
of requested data types. Based on that, the policy engine of
the application automatically derives an access structure,
and runs keygen of KP-ABE to generate the user secret key
that embeds her access structure. In addition, the data
attributes can be organized in a hierarchical manner for
efficient policy generation, see Fig. 2. When the user is
granted all the file types under a category, her access
privilege will be represented by that category instead.
For the PUDs, the system defines role attributes, and a
reader in a PUD obtains secret key from AAs, which binds
the user to her claimed attributes/roles. For example, a
physician in it would receive “hospital A, physician, M.D.,
internal medicine” as her attributes from the AAs. In
practice, there exist multiple AAs each governing a different
subset of role attributes. For instance, hospital staffs shall
have a different AA from pharmacy specialists. This is
reflected by (1) in Fig. 1. MA-ABE is used to encrypt the data,

Fig. 2. The attribute hierarchy of
files—leaf nodes are atomic file
categories while internal nodes are compound categories. Dark boxes
are the categories that a PSD’s data reader have access to.

and the concrete mechanism will be presented in Section 4.
In addition, the AAs distribute write keys that permit
contributors in their PUD to write to some patients’ PHR (2).
PHR encryption and access. The owners upload ABE-
encrypted PHR files to the server (3). Each owner’s PHR file
is encrypted both under a certain fine-grained and role-
based access policy for users from the PUD to access, and
under a selected set of data attributes that allows access from
users in the PSD. Only authorized users can decrypt the PHR
files, excluding the server. For improving efficiency, the data
attributes will include all the intermediate file types from a
leaf node to the root. For example, in Fig. 2, an “allergy” file’s
attributes are fP HR; medical history; allergyg. The data
readers download PHR files from the server, and they can
decrypt the files only if they have suitable attribute-based
keys (5). The data contributors will be granted write access to
someone’s PHR, if they present proper write keys (4).
User revocation. Here, we consider revocation of a data
reader or her attributes/access privileges. There are several
possible cases:

1.

2.

4.

revocation of one or more role attributes of a public
domain user;
revocation of a public domain user which is equiva-
lent to revoking all of that user’s attributes. These
operations are done by the AA that the user belongs
to, where the actual computations can be delegated to
the server to improve efficiency (8).
3. Revocation of a personal domain user’s access
privileges;
revocation of a personal domain user. These can be
initiated through the PHR owner’s client application
in a similar way.
Policy updates. A PHR owner can update her sharing
policy for an existing PHR document by updating the
attributes (or access policy) in the ciphertext. The supported
operations include add/delete/modify, which can be done
by the server on behalf of the user.
Break-glass. When an emergency happens, the regular
access policies may no longer be applicable. To handle this
situation, break-glass access is needed to access the victim’s
PHR. In our framework, each owner’s PHR’s access right is
also delegated to an emergency department (ED, (6)). To
prevent from abuse of break-glass option, the emergency
staff needs to contact the ED to verify her identity and the

136

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 1,

JANUARY 2013

emergency situation, and obtain temporary read keys (7).
After the emergency is over, the patient can revoke the
emergent access via the ED.
An example. Here, we demonstrate how our framework
works using a concrete example. Suppose PHR owner Alice
is a patient associated with hospital A. After she creates a
PHR file F1 (labeled as “PHR; medical history; allergy;
emergency” in Fig. 2), she first encrypts it according to both
F1 ’s data labels (under the YWRL KP-ABE), and a role-
based file access policy P 1 (under our revocable MA-ABE).
This policy can be decided based on recommended settings
by the system, or Alice’s own preference. It may look like
P 1 :¼ ‘‘ðprof ession ¼ physicianÞ
^ ðspecialty ¼ internal medicineÞ
^ ðorganization ¼ hospital AÞ’’:

She also sends the break-glass key to the ED. In addition,
Alice determines the access rights of users in her PSD,
which can be done either online or offline. For example, she
may approve her friend Bob’s request to access files with
labels fpersonal inf og or fmedical historyg. Her client
application will distribute a secret key with the access
structure ðpersonal inf o _ medical historyÞ to Bob. When
Bob wants to access another file F2 with labels “PHR—me-
dical history—medications,” he is able to decrypt F2 due to
the “medical history” attribute. For another user Charlie
who is a physician specializing in internal medicine in
hospital B in the PUD, he obtains his secret key from
multiple AAs such as the American Medical Association
(AMA), the American Board of Medical Specialties (ABMS),
and the American Hospital Association (AHA). But he
cannot decrypt F1 , because his role attributes do not satisfy
P 1 . Finally, an emergency room staff, Dorothy who
temporarily obtains the break-glass key from ED, can gain
access to F1 due to the emergency attribute in that key.
Remarks. The separation of PSD/PUD and data/role
attributes reflects the real-world situation. First, in the PSD, a
patient usually only gives personal access of his/her
sensitive PHR to selected users, such as family members
and close friends, rather than all the friends in the social
network. Different PSD users can be assigned different
access privileges based on their relationships with the
owner. In this way, patients can exert fine-control over the
access for each user in their PSDs. Second, by our multi-
domain and multiauthority framework, each public user
only needs to contact AAs in its own PUD who collabora-
tively generates a secret key for the user, which reduces the
workload per AA (since each AA handles fewer number of
attributes per key issuing). In addition, the multiauthority
ABE is resilient to compromise of up to N   2 AAs in a PUD,
which solves the key-escrow problem. Furthermore, in our
framework user’s role verification is much easier. Different
organizations can form their own (sub)domains and become
AAs to manage and certify different sets of attributes, which
is similar to divide and rule.

4.1 Using MA-ABE in the Public Domain
For the PUDs, our framework delegates the key management
functions to multiple attribute authorities. In order to
achieve stronger privacy guarantee for data owners, the
Chase-Chow (CC) MA-ABE scheme [21] is used, where each
authority governs a disjoint set of attributes distributively. It
is natural to associate the ciphertext of a PHR document with
an owner-specified access policy for users from PUD.
However, one technical challenge is that CC MA-ABE is
essentially a KP-ABE scheme, where the access policies are
enforced in users’ secret keys, and those key-policies do not
directly translate to document access policies from the
owners’ points of view. By our design, we show that by
agreeing upon the formats of the key-policies and the rules of
specifying which attributes are required in the ciphertext, the
CC MA-ABE can actually support owner-specified docu-
ment access policies with some degree of flexibility (such as
the one in Fig. 4), i.e., it functions similar to CP-ABE.2
In order to allow the owners to specify an access policy
for each PHR document, we exploit the fact that the basic
CC MA-ABE works in a way similar to fuzzy-IBE, where
the threshold policies (e.g., k out of n) are supported.
Since the threshold gate has an intrinsic symmetry from
both the encryptor and the user’s point of views, we can
predefine the formats of the allowed document policies as
well as those of the key-policies, so that an owner can
enforce a file access policy through choosing which set of
attributes to be included in the ciphertext.

4.1.1 Basic Usage of MA-ABE
Setup. In particular, the AAs first generate the MK s and P K
using setup as in CC MA-ABE. The kth AA defines a disjoint
set of role attributes UUk , which are relatively static properties
of the public users. These attributes are classified by their
types, such as profession and license status, medical
specialty, and affiliation where each type has multiple
possible values. Basically, each AA monitors a disjoint subset
of attribute types. For example, in the healthcare domain, the
AMA may issue medical professional licenses like “physi-
cian,” “M.D.,” “nurse,” “entry-level license,” etc., the ABMS
could certify specialties like “internal medicine,” “surgery,”
etc; and AHA may define user affiliations such as “hospital
A” and “pharmacy D.” In order to represent the “do not
care” option for the owners, we add one wildcard attribute “ ”
in each type of the attributes.
Document policy generation and encryption. In the basic
usage, we consider a special class of access policy—conjunc-
t ive norma l form (CNF ) , P :¼ ððA1 ¼ a1;1 Þ _    _ ðA1 ¼
a1;d1 ÞÞ ^    ^ ððAm ¼ am;1 Þ _    _ ðAm ¼ am;dm ÞÞ, where ai;j
could be “*,” and m is the total number of attribute types.
For such a file access policy, an owner encrypts the file as
follows (all the attributes in this section are role attributes):
Definition 1 (Basic Encryption Rule for PUD). Let P be in
CNF form, then P is required to contain at least one attribute
from each type, and the encryptor associates the ciphertext with
all the attributes on the leaf of the access tree corresponding to P .

4 MAIN DESIGN ISSUES

In this section, we address several key design issues in
secure and scalable sharing of PHRs in cloud computing,
under the proposed framework.

Key policy generation and key distribution. In CC [21],
the format of the key-policies is restricted to conjunctions

2. Recently Lewko and Waters proposed a multiauthority CP-ABE
construction [29], but it does not support on-demand attribute revocation.

LI ET AL.: SCALABLE AND SECURE SHARING OF PERSONAL HEALTH RECORDS IN CLOUD COMPUTING USING ATTRIBUTE-BASED...

137

TABLE 2
Sample Secret Keys and Key-Policies for Three Public Users in the Health Care Domain

among different AAs, i.e., P :¼ P1 ^    ^ PN , where Pk could
correspond to arbitrary monotonic access structure. To be
able to implement the CNF document policy, each AA need
to follow the rule of key distribution:

Definition 2 (Basic key policy generation rule for PUD).
Let P be in the above form. For the secret key of user u, AAu
k should
contain at least one attribute from every type of attributes
governed by AAk , and always include the wildcard associated
with each type. In addition, the key policy Pk of u issued by AAk
is ð1 out of nk1 Þ ^    ^ ð1 out of nkt Þ, where nk1 . . . nkt are the
indices of attribute types governed by AAk .

In the above, AAu
k is the set of role attributes u obtains from
AAk . After key distribution, the AAs can remain offline for
most of the time. A detailed key distribution example is
given in Table 2.
The following two properties ensure that the set of users
that can decrypt a file with an access policy P is equivalent
to the set of users with key access structures such that the
ciphertext’s attribute set (P ’s leaf nodes) will satisfy.
Definition 3 (Correctness). Given a ciphertext and its
corresponding file access policy P and its leaf node set
LðP Þ ¼ AAC , a user access tree T and its leaf node set LðT Þ ¼
AAu , P ðLðT ÞÞ ¼ 1 ) T ðLðP ÞÞ ¼ 1. That is, whenever the
attributes in user secret key satisfy the file access policy,
the attributes in the access policy should satisfy the access
structure in user secret key.
Definition 4 (Completeness). Conversely, T ðLðP ÞÞ ¼ 1 )
P ðLðT ÞÞ ¼ 1.
Theorem 1. Following the above proposed key generation and
encryption rules, the CNF file access policy achieves both
correctness and completeness.

.

Proof. In the following, subscript i of an attribute set
denotes the subset of attributes belonging to the ith type.
correctness () ). If P ðLðT ÞÞ ¼ 1 (i.e., LðT Þ satis-
i \ Li ðT Þ. Since the
fies P ), 8i ¼ 1; . . . ; m; 9a 2 AAC
ith policy term in P (corresponding to user access
tree T ) is “1 out of ni ,” this implies T ðLðP ÞÞ ¼ 1.
. Completeness (( ): it is easy to see the above is
reversible, due to the symmetry of set
inter-
tu
section.
The above theorem essentially states, the CC MA-ABE
can be used in a fashion like CP-ABE when the document
access policy is CNF. In practice, the above rules need to be
agreed and followed by each owner and AA. It is easy to
generalize the above conclusions to conjunctive forms with
each term being a threshold logic formula, which will not be
elaborated here.

4.1.2 Achieving More Expressive File Access Policies
By enhancing the key-policy generation rule, we can enable
more expressive encryptor’s access policies. We exploit an
observation that
in practice, a user’s attributes/roles
belonging to different types assigned by the same AA are
often correlated with respect to a primary attribute type. In
the following, an attribute tuple refers to the set of attribute
values governed by one AA (each of a different type) that
are correlated with each other.

Definition 5 (Enhanced Key-Policy Generation Rule). In
addition to the basic key-policy generation rule, the attribute
tuples assigned by the same AA for different users do not
intersect with each other, as long as their primary attribute
types are distinct.

Definition 6 (Enhanced Encryption Rule). In addition to the
basic encryption rule, as long as there are multiple attributes of
the same primary type, corresponding nonintersected attribute
tuples are included in the ciphertext’s attribute set.

This primary-type based attribute association is illu-
strated in Fig. 3. Note that there is a “horizontal association”
between two attributes belonging to different types as-
signed to each user. For example, in the first AA (AMA) in
Table 2, “license status” is associated with “profession,” and
“profession” is a primary type. That means, a physician’s
possible set of license status do not intersect with that of a
nurse’s, or a pharmacist’s. An “M.D.” license is always
associated with “physician,” while “elderly’s nursing
licence” is always associated with “nurse.” Thus, if the
second level key policy within the AMA is “1 out of n1 ^ 1
out of n2 ,” a physician would receive a key like “(physician
OR *) AND (M.D. OR *)” (recall the assumption that each
user can only hold at most one role attribute in each type),
nurse’s will be like “(nurse OR *) AND (elderly’s nursing
licence OR *).” Meanwhile, the encryptor can be made
aware of this correlation, so she may include the attribute
set:
{physician, M.D., nurse, elderly’s nursing licence}
during encryption. Due to the attribute correlation, the set
of users that can have access to this file can only possess one
out of two sets of possible roles, which means the following
policy is enforced: “(physician AND M.D.) OR (nurse AND

Fig. 3. Illustration of the enhanced key-policy generation rule. Solid
horizontal lines represent possible attribute associations for two users.

138

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 1,

JANUARY 2013

defined according to the MA-ABE scheme (also in supple-
mentary material, available online), and AAu
k include attri-
butes in the key policy issued by AAk .
f i l e F i s : EðF Þ ¼ hEABE ðF EK Þ;
T h e c i ph e r t e x t o f
EF EK ðF Þi, where EF EK ðF Þ is a symmetric key encryption
of F , and EABE ðF EK Þ ¼ hEP SD ðF EK Þ; EP U D ðF EK Þi, where
each of the ciphertexts are encrypted using the YWRL ABE
scheme and MA-ABE scheme, respectively.

4.2 Enhancing MA-ABE for User Revocation
The original CC MA-ABE scheme does not enable efficient
and on-demand user revocation. To achieve this for MA-
ABE, we combine ideas from YWRL’s revocable KP-ABE [9],
[15] (its details are shown in supplementary material,
available online), and propose an enhanced MA-ABE
scheme. In particular, an authority can revoke a user or
user’s attributes immediately by reencrypting the cipher-
texts and updating users’ secret keys, while a major part of
these operations can be delegated to the server which
enhances efficiency.
The idea to revoke one attribute of a user in MA-ABE is
as follows: The AA who governs this attribute actively
updates that attribute for all the affected unrevoked users.
To this end, the following updates should be carried out:
1)
the public/master key components for the affected
attribute; (2) the secret key component corresponding to
that attribute of each unrevoked user; 3) Also, the server
shall update all the ciphertexts containing that attribute. In
order to reduce the potential computational burden for the
AAs, we adopt proxy encryption to delegate operations 2
and 3 to the server, and use lazy-revocation to reduce the
overhead. In particular, each data attribute i is associated
with a version number veri . Upon each revocation event, if i
is an affected attribute, the AA submits a rekey rki$i0 ¼ t0
i =ti
to the server, who then reencrypts the affected ciphertexts
and increases their version numbers. The unrevoked users’
secret key components are updated via a similar operation
using the rekey. To delegate secret key updates to the
server, a dummy attribute needs to be additionally defined
by each of N   1 AAs, which are always ANDed with each
user’s key-policy to prevent the server from grasping the
secret keys. This also maintains the resistance against up to
N   2 AA collusion of MA-ABE (as will be shown by our
security proof). Using lazy-revocation, the affected cipher-
texts and user secret keys are only updated when an
affected unrevoked user logs into the system next time. By
the form of the rekey, all the updates can be aggregated
from the last login to the most current one.
To revoke a user in MA-ABE, one needs to find out a
minimal subset of attributes ( ) such that without it the
user’s secret key’s access structure (AAu ) will never be
satisfied. Because our MA-ABE scheme requires conjunctive
access policy across the AAs, it suffices to find a minimal
subset by each AAk (k  AAu
k ), without which AAu
k will not
be satisfied, and then compute the minimal set (kmin ) out of
all k . The AAkmin will initiate the revocation operation.
The enhanced CC MA-ABE scheme with immediate
revocation capabilities is formally described in Fig. 5. It has
nine algorithms, where MinimalSet, ReKeyGen, ReEnc, and
KeyUpdate are related to user revocation, and PolicyUpdate

Fig. 4. An example policy realizable under our framework using MA-
ABE, following the enhanced key generation and encryption rules.

elderly’s nursing licence).” The direct consequence is it
enables a disjunctive normal form (DNF) encryptor access
policy to appear at the second level. If the encryptor wants
to enforce such a DNF policy under an AA, she can simply
include all the attributes in that policy in the ciphertext.
Furthermore,
if one wants to encrypt with wildcard
attributes in the policy, say: “(physician AND M.D.) OR
(nurse AND any nursing license)” the same idea can be used,
i.e., we can simply correlate each “profession” attribute with
its proprietary “ ” attribute. So we will have “nursing license ,
physician license ,” etc., in the users’ keys. The above discussion is
summarized in Fig. 4 by an example encryptor’s policy.
If there are multiple PUDs, then P ¼ [P U Dj fP P U Dj g, and
multiple sets of ciphertext components needs to be included.
Since in reality, the number of PUDs is usually small, this
method is more efficient and secure than a straightforward
application of CP-ABE in which each organization acts as an
authority that governs all types of attributes [1], and the
length of ciphertext grows linearly with the number of
organizations. For efficiency, each file is encrypted with a
randomly generated file encryption key (F EK ), which is
then encrypted by ABE.

4.1.3 Summary
In this above, we present a method to enforce owner’s
access policy during encryption, which utilizes the MA-
ABE scheme in a way like CP-ABE. The essential idea is to
define a set of key-generation rules and encryption rules.
There are two layers in the encryptor’s access policy, the
first one is across different attribute authorities while the
second is across different attributes governed by the same AA.
For the first layer, conjunctive policy is enabled; for the
second, either k-out-of-n or DNF policy are supported. We
exploit the correlations among attribute types under an AA
to enable the extended second-level DNF policy.
Next, we summarize the formats of user secret key and
ciphertext in our framework. A user u in an owner’s PSD has
¼ hfDi gi2AAu
i, where Di follows
the following keys: SK P SD
u
P SD
the construction of the YWRL ABE scheme (shown in
supplementary material, available online), and AAu
P SD is the
attribute set in the key policy for u. For a user u in a PUD,
¼ hDu ; fDk;i gk2f1;...;N g;i2AAu
i, where Du and Dk;i are
SK P UD
u
k

LI ET AL.: SCALABLE AND SECURE SHARING OF PERSONAL HEALTH RECORDS IN CLOUD COMPUTING USING ATTRIBUTE-BASED...

139

not be able to write to patients that are not treated by her.
Therefore, we combine signatures with the hash chain
technique to achieve our goals.
Suppose the time granularity is set to t, and the time is
divided into periods of t. For each working cycle (e.g., a
day), an organization generates a hash chain [30], [31]:
H ¼ fh0 ; h1 ; . . . ; hn g, where H ðhi 1 Þ ¼ hi , 1  i  n. At time
0, the organization broadcasts a signature of the chain end hn
(org ðhn Þ) to all users in its domain, where ðÞ stands for an
unforgeable signature scheme. After that it multicasts hn i to
the set of authorized contributors at each time period i. Note
that, the above method enables timely revocation of write
access, i.e., the authority simply stops issuing hashes for a
contributor at the time of revocation. In addition, an owner
could distribute a time-related signature: owner ðts; ttÞ to the
entities that requests write access (which can be delegated to
the organization), where ts is the start time of the granted
time window, and tt is the end of the time window. For
example, to enable a billing clerk to add billing information
to Alice’s PHR, Alice can specify “8 am to 5 pm” as the
granted time window at the beginning of a clinical visit. Note
that, for contributors in the PSD of the owner, they only need
to obtain signatures from the owner herself.
Generally, during time period j, an authorized contri-
butor w submits a “ticket” to the server after being
authenticated to it:
Epkserver ðowner ðtskttÞkorg ðhn Þkhn j krÞ;
where Epkserver is the public key encryption using the server’s
public key, and r is a nonce to prevent replay attack. The
server verifies if the signatures are correct using both org’s
and owner’s public keys, and whether H j ðhn j Þ ¼ hn , where
H j ðÞ means hash j times. Only if both holds, the contributor
is granted write access and the server accepts the contents
uploaded subsequently.

4.4 Handle Dynamic Policy Changes
Our scheme should support the dynamic add/modify/
delete of part of the document access policies or data
attributes by the owner. For example, if a patient does not
want doctors to view her PHR after she finishes a visit to a
hospital, she can simply delete the ciphertext components
corresponding to attribute “doctor” in her PHR files. Adding
and modification of attributes/access policies can be done by
proxy reencryption techniques [22]; however, they are
expensive. To make the computation more efficient, each
owner could store the random number s used in encrypting
the F EK 3 of each document on her own computer, and
construct new ciphertext components corresponding to
added/changed attributes based on s. The PolicyUpdate
algorithm is shown in Fig. 5.
To reduce the storage cost, the owner can merely keep a
random seed s0 and generate the s for each encrypted file from
s0 , such as using a pseudorandom generator. Thus, the main
computational overhead to modify/add one attribute in the
ciphertext is just one modular exponentiation operation.

3. The details of the encryption algorithms are shown in supplementary
material, available online.

Fig. 5. The enhanced MA-ABE scheme with on-demand revocation
capabilities.

is for handling dynamic policy changes. A version number
is used to record and differentiate the system states (P K ,
MK , SK , CT ) after each revocation operation. Since this
scheme combines [9] and [21], the differences with respect
to each of them are highlighted.

4.3 Enforce Write Access Control
If there is no restrictions on write access, anyone may write to
someone’s PHR using only public keys, which is undesirable.
By granting write access, we mean a data contributor should
obtain proper authorization from the organization she is in
(and/or from the targeting owner), which shall be able to be
verified by the server who grants/rejects write access.
A naive way is to let each contributor obtain a signature
from her organization every time she intends to write. Yet
this requires the organizations be always online. The
observation is that, it is desirable and practical to authorize
according to time periods whose granularity can be adjusted.
For example, a doctor should be permitted to write only
during her office hours; on the other hand, the doctor must

140

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 1,

JANUARY 2013

TABLE 3
Comparison of Security

4.5 Deal with Break-Glass Access
For certain parts of the PHR data, medical staffs need to
have temporary access when an emergency happens to a
patient, who may become unconscious and is unable to
change her access policies beforehand. The medical staffs
will need some temporary authorization (e.g., emergency
key) to decrypt those data. Under our framework, this can
be naturally achieved by letting each patient delegate her
emergency key to an emergency department. Specifically,
in the beginning, each owner defines an “emergency”
attribute and builds it into the PSD part of the ciphertext of
each PHR document that she allows break-glass access. She
then generates an emergency key skEM using the single-
node key-policy “emergency,” and delegates it to the ED
who keeps it in a database of patient directory. Upon
emergency, a medical staff authenticates herself to the ED,
requests and obtains the corresponding patient’s skEM , and
then decrypts the PHR documents using skEM . After the
patient recovers from the emergency, she can revoke the
break-glass access by computing a rekey: rkEM , submit it to
the ED and the server to update her skEM and CT to their
newest versions, respectively.

Remarks. We note that, although using ABE and MA-ABE
enhances the system scalability, there are some limita-
tions in the practicality of using them in building PHR
systems. For example, in workflow-based access control
scenarios, the data access right could be given based on
users’ identities rather than their attributes, while ABE
does not handle that efficiently. In those scenarios one
may consider the use of attribute-based broadcast
encryption (ABBE) [32]. In addition, the expressibility
of our encryptor’s access policy is somewhat limited by
that of MA-ABE’s, since it only supports conjunctive
policy across multiple AAs. In practice, the credentials
from different organizations may be considered equally
effective, in that case distributed ABE schemes [33] will
be needed. We designate those issues as future works.

5 SECURITY ANALYSIS

In this section, we analyze the security of the proposed PHR
sharing solution. First we show it achieves data confidenti-
ality (i.e., preventing unauthorized read accesses), by
proving the enhanced MA-ABE scheme (with efficient
revocation) to be secure under the attribute-based selective-
set model [21], [34]. We have the following main theorem.

Theorem 2. The enhanced MA-ABE scheme guarantees data
confidentiality of the PHR data against unauthorized users
and the curious cloud service provider, while maintaining the
collusion resistance against users and up to N   2 AAs.

In addition, our framework achieves forward secrecy, and
security of write access control. For detailed security analysis
and proofs, please refer to the online supplementary
material, available online, of this paper.
We also compare the security of our scheme with several
existing works, in terms of confidentiality guarantee, access
control granularity, and supported revocation method, etc.
We choose four representative state-of-the-art schemes to
compare with:

3.

4.

1.

2.

the VFJPS scheme [28] based on access control list
(ACL);
the BCHL scheme based on HIBE [8] where each
owner acts as a key distribution center;
the HN revocable CP-ABE scheme [23], where we
adapt it by assuming using one PUD with a single
authority and multiple PSDs to fit our setting;
the NGS scheme in [16] which is a privacy-
preserving EHR system that adopts attribute-based
broadcast encryption to achieve data access control;
5. The RNS scheme in [25] that enhances the Lewko-
Waters MA-ABE with revocation capability for data
access control in the cloud.
The results are shown in Table 3. It can be seen that, our
scheme achieves high privacy guarantee and on-demand
revocation. The conjunctive policy restriction only applies for
PUD, while in PSD a user’s access structure can still be
arbitrary monotonic formula. In comparison with the RNS
scheme, in RNS the AAs are independent with each other,
while in our scheme the AAs issue user secret keys
collectively and interactively. Also, the RNS scheme supports
arbitrary monotonic Boolean formula as file access policy.
However, our user revocation method is more efficient in
terms of communication overhead. In RNS, upon each
revocation event, the data owner needs to recompute and
send new ciphertext components corresponding to revoked
attributes to all the remaining users. In our scheme, such
interaction is not needed. In addition, our proposed frame-
work specifically addresses the access requirements in cloud-
based health record management systems by logically
dividing the system into PUD and PSDs, which considers
both personal and professional PHR users. Our revocation
methods for ABE in both types of domains are consistent. The
RNS scheme only applies to the PUD.

6 SCALABILITY AND EFFICIENCY
6.1 Storage and Communication Costs
First, we evaluate the scalability and efficiency of our
solution in terms of storage, communication, and computa-
tion costs. We compare with previous schemes in terms of

LI ET AL.: SCALABLE AND SECURE SHARING OF PERSONAL HEALTH RECORDS IN CLOUD COMPUTING USING ATTRIBUTE-BASED...

141

TABLE 4
Notations for Efficiency Comparison

ciphertext size, user secret key size, public key/information
size, and revocation (rekeying) message size.
Our analysis is based on the worst case where each user
may potentially access part of every owners’ data. Table 4 is
a list of notations, where in our scheme: jU j ¼ jU D j þ jU R j,
tc ¼ jAAC
P SD j þ jAAC
P U D j (includes one emergency attribute),
and tu ¼ jAAu
P SD j þ jAAu
P UD j (a user could be both in a PSD
and PUD). Note that, since the HN, NGS, and RNS schemes
do not separate PSD and PUD, their jU j ¼ jU R j, tc ¼ jAAC
P U D j,
and tu ¼ jAAu
P U D j. However, they only apply to PHR access
P  Oðt2
c Þ in the RNS scheme,
in the PUD. In addition, S 0
while SP  Oðtc logtc Þ for the rest.
The results are given in Table 5. The ciphertext size only
accounts for the encryption of F EK . In our scheme, for
simplicity we assume there is only one PUD, thus the
ciphertext includes m additional wildcard attributes and up
to N   1 dummy attributes. Our scheme requires a secret
key size that is linear with jAAu j, the number of attributes of
each user, while in the VFJPS and BCHL schemes this is
linear with No , since a user needs to obtain at least one key
from each owner whose PHR file the user wants to access.
For public key size, we count the size of the effective
information that each user needs to obtain. The VFJPS
scheme requires each owner to publish a directed acyclic
graph representing her ACL along with key assignments,
which essentially amounts to OðNu Þ per owner. This puts a
large burden either in communication or storage cost on the
system. For rekeying, we consider revocation of one user by
an owner in VFJPS and BCHL. In VFJPS, revoking one user
from a file may need overencryption and issuing of new
public tokens for all the rest of users in the worst case. The
NGS scheme achieves direct user revocation using ABBE,
which eliminates the need of rekeying and reencryption;
however, attribute revocation is not achieved; and for the
revocable ABBE in [32], either the ciphertext size is linear
with the number of revoked users, or the public key is linear
with the total number of users in the system.4 For the RNS
scheme, the main drawback is the large size of revocation
messages to be transmitted to nonrevoked users.
In our scheme, revocation of one user u requires
revoking a minimum set of data attributes that makes her
access structure unsatisfiable. From Table 5, it can be seen
that our scheme has much smaller secret key size compared
with VFJPS and BCHL, smaller rekeying message size than

4. In Table 5, for NGS scheme we only listed the efficiency of one of the
two constructions in [32]. m and l are the maximum number of attributes in
a ciphertext policy and user’s secret key, respectively.

VFJPS, HN, and RNS, the size of ciphertext is smaller than
NGS while being comparable with HN and RNS. The public
key size is smal ler than VFJPS and BCHL , and is
comparable with that of RNS; while it seems larger than
those of HN and NGS, note that we can use the large
universe constructions [21]
to dramatically reduce the
public key size. Overall, compared with non-ABE schemes,
our scheme achieves higher scalability in key management.
Compared with existing revocable ABE schemes, the main
advantage of our solution is small rekeying message sizes.
To revoke a user, the maximum rekeying message size is
linear with the number of attributes in that user’s secret key.
These indicate our scheme is more scalable than existing
works. To further show the storage and communication costs,
we provide a numerical analysis using typical parameter
settings in the supplementary material, available online.

6.2 Computation Costs
Next, we evaluate the computational cost of our scheme
through combined implementation and simulation. We
provide the first implementation of the GPSW KP-ABE
scheme [35] (to the best of our knowledge), and also
integrated the ABE algorithms into a prototype PHR
system, Indivo [27], [36]. The GPSW KP-ABE scheme is
tested on a PC with 3.4 GHz processor, using the pairing-
based cryptography (PBC) library [37]. The public para-
meters are chosen to provide 80 bits security level, and we
use a pairing-friendly type-A 160-bit elliptic curve group
[37]. This parameter setting has also been adopted in other
related works in ABE [19], [38]. We then use the ABE
algorithms to encrypt randomly generated XML-formatted
files (since real PHR files are difficult to obtain), and
implement the user-interfaces for data input and output.
Due to space limitations, the details of prototype imple-
mentation are reported in [36].
In the supplementary material, available online, (Fig. 2),
we present benchmarks of cryptographic operations and
detailed timing results for the two ABE algorithms used by
our framework. It is shown that, the decryption operation
in our enhanced MA-ABE scheme is quite fast, because it
involves only jAAC
P UD j þ 1 pairing operations (in contrast,
the RNS scheme involves 2jAAC
P U D j þ 1 pairing operations).
The time costs of key generation, encryption, and decryp-
tion processes are all linear with the number of attributes.
For 50 attributes, they all take less than 0.5 s.
From the system aspect, each data owner (patient) uses
the YWRL ABE scheme for setup, key generation and
revocation, uses both YWRL and enhanced MA-ABE for
encryption. Each PSD user adopts the YWRL scheme for
decryption, while each PUD user adopts the enhanced
MA-ABE scheme for decryption. Each AA uses enhanced
MA-ABE for setup, key generation and revocation. Next,
we provide estimations of computation times of each party
in the system in Table 6. The values are calculated from the
example parameters and benchmark results, where ex-
ponentiation times Exp1 ¼ 6:4 ms, ExpT ¼ 0:6 ms, pairing
time TP ¼ 2:5 ms.
Finally, we simulate the server’s computation cost spent
in user revocation to evaluate the system performance of
user revocation. Especially,
the lazy-revocation method
greatly reduces the cost of revocation, because it aggregates
multiple ciphertext/key update operations, which amor-
tizes the computations over time. The details of
the

142

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 1,

JANUARY 2013

TABLE 5
Comparison of Efficiency

TABLE 6
Computation Complexity for Each Party in the System, and Numerical Estimation of Time Costs Assuming Following Parameters
(Also Used in Supplementary Material, Available Online): jU D j ¼ 50, jU R j ¼ 100, N ¼ 5 (Number of AAs), jAAC
P SD j ¼ 5, jAAC
P UD j ¼ 35,
jAAu j ¼ m ¼ 15, jLðT Þj ¼ 10, j 0 j ¼ 5 (a Minimal Number of Attributes to Revoke a User)

experimental/simulation evaluation results are presented
in the supplementary material, available online.

7 CONCLUSION

In this paper, we have proposed a novel framework of
secure sharing of personal health records in cloud comput-
ing. Considering partially trustworthy cloud servers, we
argue that
to fully realize the patient-centric concept,
patients shall have complete control of their own privacy
through encrypting their PHR files to allow fine-grained
access. The framework addresses the unique challenges
brought by multiple PHR owners and users, in that we
greatly reduce the complexity of key management while
enhance the privacy guarantees compared with previous
works. We utilize ABE to encrypt the PHR data, so that
patients can allow access not only by personal users, but
also various users from public domains with different
professional roles, qualifications, and affiliations. Further-
more, we enhance an existing MA-ABE scheme to handle
efficient and on-demand user revocation, and prove its
security. Through implementation and simulation, we show
that our solution is both scalable and efficient.

ACKNOWLEDGMENTS

This work was supported in part by the US National Science
Foundation under grants CNS-0831628, CNS-0831963, CNS-
1054317, CNS-1116939, and CNS-1155988. Ming Li’s work
was also supported in part by a USU seed grant 100022. The
preliminary version of this paper appeared in SecureComm
2010 [1].

REFERENCES
[1] M. Li, S. Yu, K. Ren, and W. Lou, “Securing Personal Health
Records in Cloud Computing: Patient-Centric and Fine-Grained
Data Access Control in Multi-Owner Settings,” Proc. Sixth Int’l
ICST Conf. Security and Privacy in Comm. Networks (SecureComm ’10),
pp. 89-106, Sept. 2010.

[5]

[6]

[9]

[8]

[4]

[2] H. Lo¨ hr, A.-R. Sadeghi, and M. Winandy, “Securing the E-Health
Cloud,” Proc. First ACM Int’l Health Informatics Symp. (IHI ’10),
pp. 220-229, 2010.
[3] M. Li, S. Yu, N. Cao, and W. Lou, “Authorized Private Keyword
Search over Encrypted Personal Health Records in Cloud
Computing,” Proc. 31st Int’l Conf. Distributed Computing Systems
(ICDCS ’11), June 2011.
“The Health Insurance Portability and Accountability Act,”
http ://www .cms .hhs .gov/HIPAAGenInfo/01_Overv iew .asp ,
2012.
“Google, Microsoft Say Hipaa Stimulus Rule Doesn’t Apply to
Them,” http://www.ihealthbeat.org/Articles/2009/4/8/, 2012.
“At Risk of Exposure - in the Push for Electronic Medical Records,
Concern Is Growing About How Well Privacy Can Be Safe-
guarded ,” h t tp ://ar t ic les . lat imes .com/2006/ jun/26/hea l th/
he-privacy26, 2006.
[7] K.D. Mandl, P. Szolovits, and I.S. Kohane, “Public Standards and
Patients’ Control: How to Keep Electronic Medical Records
Accessible but Private,” BMJ, vol. 322, no. 7281, pp. 283-287,
Feb. 2001.
J. Benaloh, M. Chase, E. Horvitz, and K. Lauter, “Patient
Controlled Encryption: Ensuring Privacy of Electronic Medical
Records,” Proc. ACM Workshop Cloud Computing Security
(CCSW ’09), pp. 103-114, 2009.
S. Yu, C. Wang, K. Ren, and W. Lou, “Achieving Secure, Scalable,
and Fine-Grained Data Access Control in Cloud Computing,”
Proc. IEEE INFOCOM ’10, 2010.
[10] C. Dong, G. Russello, and N. Dulay, “Shared and Searchable
Encrypted Data for Untrusted Servers,” J. Computer Security,
vol. 19, pp. 367-397, 2010.
[11] V. Goyal, O. Pandey, A. Sahai, and B. Waters, “Attribute-Based
Encryption for Fine-Grained Access Control of Encrypted Data,”
Proc. 13th ACM Conf. Computer and Comm. Security (CCS ’06),
pp. 89-98, 2006.
[12] M. Li, W. Lou, and K. Ren, “Data Security and Privacy in Wireless
Body Area Networks,” IEEE Wireless Comm. Magazine, vol. 17,
no. 1, pp. 51-58, Feb. 2010.
[13] A. Boldyreva, V. Goyal, and V. Kumar, “Identity-Based Encryp-
tion with Efficient Revocation,” Proc. 15th ACM Conf. Computer and
Comm. Security (CCS), pp. 417-426, 2008.
[14] L. Ibraimi, M. Petkovic, S. Nikova, P. Hartel, and W. Jonker,
“Ciphertext-Policy Attribute-Based Threshold Decryption with
Flexible Delegation and Revocation of User Attributes,” 2009.
[15] S. Yu, C. Wang, K. Ren, and W. Lou, “Attribute Based Data
Sharing with Attribute Revocation,” Proc. Fifth ACM Symp.
Information, Computer and Comm. Security (ASIACCS ’10), 2010.

LI ET AL.: SCALABLE AND SECURE SHARING OF PERSONAL HEALTH RECORDS IN CLOUD COMPUTING USING ATTRIBUTE-BASED...

143

[19]

[23]

[20]

[16] S. Narayan, M. Gagne´ , and R. Safavi-Naini, “Privacy Preserving
EHR System Using Attribute-Based Infrastructure,” Proc. ACM
Cloud Computing Security Workshop (CCSW ’10), pp. 47-52, 2010.
[17] X. Liang, R. Lu, X. Lin, and X.S. Shen, “Patient Self-Controllable
Access Policy on Phi in Ehealthcare Systems,” Proc. Advances in
Health Informatics Conf. (AHIC 10), 2010.
[18] L. Ibraimi, M. Asim, and M. Petkovic, “Secure Management of
Personal Health Records by Applying Attribute-Based Encryp-
tion,” technical report, Univ. of Twente, 2009.
J. Bethencourt, A. Sahai, and B. Waters, “Ciphertext-Policy
Attribute-Based Encryption,” Proc. IEEE Symp. Security and Privacy
(SP ’07), pp. 321-334, 2007.
J.A. Akinyele, C.U. Lehmann, M.D. Green, M.W. Pagano, Z.N.J.
Peterson, and A.D. Rubin, “Self-Protecting Electronic Medical
Records Using Attribute-Based Encryption,” Cryptology ePrint
Archive, Report 2010/565, http://eprint.iacr.org/, 2010.
[21] M. Chase and S.S. Chow, “Improving Privacy and Security in
Multi-Authority Attribute-Based Encryption,” Proc. 16th ACM
Conf. Computer and Comm. Security (CCS ’09), pp. 121-130, 2009.
[22] X. Liang, R. Lu, X. Lin, and X.S. Shen, “Ciphertext Policy Attribute
Based Encryption with Efficient Revocation,” technical report,
Univ. of Waterloo, 2010.
J. Hur and D.K. Noh, “Attribute-Based Access Control with
Efficient Revocation in Data Outsourcing Systems,” IEEE Trans.
Parallel and Distributed Systems, vol. 22, no. 7, pp. 1214-1221, July
2011.
[24] S. Jahid, P. Mittal, and N. Borisov, “Easier: Encryption-Based
Access Control in Social Networks with Efficient Revocation,”
Proc. ACM Symp.
Information, Computer and Comm. Security
(ASIACCS), Mar. 2011.
[25] S. Ruj, A. Nayak, and I. Stojmenovic, “DACC: Distributed Access
Control in Clouds,” Proc. IEEE 10th Int’l Conf. Trust, Security and
Privacy in Computing and Comm. (TrustCom), 2011.
[26] A. Lewko and B. Waters, “Decentralizing Attribute-Based
Encryption,” EUROCRYPT: Proc. 30th Ann. Int’l Conf. Theory and
Applications of Cryptographic Techniques: Advances in Cryptology,
pp. 568-588, 2011.
[27] “Indivo.” http://indivohealth.org/, 2012.
[28] S.D.C. di Vimercati, S. Foresti, S. Jajodia, S. Paraboschi, and P.
Samarati, “Over-Encryption: Management of Access Control
Evolution on Outsourced Data,” Proc. 33rd Int’l Conf. Very Large
Data Bases (VLDB ’07), pp. 123-134, 2007.
[29] A. Lewko and B. Waters, “Decentralizing Attribute-Based
Encryption,” EUROCRYPT: Proc. 30th Ann. Int’l Conf. Theory and
Applications of Cryptographic Techniques: Advances in Cryptology,
pp. 568-588, 2011.
[30] A. Perrig, R. Szewczyk, J.D. Tygar, V. Wen, and D.E. Culler,
“Spins: Security Protocols for Sensor Networks,” Wireless Network-
ing, vol. 8, pp. 521-534, Sept. 2002.
[31] H. Yang, H. Luo, F. Ye, S. Lu, and L. Zhang, “Security in Mobile
Ad Hoc Networks: Challenges and Solutions,” IEEE Wireless
Comm., vol. 11, no. 1, pp. 38-47, Feb. 2004.
[32] N. Attrapadung and H.
Imai, “Conjunctive Broadcast and
Attribute-Based Encryption,” Proc. Third Int’l Conf. Palo Alto on
Pairing-Based Cryptography-Pairing, pp. 248-265, 2009.
[33] S. Mu¨ ller, S. Katzenbeisser, and C. Eckert, “Distributed Attribute-
Based Encryption,” Proc. 11th Int’l Conf. Information Security and
Cryptology (ICISC 08), pp. 20-36, 2009.
[34] S. Chow, “New Privacy-Preserving Architectures for Identity-/
Attribute-Based Encryption,” PhD thesis, NYU, 2010.
[35] Y. Zheng, “Key-Policy Attribute-Based Encryption Scheme Im-
plementation,”
http://www.cnsr.ictas.vt.edu/resources.html,
2012.
[36] Y. Zheng, “Privacy-Preserving Personal Health Record System
Using Attribute-Based Encryption,” master’s thesis, Worcester
Polytechnic Inst., 2011.
[37] B. Lynn, “The Pbc Library,” http://crypto.stanford.edu/pbc/,
2012.
[38] M. Pirretti, P. Traynor, P. McDaniel, and B. Waters, “Secure
Attribute-Based Systems,” J. Computer Security, vol. 18, no. 5,
pp. 799-837, 2010.

Ming Li (S’08-M’11) received the BE and ME
degrees both in electronic and information
engineering from Beihang University in China
and the PhD degree in electrical and computer
engineering from Worcester Polytechnic Insti-
tute in 2011. He is an assistant professor in the
Computer Science Department at Utah State
University. His current research interests are in
cyber security and privacy, with emphases on
data security and privacy in cloud computing,
security in wireless networks and cyber-physical systems. He is a
member of the IEEE and the ACM.

Shucheng Yu (S’07-M’10)
received the BS
degree in computer science from Nanjing Uni-
versity of Post & Telecommunication in China,
the MS degree in computer sc ience from
Tsinghua University, and the PhD degree in
electrical and computer engineering from Wor-
cester Polytechn ic Inst itute. He joined the
Computer Science Department at the University
of Arkansas at Little Rock as an assistant
professor in 2010. His research interests are in
the general areas of Network Security and Applied Cryptography. His
current
research interests include Secure Data Services in cloud
computing, Attribute-Based Cryptography, and Security and Privacy
Protection in Cyber Physical Systems. He is a member of the IEEE.

Yao Zheng (S’11) received the BS degree in
microelectronic from Fudan University in 2007
and the MS degree in electrical engineering from
Worcester Polytechnic Institute in 2011. He is
working toward the PhD student at Virginia
Tech. Between 2007 and 2009, he worked as
a R&D developer for Siemens RTS Department
focusing on industrial networks. His MS thesis
concentrates on EMR, PHR integration and
development of secure protocol and interface
between E-health cloud and local hospitals. His current interest are in
android application security and linux kernel development. He is a
student member of the IEEE.

Kui Ren (SM’11) received the BEng and MEng
degrees both from Zhejiang University in 1998
and 2001, respectively, and the PhD degree in
electrical and computer engineering from Wor-
cester Polytechnic Institute in 2007. He is an
assistant professor in the Department of Elec-
tr ical and Computer Eng ineer ing at
I ll inois
Institute of Technology. His research focuses
on data service outsourcing security in cloud
computing, secure computation outsourcing in
cloud computing, and cyber physical system security. His research is
supported by US national Science Foundation (NSF), US Department of
Energy (DOE), AFRL, and Amazon. He serves on the editorial boards of
IEEE Transactions on Smart Grid and IEEE Wireless Communications.
He is a member of Internet Privacy Task Force of Illinois State. He is a
recipient of NSF CAREER Award in 2011 and a corecipient of IEEE
ICNP’11 best paper award. He is a senior member of the IEEE and a
member of the ACM.

Wenjing Lou (S’01-M’03-SM’08) received the
PhD degree in electrical and computer engineer-
ing at the University of Florida in 2003. She is an
associate professor at Virg in ia Po lytechn ic
Institute and State University. Prior to joining
Virginia Tech in 2011, she was on the faculty of
Worcester Polytechnic Institute from 2003 to
2011. Her current research interests are in cyber
security, with emphases on wireless network
security and data security and privacy in cloud
computing. She serves on the editorial board of multiple premier IEEE
journals and has chaired multiple security conferences or symposiums.
She was a recipient of the US National Science Foundation (NSF)
CAREER award in 2008. She is a senior member of the IEEE.

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 5, MAY 2013

951

Load Rebalancing for Distributed
File Systems in Clouds

Hung-Chang Hs iao, Member, IEEE Computer Soc iety, Hsueh-Y i Chung,
Ha iy ing Shen, Member, IEEE, and Yu-Chang Chao

Abstract—Distributed file systems are key building blocks for cloud computing applications based on the MapReduce programming
paradigm. In such file systems, nodes simultaneously serve computing and storage functions; a file is partitioned into a number of
chunks allocated in distinct nodes so that MapReduce tasks can be performed in parallel over the nodes. However, in a cloud
computing environment, failure is the norm, and nodes may be upgraded, replaced, and added in the system. Files can also be
dynamically created, deleted, and appended. This results in load imbalance in a distributed file system; that is, the file chunks are not
distributed as uniformly as possible among the nodes. Emerging distributed file systems in production systems strongly depend on a
central node for chunk reallocation. This dependence is clearly inadequate in a large-scale, failure-prone environment because the
central load balancer is put under considerable workload that is linearly scaled with the system size, and may thus become the
performance bottleneck and the single point of failure. In this paper, a fully distributed load rebalancing algorithm is presented to cope
with the load imbalance problem. Our algorithm is compared against a centralized approach in a production system and a competing
distributed solution presented in the literature. The simulation results indicate that our proposal is comparable with the existing
centralized approach and considerably outperforms the prior distributed algorithm in terms of load imbalance factor, movement cost,
and algorithmic overhead. The performance of our proposal implemented in the Hadoop distributed file system is further investigated in
a cluster environment.

Index Terms—Load balance, distributed file systems, clouds

Ç

1 INTRODUCTION
C LOUD Computing (or cloud for short) is a compelling
technology. In clouds, clients can dynamically allocate
their resources on-demand without sophisticated deploy-
ment and management of resources. Key enabling technol-
ogies for clouds include the MapReduce programming
paradigm [1], distributed file systems (e.g.,
[2],
[3]),
virtualization (e.g., [4], [5]), and so forth. These techniques
emphasize scalability, so clouds (e.g., [6]) can be large in
scale, and comprising entities can arbitrarily fail and join
while maintaining system reliability.
Distributed file systems are key building blocks for cloud
computing applications based on the MapReduce program-
ming paradigm. In such file systems, nodes simultaneously
serve computing and storage functions; a file is partitioned
into a number of chunks allocated in distinct nodes so that

. H.-C. Hsiao is with the Department of Computer Science and Information
Engineering, National Cheng Kung University, Tainan 70101, Taiwan.
E-mail: hchsiao@csie.ncku.edu.tw.
. H.-Y. Chung is with the Department of Computer Science and Information
Engineering, Distributed Computing Research Laboratory, National Cheng
Kung University, Tainan 70101, Taiwan.
E-mail: p7697138@mail.ncku.edu.tw.
. H. Shen is with the Holcombe Department of Electrical and Computer
Engineering, Clemson University, Clemson, SC 29634.
E-mail: shenh@clemson.edu.
. Y.-C. Chao is with the Cloud Platform Technology Department, Cloud
Service Application Center, Industrial Technology Research Institute
South Campus, Tainan 709, Taiwan. E-mail: ycchao@itri.org.tw.

Manuscript received 18 Jan. 2012; revised 27 May 2012; accepted 7 June 2012;
published online 22 June 2012.
Recommended for acceptance by H. Jiang.
For information on obtaining reprints of this article, please send e-mail to:
tpds@computer.org, and reference IEEECS Log Number TPDS-2012-01-0037.
Digital Object Identifier no. 10.1109/TPDS.2012.196.

MapReduce tasks can be performed in parallel over the
nodes. For example, consider a wordcount application that
counts the number of distinct words and the frequency of
each unique word in a large file. In such an application, a
cloud partitions the file into a large number of disjointed
and fixed-size pieces (or file chunks) and assigns them to
different cloud storage nodes (i.e., chunkservers). Each
storage node (or node for short)
then calculates the
frequency of each unique word by scanning and parsing
its local file chunks.
In such a distributed file system, the load of a node is
typically proportional to the number of file chunks the node
possesses [3]. Because the files in a cloud can be arbitrarily
created, deleted, and appended, and nodes can be up-
graded, replaced and added in the file system [7], the file
chunks are not distributed as uniformly as possible among
the nodes. Load balance among storage nodes is a critical
function in clouds. In a load-balanced cloud, the resources
can be well utilized and provisioned, maximizing the
performance of MapReduce-based applications.
State-of-the-art distributed file systems (e.g., Google GFS
[2] and Hadoop HDFS [3]) in clouds rely on central nodes to
manage the metadata information of the file systems and to
balance the loads of storage nodes based on that metadata.
The centralized approach simplifies the design and im-
plementation of a distributed file system. However, recent
experience (e.g., [8]) concludes that when the number of
storage nodes, the number of files and the number of
accesses to files increase linearly, the central nodes (e.g., the
master in Google GFS) become a performance bottleneck, as
they are unable to accommodate a large number of file
accesses due to clients and MapReduce applications. Thus,

1045-9219/13/$31.00 ß 2013 IEEE

Published by the IEEE Computer Society

952

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 5, MAY 2013

depending on the central nodes to tackle the load imbalance
problem exacerbate their heavy loads. Even with the latest
development in distributed file systems, the central nodes
may still be overloaded. For example, HDFS federation [9]
suggests an architecture with multiple namenodes (i.e., the
nodes managing the metadata information). Its file system
namespace is statically and manually partitioned to a
number of namenodes. However, as the workload experi-
enced by the namenodes may change over time and no
adaptive workload consolidation and/or migration scheme
is offered to balance the loads among the namenodes, any of
the namenodes may become the performance bottleneck.
In this paper, we are interested in studying the load
rebalancing problem in distributed file systems specialized
for large-scale, dynamic and data-intensive clouds. (The
terms “rebalance” and “balance” are interchangeable in this
paper.) Such a large-scale cloud has hundreds or thousands
of nodes (and may reach tens of thousands in the future).
Our objective is to allocate the chunks of files as uniformly
as possible among the nodes such that no node manages an
excessive number of chunks. Additionally, we aim to
reduce network traffic (or movement cost) caused by
rebalancing the loads of nodes as much as possible to
maximize the network bandwidth available to normal
applications. Moreover, as failure is the norm, nodes are
newly added to sustain the overall system performance [2],
[3], resulting in the heterogeneity of nodes. Exploiting
capable nodes to improve the system performance is, thus,
demanded.
Specifically, in this study, we suggest offloading the load
rebalancing task to storage nodes by having the storage
nodes balance their loads spontaneously. This eliminates
the dependence on central nodes. The storage nodes are
structured as a network based on distributed hash tables
(DHTs), e.g., [10], [11], [12]; discovering a file chunk can
simply refer to rapid key lookup in DHTs, given that a
unique handle (or identifier) is assigned to each file chunk.
DHTs enable nodes to self-organize and -repair while
constantly offering lookup functionality in node dynamism,
simplifying the system provision and management.
In summary, our contributions are threefold as follows:

.

.

By leveraging DHTs, we present a load rebalancing
algorithm for distributing file chunks as uniformly
as possible and minimizing the movement cost as
much as possible. Particularly, our proposed algo-
rithm operates in a distributed manner in which
nodes perform their load-balancing tasks indepen-
dently without synchronization or global knowledge
regarding the system.
Load-balancing algorithms based on DHTs have
been extensively studied (e.g., [13], [14], [15], [16],
[17],
[18],
[19],
[20],
[21],
[22]). However, most
existing solutions are designed without considering
both movement cost and node heterogeneity and
may introduce significant maintenance network
traffic to the DHTs. In contrast, our proposal not
only takes advantage of physical network locality in
the reallocation of file chunks to reduce the move-
ment cost but also exploits capable nodes to improve
the overall system performance. Additionally, our

algorithm reduces algorithmic overhead introduced
to the DHTs as much as possible.
. Our proposal is assessed through computer simula-
tions. The simulation results indicate that although
each node performs our load rebalancing algorithm
independently without acquiring global knowledge,
our proposal
is comparable with the centralized
approach in Hadoop HDFS [3] and remarkably
outperforms the competing distributed algorithm
in [14] in terms of load imbalance factor, movement
cost, and algorithmic overhead. Additionally, our
load-balancing algorithm exhibits a fast convergence
rate. We derive analytical models to validate the
efficiency and effectiveness of our design. Moreover,
we have implemented our load-balancing algorithm
in HDFS and investigated its performance in a
cluster environment.
The remainder of the paper is organized as follows: the
load rebalancing problem is formally specified in Section 2.
Our load-balancing algorithm is presented in Section 3. We
evaluate our proposal through computer simulations and
discuss the simulation results in Section 4. In Section 5, the
performance of our proposal is further investigated in a
cluster environment. Our study is summarized in Section 6.
Due to space limitation, we defer the extensive discussion
of related works in the appendix, which can be found on
the Computer Society Digital Library at http://doi.
ieeecomputersociety.org/10.1109/TPDS.2012.196.

2 LOAD REBALANCING PROBLEM

We consider a large-scale distributed file system consisting
of a set of chunkservers V in a cloud, where the cardinality of
V is jV j ¼ n. Typically, n can be 1,000, 10,000, or more. In
f iles are stored in the n
the sys tem , a number of
chunkservers. First, let us denote the set of files as F . Each
file f 2 F is partitioned into a number of disjointed, fixed-
size chunks denoted by Cf . For example, each chunk has
the same size, 64 Mbytes, in Hadoop HDFS [3]. Second, the
load of a chunkserver is proportional to the number of
chunks hosted by the server [3]. Third, node failure is the
norm in such a distributed system, and the chunkservers
may be upgraded, replaced and added in the system.
Finally, the files in F may be arbitrarily created, deleted,
and appended. The net effect results in file chunks not
being uniformly distributed to the chunkservers. Fig. 1
illustrates an example of the load rebalancing problem with
the assumption that the chunkservers are homogeneous
and have the same capacity.
Our objective in the current study is to design a load
rebalancing algorithm to reallocate file chunks such that the
chunks can be distributed to the system as uniformly as
possible while reducing the movement cost as much as
possible. Here, the movement cost is defined as the number
of chunks migrated to balance the loads of the chunkser-
vers. Let A be the ideal number of chunks that any
chunkserver i 2 V is required to manage in a system-wide
load-balanced state, that is,

HSIAO ET AL.: LOAD REBALANCING FOR DISTRIBUTED FILE SYSTEMS IN CLOUDS

953



P
Fig. 1. An example illustrates the load rebalancing problem, where (a) an initial distribution of chunks of six files f1 , f2 , f3 , f4 , f5 , and f6 in three nodes
N1 , N2 , and N3 , (b) files f2 and f5 are deleted, (c) f6 is appended, and (d) node N4 joins. The nodes in (b), (c), and (d) are in a load-imbalanced state.
f 2F Cf
n

A ¼

ð1Þ

:

Then, our load rebalancing algorithm aims to minimize the
load imbalance factor in each chunkserver i as follows:
ð2Þ
kLi   Ak;
where Li denotes the load of node i (i.e., the number of file
chunks hosted by i) and k  k represents the absolute value
function. Note that “chunkservers” and “nodes” are
interchangeable in this paper.
Theorem 1. The load rebalancing problem is N P -hard.
Proof. By restriction, an instance of the decision version of
the load rebalancing problem is the knapsack problem
[23]. That is, consider any node i 2 V . i seeks to store a
subset of the file chunks in F such that the number of
chunks hosted by i is not more than A, and the “value”
of the chunks hosted is at least , which is defined as the
inverse of the sum of the movement cost caused by the
tu
migrated chunks.

To simplify the discussion, we first assume a homo-
geneous environment, where migrating a file chunk
between any two nodes takes a unit movement cost and
each chunkserver has the identical storage capacity. How-
ever, we will later deal with the practical considerations of
node capacity heterogeneity and movement cost based on
chunk migration in physical network locality.

3 OUR PROPOSAL

Table 1 in Appendix B, which is available in the online
supplemental material, summarizes the notations frequently
used in the following discussions for ease of reference.

3.1 Architecture
The chunkservers in our proposal are organized as a DHT
network; that is, each chunkserver implements a DHT
protocol such as Chord [10] or Pastry [11]. A file in the
system is partitioned into a number of fixed-size chunks,
and “each” chunk has a unique chunk handle (or chunk
identifier) named with a globally known hash function such
as SHA1 [24]. The hash function returns a unique identifier
for a given file’s pathname string and a chunk index. For
example, the identifiers of the first and third chunks of file
“/user/tom/tmp/a.log” are , respectively , SHA1(/
user/tom/tmp/a.log, 0) and SHA1(/user/tom/
tmp/a.log, 2). Each chunkserver also has a unique ID.
the chunkservers in V by
We represent
the IDs of
1
n ; 2
n ; 3
n ; . . . ; n
for short, denote the n chunkservers as
n ;

1; 2; 3; . . . ; n. Unless otherwise clearly indicated, we denote
the successor of chunkserver i as chunkserver i þ 1 and the
successor of chunkserver n as chunkserver 1. In a typical
DHT, a chunkserver i hosts the file chunks whose handles
n, except for chunkserver n, which manages
are within ði 1
n ; i
n.
the chunks whose handles are in ðn
n ; 1
To discover a file chunk, the DHT lookup operation is
performed. In most DHTs, the average number of nodes
visited for a lookup is Oðlog nÞ [10], [11] if each chunkserver
i maintains log2 n neighbors, that is, nodes i þ 2k mod n for
k ¼ 0; 1; 2; . . . ; log2 n   1. Among the log2 n neighbors, the
one i þ 20 is the successor of i. To look up a file with l
chunks, l lookups are issued.
DHTs are used in our proposal for the following reasons:

.

The chunkservers self-configure and self-heal in our
proposal because of their arrivals, departures, and
failures, simplifying the system provisioning and
management. Specifically, typical DHTs guarantee
that if a node leaves, then its locally hosted chunks
are reliably migrated to its successor; if a node joins,
then it allocates the chunks whose IDs immediately
precede the joining node from its successor to
manage. Our proposal heavily depends on the node
arrival and departure operations to migrate file
chunks among nodes. Interested readers are referred
to [10], [11] for the details of the self-management
technique in DHTs.
. While lookups take a modest delay by visiting
Oðlog nÞ nodes in a typical DHT, the lookup latency
can be reduced because discovering the l chunks of a
file can be performed in parallel. On the other hand,
our proposal is independent of the DHT protocols.
To further reduce the lookup latency, we can adopt
state-of-the-art DHTs such as Amazon’s Dynamo in
[12] that offer one-hop lookup delay.
The DHT network is transparent to the metadata
management in our proposal. While the DHT net-
work specifies the locations of chunks, our proposal
can be integrated with existing large-scale distribu-
ted file systems, e.g., Google GFS [2] and Hadoop
HDFS [3],
in which a centralized master node
manages the namespace of the file system and the
mapping of file chunks to storage nodes. Specifically,
to incorporate our proposal with the master node in
GFS, each chunkserver periodically piggybacks its
locally hosted chunks’ information to the master in a
heartbeat message [2] so that the master can gather
the locations of chunks in the system.

.

954

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 5, MAY 2013

.

In DHTs, if nodes and file chunks are designated
with uniform IDs, the maximum load of a node is
guaranteed to be Oðlog nÞ times the average in a
nÞ [14], [16], [25], thus balancing
probability of 1   Oð1
the loads of nodes to a certain extent. However, our
proposal presented in Section 3.2 performs well for
both uniform and nonuniform distributions of IDs of
nodes and file chunks due to arbitrary file creation/
deletion and node arrival/departure.
As discussed, the load rebalancing problem defined in
Section 2 is N P -hard (Theorem 1), which is technically
challenging and thus demands an in-depth study. Ortho-
gonal issues such as metadata management, file consistency
models, and replication strategies are out of the scope of our
study, and independent studies are required.

3.2 Load Rebalancing Algorithm

3.2.1 Overview
A large-scale distributed file system is in a load-balanced
state if each chunkserver hosts no more than A chunks. In
our proposed algorithm, each chunkserver node i first
estimates whether it is underloaded (light) or overloaded
(heavy) without global knowledge. A node is light if the
number of chunks it hosts is smaller than the threshold of
ð1   L ÞA (where 0  L < 1). In contrast, a heavy node
manages the number of chunks greater than ð1 þ U ÞA,
where 0  U < 1. L and U are system parameters. In
the following discussion, if a node i departs and rejoins as a
successor of another node j, then we represent node i as
node j þ 1, node j’s original successor as node j þ 2, the
successor of node j’s original successor as node j þ 3, and
so on. For each node i 2 V , if node i is light, then it seeks a
heavy node and takes over at most A chunks from the
heavy node.
We first present a load-balancing algorithm, in which
each node has global knowledge regarding the system, that
leads to low movement cost and fast convergence. We then
extend this algorithm for the situation that
the global
knowledge is not available to each node without degrading
its performance. Based on the global knowledge, if node i
finds it is the least-loaded node in the system, i leaves the
system by migrating its locally hosted chunks to its
successor i þ 1 and then rejoins instantly as the successor
of the heaviest node (say, node j). To immediately relieve
node j’s load, node i requests minfLj   A; Ag chunks from
j. That is, node i requests A chunks from the heaviest node j
if j’s load exceeds 2A; otherwise, i requests a load of Lj   A
from j to relieve j’s load.
Node j may still remain as the heaviest node in the
system after it has migrated its load to node i. In this case,
the current least-loaded node, say node i0 , departs and then
rejoins the system as j’s successor. That is, i0 becomes node
j þ 1, and j’s original successor i thus becomes node j þ 2.
Such a process repeats iteratively until j is no longer the
heaviest. Then, the same process is executed to release the
extra load on the next heaviest node in the system. This
process repeats until all the heavy nodes in the system
become light nodes. Such a load-balancing algorithm by
mapping the least-loaded and most-loaded nodes in the
system has properties as follows:

.

.

Low movement cost. As node i is the lightest node
among all chunkservers,
the number of chunks
migrated because of i’s departure is small with the
goal of reducing the movement cost.
Fast convergence rate. The least-loaded node i in the
system seeks to relieve the load of the heaviest node
j, leading to quick system convergence towards the
load-balanced state.
The mapping between the lightest and heaviest nodes at
each time in a sequence can be further improved to reach
the global load-balanced system state. The time complexity
of the above algorithm can be reduced if each light node can
know which heavy node it needs to request chunks
beforehand, and then all
light nodes can balance their
loads in parallel. Thus, we extend the algorithm by pairing
the top-k1 underloaded nodes with the top-k2 overloaded
nodes. We use U to denote the set of top-k1 underloaded
nodes in the sorted list of underloaded nodes, and use O to
denote the set of top-k2 overloaded nodes in the sorted list
of overloaded nodes. Based on the above-introduced load-
balancing algorithm, the light node that should request
2  k2 ) most loaded node in O is the
chunks from the k0
2 th (k0
3
2
P
1  k1 ) least loaded node in U , and
k0
1 th (k0
7777777;
6666666
k0
Li   A
ð
Þ
ith most loaded node 2 O
2
i ¼ 1
1 ¼
k0
A
P
k0
ðLi   AÞ denotes the sum of the excess
2
where
ith most loaded node 2O
2 heavy nodes. It means that the top-k0
loads in the top-k0
i¼1
1
light nodes should leave and rejoin as successors of the top-
k0
2 overloaded nodes. Thus, according to (3), based on its
1 in U , each light node can compute k0
position k0
2 to identify
the heavy node to request chunks. Light nodes concurrently
request chunks from heavy nodes, and this significantly
reduces the latency of the sequential algorithm in achieving
the global system load-balanced state.
We have introduced our algorithm when each node has
global knowledge of the loads of all nodes in the system.
However, it is a formidable challenge for each node to have
such global knowledge in a large-scale and dynamic
computing environment. We then introduce our basic
algorithms that perform the above idea in a distributed
manner without global knowledge in Sections 3.2.2. Section
3.2.3 improves our proposal by taking advantage of
physical network locality to reduce network traffic caused
by the migration of file chunks. Recall that we first assume
that the nodes have identical capacities in order to simplify
the discussion. We then discuss the exploitation of node
capacity heterogeneity in Section 3.2.4. Finally, high file
availability is usually demanded from large-scale and
dynamic distributed storage systems that are prone to
failures. To deal with this issue, Section 3.2.5 discusses the
maintenance of replicas for each file chunk.

ð3Þ

3.2.2 Basic Algorithms
Algorithms 1 and 2 (see Appendix C, which is available in
the online supplemental material) detail our proposal;

HSIAO ET AL.: LOAD REBALANCING FOR DISTRIBUTED FILE SYSTEMS IN CLOUDS

955

Algorithm 1 specifies the operation that a light node i seeks
an overloaded node j, and Algorithm 2 shows that i requests
some file chunks from j. Without global knowledge, pairing
the top-k1 light nodes with the top-k2 heavy nodes is clearly
challenging. We tackle this challenge by enabling a node to
execute the load-balancing algorithm introduced in Section
3.2.1 based on a sample of nodes. In the basic algorithm, each
node implements the gossip-based aggregation protocol in [26]
and [27] to collect the load statuses of a sample of randomly
selected nodes. Specifically, each node contacts a number of
randomly selected nodes in the system and builds a vector
denoted by V . A vector consists of entries, and each entry
contains the ID, network address and load status of a
randomly selected node. Using the gossip-based protocol,
e
each node i exchanges its locally maintained vector with its
neighbors until its vector has s entries. It then calculates the
Ai and regards it as
average load of the s nodes denoted by
an estimation of A (Line 1 in Algorithm 1).
If node i finds itself is a light node (Line 2 in Algorithm
1), it seeks a heavy node to request chunks. Node i sorts the
nodes in its vector including itself based on the load status
and finds its position k0
1 in the sorted list, i.e., it is the top-k1
underloaded node in the list (Lines 3-5 in Algorithm 1).
e
Node i finds the top-k0
2 overloaded nodes in the list such
that the sum of these nodes’ excess loads is the least greater
Ai (Line 6 in Algorithm 1). Formula (ii) in
than or equal to k0
1
the algorithm is derived from (3). The complexity of the step
in Line 6 is OðjV jÞ. Then, the k0
2 th overloaded node is the
heavy node that node i needs to request chunks (Line 7 in
Algorithm 1). Considering the step in Line 4, the overall
complexity of Algorithm 1 is then OðjV j log jV jÞ.
We note the following:

.

. Our proposal is distributed in the sense that each
node in the system performs Algorithms 1 and 2
simultaneously without synchronization. It is possi-
ble that a number of distinct nodes intend to share
the load of node j (Line 1 of Algorithm 2). Thus, j
offloads parts of its load to a randomly selected node
among the requesters. Similarly, a number of heavy
nodes may select an identical light node to share
their loads. If so, the light node randomly picks one
of the heavy nodes in the reallocation.
The nodes perform our load rebalancing algorithm
periodically, and they balance their loads and
minimize the movement cost in a best effort fashion.
Example: Fig. 2 depicts a working example of our proposed
algorithm. There are n ¼ 10 chunkservers in the system; the
initial loads of the nodes are shown in Fig. 2a. Assume
L ¼ U ¼ 0 in the example. Then, nodes N 1, N 2, N 3, N 4,
and N 5 are light, and nodes N 6, N 7, N 8, N 9, and N 10 are
heavy. Each node performs the load-balancing algorithm
independently, and we choose N 1 as an example to explain
the load-balancing algorithm. N 1 first queries the loads of
e
N 3, N 6, N 7, and N 9 selected randomly from the system
(Fig. 2b). Based on the samples, N 1 estimates the ideal load
AN 1 ¼ LN 1 þLN 3þLN 6 þLN 7þLN 9
A (i.e.,
). It notices that it is a light
5
node. It then finds the heavy node it needs to request
chunks. The heavy node is the most loaded node (i.e., N 9)
as N 1 is the lightest among N1 and its sampled nodes
fN 3; N 6; N 7; N 9g (Line 6 in Algorithm 1). N 1 then sheds its

Fig. 2. An example illustrating our algorithm, where (a) the initial loads of
chunkservers N 1; N 2; . . . ; N 10, (b) N 1 samples the loads of N 1, N 3,
e
N 6, N 7, and N 9 in order to perform the load rebalancing algorithm,
(c) N 1 leaves and sheds its loads to its successor N 2, and then rejoins
AN 1 chunks (the ideal number of
as N 9’s successor by allocating
e
chunks N 1 estimates to manage) from N 9, (d) N 4 collects its sample set
fN 3; N 4; N 5; N 6; N 7g, and (e) N 4 departs and shifts its load to N 5, and
it then rejoins as the successor of N 6 by allocating L6  
AN 4 chunks
from N 6.

e
e
e
load to its successor N 2, departs from the system, and
rejoins the system as the successor of N 9. N 1 allocates
minfLN 9  
AN 1 ;
AN 1 g¼
AN 1 chunks from N 9 (Lines 5 and 6
in Algorithm 2).
In the example, N 4 also performs the load rebalancing
algorithm by first sampling fN 3; N 4; N 5; N 6; N 7g (Fig. 2d).
e
e
e
Similarly, N 4 determines to rejoin as the successor of N 6. N 4
then migrates its load to N 5 and rejoins as the successor of
N 6 (Fig. 2e). N 4 requests minfLN 6  
AN 4
AN 4 g¼ L6  
AN 4 ;
chunks from N 6.
Our load-balancing algorithm offers an analytical per-
formance guarantee and exhibits a fast convergence rate in
terms of algorithmic rounds. Let the initial number of heavy
nodes be k (where k  jV j ¼ n). Then, we have the major
analytical result as follows:
Theorem 2. Algorithms 1 and 2 take Oðlog log kÞ algorithmic
rounds in expectation such that the system contains no light
nodes.

For the detailed proof of Theorem 2, interested readers
are referred to Appendix D, which is available in the online
supplemental material.

3.2.3 Exploiting Physical Network Locality
A DHT network is an overlay on the application level. The
logical proximity abstraction derived from the DHT does
not necessarily match the physical proximity information in
reality. That means a message traveling between two

956

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 5, MAY 2013

neighbors in a DHT overlay may travel a long physical
distance through several physical network links. In the
load-balancing algorithm, a light node i may rejoin as a
successor of a remote heavy node j. Then, the requested
chunks migrated from j to i need to traverse several
physical network links, thus generating considerable net-
work traffic and consuming significant network resources
(i.e., the buffers in the switches on a communication path
for transmitting a file chunk from a source node to a
destination node).
We improve our proposal by exploiting physical net-
work locality. Basically, instead of collecting a single vector
(i.e., V in Algorithm 1) per algorithmic round, each light
node i gathers nV vectors. Each vector is built using the
method introduced previously. From the nV vectors, the
light node i seeks nV heavy nodes by invoking Algorithm 1
(i.e., SEEK) for each vector and then selects the physically
closest heavy node based on the message round-trip delay.
In Algorithm 3 (see Appendix C, which is available in
the online supplemental material), Lines 2 and 3 take
OðnV jV j log jV jÞ. We will offer a rigorous performance
analysis for the effect of varying nV in Appendix E, which
is available in the online supplemental material. Specifically,
we discuss the tradeoff between the value of nV and the
movement cost. A larger nV introduces more overhead for
message exchanges, but results in a smaller movement cost.
To demonstrate Algorithm 3, consider the example
shown in Fig. 2. Let nV ¼ 2. In addition to the sample set
V 1 ¼ fN 1; N 3; N 6; N 7; N 9g (Fig. 2b), N 1 gathers another
sample set, say, V 2 ¼ fN 1; N 4; N 5; N 6; N 8g. N 1 identifies
the heavy node N 9 in V 1 and N 8 in V 2 . Suppose N 9 is
physically closer to N 1 than N 8. Thus, N 1 rejoins as a
successor of N 9 and then receives chunks from N 9. Node i
also offloads its original load to its successor. For example,
in Figs. 2a and 2b, node N 1 migrates its original load to its
successor node N 2 before N 1 re joins as node N 9’s
successor. To minimize the network traffic overhead in
shifting the load of the light node i to node i þ 1, we suggest
initializing the DHT network such that every two nodes
with adjacent IDs (i.e., nodes i and i þ 1) are geometrically
close. As such, given the potential IP addresses of the
participating nodes (a 4D lattice) in a storage network,
we depend on the space-filling curve technique (e.g., Hilbert
curve in [28]) to assign IDs to the nodes, making physically
close nodes have adjacent IDs. More specifically, given a 4D
lattice representing all IP addresses of storage nodes, the
space-filling curve attempts to visit each IP address and
assign a unique ID to each address such that geometrically
close IP addresses are assigned with numerically close IDs.
By invoking the space filling curve function with the input
of an IP address, a unique numerical ID is returned.
Algorithm 3 has the performance guarantee for the data
center networks with -power law latency expansion. By
latency we mean the number of physical links traversed by
a message between two storage nodes in a network. For
example, BCube [29] and CamCube [30], respectively,
organize the data center network as a hypercube inter-
connect and a 2D torus network, thus exhibiting 3- and 2-
power law latency expansion. For further discussion,
interested readers are referred to Appendix E, which is
available in the online supplemental material.

;

ð5Þ

3.2.4 Taking Advantage of Node Heterogeneity
Nodes participating in the file system are possibly hetero-
geneous in terms of the numbers of file chunks that the
nodes can accommodate. We assume that there is one
bottleneck resource for optimization although a node’s
capacity in practice should be a function of computational
power, network bandwidth, and storage space [20], [31].
Given the capacities of nodes (denoted by f1 ; 2 ; . . . ; n g),
we enhance the basic algorithm in Section 3.2.2 as follows:
each node i approximates the ideal number of file chunks
e
that it needs to host in a load balanced state as follows:
ð4Þ
Ai ¼ i ;
where  is the load per unit capacity a node should manage in
 ¼ mP
the load balanced state and
n
k¼1 k
where m is the number of file chunks stored in the system.
As mentioned previously, in the distributed file system
for MapReduce-based applications, the load of a node is
typically proportional to the number of file chunks the node
possesses [3]. Thus, the rationale of this design is to ensure
that the number of file chunks managed by node i is
proportional to its capacity. To estimate the aggregate  , our
proposal again relies on the gossip-based aggregation
protocol in [26] and [27] in computing the value.
Algorithm 4 in Appendix C, which is available in the
online supplemental material, presents the enhancement for
e
Algorithm 1 to exploit node heterogeneity, which is similar
to Algorithm 1 and is self-explanatory. If a node i estimates
that it is light (i.e., Li < ð1   L Þ
Ai ), i then rejoins as a
successor of a heavy node j. i seeks j based on its sampled
node set V . i sorts the set in accordance with Lt
, the load per
t
capacity unit a node currently receives, for all t 2 V . When
node i notices that it is the kth least-loaded node (Line 6 in
Algorithm 4), it then identifies node j and rejoins as a
successor of node j. Node j is the least-loaded node in the
P
set of nodes P  V having the minimum cardinality, where
e
P
1) the nodes in P are heavy, and 2) the total excess load of
e
Aj
nodes in P is not
k
(Line 7 in
less than
jth light node in V
Aj
j¼1
k
indicates the sum of
Algorithm 4). Here,
jth light node in V
loads that the top-k light nodes in V will manage in a load-
j¼1
balanced system state.

3.2.5 Managing Replicas
In distributed file systems (e.g., Google GFS [2] and Hadoop
HDFS [3]), a constant number of replicas for each file chunk
are maintained in distinct nodes to improve file availability
with respect to node failures and departures. Our current
load-balancing algorithm does not treat replicas distinctly.
It is unlikely that two or more replicas are placed in an
identical node because of the random nature of our load
rebalancing algorithm. More specifically, each underloaded
node samples a number of nodes, each selected with a
probability of 1
n , to share their loads (where n is the total
number of storage nodes). Given k replicas for each file
chunk (where k is typically a small constant, and k ¼ 3 in
GFS), the probability that k0 replicas (k0  k) are placed in an

HSIAO ET AL.: LOAD REBALANCING FOR DISTRIBUTED FILE SYSTEMS IN CLOUDS

957

four distinct geometric distributions. Specifically,
these
distributions indicate that a small number of nodes initially
possess a large number of chunks. The four workloads
exhibit different variations of the geometric distribution.
We have compared our algorithm with the competing
algorithms called centralized matching in [3] and distributed
matching in [14], respectively. In Hadoop HDFS [3], a
standalone load-balancing server (i.e., balancer)
is em-
ployed to rebalance the loads of storage nodes. The server
acquires global information on the file chunks distributed in
the system from the namenode that manages the metadata
of the entire file system. Based on this global knowledge, it
partitions the node set into two subsets, where one (denoted
by O) contains overloaded nodes, and the other (denoted by
U )
the
includes the underloaded nodes. Conceptually,
balancer randomly selects one heavy node i 2 O and one
light node j 2 U to reallocate their loads. The reallocation
terminates if the balancer cannot find a pair of heavy and
light nodes to reallocate their loads. Notably, to exploit
physical network locality and thus reduce network traffic,
the balancer first pairs i and j if i and j appear in the same
rack. If a node in a rack remains unbalanced, and if it cannot
find any other node in the same rack to pair, then the node
will be matched with another node, in a foreign rack. The
balancer in HDFS does not differentiate different locations
of foreign racks when performing the matches. In our
simulations, each rack has 32 nodes in default.
On the contrary, a storage node i in the decentralized
random matching algorithm in [14] independently and
randomly selects another node j to share its load if the ratio
of i’s load to j’s is smaller (or larger) than a predefined
threshold  (or 1
 ). As suggested by [14],  is greater than 0
4 . In our simulations,  ¼ 1
and not more than 1
4 . To be
comparable, we also implement this algorithm on the Chord
DHT. Thus, when node i attempts to share the load of node
j, node i needs to leave and rejoin as node j’s successor.
In our algorithm, we set L ¼ U ¼ 0:2 in default. Each
node maintains nV vectors, each consisting of s ¼ 100
random samples of nodes (entries in a vector may be
duplicated), for estimating A. nV ¼ 1 in default.
The cloud network topology interconnecting the storage
nodes simulated is a 2D torus direct network, as suggested
by the recent studies in [30] and [33]. (In Appendix E, which
is available in the online supplemental material, we also
investigate the performance effects on the hypercube
topology in [29].) Finally, unless otherwise noted, each
node has an identical capacity in the simulations.
Due to space limitation, we report the major performance
results in Section 4.2. Extensive performance results can
be found in the appendix, which is available in the online
supplemental material,
including the effect of varying
the number of file chunks (Appendix G, which is available
in the online supplemental material), the effect of different
numbers of samples (Appendix H, which is available in the
online supplemental material),
the effect of different
algorithmic rounds (Appendix I, which is available in the
online supplemental material) and the effect of system
dynamics (Appendix J, which is available in the online
supplemental material).

Fig. 3. The workload distribution.

identical node due to migration of our load-balancing
algorithm is ð1
nÞk0
independent of their initial locations. For
example, in a file system with n ¼ 1;000 storage nodes and
k ¼ 3, then the probabilities are only 1
106 and 1
109 for two and
three replicas stored in the same node, respectively.
Consequently, the probability of more than one replica
 
appearing in a node due to our proposal is approximately
X
(as k  n)
i
k
1
n

ð6Þ

:

i¼2

Replica management in distributed systems has been
extensively discussed in the literature. Given any file chunk,
our proposal implements the directory-based scheme in [32] to
trace the locations of k replicas for the file chunk. Precisely,
the file chunk is associated with k   1 pointers that keep
track of k   1 randomly selected nodes storing the replicas.
We have investigated the percentage of nodes storing
redundant replicas due to our proposal. In our experiments,
the number of file chunks and the number of nodes in the
system are m ¼ 10;000 and n ¼ 1;000, respectively. (Details
of the experimental settings are discussed in Section 4.)
Among the m ¼ 10;000 file chunks, we investigate in the
experiment k ¼ 2; 4; 8 replicas for each file chunk; that is,
there are 5,000, 2,500 and 1,250 unique chunks in the
system, respectively. The experimental results indicate that
the number of nodes managing more than one redundant
chunk due to our proposal is very small. Specifically, each
node maintains no redundant replicas for k ¼ 2; 4, and only
2 percent of nodes store  2 redundant replicas for k ¼ 8.

4 SIMULATIONS
4.1 Simulation Setup and Workloads
The performance of our algorithm is evaluated through
computer simulations. Our simulator is implemented with
Pthreads. In the simulations, we carry out our proposal
based on the Chord DHT protocol [10] and the gossip-based
aggregation protocol in [26] and [27]. In the default setting,
the number of nodes in the system is n ¼ 1;000, and the
number of file chunks is m ¼ 10;000. To the best of our
knowledge, there are no representative realistic workloads
available. Thus, the number of file chunks initially hosted by
a node in our simulations follows the geometric distribu-
tion, enabling stress tests as suggested in [15] for various
load rebalancing algorithms. Fig. 3 shows the cumulative
distribution functions (CDF) of the file chunks in the
simulations, where workloads A, B, C, and D represent

958

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 5, MAY 2013

Fig. 4. The load distribution.

Fig. 6. The message overhead.

4.2 Simulation Results
Fig. 4 presents the simulation results of the load distribution
after performing the investigated load-balancing algorithms.
Here, the nodes simulated have identical capacity. The
simulation results show that centralized matching
performs very well as the load balancer gathers the global
information from the namenode managing the entire file
system. Since A ¼ 10 is the ideal number of file chunks a node
should manage in a load-balanced state, in centralized
matching, most nodes have 10 chunks. In contrast, dis-
tributed matching performs worse than centralized
matching and our proposal. This is because each node
randomly probes other nodes without global knowledge
about the system. Although our proposal is distributed and
need not require each node to obtain global system knowl-
edge, it is comparable with centralized matching and
remarkably outperforms distributed matching in terms
of load imbalance factor.
Fig. 5 shows the movement costs of centralized
matching, distributed matching, and our algorithm,
where the movement costs have been normalized to that of
centralized matching (indicated by the horizontal line
in the figure). Clearly, the movement cost of our proposal is
only 0.37 times the cost of distributed matching. Our
algorithm matches the top least-loaded light nodes with the
top most-loaded heavy nodes, leading to a fewer number of
file chunks migrated. In contrast, in distributed match-
ing, a heavy node i may be requested to relieve another
node j with a relatively heavier load, resulting in the
migration of a large number of chunks originally hosted by
i to i’s successor. We also observe that our proposal may
incur slightly more movement cost than that of centra-
lized matching. This is because in our proposal, a light
node needs to shed its load to its successor.
Fig. 6 shows the total number of messages generated by a
load rebalancing algorithm, where the message overheads
in distributed matching and our proposal are normal-
ized to that of centralized matching. The simulation

results indicate that centralized matching introduces
much less message overhead than distributed match-
ing and our proposal, as each node in centralized
matching simply informs the centralized load balancer of
its load and capacity. On the contrary, in distributed
matching and our proposal, each node probes a number of
existing nodes in the system, and may then reallocate its
load from/to the probed nodes, introducing more mes-
sages. We also see that our proposal clearly produces less
message overhead than distributed computing. Speci-
fically, any node i in our proposal gathers partial system
knowledge from its neighbors [26], [27], whereas node i in
distributed matching takes Oðlog nÞ messages to probe
a randomly selected node in the network.
Both distributed matching [14] and our proposal
depend on the Chord DHT network in the simulations.
However, nodes may leave and rejoin the DHT network for
load rebalancing, thus increasing the overhead required to
maintain the DHT structure. Thus, we further investigate
the number of rejoining operations. Note that centra-
lized matching introduces no rejoining overhead be-
cause nodes in centralized matching does not need to
self-organize and self-heal for rejoining operations. Fig. 7
illustrates the simulation results, where the number of
rejoining operations caused by our algorithm is normalized
to that of distributed matching (indicated by the
horizontal
line). We see that
the number of rejoining
operations in distributed matching can be up to two
times greater than that of our algorithm. This is because a
heavy node in distributed matching may leave and
rejoin the network to reduce the load of another heavy
node. On the contrary, in our proposal, only light nodes
rejoin the system as successors of heavy nodes. Our
algorithm attempts to pair light and heavy nodes precisely,
thus reducing the number of rejoining operations.
In Section 3.2.3, we improve our basic load rebalancing
algorithm by exploiting physical network locality. The
network traffic introduced by centralized matching,

Fig. 5. The movement cost.

Fig. 7. The rejoining cost.

HSIAO ET AL.: LOAD REBALANCING FOR DISTRIBUTED FILE SYSTEMS IN CLOUDS

959

Fig. 8. The WMC.

Fig. 10. The effect of heterogeneity.

distributed matching, and our proposal
is thus
cost (WMC) as follows:X
investigated. Specifically, we define the weighted movement

sizei  linki ;

ð7Þ

chunk i 2M
where M denotes the set of
file chunks selected for
reallocation by a load rebalancing algorithm, sizei is the
size of file chunk i, and linki represents the number of
physical links chunk i traverses. In the simulations, the size
of each file chunk is identical. We assume that sizei ¼ 1 for
all i 2 M without loss of generality. Hence, based on (7), the
greater the WMC, the more physical network links used for
load reallocation.
Fig. 8 shows the WMC’s caused by centralized
matching, distributed matching, and our proposal
(where the costs of distributed matching and our
proposal are normalized to that of centralized match-
ing, and Fig. 9 presents the corresponding CDF for the
numbers of physical
links traversed by file chunks for
workload C. (The other workloads are not presented due to
space limitation.) Notably, as suggested by Raicu et al. [30],
[33], a 2D torus direct network is simulated in the
experiment . The simulation results indicate that our
proposal clearly outperforms centralized matching
and distributed matching in terms of the WMC. In
contrast
to distributed matching, which does not
exploit the physical network locality for pairing light and
heavy nodes, centralized matching initially pairs
nodes present in the same rack and then matches light
nodes with heavy ones in different racks. (Here, each rack
contains 32 nodes.) However, centralized matching
does not differentiate the locations of nodes in different
racks for matching. Unlike centralized matching and
distributed matching, each light node in our proposal
first finds eight matched heavy nodes from its eight vectors
(i.e., nV ¼ 8 in this experiment), and then chooses the

Fig. 9. The breakdown of WMC.

physically closest node to pair with,
leading a shorter
physical distance for migrating a chunk. This operation
effectively differentiates nodes in different network loca-
tions, and considerably reduces the WMC.
As previously mentioned in Section 3.2.3, our proposal
organizes nodes in the Chord ring such that adjacent nodes
in the ring are physically close. Before rejoining a node, the
node departs and migrates its locally hosted file chunks to
its physically close successor. The simulation results
illustrate that  45% of file chunks in our proposal are
moved to the physically closest nodes, which is due to our
design having a locality-aware Chord ring (see Fig. 9).
Interested readers may refer to Appendix E, which is
for the
available in the online supplemental material,
analytical model
that details the performance of
the
locality-oblivious and locality-aware approaches discussed
in this section. Moreover, in Appendix E, which is available
in the online supplemental material, the effects of the
different numbers of racks in centralized matching
and the different numbers of node vectors nV maintained by
a node in our proposal are investigated.
We then investigate the effect of node heterogeneity for
centralized matching, distributed matching, and
our proposal. In this experiment, the capacities of nodes
follow the power-law distribution, namely, the Zipf dis-
tribution [19], [20], [22]. Here, the ideal number of file
chunks per unit capacity a node should host is approxi-
mately equal to  ¼ 0:5. The maximum and minimum
capacities are 110 and 2, respectively, and the mean is  11.
Fig. 10 shows the simulation results for workload C. In
Fig. 10, the ratio of the number of file chunks hosted by each
node i 2 V to i’s capacity, denoted by , is measured. Node
i attempts to minimize k   k in order to approach its
load-balanced state. The simulation results indicate that
centralized matching performs better than distrib-
uted matching and our proposal. This is because capable
nodes in distributed matching and our proposal may
need to offload their loads to their successors that are
incapable of managing large numbers of file chunks. We
also see that our proposal manages to perform reasonably
well, clearly outperforming distributed matching. In
our proposal, although a light node may shed its load to its
successor j, which is incapable and accordingly overloaded,
another light node can quickly discover the heavy node j to
share j’s load. In particular, our proposal seeks the top-k
light nodes in the reallocation and thus reduces the
movement cost caused by rejoining these light nodes as
compared to distributed matching.

960

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 5, MAY 2013

Fig. 11. The experimental environment and performance results, where (a) shows the setup of the experimental environment, (b) indicates the time
elapsed of performing the HDFS load balancer and our proposal, and (c) and (d) show the distributions of file chunks for the HDFS load balancer and
our proposal, respectively

5 IMPLEMENTATION AND MEASUREMENT
5.1 Experimental Environment Setup
We have implemented our proposal
in Hadoop HDFS
0.21.0, and assessed our implementation against the load
balancer in HDFS. Our implementation is demonstrated
through a small-scale cluster environment
(Fig. 11a)
consis ting of a s ingle , dedica ted namenode and 25
datanodes, each with Ubuntu 10.10 [34]. Specifically, the
namenode is equipped with Intel Core 2 Duo E7400
processor and 3 Gbytes RAM. As the number of file chunks
in our experimental environment is small, the RAM size of
the namenode is sufficient to cache the entire namenode
process and the metadata information,
including the
directories and the locations of file chunks.
In the experimental environment, a number of clients are
established to issue requests to the namenode. The requests
include commands to create directories with randomly
designated names, to remove directories arbitrarily chosen,
etc. Due to the scarce resources in our environment, we
have deployed 4 clients to generate requests to the name-
node. However, this cannot overload the namenode to
mimic the situation as reported in [8]. To emulate the load
of the namenode in a production system and investigate the
effect of the namenode’s load on the performance of a load-
balancing algorithm, we additionally limit the processor
cycles available to the namenode by varying the maximum
processor utilization, denoted by M, available to the
namenode up to M ¼ 1%; 2%; 8%; 16%; 32%; 64%; 99%. The
lower processor availability to the namenode represents
the less CPU cycles that the namenode can allocate to
handle the clients’ requests and to talk to the load balancer.
As data center networks proposed recently (e.g., [29]) can
offer a fully bisection bandwidth, the total number of
chunks scattered in the file system in our experiments is
limited to 256 such that the network bandwidth in our
environment (i.e., all nodes are connected with a 100 Mbps
fast Ethernet switch) is not the performance bottleneck.
Particularly, the size of a file chunk in the experiments is set
to 16 Mbytes. Compared to each experimental run requiring
20-60 minutes, transferring these chunks takes no more than
100  328 seconds  5:5 minutes in case the network
162568
bandwidth is fully utilized. The initial placement of the
256 file chunks follows the geometric distribution as
discussed in Section 4.
For each experimental run, we quantity the time elapsed to
complete the load-balancing algorithms, including the HDFS
load balancer and our proposal. We perform 20 runs for a

given M and average the time required for executing a load-
balancing algorithm. Additionally, the 5- and 95-percentiles
are reported. For our proposal, we let U ¼ L ¼ 0:2. Each
datanode performs 10 random samples.
Note that 1) in the experimental results discussed later,
we favor HDFS by dedicating a standalone node to perform
the HDFS load-balancing function. By contrast, our propo-
sal excludes the extra, standalone node. 2) The datanodes in
our cluster environment are homogeneous, each with Intel
Celeron 430 and 3 Gbytes RAM. We, thus, do not study the
effect of the node heterogeneity on our proposal. 3) We also
do not investigate the effect of network locality on our
proposal as the nodes in our environment are only linked
with a single switch.

5.2 Experimental Results
We demonstrate in Fig. 11 the experimental results. Fig. 11b
shows the time required for performing the HDFS load
balancer and our proposal. Our proposal clearly outper-
forms the HDFS load balancer. When the namenode is
heavily loaded (i.e., small M’s), our proposal remarkably
performs better than the HDFS load balancer. For example,
if M ¼ 1%, the HDFS load balancer takes approximately
60 minutes to balance the loads of datanodes. By contrast,
our proposal
takes nearly 20 minutes in the case of
M ¼ 1%. Specifically, unlike the HDFS load balancer, our
proposal is independent of the load of the namenode.
In Figs. 11c and 11d, we further show the distributions of
chunks after performing the HDFS load balancer and our
proposal. As there are 256 file chunks and 25 datanodes, the
ideal number of chunks that each datanode needs to host is
25  10. Due to space limitat ion , we only offer the
256
experimental results for M ¼ 1 and the results for M 6¼ 1
conclude the similar. Figs. 11c and 11d indicate that our
proposal is comparable to the HDFS load balancer, and
balances the loads of datanodes, effectively.

6 SUMMARY

A novel load-balancing algorithm to deal with the load
rebalancing problem in large-scale, dynamic, and distrib-
uted file systems in clouds has been presented in this paper.
Our proposal strives to balance the loads of nodes and
reduce the demanded movement cost as much as possible,
while taking advantage of physical network locality and
node heterogeneity. In the absence of representative real
workloads (i.e., the distributions of file chunks in a large-
scale storage system)
in the public domain, we have

HSIAO ET AL.: LOAD REBALANCING FOR DISTRIBUTED FILE SYSTEMS IN CLOUDS

961

[16]

[13] A. Rao, K. Lakshminarayanan, S. Surana, R. Karp, and I. Stoica,
“Load Balancing in Structured P2P Systems,” Proc. Second Int’l
Workshop Peer-to-Peer Systems (IPTPS ’02), pp. 68-79, Feb. 2003.
[14] D. Karger and M. Ruhl, “Simple Efficient Load Balancing
Algorithms for Peer-to-Peer Systems,” Proc. 16th ACM Symp.
Parallel Algorithms and Architectures (SPAA ’04), pp. 36-43, June
2004.
[15] P. Ganesan, M. Bawa, and H. Garcia-Molina, “Online Balancing of
Range-Partitioned Data with Applications to Peer-to-Peer Sys-
tems,” Proc. 13th Int’l Conf. Very Large Data Bases (VLDB ’04),
pp. 444-455, Sept. 2004.
J.W. Byers, J. Considine, and M. Mitzenmacher, “Simple Load
Balancing for Distributed Hash Tables,” Proc. First Int’l Workshop
Peer-to-Peer Systems (IPTPS ’03), pp. 80-87, Feb. 2003.
[17] G.S. Manku, “Balanced Binary Trees for ID Management and
Load Balance in Distributed Hash Tables,” Proc. 23rd ACM Symp.
Principles Distributed Computing (PODC ’04), pp. 197-205, July
2004.
[18] A. Bharambe, M. Agrawal, and S. Seshan, “Mercury: Supporting
Scalable Multi-Attribute Range Queries,” Proc. ACM SIGCOMM
’04, pp. 353-366, Aug. 2004.
[19] Y. Zhu and Y. Hu, “Efficient, Proximity-Aware Load Balancing for
DHT-Based P2P Systems,” IEEE Trans. Parallel and Distributed
Systems, vol. 16, no. 4, pp. 349-361, Apr. 2005.
[20] H. Shen and C.-Z. Xu, “Locality-Aware and Churn-Resilient Load
Balancing Algorithms in Structured P2P Networks,” IEEE Trans.
Parallel and Distributed Systems, vol. 18, no. 6, pp. 849-862, June
2007.
[21] Q.H. Vu, B.C. Ooi, M. Rinard, and K.-L. Tan, “Histogram-Based
Global Load Balancing in Structured Peer-to-Peer Systems,” IEEE
Trans. Knowledge Data Eng., vol. 21, no. 4, pp. 595-608, Apr. 2009.
[22] H.-C. Hsiao, H. Liao, S.-S. Chen, and K.-C. Huang, “Load Balance
with Imperfect Information in Structured Peer-to-Peer Systems,”
IEEE Trans. Parallel Distributed Systems, vol. 22, no. 4, pp. 634-649,
Apr. 2011.
[23] M.R. Garey and D.S. Johnson, Computers and Intractability: A Guide
to the Theory of NP-Completeness. W.H. Freeman and Co., 1979.
[24] D. Eastlake and P. Jones, “US Secure Hash Algorithm 1 (SHA1),”
RFC 3174, Sept. 2001.
[25] M. Raab and A. Steger, “Balls into Bins-A Simple and Tight
Analysis,” Proc. Second Int’l Workshop Randomization and Approx-
imation Techniques in Computer Science, pp. 159-170, Oct. 1998.
[26] M.
Jelasity, A. Montresor, and O. Babaoglu, “Gossip-Based
Aggregation in Large Dynamic Networks,” ACM Trans. Computer
Systems, vol. 23, no. 3, pp. 219-252, Aug. 2005.
[27] M. Jelasity, S. Voulgaris, R. Guerraoui, A.-M. Kermarrec, and M.V.
Steen, “Gossip-Based Peer Sampling,” ACM Trans. Computer
Systems, vol. 25, no. 3, Aug. 2007.
[28] H. Sagan, Space-Filling Curves, first ed. Springer, 1994.
[29] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang,
and S. Lu, “BCube: A High Performance, Server-Centric Network
Architecture for Modular Data Centers,” Proc. ACM SIGCOMM
’09, pp. 63-74, Aug. 2009.
[30] H. Abu-Libdeh, P. Costa, A. Rowstron, G. O’Shea, and A.
Donnelly, “Symbiotic Routing in Future Data Centers,” Proc.
ACM SIGCOMM ’10, pp. 51-62, Aug. 2010.
[31] S. Surana, B. Godfrey, K. Lakshminarayanan, R. Karp, and I.
Stoica, “Load Balancing in Dynamic Structured P2P Systems,”
Performance Evaluation, vol. 63, no. 6, pp. 217-240, Mar. 2006.
[32] S. Iyer, A. Rowstron, and P. Druschel, “Squirrel: A Decentralized
Peer-to-Peer Web Cache,” Proc. 21st Ann. Symp. Principles of
Distributed Computing (PODC ’02), pp. 213-222, July 2002.
I. Raicu,
I.T. Foster, and P. Beckman, “Making a Case for
Distributed File Systems at Exascale,” Proc. Third Int’l Workshop
Large-Scale System and Application Performance (LSAP ’11), pp. 11-
18, June 2011.
[34] Ubuntu, http://www.ubuntu.com/, 2012.

[33]

investigated the performance of our proposal and com-
pared it against competing algorithms through synthesized
probabilistic distributions of file chunks. The synthesis
workloads stress test
the load-balancing algorithms by
creating a few storage nodes that are heavily loaded. The
computer simulation results are encouraging,
indicating
that our proposed algorithm performs very well. Our
proposal is comparable to the centralized algorithm in the
Hadoop HDFS production system and dramatically out-
performs the competing distributed algorithm in [14] in
terms of
load imbalance factor, movement cost, and
algorithmic overhead. Particularly, our load-balancing
algorithm exhibits a fast convergence rate. The efficiency
and effectiveness of our design are further validated
by analytical models and a real implementation with a
small-scale cluster environment.

ACKNOWLEDGMENTS
The authors are grateful to the anonymous reviewers who
have provided us with valuable comments to improve their
study. Hung-Chang Hsiao and Chung-Hsueh Yi were
partially supported by Taiwan National Science Council
under Grants 100-2221-E-006-193 and 101-2221-E-006-097,
and by the Ministry of Education, Taiwan, under the NCKU
Project of Promoting Academic Excellence & Developing
World Class Research Centers. Haiying Shen was sup-
ported in part by US National Science Foundation (NSF)
grants CNS-1254006, CNS-1249603, OCI-1064230, CNS-
1049947, CNS-1156875, CNS-0917056 and CNS-1057530,
CNS-1025652, CNS-0938189, CSR-2008826, CSR-2008827,
Microsoft Research Faculty Fellowship 8300751, and the
US Department of Energy’s Oak Ridge National Laboratory
including the Extreme Scale Systems Center located at
ORNL and DoD 4000111689.

[2]

REFERENCES
J. Dean and S. Ghemawat, “MapReduce: Simplified Data Proces-
[1]
sing on Large Clusters,” Proc. Sixth Symp. Operating System Design
and Implementation (OSDI ’04), pp. 137-150, Dec. 2004.
S. Ghemawat, H. Gobioff, and S.-T. Leung, “The Google File
System,” Proc. 19th ACM Symp. Operating Systems Principles (SOSP
’03), pp. 29-43, Oct. 2003.
[3] Hadoop Distributed File System, http://hadoop.apache.org/
hdfs/, 2012.
[4] VMware, http://www.vmware.com/, 2012.
[5] Xen, http://www.xen.org/, 2012.
[6] Apache Hadoop, http://hadoop.apache.org/, 2012.
[7] Hadoop Distributed File System “Rebalancing Blocks,” http://
developer.yahoo.com/hadoop/tutorial/module2.html#rebalan-
cing, 2012.
[8] K. McKusick and S. Quinlan, “GFS: Evolution on Fast-Forward,”
Comm. ACM, vol. 53, no. 3, pp. 42-49, Jan. 2010.
[9] HDFS Federation, http://hadoop.apache.org/common/docs/
r0.23.0/hadoop-yarn/hadoop-yarn-site/Federation.html, 2012.
I. Stoica, R. Morris, D. Liben-Nowell, D.R. Karger, M.F. Kaashoek,
F. Dabek, and H. Balakrishnan, “Chord: A Scalable Peer-to-Peer
Lookup Protocol for Internet Applications,” IEEE/ACM Trans.
Networking, vol. 11, no. 1, pp. 17-21, Feb. 2003.
[11] A. Rowstron and P. Druschel, “Pastry: Scalable, Distributed Object
Location and Routing for Large-Scale Peer-to-Peer Systems,” Proc.
IFIP/ACM Int’l Conf. Distributed Systems Platforms Heidelberg,
pp. 161-172, Nov. 2001.
[12] G. DeCandia, D. Hastorun, M.
Jampani, G. Kakulapati, A.
Lakshman, A. Pilchin, S. Sivasubramanian, P. Vosshall, and W.
Vogels, “Dynamo: Amazon’s Highly Available Key-Value Store,”
Proc. 21st ACM Symp. Operating Systems Principles (SOSP ’07),
pp. 205-220, Oct. 2007.

[10]

962

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 5, MAY 2013

Hung-Chang Hsiao received the PhD degree in
computer science from National Tsing Hua
University, Taiwan, in 2000. Currently, he is a
professor in computer science and information
engineering, National Cheng Kung University,
Taiwan, since August 2012. He was also a
postdoctoral researcher in computer science,
National Tsing Hua University,
from October
2000 to July 2005. His research interests include
distributed computing, and randomized algo-
rithm design and performance analysis. He is a member of the IEEE
and the IEEE Computer Society.

Hsueh-Yi Chung received the BS degree in
computer science and engineering at Tatung
University, Taiwan, in 2009, and the MS degree
in computer science and information engineering
from National Cheng Kung University, Taiwan, in
2012. His research interests include cloud
computing and distributed storage.

Haiying Shen rece ived the BS degree in
computer science and engineering from Tongji
University, China, in 2000, and the MS and PhD
degrees in computer engineering from Wayne
State University, in 2004 and 2006, respectively.
Currently, she is an assistant professor in the
Holcombe Department of Electrical and Compu-
ter Engineering at Clemson University. Her
research interests include distributed and paral-
lel computer systems and computer networks,
with an emphasis on peer-to-peer and content delivery networks, mobile
computing, wireless sensor networks, and grid and cloud computing.
She was the program cochair for a number of international conferences
and member of the Program Committees of many leading conferences.
She is a Microsoft Faculty fellow of 2010 and a member of the ACM and
the IEEE.

Yu-Chang Chao received the BS degree in
computer science and information engineering
from Tamkang University, Taipei, Taiwan, and
the MS degree in compu te r sc ience and
information engineering from Nation Chen Kung
University, Tainan, Taiwan, 2000. He is now a
research staff member in Industrial Technology
Research Institute (ITRI). His research interests
include cloud computing, home networking, and
multimedia networking.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 9, SEPTEMBER 2013

1717

An Efficient and Secure Dynamic Auditing
Protocol for Data Storage in Cloud Computing

Kan Yang, Student Member, IEEE, and X iaohua J ia, Fe l low, IEEE

Abstract—In cloud computing, data owners host their data on cloud servers and users (data consumers) can access the data from
cloud servers. Due to the data outsourcing, however, this new paradigm of data hosting service also introduces new security challenges,
which requires an independent auditing service to check the data integrity in the cloud. Some existing remote integrity checking methods
can only serve for static archive data and, thus, cannot be applied to the auditing service since the data in the cloud can be dynamically
updated. Thus, an efficient and secure dynamic auditing protocol is desired to convince data owners that the data are correctly stored in
the cloud. In this paper, we first design an auditing framework for cloud storage systems and propose an efficient and privacy-preserving
auditing protocol. Then, we extend our auditing protocol to support the data dynamic operations, which is efficient and provably secure in
the random oracle model. We further extend our auditing protocol to support batch auditing for both multiple owners and multiple clouds,
without using any trusted organizer. The analysis and simulation results show that our proposed auditing protocols are secure and
efficient, especially it reduce the computation cost of the auditor.

Index Terms—Storage auditing, dynamic auditing, privacy-preserving auditing, batch auditing, cloud computing

Ç

1 INTRODUCTION
C LOUD storage is an important service of cloud comput-
ing [1], which allows data owners (owners) to move
data from their local computing systems to the cloud. More
and more owners start to store the data in the cloud [2].
However, this new paradigm of data hosting service also
introduces new security challenges [3]. Owners would
worry that the data could be lost in the cloud. This is
because data loss could happen in any infrastructure, no
matter what high degree of reliable measures cloud service
providers would take [4], [5], [6], [7], [8]. Sometimes, cloud
service providers might be dishonest. They could discard
the data that have not been accessed or rarely accessed to
save the storage space and claim that the data are still
correctly stored in the cloud. Therefore, owners need to be
convinced that the data are correctly stored in the cloud.
Traditionally, owners can check the data integrity based
on two-party storage auditing protocols [9], [10], [11], [12],
[13], [14], [15], [16], [17]. In cloud storage system, however,
it
is inappropriate to let either side of cloud service
providers or owners conduct such auditing, because none
of them could be guaranteed to provide unbiased auditing
result. In this situation, third-party auditing is a natural
choice for the storage auditing in cloud computing. A third-
party auditor (auditor) that has expertise and capabilities
can do a more efficient work and convince both cloud
service providers and owners.
For the third-party auditing in cloud storage systems,
there are several important requirements that have been

. The authors are with the Department of Computer Science, City University
of Hong Kong, Tat Chee Avenue, Kowloon, Hong Kong, SAR.
E-mail: kanyang3@student.cityu.edu.hk, csjia@cityu.edu.hk.

Manuscript received 13 Aug. 2011; revised 7 Sept. 2012; accepted 9 Sept.
2012; published online 21 Sept. 2012.
Recommended for acceptance by L.E. Li.
For information on obtaining reprints of this article, please send e-mail to:
tpds@computer.org, and reference IEEECS Log Number TPDS-2011-08-0534.
Digital Object Identifier no. 10.1109/TPDS.2012.278.

proposed in some previous works [18], [19]. The auditing
protocol should have the following properties: 1) Confiden-
tiality. The auditing protocol should keep owner’s data
confidential against the auditor. 2) Dynamic auditing. The
auditing protocol should support the dynamic updates of
the data in the cloud. 3) Batch auditing. The auditing
protocol should also be able to support the batch auditing
for multiple owners and multiple clouds.
Recently, several remote integrity checking protocols
were proposed to allow the auditor to check the data
integrity on the remote server [20], [21], [22], [23], [24], [25],
[26], [27], [28]. Table 1 gives the comparisons among some
existing remote integrity checking schemes in terms of the
performance, the privacy protection, the support of dy-
namic operations and the batch auditing for multiple
owners and multiple clouds. From Table 1, we can find
that many of them are not privacy preserving or cannot
support the data dynamic operations, so that they cannot be
applied to cloud storage systems.
In [23],
the authors proposed a dynamic auditing
protocol that can support the dynamic operations of the
data on the cloud servers, but this method may leak the
data content to the auditor because it requires the server to
send the linear combinations of data blocks to the auditor.
In [24],
the authors extended their dynamic auditing
scheme to be privacy preserving and support the batch
auditing for multiple owners. However, due to the large
number of data tags, their auditing protocols may incur a
heavy storage overhead on the server. In [25], Zhu et al.
proposed a cooperative provable data possession scheme
that can support the batch auditing for multiple clouds and
also extend it to support the dynamic auditing in [26].
However, their scheme cannot support the batch auditing
for multiple owners. That
is because parameters for
generating the data tags used by each owner are different,
and thus, they cannot combine the data tags from multiple
owners to conduct the batch auditing. Another drawback is

1045-9219/13/$31.00 ß 2013 IEEE

Published by the IEEE Computer Society

1718

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 9, SEPTEMBER 2013

TABLE 1
Comparison of Remote Integrity Checking Schemes

n is the total number of data blocks of a file; t is the number of challenged data blocks in an auditing query; s is the number of sectors in each data
block;  is the probability of block/sector corruption (suppose the probability of corruption is the same for the equal size of data block or sector).

that their scheme requires an additional trusted organizer to
send a commitment to the auditor during the multicloud
batch auditing, because their scheme applies the mask
technique to ensure the data privacy. However, such
additional organizer is not practical
in cloud storage
systems. Furthermore, both Wang’s schemes and Zhu’s
schemes incur heavy computation cost of the auditor, which
makes the auditor a performance bottleneck.
In this paper, we propose an efficient and secure
dynamic auditing protocol, which can meet the above-
listed requirements. To solve the data privacy problem, our
method is to generate an encrypted proof with the challenge
stamp by using the Bilinearity property of the bilinear
pairing, such that the auditor cannot decrypt it but can
verify the correctness of the proof. Without using the mask
technique, our method does not require any trusted
organizer during the batch auditing for multiple clouds.
in our method, we let the server
On the other hand,
compute the proof as an intermediate value of
the
verification, such that the auditor can directly use this
intermediate value to verify the correctness of the proof.
Therefore, our method can greatly reduce the computing
loads of the auditor by moving it to the cloud server.
Our original contributions can be summarized as follows:

1. We design an auditing framework for cloud storage
systems and propose a privacy-preserving and
efficient storage auditing protocol. Our auditing
protocol ensures the data privacy by using crypto-
graphy method and the Bilinearity property of the
bilinear pairing, instead of using the mask techni-
que. Our auditing protocol incurs less communica-
tion cost between the auditor and the server. It also
reduces the computing loads of
the auditor by
moving it to the server.
2. We extend our auditing protocol to support the data
dynamic operations, which is efficient and provably
secure in the random oracle model.
3. We further extend our auditing protocol to support
batch auditing for not only multiple clouds but also
multiple owners. Our multicloud batch auditing
does not require any additional trusted organizer.
The multiowner batch auditing can greatly improve
the auditing performance, especially in large-scale
cloud storage systems.

The remaining of this paper is organized as follows: In
Section 2, we describe definitions of the system model and
security model. In Section 3, we propose an efficient and
inherently secure auditing protocol and extend it to support
the dynamic auditing in Section 4. We further extend our
auditing protocol to support the batch auditing for multiple
owners and multiple clouds in Section 5. Section 6 give the
performance analysis of our proposed auditing protocols in
terms of communication cost and computation cost. The
security proof will be shown in the supplemental file, which
can be found on the Computer Society Digital Library at
http://doi.ieeecomputersociety.org/10.1109/TPDS.2012.
278. In Section 7, we give the related work on storage
auditing. Finally, the conclusion is given in Section 8.

2 PRELIMINARIES AND DEFINITIONS

In this section, we first describe the system model and
give the definition of storage auditing protocol. Then, we
define the threat model and security model for a storage
auditing system.

2.1 Definition of a System Model
We consider an auditing system for cloud storage as shown
in Fig. 1, which involves data owners (owner), the cloud
server (server), and the third-party auditor (auditor). The
owners create the data and host their data in the cloud.
The cloud server stores the owners’ data and provides the
data access to users (data consumers). The auditor is a
trusted third-party that has expertise and capabilities to
provide data storage auditing service for both the owners
and servers. The auditor can be a trusted organization
managed by the government, which can provide unbiased
auditing result for both data owners and cloud servers.

Fig. 1. System model of the data storage auditing.

YANG AND JIA: AN EFFICIENT AND SECURE DYNAMIC AUDITING PROTOCOL FOR DATA STORAGE IN CLOUD COMPUTING

1719

TABLE 2
Notations

Before describing the auditing protocol definition, we
first define some notations as listed in Table 2.

Definition 1 (Storage auditing protocol). A storage auditing
protocol consists of the following five algorithms:
1. KeyGenðÞ ! ðskh ; skt ; pkt Þ. This key generation
algorithm takes no input other than the implicit
security parameter . It outputs a secret hash key
skh and a pair of secret-public tag key ðskt ; pkt Þ.
2. TagGenðM ; skt ; skh Þ ! T . The tag generation algo-
rithm takes as inputs an encrypted file M , the secret tag
key skt , and the secret hash key skh . For each data block
mi , it computes a data tag ti based on skh and skt . It
outputs a set of data tags T ¼ fti gi2½1;n .
3. ChallðMinf o Þ ! C. The challenge algorithm takes
as input the abstract information of the data Minf o
(e.g.,
file identity, total number of blocks, version
number, time stamp, etc.). It outputs a challenge C.
4. ProveðM ; T ; CÞ ! P . The prove algorithm takes as
inputs the file M , the tags T , and the challenge from
the auditor C. It outputs a proof P .
5. VerifyðC; P ; skh ; pkt ; Minf o Þ ! 0=1. The verification
algorithm takes as inputs P from the server, the secret
hash key skh , the public tag key pkt , and the abstract
information of the data Minf o . It outputs the auditing
result as 0 or 1.

2.2 Definition of a Security Model
We assume the auditor is honest but curious. It performs
honestly during the whole auditing procedure, but it is
curious about the received data. But the sever could be
dishonest and may launch the following attacks:

2.

1. Replace attack. The server may choose another valid
and uncorrupted pair of data block and data tag
ðmk ; tk Þ to replace the challenged pair of data block
and data tag ðmi ; ti Þ, when it already discarded mi
or ti .
Forge attack. The server may forge the data tag of
data block and deceive the auditor, if the owner’s
secret tag keys are reused for the different versions
of data.
3. Replay attack. The server may generate the proof
from the previous proof or other information, with-
out retrieving the actual owner’s data.

3 EFFICIENT AND PRIVACY-PRESERVING AUDITING
PROTOCOL

In this section, we first present some techniques we applied
in the design of our efficient and privacy-preserving
auditing protocol. Then, we describe the algorithms and
the detailed construction of our auditing protocol for cloud
storage systems. The correctness proof will be shown in the
supplemental file, available online.

3.1 Overview of Our Solution
The main challenge in the design of data storage auditing
protocol is the data privacy problem (i.e., the auditing protocol
should protect the data privacy against the auditor.). This is
because: 1) For public data, the auditor may obtain the data
information by recovering the data blocks from the data
proof. 2) For encrypted data, the auditor may obtain content
keys somehow through any special channels and could be
able to decrypt the data. To solve the data privacy problem,
our method is to generate an encrypted proof with the
challenge stamp by using the bilinearity property of the
bilinear pairing, such that the auditor cannot decrypt it, but
the auditor can verify the correctness of the proof without
decrypting it.
Although the auditor has sufficient expertise and cap-
abilities to conduct the auditing service, the computing
ability of an auditor is not as strong as cloud servers. Since
the auditor needs to audit for many cloud servers and a large
number of data owners, the auditor could be the perfor-
mance bottleneck. In our method, we let the server compute
the proof as an intermediate value of the verification
(calculated by the challenge stamp and the linear combina-
tions of data blocks), such that the auditor can use this
intermediate value to verify the proof. Therefore, our method
can greatly reduce the computing loads of the auditor by
moving it to the cloud server.
To improve the performance of an auditing system,
we apply the data fragment
technique and homomorphic
verifiable tags in our method. The data fragment technique
can reduce number of data tags, such that it can reduce the
storage overhead and improve the system performance. By
using the homomorphic verifiable tags, no matter how
many data blocks are challenged, the server only responses
the sum of data blocks and the product of tags to the
auditor, whose size is constant and equal to only one data
block. Thus, it reduces the communication cost.

3.2 Algorithms for Auditing Protocol
Suppose a file F has m data components as F ¼ ðF1 ; . . . ; Fm Þ.
Each data component has its physical meanings and can be
updated dynamically by the data owners. For public data
components, the data owner does not need to encrypted it,
but for private data component, the data owner needs to
encrypt it with its corresponding key.
Each data component Fk is divided into nk data blocks
denoted as Fk ¼ ðmk1 ; mk2 ; . . . ; mknk Þ. Due to the security
reason, the data block size should be restricted by the
security parameter. For example, suppose the security level
is set to be 160 bit (20 Byte), the data block size should be
20 Byte. A 50-KByte data component will be divided into
2,500 data blocks and generate 2,500 data tags, which incurs
50-KByte storage overhead.

1720

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 9, SEPTEMBER 2013

By using the data fragment technique, we further split
each data block into sectors. The sector size is restricted by
the security parameter. We generate one data tag for each
data block that consists of s sectors, such that less data tags
are generated. In the same example above, a 50-KByte data
component only incurs 50/s KByte storage overhead. In real
storage systems, the data block size can be various. That is,
different data blocks could have different number of
sectors. For example, if a data block mi will be frequently
then si could be large, but for those frequently
read,
updated data blocks, si could be relatively small.
For simplicity, we only consider one data component in
our construction and constant number of sectors for each
data block. Suppose there is a data component M , which is
divided into n data blocks, and each data block is further split
into s sectors. For data blocks that have different number of
sectors, we first select the maximum number of sectors smax
among all the sector numbers si . Then, for each data block mi
with si sectors, si < smax , we simply consider that the data
block mi has smax sectors by setting mij ¼ 0 for si < j  smax .
Because the size of each sector is constant and equal to the
security parameter p, we can calculate the number of data
blocks as n ¼ sizeof ðM Þ
. We denote the encrypted data
slog p
component as M ¼ fmij gi2½1;n;j2½1;s .
Let GG1 ; GG2 , and GGT be the multiplicative groups with
the same prime order p and e : GG1  GG2 ! GGT be the
bilinear map. Let g1 and g2 be the generators of GG1 and GG2 ,
respectively. Let h : f0; 1g ! GG1 be a keyed secure hash
function that maps the Minf o to a point in GG1 .
Our storage auditing protocol consists of the following
algorithms:
KeyGenðÞ ! ðpkt ; skt ; skh Þ. The key generation algo-
rithm takes no input other than the implicit security
parameter . It chooses two random number skt ; skh 2 ZZp
as the secret tag key and the secret hash key. It outputs the
public tag key as pkt ¼ gskt
2 2 GG2 , the secret tag key skt and
the secret hash key skh .
TagGenðM ; skt ; skh Þ ! T . The tag generation algorithm
takes each data component M , the secret tag key skt , and
the secret hash key skh as inputs. It first chooses s random
values x1 ; x2 ; . . . ; xs 2 ZZp and computes uj ¼ gxj
1 2 GG1 for all
j 2 ½1; s. For each data block mi ði 2 ½1; nÞ, it computes a
 
!
Y
data tag ti as
s
ti ¼ hðskh ; Wi Þ 

j¼1
where Wi ¼ F IDki (the “k” denotes the concatenation
operation), in which F ID is the identifier of the data and i
represents the block number of mi . It outputs the set of data
tags T ¼ fti gi2½1;n .
ChallðMinf o Þ ! C. The challenge algorithm takes the
abstract information of the data Minf o as the input. It selects
some data blocks to construct the Challenge Set Q and
generates a random number vi 2 ZZ
p for each chosen data
block mi ði 2 QÞ. Then, it computes the challenge stamp R ¼
ðpkt Þr by randomly choosing a number r 2 ZZ
p . It outputs the
challenge as C ¼ ðfi; vi gi2Q ; RÞ.
ProveðM ; T ; CÞ ! P . The prove algorithm takes as inputs
the data M and the received challenge C ¼ ðfi; vi gi2Q ; RÞ.

umij
j

skt

;

Fig. 2. Framework of our privacy-preserving auditing protocol.
Y
The proof consists of the tag proof T P and the data proof DP .
The tag proof is generated as
T P ¼

tvi
i :

i2Q

To generate the data proof, it first computes the sector linear
X
combination of all the challenged data blocks M Pj for each
j 2 ½1; s as

M Pj ¼
i2Q
Y
Then, it generates the data proof DP as
s

vi  mij :

DP ¼

eðuj ; RÞM Pj :

j¼1
It outputs the proof P ¼ ðT P ; DP Þ.
VerifyðC; P ; skh ; pkt ; Minf o Þ ! 0=1. The verification algo-
rithm takes as inputs the challenge C , the proof P , the secret
hash key skh , the public tag key pkt , and the abstract
information of the data component. It first computes the
identifier hash values hðskh ; Wi Þ of all the challenged data
Y
blocks and computes the challenge hash Hchal as
Hchal ¼
hðskh ; Wi Þrvi :

i2Q


 
It then verifies the proof from the server by the following
verification equation:
DP  eðHchal ; pkt Þ ¼ e
:

ð1Þ

T P ; gr
2

If the above verification equation (1) holds, it outputs 1.
Otherwise, it outputs 0.

3.3 Construction of Our Privacy-Preserving
Auditing Protocol
As illustrated in Fig. 2, our storage auditing protocol
consists of three phases: owner initialization, confirmation
auditing, and sampling auditing. During the system initializa-
tion, the owner generates the keys and the tags for the data.
After storing the data on the server, the owner asks the
auditor to conduct the confirmation auditing to make sure

YANG AND JIA: AN EFFICIENT AND SECURE DYNAMIC AUDITING PROTOCOL FOR DATA STORAGE IN CLOUD COMPUTING

1721

that their data is correctly stored on the server. Once
confirmed, the owner can choose to delete the local copy of
the data. Then, the auditor conducts the sampling auditing
periodically to check the data integrity.
Phase 1: Owner initialization. The owner runs the key
generation algorithm KeyGen to generate the secret hash
key skh , the pair of secret-public tag key ðskt ; pkt Þ. Then, it
runs the tag generation algorithm TagGen to compute the
data tags. After all the data tags are generated, the owner
sends each da ta componen t M ¼ fmi gi2½1;n and i ts
corresponding data tags T ¼ fti gi2½1;n to the server together
with the set of parameters fuj gj2½1;s . The owner then sends
the public tag key pkt , the secret hash key skh , and the
abstract information of the data Minf o to the auditor, which
includes the data identifier F ID, the total number of data
blocks n.
Phase 2: Confirmation auditing. In our auditing construc-
tion, the auditing protocol only involves two-way commu-
nication: Challenge and Proof. During the confirmation
auditing phase, the owner requires the auditor to check
whether the owner’s data are correctly stored on the server.
The auditor conducts the confirmation auditing phase as

1. The auditor runs the challenge algorithm Chall to
generate the challenge C for all the data blocks in the
data component and sends the C ¼ ðfi; vi gi2Q ; RÞ to
the server.
2. Upon receiving the challenge C from the auditor, the
server runs the prove algorithm Prove to generate the
proof P ¼ ðT P ; DP Þ and sends it back to the auditor.
3. When the auditor receives the proof P from the server,
it runs the verification algorithm Verify to check the
correctness of P and extract the auditing result.
The auditor then sends the auditing result to the owner.
If the result is true, the owner is convinced that its data are
correctly stored on the server, and it may choose to delete
the local version of the data.
Phase 3: Sampling auditing. The auditor will carry out the
sampling auditing periodically by challenging a sample set
of data blocks. The frequency of taking auditing operation
depends on the service agreement between the data owner
and the auditor (and also depends on how much trust the
data owner has over the server). Similar to the confirmation
auditing in Phase 2, the sampling auditing procedure also
contains two-way communication as illustrated in Fig. 2.
Suppose each sector will be corrupted with a probability
of  on the server. For a sampling auditing involved with t
challenged data blocks, the probability of detection can be
calculated as

P rðt; sÞ ¼ 1   ð1   Þts :
That is this t-block sampling auditing can detect any data
corruption with a probability of P rðt; sÞ.

4 SECURE DYNAMIC AUDITING

In cloud storage systems, the data owners will dynamically
update their data. As an auditing service, the auditing
protocol should be designed to support the dynamic data,
as well as the static archive data. However, the dynamic

operations may make the auditing protocols insecure.
Specifically, the server may conduct two following attacks:
1) Replay attack. The server may not update correctly the
owner’s data on the server and may use the previous
version of the data to pass the auditing. 2) Forge attack.
When the data owner updates the data to the current
version, the server may get enough information from the
dynamic operations to forge the data tag. If the server could
forge the data tag, it can use any data and its forged data tag
to pass the auditing.

4.1 Our Solution
To prevent the replay attack, we introduce an index table
(ITable) to record the abstract information of the data. The
ITable consists of four components: Index, Bi , Vi , and Ti . The
Index denotes the current block number of data block mi in
the data component M . Bi denotes the original block
number of data block mi , and Vi denotes the current version
number of data block mi . Ti is the time stamp used for
generating the data tag.
This ITable is created by the owner during the owner
initialization and managed by the auditor. When the owner
completes the data dynamic operations, it sends an update
message to the auditor for updating the ITable that is stored
on the auditor. After the confirmation auditing, the auditor
sends the result to the owner for the confirmation that the
owner’s data on the server and the abstraction information
on the auditor are both up-to-date. This completes the data
dynamic operation.
To deal with the forge attack, we can modify the tag
generation algorithm TagGen. Specifically, when generat-
ing the data tag ti for the data block mi , we insert all the
abstract information into the data tag by setting Wi ¼
F IDkikBi kVi kTi , such that the server cannot get enough
information to forge the data tag from dynamic opera-
tions. The detailed proof will be given in the supple-
mental file, available online.

4.2 Algorithms and Constructions for Dynamic
Auditing
The dynamic auditing protocol consists of four phases:
owner initialization, confirmation auditing, sampling audit-
ing, and dynamic auditing.
The first
three phases are similar to our privacy-
preserving auditing protocol as described in the above
section. The only differences are the tag generation algo-
rithm TagGen and the ITable generation during the owner
initialization phase. Here, as illustrated in Fig. 3, we only
describe the dynamic auditing phase, which contains three
steps: data update, index update, and update confirmation.
Step 1: Data update. There are three types of data update
operations that can be used by the owner: modification,
insertion, and deletion. For each update operation, there is
a corresponding algorithm in the dynamic auditing to
process the operation and facilitate the future auditing,
defined as follows:
i Þ. The modification
i ; skt ; skh Þ ! ðM sgmodif y ; t
Modifyðm
algorithm takes as inputs the new version of data block m
i ,
the secret tag key skt , and the secret hash key skh . It
i , new time stamp T 
generates a new version number V 
i ,
and calls the TagGen to generate a new data tag t
i for data

1722

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 9, SEPTEMBER 2013
IInsertðM sginsert Þ. The index insertion algorithm takes as
input the update message M sginsert . It inserts a new record
i Þ in ith position in the ITable. It then moves the
ði; B
i ; T 
i ; V 
original ith record and other records after the ith position in
the previous ITable backward in order, with the index
number increased by 1.
IDeleteðM sgdelete Þ. The index deletion algorithm takes as
input the update message M sgdelete . It deletes the ith record
ði; Bi ; Vi ; Ti Þ in the ITable and all the records after the ith
position in the original ITable moved forward in order, with
the index number decreased by 1.
Table 3 shows the change of ITable according to the
different type of data update operation. Table 3a describes
the initial table of the data M ¼ fm1 ; m2 ; . . . ; mn g, and
Table 3b describes the ITable after m2 is updated. Table 3c
shows the ITable after a new data block is inserted before
m2 , and Table 3d shows the ITable after m2 is deleted.
Step 3: Update confirmation. After the auditor updates the
ITable, it conducts a confirmation auditing for the updated
data and sends the result to the owner. Then, the owner can
choose to delete the local version of data according to the
update confirmation auditing result.

Fig. 3. Framework of auditing for dynamic operations.
i . It outputs the new tag t
block m
i and the update message
M sgmodif y ¼ ði; Bi ; V 
i Þ. Then, it sends the new pair of
i ; T 
data block and tag ðm
i Þ to the server and sends the
i ; t
update message M sgmodif y to the auditor.
i Þ. The insertion algo-
i ; skt ; skh Þ ! ðM sginsert ; t
Insertðm
rithm takes as inputs the new data block m
i , the secret tag
key skt , and the secret hash key skh . It inserts a new data
block m
i before the ith position. It generates an original
number B
i , a new version number V 
i , and a new time stamp
i . Then, it calls the TagGen to generate a new data tag t
T 
i for
the new data block m
i . It outputs the new tag t
i and the
update message M sginsert ¼ ði; B
i Þ. Then, it inserts
i ; V 
i ; T 
the new pair of data block and tag ðm
i Þ on the server and
i ; t
sends the update message M sginsert to the auditor.
Deleteðmi Þ ! M sgdelete . The deletion algorithm takes as
input the data block mi . It outputs the update message
M sgdelete ¼ ði; Bi ; Vi ; Ti Þ. It then deletes the pair of data
block and its tag ðmi ; ti Þ from the server and sends the
update message M sgdelete to the auditor.
Step 2: Index update. Upon receiving the three types of
update messages, the auditor calls three corresponding
algorithms to update the ITable. Each algorithm is designed
as follows:
IModifyðM sgmodif y Þ. The index modification algorithm
takes the update message M sgmodif y as input. It replaces the
version number Vi by the new one V 
i and modifies Ti by the
new time stamp T 
i .

5 BATCH AUDITING FOR MULTIOWNER AND
MULTICLOUD

Data storage auditing is a significant service in cloud
computing that helps the owners check the data integrity on
the cloud servers. Due to the large number of data owners,
the auditor may receive many auditing requests from
multiple data owners. In this situation, it would greatly
improve the system performance,
if the auditor could
combine these auditing requests together and only conduct
the batch auditing for multiple owners simultaneously. The
previous work [25] cannot support the batch auditing for
multiple owners. That is because parameters for generating
the data tags used by each owner are different, and thus, the
auditor cannot combine the data tags from multiple owners
to conduct the batch auditing.
On the other hand, some data owners may store their
data on more than one cloud servers. To ensure the
owner’s data integrity in all the clouds, the auditor will
send the auditing challenges to each cloud server that
hosts the owner’s data and verify all the proofs from them.
is
it
the auditor,
To reduce the computation cost of

TABLE 3
ITable of the Abstract Information of Data M

desirable to combine all these responses together and do
the batch verification.
In the previous work [25], the authors proposed a
cooperative provable data possession for integrity verifica-
tion in multicloud storage. In their method, the authors apply
the mask technique to ensure the data privacy, such that it
requires an additional trusted organizer to send a commit-
ment to the auditor during the commitment phase in
multicloud batch auditing. In our method, we apply the
encryption method with the bilinearity property of the
bilinear pairing to ensure the data privacy, rather than
the mask technique. Thus, our multicloud batch auditing
protocol does not have any commitment phase, such that our
method does not require any additional trusted organizer.

1723
YANG AND JIA: AN EFFICIENT AND SECURE DYNAMIC AUDITING PROTOCOL FOR DATA STORAGE IN CLOUD COMPUTING
the challenged subset Qkl from each server Sl ðl 2 Schal Þ. It
then generates a random number vkl;i for each chosen data
block mkl;i ðk 2 Ochal ; l 2 Schal ; i 2 Qkl Þ.
It also chooses a
random number r 2 ZZ
p and computes the set of challenge
stamp fRk gk2Ochal¼pkr
. It outputs the challenge as
t;k
C ¼ ðfC l gl2Schal
; fRk gk2Ochal Þ;
where Cl ¼ fðk; l; i; vkl;i Þgk2Ochal .
Then, the auditor sends each Cl to each cloud server
Sl ðl 2 Schal Þ together with the challenge stamp fRk gk2Ochal .
Step 2: Batch proof. Upon receiving the challenge, each
server Sl ðl 2 Schal Þ generates a proof P l ¼ ðT Pl ; DPl Þ by
using the following batch prove algorithm BProve and
sends the proof P l to the auditor.
Cl ; fRk gk2Ochal Þ ! P l .
fTkl gk2Ochal ,
BProveðfMkl gk2Ochal ,
The batch prove algorithm takes as inputs the data
fMkl gk2Ochal , the data tags fTkl gk2Ochal , the received challenge
Cl , and the challenge stamp fRk gk2Ochal . It generates the tag
Y
Y
proof T Pl as

5.1 Algorithms for Batch Auditing for Multiowner
and Multicloud
Let O be the set of owners and S be the set of cloud servers.
The batch auditing for multiowner and multicloud can be
constructed as follows:
Phase 1: Owner initialization. Each owner Ok ðk 2 OÞ runs
the key generation algorithm KeyGen to generate the pair of
secret-public tag key ðskt;k ; pkt;k Þ and a set of secret hash key
fskh;kl gl2S . That is, for different cloud servers, the owner has
different secret hash keys. We denote each data component
as Mkl , which means that this data component is owned
by the owner Ok and stored on the cloud server Sl . Suppose
the data component Mkl is divided into nkl data blocks,
and each data block is further split into s sectors. (Here,
we assume that each data block is further split into the same
number of sectors. We can use the similar technique
proposed in Section 3.2 to deal with the situation that each
data block is split into different number of sectors.) The
 
!
owner Ok runs the tag generation algorithm TagGen to
generate the data tags Tkl ¼ ftkl;i gi2½1;nkl  as
Y
s
tkl;i ¼ hðskh;kl ; Wkl;i Þ 

j¼1
where Wkl;i ¼ F IDkl kikBkl;i kVkl;i kTkl;i .
After all the data tags are generated, each owner Ok ðk 2
OÞ sends the data component Mkl ¼ fmkl;ij gk2O;l2S
i2½1;nkl ;j2½1;s and
the data tags Tkl ¼ ftkl;i gk2O;l2S
i2½1;nkl  to the corresponding server
Sl . Then, it sends the public tag key pkt;k , the set of secret
hash key fskhl;k gl2S ,
the abstract
information of data
fMinf o;kl gk2O;l2S to the auditor.
Phase 2: Batch auditing for multiowner and multicloud. Let
Ochal and Schal denote the involved set of owners and cloud
servers involved in the batch auditing, respectively. The
batch auditing also consists of three steps: batch challenge,
batch proof, and batch verification.
Step 1: Batch challenge. During this step, the auditor runs
the batch challenge algorithm BChall to generate a batch
challenge C for a set of challenged owners Ochal and a set
of clouds Schal . The batch challenge algorithm is defined
as follows:
BChallðfMinf o;kl gk2O;l2S Þ ! C. The batch challenge algo-
rithm takes all the abstract information as input. It selects a
set of owners Ochal and a set of cloud servers Schal . For each
data owner Ok ðk 2 Ochal Þ, it chooses a set of data blocks as

umkl;ij
k;j

skt;k

;

T Pl ¼

tvkl;i
kl;i :

k2Ochal

i2Qkl

DPl ¼

eðuk;j ; Rk ÞM Pkl;j :

for each j 2 ½1; s,
Then,
it computes the sector linear
X
combination M Pkl;j of all the chosen data blocks of each
owner Ok ðk 2 Ochal Þ as
M Pkl;j ¼
i2Qkl
Y
Y
and generates the data proof DPl as
s

vkl;i  mkl;ij ;

j¼1
k2Ochal
It outputs the proof P l ¼ ðT Pl ; DPl Þ.
Step 3: Batch verification. Upon receiving all the proofs
from the challenged servers, the auditor runs the following
batch verification algorithm BVerify to check the correctness
of the proofs.
BVerifyðC; fP l g; fskh;lk g; fpkt;k g; fMinf o;kl gÞ ! 0=1. T h e
batch verification algorithm takes as inputs the challenge
the proofs fP l gl2Schal ,
C,
the set of secret hash keys
fskh;kl gk2Ochal ;l2Schal , the public tag keys fpkt;k gk2Ochal , and the
abs t rac t
in fo rma t ion o f
the cha l lenged da ta b locks
fMinf o;kl gk2Ochal ;l2Schal . For each owner Ok ðk 2 Ochal Þ, it com-
i d e n t i f i e r h a s h v a l u e s fhðskh;kl ;
p u t e s t h e s e t o f
Wkl;i Þgl2Schal ;i2Qkl for all the chosen data blocks from each
challenged server and use these hash values to compute a
Y
Y
challenge hash Hchal;k as

Hchal;k ¼

l2Schal

i2Qkl

hðskh;kl ; Wkl;i Þrvkl;i :

the data owners’
When finished the calculation of all
challenge hash fHchal;k gk2Ochal , it verifies the proofs by the

  Q
Y
Q
batch verification equation as
T Pl ; gr
DPl ¼ e
l2Schal
2
eðHchal;k ; pkt;k Þ :
k2Ochal

ð2Þ

l2Schal

If (2) is true, it outputs 1. Otherwise, it outputs 0.

1724

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 9, SEPTEMBER 2013

TABLE 4
Communication Cost Comparison of Batch Auditing for K
Owners and C Clouds

t is the number of challenged data blocks from each owner on each
cloud server; s is the number of sectors in each data block; n is the total
number of data blocks of a file in Wang’s scheme.

6 PERFORMANCE ANALYSIS OF OUR AUDITING
PROTOCOLS

Storage auditing is a very resource demanding service in
terms of computational resource, communication cost, and
memory space. In this section, we give the communica-
tion cost comparison and computation complexity com-
parison between our scheme and two existing works:
the Audit protocol proposed by Wang et al. [23], [24] and
the IPDP proposed by Zhu et al. [25], [26]. The storage
overhead analysis will be shown in the supplemental file,
available online.

6.1 Communication Cost
Because the communication cost during the initialization is
almost the same in these three auditing protocols, we only
compare the communication cost between the auditor and
the server, which consists of the challenge and the proof.
Consider a batch auditing with K owners and C cloud
servers. Suppose the number of challenged data block from
each owner on different cloud servers is the same, denoted
as t, and the data block are split into s sectors in Zhu’s IPDP
and our scheme. We do the comparison under the same
probability of detection. That is, in Wang’s scheme, the
number of data blocks from each owner on each cloud
server should be st. The result is described in Table 4.
From the table, we can see that the communication cost in
Wang’s auditing scheme is not only linear to C , K , t, s, but
also linear to the total number of data blocks n. As we know,
in large-scale cloud storage systems, the total number of
data blocks could be very large. Therefore, Wang’s auditing
scheme may incur high communication cost.
Our scheme and Zhu’s IPDP have the same total
communication cost during the challenge phase. During
the proof phase, the communication cost of the proof in our

scheme is only linear to C , but
the
in Zhu’s IPDP ,
communication cost of the proof is not only linear to C and
K , but also linear to s. That is because Zhu’s IPDP uses the
mask technique to protect the data privacy, which requires to
send both the masked proof and the encrypted mask to the
auditor. In our scheme, the server is only required to send the
encrypted proof to the auditor and, thus,
incurs less
communication cost than Zhu’s IPDP.

6.2 Computation Complexity
We simulate the computation of the owner, the server, and
the auditor on a Linux system with an Intel Core 2 Duo
CPU at 3.16 GHz and 4.00-GB RAM. The code uses the
pairing-based cryptography library version 0.5.12 to simu-
late our auditing scheme and Zhu’s IPDP scheme (Under
the same detection of probability, Wang’s scheme requires
much more data blocks than our scheme and Zhu’s scheme,
such that the computation time is almost s times more than
our scheme and Zhu’s IPDP, and thus, it is not compar-
able). The elliptic curve we used is a MNT d159 curve,
where the base field size is 159 bit and the embedding
degree is 6. The d159 curve has a 160-bit group order, which
means p is a 160-bit length prime. All the simulation results
are the mean of 20 trials.

6.2.1 Computation Cost of the Auditor
We compare the computation time of the auditor versus the
number of data blocks, the number of clouds, and the
number of owners in Fig. 4.
Fig. 4a shows the computation time of the auditor versus
the number of challenged data blocks in the single cloud and
single owner case. In this figure, the number of data blocks
goes to 500 (i.e., the challenged data size equals to 500 KByte),
but it can illustrate the linear relationship between the
computation cost of the auditor versus the challenged data
size. From Fig. 4a, we can see that our scheme incurs less
computation cost of the auditor than Zhu’s IPDP scheme,
when coping with large number of challenged data blocks.
In real cloud storage systems, the data size is very large
(e.g., petabytes), our scheme apply the sampling auditing
method to ensure the integrity of such large data.
The sample size and the frequency are determined by the
service-level agreement. From the simulation results,
we can estimate that it requires 800 seconds to audit for
1-GByte data. However, the computing abilities of the
cloud server and the auditor are much more powerful than

Fig. 4. Comparison of computation cost of the auditor (s ¼ 50).

YANG AND JIA: AN EFFICIENT AND SECURE DYNAMIC AUDITING PROTOCOL FOR DATA STORAGE IN CLOUD COMPUTING

1725

burden to the server because they relied on the PDP scheme
proposed by Ateniese.
the authors proposed a dynamic auditing
In [23],
protocol that can support the dynamic operations of the
data on the cloud servers, but this method may leak the data
content to the auditor because it requires the server to send
the linear combinations of data blocks to the auditor. In [24],
the authors extended their dynamic auditing scheme to be
privacy preserving and support the batch auditing for
multiple owners. However, due to the large number of data
tags, their auditing protocols will incur a heavy storage
overhead on the server. In [25], Zhu et al. proposed a
cooperative provable data possession scheme that can
support the batch auditing for multiple clouds and also
extend it to support the dynamic auditing in [26]. However,
it is impossible for their scheme to support the batch
auditing for multiple owners. That is because parameters for
generating the data tags used by each owner are different,
and thus, they cannot combine the data tags from multiple
owners to conduct the batch auditing. Another drawback is
that their scheme requires an additional trusted organizer to
send a commitment to the auditor during the batch auditing
for multiple clouds, because their scheme applies the mask
technique to ensure the data privacy. However, such
additional organizer is not practical
in cloud storage
systems. Furthermore, both Wang’s schemes and Zhu’s
schemes incur heavy computation cost of the auditor, which
makes the auditing system inefficient.

8 CONCLUSION

In this paper, we proposed an efficient and inherently
secure dynamic auditing protocol. It protects the data
privacy against the auditor by combining the cryptography
method with the bilinearity property of bilinear paring,
rather than using the mask technique. Thus, our multicloud
batch auditing protocol does not require any additional
organizer. Our batch auditing protocol can also support the
batch auditing for multiple owners. Furthermore, our
auditing scheme incurs less communication cost and less
computation cost of the auditor by moving the computing
loads of auditing from the auditor to the server, which
greatly improves the auditing performance and can be
applied to large-scale cloud storage systems.

REFERENCES
[1] P. Mell and T. Grance, “The NIST Definition of Cloud Comput-
ing,” technical report, Nat’l Inst. of Standards and Technology,
2009.
[2] M. Armbrust, A. Fox, R. Griffith, A.D. Joseph, R.H. Katz, A.
Konwinski, G. Lee, D.A. Patterson, A. Rabkin, I. Stoica, and M.
Zaharia, “A View of Cloud Computing,” Comm. ACM, vol. 53,
no. 4, pp. 50-58, 2010.
[3] T. Velte, A. Velte, and R. Elsenpeter, Cloud Computing: A Practical
Approach, first ed., ch. 7. McGraw-Hill, 2010.
J. Li, M.N. Krohn, D. Mazie` res, and D. Shasha, “Secure Untrusted
Data Repository (SUNDR),” Proc. Sixth Conf. Symp. Operating
Systems Design Implementation, pp. 121-136, 2004.
[5] G.R. Goodson, J.J. Wylie, G.R. Ganger, and M.K. Reiter, “Efficient
Byzantine-Tolerant Erasure-Coded Storage,” Proc.
Int’l Conf.
Dependable Systems and Networks, pp. 135-144, 2004.
[6] V. Kher and Y. Kim, “Securing Distributed Storage: Challenges,
Techniques, and Systems,” Proc. ACM Workshop Storage Security
and Survivability (StorageSS), V. Atluri, P. Samarati, W. Yurcik, L.
Brumbaugh, and Y. Zhou, eds., pp. 9-25, 2005.

[4]

Fig. 5. Comparison of computation cost on the server (s ¼ 50).

our simulation PC, so the computation time can be
relatively small. Therefore, our auditing scheme is practical
in large-scale cloud storage systems.
Fig. 4b describes the computation cost of the auditor of
the multicloud batch auditing scheme versus the number of
challenged clouds. It is easy to find that our scheme incurs
less computation cost of the auditor than Zhu’s IPDP
scheme, especially when there are a large number of clouds
in the large-scale cloud storage systems.
Because Zhu’s IPDP does not support the batch auditing
for multiple owners,
in our simulation, we repeat the
computation for several times that is equal to the number of
data owners. Then, as shown in Fig. 4c, we compare the
computation cost of the auditor between our multiowner
batch auditing and the general auditing protocol that does
not support the multiowner batch auditing (e.g., Zhu’s
IPDP). Fig. 4c also demonstrates that the batch auditing for
multiple owners can greatly reduce the computation cost.
Although in our simulation the number of data owners goes
to 500, it can illustrate the trend of computation cost of the
auditor that our scheme is much more efficient than Zhu’s
scheme in large-scale cloud storage systems that may have
millions to billions of data owners.

6.2.2 Computation Cost of the Server
We compare the computation cost of the server versus the
number of data blocks in Fig. 5a and the number of data
owners in Fig. 5b. Our scheme moves the computing loads
of the auditing from the auditor to the server, such that it
can greatly reduce the computation cost of the auditor.

7 RELATED WORK

To support
the dynamic auditing, Ateniese et al. [29]
developed a dynamic provable data possession protocol
based on cryptographic hash function and symmetric key
encryption. Their idea is to precompute a certain number of
metadata during the setup period, so that the number of
updates and challenges is limited and fixed beforehand. In
their protocol, each update operation requires recreating all
the remaining metadata, which is problematic for large files.
Moreover, their protocol cannot perform block insertions
anywhere (only append-type insertions are allowed). Erway
et al. [22] also extended the PDP model to support dynamic
updates on the stored data and proposed two dynamic
provable data possession scheme by using a new version of
authenticated dictionaries based on rank information.
However, their schemes may cause heavy computation

1726

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 9, SEPTEMBER 2013

[29] G. Ateniese, R.D. Pietro, L.V. Mancini, and G. Tsudik, “Scalable
and Efficient Provable Data Possession,” IACR Cryptology ePrint
Archive, vol. 2008, p. 114, 2008.
[30] R. Ostrovsky, A. Sahai, and B. Waters, “Attribute-Based Encryp-
tion with Non-Monotonic Access Structures,” Proc. ACM Conf.
Computer and Comm. Security (CCS ’07), P. Ning, S.D.C. di
Vimercati, and P.F. Syverson, eds., Oct. 2007.

Kan Yang received the BEng degree from the
University of Science and Technology of China
in 2008. He is currently working toward the PhD
degree in the Department of Computer Science
at
the C ity Un iversi ty of Hong Kong. H is
research interests include cryptography, infor-
mation security, cloud computing and distributed
systems. He is a student member of the IEEE
and the IEEE Computer Society.

Xiaohua Jia received the BSc and MEng
degrees from the University of Science and
Techno logy o f Ch ina ,
in 1984 and 1987 ,
respectively, and the DSc degree in information
science from the University of Tokyo in 1991. He
is currently a chair professor in the Department
of Computer Science at the City University of
Hong Kong. His research interests include cloud
computing and distributed systems, computer
networks, wireless sensor networks and mobile
wireless networks. He is an editor of the IEEE Transactions on Parallel
and Distributed Systems (2006-2009), Wireless Networks, Journal of
World Wide Web, Journal of Combinatorial Optimization, and so on. He
is the general chair of ACM MobiHoc 2008, TPC cochair of IEEE MASS
2009, area chair of
IEEE
IEEE INFOCOM 2010, TPC cochair of
GlobeCom 2010 Ad Hoc and Sensor Networking Symp, and panel
cochair of IEEE INFOCOM 2011. He is a fellow of the IEEE and the
IEEE Computer Society.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

[7] L.N. Bairavasundaram, G.R. Goodson, S. Pasupathy, and J.
Schindler, “An Analysis of Latent Sector Errors in Disk Drives,”
Proc. ACM SIGMETRICS Int’l Conf. Measurement and Modeling of
Computer Systems, L. Golubchik, M.H. Ammar, and M. Harchol-
Balter, eds., pp. 289-300, 2007.
[8] B. Schroeder and G.A. Gibson, “Disk Failures in the Real World:
What Does an MTTF of 1,000,000 Hours Mean to You?” Proc.
USENIX Conf. File and Storage Technologies, pp. 1-16, 2007.
[9] M. Lillibridge, S. Elnikety, A. Birrell, M. Burrows, and M. Isard,
“A Cooperative Internet Backup Scheme,” Proc. USENIX Ann.
Technical Conf., pp. 29-41, 2003.
[10] Y. Deswarte, J. Quisquater, and A. Saidane, “Remote Integrity
Checking,” Proc. Sixth Working Conf. Integrity and Internal Control
in Information Systems (IICIS), Nov. 2004.
[11] M. Naor and G.N. Rothblum, “The Complexity of Online Memory
Checking,” J. ACM, vol. 56, no. 1, article 2, 2009.
[12] A. Juels and B.S. Kaliski Jr., “Pors: Proofs of Retrievability for Large
Files,” Proc. ACM Conf. Computer and Comm. Security, P. Ning,
S.D.C. di Vimercati, and P.F. Syverson, eds., pp. 584-597, 2007.
[13] T.J.E. Schwarz and E.L. Miller, “Store, Forget, and Check: Using
Algebraic Signatures to Check Remotely Administered Storage,”
Proc. 26th IEEE Int’l Conf. Distributed Computing Systems, p. 12,
2006.
[14] D.L.G. Filho and P.S.L.M. Barreto, “Demonstrating Data Posses-
sion and Uncheatable Data Transfer,” IACR Cryptology ePrint
Archive, vol. 2006, p. 150, 2006.
[15] F. Sebe´ , J. Domingo-Ferrer, A. Martı´nez-Balleste´ , Y. Deswarte, and
J.-J. Quisquater, “Efficient Remote Data Possession Checking in
Critical Information Infrastructures,” IEEE Trans. Knowledge Data
Eng., vol. 20, no. 8, pp. 1034-1038, Aug. 2008.
[16] G. Yamamoto, S. Oda, and K. Aoki, “Fast Integrity for Large
Data,” Proc. ECRYPT Workshop Software Performance Enhancement
for Encryption and Decryption, pp. 21-32, June 2007.
[17] M.A. Shah, M. Baker, J.C. Mogul, and R. Swaminathan, “Auditing
to Keep Online Storage Services Honest,” Proc. 11th USENIX
Workshop Hot Topics in Operating Systems (HOTOS), G.C. Hunt, ed.,
2007.
[18] C. Wang, K. Ren, W. Lou, and J. Li, “Toward Publicly Auditable
Secure Cloud Data Storage Services,” IEEE Network, vol. 24, no. 4,
pp. 19-24, July/Aug. 2010.
[19] K. Yang and X. Jia, “Data Storage Auditing Service in Cloud
Computing: Challenges, Methods and Opportunities,” World Wide
Web, vol. 15, no. 4, pp. 409-428, 2012.
[20] G. Ateniese, R.C. Burns, R. Curtmola, J. Herring, L. Kissner, Z.N.J.
Peterson, and D.X. Song, “Provable Data Possession at Untrusted
Stores,” Proc. ACM Conf. Computer and Comm. Security, P. Ning,
S.D.C. di Vimercati, and P.F. Syverson, eds., pp. 598-609, 2007.
[21] H. Shacham and B. Waters, “Compact Proofs of Retrievability,”
Proc. 14th Int’l Conf. Theory and Application of Cryptology and
J. Pieprzyk, ed.,
Information Security: Advances in Cryptology,
pp. 90-107, 2008.
[22] C.C. Erway, A. Ku¨ pc¸u¨ , C. Papamanthou, and R. Tamassia,
“Dynamic Provable Data Possession,” Proc. ACM Conf. Computer
and Comm. Security, E. Al-Shaer, S. Jha, and A.D. Keromytis, eds.,
pp. 213-222, 2009.
[23] Q. Wang, C. Wang, K. Ren, W. Lou, and J. Li, “Enabling Public
Auditability and Data Dynamics for Storage Security in Cloud
Computing,” IEEE Trans. Parallel Distributed Systems, vol. 22, no. 5,
pp. 847-859, May 2011.
[24] C. Wang, Q. Wang, K. Ren, and W. Lou, “Privacy-Preserving
Public Auditing for Data Storage Security in Cloud Computing,”
Proc. IEEE INFOCOM, pp. 525-533, 2010.
[25] Y. Zhu, H. Hu, G. Ahn, and M. Yu, “Cooperative Provable Data
Possession for Integrity Verification in Multi-Cloud Storage,”
IEEE Trans. Parallel and Distributed Systems, vol. 23, no. 12,
pp. 2231-2244, Dec. 2012.
[26] Y. Zhu, H. Wang, Z. Hu, G.-J. Ahn, H. Hu, and S.S. Yau, “Dynamic
Audit Services for Integrity Verification of Outsourced Storages in
Clouds,” Proc. ACM Symp. Applied Computing, W.C. Chu, W.E.
Wong, M.J. Palakal, and C.-C. Hung, eds., pp. 1550-1557, 2011.
[27] K. Zeng, “Publicly Verifiable Remote Data Integrity,” Proc. 10th
Int’l Conf. Information and Comm. Security, L. Chen, M.D. Ryan, and
G. Wang, eds., pp. 419-434, 2008.
[28] G. Ateniese, S. Kamara, and J. Katz, “Proofs of Storage from
Homomorphic Identification Protocols,” Proc. Int’l Conf. Theory
and Application of Cryptology and Information Security: Advances in
Cryptology, M. Matsui, ed., pp. 319-333, 2009.

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 6,

JUNE 2013

1107

Dynamic Resource Allocation Using Virtual
Machines for Cloud Computing Environment

Zhen X iao, Sen ior Member, IEEE, We i j ia Song, and Q i Chen

Abstract—Cloud computing allows business customers to scale up and down their resource usage based on needs. Many of the
touted gains in the cloud model come from resource multiplexing through virtualization technology. In this paper, we present a system
that uses virtualization technology to allocate data center resources dynamically based on application demands and support green
computing by optimizing the number of servers in use. We introduce the concept of “skewness” to measure the unevenness in the
multidimensional resource utilization of a server. By minimizing skewness, we can combine different types of workloads nicely and
improve the overall utilization of server resources. We develop a set of heuristics that prevent overload in the system effectively while
saving energy used. Trace driven simulation and experiment results demonstrate that our algorithm achieves good performance.

Index Terms—Cloud computing, resource management, virtualization, green computing

Ç

1 INTRODUCTION
THE elasticity and the lack of upfront capital investment
offered by cloud computing is appealing to many
businesses. There is a lot of discussion on the benefits and
costs of the cloud model and on how to move legacy
applications onto the cloud platform. Here we study a
different problem: how can a cloud service provider best
multiplex its virtual resources onto the physical hardware?
This is important because much of the touted gains in the
cloud model come from such multiplexing. Studies have
found that servers in many existing data centers are often
severely underutilized due to overprovisioning for the peak
demand [1], [2]. The cloud model is expected to make such
practice unnecessary by offering automatic scale up and
down in response to load variation. Besides reducing the
hardware cost, it also saves on electricity which contributes
to a significant portion of the operational expenses in large
data centers.
Virtual machine monitors (VMMs) like Xen provide a
mechan ism for mapp ing v irtua l machines (VMs)
to
physical resources [3]. This mapping is largely hidden
from the cloud users. Users with the Amazon EC2 service
[4], for example, do not know where their VM instances
run. It is up to the cloud provider to make sure the
underlying physical machines (PMs) have sufficient re-
sources to meet their needs. VM live migration technology
makes it possible to change the mapping between VMs and
PMs while applications are running [5], [6]. However, a
policy issue remains as how to decide the mapping
adaptively so that the resource demands of VMs are met
while the number of PMs used is minimized. This is

. The authors are with the Department of Computer Science, Peking
University, No. 5, Yiheyuan Road, Haidian District, Beijing 100871, P.R.
China. E-mail: {xiaozhen, songweijia}@pku.edu.cn, chenqi@net.pku.edu.cn.

Manuscript received 26 Feb. 2012; revised 18 July 2012; accepted 17 Sept.
2012; published online 21 Sept. 2012.
Recommended for acceptance by V.B. Misic, R. Buyya, D. Milojicic, and
Y. Cui.
For information on obtaining reprints of this article, please send e-mail to:
tpds@computer.org, and reference IEEECS Log Number
TPDSSI-2012-02-0148.
Digital Object Identifier no. 10.1109/TPDS.2012.283.

challenging when the resource needs of VMs are hetero-
geneous due to the diverse set of applications they run and
vary with time as the workloads grow and shrink. The
capacity of PMs can also be heterogenous because multiple
generations of hardware coexist in a data center.
We aim to achieve two goals in our algorithm:

. Overload avoidance. The capacity of a PM should be
sufficient to satisfy the resource needs of all VMs
running on it. Otherwise, the PM is overloaded and
can lead to degraded performance of its VMs.
. Green computing. The number of PMs used should be
minimized as long as they can still satisfy the needs
of all VMs. Idle PMs can be turned off to save energy.
There is an inherent tradeoff between the two goals in the
face of changing resource needs of VMs. For overload
avoidance, we should keep the utilization of PMs low to
reduce the possibility of overload in case the resource needs
of VMs increase later. For green computing, we should keep
the utilization of PMs reasonably high to make efficient use
of their energy.
In this paper, we present the design and implementation
of an automated resource management system that achieves
a good balance between the two goals. We make the
following contributions:

. We develop a resource allocation system that can
avoid overload in the system effectively while
minimizing the number of servers used.
. We introduce the concept of “skewness” to measure
the uneven utilization of a server. By minimizing
skewness, we can improve the overall utilization of
servers in the face of multidimensional resource
constraints.
. We design a load prediction algorithm that can
capture the future resource usages of applications
accurately without
looking inside the VMs. The
algorithm can capture the rising trend of resource
usage patterns and help reduce the placement churn
significantly.

1045-9219/13/$31.00 ß 2013 IEEE

Published by the IEEE Computer Society

1108

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 6,

JUNE 2013

can change the CPU allocation among the VMs by adjusting
their weights in its CPU scheduler. The MM Alloter on
domain 0 of each node is responsible for adjusting the local
memory allocation.
The hot spot solver in our VM Scheduler detects if the
resource utilization of any PM is above the hot threshold
(i.e., a hot spot). If so, some VMs running on them will be
migrated away to reduce their load. The cold spot solver
checks if the average utilization of actively used PMs
(APMs) is below the green computing threshold. If so, some of
those PMs could potentially be turned off to save energy. It
identifies the set of PMs whose utilization is below the cold
threshold (i.e., cold spots) and then attempts to migrate away
all their VMs. It then compiles a migration list of VMs and
passes it to the Usher CTRL for execution.

3 PREDICTING FUTURE RESOURCE NEEDS

We need to predict the future resource needs of VMs. As
said earlier, our focus is on Internet applications. One
solution is to look inside a VM for application level statistics,
e.g., by parsing logs of pending requests. Doing so requires
modification of the VM which may not always be possible.
Instead, we make our prediction based on the past external
behaviors of VMs. Our first attempt was to calculate an
exponentially weighted moving average (EWMA) using a
TCP-like scheme
E ðtÞ ¼   E ðt   1Þ þ ð1   Þ  OðtÞ; 0    1;
where E ðtÞ and OðtÞ are the estimated and the observed
load at time t, respectively.  reflects a tradeoff between
stability and responsiveness.
We use the EWMA formula to predict the CPU load on
the DNS server in our university. We measure the load
every minute and predict the load in the next minute.
Fig. 2a shows the results for  ¼ 0:7. Each dot in the figure
is an observed value and the curve represents the predicted
values. Visually, the curve cuts through the middle of the
dots which indicates a fairly accurate prediction. This is also
verified by the statistics in Table 1. The parameters in the
parenthesis are the  values. W is the length of
the
measurement window (explained later). The “median”
error is calculated as a percentage of the observed value:
jEðtÞ   OðtÞj=OðtÞ. The “higher” and “lower” error percen-
tages are the percentages of predicted values that are higher
or lower than the observed values, respectively. As we can
see, the prediction is fairly accurate with roughly equal
percentage of higher and lower values.
Although seemingly satisfactory, this formula does not
capture the rising trends of resource usage. For example,
when we see a sequence of OðtÞ ¼ 10; 20; 30, and 40, it is
reasonable to predict the next value to be 50. Unfortunately,
when  is between 0 and 1, the predicted value is always
between the historic value and the observed one. To reflect
the “acceleration,” we take an innovative approach by
setting  to a negative value. When  1   < 0, the above
formula can be transformed into the following:
E ðtÞ ¼  jj  Eðt   1Þ þ ð1 þ jjÞ  OðtÞ
¼ OðtÞ þ jj  ðOðtÞ   Eðt   1ÞÞ;

Fig. 1. System architecture.

The rest of the paper is organized as follows. Section 2
provides an overview of our system and Section 3 describes
our algorithm to predict resource usage. The details of our
algorithm are presented in Section 4. Sections 5 and
6 present simulation and experiment results, respectively.
Section 7 discusses related work. Section 8 concludes.

2 SYSTEM OVERVIEW

The architecture of the system is presented in Fig. 1. Each
PM runs the Xen hypervisor (VMM) which supports a
privileged domain 0 and one or more domain U [3]. Each
VM in domain U encapsulates one or more applications
such as Web server, remote desktop, DNS, Mail, Map/
Reduce, etc. We assume all PMs share a backend storage.
The multiplexing of VMs to PMs is managed using the
Usher framework [7]. The main logic of our system is
implemented as a set of plug-ins to Usher. Each node runs
an Usher local node manager (LNM) on domain 0 which
collects the usage statistics of resources for each VM on that
node. The CPU and network usage can be calculated by
monitoring the scheduling events in Xen. The memory
usage within a VM, however ,
is not visible to the
hypervisor. One approach is to infer memory shortage of
a VM by observing its swap activities [8]. Unfortunately, the
guest OS is required to install a separate swap partition.
it may be too late to adjust the memory
Furthermore,
allocation by the time swapping occurs.
Instead we
implemented a working set prober (WS Prober) on each
hypervisor to estimate the working set sizes of VMs
running on it. We use the random page sampling technique
as in the VMware ESX Server [9].
The statistics collected at each PM are forwarded to the
Usher central controller (Usher CTRL) where our VM
scheduler runs. The VM Scheduler is invoked periodically
and receives from the LNM the resource demand history of
VMs, the capacity and the load history of PMs, and the
current layout of VMs on PMs.
The scheduler has several components. The predictor
predicts the future resource demands of VMs and the
future load of PMs based on past statistics. We compute the
load of a PM by aggregating the resource usage of its VMs.
The details of
the load prediction algorithm will be
described in the next section. The LNM at each node first
attempts to satisfy the new demands locally by adjusting
the resource allocation of VMs sharing the same VMM. Xen

XIAO ET AL.: DYNAMIC RESOURCE ALLOCATION USING VIRTUAL MACHINES FOR CLOUD COMPUTING ENVIRONMENT

1109

Fig. 2. CPU load prediction for the DNS server at our university. W is the measurement window.

On the other hand, when the observed resource usage is
going down, we want to be conservative in reducing our
estimation. Hence, we use two parameters, "  and # , to
control how quickly E ðtÞ adapts to changes when OðtÞ is
increasing or decreasing, respectively. We call this the Fast
Up and Slow Down (FUSD) algorithm. Fig. 2b shows the
the FUSD a lgor ithm for "  ¼  0:2,
effect iveness of
#  ¼ 0:7.
(These values are selected based on field
experience with traces collected for several
Internet
applications.) Now the predicted values are higher than
the observed ones most of the time: 77 percent according to
Table 1. The median error is increased to 9.4 percent
because we trade accuracy for safety.
It
is still quite
acceptable nevertheless.
So far we take OðtÞ as the last observed value. Most
applications have their SLOs specified in terms of a certain
percentiles of requests meeting a specific performance level.
More generally, we keep a window of W recently observed
values and take OðtÞ as a high percentile of them. Fig. 2c
shows the result when W ¼ 8 and we take the 90% th
percentile of the peak resource demand. The figure shows
that the prediction gets substantially better.
We have also investigated other prediction algorithms.
Linear autoregression (AR) models,
for example, are
broadly adopted in load prediction by other works [10],
[11], [12]. It models a predictive value as linear function of
its past observations. Model parameters are determined by
training with historical values. AR predictors are capable of
incorporating the seasonal pattern of load change. For
instance, the SPAR(4,2) [10] estimate the future logging rate
of MSN clients from six past observations, two of which are
the latest observations and the other four at the same time
in the last four weeks.
We compare SPAR(4,2) and FUSDð 0:2; 0:7Þ in Fig. 3.
“lpct” refers to the percentage of low errors while “std”
refers to standard deviation. Both algorithms are used to
predict the CPU utilization of the aforementioned DNS
server in a 1-day duration. The predicting window is

TABLE 1
Load Prediction Algorithms

8 minute. The standard deviation (std) of SPAR (4,2) is
about 16 percent smaller than that of FUSDð 0:2; 0:7Þ,
which means SPAR (4,2) achieves sightly better percision.
This is because it takes advantage of tiding pattern of the
load. However, SPAR(4,2) neither avoids low prediction
nor smooths the load. The requirement of a training phase
to determine parameters is inconvenient, especially when
the load pattern changes. Therefore, we adopt the simpler
EWMA variance. Thorough investigation on prediction
algorithms are left as future work.
As we will see later in the paper, the prediction algorithm
plays an important role in improving the stability and
performance of our resource allocation decisions.

4 THE SKEWNESS ALGORITHM

We introduce the concept of skewness to quantify the
unevenness in the utilization of multiple resources on a
server. Let n be the number of resources we consider and ri
s
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
be the utilization of the ith resource. We define the resource


X
skewness of a server p as
n
  1
ri
2
r

skewnessðpÞ ¼

;

i¼1

where r is the average utilization of all resources for
server p.
In practice, not all
types of resources are
performance critical and hence we only need to consider
bottleneck resources in the above calculation. By mini-
mizing the skewness, we can combine different types of
workloads nicely and improve the overall utilization of
server resources. In the following, we describe the details
of our algorithm. Analysis of the algorithm is presented
in Section 1 in the supplementary file, which can be

Fig. 3. Comparison of SPAR and FUSD.

1110

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 6,

JUNE 2013

found on the ComputerSociety Digital Library at http://
doi.ieeecomputersociety.org/10.1109/TPDS.2012.283.

4.1 Hot and Cold Spots
Our algorithm executes periodically to evaluate the
resource allocation status based on the predicted future
resource demands of VMs. We define a server as a hot spot
if the utilization of any of its resources is above a hot
threshold. This indicates that the server is overloaded and
hence some VMs running on it should be migrated away.
X
We define the temperature of a hot spot p as the square sum
of its resource utilization beyond the hot threshold:
ðr   rt Þ2 ;
temperatureðpÞ ¼

r2R

where R is the set of overloaded resources in server p and rt
threshold for resource r.
is the hot
(Note that only
overloaded resources are considered in the calculation.)
The temperature of a hot spot reflects its degree of overload.
If a server is not a hot spot, its temperature is zero.
We define a server as a cold spot if the utilizations of all its
resources are below a cold threshold. This indicates that the
server is mostly idle and a potential candidate to turn off to
save energy. However, we do so only when the average
resource utilization of all actively used servers (i.e., APMs)
in the system is below a green computing threshold. A server
is actively used if it has at least one VM running. Otherwise,
it is inactive. Finally, we define the warm threshold to be a
level of resource utilization that is sufficiently high to justify
having the server running but not so high as to risk
becoming a hot spot in the face of temporary fluctuation of
application resource demands.
Different types of resources can have different thresh-
olds. For example, we can define the hot thresholds for CPU
and memory resources to be 90 and 80 percent, respectively.
Thus a server is a hot spot if either its CPU usage is above
90 percent or its memory usage is above 80 percent.

4.2 Hot Spot Mitigation
We sort the list of hot spots in the system in descending
temperature (i.e., we handle the hottest one first). Our goal
is to eliminate all hot spots if possible. Otherwise, keep their
temperature as low as possible. For each server p, we first
decide which of its VMs should be migrated away. We sort
its list of VMs based on the resulting temperature of
the server if that VM is migrated away. We aim to migrate
away the VM that can reduce the server’s temperature the
most. In case of ties, we select the VM whose removal can
reduce the skewness of the server the most. For each VM in
the list, we see if we can find a destination server to
accommodate it. The server must not become a hot spot
after accepting this VM. Among all such servers, we select
one whose skewness can be reduced the most by accepting
this VM. Note that this reduction can be negative which
means we select the server whose skewness increases the
least.
If a destination server is found, we record the
migration of the VM to that server and update the predicted
load of related servers. Otherwise, we move onto the next
VM in the list and try to find a destination server for it. As
long as we can find a destination server for any of its VMs,
we consider this run of the algorithm a success and then

move onto the next hot spot. Note that each run of the
algorithm migrates away at most one VM from the
overloaded server. This does not necessarily eliminate the
hot spot, but at least reduces its temperature. If it remains a
hot spot in the next decision run, the algorithm will repeat
this process. It is possible to design the algorithm so that it
can migrate away multiple VMs during each run. But this
can add more load on the related servers during a period
when they are already overloaded. We decide to use this
more conservative approach and leave the system some
time to react before initiating additional migrations.

4.3 Green Computing
When the resource utilization of active servers is too low,
some of them can be turned off to save energy. This is
handled in our green computing algorithm. The challenge
here is to reduce the number of active servers during low
load without sacrificing performance either now or in the
future. We need to avoid oscillation in the system.
Our green computing algorithm is invoked when the
average utilizations of all resources on active servers are
below the green computing threshold. We sort the list of
cold spots in the system based on the ascending order of
their memory size. Since we need to migrate away all its
VMs before we can shut down an underutilized server, we
define the memory size of a cold spot as the aggregate
memory size of all VMs running on it. Recall that our model
assumes all VMs connect to a shared back-end storage.
Hence, the cost of a VM live migration is determined mostly
by its memory footprint. Section 7 in the supplementary file
explains why the memory is a good measure in depth. We
try to eliminate the cold spot with the lowest cost first.
For a cold spot p, we check if we can migrate all its VMs
somewhere else. For each VM on p, we try to find a
destination server to accommodate it. The resource utiliza-
tions of the server after accepting the VM must be below the
warm threshold. While we can save energy by consolidating
underutilized servers, overdoing it may create hot spots in
the future. The warm threshold is designed to prevent that.
If multiple servers satisfy the above criterion, we prefer one
that is not a current cold spot. This is because increasing
load on a cold spot reduces the likelihood that it can be
eliminated. However, we will accept a cold spot as the
destination server if necessary. All things being equal, we
select a destination server whose skewness can be reduced
the most by accepting this VM. If we can find destination
servers for all VMs on a cold spot, we record the sequence
of migrations and update the predicted load of related
servers. Otherwise, we do not migrate any of its VMs. The
list of cold spots is also updated because some of them may
no longer be cold due to the proposed VM migrations in the
above process.
The above consolidation adds extra load onto the related
servers. This is not as serious a problem as in the hot spot
mitigation case because green computing is initiated only
when the load in the system is low. Nevertheless, we want
to bound the extra load due to server consolidation. We
restrict the number of cold spots that can be eliminated in
each run of the algorithm to be no more than a certain
percentage of active servers in the system. This is called the
consolidation limit.

XIAO ET AL.: DYNAMIC RESOURCE ALLOCATION USING VIRTUAL MACHINES FOR CLOUD COMPUTING ENVIRONMENT

1111

TABLE 2
Parameters in Our Simulation

Note that we eliminate cold spots in the system only
when the average load of all active servers (APMs) is below
the green computing threshold. Otherwise, we leave those
cold spots there as potential destination machines for future
offloading. This is consistent with our philosophy that
green computing should be conducted conservatively.

4.4 Consolidated Movements
The movements generated in each step above are not
executed until all steps have finished. The list of movements
are then consolidated so that each VM is moved at most
once to its final destination. For example, hot spot mitigation
may dictate a VM to move from PM A to PM B, while green
computing dictates it to move from PM B to PM C. In the
actual execution, the VM is moved from A to C directly.

5 SIMULATIONS

We evaluate the performance of our algorithm using trace
driven simulation. Note that our simulation uses the same
code base for the algorithm as the real implementation in
the experiments. This ensures the fidelity of our simulation
results. Traces are per-minute server resource utilization,
such as CPU rate, memory usage, and network traffic
statistics, collected using tools like “perfmon” (Windows),
the “/proc” file system (Linux), “pmstat/vmstat/netstat”
commands (Solaris), etc.. The raw traces are pre-processed
into “Usher” format so that the simulator can read them.
We collected the traces from a variety of sources:

.

. Web InfoMall. The largest online Web archive in
China (i.e., the counterpart of Internet Archive in the
US) with more than three billion archived Web pages.
RealCourse. The largest online distance learning
system in China with servers distributed across 13
major cities.
. AmazingStore. The largest P2P storage system in
China.
We also collected traces from servers and desktop
computers in our university including one of our mail
servers,
the central DNS server, and desktops in our
department. We postprocessed the traces based on days
collected and use random sampling and linear combination
of the data sets to generate the workloads needed. All
simulation in this section uses the real trace workload unless
otherwise specified.
The default parameters we use in the simulation are
shown in Table 2. We used the FUSD load prediction
algorithm with "  ¼  0:2, #  ¼ 0:7, and W ¼ 8. In a
dynamic system, those parameters represent good knobs to
tune the performance of the system adaptively. We choose
the default parameter values based on empirical experience

Fig. 4. Impact of thresholds on the number of APMs.

working with many Internet applications. In the future, we
plan to explore using AI or control theoretic approach to
find near optimal values automatically.

5.1 Effect of Thresholds on APMs
We first evaluate the effect of the various thresholds used in
our algorithm. We simulate a system with 100 PMs and
1,000 VMs (selected randomly from the trace). We use
random VM to PM mapping in the initial
layout. The
scheduler is invoked once per minute. The bottom part of
Fig. 4 show the daily load variation in the system. The x-
axis is the time of the day starting at 8 am. The y-axis is
overloaded with two meanings: the percentage of the load
or the percentage of APMs (i.e., Active PMs) in the system.
Recall that a PM is active (i.e., an APM) if it has at least one
VM running. As can be seen from the figure, the CPU load
demonstrates diurnal patterns which decreases substan-
tially after midnight. The memory consumption is fairly
stable over the time. The network utilization stays very low.
The top part of Fig. 4 shows how the percentage of APMs
vary with the load for different thresholds in our algorithm.
For example, “h0.7 g0.3 c0.1” means that the hot, the green
computing, and the cold thresholds are 70 , 30, and
10 percent, respectively. Parameters not shown in the figure
take the default values in Table 2. Our algorithm can be
made more or less aggressive in its migration decision by
tuning the thresholds. The figure shows that lower hot
thresholds cause more aggressive migrations to mitigate hot
spots in the system and increases the number of APMs, and
higher cold and green computing thresholds cause more
aggressive consolidation which leads to a smaller number of
APMs. With the default thresholds in Table 2, the percentage
of APMs in our algorithm follows the load pattern closely.
To examine the performance of our algorithm in more
extreme situations, we also create a synthetic workload which
mimics the shape of a sine function (only the positive part)
and ranges from 15 to 95 percent with a 20 percent random
fluctuation. It has a much larger peak-to-mean ratio than the
real trace. The results are shown in Section 2 of the
supplementary file, which can be found on the Computer
Society Digital Library.

5.2 Scalability of the Algorithm
We evaluate the scalability of our algorithm by varying the
number of VMs in the simulation between 200 and 1,400.
The ratio of VM to PM is 10:1. The results are shown in

1112

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 6,

JUNE 2013

Fig. 5. Scalability of the algorithm with system size.

Fig. 5. Fig. 5a shows that the average decision time of our
algorithm increases with the system size. The speed of
increase is between linear and quadratic. We break down
the decision time into two parts: hot spot mitigation
(marked as “hot”) and green computing (marked as “cold”).
We find that hot spot mitigation contributes more to
the decision time. We also find that the decision time for
the synthetic workload is higher than that for the real trace
due to the large variation in the synthetic workload. With
140 PMs and 1,400 VMs,
the decision time is about
1.3 seconds for the synthetic workload and 0.2 second for
the real trace.
Fig. 5b shows the average number of migrations in the
whole system during each decision. The number of
migrations is small and increases roughly linearly with
the system size. We find that hot spot contributes more to
the number of migrations. We also find that the number of
migrations in the synthetic workload is higher than that in
the real trace. With 140 PMs and 1,400 VMs, on average
each run of our algorithm incurs about three migrations in
the whole system for the synthetic workload and only 1.3
migrations for the real trace. This is also verified by Fig. 5c
which computes the average number of migrations per VM
in each decision. The figure indicates that each VM
experiences a tiny, roughly constant number of migrations
during a decision run, independent of the system size. This
number is about 0.0022 for the synthetic workload and
0.0009 for the real trace. This translates into roughly one
migration per 456 or 1,174 decision intervals, respectively.
The stability of our algorithm is very good.

We also conduct simulations by varying the VM to PM
ratio. With a higher VM to PM ratio, the load is distributed
more evenly among the PMs. The results are presented in
Section 4 of the supplementary file, which is available online.

5.3 Effect of Load Prediction
We compare the execution of our algorithm with and
without load prediction in Fig. 6. When load prediction is
disabled, the algorithm simply uses the last observed load
in its decision making. Fig. 6a shows that load prediction
significantly reduces the average number of hot spots in the
system during a decision run. Notably, prediction prevents
over 46 percent hot spots in the simulation with 1,400 VMs.
This demonstrates its high effectiveness in preventing
server overload proactively. Without prediction, the algo-
rithm tries to consolidate a PM as soon as its load drops
below the threshold. With prediction,
the algorithm
correctly foresees that the load of the PM will increase
above the threshold shortly and hence takes no action.
This leaves the PM in the “cold spot” state for a while.
However, it also reduces placement churns by avoiding
unnecessary migrations due to temporary load fluctuation.
Consequently, the number of migrations in the system with
load prediction is smaller than that without prediction as
shown in Fig. 6c. We can adjust the conservativeness of
load prediction by tuning its parameters, but the current
configuration largely serves our purpose (i.e., error on the
side of caution). The only downside of having more cold
spots in the system is that it may increase the number of
APMs. This is investigated in Fig. 6b which shows that the
average numbers of APMs remain essentially the same with

Fig. 6. Effect of load prediction.

XIAO ET AL.: DYNAMIC RESOURCE ALLOCATION USING VIRTUAL MACHINES FOR CLOUD COMPUTING ENVIRONMENT

1113

Fig. 7. Algorithm effectiveness.

or without load prediction (the difference is less than
1 percent). This is appealing because significant overload
protection can be achieved without sacrificing resources
efficiency. Fig. 6c compares the average number of
migrations per VM in each decision with and without load
prediction. It shows that each VM experiences 17 percent
fewer migrations with load prediction.

6 EXPERIMENTS

Our experiments are conducted using a group of 30 Dell
PowerEdge blade servers with Intel E5620 CPU and 24 GB
of RAM. The servers run Xen-3.3 and Linux 2.6.18. We
periodically read load statistics using the xenstat library
(same as what xentop does). The servers are connected
over a Gigabit ethernet to a group of four NFS storage
servers where our VM Scheduler runs. We use the same
default parameters as in the simulation.

6.1 Algorithm Effectiveness
We evaluate the effectiveness of our algorithm in overload
mitigation and green computing. We start with a small scale
experiment consisting of three PMs and five VMs so that we
can present the results for all servers in Fig. 7. Different
shades are used for each VM. All VMs are configured with
128 MB of RAM. An Apache server runs on each VM. We
use httperf to invoke CPU intensive PHP scripts on the
Apache server. This allows us to subject the VMs to different
degrees of CPU load by adjusting the client request rates.
The utilization of other resources are kept low.
We first increase the CPU load of the three VMs on PM1
to create an overload. Our algorithm resolves the overload
by migrating VM3 to PM3 . It reaches a stable state under
high load around 420 seconds. Around 890 seconds, we
decrease the CPU load of all VMs gradually. Because the
FUSD prediction algorithm is conservative when the load
decreases, it takes a while before green computing takes
effect. Around 1,700 seconds, VM3 is migrated from PM3 to
PM2 so that PM3 can be put into the standby mode. Around
2,200 seconds, the two VMs on PM1 are migrated to PM2 so
that PM1 can be released as well. As the load goes up and
down, our algorithm will repeat the above process: spread
over or consolidate the VMs as needed.
Next we extend the scale of the experiment to 30 servers.
We use the TPC-W benchmark for this experiment. TPC-W
is an industry standard benchmark for e-commerce

Fig. 8. #APMs varies with TPC-W load.

applications which simulates the browsing and buying
behaviors of customers [13]. We deploy 8 VMs on each
server at the beginning. Each VM is configured with one
virtual CPU and two gigabyte memory. Self-ballooning is
enabled to allow the hypervisor to reclaim unused memory.
Each VM runs the server side of the TPC-W benchmark
corresponding to various types of the workloads: browsing,
shopping, hybrid workloads, etc. Our algorithm is invoked
every 10 minutes.
Fig. 8 shows how the number of APMs varies with the
average number of requests to each VM over time. We keep
the load on each VM low at the beginning. As a result, green
computing takes effect and consolidates the VMs onto a
smaller number of servers.1 Note that each TPC-W server,
even when idle, consumes several hundreds megabytes of
memory. After two hours, we increase the load dramatically
to emulate a “flash crowd” event. The algorithm wakes up
the stand-by servers to offload the hot spot servers. The
figure shows that the number of APMs increases accord-
ingly. After the request rates peak for about one hour, we
reduce the load gradually to emulate that the flash crowd is
over. This triggers green computing again to consolidate the
underutilized servers. Fig. 8 shows that over the course of
the experiment, the number of APM rises much faster than it
falls. This is due to the effect of our FUSD load prediction.
The figure also shows that the number of APMs remains at a
slightly elevated level after the flash crowd. This is because
the TPC-W servers maintain some data in cache and hence
its memory usage never goes back to its original level.
To quantify the energy saving, we measured the
electric power consumption under various TPC-W work-
loads with the built-in watt-meter in our blade systems.
We find that an idle blade server consumes about
130 Watts and a fully utilized server consumes about
205 Watts. In the above experiment, a server on average
spends 48 percent of the time in standby mode due to
green computing. This translates into roughly 62 Watts
power-saving per server or 1,860 Watts for the group of
30 servers used in the experiment.

6.2 Impact of Live Migration
One concern about the use of VM live migration is its
impact on application performance. Previous studies have

1. There is a spike on the number of APMs at the very beginning because
it takes a while to deploy the 240 VMs onto 30 servers.

1114

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 6,

JUNE 2013

Fig. 9. Impact of live migration on TPC-W performance.

found this impact to be small [5]. We investigate this
impact in our own experiment. We extract the data on the
340 live migrations in our 30 server experiment above. We
find that 139 of them are for hot spot mitigation. We focus
on these migrations because that is when the potential
impact on application performance is the most. Among the
139 migrations, we randomly pick seven corresponding
TPC-W sessions undergoing live migration. All
these
sessions run the “shopping mix” workload with 200 emu-
lated browsers. As a target for comparison, we rerun the
session with the same parameters but perform no migra-
tion and use the resulting performance as the baseline.
Fig. 9 shows the normalized Web interactions per second
(WIPS) for the 7 sessions. WIPS is the performance metric
used by TPC-W. The figure shows that most live migration
sessions exhibit no noticeable degradation in performance
compared to the baseline: the normalized WIPS is close to
1. The only exception is session 3 whose degraded
performance is caused by an extremely busy server in
the original experiment.
Next we take a closer look at one of the sessions in Fig. 9
and show how its performance vary over time in Fig. 10.
The dots in the figure show the WIPS every second. The two
curves show the moving average over a 30 second window
as computed by TPC-W. We marked in the figure when live
migration starts and finishes. With self-ballooning enabled,
the amount of memory transferred during the migration is
about 600 MB. The figure verifies that live migration causes
no noticeable performance degradation. The duration of the
migration is under 10 seconds. Recall that our algorithm is
invoked every 10 minutes.

Fig. 10. TPC-W performance with and without live migration.

6.3 Resource Balance
Recall that the goal of the skewness algorithm is to mix
workloads with different resource requirements together so
that the overall utilization of server capacity is improved. In
this experiment, we see how our algorithm handles a mix of
CPU, memory, and network intensive workloads. We vary
the CPU load as before. We inject the network load by
sending the VMs a series of network packets. The memory
intensive applications are created by allocating memory on
demand. Again we start with a small scale experiment
consisting of two PMs and four VMs so that we can present
the results for all servers in Fig. 11. The two rows represent
the two PMs. The two columns represent the CPU and
network dimensions, respectively. The memory consump-
tion is kept low for this experiment.
Initially, the two VMs on PM1 are CPU intensive while
the two VMs on PM2 are network intensive. We increase the
load of
their bottleneck resources gradually. Around
500 seconds, VM4 is migrated from PM2 to PM1 due to
the network overload in PM2 . Then around 600 seconds,
VM1 is migrated from PM1 to PM2 due to the CPU overload
in PM1 . Now the system reaches a stable state with
a balanced resource utilization for both PMs—each with a
CPU intensive VM and a network intensive VM. Later we
decrease the load of all VMs gradually so that both PMs
become cold spots. We can see that the two VMs on PM1 are
consolidated to PM2 by green computing.

Fig. 11. Resource balance for mixed workloads.

XIAO ET AL.: DYNAMIC RESOURCE ALLOCATION USING VIRTUAL MACHINES FOR CLOUD COMPUTING ENVIRONMENT

1115

[20]. Our work also belongs to this category. Sandpiper
combines multidimensional load information into a single
Volume metric [8]. It sorts the list of PMs based on their
volumes and the VMs in each PM in their volume-to-size
ratio (VSR). This unfortunately abstracts away critical
information needed when making the migration decision.
It then considers the PMs and the VMs in the presorted
order. We give a concrete example in Section 1 of the
supplementary file, which is available online, where their
algorithm selects the wrong VM to migrate away during
overload and fails to mitigate the hot spot. We also
compare our algorithm and theirs in real experiment. The
results are analyzed in Section 5 of the supplementary file,
which is available online,
to show how they behave
differently. In addition, their work has no support for
green computing and differs from ours in many other
aspects such as load prediction.
The HARMONY system applies virtualization technol-
ogy across multiple resource layers [20]. It uses VM and data
migration to mitigate hot spots not just on the servers, but
also on network devices and the storage nodes as well. It
introduces the Extended Vector Product (EVP) as an indicator
of imbalance in resource utilization. Their load balancing
algorithm is a variant of the Toyoda method [21] for
multidimensional knapsack problem. Unlike our system,
their system does not support green computing and load
prediction is left as future work. In Section 6 of the
supplementary file, which is available online, we analyze
the phenomenon that V ectorDot behaves differently com-
pared with our work and point out the reason why our
algorithm can utilize residual resources better.
Dynamic placement of virtual servers to minimize SLA
violations is studied in [12]. They model it as a bin packing
problem and use the well-known first-fit approximation
algorithm to calculate the VM to PM layout periodically.
That algorithm, however, is designed mostly for offline use.
It is likely to incur a large number of migrations when
applied in online environment where the resource needs of
VMs change dynamically.

7.3 Green Computing
Many efforts have been made to curtail energy consumption
in data centers. Hardware-based approaches include novel
thermal design for lower cooling power, or adopting power-
proportional and low-power hardware. Work [22] uses
dynamic voltage and frequency scaling (DVFS) to adjust
CPU power according to its load. We do not use DVFS for
green computing, as explained in Section 7 of the supple-
mentary file. PowerNap [23] resorts to new hardware
technologies such as solid state disk (SSD) and Self-Refresh
DRAM to implement rapid transition(less than 1ms) between
full operation and low power state, so that it can “take a nap”
in short idle intervals. When a server goes to sleep,
Somniloquy [24] notifies an embedded system residing on a
special designed NIC to delegate the main operating system.
It gives the illusion that the server is always active.
Our work belongs to the category of pure-software low-
cost solutions [10], [12], [14], [25], [26], [27]. Similar to
Somniloquy [24], SleepServer [26] initiates virtual machines
on a dedicated server as delegate, instead of depending on a
special NIC. LiteGreen [25] does not use a delegate. Instead it
migrates the desktop OS away so that the desktop can sleep. It
requires that the desktop is virtualized with shared storage.

Fig. 12. VM distribution over time.

Next we extend the scale of the experiment to a group of
72 VMs running over 8 PMs. Half of the VMs are CPU
intensive, while the other half are memory intensive.
Initially, we keep the load of all VMs low and deploy all
CPU intensive VMs on PM4 and PM5 while all memory
intensive VMs on PM6 and PM7 . Then we increase the load
on all VMs gradually to make the underlying PMs hot
spots. Fig. 12 shows how the algorithm spreads the VMs to
other PMs over time. As we can see from the figure, the
algorithm balances the two types of VMs appropriately. The
figure also shows that the load across the set of PMs
becomes well balanced as we increase the load.

7 RELATED WORK
7.1 Resource Allocation at the Application Level
Automatic scaling of Web applications was previously
studied in [14] and [15] for data center environments. In
MUSE [14], each server has replicas of all web applications
running in the system. The dispatch algorithm in a frontend
L7-switch makes sure requests are reasonably served while
minimizing the number of underutilized servers. Work [15]
uses network flow algorithms to allocate the load of an
application among its running instances. For connection
oriented Internet services like Windows Live Messenger,
work [10] presents an integrated approach for load
dispatching and server provisioning. All works above do
not use virtual machines and require the applications be
structured in a multitier architecture with load balancing
provided through an front-end dispatcher. In contrast, our
work targets Amazon EC2-style environment where it
places no restriction on what and how applications are
constructed inside the VMs. A VM is treated like a
blackbox. Resource management
is done only at
the
granularity of whole VMs.
MapReduce [16] is another type of popular Cloud service
where data locality is the key to its performance. Quincy
[17] adopts min-cost flow model
in task scheduling to
maximize data locality while keeping fairness among
different jobs. The “Delay Scheduling” algorithm [18] trades
execution time for data locality. Work [19] assign dynamic
priorities to jobs and users to facilitate resource allocation.

7.2 Resource Allocation by Live VM Migration
VM live migration is a widely used technique for dynamic
resource allocation in a virtualized environment [8], [12],

1116

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 6,

JUNE 2013

[14]

[12] N. Bobroff, A. Kochut, and K. Beaty, “Dynamic Placement of
Virtual Machines for Managing SLA Violations,” Proc. IFIP/IEEE
Int’l Symp. Integrated Network Management (IM ’07), 2007.
[13] “TPC-W: Transaction Processing Performance Council,” http://
www.tpc.org/tpcw/, 2012.
J.S. Chase, D.C. Anderson, P.N. Thakar, A.M. Vahdat, and R.P.
Doyle, “Managing Energy and Server Resources in Hosting
Centers,” Proc. ACM Symp. Operating System Principles (SOSP ’01),
Oct. 2001.
[15] C. Tang, M. Steinder, M. Spreitzer, and G. Pacifici, “A Scalable
Application Placement Controller for Enterprise Data Centers,”
Proc. Int’l World Wide Web Conf. (WWW ’07), May 2007.
[16] M. Zaharia, A. Konwinski, A.D. Joseph, R.H. Katz, and I. Stoica,
“Improving MapReduce Performance in Heterogeneous Environ-
ments,” Proc. Symp. Operating Systems Design and Implementation
(OSDI ’08), 2008.
[17] M. Isard, V. Prabhakaran, J. Currey, U. Wieder, K. Talwar, and A.
Goldberg, “Quincy: Fair Scheduling for Distributed Computing
Clusters,” Proc. ACM Symp. Operating System Principles (SOSP ’09),
Oct. 2009.
[18] M. Zaharia, D. Borthakur, J. Sen Sarma, K. Elmeleegy, S. Shenker,
and I. Stoica, “Delay Scheduling: A Simple Technique for
Achieving Locality and Fairness in Cluster Scheduling,” Proc.
European Conf. Computer Systems (EuroSys ’10), 2010.
[19] T. Sandholm and K. Lai, “Mapreduce Optimization Using
Regulated Dynamic Prioritization,” Proc. Int’l Joint Conf. Measure-
ment and Modeling of Computer Systems (SIGMETRICS ’09), 2009.
[20] A. Singh, M. Korupolu, and D. Mohapatra, “Server-Storage
Virtualization: Integration and Load Balancing in Data Centers,”
Proc. ACM/IEEE Conf. Supercomputing, 2008.
[21] Y. Toyoda, “A Simplified Algorithm for Obtaining Approximate
Solutions to Zero-One Programming Problems,” Management
Science, vol. 21, pp. 1417-1427, Aug. 1975.
[22] R. Nathuji and K. Schwan, “Virtualpower: Coordinated Power
Management
in Virtualized Enterprise Systems,” Proc. ACM
SIGOPS Symp. Operating Systems Principles (SOSP ’07), 2007.
[23] D. Meisner, B.T. Gold, and T.F. Wenisch, “Powernap: Eliminating
Server Idle Power,” Proc. Int’l Conf. Architectural Support for
Programming Languages and Operating Systems (ASPLOS ’09), 2009.
[24] Y. Agarwal, S. Hodges, R. Chandra, J. Scott, P. Bahl, and R. Gupta,
“Somniloquy: Augmenting Network Interfaces to Reduce Pc
Energy Usage,” Proc. USENIX Symp. Networked Systems Design
and Implementation (NSDI ’09), 2009.
[25] T. Das, P. Padala, V.N. Padmanabhan, R. Ramjee, and K.G. Shin,
“Litegreen: Saving Energy in Networked Desktops Using Virtua-
lization,” Proc. USENIX Ann. Technical Conf., 2010.
[26] Y. Agarwal, S. Savage, and R. Gupta, “Sleepserver: A Software-
Only Approach for Reducing the Energy Consumption of PCS
within Enterprise Environments,” Proc. USENIX Ann. Technical
Conf., 2010.
[27] N. Bila, E.d. Lara, K. Joshi, H.A. Lagar-Cavilla, M. Hiltunen, and
M. Satyanarayanan, “Jettison: Efficient Idle Desktop Consolida-
tion with Partial VM Migration,” Proc. ACM European Conf.
Computer Systems (EuroSys ’12), 2012.

Zhen Xiao is a professor in the Department of
Computer Science at Peking University. He
received the PhD degree from Cornell University
in January 2001. After that, he worked as a senior
technical staff member at AT&T Labs, New
Jersey and then as a research staff member at
IBM T.J. Watson Research Center. His research
interests include cloud computing, virtualization,
and various distributed systems issues. He is a
senior member of the IEEE and ACM.

Jettison [27] invents “partial VM migration,” a variance of live
VM migration, which only migrates away necessary working
set while leaving infrequently used data behind.

8 CONCLUSION

We have presented the design, implementation, and evalua-
tion of a resource management system for cloud computing
services. Our system multiplexes virtual to physical re-
sources adaptively based on the changing demand. We use
the skewness metric to combine VMs with different resource
characteristics appropriately so that the capacities of servers
are well utilized. Our algorithm achieves both overload
avoidance and green computing for systems with multi-
resource constraints.

ACKNOWLEDGMENTS

The authors would like to thank the anonymous reviewers for
their invaluable feedback. This work was supported by the
National Natural Science Foundation of China (Grant No.
61170056), the National High Technology Research and
Development Program (“863” Program) of China (Grant No.
2013AA013203), National Basic Research Program of China
(Grant No. 2009CB320505) and Digital Resource Security
Protection Service Based on Trusted Identity Federation and
Cloud Computation SubProject of 2011 Information Security
Special Project sponsored by National Development and
Reform Commission.

[4]

[3]

REFERENCES
[1] M. Armbrust et al., “Above the Clouds: A Berkeley View of Cloud
Computing,” technical report, Univ. of California, Berkeley, Feb.
2009.
[2] L. Siegele, “Let It Rise: A Special Report on Corporate IT,” The
Economist, vol. 389, pp. 3-16, Oct. 2008.
P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, R.
Neugebauer, I. Pratt, and A. Warfield, “Xen and the Art of
Virtualization,” Proc. ACM Symp. Operating Systems Principles
(SOSP ’03), Oct. 2003.
“Amazon elastic compute cloud (Amazon EC2),” http://aws.
amazon.com/ec2/, 2012.
[5] C. Clark, K. Fraser, S. Hand, J.G. Hansen, E. Jul, C. Limpach, I.
Pratt, and A. Warfield, “Live Migration of Virtual Machines,”
Proc . Symp . Ne two rk ed Sys t ems Des ign and Imp lemen ta t ion
(NSDI ’05), May 2005.
[6] M. Nelson, B.-H. Lim, and G. Hutchins, “Fast Transparent
Migration for Virtual Machines,” Proc. USENIX Ann. Technical
Conf., 2005.
[7] M. McNett, D. Gupta, A. Vahdat, and G.M. Voelker, “Usher: An
Extensible Framework for Managing Clusters of Virtual Ma-
chines,” Proc. Large Installation System Administration Conf.
(LISA ’07), Nov. 2007.
[8] T. Wood, P. Shenoy, A. Venkataramani, and M. Yousif, “Black-Box
and Gray-Box Strategies for Virtual Machine Migration,” Proc.
Symp. Networked Systems Design and Implementation (NSDI ’07),
Apr. 2007.
[9] C.A. Waldspurger, “Memory Resource Management in VMware
ESX Server,” Proc. Symp. Operating Systems Design and Implementa-
tion (OSDI ’02), Aug. 2002.
[10] G. Chen, H. Wenbo, J. Liu, S. Nath, L. Rigas, L. Xiao, and F. Zhao,
“Energy-Aware Server Provisioning and Load Dispatching for
Connection-Intensive Internet Services,” Proc. USENIX Symp.
Networked Systems Design and Implementation (NSDI ’08), Apr. 2008.
[11] P. Padala, K.-Y. Hou, K.G. Shin, X. Zhu, M. Uysal, Z. Wang, S.
Singhal, and A. Merchant, “Automated Control of Multiple
Virtualized Resources,” Proc. ACM European conf. Computer
Systems (EuroSys ’09), 2009.

XIAO ET AL.: DYNAMIC RESOURCE ALLOCATION USING VIRTUAL MACHINES FOR CLOUD COMPUTING ENVIRONMENT

1117

Wei j ia Song rece ived the bache lor ’s and
master’s degrees from Beijing Institute of Tech-
nology. Currently, he is a doctoral student at
Peking University. His current research focuses
on resource schedu l ing prob lems in c loud
systems.

Qi Chen received the bachelor’s degree from
Peking University in 2010. Currently, she is a
doctoral student at Pek ing Un ivers ity. Her
current research focuses on the cloud comput-
ing and parallel computing.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

1182

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 6,

JUNE 2013

Mona: Secure Multi-Owner Data Sharing
for Dynamic Groups in the Cloud

Xuefeng L iu, Yuq ing Zhang, Member, IEEE, Boyang Wang, and J ingbo Yan

Abstract—With the character of low maintenance, cloud computing provides an economical and efficient solution for sharing group
resource among cloud users. Unfortunately, sharing data in a multi-owner manner while preserving data and identity privacy from an
untrusted cloud is still a challenging issue, due to the frequent change of the membership. In this paper, we propose a secure multi-
owner data sharing scheme, named Mona, for dynamic groups in the cloud. By leveraging group signature and dynamic broadcast
encryption techniques, any cloud user can anonymously share data with others. Meanwhile, the storage overhead and encryption
computation cost of our scheme are independent with the number of revoked users. In addition, we analyze the security of our scheme
with rigorous proofs, and demonstrate the efficiency of our scheme in experiments.

Index Terms—Cloud computing, data sharing, privacy-preserving, access control, dynamic groups

Ç

1 INTRODUCTION
C LOUD computing is recognized as an alternative to
traditional
information technology [1] due to its
intrinsic resource-sharing and low-maintenance character-
istics. In cloud computing, the cloud service providers
(CSPs), such as Amazon, are able to deliver various services
to cloud users with the help of powerful datacenters. By
migrating the local data management systems into cloud
servers, users can enjoy high-quality services and save
significant investments on their local infrastructures.
One of the most fundamental services offered by cloud
providers is data storage. Let us consider a practical data
application. A company allows its staffs in the same group
or department to store and share files in the cloud. By
utilizing the cloud, the staffs can be completely released
from the troublesome local data storage and maintenance.
However, it also poses a significant risk to the confidenti-
ality of those stored files. Specifically, the cloud servers
managed by cloud providers are not fully trusted by users
while the data files stored in the cloud may be sensitive and
confidential, such as business plans. To preserve data
privacy, a basic solution is to encrypt data files, and then
upload the encrypted data into the cloud [2]. Unfortunately,
designing an efficient and secure data sharing scheme for
groups in the cloud is not an easy task due to the following
challenging issues.

. X. Liu, B. Wang, and J. Yan are with the National Key Laboratory of
Integrated Services Networks, Xidian University, No. 2, Taibai Road, Xı´an
city 710071, Shaanxi province, China.
E-mail: {liuxf, bywang, yanjb}@mail.xidian.edu.cn, yanjb@nipc.org.cn.
. Y. Zhang is with the National Computer Network Intrusion Protection
Center, Graduate University of Chinese Academy of Sciences, No. 19,
Yuquan Road, Beijing 100049, China.
E-mail: Zhangyq@gucas.ac.cn, zhangyq@ucas.ac.cn.

Manuscript received 29 Feb. 2012; revised 1 Oct. 2012; accepted 22 Nov.
2012; published online 4 Dec. 2012.
Recommended for acceptance by V.B. Misic, R. Buyya, D. Milojicic, and
Y. Cui.
For information on obtaining reprints of this article, please send e-mail to:
tpds@computer.org, and reference IEEECS Log Number
TPDSSI-2012-02-0167.
Digital Object Identifier no. 10.1109/TPDS.2012.331.

identity privacy is one of the most significant
First,
obstacles for the wide deployment of cloud computing.
Without the guarantee of identity privacy, users may be
unwilling to join in cloud computing systems because their
real identities could be easily disclosed to cloud providers
and attackers. On the other hand, unconditional identity
privacy may incur the abuse of privacy. For example, a
misbehaved staff can deceive others in the company by
sharing false files without being traceable. Therefore,
traceability, which enables the group manager (e.g., a
company manager) to reveal the real identity of a user, is
also highly desirable.
Second, it is highly recommended that any member in a
group should be able to fully enjoy the data storing and
sharing services provided by the cloud, which is defined as
the multiple-owner manner. Compared with the single-owner
manner [3], where only the group manager can store and
modify data in the cloud, the multiple-owner manner is more
flexible in practical applications. More concretely, each user
in the group is able to not only read data, but also modify his/
her part of data in the entire data file shared by the company.
Last but not least, groups are normally dynamic in
practice, e.g., new staff participation and current employee
revocation in a company. The changes of membership make
secure data sharing extremely difficult. On one hand, the
anonymous system challenges new granted users to learn
the content of data files stored before their participation,
because it is impossible for new granted users to contact
with anonymous data owners, and obtain the correspond-
ing decryption keys. On the other hand, an efficient
membership revocation mechanism without updating the
secret keys of
the remaining users is also desired to
minimize the complexity of key management.
Several security schemes for data sharing on untrusted
servers have been proposed [4], [5], [6]. In these approaches,
data owners store the encrypted data files in untrusted
storage and distribute the corresponding decryption keys
only to authorized users. Thus, unauthorized users as well
as storage servers cannot learn the content of the data files
because they have no knowledge of the decryption keys.

1045-9219/13/$31.00 ß 2013 IEEE

Published by the IEEE Computer Society

LIU ET AL.: MONA: SECURE MULTI-OWNER DATA SHARING FOR DYNAMIC GROUPS IN THE CLOUD

1183

However, the complexities of user participation and revoca-
tion in these schemes are linearly increasing with the
number of data owners and the number of revoked users,
respectively. By setting a group with a single attribute, Lu et
al. [7] proposed a secure provenance scheme based on the
ciphertext-policy attribute-based encryption technique [8],
which allows any member in a group to share data with
others. However,
the issue of user revocation is not
addressed in their scheme. Yu et al. [3] presented a scalable
and fine-grained data access control scheme in cloud
computing based on the key policy attribute-based encryp-
tion (KP-ABE) technique [9]. Unfortunately, the single-
owner manner hinders the adoption of their scheme into the
case, where any user is granted to store and share data.
Our contributions. To solve the challenges presented
above, we propose Mona, a secure multi-owner data
sharing scheme for dynamic groups in the cloud. The main
contributions of this paper include:

1. We propose a secure multi-owner data sharing
scheme. It implies that any user in the group can
securely share data with others by the untrusted
cloud.
2. Our proposed scheme is able to support dynamic
groups efficiently. Specifically, new granted users
can directly decrypt data files uploaded before their
participation without contacting with data owners.
User revocation can be easily achieved through a
novel revocation list without updating the secret
keys of the remaining users. The size and computa-
tion overhead of encryption are constant and
independent with the number of revoked users.
3. We provide secure and privacy-preserving access
control to users, which guarantees any member in a
group to anonymously utilize the cloud resource.
Moreover, the real identities of data owners can be
revealed by the group manager when disputes occur.
4. We provide rigorous security analysis, and per-
form extensive simulations to demonstrate the
efficiency of our scheme in terms of storage and
computation overhead.
The remainder of this paper is organized as follows:
Section 2 overviews the related work. In Section 3, some
preliminaries and cryptographic primitives are reviewed. In
Section 4, we describe the system model and our design
goals. In Section 5, the proposed scheme is presented in
detail, followed by the security analysis and the perfor-
mance analysis in Sections 6 and 7. Finally, we conclude the
paper in Section 8.

2 RELATED WORK

In [4], Kallahalla et al. proposed a cryptographic storage
system that enables secure file sharing on untrusted servers,
named Plutus. By dividing files into filegroups and encrypt-
ing each filegroup with a unique file-block key, the data
owner can share the filegroups with others through deliver-
ing the corresponding lockbox key, where the lockbox key is
used to encrypt the file-block keys. However, it brings about
a heavy key distribution overhead for large-scale file sharing.
Additionally, the file-block key needs to be updated and
distributed again for a user revocation.

In [5], files stored on the untrusted server include two
parts: file metadata and file data. The file metadata implies
the access control
information including a series of
encrypted key blocks, each of which is encrypted under
the public key of authorized users. Thus, the size of the file
metadata is proportional to the number of authorized users.
The user revocation in the scheme is an intractable issue
especially for large-scale sharing, since the file metadata
needs to be updated. In their extension version, the NNL
construction [10]
is used for efficient key revocation.
However, when a new user joins the group, the private
key of each user in an NNL system needs to be recomputed,
the application for dynamic groups.
which may limit
Another concern is that
the computation overhead of
encryption linearly increases with the sharing scale.
Ateniese et al. [6] leveraged proxy reencryptions to
secure distributed storage. Specifically,
the data owner
encrypts blocks of content with unique and symmetric
content keys, which are further encrypted under a master
public key. For access control,
the server uses proxy
cryptography to directly reencrypt the appropriate content
key(s) from the master public key to a granted user’s public
key. Unfortunately, a collusion attack between the un-
trusted server and any revoked malicious user can be
launched, which enables them to learn the decryption keys
of all the encrypted blocks.
In [3], Yu et al. presented a scalable and fine-grained data
access control scheme in cloud computing based on the KP-
ABE technique. The data owner uses a random key to
encrypt a file, where the random key is further encrypted
with a set of attributes using KP-ABE. Then, the group
manager assigns an access structure and the corresponding
secret key to authorized users, such that a user can only
decrypt a ciphertext if and only if the data file attributes
satisfy the access structure. To achieve user revocation, the
manager delegates tasks of data file reencryption and user
secret key update to cloud servers. However, the single-
owner manner may hinder the implementation of applica-
tions with the scenario, where any member in a group
should be allowed to store and share data files with others.
Lu et al. [7] proposed a secure provenance scheme,
which is built upon group signatures and ciphertext-policy
attribute-based encryption techniques. Particularly,
the
system in their scheme is set with a single attribute. Each
user obtains two keys after the registration: a group
signature key and an attribute key. Thus, any user is able
to encrypt a data file using attribute-based encryption and
others in the group can decrypt the encrypted data using
their attribute keys. Meanwhile, the user signs encrypted
data with her group signature key for privacy preserving
and traceability. However, user revocation is not supported
in their scheme.
From the above analysis, we can observe that how to
securely share data files in a multiple-owner manner for
dynamic groups while preserving identity privacy from an
untrusted cloud remains to be a challenging issue. In this
paper, we propose a novel Mona protocol for secure data
sharing in cloud computing. Compared with the existing
works, Mona offers unique features as follows:

1. Any user in the group can store and share data files
with others by the cloud.

1184

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 6,

JUNE 2013

2. The encryption complexity and size of ciphertexts
are independent with the number of revoked users
in the system.
3. User revocation can be achieved without updating
the private keys of the remaining users.
4. A new user can directly decrypt the files stored in
the cloud before his participation.

3 PRELIMINARIES
3.1 Bilinear Maps
Let G1 and G2 be an additive cyclic group and a
multiplicative cyclic group of the same prime order q,
respectively [11]. Let e : G1  G1 ! G2 denote a bilinear
map constructed with the following properties:
q and P ; Q 2 G1 , eðaP ; bQÞ ¼
1. Bilinear: For all a; b 2 Z 
eðP ; QÞab .
2. Nondegenerate: There exists a point P such that
eðP ; P Þ 6¼ 1.
3. Computable: There is an efficient algorithm to
compute eðP ; QÞ for any P ; Q 2 G1 .

3.2 Complexity Assumptions
Definition 1 (q-strong Diffie-Hellman (q-SDH) Assump-
tion [12]). Given ðP1 ; P2 ; P2 ;  2P2 ; . . . ;  q P2 Þ, it is infeasible
þx P1 , where x 2 Z 
1
q .
to compute
Definition 2 (Decision linear (DL) Assumption [12]). Given
P1 ; P2 ; P3 ; aP1 ; bP2 ; cP3 , it is infeasible to decide whether a þ
b ¼ c mod q.
Definition 3 (Weak Bilinear Diffie-Hellman Exponent
(WBDHE) Assumption [13]). For unknown a 2 Z 
q , given
Y ; aY ; a2Y ; ::; alY ; P 2 G1 , it is infeasible to compute eðY ; P Þ1
a .
Definition 4 ((t,n)-general Diffie-Hellman Exponent
i¼1 ðX þ xi Þ and
(GDHE) Assumption [14]). Let f ðXÞ ¼ r
i¼1 ðX þ x
i Þ be the two random univariate poly-
gðXÞ ¼ n r
0
nomials. For unknown k;  2 Z 
q , given
G0 ; G0 ; :::;  t 1G0 ; f ð ÞG0 ; P0 ; :::;  t 1P0 ; kgð ÞH0 2 G1 and
eðG0 ; H0 Þf 2 ð ÞgðÞ 2 G2 ;
it is infeasible to compute eðG0 ; H0 Þkf ð Þgð Þ 2 G2 .

3.3 Group Signature
The concept of group signatures was first introduced in [15]
by Chaum and van Heyst. In general, a group signature
scheme allows any member of the group to sign messages
while keeping the identity secret from verifiers. Besides, the
designated group manager can reveal the identity of the
signature’s originator when a dispute occurs, which is
denoted as traceability. In this paper, a variant of the short
group signature scheme [12] will be used to achieve
anonymous access control, as it supports efficient member-
ship revocation.

3.4 Dynamic Broadcast Encryption
Broadcast encryption [16] enables a broadcaster
to
transmit encrypted data to a set of users so that only a
privileged subset of users can decrypt the data. Besides
the above characteristics, dynamic broadcast encryption
also allows the group manager to dynamically include
new members while preserving previously computed

Fig. 1. System model.

information,
i.e., user decryption keys need not be
recomputed, the morphology and size of ciphertexts are
unchanged and the group encryption key requires no
modification. The first formal definition and construction
of dynamic broadcast encryption are introduced based on
the bilinear pairing technique in [14], which will be used
as the basis for file sharing in dynamic groups.

4 SYSTEM MODEL AND DESIGN GOALS
4.1 System Model
We consider a cloud computing architecture by combining
with an example that a company uses a cloud to enable
its staffs in the same group or department to share files.
The system model consists of three different entities: the
cloud, a group manager (i.e., the company manager), and
a large number of group members (i.e.,
the staffs) as
illustrated in Fig. 1.
Cloud is operated by CSPs and provides priced abundant
storage services. However, the cloud is not fully trusted by
users since the CSPs are very likely to be outside of the
cloud users’ trusted domain. Similar to [3], [7], we assume
that the cloud server is honest but curious. That is, the cloud
server will not maliciously delete or modify user data due
to the protection of data auditing schemes [17], [18], but will
try to learn the content of the stored data and the identities
of cloud users.
Group manager takes charge of system parameters
generation, user registration, user revocation, and revealing
the real identity of a dispute data owner. In the given
example, the group manager is acted by the administrator
of the company. Therefore, we assume that the group
manager is fully trusted by the other parties.
Group members are a set of registered users that will store
their private data into the cloud server and share them with
others in the group. In our example, the staffs play the role
of group members. Note that, the group membership is
dynamically changed, due to the staff resignation and new
employee participation in the company.

4.2 Design Goals
In this section, we describe the main design goals of the
proposed scheme including access control, data confidenti-
ality, anonymity and traceability, and efficiency as follows:
Access control: The requirement of access control is twofold.
First, group members are able to use the cloud resource for
data operations. Second, unauthorized users cannot access
the cloud resource at any time, and revoked users will be
incapable of using the cloud again once they are revoked.
Data confidentiality: Data confidentiality requires that
unauthorized users including the cloud are incapable of

LIU ET AL.: MONA: SECURE MULTI-OWNER DATA SHARING FOR DYNAMIC GROUPS IN THE CLOUD

1185

learning the content of the stored data. An important and
challenging issue for data confidentiality is to maintain its
availability for dynamic groups. Specifically, new users
should decrypt the data stored in the cloud before their
participation, and revoked users are unable to decrypt the
data moved into the cloud after the revocation.
Anonymity and traceability: Anonymity guarantees that
group members can access the cloud without revealing the
real identity. Although anonymity represents an effective
protection for user identity, it also poses a potential inside
attack risk to the system. For example, an inside attacker
may store and share a mendacious information to derive
substantial benefit. Thus, to tackle the inside attack, the
group manager should have the ability to reveal the real
identities of data owners.
Efficiency: The efficiency is defined as follows: Any group
member can store and share data files with others in the
group by the cloud . User revocation can be achieved
without
involving the remaining users. That
is,
the
remaining users do not need to update their private keys
or reencryption operations. New granted users can learn all
the content data files stored before his participation without
contacting with the data owner.

5 THE PROPOSED SCHEME: MONA
5.1 Overview
To achieve secure data sharing for dynamic groups in the
cloud, we expect to combine the group signature and
dynamic broadcast encryption techniques. Specially, the
group signature scheme enables users to anonymously use
the cloud resources, and the dynamic broadcast encryption
technique allows data owners to securely share their data
files with others including new joining users.
Unfortunately, each user has to compute revocation
parameters to protect the confidentiality from the revoked
users in the dynamic broadcast encryption scheme, which
results in that both the computation overhead of
the
encryption and the size of the ciphertext increase with the
number of revoked users. Thus, the heavy overhead and
large ciphertext size may hinder the adoption of
the
broadcast encryption scheme to capacity-limited users.
To tackle this challenging issue, we let
the group
manager compute the revocation parameters and make
the result public available by migrating them into the cloud.
Such a design can significantly reduce the computation
overhead of users to encrypt files and the ciphertext size.
Specially, the computation overhead of users for encryption
operations and the ciphertext size are constant and
independent of the revocation users.

5.2 Scheme Description
This section describes the details of Mona including system
initialization, user registration, user revocation, file genera-
tion, file deletion, file access and traceability.

5.2.1 System Initialization
The group manager takes charge of system initialization
as follows:
. Generating a bilinear map group system S ¼ ðq; G1 ;
G2 ; eð; ÞÞ.

TABLE 1
Revocation List

.

.

.

Selecting two random elements H ; H0 2 G1 along
with two random numbers 1 ; 2 2 Z 
q , and comput-
1 H a n d V ¼  1
2 H 2 G1 s u c h t h a t
i n g U ¼  1
1  U ¼ 2  V ¼ H . In addition, the group manager
computes H1 ¼ 1H0 and H2 ¼ 2H0 2 G1 .
Randomly choosing two elements P ; G 2 G1 and a
q , and computing W ¼   P , Y ¼   G
number  2 Z 
and Z ¼ eðG; P Þ, respectively.
Publishing the system parameters including ðS ; P ;
H ; H0 ; H1 ; H2 ; U ; V ; W ; Y ; Z ; f ; f1 ; EncðÞÞ, where f is
a one-way hash function: f0; 1g ! Z 
q ; f1 is hash
func t ion : f0; 1g ! G1 ; and Enck ðÞ is a secure
symmetric encryption algorithm with secret key k.
In the end, the parameter ð ; 1 ; 2 ; GÞ will be kept secret
as the master key of the group manager.
5.2.2 User Registration
For the registration of user i with identity IDi , the group
8><>:
manager randomly selects a number xi 2 Z 
q and computes
Ai ; Bi as the following equation:
 P 2 G1
Ai ¼ 1
 þ xi
 G 2 G1 :
Bi ¼ xi
 þ xi
Then, the group manager adds ðAi ; xi ; IDi Þ into the
group user list, which will be used in the traceability phase.
After the reg istrat ion , user i obtains a pr ivate key
ðxi ; Ai ; Bi Þ, which will be used for group signature genera-
tion and file decryption.

ð1Þ

5.2.3 User Revocation
User revocation is performed by the group manager via a
public available revocation list ðRLÞ, based on which group
members can encrypt
their data files and ensure the
confidentiality against the revoked users. As illustrated in
Table 1, the revocation list is characterized by a series of
time stamps (t1 < t2 <; :::; tr ). Let IDgroup denote the group
identity. The tuple ðAi ; xi ; ti Þ represents that user i with the
partial private key (Ai ; xi ) is revoked at time ti . P1 ; P2 ; :::; Pr
8>>>>>>>>>>><>>>>>>>>>>>:
and Zr are calculated by the group manager with the
private secret  as follows:
P1 ¼ 1
 P 2 G1
 þ x1
ð þ x1 Þð þ x2 Þ  P 2 G1
P2 ¼
1

ð þ x1 Þð þ x2 Þ    ð þ xr Þ  P 2 G1
1
1
ð þ x1 Þð þ x2 Þ    ð þ xr Þ 2 G2 :
Zr ¼ Z
Motivated by the verifiable reply mechanism in [19], to
guarantee that users obtain the latest version of
the

Pr ¼

ð2Þ

1186

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 6,

JUNE 2013

revocation list, we let
the group manger update the
revocation list each day even no user has being revoked in
the day. In other words, the others can verify the freshness
of the revocation list from the contained current date tRL . In
addition, the revocation list is bounded by a signature
sigðRLÞ to declare its validity. The signature is generated by
the group manager with the BLS signature algorithm [20],
i.e., sigðRLÞ ¼ f1 ðRLÞ. Finally, the group manager mi-
grates the revocation list into the cloud for public usage.

5.2.4 File Generation
To store and share a data file in the cloud, a group member
performs the following operations:

1. Getting the revocation list from the cloud. In this
step, the member sends the group identity IDgroup as
a request to the cloud. Then, the cloud responds the
revocation list RL to the member.
2. Verifying the validity of the received revocation list.
First, checking whether the marked date is fresh.
Second, verifying the contained signature sigðRLÞ by
the equation eðW ; f1 ðRLÞÞ ¼ eðP ; sigðRLÞÞ.
If
the
revocation list is invalid, the data owner stops this
scheme.
3. Encrypting the data file M . This encryption process
can be divided into two cases according to the
revocation list.

ð3Þ

a. Case 1. There is no revoked user in the revocation
list:
Selecting a unique data file identity IDdata ;
i.
ii. Choosing a random number k 2 Z 
q ;
8>><>>:
iii. Computing the parameters C1 ; C2 ; K ; C as
the following equation:
C1 ¼ k  Y 2 G1
C2 ¼ k  P 2 G1
K ¼ Z k 2 G2
C ¼ EncK ðM Þ:
b. Case 2. There are r revoked users in the revocation
list.
Selecting a unique data file identity IDdata ;
i.
ii. Choosing a random number k 2 Z 
q ;
8>><>>:
iii. Computing the parameters C1 ; C2 ; K ; C as
the following equation:
C1 ¼ k  Y 2 G1
C2 ¼ k  Pr 2 G1
K ¼ Z k
r 2 G2
C ¼ EncK ðM Þ:
In (4), Zr and Pr are directly obtained
from the revocation list.
Selecting a random number  and computing f ð Þ.
The hash value will be used for data file deletion
opera t ion .
In add i t ion ,
the da ta owner adds
ðIDdata ;  Þ into his local storage.
5. Constructing the uploaded data file as shown in
Table 2, where tdata denotes the current time on the

ð4Þ

4.

TABLE 2
Message Format for Uploading Data

member, and  is a group signature on ðIDdata ; C1 ;
C2 ; C ; f ð Þ; tdata Þ compu ted by the da ta owne r
through Algorithm 1 with the private key ðA; xÞ.
6. Uploading the data shown in Table 2 into the cloud
server and adding the IDdata into the local shared
data list maintained by the manager. On receiving the
data, the cloud first invokes Algorithm 2 to check its
validity. If the algorithm returns true, the group
signature is valid; otherwise, the cloud abandons the
data. In addition, if several users have been revoked
by the group manager, the cloud also performs
revocation verification by using Algorithm 3. Finally,
the data file will be stored in the cloud after successful
group signature and revocation verifications.

5.2.5 File Deletion
File stored in the cloud can be deleted by either the group
manager or the data owner (i.e., the member who uploaded
the file into the server). To delete a file IDdata , the group
manager computes a signature f1 ðIDdata Þ and sends the
signature along with IDdata to the cloud. The cloud will
the equation eðf1 ðIDdata Þ; P Þ ¼ eðW ;
delete the file if
f1 ðIDdata ÞÞ holds.
Algorithm (1). Signature Generation
Input: Private key ðA; xÞ, system parameter ðP ; U ; V ; H ; W Þ
and data M .
Output: Generate a valid group signature on M .
begin
Select random numbers ;  ; r ; r ; rx ; r1 ; r2 2 Z 
8>>>>>>>>>><>>>>>>>>>>:
q
Set 1 ¼ x and 2 ¼ x
Computes the following values
T1 ¼   U
T2 ¼   V
T3 ¼ Ai þ ð þ Þ  H
R1 ¼ r  U
R2 ¼ r  V
R3 ¼ eðT3 ; P Þrx eðH ; W Þ r r eðH ; P Þ r1  r2
R4 ¼ rx  T1   r1  U
R5 ¼ rx  T2   r2  V
8>>>><>>>>:
Set c ¼ f ðM ; T1 ; T2 ; T3 ; R1 ; R2 ; R3 ; R4 ; R5 Þ
Construct the following numbers
s ¼ r þ c
s ¼ r þ c
sx ¼ rx þ cx
s1 ¼ r1 þ c1
s2 ¼ r2 þ c2
Return  ¼ ðT1 ; T2 ; T3 ; c; s ; s ; sx ; s1 ; s2 Þ

.

.

end

Algorithm (2). Signature Verification
Input: System parameter ðP ; U ; V ; H ; W Þ, M and a
signature  ¼ ðT1 ; T2 ; T3 ; c; s ; s ; sx ; s1 ; s2 Þ

1187

LIU ET AL.: MONA: SECURE MULTI-OWNER DATA SHARING FOR DYNAMIC GROUPS IN THE CLOUD
8>>>>>>>><>>>>>>>>:
Output: True or False.
begin
Compute the following values
~R1 ¼ s  U   c  T1
~R2 ¼ s  V   c  T2
~R3 ¼ ðeðT3 ; W Þ
eðP ; P Þ Þc eðT3 ; P Þsx eðH ; W Þ s s
eðH ; P Þ s1  s2
f
f
f
f
f
~R4 ¼ sx  T1   s1  U
~R5 ¼ sx  T2   s2  V
if c ¼ f ðM ; T1 ; T2 ; T3 ;
R5 Þ
R4 ;
R3 ;
R2 ;
R1 ;
Return True
else
Return False

revocation list. After a successful verification, the
cloud server responds the corresponding data file
and the revocation list to the user.
2. Checking the validity of the revocation list. This
operation is similar to the step 2 of file generation
phase.
3. Verifying the validity of the file and decrypting it.
The format of the downloaded file coincides with
that given in Table 2. This operation can be divided
into three cases according to the time stamp tdata and
the revocation list. Suppose that there are r revoked
users in the revocation list.
a. Case 1 ðtdata < t1 Þ. This case indicates that there is
no revoked user before the data file is uploaded

.

end

Algorithm (3). Revocation Verification
Input: System parameter ðH0 ; H1 ; H2 Þ, a group signature
, and a set of revocation keys A1 ; :::; Ar
Output: Valid or Invalid.
begin
set temp ¼ eðT1 ; H1 ÞeðT2 ; H2 Þ
for i ¼ 1 to n
if eðT3   Ai ; H0 Þ ¼ temp
Return Valid
end if
end for
Return Invalid

end

In addition, Mona also allows data owners to delete their
files stored in the cloud. Specially, the data owner does the
following actions:
. Obtaining the tuple ðIDdata ;  Þ from his local storage.
.
Invoking Algorithm 1 to compute a group signature
on ðIDdata ;  Þ.
Sending ðIDdata ;  Þ and the signature as a deletion
request to the cloud.
Upon receiving the deletion request, the cloud calls
Algorithms 2 and 3 to verify the group signature. After a
successful group signature verification,
the cloud will
delete the data file if f ð Þ equals to the hash value
contained in the file.

.

5.2.6 File Access
To learn the content of a shared file, a member does the
following actions:

1. Getting the data file and the revocation list from the
cloud server. In this operation, the user first adopts
its private key ðA; xÞ to compute a signature u on
the message ðIDgroup ; IDdata ; tÞ by using Algorithm 1,
where t denote the current time, and the IDdata can
be obtained from the local shared file list maintained
by the manager. Then, the user sends a data request
containing ðIDgroup ; IDdata ; t; u Þ to the cloud server.
Upon rece iving the request ,
the cloud server
employs Algorithm 2 to check the validity of the
signature and performs a revocation verification
with Algorithm 3 if necessary according to the

i.

Invoking Algorithm 2 to check the group
signature . If the algorithm returns false,
the user stops this protocol.
ii. Using his partial private key ðA; BÞ to
compute ^K ¼ eðC1 ; AÞeðC2 ; BÞ.
iii. Decrypting the ciphertext C with the com-
puted key ^K .




Correctness:
^K ¼ eðC1 ; AÞeðC2 ; BÞ
 G
e k  P ;
 P
¼ e k  Y ;
1
 þ x
þx eðP ; GÞ kx
¼ eðG; P Þ k
þx
¼ Z k ¼ K :

x
 þ x

b. Case 2 ðti < tdata < tiþ1 Þ. This case indicates that i
revoked users have been revoked before the
data file is uploaded

ii.

¼

i. Verifying the group signature  by using
Algorithm 2.
Inputting A1 ; A2 ; :::Ai to call Algorithm 3. If
the algorithm returns invalid,
the user
terminates this operation.
Q
iii. Computing the value
Ai;r ¼
1
¼1 ð þ x Þ P
ð þ xÞ
i
by using Algorithm 4 with the input ðA; xÞ;
 
!
ðP1 ; x1 Þ; :::; ðPi ; xi Þ. The correctness of Ai;r is
Q
due to the following relation:
!
 
Pi  
1
Q
¼1 ð þ x Þ P
i 1
ð þ xÞ
ð þ xÞ   ð þ xi Þ
Q
P
i 1
ð þ xÞð þ xi Þð
¼1 ð þ x ÞÞ
1
¼1 ð þ x Þ P :
i
iv. Calculating the decryption key ^K ¼ eðC1 ;
Ai;r ÞeðC2 ; BÞ.
v. Decrypt the ciphertext C with the key ^K .

¼ 1
x   xi

1
x   xi

ð þ xÞ

1188

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 6,
!

Theorem 1. Based on the group signature technique,
proposed scheme can achieve efficient access control.

JUNE 2013

the

Proof. To access the cloud, a user needs to compute a group
signature for his/her authentication. The employed
group signature scheme can be regarded as a variant of
the short group signature [12], which inherits the inherent
unforgeability property, anonymous authentication, and
tracking capability. The demonstration of Theorem 1 can
tu
be derived from the following three lemmas:

Lemma 1.1. Unrevoked users are able to access the cloud.

eðT3 ; P Þsx eðH ; W Þ s s eðH ; P Þ s1  s2

Proof. The proof of Lemma 1.1 is equivalent to the correctness
of Algorithm 2 (group signature verification). ~R1 ¼ R1
holds since ~R1 ¼ s  U   c  T1 ¼ ðr þ cÞU   c    U ¼


R1 . Analogously, we can directly obtain ~R2 ¼ R2 ; ~R4 ¼ R4 ;
~R5 ¼ R5 . ~R3 ¼ R3 holds due to the following relations:


~R3 ¼ eðT3 ; W Þ
c
eðP ; P Þ
¼ eðT3 ; W Þ
eðT3 ; P Þrx þcxi eðH ; W Þ r c r  c
eðP ; P Þ


eðH ; P Þ r1  cxi  r2  cxi 
¼ eðT3 ; W Þ
c
eðT3 ; xiP Þc eð ð þ ÞH ; W þ xiP Þc
eðP ; P Þ


eðT3 ; P Þrx eðH ; W Þ r r eðH ; P Þ r1  r2


¼ eðT3 ; W Þ
c
eðT3 ; xiP Þc eð ð þ ÞH ; W þ xiP Þc
eðP ; P Þ


R3 ¼ eðT3 ; W Þ
eðT3   ð þ ÞH ; W þ xiP Þc eðT3 ; W Þ cR3
eðP ; P Þ
¼ eðAi ; W þ xiP Þ
eðP ; P Þ

R3 ¼ R3 :

c

c

c

ut

Lemma 1.2. Revoked users cannot utilize the cloud after their
revocation.

Proof. Lemma 1.2 is equivalent
to the correctness of
Algorithm 3 (revocation verification). The correctness of
revocation verification is based on the following relation:
eðT3   Ai ; H0 Þ ¼ eðAi þ ð þ Þ  H   Ai ; H0 Þ
¼ eðH ; H0 ÞeðH ; H0 Þ
¼ eðU ; 1H0 ÞeðV ; 2H0 Þ
¼ eðT1 ; H1 ÞeðT2 ; H2 Þ:

ut

Lemma 1.3. An attacker is unable to access the cloud server based
on the assumption of the intractability of q-SDH problem in G1 .

Proof. The brief security analysis can be shown as follows:
Suppose that an attacker A succeeds to forge a valid
group signature with a nonnegligible probability in
polynomial time. In addition, we assume f is a random
oracle. According to the Forking Lemma [21], by using the

 
Correctness
Q
eðC1 ; Ai;r ÞeðC2 ; BÞ


¼ e kY ;
1
¼1 ð þ x Þ P
i
e kPi ;

ð þ xÞ
Q
 G
x
 þ x
Q
k
¼ eðP ; GÞ
i
ðþxÞ
¼1
kþkx
¼ eðP ; GÞ
i
ðþxÞ
¼1

ðþx Þ

eðP ; GÞ
ðþxÞ
ðþx Þ ¼ Z k
i ¼ K :

Q
kx
i
¼1

ðþx Þ

c. Case 3 ðtr < tdata Þ. This case indicates that r
revoked users have been revoked before the
data file is uploaded

ii.

i. Verifying the group signature  by using
Algorithm 2.
Inputting A1 ; A2 ; :::Ar to call Algorithm 3. If
the algorithm returns invalid,
the user
terminates this operation.
Q
iii. Computing the value
Ar;r ¼
1
ð þ xÞ
¼1 ð þ x Þ P
r
by using Algorithm 4 with the input ðA; xÞ;
ðP1 ; x1 Þ; :::; ðPr ; xr Þ.
iv. Calculating the decryption key ^K ¼ eðC1 ;
Ar;r ÞeðC2 ; BÞ.
v. Decrypting the ciphertext C with the key ^K .

5.2.7 Traceability
When a data dispute occurs,
the tracing operation is
performed by the group manager to identify the real
identity of the data owner. Given a signature  ¼ ðT1 ; T2 ;
T3 ; c; s ; s ; sx ; s1 ; s2 Þ,
the group manager employs his
private key ð1 ; 2 Þ to compute Ai ¼ T3   ð1  T1 þ 2  T2 Þ.
Given the parameter Ai , the group manager can look up the
user list to find the corresponding identity.

Algorithm 4. Parameters Computing
Input: The revoked user parameters ðP1 ; x1 Þ; :::; ðPr ; xr Þ,
and the private key ðA; xÞ.
Output: Ar;r or NULL
begin
set temp ¼ A
for  ¼ 1 to r
if x ¼ x
return NULL
else
set temp ¼ 1
x x
return temp

ðP   tempÞ

end

6 SECURITY ANALYSIS

In this section, we prove the security of Mona in terms of
access control, data confidentiality, anonymity and trace-
ability that are defined in Section 4.2.

1189

ð5Þ

LIU ET AL.: MONA: SECURE MULTI-OWNER DATA SHARING FOR DYNAMIC GROUPS IN THE CLOUD
8>>>><>>>>:
oracle replay technique, the attacker A obtains two valid
signatures ðM ; 0 ; c; 1 Þ and ðM ; 0 ; c
1 Þ as follows:
0
0
; 
0 ¼ ðT1 ; T2 ; T3 ; c; R1 ; R2 ; R3 ; R4 ; R5 Þ
c ¼ f ðM ; T1 ; T2 ; T3 ; R1 ; R2 ; R3 ; R4 ; R5 Þ
0 ¼ f
0 ðM ; T1 ; T2 ; T3 ; R1 ; R2 ; R3 ; R4 ; R5 Þ
c
1 ¼ ðs ; s ; sx ; s1 ; s2 Þ
8>>>>><>>>>>:
1 ¼ ðs
2 Þ
0
0
0
0
0
0
 ; s
 ; s
x ; s
; s

1
 ¼ r þ c
s ¼ r þ c; s
0
0

 ¼ r þ c
s ¼ r þ c ; s
0
0

sx ¼ rx þ cx; s
x ¼ rx þ c
0
0
x
1 ¼ r1 þ c
s1 ¼ r1 þ c1 ; s
0
1
s2 ¼ r2 þ c2 ; s
2 ¼ r2 þ c
0
2 :
Then, A can compute an SDH tuple ð ^x ¼ sx =c; ^A ¼
T3   ððs þ s Þ=cÞ  H Þ such that ^A ¼ 1
þ ^x and
eð ^A; W þ ^xP Þ ¼ eðP ; P Þ;
where sx ¼ sx   s
, s ¼ s   s
x , c ¼ c   c
0
0
0
 , and
s ¼ s   s
0
 . Obviously, this contradicts with q-SDH
tu
assumption.

ð6Þ

is also unable to compute the decryption key through the
equation eðC1 ; Ai ÞeðC2 ; Bi Þ ¼ Z k . Thus, the correctness of
tu
Lemma 3.1 can be ensured.
Lemma 3.2. Even under the collusion with revoked users, the
cloud server is also incapable of learning the content of the files
stored after their revocation.
Proof. We first define two polynomial functions f ðXÞ ¼
i¼1 ðX þ xi Þ and gðXÞ ¼ n r
i¼1 ðX þ x
i Þ. Let G0 and P0
0
r
denote two elements in group G1 . Then, we set G ¼
f ð ÞG0 and P ¼ f ð Þgð ÞP0 . To maintain the confidenti-
ality against the revoked users, the data owner computes
8>>>><>>>>:
the header information C1 ; C2 and the encryption key K
as follows:
C1 ¼ kY ¼ kf ð Þ  G0
ð þ x1 Þð þ x2 Þ    ð þ xr Þ P ¼ kgð ÞP0
C2 ¼ kPr ¼
k
K ¼ Z k
r ¼ Z
ðþx1 Þðþx2 Þðþxr Þ ¼ Z
k
k
f ð Þ
¼ eðG; P Þf ðÞ ¼ eðG0 ; H0 Þkf ðÞgðÞ
We can observe that it is impossible for revoked users to
compute the encryption key K , since “given kf ð Þ  G0
and kgð ÞP0 , computing eðG0 ; H0 Þkf ð ÞgðÞ
” is an instance of
(t,n)-GDHE problem, which has been demonstrated to be
tu
intractable in polynomial time [14].
By the analysis above, we conclude that the proposed
scheme achieves the security goals including access control,
data confidentiality as well as anonymity and traceability.

ð7Þ

:

0

0

Theorem 2. The proposed scheme supports privacy preserving
and traceability.

Proof. The demonstration of this theorem is twofold. On one
hand, the group manager has the ability to identify the real
signer. Given a valid group signature  ¼ ðT1 ; T2 ; T3 ; c; s ;
s ; sx ; s1 ; s2 Þ and the private tuple ð1 ; 2 Þ, the group
manger can compute the private key of the signer through
the equation Ai ¼ T3   ð1  T1 þ 2  T2 Þ. The correctness
of the equation holds based on the following relation:
T3   ð1  T1 þ 2  T2 Þ ¼ Ai þ ð þ Þ
H   ð1  U þ 2  V Þ ¼ Ai :

On the other hand, other entities cannot reveal the
signer’s identity from a group signature. Otherwise, DL
assumption will be in contradiction. Further proofs about
the correctness, unforgeability, anonymity and traceabil-
tu
ity of group signatures can be found in [12].
Theorem 3. The proposed scheme protects data confidentiality
under the hardness of the WBDHE problem and GDHE problem.

Proof. Theorem 3 can be deduced from the following two
lemmas:

Lemma 3.1. The cloud server is unable to learn the content of the
stored files.
Proof. To prove this lemma, we take a data file ðC1 ; C2 ; C Þ as
an example to demonstrate the data confidentiality,
where C1 ¼ k  Y ; C2 ¼ k  P ; K ¼ Z k ; C ¼ EncK ðM Þ and
no user has been revoked before the data file is uploaded.
Suppose that the cloud server can compute K ¼ Z k , i.e.,
“given C1 ¼ k  Y ; C2 ¼ k  P ; P , for unknown  , comput-
ing eðC1 ; P Þ1
 ¼ eðG; P Þk ¼ K .” This contradicts with the
WBDHE assumption. On the other hand, given the
revocation list, the cloud server learns the partial private
key of a revoked user i, i.e., ðAi ; xi Þ for a revoked user.
Without the knowledge of the other part private key Bi , it

7 PERFORMANCE EVALUATION

In this section, we first analyze the storage cost of Mona,
and then perform experiments to test its computation cost.

7.1 Storage
Without loss of generality, we set q ¼ 160 and the elements
in G1 and G2 to be 161 and 1,024 bit, respectively. In
addition, we assume the size of the data identity is 16 bits,
which yield a group capacity of 216 data files. Similarly, the
size of user and group identity are also set as 16 bits.
Group manager. In Mona, the master private key of the
group manager is ðG;  ; 1 ; 2 Þ 2 G1  Zq
3 . Additionally,
the user list and the shared data list should be stored at the
group manager. Considering an actual system with 200 users
and assuming that each user share 50 files in average, the
total storage of the group manager is ð80:125 þ 42:125  200 þ
2  10; 000Þ  10 3  28:5 Kbytes, which is very acceptable.
Group members. Essentially, each user in our scheme only
needs to store its private key ðAi ; Bi ; xi Þ 2 G1
2  Zq , which
is about 60 bytes. It is worth noting that there is a tradeoff
between the storage and the computation overhead. For
the four pairing operations including ðeðH ;
example,
W Þ; eðH ; P Þ; eðP ; P Þ; eðAi ; P ÞÞ 2 G2
4 can be precomputed
once and stored for the group signature generation and
verification. Therefore, the total storage of each users is
about 572 bytes.
The extra storage overhead in the cloud. In Mona, the format
of files stored in the cloud is shown in Table 2. Since C3 is the
ciphertext of the file under the symmetrical encryption, the
extra storage overhead to store the file is about 248 bytes,
which includes ðIDgroup ; IDdata ; C1 ; C2 ; C3 ; f ð Þ; tdata ; Þ.

1190

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 24, NO. 6,

JUNE 2013

Fig. 3. Comparison on computation cost for file access between Mona
and ODBE [14].
costs a group signature on a message ðIDdata ;  Þ, where  is a
160-bit number in Z 
q .

7.2.2 Cloud Computation Cost
To evaluate the performance of the cloud in Mona, we test
its computation cost to respond various client operation
requests including file generation, file access, and file
deletion. Assuming the sizes of requested files are 100 and
10 MB, the test results are given in Table 3. It can be seen that
the computation cost of the cloud is deemed acceptable,
even when the number of revoked users is large. This is
because the cloud only involves group signature and
revocation verifications to ensure the validity of
the
requestor for all operations. In addition, it is worth noting
that the computation cost is independent with the size of the
requested file for access and deletion operations, since the
size of signed message is constant, e.g., ðIDgroup ; IDdata ; tÞ in
file access and ðIDdata ;  Þ in file deletion requests as
described in Section 5.

8 CONCLUSION

In this paper, we design a secure data sharing scheme,
Mona, for dynamic groups in an untrusted cloud. In Mona,
a user is able to share data with others in the group without
revealing identity privacy to the cloud. Additionally, Mona
supports efficient user revocation and new user joining.
More specially, efficient user revocation can be achieved
through a public revocation list without updating the
private keys of the remaining users, and new users can
directly decrypt
files stored in the cloud before their
participation. Moreover,
the storage overhead and the
encryption computation cost are constant. Extensive ana-
lyses show that our proposed scheme satisfies the desired
security requirements and guarantees efficiency as well.

TABLE 3
Computation Cost of the Cloud (s)

Fig. 2. Comparison on computation cost for file generation between
Mona and ODBE [14].

7.2 Simulation
To study the performance, we have simulated Mona by
using C programming language with GMP Library [22],
Miracl Library [23], and PBC Library [24]. The simulation
consists of three components: client side, manager side as
well as cloud side. Both client-side and manager-side
processes are conducted on a laptop with Core 2 T7250
2.0 GHz, DDR2 800 2G, Ubuntu 10.04 X86. The cloud-side
process is implemented on a machine that equipped with
Core 2 i3-2350 2.3 GHz, DDR3 1066 2G,Ubuntu 12.04 X64. In
the simulation, we choose an elliptic curve with 160-bit
group order, which provides a competitive security level
with 1,024-bit RSA.

7.2.1 Client Computation Cost
In Fig. 2, we list the comparison on computation cost of
clients for data generation operations between Mona and the
way that directly using the original dynamic broadcast
encryption (ODBE) [14]. It is easily observed that the
computation cost in Mona is irrelevant to the number of
revoked users. On the contrary, the computation cost
increases with the number of revoked users in ODBE. The
reason is that the parameters ðPr ; Zr Þ can be obtained from
the revocation list without sacrificing the security in Mona,
while several time-consuming operations including point
multiplications in G1 and exponentiations in G2 have to be
performed by clients to compute the parameters in ODBE.
From Figs. 2a and 2b, we can find out that sharing a 10-Mbyte
file and a 100-Mbyte one, cost a client about 0.2 and
1.4 seconds in our scheme, respectively, which implies that
the symmetrical encryption operation domains the compu-
tation cost when the file is large.
The computation cost of clients for file access operation
with the size of 10 and 100 Mbytes are illustrated in Fig. 3.
The computation cost in Mona increases with the number of
revoked users, as clients require to perform Algorithms 3
and 4 to compute the parameter Ar;r and check whether the
data owner is a revoked user. Besides the above operations,
P1 ; P2 ; :::; Pr need to be computed by clients in ODBE.
Therefore, Mona is still superior than ODBE in terms of
computation cost. Similar to the data generation operation,
the total computation cost is mainly determined by the
symmetrical decryption operation if the accessed file is large,
which can be verified from Figs. 3a and 3b. In addition, the
file deletion for clients is about 0.075 seconds, because it only

LIU ET AL.: MONA: SECURE MULTI-OWNER DATA SHARING FOR DYNAMIC GROUPS IN THE CLOUD

1191

ACKNOWLEDGMENTS
The authors thank the editors and anonymous reviewers for
their valuable comments to significantly improve the
quality of this paper. This work was supported in part by
the National Science Foundation of China under Grant
Nos. 60970140, 61272481, and 61272522.

[21] D. Pointcheval and J. Stern, “Security Arguments for Digital
Signatures and Blind Signatures,” J. Cryptology, vol. 13, no. 3,
pp. 361-396, 2000.
[22] The GNU Multiple Precision Arithmetic Library (GMP), http://
gmplib.org/, 2013.
[23] Multiprecision Integer and Rational Arithmetic C/C++ Library
(MIRACL), http://certivox.com/, 2013.
[24] The Pairing-Based Cryptography Library (PBC), http://crypto.
stanford.edu/pbc/howto.html, 2013.

Xuefeng Liu rece ived the BSc degree in
information secur ity from Xidian University,
China, 2007. He joined Xidian University in
2007 for
the MSc and PhD degrees. H is
research interests include wireless network
security, cloud computing, mobile computing
and applied cryptography.

Yuqing Zhang received the BSc and MSc
degrees in computer science from Xidian Uni-
versity, China, in 1987 and 1990, respectively.
He received the PhD degree in cryptography
from Xidian University in 2000. He is a professor
and supervisor of PhD students at the Graduate
University of Chinese Academy of Sciences. His
research interests include cryptography, wire-
less security and trust management. He is a
member of the IEEE.

Boyang Wang received the BS degree in
information security from Xidian University in
2007. He is currently working toward the PhD
degree from Xidian University, Xi’an, China. His
research interests focus on security and privacy
issues in cloud computing, social network and
network coding.

J ingbo Yan rece ived the BSc degree in
information secur ity from Xidian University,
China, 2009. He joined Xidian University in
2009 for
the MSc and PhD degrees. H is
research interests include privacy, applied cryp-
tography and computer security.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

[2]

[3]

REFERENCES
[1] M. Armbrust, A. Fox, R. Griffith, A.D. Joseph, R.H. Katz, A.
Konwinski, G. Lee, D.A. Patterson, A. Rabkin, I. Stoica, and M.
Zaharia, “A View of Cloud Computing,” Comm. ACM, vol. 53,
no. 4, pp. 50-58, Apr. 2010.
S. Kamara and K. Lauter, “Cryptographic Cloud Storage,” Proc.
Int’l Conf. Financial Cryptography and Data Security (FC), pp. 136-
149, Jan. 2010.
S. Yu, C. Wang, K. Ren, and W. Lou, “Achieving Secure, Scalable,
and Fine-Grained Data Access Control in Cloud Computing,”
Proc. IEEE INFOCOM, pp. 534-542, 2010.
[4] M. Kallahalla, E. Riedel, R. Swaminathan, Q. Wang, and K. Fu,
“Plutus: Scalable Secure File Sharing on Untrusted Storage,” Proc.
USENIX Conf. File and Storage Technologies, pp. 29-42, 2003.
[5] E. Goh, H. Shacham, N. Modadugu, and D. Boneh, “Sirius:
Securing Remote Untrusted Storage,” Proc. Network and Distributed
Systems Security Symp. (NDSS), pp. 131-145, 2003.
[6] G. Ateniese, K. Fu, M. Green, and S. Hohenberger, “Improved
Proxy Re-Encryption Schemes with Applications to Secure
Distributed Storage,” Proc. Network and Distributed Systems Security
Symp. (NDSS), pp. 29-43, 2005.
[7] R. Lu, X. Lin, X. Liang, and X. Shen, “Secure Provenance: The
Essential of Bread and Butter of Data Forensics in Cloud
Computing,” Proc. ACM Symp. Information, Computer and Comm.
Security, pp. 282-292, 2010.
[8] B. Waters, “Ciphertext-Policy Attribute-Based Encryption: An
Expressive, Efficient, and Provably Secure Realization,” Proc. Int’l
Conf. Practice and Theory in Public Key Cryptography Conf. Public Key
Cryptography, http://eprint.iacr.org/2008/290.pdf, 2008.
[9] V. Goyal, O. Pandey, A. Sahai, and B. Waters, “Attribute-Based
Encryption for Fine-Grained Access Control of Encrypted
Data,” Proc. ACM Conf. Computer and Comm. Security (CCS),
pp. 89-98, 2006.
[10] D. Naor, M. Naor, and J.B. Lotspiech, “Revocation and Tracing
Schemes for Stateless Receivers,” Proc. Ann. Int’l Cryptology Conf.
Advances in Cryptology (CRYPTO), pp. 41-62, 2001.
[11] D. Boneh and M. Franklin, “Identity-Based Encryption from the
Weil Pairing,” Proc. Int’l Cryptology Conf. Advances in Cryptology
(CRYPTO), pp. 213-229, 2001.
[12] D. Boneh, X. Boyen, and H. Shacham, “Short Group Signature,”
Proc. Int’l Cryptology Conf. Advances in Cryptology (CRYPTO),
pp. 41-55, 2004.
[13] D. Boneh, X. Boyen, and E. Goh, “Hierarchical Identity Based
Encryption with Constant Size Ciphertext,” Proc. Ann. Int’l Conf.
Theory and Applications of Cryptographic Techniques (EUROCRYPT),
pp. 440-456, 2005.
[14] C. Delerablee, P. Paillier, and D. Pointcheval, “Fully Collusion
Secure Dynamic Broadcast Encryption with Constant-Size Ci-
phertexts or Decryption Keys,” Proc. First Int’l Conf. Pairing-Based
Cryptography, pp. 39-59, 2007.
[15] D. Chaum and E. van Heyst, “Group Signatures,” Proc. Int’l Conf.
Theory and Applications of Cryptographic Techniques (EUROCRYPT),
pp. 257-265, 1991.
[16] A. Fiat and M. Naor, “Broadcast Encryption,” Proc. Int’l Cryptology
Conf. Advances in Cryptology (CRYPTO), pp. 480-491, 1993.
[17] B. Wang, B. Li, and H. Li, “Knox: Privacy-Preserving Auditing for
Shared Data with Large Groups in the Cloud,” Proc. 10th Int’l
Conf. Applied Cryptography and Network Security, pp. 507-525, 2012.
[18] C. Wang, Q. Wang, K. Ren, and W. Lou, “Privacy-Preserving
Public Auditing for Data Storage Security in Cloud Computing,”
Proc. IEEE INFOCOM, pp. 525-533, 2010.
[19] B. Sheng and Q. Li, “Verifiable Privacy-Preserving Range Query
in Two-Tiered Sensor Networks,” Proc. IEEE INFOCOM, pp. 46-
50, 2008.
[20] D. Boneh, B. Lynn, and H. Shacham, “Short Signature from the
Weil Pairing,” Proc. Int’l Conf. Theory and Application of Cryptology
and Information Security: Advances in Cryptology, pp. 514-532, 2001.

384

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 2, FEBRUARY 2014

Decentralized Access Control with Anonymous
Authentication of Data Stored in Clouds

Sushmita Ruj, Member, IEEE, Mi los Sto jmenov ic, Member, IEEE, and
Amiya Nayak, Sen ior Member, IEEE

Abstract—We propose a new decentralized access control scheme for secure data storage in clouds that supports anonymous
authentication. In the proposed scheme, the cloud verifies the authenticity of the series without knowing the user’s identity before storing
data. Our scheme also has the added feature of access control in which only valid users are able to decrypt the stored information. The
scheme prevents replay attacks and supports creation, modification, and reading data stored in the cloud. We also address user
revocation. Moreover, our authentication and access control scheme is decentralized and robust, unlike other access control schemes
designed for clouds which are centralized. The communication, computation, and storage overheads are comparable to centralized
approaches.

Index Terms—Access control, authentication, attribute-based signatures, attribute-based encryption, cloud storage

Ç

1 INTRODUCTION
R ESEARCH in cloud computing is receiving a lot of
attention from both academic and industrial worlds.
In cloud computing, users can outsource their computation
and storage to servers (also called clouds) using Internet.
This frees users from the hassles of maintaining resources
on-site. Clouds can provide several types of services like
applications (e.g., Google Apps, Microsoft online), infra-
structures (e.g., Amazon’s EC2, Eucalyptus, Nimbus), and
platforms to help developers write applications (e.g.,
Amazon’s S3, Windows Azure).
Much of the data stored in clouds is highly sensitive, for
example, medical records and social networks. Security and
privacy are, thus, very important issues in cloud comput-
ing. In one hand, the user should authenticate itself before
initiating any transaction, and on the other hand, it must be
ensured that the cloud does not tamper with the data that is
outsourced. User privacy is also required so that the cloud
or other users do not know the identity of the user. The
cloud can hold the user accountable for the data it
outsources, and likewise, the cloud is itself accountable
for the services it provides. The validity of the user who
stores the data is also verified. Apart from the technical
solutions to ensure security and privacy, there is also a need
for law enforcement.

. S. Ruj is with Indian Statistical Institute, 203 B.T. Road, Kolkata 700108,
India. E-mail: sush@isical.ac.in.
. M. Stojmenovic is with the Univerzitet Singidunum, Danijelova 32,
Belgrade 11000, Serbia. E-mail: mstojmenovic@singidunum.ac.rs.
. A. Nayak is with the School of Electrical Engineering and Computer
Science, University of Ottawa, 800 King Edward, Ottawa K1N6N5, ON,
Canada. E-mail: anayak@site.uottawa.ca.

Manuscript received 30 Sept. 2012; revised 28 Dec. 2012; accepted 11 Jan.
2013; published online 14 Feb. 2013.
Recommended for acceptance by X. Li, P. McDaniel, R. Poovendran, and
G. Wang.
For information on obtaining reprints of this article, please send e-mail to:
tpds@computer.org, and reference IEEECS Log Number
TPDSSI-2012-09-1006.
Digital Object Identifier no. 10.1109/TPDS.2013.38.

Recently, Wang et al. [2] addressed secure and depend-
able cloud storage. Cloud servers prone to Byzantine
failure, where a storage server can fail in arbitrary ways
[2]. The cloud is also prone to data modification and server
colluding attacks. In server colluding attack, the adversary
can compromise storage servers, so that it can modify data
files as long as they are internally consistent. To provide
secure data storage,
the data needs to be encrypted.
However, the data is often modified and this dynamic
property needs to be taken into account while designing
efficient secure storage techniques.
Efficient search on encrypted data is also an important
concern in clouds. The clouds should not know the query
but should be able to return the records that satisfy the
query. This is achieved by means of searchable encryption
[3], [4]. The keywords are sent to the cloud encrypted, and
the cloud returns the result without knowing the actual
keyword for the search. The problem here is that the data
records should have keywords associated with them to
enable the search. The correct records are returned only
when searched with the exact keywords.
Security and privacy protection in clouds are being
explored by many researchers. Wang et al. [2] addressed
storage security using Reed-Solomon erasure-correcting
codes. Authentication of users using public key crypto-
graphic techniques has been studied in [5]. Many homo-
morphic encryption techniques have been suggested [6], [7]
to ensure that the cloud is not able to read the data while
performing computations on them. Using homomorphic
encryption, the cloud receives ciphertext of the data and
performs computations on the ciphertext and returns the
encoded value of the result. The user is able to decode the
result, but the cloud does not know what data it has
operated on. In such circumstances, it must be possible for
the user to verify that the cloud returns correct results.
Accountability of clouds is a very challenging task and
involves technical
issues and law enforcement. Neither
clouds nor users should deny any operations performed or
requested. It is important to have log of the transactions
performed; however, it is an important concern to decide

1045-9219/14/$31.00 ß 2014 IEEE

Published by the IEEE Computer Society

RUJ ET AL.: DECENTRALIZED ACCESS CONTROL WITH ANONYMOUS AUTHENTICATION OF DATA STORED IN CLOUDS

385

how much information to keep in the log. Accountability
has been addressed in TrustCloud [8]. Secure provenance
has been studied in [9].
Considering the following situation: A law student,
Alice, wants to send a series of reports about some
malpractices by authorities of University X to all
the
professors of University X , research chairs of universities in
the country, and students belonging to Law department in
all universities in the province. She wants to remain
anonymous while publishing all evidence of malpractice.
She stores the information in the cloud. Access control is
important in such case, so that only authorized users can
access the data. It is also important to verify that the
information comes from a reliable source. The problems of
access control, authentication, and privacy protection
should be solved simultaneously. We address this problem
in its entirety in this paper.
Access control in clouds is gaining attention because it is
important that only authorized users have access to valid
service. A huge amount of information is being stored in the
cloud, and much of this is sensitive information. Care
should be taken to ensure access control of this sensitive
information which can often be related to health, important
documents (as in Google Docs or Dropbox) or even
personal information (as in social networking). There are
broadly three types of access control: user-based access control
(UBAC), role-based access control (RBAC), and attribute-based
access control (ABAC). In UBAC, the access control
list
contains the list of users who are authorized to access data.
This is not feasible in clouds where there are many users. In
RBAC (introduced by Ferraiolo and Kuhn [10]), users are
classified based on their individual roles. Data can be
accessed by users who have matching roles. The roles are
defined by the system. For example, only faculty members
and senior secretaries might have access to data but not the
junior secretaries. ABAC is more extended in scope, in
which users are given attributes, and the data has attached
access policy. Only users with valid set of attributes,
satisfying the access policy, can access the data. For
instance, in the above example certain records might be
accessible by faculty members with more than 10 years of
research experience or by senior secretaries with more than
8 years experience. The pros and cons of RBAC and ABAC
are discussed in [11]. There has been some work on ABAC
in clouds (for example, [12], [13], [14], [15], [16]). All these
work use a cryptographic primitive known as attribute-
based encryption (ABE). The eXtensible access control
markup language [17] has been proposed for ABAC in
clouds [18].
An area where access control is widely being used is
health care. Clouds are being used to store sensitive
information about patients to enable access to medical
professionals, hospital staff, researchers, and policy makers.
It is important to control the access of data so that only
authorized users can access the data. Using ABE,
the
records are encrypted under some access policy and stored
in the cloud. Users are given sets of attributes and
corresponding keys. Only when the users have matching
set of attributes, can they decrypt the information stored in
the cloud. Access control in health care has been studied in
[12] and [13].

Access control is also gaining importance in online social
networking where users (members) store their personal
information, pictures, videos and share them with selected
groups of users or communities they belong to. Access
control in online social networking has been studied in [19].
Such data are being stored in clouds. It is very important
that only the authorized users are given access to those
information. A similar situation arises when data is stored
in clouds, for example, in Dropbox, and shared with certain
groups of people.
It is just not enough to store the contents securely in the
cloud but it might also be necessary to ensure anonymity of
the user. For example, a user would like to store some
sensitive information but does not want to be recognized.
The user might want to post a comment on an article, but
does not want his/her identity to be disclosed. However,
the user should be able to prove to the other users that he/
she is a valid user who stored the information without
revealing the identity. There are cryptographic protocols
like ring signatures [20], mesh signatures [21], group
signatures [22], which can be used in these situations. Ring
signature is not a feasible option for clouds where there are
a large number of users. Group signatures assume the pre-
existence of a group which might not be possible in clouds.
Mesh signatures do not ensure if the message is from a
single user or many users colluding together. For these
reasons, a new protocol known as attribute-based signature
(ABS) has been applied. ABS was proposed by Maji et al.
[23]. In ABS, users have a claim predicate associated with a
message. The claim predicate helps to identify the user as
an authorized one, without revealing its identity. Other
users or the cloud can verify the user and the validity of the
message stored. ABS can be combined with ABE to achieve
authenticated access control without disclosing the identity
of the user to the cloud.
Existing work [12], [13], [14], 15], [16], [18], [38] on access
control in cloud are centralized in nature. Except [38] and
[18], all other schemes use ABE. The scheme in [38] uses a
symmetric key approach and does not support authentica-
tion. The schemes [12], [13], [16] do not support authentica-
tion as well. Earlier work by Zhao et al. [15] provides
privacy preserving authenticated access control in cloud.
However, the authors take a centralized approach where a
single key distribution center (KDC) distributes secret keys
and attributes to all users. Unfortunately, a single KDC is
not only a single point of failure but difficult to maintain
because of the large number of users that are supported in a
cloud environment. We, therefore, emphasize that clouds
should take a decentralized approach while distributing
secret keys and attributes to users. It is also quite natural for
clouds to have many KDCs in different locations in the
world. Although Yang et al. [34] proposed a decentralized
approach, their technique does not authenticate users, who
want to remain anonymous while accessing the cloud. In an
earlier work, Ruj et al. [16] proposed a distributed access
control mechanism in clouds. However, the scheme did not
provide user authentication. The other drawback was that a
user can create and store a file and other users can only read
the file. Write access was not permitted to users other than
the creator. In the preliminary version of this paper [1], we

386

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 2, FEBRUARY 2014

extend our previous work with added features that enables
to authenticate the validity of
the message without
revealing the identity of the user who has stored informa-
tion in the cloud. In this version we also address user
revocation, that was not addressed in [1]. We use ABS
scheme [24] to achieve authenticity and privacy. Unlike
[24], our scheme is resistant to replay attacks, in which a
user can replace fresh data with stale data from a previous
write, even if it no longer has valid claim policy. This is an
important property because a user, revoked of its attributes,
might no longer be able to write to the cloud. We, therefore,
add this extra feature in our scheme and modify [24]
appropriately. Our scheme also allows writing multiple
times which was not permitted in our earlier work [16].

1.1 Our Contributions
The main contributions of this paper are the following:

1. Distributed access control of data stored in cloud so
that only authorized users with valid attributes can
access them.
2. Authentication of users who store and modify their
data on the cloud.
3. The identity of the user is protected from the cloud
during authentication.
4. The architecture is decentralized, meaning that there
can be several KDCs for key management.
5. The access control and authentication are both
collusion resistant, meaning that no two users can
collude and access data or authenticate themselves,
if they are individually not authorized.
6. Revoked users cannot access data after they have
been revoked.
7. The proposed scheme is resilient to replay attacks. A
writer whose attributes and keys have been revoked
cannot write back stale information.
8. The protocol supports multiple read and write on
the data stored in the cloud.
9. The costs are comparable to the existing centralized
approaches, and the expensive operations are mostly
done by the cloud.

1.2 Organization
The paper is organized as follows: Related work is
presented in Section 2. The mathematical background and
assumptions are detailed in Section 3. We present our
privacy preserving access control scheme in Section 4
followed by a real life example in Section 5. The security
is analyzed in Section 6. Computation complexity is
discussed in Section 7, and comparison with other work is
presented in Section 8. We conclude in Section 9.

2 RELATED WORK

ABE was proposed by Sahai and Waters [26]. In ABE, a user
has a set of attributes in addition to its unique ID. There are
two classes of ABEs. In key-policy ABE or KP-ABE (Goyal
et al. [27]), the sender has an access policy to encrypt data. A
writer whose attributes and keys have been revoked cannot
write back stale information. The receiver receives attributes
and secret keys from the attribute authority and is able to

decrypt
In
it has matching attributes.
information if
Ciphertext-policy, CP-ABE ([28], [29]), the receiver has the
access policy in the form of a tree, with attributes as leaves
and monotonic access structure with AND, OR and other
threshold gates.
All the approaches take a centralized approach and
allow only one KDC, which is a single point of failure.
Chase [30] proposed a multiauthority ABE, in which there
are several KDC authorities (coordinated by a trusted
authority) which distribute attributes and secret keys to
users. Multiauthority ABE protocol was studied in [31] and
[32], which required no trusted authority which requires
every user to have attributes from at all the KDCs. Recently,
Lewko and Waters [35] proposed a fully decentralized ABE
where users could have zero or more attributes from each
authority and did not require a trusted server. In all these
cases, decryption at user’s end is computation intensive. So,
this technique might be inefficient when users access using
their mobile devices. To get over this problem, Green et al.
[33] proposed to outsource the decryption task to a proxy
server, so that
the user can compute with minimum
resources (for example, hand held devices). However, the
presence of one proxy and one KDC makes it less robust
than decentralized approaches. Both these approaches had
no way to authenticate users, anonymously. Yang et al. [34]
presented a modification of [33], authenticate users, who
want to remain anonymous while accessing the cloud.
To ensure anonymous user authentication ABSs were
introduced by Maji et al. [23]. This was also a centralized
approach. A recent scheme by Maji et al. [24] takes a
decentralized approach and provides authentication without
disclosing the identity of the users. However, as mentioned
earlier in the previous section it is prone to replay attack.

3 BACKGROUND

In this section, we present our cloud storage model,
adversary model and the assumptions we have made in
the paper. Table 1 presents the notations used throughout
the paper. We also describe mathematical background used
in our proposed solution.

3.1 Assumptions
We make the following assumptions in our work:

1. The cloud is honest-but-curious, which means that
the cloud administrators can be interested in view-
ing user’s content, but cannot modify it. This is a
valid assumption that has been made in [12] and
[13]. Honest-but-curious model of adversaries do not
tamper with data so that they can keep the system
functioning normally and remain undetected.
2. Users can have either read or write or both accesses
to a file stored in the cloud.
3. All communications between users/clouds are se-
cured by secure shell protocol, SSH.

3.2 Formats of Access Policies
Access policies can be in any of the following formats:
1) Boolean functions of attributes, 2) linear secret sharing
scheme (LSSS) matrix, or 3) monotone span programs. Any
access structure can be converted into a Boolean function

RUJ ET AL.: DECENTRALIZED ACCESS CONTROL WITH ANONYMOUS AUTHENTICATION OF DATA STORED IN CLOUDS

387

TABLE 1
Notations

3.4 Attribute-Based Encryption
ABE with multiple authorities as proposed by Lewko and
Waters [35] proceeds as follows [16]:

3.4.1 System Initialization
Select a prime q, generator g of G0 , groups G0 and GT of
order q, a map e : G0  G0 ! GT , and a hash function H :
f0; 1g ! G0 that maps the identities of users to G0 . The
T
hash function used here is SHA-1. Each KDC Aj 2 A has a
Lj ¼  for
set of attributes Lj . The attributes disjoint (Li
i 6¼ j). Each KDC also chooses two random exponents
i ; yi 2 ZZq . The secret key of KDC Aj is
SK ½j ¼ fi ; yi ; i 2 Lj g:
The public key of KDC Aj is published
P K ½j ¼ feðg; gÞi ; gyi ; i 2 Lj g:

ð2Þ

ð1Þ

3.4.2 Key Generation and Distribution by KDCs
User Uu receives a set of attributes I ½j; u from KDC Aj , and
corresponding secret key ski;u for each i 2 I ½j; u
ð3Þ
ski;u ¼ gi H ðuÞyi ;
where i ; yi 2 SK ½j. Note that all keys are delivered to the
user securely using the user’s public key, such that only that
user can decrypt it using its secret key.

3.4.3 Encryption by Sender
The encryption function is ABE :EncryptðM SG; X Þ. Sender
decides about the access tree X . LSSS matrix R can be
derived as described in Section 3.2. Sender encrypts
message M SG as follows:
1. Choose a random seed s 2 ZZq and a random vector
v 2 ZZh
q , with s as its first entry; h is the number of
leaves in the access tree (equal to the number of rows
in the corresponding matrix R).
2. Calculate x ¼ Rx  v, where Rx is a row of R.
3. Choose a random vector w 2 ZZh
q with 0 as the first
entry.
4. Calculate !x ¼ Rx  w.
For each row Rx of R, choose a random x 2 ZZq .
5.
6. The following parameters are calculated:
C0 ¼ M SGeðg; gÞs ;
C1;x ¼ eðg; gÞx eðg; gÞðxÞ x ; 8x;
C2;x ¼ gx 8x;
C3;x ¼ gyðxÞ x g!x 8x;
where ðxÞ is mapping from Rx to the attribute i that
is located at the corresponding leaf of the access tree.
7. The ciphertext C is sent by the sender (it also
includes the access tree via R matrix):
C ¼ hR; ; C0 ; fC1;x ; C2;x ; C3;x ; 8xgi:

ð4Þ

ð5Þ

3.4.4 Decryption by Receiver
The decryption function is ABE :DecryptðC ; fski;u gÞ, where
C is given by (5). Receiver Uu takes as input ciphertext C ,
secret keys fski;u g, group G0 , and outputs message msg. It

[35]. An example of a Boolean function is ðða1 ^ a2 ^ a3 Þ _
ða4 ^ a5 ÞÞ ^ ða6 _ a7 ÞÞ, where a1 ; a2 ; . . . ; a7 are attributes.
Let Y : f0; 1gn ! f0; 1g be a monotone Boolean function
[24]. A monotone span program for Y over a field IF is an
l  t matrix M with entries in IF, along with a labeling
function a : ½l ! ½n that associates each row of M with an
input variable of Y , such that, for every ðx1 ; x2 . . . ; xn Þ 2
f0; 1gn , the following condition is satisfied:
Y ðx1 ; x2 ; . . . ; xn Þ ¼ 1 , 9v 2 IF1l : vM
¼ ½1; 0; 0; . . . ; 0 and ð8i : xaðiÞ
¼ 0 ) vi ¼ 0Þ:
In other words, Y ðx1 ; x2 ; . . . ; xn Þ ¼ 1 if and only if the rows
of M indexed by fijxaðiÞ ¼ 1g span the vector ½1; 0; 0; . . . ; 0.
Span programs can be constructed from Boolean functions in
a similar way as shown later in Section 5.

3.3 Mathematical Background
We will use bilinear pairings on elliptic curves. Let G be a
cyclic group of prime order q generated by g. Let GT be a
group of order q. We can define the map e : G  G ! GT .
The map satisfies the following properties:
eðaP ; bQÞ ¼ eðP ; QÞab for all P ; Q 2 G and a; b 2 ZZq ,
1.
ZZq ¼ f0; 1; 2; . . . ; q   1g.
2. Nondegenerate: eðg; gÞ 6¼ 1.
Bilinear pairing on elliptic curves groups is used. We do
not discuss the pairing functions which mainly use Weil and
Tate pairings [36] and computed using Miller’s algorithm.
The choice of curve is an important consideration because it
determines the complexity of pairing operations.

388

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 2, FEBRUARY 2014

2.

obtains the access matrix R and mapping  from C . It then
T
executes the following steps:
1. Uu calculates the set of attributes fðxÞ : x 2 Xg
Iu
that are common to itself and the access matrix. X is
the set of rows of R.
For each of these attributes, it checks if there is a
subset X 0 of rows of R, such that
the vector
ð1; 0; . . . ; 0Þ is their linear combination .
P
If not ,
decryption is impossible. If yes, it calculates con-
stants cx 2 ZZq , such that
x2X 0 cxRx ¼ ð1; 0; . . . ; 0Þ.
3. Decryption proceeds as follows:
For each x 2 X 0 , decðxÞ ¼ C1;x eðH ðuÞ;C3;x Þ
a.
eðskðxÞ;u ;C2;x Þ .
b. Uu computes M SG ¼ C0 =x2X 0 decðxÞ.
3.5 Attribute-Based Signature Scheme
ABS scheme [24] has the following steps.

3.5.1 System Initialization
Select a prime q, and groups G1 and G2 , which are of order
q. We define the mapping ^e : G1  G1 ! G2 . Let g1 ; g2 be
generators of G1 and hj be generators of G2 , for j 2 ½tmax ,
for arbitrary tmax . Let H be a hash function. Let A0 ¼ ha0
0 ,
where a0 2 ZZ
is chosen at random. ðT S ig; T V erÞ mean
q
T S ig is the private key with which a message is signed and
T V er is the public key used for verification. The secret key
for the trustee is T SK ¼ ða0 ; T S igÞ and public key is
T P K ¼ ðG1 ; G2 ; H; g1 ; A0 ; h0 ; h1 ; . . . ; htmax ; g2 ; T V erÞ.

3.5.2 User Registration
For a user with identity Uu the KDC draws at random
Kbase 2 G. Let K0 ¼ K 1=a0
base . The following token  is output
 ¼ ðu; Kbase ; K0 ; Þ;
ð6Þ
where  is signature on ukKbase using the signing key T S ig.

3.5.3 KDC Setup
Choose a; b 2 ZZ
j , Bij ¼ hb
q randomly and compute: Aij ¼ ha
j ,
for Ai 2 AA, j 2 ½tmax . The private key of ith KDC is
ASK ½i ¼ ða; bÞ and public key AP K ½i ¼ ðAij ; Bij jj 2 ½tmax Þ.

3.5.4 Attribute Generation
The token verification algorithm verifies the signature
contained in  using the signature verification key T V er in
T P K . This algorithm extracts Kbase from  using ða; bÞ
from ASK ½i and computes Kx ¼ K 1=ðaþbxÞ
, x 2 J ½i; u. The
base
key Kx can be checked for consistency using algorithm

 
ABS :K eyCheckðT P K ; AP K ½i;  ; Kx Þ, which checks
¼ ^eðKbase ; hj Þ;
Kx ; AijBx
^e
ij
for all x 2 J ½i; u and j 2 ½tmax .

3.5.5 Sign
The algorithm
ABS :S ignðT P K ; fAP K ½i : i 2 AT ½ug;
 ; fKx : x 2 Ju g; M SG; Y Þ;

has input the public key of the trustee, the secret key of the
signer, the message to be signed and the policy claim Y . The

policy claim is first converted into the span program
M 2 ZZlt
, with rows labeled with attributes. Mx denotes
q
row x of M . Let 0 denote the mapping from rows to the
attributes. So, 0 ðxÞ is the mapping from Mx to attribute x. A
vec tor v is compu ted tha t sa t is f ies the ass ignmen t
fx : x 2 J ½i; ug. Compute  ¼ HðM SGkY Þ. Choose r0 2 ZZ


 
 
q
and ri 2 ZZq ; i 2 Ju , and compute:
Y ¼ K r0
ri ð8i 2 Ju Þ;
base ; Si ¼
r0 :
K vi

 
i

g2 g
1

ð7Þ

0 ; Pj ¼ i2AT ½u
W ¼ K r0
The signature is calculated as
 ¼ ðY ; W ; S1 ; S2 ; . . . ; St ; P1 ; P2 ; . . . ; Pt Þ:

AijB0 ðiÞ
ij

Mij ri ð8j 2 ½tÞ:

ð8Þ

ð9Þ

3.5.6 Verify
Algorithm
ABS :V erif yðT P K ;  ¼ ðY ; W ; S1 ; S2 ; . . . ; St ;
P1 ; P2 ; . . . ; Pt Þ; M SG; Y Þ;
converts Y to the corresponding monotone program
M 2 ZZlt
, with rows labeled with attributes. Compute  ¼
q
HðM SGkY Þ.
If Y ¼ 1, ABS :V erif y ¼ 0 mean ing fa lse .
Otherwise, the following constraints are checked
^eðW ; A0 Þ ¼? ^eðY ; h0 Þ;


 



 
; j ¼ 1;
^eðY ; h1 Þ^e
g2 g
1 ; P1
g2 g
^e
; j > 1;
1 ; Pj

 
Si ; Ai0 jB0 ðiÞ
i2l ^e
i0 j
where i0 ¼ AT ½i.

ð11Þ

ð10Þ

¼?

Mij

4 PROPOSED PRIVACY PRESERVING
AUTHENTICATED ACCESS CONTROL SCHEME

In this sect ion , we propose our pr ivacy preserv ing
authenticated access control scheme. According to our
scheme a user can create a file and store it securely in the
cloud. This scheme consists of use of the two protocols ABE
and ABS, as discussed in Sections 3.4 and 3.5, respectively.
We will first discuss our scheme in details and then provide
a concrete example to demonstrate how it works. We refer
to the Fig. 1. There are three users, a creator, a reader, and
writer. Creator Alice receives a token  from the trustee,
who is assumed to be honest. A trustee can be someone like
the federal government who manages social
insurance
numbers etc. On presenting her id (like health/social
insurance number), the trustee gives her a token  . There
are multiple KDCs (here 2), which can be scattered. For
example, these can be servers in different parts of the world.
A creator on presenting the token to one or more KDCs
receives keys for encryption/decryption and signing. In the
Fig. 1, SK s are secret keys given for decryption, Kx are keys
for signing. The message M SG is encrypted under the
access policy X . The access policy decides who can access
the data stored in the cloud. The creator decides on a claim
policy Y , to prove her authenticity and signs the message
under this claim. The ciphertext C with signature is c, and is
sent to the cloud. The cloud verifies the signature and stores

389
RUJ ET AL.: DECENTRALIZED ACCESS CONTROL WITH ANONYMOUS AUTHENTICATION OF DATA STORED IN CLOUDS
 ¼ ABS :S ignðPublic key of trustee; Public key of KDCs;
token; signing key; message; access claimÞ:

The following information is then sent in the cloud
ð13Þ
c ¼ ðC ;  ; ; Y Þ:
The cloud on receiving the information verifies the
access claim using the algorithm ABS :verif y. The creator
checks the value of V ¼ ABS :V erif yðT P K ; ; c; Y Þ. If V ¼ 0,
then authentication has failed and the message is discarded.
Else, the message ðC ;  Þ is stored in the cloud.

4.2 Reading from the Cloud
When a user requests data from the cloud, the cloud sends
the ciphertext C using SSH protocol. Decryption proceeds
using algorithm ABE :DecryptðC ; fski;u gÞ and the message
M SG is calculated as given in Section 3.4.4.

4.3 Writing to the Cloud
To write to an already existing file, the user must send its
message with the claim policy as done during file creation.
The cloud verifies the claim policy, and only if the user is
authentic, is allowed to write on the file.

4.4 User Revocation
We have just discussed how to prevent replay attacks. We
will now discuss how to handle user revocation. It should
be ensured that users must not have the ability to access
data, even if they possess matching set of attributes. For this
reason, the owners should change the stored data and send
updated information to other users. The set of attributes Iu
possessed by the revoked user Uu is noted and all users
change their stored data that have attributes i 2 Iu . In [13],
revocation involved changing the public and secret keys of
the minimal set of attributes which are required to decrypt
the data. We do not consider this approach because here
different data are encrypted by the same set of attributes, so
such a minimal set of attributes is different for different
users. Therefore, this does not apply to our model. Once
the attributes Iu are identified, all data that possess the
attributes are collected. For each such data record, the
following steps are then carried out:
1. A new value of s, snew 2 ZZq is selected.
2. The first entry of vector vnew is changed to new snew .
3. x ¼ Rx vnew is calculated, for each row x corre-
sponding to leaf attributes in Iu .
4. C1;x is recalculated for x.
5. New value of C1;x is securely transmitted to the cloud.
6. New C0 ¼ M eðg; gÞsnew is calculated and stored in the
cloud.
7. New value of C1;x is not stored with the data, but is
transmitted to users, who wish to decrypt the data.
We note here that the new value of C1;x is not stored in
the cloud but transmitted to the nonrevoked users who have
attribute corresponding to x. This prevents a revoked user to
decrypt the new value of C0 and get back the message.

5 REAL LIFE EXAMPLE

We now revisit the problem we stated in the introduction.
We will use a relaxed setting. Suppose Alice is a law
student and wants to send a series of reports about

Fig. 1. Our secure cloud storage model.

the ciphertext C . When a reader wants to read, the cloud
sends C . If the user has attributes matching with access
policy, it can decrypt and get back original message.
Write proceeds in the same way as file creation. By
designating the verification process to the cloud, it relieves
the individual users from time consuming verifications.
When a reader wants to read some data stored in the cloud,
it tries to decrypt it using the secret keys it receives from the
KDCs. If it has enough attributes matching with the access
policy, then it decrypts the information stored in the cloud.

4.1 Data Storage in Clouds
A user Uu first registers itself with one or more trustees. For
simplicity we assume there is one trustee. The trustee gives
it a token  ¼ ðu; Kbase ; K0 ; Þ, where  is the signature on
ukKbase signed with the trustees private key T S ig (by (6)).
The KDCs are given keys P K ½i; SK ½i for encryption/
decryption and ASK ½i; AP K ½i for signing/verifying. The
user on presenting this token obtains attributes and secret
keys from one or more KDCs. A key for an attribute x
belonging to KDC Ai is calculated as Kx ¼ K 1=ðaþbxÞ
, where
base
ða; bÞ 2 ASK ½i. The user also receives secret keys skx;u for
encrypting messages. The user then creates an access policy
X which is a monotone Boolean function. The message is
then encrypted under the access policy as
C ¼ ABE :EncryptðM SG; X Þ:
ð12Þ
The user also constructs a claim policy Y to enable the cloud
to authenticate the user. The creator does not send the
message M SG as is, but uses the time stamp  and creates
HðC Þk . This is done to prevent replay attacks. If the time
stamp is not sent, then the user can write previous stale
message back to the cloud with a valid signature, even
when its claim policy and attributes have been revoked. The
original work by Maji et al. [24] suffers from replay attacks.
In their scheme, a writer can send its message and correct
signature even when it no longer has access rights. In our
scheme a writer whose rights have been revoked cannot
create a new signature with new time stamp and, thus,
cannot write back stale information.
then signs the
It
message and calculates the message signature as

390

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 2, FEBRUARY 2014

Later when a valid user, say Bob wants to modify any of
these reports he also attaches a set of claims which the cloud
verifies. For example, Bob is a research chair and might
send a claim “Research chair” or “Department head” which
is then verified by the cloud. It then sends the encrypted
data to the Bob. Since Bob is a valid user and has matching
attributes, he can decrypt and get back the information.
If Bob wants to read the contents without modifying
them, then there is no need to attach a claim. He will be able
to decrypt only if he is a Professor in University X or a
Research chair in one of the universities X ; Y ; Z or a student
belonging to Department of Law in university X .
Here it is to be noted that the attributes can belong to
several KDCs. For example, the Professors belonging to
university X have credentials given by the university X ,
and the Ph.D. degree from a University P, the student
counselor might be a psychologist authorized by the
Canadian Psychological Association and assigned an
employee number by a university,
the research chairs
can be jointly appointed by the universities X , Y , Z and
the government. The students can have credentials from
the university and also a department.
the
for example,
Initially, Alice goes to a trustee,
Canadian health service and presents her a health insurance
number or federal agency presents her a social insurance
number. Either or both of these trustees can give her
token(s)  ¼ ðu; Kbase ; K0 ; Þ. With the token she approaches
the KDCs in the university X and department D and
obtains the secret keys for decryption and for keys Kx and
Ky for signing the assess policy. She can also access the
public keys AP K ½i of other KDCs. The entire process is
carried on in the following way:

5.1 Data Storage in Clouds
Let the data be denoted by M SG, X is the access policy

((Prof AND Uni. X) OR
(Research Chair AND ((Uni X OR Uni Y) OR Uni Z)) OR
((Student AND Dept Law)AND Uni X).

Alice encrypts the data and obtains the ciphertext
C ¼ EncðM SG; X Þ:
Alice also decides on a claim policy Y which is shown in
Fig. 2. From the matrix, v ¼ ð1; 1; 0; 0; 0Þ can be calculated.
The values of Y ; W ; S1 ; S2 ; S3 ; S4 ; S5 ; P1 ; P2 can be calcu-
lated.  ¼ HðM SGkY Þ. The current time stamp  is attached
to the ciphertext to prevent replay attacks. The signature 
is calculated as ABS :S ign. The ciphertext
c ¼ ðC ;  ; ; Y Þ:

is then send to the cloud. The cloud verifies the signature
using the function ABS :V erif y as given in (11). If Alice has
valid credentials then the ciphertext ðC ;  Þ is stored, else it
is discarded.

5.2 Reading from the Cloud and Modifying Data
Suppose Bob wants to access the records stored by Alice.
Bob then decrypts the message M SG using his secret keys
using function ABE :Decrypt. Writing proceeds like file
creation. It is to be noted that the time  is added to the data
so that even if Bob’s credentials are revoked, he cannot
write stale data in the cloud.

Fig. 2. Example of claim policy.

malpractices by authorities of University X to all
the
professors of University X , Research chairs of universities
X ; Y ; Z and students belonging to Law department
in
university X . She wants to remain anonymous, while
publishing all evidence. All information is stored in the
cloud. It is important that users should not be able to know
her identity, but must trust that the information is from a
valid source. For this reason she also sends a claim message
which states that she “Is a law student” or “Is a student
counselor” or “Professor at university X .” The tree
corresponding to the claim policy is shown in Fig. 2.
The leaves of the tree consists of attributes and the
intermediary nodes consists of Boolean operators. In this
example the attributes are “Student,” “Prof,” “Dept Law,”
“Uni X,” “Counselor.” The above claim policy can be
written as a Boolean function of attributes as

((Student AND Dept Law) OR (Prof AND Uni X)) OR
(Student Counselor).

Boolean functions can also be represented by access tree,
with attributes at the leaves and ANDð^Þ and ORð_Þ as the
intermediate nodes and root. Boolean functions can be
converted to LSSS matrix as below: Let v½x be parents
vector. If node x ¼ AND, then the left child is ðv½xj1Þ, and
the right child is ð0; . . . ; 1Þ. If x ¼ OR, then both children
also have unchanged vector v½x. Finally, pad with 0s in
front, such that all vectors are of equal length. The proof of
validity of the algorithm is given in [25]. We do not present
it here due to lack of space.
0
1
Using this algorithm, the span program for this policy is
CCCCA:
BBBB@

1
1
0  1
1
1
0  1
1
0
An assignment v ¼ ðv1 ; v2 ; v3 ; v4 ; v5 Þ satisfies this span
program if vM ¼ ð1; 0Þ.
The cloud should verify that Alice indeed satisfies this
claim. Since she is a law student, v ¼ ð1; 1; 0; 0; 0Þ and is a
valid assignment. As a valid user she can then store all the
encrypted records under the set of access policy that she has
decided. The access policy in case of Alice is

M ¼

((Prof AND Uni. X) OR
(Research Chair AND ((Uni X OR Uni Y) OR Uni Z)) OR
((Student AND Dept Law)AND Uni X).

RUJ ET AL.: DECENTRALIZED ACCESS CONTROL WITH ANONYMOUS AUTHENTICATION OF DATA STORED IN CLOUDS

391

6 SECURITY OF THE PROTOCOL

In this section, we will prove the security of the protocol.
We will show that our scheme authenticates a user who
wants to write to the cloud. A user can only write provided
the cloud is able to validate its access claim. An invalid user
cannot receive attributes from a KDC, if it does not have the
credentials from the trustee. If a user’s credentials are
revoked, then it cannot replace data with previous stale
data, thus preventing replay attacks.

Theorem 1. Our access control scheme is secure (no outsider or
cloud can decrypt ciphertexts), collusion resistant and allows
access only to authorized users.

Proof. We first show that no unauthorized user can access
data from the cloud. We will first prove the validity of
our scheme. A user can decrypt data if and only if it has a
matching set of attributes. This follows from the fact that
P
access structure S (and hence matrix R) is constructed if
and only if there exists a set of rows X 0 in R, and linear
constants cx 2 ZZq , such that
x2X 0 cxRx ¼ ð1; 0; . . . ; 0Þ. A
proof of this appear in [25, Chapter 4].
We note that
decðxÞ ¼ C1;x eðH ðuÞ; C3;x Þ
eðskðxÞ;u ; C2;x Þ ¼ eðg; gÞx eðH ðuÞ; gÞ!x :

ð14Þ

Thus,

ð15Þ

x2X 0 decðxÞ
¼ x2X 0 ðeðg; gÞx eðH ðuÞ; gÞ!x Þcx
¼ eðg; gÞs :
Equation (15) above holds because x ¼ Rx  v and
!x ¼ Rx  w, where v  ð1; 0; . . . ; 0Þ ¼ r and !  ð1; 0; . . . ;
0Þ ¼ 0. C0 =x2X 0 decðxÞ ¼ C0 =eðg; gÞs ¼ M .
P
For an invalid user, there does not exists attributes
x2X 0 cxRx ¼
correspond ing to rows x, such tha t
ð1; 0; . . . ; 0Þ. Thus, eðg; gÞs cannot be calculated.
We next show that two or more users cannot collude
P
and gain access to data that they are not individually
supposed to access. Suppose that there exist attributes
x2X cxRx ¼
ðxÞ f r om t h e c o l l ud e r s , su c h th a t
ð1; 0; . . . ; 0Þ. However, eðH ðuÞ; gÞ!x needs to be calculated
according to (15). Since different users have different
values of eðH ðuÞ; gÞ, even if they combine their attributes,
they cannot decrypt the message.
We next observe that the cloud cannot decode stored
data. This is because it does not posses the secret keys
ski;u (by (3)). Even if it colludes with other users, it
cannot decrypt data which the users cannot themselves
decrypt, because of the above reason (same as collusion
of users). The KDCs are located in different servers and
are not owned by the cloud. For this reason, even if some
(but not all) KDCs are compromised, the cloud cannot
tu
decode data.
Theorem 2. Our authentication scheme is correct, collusion
secure, resistant to replay attacks, and protects privacy of
the user.

Proof. We first note that only valid users registered with the
trustee(s) receive attributes and keys from the KDCs. A

TABLE 2
Notations

user’s token is  ¼ ðu; Kbase ; K0 ; Þ, where  is signature
on ukKbase with T S ig belonging to the trustee. An invalid
user with a different user-id cannot create the same
signature because it does not know T S ig.
We next show that only a valid user with valid
access claim is only able to store the message in the
cloud. This follows from the functions ABS :S ign and
ABS :V erif y given in Section 3.5. A user who wants to
create a file and tries to make a false access claim,
cannot do so, because it will not have attribute keys Kx
from the related KDCs. At the same time since the
message is encrypted, a user without valid access
policy cannot decrypt and change the information.
Two users cannot collude and create an access policy
consisting of attributes shared between them. Suppose,
there are two users A and B who have attributes xA and
xB , respectively. They have the following information
KbaseA ; KxA and KbaseB ; KxB , respectively. A new value of
KxB ¼ K 1=ðaþbx0 Þ
cannot be calculated by B, because it
does not know the values of ða; bÞ. Thus, the authentica-
baseA
tion is collusion secure.
Our scheme is resistant to replay attacks. If a writer’s
access claims are revoked, it cannot replace a data with
stale information from previous writes. This is because it
has to attach a new time stamp  and sign the message
HðC Þk again. Since it does not have attributes, it cannot
tu
have a valid signature.

The mathematical proofs of security of our scheme
follows from the security proofs of [23], [35] and has been
omitted for the lack of space.

7 COMPUTATION COMPLEXITY

In this section, we present the computation complexity of
the privacy preserving access control protocol. We will
calculate the computations required by users (creator,
reader, writer) and that by the cloud. Table 2 presents
notations used for different operations.
The creator needs to encrypt the message and sign it.
Creator needs to calculate one pairing eðg; gÞ. Encryption
takes two exponentiations to calculate each of C1;x . So this
requires 2mET time, where m is the number of attributes.
User needs to calculate three exponentiation to calculate C2;x
and C3;x . So time taken for encryption is ð3m þ 1ÞE0 þ
2mET þ P . To sign the message, Y ; W ; S 0
i s and Pj s have to
be calculated as well as HðC Þ. So, time taken to sign is
ð2l þ 2ÞE1 þ 2tE2 þ H .
The cloud needs to verify the signature. This requires
checking for (11). Time taken to verify is ðl þ 2tÞ ^P þ
lðE1 þ E2 Þ þ H . To read, a user needs only to decrypt the

392

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 2, FEBRUARY 2014

TABLE 3
Comparison of Our Scheme with Existing Access Control Schemes

TABLE 4
Comparison of Computation and Size of Ciphertext While Creating a File

TABLE 5
Comparison of Computation during Read and Write by User and Cloud

ciphertext. This requires 2m pairings to calculate eðH ðuÞ;
C3;x Þ and eðskðxÞ;u ; C2;x Þ and OðmhÞ to find the vector c.
Decryption takes 2mP þ H þ OðmhÞ. Writing is similar to
creating a record. The size of ciphertext with signature is
2mjG0 j þ mjGT j þ m2 þ jM SGj þ ðl þ t þ 2ÞjG1 j.
When revocation is required, C0 needs to be recalculated.
eðg; gÞ is previously calculated. So, only one scalar multi-
plication is needed. If the user revoked is Uu , then for each
x, C1;x has to be recomputed. eðg; gÞ is already computed.
Thus, only two scalar multiplication needs to be done, for
each x. So a total of 2m0 þ 1 scalar multiplications are done
by the cloud, where m0
is the number of attributes
belonging to all revoked users. Users need not compute
any scalar multiplication or pairing operations. Additional
communication overhead is Oððm0 þ 1ÞjGT jÞ.
The curves chosen are either MNT curves (proposed by
Miyaji, Nakabayashi, and Takano) or supersingular curves.
Considering the requirements, elliptic curve group of size
159, with an embedding degree 6 (type d curves of pairing-
based cryptography (PBC) [36]) can be used. Pairing takes
14 ms on Intel Pentium D, 3.0-GHz CPU [16]. Such
operations are very suitable for a cloud computing
environment. A new library for ABE is also available at [37].

PBC library [36] is a C library which is built above
GNU GMP (GNU Math Precision) library and contains
functions to implement elliptic curves and pairing opera-
tions. Each element of G needs 512 bits at an 80-bit
security level and 1,536 bits when 128-bit of security are
chosen [39]. Each cryptographic operation was implemen-
ted using the PBC library ver. 0.4.18 on a 3.0-GHZ
processor PC. The public key parameters were selected
to provide 80-bit security level. According to [39],
implementation uses 160-bit elliptic curve group on the
supersingular curve y2 ¼ x3 þ x over a 512-bit finite field.
The computational cost for a pairing operation is 2.9 ms
and that of exponentiation on G (and G0 ) and GT (and G2 )
are 1 and 0.2 ms, respectively.
We will compare our computation costs with existing
schemes like [12], [13], [15] in Section 8.

8 COMPARISON WITH OTHER ACCESS CONTROL
SCHEMES IN CLOUD

We compare our scheme with other access control schemes
(in Table 3) and show that our scheme supports many

RUJ ET AL.: DECENTRALIZED ACCESS CONTROL WITH ANONYMOUS AUTHENTICATION OF DATA STORED IN CLOUDS

393

features that the other schemes did not support. 1-W-M-R
means that only one user can write while many users can
read. M-W-M-R means that many users can write and read.
We see that most schemes do not support many writes
which is supported by our scheme. Our scheme is robust
and decentralized, most of the others are centralized. Our
scheme also supports privacy preserving authentication,
which is not supported by others. Most of the schemes do
not support user revocation, which our scheme does. In
Tables 4 and 5, we compare the computation and commu-
nication costs incurred by the users and clouds and show
that our distributed approach has comparable costs to
centralized approaches. The most expensive operations
involving pairings and is done by the cloud. If we compare
the computation load of user during read we see that our
scheme has comparable costs. Our scheme also compares
well with the other authenticated scheme of [15].

9 CONCLUSION

We have presented a decentralized access control technique
with anonymous authentication, which provides user
revocation and prevents replay attacks. The cloud does
not know the identity of the user who stores information,
but only verifies the user’s credentials. Key distribution is
done in a decentralized way. One limitation is that the
cloud knows the access policy for each record stored in the
cloud. In future, we would like to hide the attributes and
access policy of a user.

ACKNOWLEDGMENTS

Th is work is par t ia l ly suppor ted by NSERC Gran t
CRDPJ386874-09 and the grant: “Digital signal processing,
and the synthesis of an information security system,”
TR32054, Serbian Ministry of Science and Education.

[4]

[3]

REFERENCES
S. Ruj, M. Stojmenovic, and A. Nayak, “Privacy Preserving Access
[1]
Control with Authentication for Securing Data in Clouds,” Proc.
IEEE/ACM Int’l Symp. Cluster, Cloud and Grid Computing, pp. 556-
563, 2012.
[2] C. Wang, Q. Wang, K. Ren, N. Cao, and W. Lou, “Toward
Secure and Dependable Storage Services in Cloud Computing,”
IEEE Trans. Services Computing, vol. 5, no. 2, pp. 220-232, Apr.-
June 2012.
J. Li, Q. Wang, C. Wang, N. Cao, K. Ren, and W. Lou, “Fuzzy
Keyword Search Over Encrypted Data in Cloud Computing,”
Proc. IEEE INFOCOM, pp. 441-445, 2010.
S. Kamara and K. Lauter, “Cryptographic Cloud Storage,” Proc.
14th Int’l Conf. Financial Cryptography and Data Security, pp. 136-
149, 2010.
[5] H. Li, Y. Dai, L. Tian, and H. Yang, “Identity-Based Authentica-
tion for Cloud Computing,” Proc. First Int’l Conf. Cloud Computing
(CloudCom), pp. 157-166, 2009.
[6] C. Gentry, “A Fully Homomorphic Encryption Scheme,” PhD
dissertation, Stanford Univ., http://www.crypto.stanford.edu/
craig, 2009.
[7] A.-R. Sadeghi, T. Schneider, and M. Winandy, “Token-Based
Cloud Computing,” Proc. Third Int’l Conf. Trust and Trustworthy
Computing (TRUST), pp. 417-429, 2010.
[8] R.K.L. Ko, P.
Jagadpramana, M. Mowbray, S. Pearson, M.
Kirchberg, Q. Liang, and B.S. Lee, “Trustcloud: A Framework
for Accountability and Trust in Cloud Computing,” HP Technical
Report HPL-2011-38 , http ://www .hp l .hp .com/techreports/
2011/HPL-2011-38.html, 2013.

[9] R. Lu, X. Lin, X. Liang, and X. Shen, “Secure Provenance: The
Essential of Bread and Butter of Data Forensics in Cloud
Computing,” Proc. Fifth ACM Symp. Information, Computer and
Comm. Security (ASIACCS), pp. 282-292, 2010.
[10] D.F. Ferraiolo and D.R. Kuhn, “Role-Based Access Controls,” Proc.
15th Nat’l Computer Security Conf., 1992.
[11] D.R. Kuhn, E.J. Coyne, and T.R. Weil, “Adding Attributes to Role-
Based Access Control,” IEEE Computer, vol. 43, no. 6, pp. 79-81,
June 2010.
[12] M. Li, S. Yu, K. Ren, and W. Lou, “Securing Personal Health
Records in Cloud Computing: Patient-Centric and Fine-Grained
Data Access Control in Multi-Owner Settings,” Proc. Sixth Int’l
ICST Conf. Security and Privacy in Comm. Networks (SecureComm),
pp. 89-106, 2010.
[13] S. Yu, C. Wang, K. Ren, and W. Lou, “Attribute Based Data
Sharing with Attribute Revocation,” Proc. ACM Symp. Information,
Computer and Comm. Security (ASIACCS), pp. 261-270, 2010.
[14] G. Wang, Q. Liu, and J. Wu, “Hierarchical Attribute-Based
Encryption for Fine-Grained Access Control in Cloud Storage
Services,” Proc. 17th ACM Conf. Computer and Comm. Security
(CCS), pp. 735-737, 2010.
[15] F. Zhao, T. Nishide, and K. Sakurai, “Realizing Fine-Grained and
Flexible Access Control to Outsourced Data with Attribute-Based
Cryptosystems,” Proc. Seventh Int’l Conf.
Information Security
Practice and Experience (ISPEC), pp. 83-97, 2011.
[16] S. Ruj, A. Nayak, and I. Stojmenovic, “DACC: Distributed Access
Control in Clouds,” Proc. IEEE 10th Int’l Conf. Trust, Security and
Privacy in Computing and Communications (TrustCom), 2011.
[17] http://docs.oasis-open.org/xacml/3.0/xacml-3.0-core-spec-cs-
01-en.pdf, 2013.
[18] http://securesoftwaredev.com/2012/08/20/xacml-in-the-cloud,
2013.
[19] S. Jahid, P. Mittal, and N. Borisov, “EASiER: Encryption-Based
Access Control in Social Networks with Efficient Revocation,”
Proc. ACM Symp.
Information, Computer and Comm. Security
(ASIACCS), 2011.
[20] R.L. Rivest, A. Shamir, and Y. Tauman, “How to Leak a Secret,”
Proc. Seventh Int’l Conf. Theory and Application of Cryptology and
Information Security (ASIACRYPT), pp. 552-565, 2001.
[21] X. Boyen, “Mesh Signatures,” Proc. 26th Ann. Int’l Conf. Advances
in Cryptology (EUROCRYPT), pp. 210-227, 2007.
[22] D. Chaum and E.V. Heyst, “Group Signatures,” Proc. Ann. Int’l
Conf. Advances in Cryptology (EUROCRYPT), pp. 257-265, 1991.
[23] H.K. Maji, M. Prabhakaran, and M. Rosulek, “Attribute-Based
Signatures: Achieving Attribute-Privacy and Collusion-Resis-
tance,” IACR Cryptology ePrint Archive, 2008.
[24] H.K. Maji, M. Prabhakaran, and M. Rosulek, “Attribute-Based
Signatures,” Topics in Cryptology - CT-RSA, vol. 6558, pp. 376-392,
2011.
[25] A. Beimel, “Secure Schemes for Secret Sharing and Key Distribu-
tion,” PhD thesis, Technion, Haifa, 1996.
[26] A. Sahai and B. Waters, “Fuzzy Identity-Based Encryption,” Proc.
Ann. Int’l Conf. Advances in Cryptology (EUROCRYPT), pp. 457-473,
2005.
[27] V. Goyal, O. Pandey, A. Sahai, and B. Waters, “Attribute-Based
Encryption for Fine-Grained Access Control of Encrypted Data,”
Proc. ACM Conf. Computer and Comm. Security, pp. 89-98, 2006.
J. Bethencourt, A. Sahai, and B. Waters, “Ciphertext-Policy
Attribute-Based Encryption,” Proc.
IEEE Symp. Security and
Privacy, pp. 321-334, 2007.
[29] X. Liang, Z. Cao, H. Lin, and D. Xing, “Provably Secure and
Efficient Bounded Ciphertext Policy Attribute Based Encryption,”
Proc. ACM Symp.
Information, Computer and Comm. Security
(ASIACCS), pp 343-352, 2009.
[30] M. Chase, “Multi-Authority Attribute Based Encryption,” Proc.
Fourth Conf. Theory of Cryptography (TCC), pp. 515-534, 2007.
[31] H. Lin, Z. Cao, X. Liang, and J. Shao, “Secure Threshold Multi-
Authority Attribute Based Encryption without a Central Author-
ity,” Proc. Progress in Cryptology Conf. (INDOCRYPT), pp. 426-436,
2008.
[32] M. Chase and S.S.M. Chow, “Improving Privacy and Security in
Multi-Authority Attribute-Based Encryption,” Proc. ACM Conf.
Computer and Comm. Security, pp. 121-130, 2009.
[33] M. Green, S. Hohenberger, and B. Waters, “Outsourcing the
Decryption of ABE Ciphertexts,” Proc. USENIX Security Symp.,
2011.

[28]

394

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 2, FEBRUARY 2014

Amiya Nayak received the BMath degree in
computer science and combinatorics and opti-
mization from the University of Waterloo, ON,
Canada,
in 1981, and the PhD degree in
systems and computer engineering from Carle-
ton University, Ottawa, Canada, in 1991. He has
more than 17 years of industrial experience in
software engineering, avionics and navigation
systems, simulation and system level perfor-
mance analysis. He is in the Editorial Board of
several journals, including IEEE Transactions on Parallel and Distributed
Systems, International Journal of Parallel, Emergent and Distributed
Systems,
International Journal of Computers and Applications, and
EURASIP Journal of Wireless Communications and Networking.
Currently, he is a full professor at the School of Electrical Engineering
the University of Ottawa. His research
and Computer Science at
interests include the area of
fault
tolerance, distributed systems/
algorithms, and mobile ad hoc networks with more than 150 publications
in refereed journals and conference proceedings. He is a senior member
of the IEEE.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

[34] K. Yang, X. Jia, and K. Ren, “DAC-MACS: Effective Data Access
Control
for Multi-Authority Cloud Storage Systems,” IACR
Cryptology ePrint Archive, p. 419, 2012.
[35] A.B. Lewko and B. Waters, “Decentralizing Attribute-Based
Encryption,” Proc. Ann. Int’l Conf. Advances in Cryptology (EURO-
CRYPT), pp. 568-588, 2011.
[36] http://crypto.stanford.edu/pbc/, 2013.
[37] “Libfenc : The Functional Encryption Library ,”http ://code .
google.com/p/libfenc/, 2013.
[38] W. Wang, Z. Li, R. Owens, and B. Bhargava, “Secure and Efficient
Access to Outsourced Data,” Proc. ACM Cloud Computing Security
Workshop (CCSW), 2009.
J. Hur and D. Kun Noh, “Attribute-Based Access Control with
Efficient Revocation in Data Outsourcing Systems,” IEEE Trans.
Parallel and Distributed Systems, vol. 22, no. 7, pp. 1214-1221, July
2011.

[39]

Sushmita Ruj
rece ived the BE degree in
computer science from Bengal Engineering
and Science University, Shibpur, India in 2004,
and the Masters and PhD degrees in computer
science from Indian Statistical Institute, India in
2006 and 2010, respectively. Between 2009 and
2010, she was a Erasmus Mundus Post Doctoral
Fellow at Lund University, Sweden and between
2010-2012, she was a Post Doctoral Fellow at
University of Ottawa, Canada. She was an
Assistant professor at
Indian Institute of Technology,
IIT,
Indore
between 2012-2013 and is currently an Assistant professor at Indian
Statistical Institute, Kolkata, India. Her research interests are in security
in mob ile ad hoc networks, veh icular networks, cloud security,
combinatorics and cryptography. She is on the Editorial Board of Ad
Hoc and Sensor Wireless Networks. Sushmita has served as Program
Co-Chair
for
ICDCS workshop, NFSP’2013,
ICC WorkshopSecure
Networking and Forensic Computing (SNFC) and has served as TPC
member for many conferences like Indocrypt, IEEE Globecom, IEEE
ICC, IEEE MASS, IEEE PIMRC, ICDCN and many others. She won best
paper awards at ISPA 2007 and IEEE PIMRC 2011. She is a member of
the IEEE.

Milos Stojmenovic received the bachelor of
computer science degree at
the School of
Information Technology and Engineering, Uni-
versity of Ottawa, ON, Canada, in 2003, the
master’s degree in computer science at Carleton
University in Ottawa, Canada, in 2005, and the
PhD degree in the same field at the University of
Ottawa, Canada, in 2008. Currently, he is an
assistant professor at the Singidunum Univer-
sity, Serbia. He was a visiting researcher at
Japans National Institute of Advanced Industrial Science and Technol-
ogy in 2009. He published more than 30 articles in the fields of computer
vision, image processing, and wireless networks. His work implements
machine learning techniques such as AdaBoost and SVM classification
which in turn use higher order autocorrelation features to perform image
segmentation and classification. He is a member of the IEEE.

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 2, FEBRUARY 2014

363

A Scalable Two-Phase Top-Down
Specialization Approach for Data
Anonymization Using MapReduce on Cloud

Xuyun Zhang, Laurence T. Yang, Sen ior Member, IEEE, Chang L iu, and J in jun Chen, Member, IEEE

Abstract—A large number of cloud services require users to share private data like electronic health records for data analysis or
mining, bringing privacy concerns. Anonymizing data sets via generalization to satisfy certain privacy requirements such as k-
anonymity is a widely used category of privacy preserving techniques. At present, the scale of data in many cloud applications
increases tremendously in accordance with the Big Data trend, thereby making it a challenge for commonly used software tools to
capture, manage, and process such large-scale data within a tolerable elapsed time. As a result, it is a challenge for existing
anonymization approaches to achieve privacy preservation on privacy-sensitive large-scale data sets due to their insufficiency of
scalability. In this paper, we propose a scalable two-phase top-down specialization (TDS) approach to anonymize large-scale data sets
using the MapReduce framework on cloud. In both phases of our approach, we deliberately design a group of innovative MapReduce
jobs to concretely accomplish the specialization computation in a highly scalable way. Experimental evaluation results demonstrate
that with our approach, the scalability and efficiency of TDS can be significantly improved over existing approaches.

Index Terms—Data anonymization, top-down specialization, MapReduce, cloud, privacy preservation

Ç

1 INTRODUCTION
C LOUD computing, a disruptive trend at present, poses a
significant impact on current IT industry and research
communities [1],
[2],
[3]. Cloud computing provides
massive computation power and storage capacity via
utilizing a large number of commodity computers together,
enabling users to deploy applications cost-effectively with-
out heavy infrastructure investment. Cloud users can
reduce huge upfront investment of IT infrastructure, and
concentrate on their own core business. However, numer-
ous potential customers are still hesitant to take advantage
of cloud due to privacy and security concerns [4], [5].
The research on cloud privacy and security has come to the
picture [6], [7], [8], [9].
Privacy is one of the most concerned issues in cloud
computing, and the concern aggravates in the context of
cloud computing although some privacy issues are not new

. X. Zhang is with the School of Computer Science and Technology,
Huazhong University of Science and Technology, Wuhan 430074, China,
and the Faculty of Engineering and IT, University of Technology, PO Box
123, Broadway, Sydney, NSW 2007, Australia.
E-mail: xyzhanggz@gmail.com.
. L.T. Yang is with the School of Computer Science and Technology,
Huazhong University of Science and Technology, Wuhan 430074, China,
and the Department of Computer Science, St. Francis Xavier University,
Annex 11B, Antigonish, NS B2G 2W5, Canada.
E-mail: ltyang@stfx.ca.
. C. Liu and J. Chen are with the Faculty of Engineering and IT, University
of Technology, PO Box 123, Broadway, Sydney, NSW 2007, Australia.
E-mail: {changliu.it, jinjun.chen}@gmail.com.

Manuscript received 16 Sept. 2012; revised 19 Jan. 2013; accepted 6 Feb. 2013;
published online 22 Feb. 2013.
Recommended for acceptance by X. Li, P. McDaniel, R. Poovendran, and
G. Wang.
For information on obtaining reprints of this article, please send e-mail to:
tpds@computer.org, and reference IEEECS Log Number
TPDSSI-2012-09-0909.
Digital Object Identifier no. 10.1109/TPDS.2013.48.

[1], [5]. Personal data like electronic health records and
financial transaction records are usually deemed extremely
sensitive although these data can offer significant human
benefits if they are analyzed and mined by organizations
such as disease research centres. For instance, Microsoft
HealthVault [10], an online cloud health service, aggregates
data from users and shares the data with research institutes.
Data privacy can be divulged with less effort by malicious
cloud users or providers because of the failures of some
traditional privacy protection measures on cloud [5]. This
can bring considerable economic loss or severe social
reputation impairment to data owners. Hence, data privacy
issues need to be addressed urgently before data sets are
analyzed or shared on cloud.
Data anonymization has been extensively studied and
widely adopted for data privacy preservation in noninter-
active data publishing and sharing scenarios [11]. Data
anonymization refers to hiding identity and/or sensitive
data for owners of data records. Then, the privacy of an
individual can be effectively preserved while certain
aggregate information is exposed to data users for diverse
analysis and mining. A variety of anonymization algorithms
with different anonymization operations have been pro-
posed [12], [13], [14], [15]. However, the scale of data sets
that need anonymizing in some cloud applications increases
tremendously in accordance with the cloud computing and
Big Data trends [1], [16]. Data sets have become so large that
anonymizing such data sets is becoming a considerable
challenge for traditional anonymization algorithms. The
researchers have begun to investigate the scalability
problem of large-scale data anonymization [17], [18].
Large-scale data processing frameworks like MapReduce
[19] have been integrated with cloud to provide powerful
computation capability for applications. So, it is promising

1045-9219/14/$31.00 ß 2014 IEEE

Published by the IEEE Computer Society

364

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 2, FEBRUARY 2014

to adopt such frameworks to address the scalability
problem of anonymizing large-scale data for privacy
preservation. In our research, we leverage MapReduce, a
widely adopted parallel data processing framework, to
address the scalability problem of the top-down specializa-
tion (TDS) approach [12] for large-scale data anonymiza-
tion. The TDS approach, offering a good tradeoff between
data utility and data consistency, is widely applied for data
anonymization [12], [20], [21], [22]. Most TDS algorithms are
centralized, resulting in their inadequacy in handling large-
scale data sets. Although some distributed algorithms have
been proposed [20], [22], they mainly focus on secure
anonymization of data sets from multiple parties, rather
than the scalability aspect. As the MapReduce computation
paradigm is relatively simple, it is still a challenge to design
proper MapReduce jobs for TDS.
In this paper, we propose a highly scalable two-phase
TDS approach for data anonymization based on MapReduce
on cloud. To make full use of the parallel capability of
MapReduce on cloud, specializations required in an anon-
ymization process are split into two phases. In the first one,
original data sets are partitioned into a group of smaller data
sets, and these data sets are anonymized in parallel,
producing intermediate results. In the second one, the
intermediate results are integrated into one, and further
anonymized to achieve consistent k-anonymous [23] data
sets. We leverage MapReduce to accomplish the concrete
computation in both phases. A group of MapReduce jobs is
deliberately designed and coordinated to perform speciali-
zations on data sets collaboratively. We evaluate our
approach by conducting experiments on real-world data
sets. Experimental results demonstrate that with our
approach, the scalability and efficiency of TDS can be
improved significantly over existing approaches.
The major contributions of our research are threefold.
First, we creatively apply MapReduce on cloud to TDS for
data anonymization and deliberately design a group of
innovative MapReduce jobs to concretely accomplish the
specializations in a highly scalable fashion. Second, we
propose a two-phase TDS approach to gain high scalability
via allowing specializations to be conducted on multiple
data partitions in parallel during the first phase. Third,
experimental results show that our approach can signifi-
cantly improve the scalability and efficiency of TDS for data
anonymization over existing approaches.
The remainder of this paper is organized as follows:
The next section reviews related work, and analyzes the
scalability problem in existing TDS algorithms. In Section 3,
we briefly present preliminary for our approach. Section 4
formulates the two-phase TDS approach, and Section 5
elaborates algorithmic details of MapReduce jobs. We
empirically evaluate our approach in Section 6. Finally, we
conclude this paper and discuss future work in Section 7.

2 RELATED WORK AND PROBLEM ANALYSIS
2.1 Related Work
Recently, data privacy preservation has been extensively
investigated [11]. We briefly review related work below.

LeFevre et al. [17] addressed the scalability problem of
anonymization algorithms via introducing scalable decision
trees and sampling techniques. Iwuchukwu and Naughton
[18] proposed an R-tree index-based approach by building a
spatial
index over data sets, achieving high efficiency.
However, the above approaches aim at multidimensional
generalization [15], thereby failing to work in the TDS
approach. Fung et al. [12], [20], [21] proposed the TDS
approach that produces anonymous data sets without the
data exploration problem [11]. A data structure Taxonomy
Indexed PartitionS (TIPS)
is exploited to improve the
efficiency of TDS. But the approach is centralized, leading
to its inadequacy in handling large-scale data sets.
Several distributed algorithms are proposed to preserve
privacy of multiple data sets retained by multiple parties.
Jiang and Clifton [24] and Mohammed et al. [22] proposed
distributed algorithms to anonymize vertically partitioned
data from different data sources without disclosing privacy
information from one party to another. Jurczyk and Xiong
[25] and Mohammed et al.
[20] proposed distributed
algorithms to anonymize horizontally partitioned data sets
retained by multiple holders. However, the above distrib-
uted algorithms mainly aim at securely integrating and
anonymizing multiple data sources. Our research mainly
focuses on the scalability issue of TDS anonymization, and
is, therefore, orthogonal and complementary to them.
As to MapReduce-relevant privacy protection, Roy et al.
[26]
investigated the data privacy problem caused by
MapReduce and presented a system named Airavat in-
corporating mandatory access control with differential
privacy. Further, Zhang et al. [27] leveraged MapReduce
to automatically partition a computing job in terms of data
security levels, protecting data privacy in hybrid cloud. Our
research exploits MapRedue itself to anonymize large-scale
data sets before data are further processed by other
MapReduce jobs, arriving at privacy preservation.

2.2 Problem Analysis
We analyze the scalability problem of existing TDS
approaches when handling large-scale data sets on cloud.
The centralized TDS approaches in [12], [20], and [21]
exploits the data structure TIPS to improve the scalability
and efficiency by indexing anonymous data records and
retaining statistical information in TIPS. The data structure
speeds up the specialization process because indexing
structure avoids frequently scanning entire data sets and
storing statistical results circumvents recomputation over-
heads. On the other hand, the amount of metadata retained
to maintain the statistical information and linkage informa-
tion of record partitions is relatively large compared with
data sets themselves,
thereby consuming considerable
memory. Moreover, the overheads incurred by maintaining
the linkage structure and updating the statistic information
will be huge when date sets become large . Hence ,
centralized approaches probably suffer from low efficiency
and scalability when handling large-scale data sets.
There is an assumption that all data processed should
fit
in memory for the centralized approaches [12] .
Unfortunately, this assumption often fails to hold in most
data-intensive cloud applications nowadays.
In cloud
environments, computation is provisioned in the form of

ZHANG ET AL.: A SCALABLE TWO-PHASE TOP-DOWN SPECIALIZATION APPROACH FOR DATA ANONYMIZATION USING MAPREDUCE ON...

365

virtual machines (VMs). Usually, cloud compute services
offer several flavors of VMs. As a result, the centralized
approaches are difficult in handling large-scale data sets
well on cloud using just one single VM even if the VM has
the highest computation and storage capability.
A distributed TDS approach [20] is proposed to address
the distributed anonymization problem which mainly
concerns privacy protection against other parties, rather
than scalability issues. Further, the approach only employs
information gain, rather than its combination with privacy
loss, as the search metric when determining the best
specializations. As pointed out in [12], a TDS algorithm
without considering privacy loss probably chooses a
specialization that leads to a quick violation of anonymity
requirements. Hence,
the distributed algorithm fails to
produce anonymous data sets exposing the same data
utility as centralized ones. Besides, the issues like commu-
nication protocols and fault tolerance must be kept in mind
when designing such distributed algorithms. As such, it is
inappropriate to leverage existing distributed algorithms to
solve the scalability problem of TDS.

3 PRELIMINARY
3.1 Basic Notations
We describe several basic notations for convenience. Let D
denote a data set containing data records. A record r 2 D
has the form r ¼ hv1 ; v2 ; . . . ; vm ; svi, where m is the number
of attributes, vi , 1  i  m, is an attribute value and sv is a
sensitive value like diagnosis. The set of sensitive values is
denoted as SV . An attribute of a record is denoted as Attr,
and the taxonomy tree of this attribute is denoted as T T . Let
DOM represent the set of all domain values in T T . The
quasi-identifier of a record is denoted as qid ¼ hq1 ; q2 ; . . . ;
qm i, where qi 2 DOMi . Quasi-identifiers , representing
groups of anonymous records, can lead to privacy breach
if they are too specific that only a small group of people are
linked to them [11]. Quasi-identifier set is denoted as
QID ¼ hAttr1 ; Attr2 ; . . . ; Attrm i. The set of the records with
qid is defined as QI-group [28], denoted by QIGðqidÞ. QI is
the acronym of quasi-identifier.
Without loss of generality, we adopt k-anonymity [23] as
the privacy model herein, i.e., for any qid 2 QID, the size
of GðqidÞ must be zero or at
least k. Otherwise,
the
individuals owning such a quasi-identifier can be linked to
sensitive information with higher confidence than ex-
pected, resulting in privacy breach. The k-anonymity
privacy model can combat such a privacy breach because
it ensures that an individual will not be distinguished from
other at least k   1 ones. The anonymity parameter k is
specified by users according to their privacy requirements.
In the TDS approach, a data set is anonymized via
performing specialization operations. A specialization
operation is to replace a domain value with all its child
values. Formally, a specialization operation is represented
as spec : p ! ChildðpÞ, where p is a domain value and
ChildðpÞ  DOM is the set of all the child values of p. The
domain values of an attribute form a “cut” through its
taxonomy tree [11]. The cut of attribute Attri , denoted as
Cuti , 1  i  m,
is a subset of values in DOMi  Cuti
contains exactly one value in each root-to-leaf path in

taxonomy tree T Ti . The cuts of all attributes determine the
anonymity of a data set. To capture the degree of
anonymization intuitively during the specialization pro-
cess, we give the subsequent definition.

Definition 1. (Anonymization Level). A vector of cuts of all
attributes is defined as anonymization level, denoted as AL.
Formally, AL ¼ hCut1 ; Cut2 ; . . . ; Cutm i, where Cuti , 1 
i  m is the cut of taxonomy tree T Ti .

Anonymization level can intuitively represent the anon-
ymization degree of a data set, i.e., the more specific AL a
data set has, the less degree of anonymization it corre-
sponds to. Thus, TDS approaches employ anonymization
level to track and manage the specialization process.
MapReduce notations can be found in Appendix A.1,
which can be found on the Computer Society Digital
Library at http://doi.ieeecomputersociety.org/10.1109/
TPDS.2013.48, (All appendices are included in the supple-
mental file).

3.2 Top-Down Specialization
Generally, TDS is an iterative process starting from the
topmost domain values in the taxonomy trees of attributes.
Each round of iteration consists of three main steps, namely,
finding the best specialization, performing specialization
and updating values of the search metric for the next round
[12]. Such a process is repeated until k-anonymity is
violated, to expose the maximum data utility. The goodness
of a specialization is measured by a search metric. We adopt
the information gain per privacy loss (IGPL), a tradeoff
metric that considers both the privacy and information
requirements, as the search metric in our approach. A
specialization with the highest IGPL value is regarded as
the best one and selected in each round. We briefly describe
how to calculate the value of IGPL subsequently to make
readers understand our approach well. Interested readers
can refer to [11] for more details.
Given a specialization spec : p ! ChildðpÞ, the IGPL of
the specialization is calculated by
IGP LðspecÞ ¼ IGðspecÞ=ðP LðspecÞ þ 1Þ:
ð1Þ
The term IGðspecÞ is the information gain after perform-
ing spec, and P LðspecÞ is the privacy loss. IGðspecÞ and
P LðspecÞ can be computed via statistical
information
derived from data sets. Let Rx denote the set of original
records containing attribute values that can be generalized
to x. jRx j is the number of data records in Rx . Let I ðRx Þ be


the entropy of Rx . Then, IGðspecÞ is calculated by
X
jRc j
jRp j
c2ChildðpÞ
Let jðRx ; svÞj denote the number of the data records with




sensitive value sv in Rx . I ðRx Þ is computed by
X
jRx ; svj
jðRx ; svÞj
jRx j
jRx j

IGðspecÞ ¼ I ðRp Þ  

I ðRx Þ ¼  

:

ð3Þ

I ðRc Þ;

ð2Þ

sv2SV

: log2

The anonymity of a data set is defined by the minimum
group size out of all QI-groups, denoted as A, i.e., A ¼
minqid2QID fjQIGðqidÞjg, where jQIGðqidÞj
is the size of

4 TWO-PHASE TOP-DOWN SPECIALIZATION
(TPTDS)

ð4Þ

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 2, FEBRUARY 2014
366
T opj 2 DOMj , 1  j  m, is the topmost domain value in
QIGðqidÞ. Let Ap ðspecÞ denote the anonym ity before
performing spec, while Ac ðspecÞ be that after performing
T Ti . AL0
i is the resultant intermediate anonymization level.
intermediate anonymization
In the second phase, all
spec. Privacy loss caused by spec is calculated by
levels are merged into one. The merged anonymization
P LðspecÞ ¼ Ap ðspecÞ   Ac ðspecÞ:
level is denoted as ALI . The merging process is formally
represen ted a s fun c t ion mergeðhAL0
p iÞ !
1 ; AL0
2 ; . . . ; AL0
ALI . The function will be detailed in Section 4.3. Then,
the whole data set D is further anonymized based on ALI ,
achieving k-anonymity finally, i.e., MRT DSðD; k; ALI Þ !
AL , where AL denotes the final anonymization level.
Ultimately, D is concretely anonymized according to AL ,
described in Section 4.4. Above all, Algorithm 1 depicts the
sketch of the two-phase TDS approach.

The sketch of
the TPTDS approach is elaborated in
Section 4.1. Three components of the TPTDS approach,
namely, data partition, anonymization level merging, and
data specialization are detailed in Sections 4.2, 4.3, and
4.4, respectively.

4.1 Sketch of Two-Phase Top-Down Specialization
We propose a TPTDS approach to conduct the computation
required in TDS in a highly scalable and efficient fashion.
The two phases of our approach are based on the two levels
of parallelization provisioned by MapReduce on cloud.
Basically, MapReduce on cloud has two levels of paralle-
lization, i.e., job level and task level. Job level parallelization
means that multiple MapReduce jobs can be executed
simultaneously to make full use of cloud infrastructure
resources. Combined with cloud, MapReduce becomes
more powerful and elastic as cloud can offer infrastructure
resources on demand ,
for example, Amazon E lastic
MapReduce service [29]. Task level parallelization refers
to that multiple mapper/reducer tasks in a MapReduce job
are executed simultaneously over data splits. To achieve
high scalability, we parallelizing multiple jobs on data
partitions in the first phase, but the resultant anonymization
levels are not
identical. To obtain finally consistent
anonymous data sets, the second phase is necessary to
integrate the intermediate results and further anonymize
entire data sets. Details are formulated as follows.
In the first phase, an original data set D is partitioned
into smaller ones. Let Di , 1  i  p, denote the data sets
P
partitioned from D the, where p is the number of partitions,
and D ¼
i¼1 Di , Di \ Dj ¼ , 1  i < j  p. The details of
p
how to partition D will be discussed in Section 4.2.
Then, we run a subroutine over each of the partitioned
data sets in parallel to make full use of the job level
parallelization of MapReduce. The subroutine is a MapRe-
duce version of centralized TDS (MRTDS) which concretely
conducts the computation required in TPTDS. MRTDS
anonymizes data partitions to generate intermediate anon-
ymization levels. An intermediate anonymization level
means that further specialization can be performed without
violating k-anonymity. MRTDS only leverages the task level
parallelization of MapReduce. More details of MRTDS will
b e e l a b o r a t ed in S e c t i on 5 . Fo rm a l l y ,
l e t
fun c t i on
MRT DS ðD; k; ALÞ ! AL0 represent a MRTDS routine that
anonymizes data set D to satisfy k-anonymity from anon-
ymization level AL to AL0 . Thus, a series of functions
MRT DS ðDi ; kI ; AL0 Þ ! AL0
i , 1  i  p, are executed simul-
taneously in the first phase. The term kI denotes the
intermediate anonymity parameter, usually given by appli-
cation domain experts. Note that kI should satisfy kI  k to
ensure privacy preservation. AL0 is the initial anonymiza-
tion level, i.e., AL0 ¼ hfT op1 g; fT op2 g; . . . ; fT opm gi, where

ALGORITHM 1. SKETCH OF TWO-PHASE TDS (TPTDS).
Input: Data set D, anonymity parameters k, kI and
the number of partitions p.
Output: Anonymous data set D .
1: Partition D into Di ,1  i  p.
2: Execute MRT DS ðDi ; kI ; AL0 Þ ! AL0
i , 1  i  p in
parallel as multiple MapReduce jobs.
3: Merge all intermediate anonymization levels into one,
p Þ ! ALI .
mergeðAL0
1 ; AL0
2 ; . . . ; AL0
4: Execute MRT DS ðD; k; ALI Þ ! AL to achieve
k-anonymity.
5: Specialize D according to AL , Output D .
In essential, TPTDS divides specialization operations
required for anonymization into the two phases. Let SP1i ,
1  i  p, denote the specialization sequence on Di in the
first phase, i.e., SP1i ¼ hspeci1 ; speci2 ; . . . ; speciji i, where ji is
the number of specializations. The first common subse-
quence of SP1i , 1  i  p, is denoted as SP I . Let SP2 denote
the specialization sequence in the second phase. SP2 is
determined by ALI rather than kI . Specifically, more
specific ALI implies smaller SP2 . Throughout TPTDS, the
specializations in the set SP I [ SP2 come into effect for
S
anonymization. The specializations in the set SP Extra ¼
ð
i¼1 SP1i Þ=SP I are extra overheads introduced by TPTDS.
p
The influence of p and kI on the efficiency is analyzed as
follows. Greater p means higher degree of parallelization in
the first phase, and less kI indicates more computation is
conducted in the first phase. Thus, greater p and less kI can
improve the efficiency. However, greater p and less kI
S
probably lead to larger SP Extra , thereby degrading the
overall efficiency. Usually, greater p causes smaller SP I and
p
i¼1 SP1i , and less kI result in larger SP1i .
larger
The basic idea of TPTDS is to gain high scalability by
making a tradeoff between scalability and data utility. We
expect that slight decrease of data utility can lead to high
scalability. The influence of p and kI on the data utility is
analyzed as follows. The data utility produced via TPTDS is
roughly determined by SP I [ SP2 . Greater p means that the
specializations in SP I are selected according to IGPL values
from smaller data sets, resulting in exposing less data
utility. However, greater p also implies smaller SP I but
larger SP2 , which means more data utility can be produced
because specializations in SP2 are selected according an
entire data set. Larger kI indicates larger SP2 , generating
more data utility.
In terms of the above analysis, the optimization of the
trade-off between scalability and data utility can be fulfilled

ZHANG ET AL.: A SCALABLE TWO-PHASE TOP-DOWN SPECIALIZATION APPROACH FOR DATA ANONYMIZATION USING MAPREDUCE ON...

367

by tuning p and kI . It is hard to quantitatively formulate the
relationships between TPTDS performance and the two
parameters due to they are data set content specific. But
users can leverage the qualitative relationships analyzed
above to tune performance heuristically.

4.2 Data Partition
When D is partitioned into Di , 1  i  p, it is required that the
distribution of data records in Di is similar to D. A data record
here can be treated as a point in an m-dimension space,
where m is the number of attributes. Thus, the intermediate
anonymization levels derived from Di , 1  i  p, can be more
similar so that we can get a better merged anonymization
level. Random sampling technique is adopted to partition D,
which can satisfy the above requirement. Specifically, a
random number rand, 1  rand  p, is generated for each
data record. A record is assigned to the partition Drand .
Algorithm 2 shows the MapReduce program of data
partition. Note that the number of Reducers should be equal
to p, so that each Reducer handles one value of rand, exactly
producing p resultant files. Each file contains a random
sample of D.

ALGORITHM 2. DATA PARTITION MAP & REDUCE.
Input: Data record (IDr , r), r 2 D, partition parameter p.
Output: Di , 1  i  p.
Map: Generate a random number rand,
where 1  rand  p; emit (rand, r).
Reduce: For each rand, emit (null, list(r)).
Once partitioned data sets Di , 1  i  p, are obtained, we
run MRT DS ðDi ; kI ; AL0 Þ on these data sets in parallel to
i , 1  i  p.
derive intermediate anonymization levels AL

4.3 Anonymization Level Merging
All intermediate anonymization levels are merged into one
in the second phase. The merging of anonymization levels is
completed by merging cuts. Specifically, let Cuta in AL0
a
and Cutb in AL0
b be two cuts of an attribute. There exist
domain values qa 2 Cuta and qb 2 Cutb that satisfy one of
the three conditions: qa is identical to qb , qa is more general
than qb , or qa is more specific than qb . To ensure that the
merged intermediate anonymization level ALI never
violates privacy requirements, the more general one is
selected as the merged one, for example, qa will be selected
if qa is more general than or identical to qb . For the case of
multiple anonymization levels, we can merge them in the
same way iteratively. The following lemma ensures that
ALI still complies privacy requirements.
i , 1  i  p,
Lemma 1. If intermediate anonymization levels AL0
satisfy kI -anonymity, the merged intermediate anonymization
l ev e l ALI w i l l sa t i s f ie s k 0 -anonym i ty , wh e r e ALI  
mergeðhAL0
p iÞ, k0  kI .
1 ; AL0
2 ; . . . ; AL0

further anonymize the entire data sets to produce final k-
anonymous data sets in the second phase.

4.4 Data Specialization
An original data set D is concretely specialized for
anonymization in a one-pass MapReduce job. After obtain-
ing the merged intermediate anonymization level ALI , we
run MRT DS ðD; k; ALI Þ on the entire data set D, and get the
final anonymization level AL . Then, the data set D is
anonymized by replacing original attribute values in D with
the responding domain values in AL .
the data
Details of Map and Reduce functions of
specialization MapReduce job are described in Algorithm
3. The Map function emits anonymous records and its
count. The Reduce function simply aggregates these anon-
ymous records and counts their number. An anonymous
record and its count represent a QI-group. The QI-groups
constitute the final anonymous data sets.

ALGORITHM 3. DATA SPECIALIZATION MAP & REDUCE.
Input: Data record (IDr , r), r 2 D. ; Anonymization
level AL .
Output: Anonymous record (r , countÞ.
Map: Construct anonymous record r ¼ p1 ; hp2 ; . . . ; pm ; svi,
pi , 1  i  m, is the parent of a specialization in current
P
AL and is also an ancestor of vi in r; emit (r ; count).
Reduce: For each r , sum  
count; emit (r , sum).

5 MAPREDUCE VERSION OF CENTRALIZED TDS

We elaborate the MRTDS in this section. MRTDS plays a
core role in the two-phase TDS approach, as it is invoked in
both phases to concretely conduct computation. Basically, a
practical MapReduce program consists of Map and Reduce
functions, and a Driver that coordinates the macro execution
of jobs. In Section 5.1, we describe the MRTDS Driver. The
Map and Reduce functions are detailed in Sections 5.2 and
5.3. Finally, we present the implementation in Section 5.4.

5.1 MRTDS Driver
Usually, a single MapReduce job is inadequate to accom-
plish a complex task in many applications. Thus, a group of
MapReduce jobs are orchestrated in a driver program to
achieve such an objective. MRTDS consists of MRTDS
Driver and two types of jobs, i.e., IGPL Initialization and
IGPL Update. The driver arranges the execution of jobs.
Algorithm 4 frames MRTDS Driver where a data set is
anonymized by TDS. It is the algorithmic design of function
MRT DS ðD; k; ALÞ ! AL0 mentioned in Section 4.2. Note
that we leverage anonymization level to manage the process
of anonymization. Step 1 initializes the values of informa-
tion gain and privacy loss for all specializations, which can
be done by the job IGPL Initialization.

The proof of Lemma 1 can be found in Appendix B.1,
which is available in the online supplemental material. Our
approach can ensure the degree of data privacy preserva-
tion, as TPTDS produces k-anonymous data sets finally.
Lemma 1 ensures that the first phase produces consistent
anonymous data sets that satisfy higher degree of privacy
preservation than users’ specification. Then, MRTDS can

ALGORITHM 4. MRTDS DRIVER.
Input: Data set D, anonymization level AL and
k-anonymity parameter k.
Output: Anonymization level AL0 .
1: Initialize the values of search metric IGPL, i.e., for each
specialization spec 2 [m
j¼1Cutj . The IGPL value of spec is
computed by job IGPL Initialization.

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 2, FEBRUARY 2014
compute jRp j, jðRp ; svÞj, jRc j, and jðRc ; svÞj. Step 1 gets the
potential specialization for the attribute values in r. Then
Step 2 emits key-value pairs containing the information of
specialization, sensitive value, and the count information of
this record. According to the above information, we
compute information gain for a potential specialization in
the corresponding Reduce function. Step 3 aims at comput-
ing the current anonymity Ap ðspecÞ, while Step 4 is to
compute anonymity Ac ðspecÞ after potential specializations.
The symbol “#” is used to identify whether a key is emitted
to compute information gain or anonymity loss, while the
symbol “$” is employed to differentiate the cases whether a
key is for computing Ap ðspecÞ or Ac ðspecÞ.
Algorithm 6 specifies the Reduce function. The first step is
to accumulate the values for each input key. If a key is for
computing information gain, then the corresponding statis-
tical information is updated in Step 2.1. I ðRp Þ, I ðRc Þ, and
IGðspecÞ are calculated if all the count information they need
has been computed in Steps 2.2 and 2.3 in terms of (2) and (3).
A salient MapReduce feature that intermediate key-value
pairs are sorted in the shuffle phase makes the computation
of IGðspecÞ sequential with respect to the order of specializa-
tions arriving at the same reducer. Hence, the reducer just
needs to keep statistical information for one specialization at
a time, which makes the reduce algorithm highly scalable.

368
2: while 9 spec 2 [m
j¼1Cutj is valid
2.1: Find the best specialization from ALi , specBest .
2.2: Update ALi to ALiþ1 .
2.3: Update information gain of the new specializations
in ALiþ1 , and privacy loss for each specialization via
job IGPL Update.
end while
AL0   AL.
Step 2 is iterative. First, the best specialization is selected
from valid specializations in current anonymization level
as described in Step 2.1. A specialization spec is a valid one
if it satisfies two conditions. One is that its parent value is
not a leaf, and the other is that the anonymity Ac ðspecÞ > k,
i.e., the data set is still k-anonymous if spec is performed.
Then,
is modified via
the current anonymization level
performing the best specialization in Step 2.2, i.e., remov-
ing the old specialization and inserting new ones that are
derived from the old one. In Step 2.3, information gain of
the newly added specializations and privacy loss of all
specializations need to be recomputed, which are accom-
plished by job IGPL Update. The iteration continues until all
specializations become invalid, achieving the maximum
data utility.
MRTDS produces the same anonymous data as the
centralized TDS in [12], because they follow the same steps.
MTRDS mainly differs from centralized TDS on calculating
IGPL values. However, calculating IGPL values dominates
the scalability of TDS approaches, as it requires TDS
algorithms to count the statistical information of data sets
iteratively. MRTDS exploits MapReduce on cloud to make
the computation of IGPL parallel and scalable. We present
IGPL Initialization and IGPL Update subsequently.

5.2 IGPL Initialization Job
The Map and Reduce functions of the job IGPL Initialization
are described in Algorithms 5 and 6, respectively. The main
task of IGPL Initialization is to initialize information gain
and privacy loss of all specializations in the initial
anonymization level AL. According to (2) and (3), the
statistical information jRp j, jðRp ; svÞj, jRc j, and jðRc ; svÞj is
required for each specialization to calculate information
gain. In terms of (4), the number of records in each current
QI-group needs computing, so does the number of records
in each QI-group after potential specializations.

ALGORITHM 5. IGPL INITIALIZATION MAP.
Input: Data record (IDr , r), r 2 D; anonymization level AL.
Output: Intermediate key-value pair (key, countÞ.
1: For each attribute value vi in r, find its specialization in
current AL: spec. Let p be the parent in spec, and c be the
p’s child value that is also an ancestor of vi in T Ti .
2: For each vi , emit (hp; c; svi; count).
3: Construct quasi-identifier qid ¼ hp1 ; p2 ; . . . ; pm i, where
pi , 1  i  m, is the parent of a specialization in current
AL. Emitðhqid ; $; #i; count).
4: For each i 2 ½1; m, replace pi in qid with its child ci ,
where ci is also the ancestor of vi . Let the resultant
quasi-identifier be qid. Emitðhqid; pi ; #i; count).
Algorithm 5 describes the Map function. The input is
data sets that consist of a number of records. IDr is the
sequence number of the record r. Steps 1 and 2 are to

.

ALGORITHM 6. IGPL INITIALIZATION REDUCE.
Input: Intermediate key-value pair (key, listðcountÞ).
Output: Information gain (spec; IGðspecÞ) and anonymity
P
(spec, Ac ðspec)), (spec, Ap ðspec)) for all specializations.
1: For each key, sum  
count.
2: For each key, if key:sv 6¼ #, update statistical counts:


j   sum þ Rc
2.1: jðRc ; svÞj   sum, Rc
j,
j
j
jðRp ; svÞj   sum þ jðRp ; svÞj, jRp j   sum þ Rp
2.2: If all sensitive values for child c have arrived,
compute I ðRc Þ according to (3).
2.3: If all children c for parent p have arrived, compute
I ðRp Þ and IGðspecÞ. Emit (spec; IGðspecÞ).
3: For each key, if key:sv ¼ #, update anonymity.
3.1: If key:c ¼ $ and sum < Ap ðspecÞ, update current
anonymity: Ap ðspecÞ   sum.
3.2: If key:c 6¼ $ and sum < Ac ðspec), update potential
anonymity of spec : Ac ðspecÞ   sum.
4: Emit (spec, Ap ðspec)) and emit (spec, Ac ðspec)).
To compute the anonymity of data sets before and after a
specialization, Step 3.1 finds the smallest number of records
out of all current QI-groups, and Step 3.2 finds all the
smallest number of records out of all potential QI-groups
for each spec ia l izat ion . Step 4 em its the resu lts of
anonymity. Note that there may be more than one key-
value pair (spec, AðspecÞ) for one specialization in output
files if more than one reducer is set. But we can find the
smallest anonymity value in the driver program. Then in
terms of (4), the privacy loss P LðspecÞ is computed. Finally,
IGP LðspecÞ for each specialization is obtained by (1).

5.3 IGPL Update Job
The IGPL Update job dominates the scalability and efficiency
of MRTDS, since it is executed iteratively as described in
Algorithm 4. So far, iterative MapReduce jobs have not been

ZHANG ET AL.: A SCALABLE TWO-PHASE TOP-DOWN SPECIALIZATION APPROACH FOR DATA ANONYMIZATION USING MAPREDUCE ON...

369

well supported by standard MapReduce framework like
Hadoop [30]. Accordingly, Hadoop variations like Haloop
[31] and Twister [32] have been proposed recently to
support efficient iterative MapReduce computation. Our
approach is based on the standard MapReduce framework
to facilitate the discussion herein.
The IGPL Update job is quite similar to IGPL Initialization,
except that it requires less computation and consumes less
network bandwidth. Thus, the former is more efficient than
the latter. Algorithm 7 describes the Map function of IGPL
Update. The Reduce function is the same as IGPL Initializa-
tion, already described in Algorithm 3.

ALGORITHM 7. IGPL UPDATE MAP.
Input: Data record (IDr , r), r 2 D; Anonymization level AL.
Output: Intermediate key-value pair (key, countÞ.
1: Let attr be the attribute of the last best specialization. The
value of this attribute in r is v. Find its specialization in
current AL: spec. Let p be the parent in spec, and c be p’s
child that is also an ancestor of v; Emit (hp; c; svi; count).
2: Construct quasi-identifier qid ¼ hp1 ; p2 ; . . . ; pm i, pi ,
1  i  m, is the parent of a specialization in current AL
and is also an ancestor of vi in r.
3: For each i 2 ½1; m, replace pi in qid with its child ci if the
specialization related to pi is valid, where ci is also the
ancestor of vi . Let the resultant quasi-identifier be qid.
Emit (hqid; pi ; #i; count).
After a specialization spec is selected as the best
candidate, it is required to compute the information gain
for the new specializations derived from spec. So, Step 1 in
Algorithm 7 only emits the key-value pairs for the new
specializations, rather than all in Algorithm 5. Note that it is
unnecessary to recompute the information gain of other
specializations because conducting the selected specializa-
tion never affects the information gain of others. Compared
with IGPL Initialization, only a part of data is processed and
less network bandwidth is consumed.
On the contrary, the anonymity values of other specia-
lizations will be influenced with high probability because
splitting QI-groups according to spec changes the minim-
ality of the smallest QI-group in last round. Therefore, we
need to compute Ac ðspecÞ for all specializations in AL,
described in Step 2 and 3 of Algorithm 7. Yet Ap ðspecÞ can
be directly obtained from the statistical information kept by
the last best specialization. Note that if the specialization
related to pi
in Step 3 is not valid, no resultant quasi-
identifier will be created.
Since the IGPL Update job dominates the scalability and
efficiency of MRTDS, we briefly analyze its complexity as
follows. Let n denote all the records in a data set, m be the
number of attributes, s be the number of mappers, and t be
the number of reducers. As a mapper emits (m þ 1) key-
takes Oð1Þ space and Oðm  n=sÞ time.
value pairs,
it
Similarly, a reducer takes Oð1Þ space and Oðm  n=tÞ time.
Note that a reducer only needs Oð1Þ space due to the
MapReduce feature that the key-value pairs are sorted in
the shuffle phase. Otherwise, the reducer needs more space
to accumulate statistic information for a variety of specia-
lizations. The communication cost is O(m  n) according to
the map function, but communication traffics can be
reduced heavily by optimization techniques like Combiner.

Fig. 1. Execution framework overview of MRTDS.

5.4 Implementation and Optimization
To elaborate how data sets are processed in MRTDS, the
execution framework based on standard MapReduce is
depicted in Fig. 1. The solid arrow lines represent the data
flows in the canonical MapReduce framework. From Fig. 1,
we can see that
the iteration of MapReduce jobs is
controlled by anonymization level AL in Driver. The data
flows for handling iterations are denoted by dashed arrow
lines. AL is dispatched from Driver to all workers including
Mappers and Reducers via the distributed cache mechanism.
The value of AL is modified in Driver according to the
output of the IGPL Initialization or IGPL Update jobs. As the
amount of such data is extremely small compared with data
sets that will be anonymized,
they can be efficiently
transmitted between Driver and workers.
We adopt Hadoop [30], an open-source implementation
of MapReduce, to implement MRTDS. Since most of Map
and Reduce functions need to access current anonymization
level AL, we use the distributed cache mechanism to pass
the content of AL to each Mapper or Reducer node as shown
in Fig. 1. Also, Hadoop provides the mechanism to set
simple global variables for Mappers and Reducers. The best
specialization is passed into the Map function of IGPL
Update job in this way. The partition hash function in the
shuffle phase is modified because the two jobs require that
the key-value pairs with the same key:p field rather than
entire key should go to the same Reducer.
To reduce communication traffics, MRTDS exploits
combiner mechanism that aggregates the key-value pairs
with the same key into one on the nodes running Map
functions. To further reduce the traffics, MD5 (Message
Digest Algorithm) is employed to compress the records
transmitted for anonymity computation, i.e., (hMD5ðqidÞ;
pi ; #i; count) is emitted in Step 4 of Algorithm 5 and Step 3
of Algorithm 7. MD5ðqidÞ is fixed-length and usually
shorter than qid. As anonymity computation causes the
most traffic as it emits m key-value pairs for each original
record, this can considerably reduce network traffics. The
Reduce function in Algorithm 6 can still correctly compute
anonymity without being aware of the content of qid.

6 EVALUATION
6.1 Overall Comparison
To evaluate the effectiveness and efficiency of our two-
phase approach, we compare it with the centralized TDS

370

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 2, FEBRUARY 2014

approach proposed in [12], denoted as CentTDS. CentTDS
is the state-of-the-art approach for TDS anonymization.
Scalability and data utility are considered for the effective-
ness. For scalability, we check whether both approaches
can still work and scale over large-scale data sets. Data
utility is measured by the metric ILoss, a general purpose
data metric proposed in [28]. Literally,
ILoss means
information loss caused by data anonymization. Basically,
higher ILoss indicates less data utility. How to calculate
ILoss can be found in Appendix A.2, which is available in
the online supplemental material. The ILoss of CentTDS
and TPTDS are denoted as ILCent and ILT P , respectively.
The execution time of CentTDS and TPTDS are denoted as
TCent and TT P , respectively.
We roughly compare TPTDS and CentTDS as follows.
TPTDS can scale over more computation nodes with the
volume of data sets increasing, thereby gaining higher
scalability. CentTDS will suffer from low scalability on
large-scale data sets because it requires too much memory,
while TPTDS can linearly scale over data sets of any size.
Correspondingly, TT P is often less than TC ent for large-scale
data sets. But note that, TC ent can be less than TT P due to
extra overheads engendered by TPTDS when the scale of
data sets or the MapReduce cluster is small. TPTDS is
to MRTDS if parameter p ¼ 1 or kI  kmax ,
equivalent
where kmax is the number of all records. As MRTDS
produces the same anonymous data as centralized TDS, the
value of ILT P is equal to ILCent when p ¼ 1 or kI  kmax . In
other cases, ILT P is probably greater than ILC ent , as some
specializations selected in TPTDS are not globally optimal.
The overheads of our approach are mainly introduced by
the MapReduce built-in operations and the parallelization
in the first phase of TPTDS. Built-in MapReduce operations
like data splitting and key-value pair sorting and transmis-
sion will cause overheads. The overheads are hard to
quantitatively measure as they are implementation-, con-
figuration-, and algorithm-specific. The extra specializations
in the first phase incur overheads affecting the efficiency of
TPTDS heavily. The relationship between the extra over-
heads and parameters p and kI is qualitatively described in
Section 4.1. The one-pass job partitioning original data sets
also generates overheads.

6.2 Experiment Evaluation

6.2.1 Experiment Settings
Our experiments are conducted in a cloud environment
named U-Cloud. U-Cloud is a cloud computing environ-
ment at
the University of Technology Sydney (UTS).
The system overview of U-Cloud has been depicted in
Fig. 2. The computing facilities of this system are located
among several labs at UTS. On top of hardware and Linux
operating system (Ubuntu), we install KVM virtualization
software [33] that virtualizes the infrastructure and provides
unified computing and storage resources. To create virtua-
lized data centers, we install OpenStack open source cloud
environment [34] for global management, resource schedul-
ing and interaction with users. Further, Hadoop [30] clusters
are built based on the OpenStack cloud platform to facilitate
large-scale data processing.

Fig. 2. Change of execution time w.r.t. data size: TPTDS versus
CentTDS.

We use Adult data set [35], a public data set commonly
used as a de facto benchmark for testing anonymization
algorithms [12], [20]. We generate data sets by enlarging the
Adult data set according to the approach in [20]. More
details of experiment data are described in Appendix C.1,
which is available in the online supplemental material.
Both TPTDS and CentTDS are implemented in Java.
Further, TPTDS is implemented with standard Hadoop
MapReduce API and executed on a Hadoop cluster built on
OpenStack. CentTDS is executed on a VM with type m1.large.
The m1.large type has four virtual CPUs and 8-GB Memory.
The maximum heap size of Java VM is set as 4 GB when
running CentTDS. The Hadoop cluster consists of 20 VMs
with type m1.medium which has two virtual CPUs and 4-GB
Memory. The k-anonymity parameter is set as 50 throughout
all experiments. Each round of experiment is repeated
20 times. The mean of the measured results is regarded as
the representative.

6.2.2 Experiment Process and Results
We conduct three groups of experiments in this section to
evaluate the effectiveness and efficiency of our approach. In
the first one, we compare TPTDS with CentTDS from the
perspectives of scalability and efficiency. In the other two,
we investigate on the tradeoff between scalability and data
utility via adjusting configurations. Generally, the execution
time and ILoss are affected by three factors, namely, the
size of a data set (S ), the number of data partitions (p), and
the intermediate anonymity parameter (kI ). How the three
factors influence the execution time and ILoss of TPTDS is
observed in the following experiments.
In the first group, we measure the change of execution
time TCent and TT P with respect to S when p ¼ 1. The size
S varies from 50 MB to 2.5 GB. The 2.5 GB data set contains
nearly 2:5  107 data records. The scale of data sets in our
experiments is much greater than that in [12] and [20].
Thus, the data sets in our experiments are big enough to
evaluate the effectiveness of our approach in terms of
data volume or the number of data records. Note that
ILC ent ¼ ILT P because TPTDS is equivalent to MRTDS
when p ¼ 1. So, we just demo the results of execution time.
The results of the first group of experiments are depicted
in Fig. 2.
Fig. 2a shows the change of TT P and TC ent with respect to
the data size ranging from 50 to 500 MB. From Fig. 2a,
we can see that both TT P and TC ent go up when data size
increases although some slight
fluctuations exist. The
fluctuations are mainly caused by the content of data sets.
TC ent surges from tens of seconds to nearly 10,000 seconds,

ZHANG ET AL.: A SCALABLE TWO-PHASE TOP-DOWN SPECIALIZATION APPROACH FOR DATA ANONYMIZATION USING MAPREDUCE ON...

371

Fig. 3. Change of execution time and ILoss w.r.t. intermediate anonymity
parameter.

Fig. 4. Change in execution time and ILoss w.r.t. number of partitions.

while TT P increase slightly. The dramatic increase of TC ent
illustrates that
the overheads incurred by maintaining
linkage structure and updating statistic information rise
considerably when data size increases. Before the point S ¼
250 MB, TT P is greater than TCent . But after the point, TT P is
greater than TC ent , and the difference between TCent and TT P
becomes larger and larger with the size of data sets
increasing. The trend of TT P and TC ent indicates that TPTDS
becomes more efficient compared with CentTDS for large-
scale data sets.
In our experiments, CentTDS fails due to insufficient
memory when the size of data set is greater than 500 MB.
Hence, CentTDS suffers from scalability problem for large-
scale data sets. To further evaluate the scalability and
efficiency of TPTDS, we run TPTDS over data sets with
larger sizes. Fig. 2b shows the change of TT P with respect to
the data size ranging from 500 MB to 2.5 GB. It can be seen
from Fig. 2b that TT P grows linearly and stably with respect
to the size of data sets. Based on the tendency of TT P , we
maintain that TPTDS is capable of scaling over large-scale
data sets efficiently.
The above experimental results demonstrate that our
approach can significantly improve the scalability and
efficiency compared with the state-of-the-art TDS approach
when anonymizing large-scale data sets.
In the last two groups, we explore the change of TT P and
ILT P with respect to parameters kI and p, respectively. We
use the values TT P and ILT P of MRTDS as baselines to
evaluate the effectiveness of TPTDS, since TPTDS makes
trade-offs between scalability and data utility based on
MRTDS. As MRTDS has the same data utility as CentTDS,
the ILoss of MRTDS is the lower bound of ILoss for TPTDS.
For convenience, the baseline execution time and ILoss are
denoted as TBase and ILBase , respectively. The values of
TBase and ILBase are calculated by setting p ¼ 1. TBase and
ILBase never vary with respect to kI and p, but we plot them
in figures for intuitive comparison. In both groups, we set
the size of data sets as 500 MB which is large enough for
evaluation according to the results in the first group.
In the second group, p is set as 3. The value of p (p > 1) is
selected randomly and does not affect our analysis as what
we want to see is the trend of TT P and ILT P with respect to
kI . Interesting readers can try other values. The conclusions
will be the same. The results of this group are depicted in
Fig. 3. As the number of records in the data set is around
5,000,000, kI varies within [50, 5,000,000]. For conciseness,
kI is indicated by Exp which is the exponent of the scientific
notation of kI , i.e., kI ¼ 5  10Exp . So, Exp ranges from 1 to 6.
Fig. 3a shows the change of execution time TT P against TBase

with respect to kI , while Fig. 3b shows the change of ILT P
against ILBase with respect to kI .
From Fig. 3a, it is seen that TT P is not greater than TBase at
each kI value, meaning that TPTDS can be more efficient
than the baseline. TT P decreases with kI increasing and hits
the bottom at Exp ¼ 4. Then, it goes up after Exp ¼ 4 and
arrives at the baseline. The change of TT P illustrates high
scalability and efficiency can be obtained by setting kI as a
neither too great nor too little value. As to ILoss, Fig. 3b
presents that ILT P is lightly greater than ILBase within the
overall domain, illustrating that splitting computation in
two phases leads to higher ILoss. ILT P is getting less in
relation to the rise of kI and reaches the baseline at Exp ¼ 6.
This trend of ILT P reveals that the less computation is
conducted in the first phase, the lower ILoss is incurred. At
the point Exp ¼ 4, TT P reduces by nearly 50 percent
compared to the baseline while the ILT P only grows by
less than 10 percent. In terms of this observation, TPTDS
can gain higher efficiency than the baseline at the cost of
slight increase of ILoss, via properly tuning parameter kI .
In the third group, kI is set as 50,000. The value is selected
randomly and does not affect our analysis because what we
want to see is the trend of TT P and ILT P with respect to the
number of partitions. The number of partitions varies from 1
to 20. The results of this group are presented in Fig. 4. Fig. 4a
demos the change of execution time TT P against TBase with
respect to p, while Fig. 4b shows the change of ILT P against
ILBase with respect to p.
Fig. 4a shows that TT P varies a lot with respect to p.
Before the point p ¼ 12, TT P is less than the baseline,
meaning that parallelization of MapReduce jobs brings
benefits. On the contrary, TT P is greater than the baseline
after the point, resulting in low efficiency. When p < 16, TT P
drops first and grows with the increase of p. In terms of the
above observation, parallelization of multiple jobs also
engenders overheads, and only proper degree of job level
parallelization can advance scalability and efficiency. From
Fig. 4b, ILT P is not less than the baseline with the growth of
p, showing that parallelization of multiple jobs can indeed
engender higher ILoss as cost. ILT P increases first and then
drops subsequently when p is getting greater. When p ¼ 4,
ILT P is nearly 50 percent of the baseline, while ILT P only
increase by less than 10 percent. Similar to kI , TPTDS can
gain higher efficiency than the baseline at the cost of slight
increase of ILoss, via properly tuning parameter p.
The second and third groups of experiments show that
TPTDS can gain high scalability at certain cost of data utility,
and it can offer an optimized tradeoff between efficiency
and data utility with tuning well-tuned parameters.

372

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 2, FEBRUARY 2014

As a conclusion, all the experimental results demonstrate
that our approach significantly improves the scalability and
efficiency of TDS over existing TDS approaches.

7 CONCLUSIONS AND FUTURE WORK

In this paper, we have investigated the scalability problem
of large-scale data anonymization by TDS, and proposed a
highly scalable two-phase TDS approach using MapReduce
on cloud. Data sets are partitioned and anonymized in
parallel in the first phase, producing intermediate results.
Then, the intermediate results are merged and further
anonymized to produce consistent k-anonymous data sets
in the second phase. We have creatively applied MapRe-
duce on cloud to data anonymization and deliberately
designed a group of
innovative MapReduce jobs to
concretely accomplish the specialization computation in a
highly scalable way. Experimental results on real-world
data sets have demonstrated that with our approach, the
scalability and efficiency of TDS are improved significantly
over existing approaches.
In cloud environment, the privacy preservation for data
analysis, share and mining is a challenging research issue
due to increasingly larger volumes of data sets, thereby
requiring intensive investigation. We will investigate the
adoption of our approach to the bottom-up generalization
algorithms for data anonymization. Based on the contribu-
tions herein, we plan to further explore the next step on
scalable privacy preservation aware analysis and schedul-
ing on large-scale data sets. Optimized balanced scheduling
strategies are expected to be developed towards overall
scalable privacy preservation aware data set scheduling.

REFERENCES
S. Chaudhuri, “What Next?: A Half-Dozen Data Management
[1]
Research Goals for Big Data and the Cloud,” Proc. 31st Symp.
Principles of Database Systems (PODS ’12), pp. 1-4, 2012.
[2] M. Armbrust, A. Fox, R. Griffith, A.D. Joseph, R. Katz, A.
Konwinski, G. Lee, D. Patterson, A. Rabkin, I. Stoica, and M.
Zaharia, “A View of Cloud Computing,” Comm. ACM, vol. 53,
no. 4, pp. 50-58, 2010.
[3] L. Wang, J. Zhan, W. Shi, and Y. Liang, “In Cloud, Can Scientific
Communities Benefit from the Economies of Scale?,” IEEE Trans.
Parallel and Distributed Systems, vol. 23, no. 2, pp.296-303, Feb.
2012.
[4] H. Takabi, J.B.D. Joshi, and G. Ahn, “Security and Privacy
Challenges in Cloud Computing Environments,” IEEE Security
and Privacy, vol. 8, no. 6, pp. 24-31, Nov. 2010.
[5] D. Zissis and D. Lekkas, “Addressing Cloud Computing Security
Issues,” Future Generation Computer Systems, vol. 28, no. 3, pp. 583-
592, 2011.
[6] X. Zhang, C. Liu, S. Nepal, S. Pandey, and J. Chen, “A Privacy
Leakage Upper-Bound Constraint Based Approach for Cost-
Effective Privacy Preserving of Intermediate Data Sets in Cloud,”
IEEE Trans. Parallel and Distributed Systems, to be published, 2012.
[7] L. Hsiao-Ying and W.G. Tzeng, “A Secure Erasure Code-Based
Cloud Storage System with Secure Data Forwarding,” IEEE Trans.
Parallel and Distributed Systems, vol. 23, no. 6, pp. 995-1003, 2012.
[8] N. Cao, C. Wang, M. Li, K. Ren, and W. Lou, “Privacy-Preserving
Multi-Keyword Ranked Search over Encrypted Cloud Data,” Proc.
IEEE INFOCOM, pp. 829-837, 2011.
P. Mohan, A. Thakurta, E. Shi, D. Song, and D. Culler, “Gupt:
Privacy Preserving Data Analysis Made Easy,” Proc. ACM
SIGMOD Int’l Conf. Management of Data (SIGMOD ’12), pp. 349-
360, 2012.
[10] Microsoft HealthVault, http://www.microsoft.com/health/ww/
products/Pages/healthvault.aspx, 2013.

[9]

[19]

[11] B.C.M. Fung, K. Wang, R. Chen, and P.S. Yu, “Privacy-Preserving
Data Publishing: A Survey of Recent Devel- opments,” ACM
Computing Surveys, vol. 42, no. 4, pp. 1-53, 2010.
[12] B.C.M. Fung, K. Wang, and P.S. Yu, “Anonymizing Classification
Data for Privacy Preservation,” IEEE Trans. Knowledge and Data
Eng., vol. 19, no. 5, pp. 711-725, May 2007.
[13] X. Xiao and Y. Tao, “Anatomy: Simple and Effective Privacy
Preservation,” Proc. 32nd Int’l Conf. Very Large Data Bases (VLDB
’06), pp. 139-150, 2006.
[14] K. LeFevre, D.J. DeWitt, and R. Ramakrishnan, “Incognito:
Efficient Full-Domain K -Anonymity,” Proc. ACM SIGMOD Int’l
Conf. Management of Data (SIGMOD ’05), pp. 49-60, 2005.
[15] K. LeFevre, D.J. DeWitt, and R. Ramakrishnan, “Mondrian
Multidimensional K -Anonymity,” Proc. 22nd Int’l Conf. Data
Eng. (ICDE ’06), 2006.
[16] V. Borkar, M.J. Carey, and C. Li, “Inside ‘Big Data Management’:
Ogres, Onions, or Parfaits?,” Proc. 15th Int’l Conf. Extending
Database Technology (EDBT ’12), pp. 3-14, 2012.
[17] K. LeFevre, D.J. DeWitt, and R. Ramakrishnan, “Workload-Aware
Anonymization Techniques for Large-Scale Data Sets,” ACM
Trans. Database Systems, vol. 33, no. 3, pp. 1-47, 2008.
[18] T. Iwuchukwu and J.F. Naughton, “K-Anonymization as Spatial
Indexing: Toward Scalable and Incremental Anonymization,”
Proc. 33rd Int’l Conf. Very Large Data Bases (VLDB ’07), pp. 746-757,
2007.
J. Dean and S. Ghemawat, “Mapreduce: Simplified Data Proces-
sing on Large Clusters,” Comm. ACM, vol. 51, no. 1, pp. 107-113,
2008.
[20] N. Mohammed, B. Fung, P.C.K. Hung, and C.K. Lee, “Centralized
and Distributed Anonymization for High-Dimensional Healthcare
Data,” ACM Trans. Knowledge Discovery from Data, vol. 4, no. 4,
Article 18, 2010.
[21] B. Fung, K. Wang, L. Wang, and P.C.K. Hung, “Privacy-
Preserving Data Publishing for Cluster Analysis,” Data and
Knowledge Eng., vol. 68, no. 6, pp. 552-575, 2009.
[22] N. Mohammed, B.C. Fung, and M. Debbabi, “Anonymity Meets
Game Theory: Secure Data Integration with Malicious Partici-
pants,” VLDB J., vol. 20, no. 4, pp. 567-588, 2011.
[23] L. Sweeney, “k-Anonymity: A Model for Protecting Privacy,” Int’l
J. Uncertainty, Fuzziness and Knowledge-Based Systems, vol. 10, no. 5,
pp. 557-570, 2002.
[24] W. Jiang and C. Clifton, “A Secure Distributed Framework for
Achieving k-Anonymity,” VLDB J., vol. 15, no. 4, pp. 316-333,
2006.
[25] P. Jurczyk and L. Xiong, “Distributed Anonymization: Achieving
Privacy for Both Data Subjects and Data Providers,” Proc. 23rd
Ann. IFIP WG 11.3 Working Conf. Data and Applications Security
XXIII (DBSec ’09), pp. 191-207, 2009.
I. Roy, S.T.V. Setty, A. Kilzer, V. Shmatikov, and E. Witchel,
“Airavat: Security and Privacy for Mapreduce,” Proc. Seventh
USENIX Conf. Networked Systems Design and Implementation (NSDI
’10), pp. 297-312, 2010.
[27] K. Zhang, X. Zhou, Y. Chen, X. Wang, and Y. Ruan, “Sedic:
Privacy-Aware Data Intensive Computing on Hybrid Clouds,”
Proc. 18th ACM Conf. Computer and Comm. Security (CCS ’11),
pp. 515-526, 2011.
[28] X. Xiao and Y. Tao, “Personalized Privacy Preservation,” Proc.
ACM SIGMOD Int’l Conf. Management of Data (SIGMOD ’06),
pp. 229-240, 2006.
[29] Amazon Web Services, “Amazon Elastic Mapreduce,” http://
aws.amazon.com/elasticmapreduce/, 2013.
[30] Apache, “Hadoop,”http://hadoop.apache.org, 2013.
[31] Y. Bu, B. Howe, M. Balazinska, and M.D. Ernst, “The Haloop
Approach to Large-Scale Iterative Data Analysis,” VLDB J., vol. 21,
no. 2, pp. 169-190, 2012.
J. Ekanayake, H. Li, B. Zhang, T. Gunarathne, S.-H. Bae, J. Qiu,
and G. Fox, “Twister: A Runtime for Iterative Mapreduce,” Proc.
19th ACM Int’l Symp. High Performance Distributed Computing
(HDPC ’10), pp. 810-818, 2010.
[33] KVM, http://www.linux-kvm.org/page/Main_Page, 2013.
[34] OpenStack, http://openstack.org/, 2013.
[35] UCI Machine Learning Repository, ftp://ftp.ics.uci.edu/pub/
machine-learning-databases/, 2013.

[26]

[32]

ZHANG ET AL.: A SCALABLE TWO-PHASE TOP-DOWN SPECIALIZATION APPROACH FOR DATA ANONYMIZATION USING MAPREDUCE ON...

373

Xuyun Zhang received the bachelor’s and
master’s degrees in computer science from
Nanjing University, China. He is currently work-
ing toward the PhD degree at the Faculty of
Engineering and IT, University of Technology,
Sydney, Australia. His research interests include
cloud computing, privacy and security, Big Data,
MapReduce, and OpenStack. He has published
several papers in refereed international journals
including IEEE Transactions on Parallel and
Distributed Systems (TPDS).

Laurence T. Yang received the BE degree in
computer science and technology from Tsin-
ghua University, China, and the PhD degree in
computer science from the University of Victoria,
Canada. He is a professor in the School of
Computer Science and Technology, Huazhong
University of Science and Technology, China
and in the Department of Computer Science, St.
Francis Xavier University, Canada. His current
research interests include parallel and distribu-
ted computing, embedded and ubiquitous computing. His research has
been supported by National Sciences and Engineering Research
Council, Canada and Canada Foundation for Innovation. He is a senior
member of the IEEE.

Chang Liu is currently working toward the PhD
degree at the University of Technology Sydney,
Australia. His research interests include cloud
computing, resource management, cryptogra-
phy, and data security.

Jinjun Chen rece ived the PhD degree in
computer science and software engineering
from Swinburne University of Technology, Aus-
tralia. He is an associate professor from the
Faculty of Engineering and IT, University of
Technology Sydney (UTS), Australia. He is the
director of Lab of Cloud Computing and Dis-
tributed Systems at UTS. His research interests
include cloud computing, Big Data, workflow
management, privacy and security, and related
various research topics. His research results have been published in
more than 100 papers in high-quality journals and at conferences,
including IEEE Transactions on Service Computing, ACM Transactions
on Autonomous and Adaptive Systems, ACM Transactions on Software
Engineering and Methodology (TOSEM), IEEE Transactions on Soft-
ware Engineering (TSE), and IEEE Transactions on Parallel and
Distributed Systems (TPDS). He received Swinburne Vice-Chancellor’s
Research Award for early career researchers (2008), IEEE Computer
Society Outstanding Leadership Award (2008-2009) and (2010-2011),
IEEE Computer Society Service Award (2007), Swinburne Faculty of
ICT Research Thesis Excellence Award (2007). He is an associate
editor for the IEEE Transactions on Parallel and Distributed Systems. He
is the vice chair of IEEE Computer Society’s Technical Committee on
Scalable Computing (TCSC), vice chair of Steering Committee of
Australasian Symposium on Parallel and Distributed Computing,
founder and coordinator of IEEE TCSC Technical Area on Workflow
in Scalable Computing Environments,
Management
founder and
steering committee co-chair of
International Conference on Cloud
and Green Computing, and International Conference on Big Data
and Distributed Systems. He is a member of the IEEE.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

2234

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 9, SEPTEMBER 2014

Authorized Public Auditing of Dynamic Big Data
Storage on Cloud with Efficient Verifiable
Fine-Grained Updates
Chang L iu, J in jun Chen, Sen ior Member, IEEE, Laurence T. Yang, Member, IEEE , Xuyun Zhang,
Ch i Yang, Ra j iv Ran jan, and Ramamohanarao Kotag ir i

Abstract—Cloud computing opens a new era in IT as it can provide various elastic and scalable IT services in a pay-as-you-go
fashion, where its users can reduce the huge capital investments in their own IT infrastructure. In this philosophy, users of cloud
storage services no longer physically maintain direct control over their data, which makes data security one of the major concerns of
using cloud. Existing research work already allows data integrity to be verified without possession of the actual data file. When the
verification is done by a trusted third party, this verification process is also called data auditing, and this th ird party is called an auditor.
However, such schemes in existence suffer from several common drawbacks. First, a necessary authorization/authentication
process is missing between the auditor and cloud service provider, i.e., anyone can challenge the cloud service provider for a proof
of integrity of certa in file, which potentially puts the quality of the so-called ‘auditing-as-a-service’ at risk; Second, although some
of the recent work based on BLS signature can already support fully dynamic data updates over fixed-size data blocks, they only
support updates with fixed-sized blocks as basic unit, which we call coarse-grained updates. As a result, every small update will cause
re-computation and updating of the authenticator for an entire file block, which in turn causes higher storage and communication overheads.
In this paper, we provide a formal analysis for possible types of fine-grained data updates and propose a scheme that can fully support
authorized auditing and fine-grained update requests. Based on our scheme, we also propose an enhancement that can dramatically reduce
communication overheads for verifying small updates. Theoretical analysis and experimental results demonstrate that our scheme can
offer not only enhanced security and flexibility, but also significantly lower overhead for big data applications with a large number of
frequent small updates, such as applications in social media and business transactions.

Index Terms—Cloud computing, big data, data security, provable data possession, authorized auditing, fine-grained dynamic data update

Ç

1 INTRODUCTION
C LOUD computing is being intensively referred to as one
of the most influential
innovations in information
technology in recent years [1], [2]. With resource virtuali-
zation, cloud can deliver computing resources and services
in a pay-as-you-go mode, which is envisioned to become as
convenient to use similar to daily-life utilities such as
electricity, gas, water and telephone in the near future [1].
These computing services can be categorized into Infra-
structure-as-a-Service (IaaS), Platform-as-a-Service (PaaS)
and Software-as-a-Service (SaaS) [3]. Many international IT

corporations now offer powerful public cloud services to
users on a scale from individual to enterprise all over the
world; examples are Amazon AWS, Microsoft Azure, and
IBM SmartCloud.
Although current development and proliferation of
cloud computing is rapid, debates and hesitations on the
usage of cloud still exist. Data security/privacy is one of
the major concerns in the adoption of cloud computing [3],
[4], [5]. Compared to conventional systems, users will lose
their direct control over their data. In this paper, we will
investigate the problem of integrity verification for big
data storage in cloud. This problem can also be called data
auditing [6], [7] when the verification is conducted by a
trusted third party. From cloud users’ perspective, it may
also be called ‘auditing-as-a-service’. To date, extensive
research is carried out to address this problem [6], [7], [8],
[9], [10], [11], [12], [13], [14], [15]. In a remote verification
scheme, the cloud storage server (CSS) cannot provide a
valid integrity proof of a given proportion of data to a
verifier unless all this data is intact. To ensure integrity of
user data stored on cloud service provider, this support is
of no less importance than any data protection mechanism
deployed by the cloud service provider (CSP) [16], no
matter how secure they seem to be, in that it will provide
Manuscript received 21 May 2013; revised 23 July 2013; accepted 24 July
2013. Date of publication 4 Aug. 2013; date of current version 13 Aug. 2014.
the verifier a piece of direct, trustworthy and real-timed
Recommended for acceptance by Y. Xiang.
intelligence of the integrity of the cloud user’s data through
For information on obtaining reprints of this article, please send e-mail to:
a challenge request. It is especially recommended that
reprints@ieee.org, and reference the Digital Object Identifier below.
data auditing is to be conducted on a regular basis for the
Digital Object Identifier no. 10.1109/TPDS.2013.191
1045-9219 Ó 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

. C. Liu is with the School of Comput. Sci. and Tech., Huazhong Uni. of
Sci. and Tech., China, and also with the Faculty of Eng. and IT, Uni. of
Tech., Sydney, Australia. E-mail: changliu.it@gmail.com.
J. Chen, X. Zhang, and C.Yang are with the Faculty of Eng. and IT, Uni.
of Tech., Sydney, Australia. E-mail: {jinjun.chen, xyzhanggz, chiyangit}@
gmail.com.
L.T. Yang is with the School of Comput. Sci. and Tech., Huazhong Uni. of
Sci. and Tech., China, and also with the Dept. of Comput. Sci., St. Francis
Xavier Uni., Canada. E-mail: ltyang@stfx.ca.
. R. Ranjan is with CSIRO Computational Informatics Division, Australia.
E-mail: rranjans@gmail.com.
. R. Kotagiri is with the Dept. of Comput. and Information Systems, The
Uni. of Melbourne, Australia. E-mail: kotagiri@unimelb.edu.au.

.

.

LIU ET AL.: PUBLIC AUDITING OF BIG DATA WITH FINE-GRAINED UPDATES ON CLOUD

2235

users who have high-level security demands over their
data.
Although existing data auditing schemes already have
various properties (see Section 2), potential risks and
inefficiency such as security risks in unauthorized auditing
requests and inefficiency in processing small updates still
exist. In this paper, we will focus on better support for
small dynamic updates, which benefits the scalability and
efficiency of a cloud storage server. To achieve this, our
scheme utilizes a flexible data segmentation strategy and a
ranked Merkle hash tree (RMHT). Meanwhile, we will
address a potential security problem in supporting public
verifiability to make the scheme more secure and robust,
which is achieved by adding an additional authorization
process among the three participating parties of client, CSS
and a third-party auditor (TPA).
Research contributions of this paper can be summarized
as follows:

1. For the first time, we formally analyze different
types of fine-grained dynamic data update requests
on variable-sized file blocks in a single dataset. To
the best of our knowledge, we are the first to pro-
pose a public auditing scheme based on BLS signa-
ture and Merkle hash tree (MHT) that can support
fine-grained update requests. Compared to existing
schemes, our scheme supports updates with a size
that is not restricted by the size of file blocks,
thereby offers extra flexibility and scalability com-
pared to existing schemes.
2. For better security, our scheme incorporates an ad-
ditional authorization process with the aim of
eliminating threats of unauthorized audit chal-
lenges from malicious or pretended third-party
auditors, which we term as ‘authorized auditing’.
3. We investigate how to improve the efficiency in
verifying frequent small updates which exist in
many popular cloud and big data contexts such as
social media. Accordingly, we propose a further
enhancement
for our scheme to make it more
suitable for this situation than existing schemes.
Compared to existing schemes, both theoretical
analysis and experimental results demonstrate that
our modified scheme can significantly lower com-
munication overheads.

For the convenience of
the readers , we list some
frequently-used acronyms in Appendix 1 which is avail-
able in the Computer Society Digital Library at http://doi.
ieeecomputersociety.org/10.1109/TPDS.2013.191.

1.1 Paper Organization
The rest of this paper is organized as follows. Section 2
discusses related work. Section 3 provides motivating ex-
amples as well as a detailed analysis of our research
problem. Section 4 provides a description of our proposed
scheme in detail, with also a detailed analysis of fine-
grained update requests and how they can be supported.
Sect ion 5 prov ides secur ity ana lys is for our des ign .
Section 6 provides experimental results. Section 7 con-
cludes our research and points out future work.

2 RELATED WORK
Compared to traditional systems, scalability and elasticity
are key advantages of cloud [1], [2], [3]. As such, efficiency
in supporting dynamic data is of great importance. Security
and privacy protection on dynamic data has been studied
extensively in the past [6], [8], [12], [17]. In this paper,
we will focus on small and frequent data updates, which is
important because these updates exist in many cloud
applications such as business transactions and online social
networks (e.g. Twitter [18]). Cloud users may also need to
split big datasets into smaller datasets and store them in
different physical servers for reliability, privacy-preserving
or efficient processing purposes.
Among the most pressing problems related to cloud is
data security/privacy [4], [5], [19]. It has been one of the
most frequently raised concerns [5], [20]. There is a lot of
work trying to enhance cloud data security/privacy with
technological approaches on CSP side, such as [21], [22]. As
discussed in Section 1, they are of equal importance as our
focus of external verifications.
Integrity verification for outsourced data storage has
attracted extensive research interest. The concept of proofs
of retrievability (POR) and its first model was proposed by
Jules et al. [14]. Unfortunately, their scheme can only be
applied to static data storage such as archive or library. In
the same year, Ateniese, et al. proposed a similar model
named ‘provable data possession’ (PDP) [10]. Their schemes
offer ‘blockless verification’ which means the verifier can
verify the integrity of a proportion of the outsourced file
through verifying a combination of pre-computed file tags
which they call homomorphic verifiable tags (HVTs) or
homomorphic linear authenticators (HLAs). Work by Shac-
ham, et al. [15] provided an improved POR model with
stateless verification. They also proposed a MAC-based
private verification scheme and the first public verification
scheme in the literature that based on BLS signature scheme
[23]. In their second scheme, the generation and verification
of integrity proofs are similar to signing and verification of
BLS signatures. When wielding the same security strength
(say, 80-bit security), a BLS signature (160 bit) is much
shorter than an RSA signature (1024 bit), which is a desired
benefit for a POR scheme. They also proved the security of
both their schemes and the PDP scheme by Ateniese, et al.
[9], [10]. From then on, the concepts of PDP and POR were in
fact unified under this new compact POR model. Ateniese,
et al. extended their scheme for enhanced scalability [8], but
only partial data dynamics and a predefined number of
challenges is supported. In 2009, Erway, et al. proposed the
first PDP scheme based on skip list that can support full
dynamic data updates [12]. However, public auditability
and variable-sized file blocks are not supported by default.
Wang, et al. [6] proposed a scheme based on BLS signature
that can support public auditing (especially from a third-
party auditor, TPA) and full data dynamics, which is one of
the latest works on public data auditing with dynamics
support. However, their scheme lacks support for fine-
grained update and authorized auditing which are the main
focuses of our work. Latest work by Wang et al. [7] added a
random masking technology on top of [6] to ensure the TPA
cannot infer the raw data file from a series of integrity

2236

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 9, SEPTEMBER 2014

Many big data applications will keep user data stored on
the cloud for small-sized but very frequent updates. A most
typical example is Twitter, where each tweet is restricted to
140 characters long (which equals 140 bytes in ASCII code).
They can add up to a total of 12 terabytes of data per day
[18]. Storage of transaction records in banking or securities
markets is a similar and more security-heavy example.
Moreover, cloud users may need to split
large-scale
datasets into smaller chunks before uploading to the cloud
for privacy-preserving [17] or efficient scheduling [19]. In
this regard, efficiency in processing small updates is
always essential in big data applications.
To better support scalability and elasticity of cloud
computing, some recent public data auditing schemes do
support da ta dynam ics . However ,
types o f upda tes
supported are limited. Therefore previous schemes may
not be suitable for some practical scenarios. Besides, there
is a potential security threat in the existing schemes. We
will discuss these problems in detail in the next Section 3.2.

3.2 Problem Analysis
3.2.1 Roles of the Participating Parties
Most PDP and POR schemes can support public data
verification. In such schemes, there are three participating
parties: client, CSS and TPA. Relationships between the
three parties are shown in Fig. 1. In brief, both CSS and TPA
are only semi-trusted to the client. In the old model, the
challenge message is very simple so that everyone can send
a challenge to CSS for the proof of a certain set of file blocks,
which can enable malicious exploits in practice. First, a
malicious party can launch distributed denial-of-service
(DDOS) attacks by sending multiple challenges from
multiple clients at a time to cause additional overhead on
CSS and congestion to its network connections, thereby
causing degeneration of service qualities. Second, an
adversary may get privacy-sensitive information from the
integrity proofs returned by CSS. By challenging the CSS
multiple times, an adversary can either get considerable
information about user data (due to the fact that returned
integrity proofs are computed with client-selected data
blocks), or gather statistical information about cloud service
status. To this end, traditional PDP models cannot quite
meet the security requirements of ‘auditing-as-a-service’,
even though they support public verifiability.

3.2.2 Verifiable Fine-Grained Dynamic Data Operations
Some of the existing public auditing schemes can already
support full data dynamics [6], [7], [12]. In their models,
only insertions, deletions and modifications on fixed-sized
blocks are discussed. Particularly, in BLS-signature-based
schemes [6], [7], [13], [15] with 80-bit security, size of each
data block is either restricted by the 160-bit prime group
order p, as each block is segmented into a fixed number of
160-bit sectors. This design is inherently unsuitable to
support variable-sized blocks, despite their remarkable
advantage of shorter integrity proofs. In fact, as described
in Section 2, existing schemes can only support insertion,
deletion or modification of one or multiple fixed-sized
blocks, which we call ‘coarse-grained’ updates.

Fig. 1. Relationship between the participating parties in a public auditing
scheme.

proofs. In their scheme, they also incorporated a strategy
first proposed in [15] to segment file blocks into multiple
‘sectors’. However, the use of this strategy was limited to
trading-off storage cost with communication cost.
Other lines of research in this area include the work of
Ateniese, et al. [24] on how to transform a mutual iden-
tification protocol to a PDP scheme; scheme by Zhu, et al.
[13] that allows different service providers in a hybrid
cloud to cooperatively prove data integrity to data owner;
and the MR-PDP Scheme based on PDP [10] proposed by
Curtmola, et al. [11] that can efficiently prove the integrity
of multiple replicas along with the original data file.

3 MOTIVATING EXAMPLES AND
PROBLEM ANALYSIS
3.1 Motivating Examples
Cos t -e f f ic iency brough t by e las t ic i ty is one o f
the
most
important reasons why cloud is being widely
adopted. For example, Vodafone Australia is currently
using Amazon cloud to provide their users with mobile
online-video-watching services. According to their statis-
tics, the number of video requests per second (RPS) can
reach an average of over 700 during less than 10 percent
of the time such as Friday nights and public holidays,
compared to a mere 70 in average in the rest 90 percent of
the time. The variation in demand is more than 9 times
[3]. Without cloud computing, Vodafone cannot avoid
purchasing computing facilities that can process 700 RPS,
but it will be a total waste for most of the time. This is
where cloud computing can save a significant amount of
investmentsVcloud’s elasticity allows the user-purchased
computation capacity to scale up or down on-the-fly at any
time. Therefore, user requests can be fulfilled without
wasting investments in computational powers. Other 2
large companies who own news.com.au and realestate.com.
au, respectively, are using Amazon cloud for the same
reason [3]. We can see through these cases that scalability
and elasticity, thereby the capability and efficiency in sup-
porting data dynamics, are of extreme importance in cloud
computing.

LIU ET AL.: PUBLIC AUDITING OF BIG DATA WITH FINE-GRAINED UPDATES ON CLOUD

2237

Fig. 2. Example of a rank-based Merkle hash tree (RMHT).

Although support for coarse-grained updates can pro-
vide an integrity verification scheme with basic scalabil-
ity, data updating operations in practice can always be
more complicated. For example,
the verifiable update
process introduced in [6], [12] cannot handle deletions or
modifications in a size lesser than a block. For insertions,
there is a simple extension that enables insertion of an
arbitrary-sized datasetVCSS can always create a new
block (or several blocks) for every insertion. However,
when there are a large number of small upgrades
(especially insertions),
the amount of wasted storage
will be huge. For example, in [6], [12] the recommended
size for a data block is 16k bytes. For each insertion of a
140-byte Twitter message, more than 99 percent of the
newly allocated storage is wastedVthey cannot be reused
until the block is deleted. These problems can all be re-
so lved if
f ine-gra ined da ta upda tes are suppor ted .
According to this observation, supporting of fine-grained
updates can bring not only additional flexibility, but also
improved efficiency.
Our model assumes the following:

Assumption 1. CSS will honestly answer all data queries to its
clients. In other words, if a user asks to retrieve a certain piece
of her data stored on CSS, CSS will not try to cheat her with
an incorrect answer.

This assumptionVreliabilityVshould be a basic service
quality guarantee for cloud storage services.
PDP and POR are different models with similar goals.
One main difference is that the file is encoded with error-
correction code in the POR model, but not in the PDP
model [6]. As in [6], [7], we will not restrict our work to
either of the models.

4 THE PROPOSED SCHEME
Some common notations are introduced in Appendix A.

4.1 Preliminaries
4.1.1 Bilinear Map
Assume a group G is a gap Diffie-Hellman (GDH) group
with prime order p. A bilinear map is a map constructed as

e : G  G ! GT where GT is a multiplicative cyclic group
with prime order. A useful e should have the following
properties: bilinearityV8 m; n 2 G ) eðma ; nb Þ ¼ eðm; nÞab ;
n o n - d e g e n e r a c y V8 m 2 G; m 6¼ 0 ) eðm; mÞ 6¼ 1; a n d
computabilityVe should be efficiently computable. For
simplicity, we will use this symmetric bilinear map in our
scheme descr ip t ion . A lternat ive ly ,
the more eff ic ient
asymmetric bilinear map e : G1  G2 ! GT may also be
applied, as was pointed out in [23].

4.1.2 Ranked Merkle Hash Tree (RMHT)
The Merkle Hash Tree (MHT) [25] has been intensively
studied in the past. In this paper we utilize an extended
MHT with ranks which we named RMHT. Similar to a
binary tree, each node N will have a maximum of 2 child
nodes. In fact, according to the update algorithm, every
non-leaf node will constantly have 2 child nodes. Informa-
tion contained in one node N in an RMHT T is represented
as fH; rN g where H is a hash value and rN is the rank of this
node. T is constructed as follows. For a leaf node LN based
on a message mi , we have H ¼ hðmi Þ, rLN ¼ si ; A parent
node of N1 ¼ fH1 ; rN 1 g and N2 ¼ fH2 ; rN 2 g is constructed
as NP ¼ fhðH1 kH2 Þ; ðrN 1 þ rN 2 Þg where k is a concatenation
operator. A leaf node mi ’s AAI Wi is a set of hash values
chosen from every of its upper level so that the root value
R can be computed through fmi ; Wi g. For example, for
the RMHT demons tra ted in F ig . 2 , m1 ’s AA I W1 ¼
fhðm2 Þ; hðeÞ; hðdÞg. According to the property of RMHT,
we know that the number of hash values included in Wi
equals the depth of mi in T .

4.2
Framework and Definitions
We first define the following block-level fine-grained
update operations:

Definition 1 (Types of Block-Level Operations in Fine-
Grained Updates). Block-level operations in fine-grained dy-
namic data updates may contain the following 6 types of
operations: partial modification PMVa consecutive part of a
certain block needs to be updated; whole-block modification
MVa whole block needs to be replaced by a new set of data;
block deletion DVa whole block needs to be deleted from the
tree structure; block insertion J Va whole block needs to be
created on the tree structure to contain newly inserted data;

Framework of public auditing scheme with data dynam-
ics support is consisted of a series of algorithms. Similar
to [12], the algorithms in our framework are: Keygen,
FilePreProc, Challenge, Verify, Genproof , PerformUpdate
and VerifyUpdate. Detailed definitions and descriptions can
be found in Appendix B.

2238
IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 9, SEPTEMBER 2014
and block splitting SPVa part of data in a block needs to be
FilePreProcðF; sk; SegReqÞ: According to the preemp-
taken out to form a new block to be inserted next to it.1
tively determined segmentation requirement SegReq (in-
cluding smax , a predefined upper-bound of the number of
segments per block), segments file F into F ¼ fmij g;
i 2 ½1; l; j 2 ½1; s; si 2 ½1; smax , i.e., F is segmented into a
total of l blocks, with the ith block having si segments.
In our settings, every file segment should of the same size
 2 ð0; pÞ and as large as possible (see [15]). Since jpj ¼ 20
bytes is used in a BLS signature with 80-bit security
(sufficient in practice),  ¼ 20 bytes is a common choice.
According to smax , a set U ¼ fuk 2 Zp gk 2 ½1; smax  is chosen so
Q
that the client can compute the HLAs i for each block:
i ¼ ðH ðmi Þ
Þ which constitute the ordered set
j¼1 umij
si
j
F ¼ fi gi2½1;l . This is similar to signing a message with BLS
signature. The client also generate a root R based on
construction of an RMHT T over H ðmi Þ and compute
sig ¼ ðH ðRÞÞ . Finally, let u ¼ ðu1 k . . . kusmax Þ, the client com-
pute the file tag for F as t ¼ nameknkukSigssk ðnameknkuÞ
and then output fF ; F; T; R; sig; tg.

4.3 Our Scheme
We now describe our proposed scheme in the aim of
supporting variable-sized data blocks, authorized third-
party auditing and fine-grained dynamic data updates.

4.3.1 Overview
Our scheme is described in three parts:

1.

Setup: the client will generate keying materials via
KeyGen and FileProc, then upload the data to CSS.
Different from previous schemes, the client will store a
RMHT instead of a MHT as metadata. Moreover, the
client will authorize the TPA by sharing a value sigAUTH .
2. Verifiable Data Updating: the CSS performs the client’s
fine-grained update requests via PerformUpdate,
then the client runs VerifyUpdate to check whether
CSS has performed the updates on both the data
blocks and their corresponding authenticators (used
for auditing) honestly.
3. Challenge, Proof Generation and Verification: De-
scribes how the integrity of the data stored on CSS is
verified by TPA via GenChallenge, GenProof and Verify.

We now describe our scheme in detail as follows.

4.3.2 Setup
This phase is similar to the existing BLS-based schemes
e x c e p t
f o r
t h e s e gm e n t a t i o n o f
f i l e b l o c k s . L e t
e : G  G ! GT be a bilinear map defined in Section 4.1,
2.H : ð0; 1Þ ! G is
where G is a GDH group supported by Zp
a collision-resistant hash function, and h is another crypto-
graphic hash function.
After all parties have finished negotiating the fundamental
parameters above, the client runs the following algorithms:
KeyGenð1k Þ: The client generates a secret value  2 Zp
and a generator g of G, then compute  ¼ g . A secret
signing key pair fspk; sskg is chosen with respect to a
designated provably secure signature scheme whose
signing algorithm is denoted as SigðÞ. This algorithm
outputs fssk; g as the secret key sk and fspk; ; gg as the
public key pk. For simplicity, in our settings, we use the
same key pair for signatures, i.e., ssk ¼ ; spk ¼ fv; gg.

1. There are other possible operations such as block merging
MEVtwo blocks need to be merged into the first block before the
second block is deleted, and data moving MVVmove a part of data
from one block to another, if the size of the second block does not
exceed smax   after this update. However, the fine-grained update
requests discussed in this paper do not involve these operations, thus
we will omit them in our current discussion. We will leave the problem
of how to exploit them in future work.
2. Most exponential operations in this paper are modulo p.
Therefore, for simplicity, we will use g instead of g mod p unless
otherwise specified.

4.3.3 Prepare for Authorization
The client asks (her choice of) TPA for its ID VID (for security,
VID is used for authorization only). TPA will then return its
ID, encrypted with the client’s public key. The client will then
compute sigAUTH ¼ Sigssk ðAUTH ktkVIDÞ and sends sigAUTH
along with the auditing delegation request to TPA for it to
compose a challenge later on.
Different from existing schemes, after the execution of
the above two algorithms, the client will keep the RMHT
‘skeleton’ with only ranks of each node and indices of each
file block to reduce fine-grained update requests to block-
level operations. We will show how this can be done in
Section 4.4. The client then sends fF ; t; F; sig; AUTH g to
CSS and deletes fF; F ; t; F; sigg from its local storage. The
CSS will construct an RMHT T based on mi and keep T
stored w ith fF ; t; F; sig; AUTH g for later ver if icat ion ,
which should be identical to the tree spawned at client-
side just a moment ago.

4.3.4 Verifiable Data Updating
Same as Setup, this process will also be between client and
CSS. We discuss 5 types of block-level updates (operations)
that will affect T : PM, M, D, J and SP (see Definition 1).
We will discuss how these requests can form fine-grained
update requests in general in Section 4.4.
The verifiable data update process for a PM-typed
update is as follows (see Fig. 3):
1. The client composes an update quest UpdateReq
defined in Section 4.2 and sends it to CSS.
2. CSS executes the following algorithm:
PerformUpdateðUpdateReq; F Þ: CSS parses UpdateReq
and get fPM; i; o; mnewg. When Type ¼ PM, CSS will update
mi and T accordingly, then output Pupdate ¼ fmi ; Wi ; R0 ; sigg
(note that Wi stays the same during the update) and the
updated file F 0
.
Upon finishing of this algorithm, CSS will send Pupdate to
the client.
3. After receiving Pupdate , the client executes the follow-
ing algorithm:
VerifyUpdateðpk; Pupdate Þ: The client computes m0
i using
fmi ; UpdateReqg,
to fmi ; Wi ; R0 ; sigg,
then parse Pupdate

LIU ET AL.: PUBLIC AUDITING OF BIG DATA WITH FINE-GRAINED UPDATES ON CLOUD

2239

Fig. 3. Verifiable PM-typed Data Update in our scheme.

compute R (and H ðRÞ) and Rnew use fmi ; Wi g and fm0
i ; Wi g
I t ver i f ies sig use H ðRÞ, and check i f
respec t ive ly .
Rnew ¼ R0 . If either of these two verifications fails, then
output FALSE and return to CSS, otherwise output TRUE .
Q
If the output of the algorithms is TRUE , then the client
m0
Þ and sig0 ¼ ðH ðR0 ÞÞ then
i ¼ ðH ðm0
i Þ
computes 0
S1
ij
j¼1 u
j
i ; sig0 g to CSS.
sends f0
4. The CSS will update i to 0
i and sig to sig0 accordingly
i ; sig0 g, or i t w i l l run
and de le te F if
i t rece ives f0
PerformUpdateðÞ again if it receives FALSE . A cheating
CSS will fail the verification and constantly receive FASLE
until it performed the update as the client requested.
Due to their similarity to the process described above,
other types of operations are only briefly discussed as
follows. For whole-block operations M, D, and J , as in
model in the existing work [6], the client can directly
compute 0
i without retrieving data from the original file F
stored on CSS, thus the client can send 0
i along with
UpdateReq in the first phase. For responding to an update
request, CSS only needs to send back H ðmi Þ instead of mi .
Other operations will be similar to where Type ¼ PM. For
an SP -typed update, in addition to updating mi to m0
i , a
new b lock m needs to be inser ted to T a f ter m0
i .
Nonetheless, as the contents in m is a part of the old mi ,
the CSS still needs to send mi back to the client. The process
afterwards will be just similar to a PM-typed upgrade,
with an only exception that the client will compute Rnew
u s i n g fm0
i ; hðm Þ; Wi g t o c om p a r e t o R0 ,
i n s t e a d o f
i ; Wi g as in the PM-typed update.
using fm0

4.3.5 Challenge, Proof Generation and Verification
In our setting, TPA must show CSS that it is indeed
authorized by the file owner before it can challenge a
certain file.
1. TPA runs the following algorithm:
GenChallengeðAcc; pk; sigAUTH Þ: According to the accu-
racy required in this auditing, TPA will decide to verify c
l blocks. Then, a challenge message
out of the total
chal ¼ fsigAUTH ; fVIDgPKCSS
; fi; vi gi2I g is generated where
VID is TPA’s ID, I is a randomly selected subset of ½1; l
with c elements and fvi 2 Zp gi2I are c randomly-chosen
coefficients. Note that VID is encrypted with the CSS’s
public key PKCSS so that CSS can later decrypt fVIDgPKCSS
with the corresponding secret key.
TPA then sends chal to CSS.

2. After receiving chal, CSS will run the following
algorithm:
GenProof ðpk; F; sigAUTH ; F; chalÞ: Let w ¼ max fsi gi2I . CSS
will first verify sigAUTH with AUTH , t, VID and the client’s
P
Q
public key spk, and output REJECT if it fails. Otherwise, CSS
will compute k ¼
i2I vimik ; k 2 ½1; w and  ¼
i2I vi
i and
compose the proof P as ¼ ffk gk2½1;w ; fH ðmi Þ; Wi gi2I ; sigg,
then ouput P . Note that during the computation of k , we will
let mik ¼ 0 if k
9
si .
After execution of this algorithm, CSS will send P to TPA.
3. After receiv ing P , TPA will run the following
algorithm:
Verifyðpk; chal; P Þ: TPA will compute R using fH ðmi Þ; Wi g
Q
and then verify sig use public keys g and  by comparing
i2I H ðmi Þvi 
eðsig; gÞ with ðH ðRÞ; vÞ. If they are equal, let ! ¼
Q
k , TPA will further check if eð; gÞ equals eð!; vÞ,
k2½1;w uk
which is similar to verifying a BLS signature. If all the
two equations hold then the algorithm returns TRUE ,
otherwise it returns FALSE .
An illustration of Challenge and Verification processes
can be found in Fig. 4.

4.4 Analysis on Fine-Grained Dynamic
Data Updates
Following the settings in our proposed scheme, we now
define a fine-grained update request for an outsourced file
divided into l variable-sized blocks, where each block is
consisted of si 2 ½1; smax  segments of a fixed size  each.
Assume an RMHT T is built upon fmi gi2½1;l for authenti-
cation, which means T must keep updated with each RMHT
operation for CSS to send back the root R for the client to
verify the correctness of this operation (see Section 4.3). We
now try to define and categorize all types of fine-grained
updates, and then analyze the RMHT operations with
Type ¼ PM; M; D; J or SP that will be invoked along with
the update of the data file.

Definition 2 (Fine-Grained Data Update Request). A
fine-grained update request is defined as FReq ¼ fo; len; mnew g,
where o indicates the starting offset of this update in F , len
indicates the data length after o that needs to be updated (so that
fo; leng can characterize an exact proportion of the original file
F that needs to be updated, which we will later call mold ), and
mnew is the new message to be inserted into F from offset o.

We assume the data needed to be obsolete and the new
data to be added shares a common starting offset o in F , as

2240

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 9, SEPTEMBER 2014

Fig. 4. Challenge, Proof Generation and Verification in our scheme.

otherwise it can be split into multiple updates defined in
Definition 2 commencing in sequence. We now introduce a
rule to follow during all update processes:

Condition 1 (Block Size Limits in Updates) . An update
operation must not cause the size of any block to exceed smax ;
After any operation, a block that has 0 bit data remaining
must be deleted from T .

Detailed analysis can be found in Appendix C, which
can be summarized as the following theorem:

Theorem 1. Any valid fine-grained update request that is in the
form of fo; len; mnew g can either directly belong to, or be split
into some smaller requests that belong to, the following 5 types
of block-level update requests: PM, M, J , D and SP .

Proof. See Appendix C.

g

Through the analysis above, we know that a large
number of small updates, no matter insert, delete or mod-
ification, will always invoke a large number of PM op-
erations. We now try to optimize PM operations in the next
section to make it more efficient.

4.5 Modification for Better Support of
Small Updates
Although our proposed scheme can support fine-grained
update requests, the client still needs to retrieve the entire
file block from CSS to compute the new HLA, in the sense
that the client is the only party that has the secret key  to
compute the new HLA but clients do not have F stored
locally. Therefore, the additional cost in communication
will be huge for frequent updates. In this section, we will
propose a modification to address this problem, utilizing
the fact that CSS only needs to send back data in the block
that stayed unchanged.
The framework we use here is identical to the one used
in our scheme introduced in Section 4.2 (which we will also
name as ’the basic scheme’ hereafter). Changes are made in
PerformUpdate and VerifyUpdate; Setup, Challenge, Proof
Generation and Verification phases are same as in our basic
scheme. Therefore, we will only describe the two algo-
rithms in the following phase:

4.5.1 Verifiable Data Updating
We also discuss PM operations here first.
PerformUpdate: After CSS has received the update
request UpdateReq from the client,
it will parse it as

fPM; I; o; mnew g and use fo; jmnew jg to gather the sectors
that are not involved in this update, which we denote as
fmij gj2M . CSS will then perform the update to get m0
i , then
then send the proof of update Pupdate ¼
compute R0 ,
ffmij gj2M ; H ðmi Þ; Wi ; R0 ; sigg to the client.
VerifyUpdate: After the client received H ðmi Þ, it will first
compute R using H ðmi Þ; Wi and verify sig, then it will
i using ffmij gj2M ; mnewg and then compute Rnew
compute m0
with fm0
i ; Wi g and compare Rnew with R0 . If Rnew ¼ R0 , then
i ; sig0 g to CSS for it to update
the client will return f0
accordingly.
For an SP operation the process will be the same to our
basic scheme as there are no new data inserted into T ,
therefore the retrieving of the entire data block is inevitable
when computations of 0
i and  are required. For other
types of operations, no old data is involved in new blocks;
therefore the processes will also remain the same. The
process is shown in Fig. 5.

4.6 Extensions and Generalizations
Our strategy can also be applied in RSA-based PDP or POR
schemes to achieve authorized auditing and fine-grained
data update requests. As RSA can inherently support
variable-sized blocks, the process will be even easier. The
batch auditing variation in [6], [7] can also be applied to our
scheme, as we did not change the construction of HLAs and
the verifications on them.
For the same reason, the random masking strategy for
privacy preserving proposed in [7] can also be incorpo-
rated into our scheme to prevent TPA from parsing the
challenged file blocks through a series of integrity proofs
to a same set of blocks. Alternatively, we can also restrict
the number of challenges to the same subset of data blocks.
When data updates are frequent enough, the success rate
of this attack will drop dramatically, because there is a
high probability that one or many of the challenged blocks
have already updated before c challenges are completed,
which is the reason we did not incorporate this strategy
into our scheme.

5 SECURITY ANALYSIS
In this section , the soundness and security of our scheme
is discussed separately in phases , as the aim and behavior
of the malicious adversary in each phase of our scheme is
different .

LIU ET AL.: PUBLIC AUDITING OF BIG DATA WITH FINE-GRAINED UPDATES ON CLOUD

2241

Fig. 5. Verifiable PM-typed Data Update in our modified scheme.

5.1 Challenge and Verification
In the challenge/verification process of our scheme, we try
to secure the scheme against a malicious CSS who tries to
cheat the verifier TPA about the integrity status of the
client’s data, which is the same as previous work on both
PDP and POR. In this step, aside from the new authoriza-
tion process (which will be discussed in detail later in this
section), the only difference compared to [6] is the RMHT
and variable-sectored blocks. Therefore, the security of this
phase can be proven through a process highly similar with
[6], using the same framework, adversarial model and
interactive games defined in [6]. A detailed security proof
for this phase is therefore omitted here.

5.2
TPA Authorization
Security of the new authorization strategy in our scheme is
based on the existential unforgeability of the chosen
signature scheme. We first define the behavior of a
malicious third-party auditor.

Definition 3 (Malicious TPA). A malicious TPA is a third
party who aims at challenging a user’s data stored on CSS for
integrity proof without this user’s permission. The malicious
TPA has access to the entire network.

According to this definition, none of the previous data
auditing schemes is resilient against a malicious TPA.
Now, in our scheme, we have the following theorem:

Theo rem 2 . Through the authorization process, no malicious
TPA can cause the CSS to respond with an integrity proof P
over an arbitrary subset of file F , namely mi ; i 2 I , unless a
negligible probability.

Proof. See Appendix D.

g

From this theorem, we can see that the security of a
public auditing scheme is strengthened by adding the
authorization process. In fact, the scheme is now resilient
against malicious or pretended auditing requests, as well
as potential DDOS attacks launched by malicious auditors.
For even higher security, the client may mix in a nonce to
the authorization message to make every auditing message
distinct, so that no one can utilize a previous authorization
message. However, this setting may not be appropriate for
many scenarios, as the client must stay online when each
auditing happens.

5.3 Verifiable Data Updating
In the verifiable updating process, the main adversary is the
untrustworthy CSS who did not carry out the data update
successfully, but still manages to return a satisfactory
response to the client thereafter. We now illustrate the security
of this phase of our scheme in the following theorem:

Theorem 3. In the verifiable update process in both our basic
scheme and the modification, CSS cannot provide the client
with the satisfactory result, i.e., R0 cannot match the Rnew
computed by the client with fH ðm0
i Þ; Wi g, if CSS did not
update the data as requested.

Proof. See Appendix D.

g

Note that in the verifiable update process, data retrieval
is a part of the verifiable update process. According to
Assumption 1, CSS will respond this query with the correct
mii . If not with Assumption 1, it is recommended to indepen-
dently retrieve fmij gj2M before the update so that CSS cannot
cheat the client intentionally, as it cannot distinguish
whether the following update is based on this retrieval.
If CSS can be trusted even more, the client may let CSS
Þ (where mij are the sectors that did not
compute ðumij
j
change) and send it back to the client, then the client will be
able to compute 0 using it along with mnew and H ðm0
i Þ. This
will keep the communication cost of this phase on a
cons tan t ly low leve l . However , as the CSS is on ly
considered semi-trusted and it is difficult for the client to
Þ without mij , this assumption is unfortunately
verify ðmij
j
too strong for the majority of scenarios.

6 EVALUATION AND EXPERIMENTAL RESULTS
We have provided an overall evaluation and comparison in
Appendix E.
We conducted our experiments on U-CloudVa cloud
computing environment located in University of Technol-
ogy, Sydney (UTS). The computing facilities of this system
are located in several labs in the Faculty of Engineering and
IT, UTS. On top of hardware and Linux OS, We installed
KVM Hypervisor [26] which virtualizes the infrastructure
and allows it to provide unified computing and storage
resources. Upon virtualized data centers, Hadoop [27] is
installed to facilitate the MapReduce programming model
and distributed file system. Moreover, we installed OpenStack

2242

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 9, SEPTEMBER 2014

open source cloud platform [28] which is responsible for
global management, resource scheduling, task distribution
and interaction with users.
We implemented both our scheme and its modification
on U-Cloud, using a virtual machine with 36 CPU cores,
32GB RAM and 1TB storage in total. As in previous work
[6], [12], we also used a 1GB randomly generated dataset
for testing. The scheme is implemented under 80-bit
security, i.e.,  ¼ jpj ¼ 160 bits. As the number of sectors s
(per block) is one of the most influential metrics to overall
performance, we will use it as our primary metrics. For
saving of the first wave of allocated storage, we used
si ¼ smax in the initial data splitting and uploading. Note
that smax decides the total number of blocks for an arbitrary
jF j. However, according to [10], the number of authenti-
cated blocks is a constant with respect
to a certain
percentage of file tampered and a certain success rate of
detection, therefore we will not take the number of audited
blocks as our primary variable of measurement. All
experimental results are an average of 20 runs.
We first tested how smax can influence the size of proof
P , which is missing in former schemes [6], [7]. From Fig. 6,
we can see that generally the proof size decreases when
smax increases, because the average depth of leaf nodes mi
of T decreases when smax increases to a certain level, es-
pecially when right after the initial uploading of F . Note
that the storage of HLA and RMHT at CSS side will also
decrease with the increase of the average number of blocks.
Therefore, a relatively large smax (but not too large, which
we will discuss along with the third experiment) is re-
commended in our dynamic setting.
Second, we tested the storage overhead for small
insertions. Without support for fine-grained updates, every
small insertion will cause creation of a whole new block
and update of related MHT nodes, which is why our
scheme has efficiency advantage . We compared our
scheme against a representative (and also recent) public
auditing scheme [6]. For comparison, we extended the
older scheme a bit to let it support the communication-
storage trade-off introduced in [15] so that it can support
larger file blocks with multiple (but only a predefined
constant number of) sectors each. The updates chosen for
experiments are 10  140 Bytes and 10  280 Bytes, filled
with random data. Results are shown in Figs. 7 and 8. For
updates of the same total size, the increased storage on CSS

Fig. 7. Comparison of the total storage overhead invoked by 10 140-byte
insertions to the i-th block in our scheme, as opposed to the direct
extension of [6].

for our scheme stays constant, while in the extended old
scheme [6] (see Section 3.2.2) the storage increases linearly
with the increase in size of the affected block. These results
demonstrated that our scheme with fine-grained data
update support can incur significantly lower storage
overhead (down to 0:14 in our test scenarios) for small
insertions when compared to existing scheme.
Third, we investigated the performance improvement
of the modification introduced in Section 4.5. We used
3 pieces of random data with sizes of 100 bytes, 140 bytes
and 180 bytes, respectively, to update several blocks that
contain 10 to 50 standard 20-byte sectors each. Data
retrieval is a key factor of communication overheads in
the verifiable update phase. For each update, we recorded
the total amount of data retrieval for both our modified
scheme and our basic scheme. The results in comparison are
shown in Fig. 9. We can see that our modified scheme always
has better efficiency with respect to data-retrieval-invoked
communication overheads, and the advantage is more
significant for larger updates. However, for an update of the
same size, the advantage will decrease with the increase of
jsi j where a larger number of sectors in the original file are
needed to be retrieved. Therefore, the block size needs to be
kept low if less communication in verifiable updates is
highly demanded.
From the experimental results on small updates, we can
see that our scheme can incur significantly lower storage
overhead while our modified scheme can dramatically
reduce communication overheads compared to the existing
scheme. In practice, the important parameter smax should

Fig. 6. Communication overhead invoked by an integrity proof with 80-bit
security under different smax for a 1GB data.

Fig. 8. Comparison of the total storage overhead invoked by 10 280-byte
insertions to the i-th block in our scheme, as opposed to the direct
extension of [6].

LIU ET AL.: PUBLIC AUDITING OF BIG DATA WITH FINE-GRAINED UPDATES ON CLOUD

2243

[4]

[2] M. Armbrust, A. Fox, R. Griffith, A.D. Joseph, R. Katz, A. Konwinski,
G. Lee, D. Patterson, A. Rabkin, I. Stoica, and M. Zaharia, ‘‘A View of
Cloud Computing,’’ Commun. ACM, vol. 53, no. 4, pp. 50-58,
Apr. 2010.
[3] Customer Presentations on Amazon Summit Australia, Sydney,
2012, accessed on: March 25, 2013. [Online]. Available : http://
aws.amazon.com/apac/awssummit-au/.
J. Yao, S. Chen, S. Nepal, D. Levy, and J. Zic, ‘‘TrustStore: Making
Amazon S3 Trustworthy With Services Composition,’’ in Proc.
10th IEEE/ACM Int’l Symposium on Cluster, Cloud and Grid
Computing (CCGRID), 2010, pp. 600-605.
[5] D. Zissis and D . Lekkas, ‘‘Addressing Cloud Computing Secu-
rity Issues,’’ Future Gen. Comput. Syst., vol. 28, no. 3, pp. 583-592,
Mar. 2011.
[6] Q. Wang, C. Wang, K. Ren, W. Lou, and J. Li, ‘‘Enabling Public
Auditability and Data Dynamics for Storage Security in Cloud
Computing,’’ IEEE Trans. Parallel Distrib. Syst., vol. 22, no. 5,
pp. 847-859, May 2011.
[7] C. Wang, Q. Wang, K. Ren, and W. Lou, ‘‘Privacy-Preserving
Public Auditing for Data Storage Security in Cloud Computing,’’
in Proc. 30st IEEE Conf. on Comput. and Commun. (INFOCOM),
2010, pp. 1-9.
[8] G. Ateniese, R.D. Pietro, L.V. Mancini, and G. Tsudik, ‘‘Scalable
and E f f ic ien t P rovab le Da ta Possess ion , ’ ’ in P ro c . 4 th In t ’ l
Con f . Secu r i ty and P r iva cy in C ommun . N e tw . (S ecu r eComm ) ,
2008 , pp . 1 -10 .
[9] G. Ateniese, R. Burns, R. Curtmola, J. Herring, O. Khan, L. Kissner,
Z. Peterson, and D. Song, ‘‘Remote Data Checking Using Provable
Data Possession,’’ ACM Trans. Inf. Syst. Security, vol. 14, no. 1, May
2011, Article 12.
[10] G. Ateniese, R.B. Johns, R. Curtmola, J. Herring, L. Kissner, Z. Peterson,
and D. Song, ‘‘Provable Data Possession at Untrusted Stores,’’ in
Proc. 14th ACM Conf. on Comput. and Commun. Security (CCS), 2007,
pp. 598-609.
[11] R. Curtmola, O. Khan, R.C. Burns, and G. Ateniese, ‘‘MR-PDP:
Multiple-Replica Provable Data Possession,’’ in Proc. 28th IEEE
Conf. on Distrib. Comput. Syst. (ICDCS), 2008, pp. 411-420.
[12] C. Erway, A. Ku¨ pc¸ u¨ , C. Papamanthou , and R . Tamassia ,
‘‘Dynamic Provable Data Possession,’’ in Proc. 16th ACM Conf.
on Comput. and Commun. Security (CCS), 2009, pp. 213-222.
[13] Y. Zhu, H. Hu, G.-J. Ahn, and M. Yu, ‘‘Cooperative Provable
Data Possession for Integrity Verification in Multi-Cloud Storage,’’
IEEE Trans. Parallel Distrib. Syst., vol. 23 , no. 12, pp. 2231-2244,
Dec. 2012.
[14] A. Juels and B.S. Kaliski Jr., ‘‘PORs: Proofs of Retrievability for
Large Files,’’ in Proc. 14th ACM Conf. on Comput. and Commun.
Security (CCS), 2007, pp. 584-597.
[15] H. Shacham and B. Waters, ‘‘Compact Proofs of Retrievability,’’
in Proc. 14th Int’l Conf. on Theory and Appl. of Cryptol. and Inf.
Security (ASIACRYPT), 2008, pp. 90-107.
[16] S. Nepal, S. Chen, J. Yao, and D. Thilakanathan, ‘‘DIaaS: Data
Integrity as a Service in the Cloud,’’ in Proc. 4th Int’l Conf. on
Cloud Computing (IEEE CLOUD), 2011, pp. 308-315.
[17] Y. He , S. Barman, and J.F. Naughton, ‘‘Preventing Equivalence
Attacks in Updated, Anonymized Data,’’ in Proc. 27th IEEE Int’l
Conf. on Data Engineering (ICDE), 2011, pp. 529-540.
[18] E. Naone, ‘‘What Twitter Learns From All Those Tweets,’’ in
Technology Review, Sept. 2010 , accessed on: March 25, 2013.
[On l in e ] . Av a i l ab l e : h t tp ://www . t e chno logy r ev i ew . com/
view/420968/what-twitter-learns-from-all-those-tweets/
[19] X. Zhang, L.T. Yang, C. Liu, and J. Chen, ‘‘A Scalable Two-Phase
Top-Down Specialization Approach for Data Anonymization
Using MapReduce on Cloud,’’ IEEE Trans. Parallel Distrib. Syst.,
vol. 25, no. 2, pp. 363-373, Feb. 2014.
‘‘Security and Privacy in the AWS Cloud,’’
[20] S.E. Schmidt,
presen ted a t
the Presen tat ion Amazon Summ i t Aus tra l ia ,
Sydney, Australia, May 2012, accessed on: March 25, 2013. [Online].
Available: http://aws.amazon.com/apac/awssummit-au/.
[21] C. Liu, X. Zhang, C. Yang, and J. Chen, ‘‘CCBKEVSession Key
Nego t ia t ion for Fas t and Secure Schedu l ing o f Sc ien t i f ic
Applications in Cloud Computing,’’ Future Gen. Comput. Syst.,
vol. 29, no. 5, pp. 1300-1308, July 2013.
[22] X. Zhang, C. Liu, S. Nepal, S. Panley, and J. Chen, ‘‘A Privacy
Leakage Upper-Bound Constraint Based Approach for Cost-
Effective Privacy Preserving of Intermediate Datasets in Cloud,’’
IEEE Trans. Parallel Distrib. Syst., vol. 24, no. 6, pp. 1192-1202,
June 2013.

Fig. 9. Percentage in saving of communication overhead in data retrieval
in the modified scheme, compared to our basic scheme.

be carefully chosen according to different data size and
different efficiency demands in storage or communica-
tions. For example, for general applications with a similar
scale (1GB per dataset and frequent 140-byte updates), a
choice of smax ¼ 30 will allow the scheme to incur sig-
nificantly lowered overheads in both storage and commu-
nications during updates. Additional analysis regarding
efficiency can be found in Appendix F.

7 CONCLUSION AND FUTURE WORK
In this paper, we have provided a formal analysis on
possible types of fine-grained data updates and proposed a
scheme that can fully support authorized auditing and
fine-grained update requests. Based on our scheme, we
have also proposed a modification that can dramatically
reduce communication overheads for verifications of small
updates. Theoretical analysis and experimental results
have demonstrated that our scheme can offer not only
enhanced security and flexibility, but also significantly
lower overheads for big data applications with a large
number of frequent small updates such as applications in
social media and business transactions.
Based on the contributions of this paper on improved
data auditing, we plan to further investigate the next step
on how to improve other server-side protection methods
for efficient data security with effective data confidentiality
and availability. Besides, we also plan to investigate
auditability-aware data scheduling in cloud computing.
As data security is also considered as a metric of quality-of-
service (QoS) along with other metrics such as storage and
computation, a highly efficient security-aware scheduling
scheme will play an essential role under most cloud
computing contexts.

ACKNOWLEDGMENT
This research work is partly supported by Australian
Research Council under Linkage Project LP0990393.

REFERENCES
[1] R. Buyya, C.S. Yeo, S. Venugopal, J. Broberg, and I. Brandic,
‘‘Cloud Computing and Emerging IT Platforms: Vision, Hype,
Reality for Delivering Computing as the 5th Utility,’’ Future Gen.
Comput. Syst., vol. 25, no. 6, pp. 599-616, June 2009.

2244

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 9, SEPTEMBER 2014

[23] D. Boneh, H. Shacham, and B. Lynn, ‘‘Short Signatures From the
Weil Pairing,’’ J. Cryptol., vol. 17, no. 4, pp. 297-319, Sept. 2004.
[24] G. Ateniese, S. Kamara, and J. Katz, ‘‘Proofs of Storage From
Homomorphic Identification Protocols,’’ in Proc. 15th Int’l Conf.
on Theory and Appl. of Cryptol. and Inf. Security (ASIACRYPT),
2009, pp. 319-333.
[25] R.C. Merkle,
‘‘A Digital Signature Based on a Conventional
Encryption Function,’’ in Proc. Int’l Cryptol. Conf. on Adv. in
Cryptol. (CRYPTO), 1987, pp. 369-378.
[26] KVM Hypervisor , accessed on : March 25 , 2013 .
Available: www.linux-kvm.org/.
[27] Hadoop MapReduce .
[On l ine ] . Ava i lab le : h t tp ://hadoop .
apache.org
[28] OpenStack Open Source Cloud Software, accessed on: March 25,
2013. [Online]. Available: http://openstack.org/

[On l ine] .

Chang Liu received BEng and MSc degrees
from Shandong University, China. He is currently
pursuing the PhD degree at Faculty of Engi-
nee r ing and IT , Un i ve r s i t y o f Techno logy
Sydney, Australia. His research interests include
cloud and distributed computing, resource man-
agement, cryptography and data security.

Jinjun Chen received the PhD degree in computer
science and software engineering from Swinburne
University of Technology, Australia. He is an
Associate Professor from Faculty of Engineering
and IT, University of Technology Sydney (UTS),
Australia. He is the Director of Lab of Cloud
Computing and Distributed Systems at UTS. His
research interests include cloud computing, big
data, workflow management, privacy and security,
and related various research topics. His research
results have been published in more than 100
papers in high quality journals and at conferences,
including IEEE
Transactions on Service Computing, ACM Transactions on Autonomous
and Adaptive Systems, ACM Transactions on Software Engineering and
Methodology (TOSEM),
IEEE Transactions on Software Engineering
(TSE), and IEEE Transactions on Parallel and Distributed Systems
(TPDS). He received Swinburne Vice-Chancellor’s Research Award for
early career researchers (2008), IEEE Computer Society Outstanding
Leadership Award (2008-2009) and (2010-2011), IEEE Computer Society
Service Award (2007), Swinburne Faculty of
ICT Research Thesis
Excellence Award (2007). He is an Associate Editor for IEEE Transactions
on Parallel and Distributed Systems. He is the Vice Chair of IEEE Computer
Society’s Technical Committee on Scalable Computing (TCSC), Vice Chair
of Steering Committee of Australasian Symposium on Parallel and
Distributed Computing, Founder and Coordinator of IEEE TCSC Technical
Area on Workflow Management in Scalable Computing Environments,
Founder and steering committee co-chair of International Conference on
Cloud and Green Computing, and International Conference on Big Data and
Distributed Systems. He is a Senior Member of IEEE.

Laurence T. Yang received the BE degree in
computer science and technology from Tsinghua
University, Beijing, China, and the PhD degree in
computer science from the University of Victoria,
Victoria, BC, Canada. He is a Professor in School of
Computer Science and Technology, Huazhong
University of Science and Technology, China and
in Department of Computer Science, St. Francis
Xavier University, Canada. His current research
interests include parallel and distributed computing,
embedded and ubiquitous computing. His research
has been supported by National Sciences and Engineering Research
Council, Canada and Canada Foundation for Innovation. He is a member
of IEEE.

Xuyun Zhang received the BS and MS degrees in
computer science from Nanjing University, China,
and is currently working towards the PhD degree at
the Faculty of Engineering & IT, University of Tech-
nology, Sydney, Australia. His research interests
include cloud computing, privacy and security, Big
Data, MapReduce and OpenStack. He has pub-
lished several papers in refereed international
journals including IEEE Transactions on Parallel
and Distributed Systems (TPDS).

Chi Yang received the BS degree from Shandong
University At Weihai, China, the MS (by research)
degree in computer science from Swinburne
University of Technology, Melbourne, Australia, in
2007, and is currently pursuing full-time the PhD
degree at the University of Technology, Sydney,
Australia. His major research interests include
distributed computing, XML data stream, scientific
workflow, Distributed System, Green Computing,
Big Data Processing and Cloud Computing.

Ra j iv Ran jan rece ived the PhD degree in
engineering from the Un iversity of Melbourne,
Australia, in 2009. He is a Research Scientist
and a Julius Fellow in CSIRO Computational
Informatics Division (formerly known as CSIRO
ICT Centre). His expertise is in datacenter cloud
computing, application provisioning, and perfor-
mance optimization. He has published 62 scien-
tific, peer-reviewed papers (7 books, 25 journals,
25 conferences, and 5 book chapters). His h-index
is 20, with a lifetime citation count of 1660+ (Google
Scholar). His papers have also received 140+ ISI citations. Seventy percent
of his journal papers and 60 percent of conference papers have been A*/A
ranked ERA publication. Dr. Ranjan has been invited to serve as the Guest
Editor for leading distributed systems journals including IEEE Transactions
on Cloud Computing, Future Generation Computing Systems, and
Software Practice and Experience. One of his papers was in 2011’s top
computer science journal, IEEE Communication Surveys and Tutorials.

Ramamohanarao (Rao) Kotagiri received the
PhD degree from Monash Un iversity. He was
awarded the Alexander von Humboldt Fellow-
ship in 1983. He has been at the University of
Melbourne since 1980 and was appointed as a
Professor in computer science in 1989. He has
held several senior positions including Head of
Computer Science and Software Engineering,
Head of the School of Electrical Engineering and
Computer Science at the University of Melbourne
and Research Director for the Cooperative Re-
search Centre for Intelligent Decision Systems. He served on the editorial
boards of the Computer Journal. At present, he is on the editorial boards
of Universal Computer Science, and Data Mining, IEEE Transactions on
Knowledge and Data Engineering and VLDB (Very Large Data Bases)
Journal. Dr. Kotagiri was the program cochair
for VLDB, PAKDD,
DASFAA, and DOOD conferences. He is a steering committee member
of IEEE ICDM, PAKDD, and DASFAA. He received a Distinguished
Contribution Award for Data Mining. He is a fellow of the Institute of
Engineers Australia,
the Australian Academy Technological Sciences
and Engineering, and the Australian Academy of Science. He was
awarded a Distinguished Contribution Award in 2009 by the Computing
Research and Education Association of Australasia.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

222

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 1,

JANUARY 2014

Privacy-Preserving Multi-Keyword Ranked
Search over Encrypted Cloud Data

N ing Cao, Member, IEEE, Cong Wang, Member, IEEE, M ing L i, Member, IEEE,
Ku i Ren, Sen ior Member, IEEE, and Wen j ing Lou, Senior Member, IEEE

Abstract—With the advent of cloud computing, data owners are motivated to outsource their complex data management systems
from local sites to the commercial public cloud for great flexibility and economic savings. But for protecting data privacy, sensitive
data have to be encrypted before outsourcing, which obsoletes traditional data utilization based on plaintext keyword search. Thus,
enabling an encrypted cloud data search service is of paramount importance. Considering the large number of data users and
documents in the cloud, it is necessary to allow multiple keywords in the search request and return documents in the order of their
relevance to these keywords. Related works on searchable encryption focus on single keyword search or Boolean keyword search,
and rarely sort the search results. In this paper, for the first time, we define and solve the challenging problem of privacy-preserving
multi-keyword ranked search over encrypted data in cloud computing (MRSE). We establish a set of strict privacy requirements for
such a secure cloud data utilization system. Among various multi-keyword semantics, we choose the efficient similarity measure of
“coordinate matching,” i.e., as many matches as possible, to capture the relevance of data documents to the search query. We
further use “inner product similarity” to quantitatively evaluate such similarity measure. We first propose a basic idea for the MRSE
based on secure inner product computation, and then give two significantly improved MRSE schemes to achieve various stringent
privacy requirements in two different threat models. To improve search experience of the data search service, we further extend
these two schemes to support more search semantics. Thorough analysis investigating privacy and efficiency guarantees of
proposed schemes is given. Experiments on the real-world data set further show proposed schemes indeed introduce low
overhead on computation and communication.

Index Terms—Cloud computing, searchable encryption, privacy-preserving, keyword search, ranked search

Ç

1 INTRODUCTION
C LOUD computing is the long dreamed vision of
computing as a utility, where cloud customers can
remotely store their data into the cloud so as to enjoy the
on-demand high-quality applications and services from a
shared pool of configurable computing resources [2], [3]. Its
great flexibility and economic savings are motivating both
individuals and enterprises to outsource their local com-
plex data management system into the cloud. To protect
data privacy and combat unsolicited accesses in the cloud
and beyond, sensitive data, for example, e-mails, personal
health records, photo albums, tax documents, financial
transactions, and so on, may have to be encrypted by data
owners before outsourcing to the commercial public cloud
[4]; this, however, obsoletes the traditional data utilization

. N. Cao is with Walmart Labs, 444 Castro St, Mountain View, CA 94041.
E-mail: ncao@walmartlabs.com.
. C. Wang is with the Department of Computer Science, City University of
Hong Kong, 83 Tat Chee Avenue, Kowloon, Hong Kong.
E-mail: congwang@cityu.edu.hk.
. M. Li is with the Department of Computer Science, Utah State University,
4205 Old Main Hill, Logan, UT 84322. E-mail: ming.li@usu.edu.
. K. Ren is with the Department of Computer Science and Engineering,
University at Buffalo, The State University of New York, 317 Davis Hall,
Buffalo, NY 14260. E-mail: kuiren@buffalo.edu.
. W. Lou is with the Department of Computer Science, Northern Virginia
Center, Virginia Polytechnic Institute and State University, 7054 Haycock
Road, Falls Church, VA 22043. E-mail: wjlou@vt.edu.

Manuscript received 6 Dec. 2012; accepted 31 Jan. 2013; published online 15
Feb. 2013.
Recommended for acceptance by D. Turgut.
For information on obtaining reprints of this article, please send e-mail to:
tpds@computer.org, and reference IEEECS Log Number TPDS-2012-12-1210.
Digital Object Identifier no. 10.1109/TPDS.2013.45.

service based on plaintext keyword search. The trivial
solution of downloading all the data and decrypting locally
is c lear ly impract ica l , due to the huge amount of
bandwidth cost in cloud scale systems. Moreover, aside
from eliminating the local storage management, storing
data into the cloud serves no purpose unless they can be
easily searched and utilized. Thus, exploring privacy-
preserving and effective search service over encrypted
cloud data is of paramount importance. Considering the
potentially large number of on-demand data users and
huge amount of outsourced data documents in the cloud,
this problem is particularly challenging as it is extremely
difficult to meet also the requirements of performance,
system usability, and scalability.
On the one hand, to meet the effective data retrieval
need, the large amount of documents demand the cloud
server to perform result relevance ranking,
instead of
returning undifferentiated results. Such ranked search
system enables data users to find the most relevant
information quickly, rather than burdensomely sorting
through every match in the content collection [5]. Ranked
search can also elegantly eliminate unnecessary network
traffic by sending back only the most relevant data, which
is highly desirable in the “pay-as-you-use” cloud para-
digm. For privacy protection, such ranking operation,
however, should not leak any keyword related information.
On the other hand, to improve the search result accuracy as
well as to enhance the user searching experience, it is also
necessary for such ranking system to support multiple
keywords search, as single keyword search often yields far
too coarse results. As a common practice indicated by
today’s web search engines (e.g., Google search), data users

1045-9219/14/$31.00 ß 2014 IEEE

Published by the IEEE Computer Society

CAO ET AL.: PRIVACY-PRESERVING MULTI-KEYWORD RANKED SEARCH OVER ENCRYPTED CLOUD DATA

223

may tend to provide a set of keywords instead of only one
as the indicator of their search interest to retrieve the most
relevant data. And each keyword in the search request is
able to help narrow down the search result
further.
“Coordinate matching” [6],
i.e., as many matches as
possible,
is an efficient similarity measure among such
multi-keyword semantics to refine the result relevance,
and has been widely used in the plaintext information
retrieval (IR) community. However, how to apply it in the
encrypted cloud data search system remains a very
challenging task because of inherent security and privacy
obstacles,
including various strict requirements like the
data privacy, the index privacy, the keyword privacy, and
many others (see Section 3.2).
In the literature, searchable encryption [7], [8], [9], [10],
[11], [12], [13], [14], [15] is a helpful technique that treats
encrypted data as documents and allows a user to securely
search through a single keyword and retrieve documents of
interest. However, direct application of these approaches to
the secure large scale cloud data utilization system would
not be necessarily suitable, as they are developed as
cryptoprimitives and cannot accommodate such high
service-level requirements like system usability, user
searching experience, and easy information discovery.
Although some recent designs have been proposed to
support Boolean keyword search [16], [17], [18], [19], [20],
[21], [22], [23], [24] as an attempt to enrich the search
flexibility, they are still not adequate to provide users with
acceptable result ranking functionality (see Section 7). Our
early works [25], [26] have been aware of this problem, and
provide solutions to the secure ranked search over
encrypted data problem but only for queries consisting of
a single keyword. How to design an efficient encrypted
data search mechanism that supports multi-keyword
semantics without privacy breaches still remains a challen-
ging open problem.
In this paper, for the first time, we define and solve the
problem of multi-keyword ranked search over encrypted
cloud data (MRSE) while preserving strict systemwise
privacy in the cloud computing paradigm. Among various
multi-keyword semantics, we choose the efficient similarity
measure of “coordinate matching,” i.e., as many matches as
possible, to capture the relevance of data documents to the
search query. Specifically, we use “inner product similar-
ity” [6], i.e., the number of query keywords appearing in a
document, to quantitatively evaluate such similarity mea-
sure of that document to the search query. During the index
construction, each document is associated with a binary
vector as a subindex where each bit represents whether
corresponding keyword is contained in the document. The
search query is also described as a binary vector where each
bit means whether corresponding keyword appears in this
search request, so the similarity could be exactly measured
by the inner product of the query vector with the data
vector. However, directly outsourcing the data vector or the
query vector will violate the index privacy or the search
privacy. To meet the challenge of supporting such multi-
keyword semantic without privacy breaches, we propose
a basic idea for the MRSE using secure inner product
computation, which is adapted from a secure k-nearest
neighbor (kNN) technique [27], and then give two sig-
nificantly improved MRSE schemes in a step-by-step
manner to achieve various stringent privacy requirements

Fig. 1. Architecture of the search over encrypted cloud data.

in two threat models with increased attack capabilities. Our
contributions are summarized as follows:

1.

For the first time, we explore the problem of multi-
keyword ranked search over encrypted cloud data,
and establish a set of strict privacy requirements for
such a secure cloud data utilization system.
2. We propose two MRSE schemes based on the
similarity measure of “coordinate matching” while
meeting different privacy requirements in two
different threat models.
3. We investigate some further enhancements of our
ranked search mechanism to support more search
semantics and dynamic data operations.
4. Thorough analysis investigating privacy and effi-
ciency guarantees of the proposed schemes is given,
and experiments on the real-world data set further
show the proposed schemes indeed introduce low
overhead on computation and communication.
Compared with the preliminary version [1] of this paper,
this journal version proposes two new mechanisms to
support more search semantics. This version also studies
the support of data/index dynamics in the mechanism
design. Moreover, we improve the experimental works by
adding the analysis and evaluation of two new schemes. In
addition to these improvements, we add more analysis on
secure inner product and the privacy part.
The remainder of this paper is organized as follows: In
Section 2, we introduce the system model, the threat model,
our design goals, and the preliminary. Section 3 describes
the MRSE framework and privacy requirements, followed by
Section 4, which describes the proposed schemes. Section 5
presents simulation results. We discuss related work on
both single and Boolean keyword searchable encryption in
Section 6, and conclude the paper in Section 7.

2 PROBLEM FORMULATION
2.1 System Model
Considering a cloud data hosting service involving three
different entities, as illustrated in Fig. 1: the data owner, the
data user, and the cloud server. The data owner has a
collection of data documents F to be outsourced to the
cloud server in the encrypted form C. To enable the
searching capability over C for effective data utilization,
the data owner, before outsourcing, will first build an
encrypted searchable index I from F , and then outsource
both the index I and the encrypted document collection C
to the cloud server. To search the document collection for t
given keywords, an authorized user acquires a correspond-
ing trapdoor T through search control mechanisms, for

224

example, broadcast encryption [10]. Upon receiving T from
a data user, the cloud server is responsible to search the
index I and return the corresponding set of encrypted
documents. To improve the document retrieval accuracy,
the search result should be ranked by the cloud server
according to some ranking criteria (e.g., coordinate match-
ing, as will be introduced shortly). Moreover, to reduce the
communication cost, the data user may send an optional
number k along with the trapdoor T so that the cloud server
only sends back top-k documents that are most relevant to
the search query. Finally, the access control mechanism [28]
is employed to manage decryption capabilities given to
users and the data collection can be updated in terms of
inserting new documents, updating existing documents,
and deleting existing documents.

JANUARY 2014
IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 1,
C—the encrypted document collection stored in the
.
cloud server, denoted as C ¼ ðC1 ; C2 ; . . . ; Cm Þ.
. W—the dictionary, i.e., the keyword set consisting of
n keyword, denoted as W ¼ ðW1 ; W2 ; . . . ; Wn Þ.
I—the searchable index associated with C , denoted
f
as ðI1 ; I2 ; . . . ; Im Þ where each subindex Ii
is built
f
for Fi .
W—the subset of W , representing the keywords in a
f
.
T e
W ¼ ðWj1 ; Wj2 ; . . . ; Wjt Þ.
search request, denoted as
. F e
W .
f
.
W—the trapdoor for the search request
W—the ranked id list of all documents according to
W .
their relevance to

.

2.2 Threat Model
The cloud server is considered as “honest-but-curious” in
our model, which is consistent with related works on cloud
security [28], [29]. Specifically, the cloud server acts in an
“honest” fashion and correctly follows the designated
protocol specification. However, it is “curious” to infer
and analyze data (including index) in its storage and
message flows received during the protocol so as to learn
additional
information. Based on what information the
cloud server knows, we consider two threat models with
different attack capabilities as follows.
Known ciphertext model. In this model, the cloud server is
supposed to only know encrypted data set C and searchable
index I , both of which are outsourced from the data owner.
Known background model. In this stronger model, the cloud
server is supposed to possess more knowledge than what can
be accessed in the known ciphertext model. Such information
may include the correlation relationship of given search
requests (trapdoors), as well as the data set related statistical
information. As an instance of possible attacks in this case,
the cloud server could use the known trapdoor information
combined with document/keyword frequency [30] to
deduce/identify certain keywords in the query.

2.3 Design Goals
To enable ranked search for effective utilization of out-
sourced cloud data under the aforementioned model, our
system design should simultaneously achieve security and
performance guarantees as follows.

.

. Mu l t i -keyword ranked search . To des ign search
schemes which allow multi-keyword query and
provide result similarity ranking for effective data
retrieval, instead of returning undifferentiated re-
sults.
Privacy-preserving. To prevent the cloud server from
learning additional information from the data set
and the index, and to meet privacy requirements
specified in Section 3.2.
Efficiency. Above goals on functionality and privacy
should be achieved with low communication and
computation overhead.

.

2.4 Notations
. F —the plaintext document collection, denoted as a
set of m data documents F ¼ ðF1 ; F2 ; . . . ; Fm Þ.

2.5 Preliminary on Coordinate Matching
As a hybrid of conjunctive search and disjunctive search,
“coordinate matching” [6] is an intermediate similarity
measure which uses the number of query keywords
appearing in the document to quantify the relevance of
that document to the query. When users know the exact
subset of the data set to be retrieved, Boolean queries
perform well with the precise search requirement specified
by the user. In cloud computing, however, this is not the
practical case, given the huge amount of outsourced data.
Therefore, it is more flexible for users to specify a list of
keywords indicating their interest and retrieve the most
relevant documents with a rank order.

3 FRAMEWORK AND PRIVACY REQUIREMENTS FOR
MRSE

In this section, we define the framework of multi-keyword
ranked search over encrypted cloud data (MRSE) and
establish various strict systemwise privacy requirements for
such a secure cloud data utilization system.

.

.

3.1 MRSE Framework
For easy presentation, operations on the data documents
are not shown in the framework since the data owner could
easily employ the traditional symmetric key cryptography
to encrypt and then outsource data. With focus on the
index and query,
the MRSE system consists of
four
algorithms as follows:
Setup ð1‘ Þ. Taking a security parameter ‘ as input, the
data owner outputs a symmetric key as SK .
BuildIndex ðF ; SK Þ. Based on the data set F , the data
owner builds a searchable index I which is encrypted by
the symmetric key SK and then outsourced to the cloud
f
f
server. After the index construction,
the document
this algorithm generates a corresponding trapdoor T e
collection can be independently encrypted and outsourced.
Trapdoor ð
W as input,
W Þ. With t keywords of interest in
. Query ðT e
.
request as (T e
W .
W ; k; I Þ. When the cloud server receives a query
the index I with the help of trapdoor T e
returns F e
W , k), it performs the ranked search on
f
W , and finally
W , the ranked id list of top-k documents sorted
W .
by their similarity with
Neither the search control nor the access control is within
the scope of this paper. While the former is to regulate how
authorized users acquire trapdoors, the later is to manage
users’ access to outsourced documents.

CAO ET AL.: PRIVACY-PRESERVING MULTI-KEYWORD RANKED SEARCH OVER ENCRYPTED CLOUD DATA

225

3.2 Privacy Requirements for MRSE
The representative privacy guarantee in the related litera-
ture, such as searchable encryption, is that the server should
learn nothing but search results. With this general privacy
description, we explore and establish a set of strict privacy
requirements specifically for the MRSE framework.
As for the data privacy, the data owner can resort to the
traditional symmetric key cryptography to encrypt the data
before outsourcing, and successfully prevent the cloud
server from prying into the outsourced data. With respect to
the index privacy, if the cloud server deduces any association
between keywords and encrypted documents from index, it
may learn the major subject of a document, even the content
of a short document [30]. Therefore, the searchable index
should be constructed to prevent the cloud server from
performing such kind of association attack. While data and
index privacy guarantees are demanded by default in the
related literature, various search privacy requirements in-
volved in the query procedure are more complex and
difficult to tackle as follows.
Keyword privacy. As users usually prefer to keep their
search from being exposed to others like the cloud server,
the most
important concern is to hide what
they are
searching, i.e., the keywords indicated by the corresponding
trapdoor. Although the trapdoor can be generated in a
cryptographic way to protect the query keywords, the cloud
server could do some statistical analysis over the search
result
to make an estimate. As a kind of statistical
information, document frequency (i.e., the number of docu-
ments containing the keyword) is sufficient to identify the
keyword with high probability [31]. When the cloud server
knows some background information of the data set, this
keyword specific information may be utilized to reverse-
engineer the keyword.
Trapdoor unlinkability. The trapdoor generation function
should be a randomized one instead of being deterministic.
In particular, the cloud server should not be able to deduce
the relationship of any given trapdoors, for example, to
determine whether the two trapdoors are formed by the
same search request. Otherwise, the deterministic trapdoor
generation would give the cloud server advantage to
accumulate frequencies of different search requests regard-
ing different keyword(s), which may further violate the
aforementioned keyword privacy requirement. So the
fundamental protection for trapdoor unlinkability is to
introduce sufficient nondeterminacy into the trapdoor
generation procedure.
Access pattern. Within the ranked search,
the access
f
W is denoted as F e
pattern is the sequence of search results where every search
result is a set of documents with rank order. Specifically, the
f
W ,
search result for the query keyword set
; F e
ðF e
consisting of the id list of all documents ranked by their
W . Then the access pattern is denoted as
relevance to
; . . .Þ which are the results of sequential searches.
W 1
W 2
Although a few searchable encryption works, for example,
[19] has been proposed to utilize private information
retrieval (PIR) technique [32], to hide the access pattern,
our proposed schemes are not designed to protect the access
pattern for the efficiency concerns. This is because any PIR-
based technique must “touch” the whole data set out-
sourced on the server which is inefficient in the large-scale
cloud system.

4 PRIVACY-PRESERVING AND EFFICIENT MRSE

To efficiently achieve multi-keyword ranked search, we
propose to employ “inner product similarity” [6]
to
quantitatively evaluate the efficient similarity measure
“coordinate matching.” Specifically, Di
is a binary data
vector for document Fi where each bit Di ½j 2 f0; 1g
represents the existence of the corresponding keyword
in that document, and Q is a binary query vector
Wj
f
indicating the keywords of interest where each bit Q½j 2
f
f0; 1g represents the existence of the corresponding key-
W . The similarity score of document
word Wj in the query
W is therefore expressed as the inner product
Fi to query
of their binary column vectors, i.e., Di  Q. For the purpose
of ranking, the cloud server must be given the capability to
compare the similarity of different documents to the
query. But, to preserve strict systemwise privacy, data
vector Di , query vector Q and their inner product Di  Q
should not be exposed to the cloud server. In this section,
we first propose a basic idea for the MRSE using secure
inner product computation, which is adapted from a
secure kNN technique, and then show how to significantly
improve it to be privacy-preserving against different threat
models in the MRSE framework in a step-by-step manner.
We further discuss supporting more search semantics and
dynamic operation.

4.1 Secure Inner Product Computation
In the secure kNN scheme [27], euclidean distance between
a data record pi and a query vector q is used to select k
nearest database records. The secret key is composed of
one ðd þ 1Þ-bit vector as S and two ðd þ 1Þ  ðd þ 1Þ
invertible matrices as fM1 ; M2 g, where d is the number of
fields for each record pi . First, every data vector pi and
query vector q are extended to ðd þ 1Þ-dimension vectors as
~pi and ~q, where the ðd þ 1Þth dimension is set to  0:5kp2
i k
and 1, respectively. Besides, the query vector ~q is scaled by
a random number r > 0 as ðrq; rÞ. Then, ~pi is split into two
random vectors as f~pi
00 g, and ~q is also split into two
0 ; ~pi
random vectors as f~q 0 ; ~q 00 g. Note here that vector S
functions as a splitting indicator. Namely, if the jth bit of
0 ½j and ~pi
00 ½j are set as the same as ~pi ½j, while ~q 0 ½j
S is 0, ~pi
and ~q 00 ½j are set to two random numbers so that their sum
is equal to ~q½j; if the jth bit of S is 1, the splitting process is
similar except that ~pi and ~q are switched. The split data
00 g is encrypted as fM T
00 g, and
vector pair f~pi
0 ; ~pi
0 ; M T
1 ~pi
2 ~pi
the split query vector pair f~q 0 ; ~q 00 g is encrypted as
fM  1
2 ~q 00 g. In the query step, the product of data
1 ~q 0 ; M  1
vector pair and query vector pair, i.e.,  0:5rðkpi k2   2pi  qÞ,
is serving as the indicator of euclidean distance ðkpi k2  
2pi  q þ kqk2 Þ to select k nearest neighbors.
As the MRSE is using the inner product similarity
instead of the euclidean distance, we need to do some
modifications on the data structure to fit
the MRSE
framework. One way to do that is by eliminating the
dimension extension, the final result changes to be the
inner product as rpi  q. While the encryption of either data
record or query vector involves two multiplications of a
d  d matrix and a d-dimension vector with complexity
Oðd2 Þ, the final inner product computation involves two
multiplications of two d-dimension vectors with complex-
ity OðdÞ. In the known ciphertext model, the splitting

226

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 1,

JANUARY 2014

00 are considered as two
0 and ~pi
vector S is unknown, so ~pi
random d-dimensional vectors. To solve the linear equa-
tions created by the encryption of data vectors, we have
2dm unknowns in m data vectors and 2d2 unknowns in
fM1 ; M2 g. Since we have only 2dm equations, which are
less than the number of unknowns, there is no sufficient
information to solve either data vectors or fM1 ; M2 g.
Similarly, ~q 0 and ~q 00 are also considered as two random
d-dimensional vectors. To solve the linear equations
created by the encryption of query vectors, we have 2d
unknowns in two query vectors and 2d2 unknowns in
fM1 ; M2 g. Since we have only 2d equations here, which are
less than the number of unknowns, there is no sufficient
information to solve either query vectors or fM1 ; M2 g.
Hence, we believe that without prior knowledge of secret
key, neither data vector nor query vector, after such a series
of processes like splitting and multiplication, can be
recovered by analyzing their corresponding ciphertexts.

4.2 MRSE_I: Privacy-Preserving Scheme in Known
Ciphertext Model
The adapted secure inner product computation scheme is
not good enough for our MRSE design. The major reason
is that the only randomness involved is the scale factor r
in the trapdoor generation, which does not provide
sufficient nondeterminacy in the overall scheme as
required by the trapdoor unlinkability requirement as well
as the keyword privacy requirement. To provide a more
advanced design for the MRSE, we now provide our
MRSE_I scheme as follows.

4.2.1 MRSE_I Scheme
In our more advanced design, instead of simply removing
the extended dimension in the query vector as we plan to
do at the first glance, we preserve this dimension extending
operation but assign a new random number t to the
extended dimension in each query vector. Such a newly
added randomness is expected to increase the difficulty
for the cloud server to learn the relationship among the
received trapdoors.
In addition, as mentioned in the
keyword privacy requirement, randomness should also
be carefully calibrated in the search result to obfuscate
the document frequency and diminish the chances for
reidentification of keywords. Introducing some randomness
in the final similarity score is an effective way toward what
we expect here. More specifically, unlike the randomness
involved in the query vector, we insert a dummy keyword
into each data vector and assign a random value to it.
Each individual vector Di is extended to ðn þ 2Þ-dimension
instead of ðn þ 1Þ, where a random variable "i representing
the dummy keyword is stored in the extended dimension.
The whole scheme to achieve ranked search with multiple
keywords over encrypted data is as follows:

.

.

Setup. The data owner randomly generates a
ðn þ 2Þ-bit vector as S and two ðn þ 2Þ  ðn þ 2Þ
invertible matrices fM1 ; M2 g. The secret key SK is in
the form of a 3-tuple as fS ; M1 ; M2 g.
BuildIndex ðF ; SK Þ. The data owner generates a
binary data vector Di for every document Fi , where
each b inary b it Di ½j represen ts whe ther
the
corresponding keyword Wj appears in the docu-

.

ment Fi . Subsequently, every plaintext subindex ~Di
is generated by applying dimension extending and
splitting procedures on Di . These procedures are
similar with those in the secure kNN computation
except that the ðn þ 1Þth entry in ~Di
is set to a
random number "i , and the ðn þ 2Þth entry in ~Di is
set to 1 during the dimension extending. ~Di
is
therefore equal to ðDi ; "i ; 1Þ. Finally, the subindex
f
f
Ii ¼ fM T
00 g is built for every encrypted
0 ; M T
~Di
~Di
1
2
document Ci .
f
W Þ. With t keywords of interest in
W as
Trapdoor ð
input, one binary vector Q is generated where each
bit Q½j indicates whether Wj 2
W is true or false. Q
is first extended to n þ 1-dimension which is set to 1,
and then scaled by a random number r 6¼ 0, and
finally extended to a ðn þ 2Þ-dimension vector as ~Q
where the last dimension is set to another random
number t. ~Q is therefore equal to ðrQ; r; tÞ. After
cesses as above, the trapdoor T e
applying the same splitting and encrypting pro-
W ; k; I Þ. With the trapdoor T e
. Query ðT e
W is generated as
~Q00 g.
fM  1
~Q0 ; M  1
1
2
W , the cloud
server computes the similarity scores of each
top-k ranked id list F e
document Fi as in (1). WLOG, we assume r > 0.
After sorting all scores, the cloud server returns the
W .

	

	
With t brought into the query vector and "i brought into
Ii  T e
each data vector, the final similarity scores would be
W ¼

00
0 ; M T
~Q00
~Q0 ; M  1
M  1
~Di
~Di
M T
1
2
1
2
00  ~Q00
0  ~Q0 þ ~Di
¼ ~Di
¼ ~Di  ~Q
¼ ðDi ; "i ; 1Þ  ðrQ; r; tÞ
¼ rðDi  Q þ "i Þ þ t:

ð1Þ

Note that in the original case, the final score is simply
rDi  q, which preserves the scale relationship for two
queries on the same keywords. But such an issue is no
longer valid in our improved scheme due to the random-
ness of both t and "i , which clearly demonstrates the
effectiveness and improved security strength of our MSRE_I
mechanism.

4.2.2 Analysis
We analyze this MRSE_I scheme from three aspects of
design goals described in Section 2.
(1), the final similarity score as yi ¼ Ii  T e
Functionality and efficiency. Assume the number of query
keywords appearing in a document Fi is xi ¼ Di  Q. From
W ¼ rðxi þ "i Þ þ t
is a linear function of xi , where the coefficient r is set as a
positive random number. However, because the random
factor "i is introduced as a part of the similarity score, the
final search result on the basis of sorting similarity scores
may not be as accurate as that in original scheme. For the
consideration of search accuracy, we can let "i follow a
normal distribution N ð; 2 Þ, where the standard deviation
 functions as a flexible tradeoff parameter among
search accuracy and security. From the consideration of
effectiveness,  is expected to be smaller so as to obtain
high precision indicating the good purity of retrieved

CAO ET AL.: PRIVACY-PRESERVING MULTI-KEYWORD RANKED SEARCH OVER ENCRYPTED CLOUD DATA

227

TABLE 1
K3 Appears in Every Document

4.3 MRSE_II: Privacy-Preserving Scheme in Known
Background Model
When the cloud server has knowledge of some back-
ground information on the outsourced data set,
for
example, the correlation relationship of two given trap-
doors, certain keyword privacy may not be guaranteed
anymore by the MRSE_I scheme. This is possible in the
known background model because the cloud server can
use scale analysis as follows to deduce the keyword
specific information, for example, document frequency,
which can be further combined with background informa-
tion to identify the keyword in a query at high probability.
After presenting how the cloud server uses scale analysis
attack to break the keyword privacy, we propose a more
advanced MRSE scheme to be privacy-preserving in the
known background model.

4.3.1 Scale Analysis Attack
Given two correlated trapdoors T1 and T2
for query
keywords fK1 ; K2 g and fK1 ; K2 ; K3 g, respectively, there
will be two special cases when searching on any three
documents as listed in Tables 1 and 2. In any of these two
8>>>>>><
cases,
there exists a system of equations among final
similarity scores yi for T1 and y0
i for T2 as follows:
y1   y2 ¼ rð1 þ "1   "2 Þ;
1   y0
2 ¼ r0 ð1 þ "1   "2 Þ;
y0
>>>>>>:
y2   y3 ¼ rð1 þ "2   "3 Þ;
3 ¼ r0 ð1 þ "2   "3 Þ;
2   y0
y0
y1   y3 ¼ rð2 þ "1   "3 Þ;
1   y0
3 ¼ r0 ð2 þ "1   "3 Þ:
y0
To this end, although the exact value of xi is encrypted as yi ,
the cloud server could deduce that whether all the three
documents contain K3 or none of them contain K3 through
checking the following equivalence relationship among all
final similarity scores in two queries
¼ y1   y3
¼ y2   y3
y1   y2
1   y0
2   y0
1   y0
y0
y0
y0
2
3
3

ð3Þ

ð2Þ

:

By extending three documents to the whole data set, the
cloud server could further deduce two possible values of
document frequency of keyword K3 . In the known back-
ground model, the server can identify the keyword K3 by

TABLE 2
K3 Does Not Appear in Either Document

Fig. 2. Distribution of
final similarity score with different standard
deviations, 10k documents, 10 query keywords. (a)  ¼ 1. (b)  ¼ 0:5.

documents. To quantitatively evaluate the search accuracy,
we set a measure as precision Pk to capture the fraction of
returned top-k documents that are included in the real top-
k list. Detailed accuracy evaluation on the real-world data
set will be given in Section 5.
As for the efficiency, our inner product-based MRSE
scheme is an outstanding approach from the performance
perspective. In the steps like BuildIndex or Trapdoor, the
generation procedure of each subindex or trapdoor involves
two multiplications of a ðn þ 2Þ  ðn þ 2Þ matrix and a
ðn þ 2Þ-dimension vector. In the Query, the final similarity
score is computed through two multiplications of two
ðn þ 2Þ-dimension vectors.
Privacy. As for the data privacy, traditional symmetric key
encryption techniques could be properly utilized here and
is not within the scope of this paper. The index privacy is
well protected if the secret key SK is kept confidential since
such vector encryption method has been proved to be
secure in the known ciphertext model
[27]. Although
we add two more dimensions to the vectors compared to
the adapted secure inner product computation, the number
of equations as 2ðn þ 2Þm is still less than the number of
unknowns as the sum of 2ðn þ 2Þm unknowns in m data
vectors and 2d2 unknowns in fM1 ; M2 g. With the random-
f
ness introduced by the splitting process and the random
numbers r, and t, our basic scheme can generate two totally
W . This nondeter-
different trapdoors for the same query
ministic trapdoor generation can guarantee the trapdoor
unlinkability which is an unsolved privacy leakage problem
in related symmetric key-based searchable encryption
schemes because of the deterministic property of trapdoor
generation [10]. Moreover, with properly selected para-
meter  for the random factor "i , even the final score results
can be obfuscated very well, preventing the cloud server
from learning the relationships of given trapdoors and the
corresponding keywords. Note that although  is expected
to be small from the effectiveness point of view, the small
one will introduce small obfuscation into the final similarity
scores, which may weaken the protection of keyword
privacy and trapdoor unlinkability. As shown in Fig. 2, the
distribution of the final similarity scores with smaller  will
enable the cloud server to learn more statistical information
about the original similarity scores, and therefore  should
be set large enough from the consideration of privacy.

228

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 1,

JANUARY 2014

referring to the keyword specific document
information about the data set.

frequency

.

.

4.3.2 MRSE_II Scheme
The privacy leakage shown above is caused by the fixed
value of random variable "i in data vector Di . To eliminate
such fixed property in any specific document, more dummy
keywords instead of only one should be inserted into every
data vector Di . All the vectors are extended to ðn þ U þ 1Þ-
dimension instead of ðn þ 2Þ, where U is the number of
dummy keywords inserted.
Improved details in the
MRSE_II scheme is presented as follows:
Setup ð1n Þ. The data owner randomly generates a
ðn þ U þ 1Þ-bit vector as S and two ðn þ U þ 1Þ 
ðn þ U þ 1Þ invertible matrices fM1 ; M2 g.
BuildIndex ðF ; SK Þ. The ðn þ j þ 1Þth entry in ~Di
f
where j 2 ½1; U 
is set to a random number "ðjÞ
during the dimension extending.
W Þ. By randomly selecting V out of U
Trapdoor ð
.
. Query ðT e
P
dummy keywords, the corresponding entries in Q
are set to 1.
W ; k; I Þ. The final similarity score computed
ðvÞ
by cloud server is equal to rðxi þ
i Þ þ ti where
"
the vth dummy keyword is included in the V
P
selected ones.

4.3.3 Analysis
P
ðvÞ
P
"
Assume the probability of two
i having the same value
should be less than 1=2! , it then means there should be at
ðvÞ
least 2! different values of
"
for each data vector. The
ðvÞ
i
is not larger than ðU
V Þ, which is
"
number of different
i
V ¼ 2. B e s id e s , con s id e r ing ðU
V Þ 
m a x im i z ed wh en U
V ÞV ¼ 2V , it is greater than 2! when U ¼ 2! and V ¼ !.
ðU
So every data vector should include at least 2! dummy
entries, and every query vector will randomly select half
dummy entries. Here, ! can be considered as a system
parameter for the tradeoff between efficiency and privacy.
With properly setting the value of !, the MRSE_II scheme is
secure against scale analysis attack, and provides various
expected privacy guarantees within the known ciphertext
model or the known background model.
Moreover, every "ðjÞ
is assumed to follow the same
uniform distribution M ð0   c; 0 þ cÞ, where the mean is 0
and the variance as 0 2 is c2 =3. According to the central limit
P
theorem, the sum of ! independent random variables "ðjÞ
follows the Normal distribution, where the mean is !0 and
q
ﬃﬃﬃ
ðvÞ
the variance is !0 2 ¼ !c2 =3. To make
"
follow the
i
Normal distribution N ð; 2 Þ as above, the value of 0 is
 so that !0 ¼ 
3
set as =! and the value of c is set as
!
and !0 2 ¼ 2 . With such parameter setting, search accuracy
is statistically the same as that in MRSE_I scheme.

4.4 MRSE_I_TF
In the ranking principle “coordinate matching,” the presence
of keyword in the document or the query is shown as 1 in the
data vector or the query vector. Actually, there are more
factors which could make impact on the search usability. For
example, when one keyword appears in most documents in
the data set, the importance of this keyword in the query is
less than other keywords which appears in less documents.

Similarly, if one document contains a query keyword in
multiple locations, the user may prefer this to the other
document which contains the query keyword in only one
location. To capture these information in the search process,
we use the TF  IDF weighting rule within the vector space
model to calculate the similarity, where TF (or term
frequency) is the number of times a given term or keyword
(we will use them interchangeably hereafter) appears within
a file (to measure the importance of the term within the
particular file), and IDF (or inverse document frequency) is
obtained by dividing the number of files in the whole
collection by the number of files containing the term
(to measure the overall importance of the term within the
whole collection). Among several hundred variations of the
TF  IDF weighting scheme, no single combination of them
outperforms any of the others universally [33]. Thus, without
loss of generality, we choose an example formula that is


X
commonly used and widely seen in the literature (see [5,
chapter 4]) for the relevance score calculation
e
ð1 þ ln fi;j Þ  ln 1 þ m
:
fj
Wj 2
W

S coreðFi ; QÞ ¼ 1
jFi j

ð4Þ

Here fi;j denotes the TF of keyword Wj in file Fi ; fj denotes
the number of files that contain keyword Wj which is called
document frequency; m denotes the total number of files in
the collection; and jFi j is the euclidean length of file Fi ,
vuut
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
X
obtained by
n
ð1 þ ln fi;j Þ2

;

j¼1

functioning as the normalization factor.
To calculate the relevance score as shown in (4) on the
se rve r s ide , we p ropose a new sea rch mechan ism
MRSE_I_TF as follows which modify related data structures
in the previous scheme MRSE_I. As for the dictionary W ,
the document frequency fj is attached to every keyword
Wj , which will be used in the generation of query
vector. In BuildIndex, for every keyword Wj appearing in
the co r respond ing en t ry Di ½j
the do cumen t Fi ,
in
the data vector Di is changed from a binary value 1 to the
i.e., 1þln fi;j
the
. Similarly,
normalized term frequency,
jFi j
query vector Q changes corresponding entries from 1 to
Þ. Finally, the similarity score is as follows:
lnð1 þ m
Ii  T e
1
0
fj


X
W ¼ rðDi  Q þ "i Þ þ t
@
A
1 þ ln fi;j
 ln 1 þ m
¼ r
jFi j
fj
Wj 2Q
¼ rðScoreðFi ; QÞ þ "i Þ þ t:

þ "i

þ t

ð5Þ

Therefore, the similarity of the document and the query in
terms of the cosine of the angle between the document
T e
vector and the query vector could be evaluated by
computing the inner product of subindex Ii and trapdoor
W . Although this similarity measurement
introduces
more computation cost during the index construction
and trapdoor generation, it captures more related informa-
tion on the content of documents and query which returns
better results of users’ interest. As we will see in Section 5,
the additional cost of this measurement in BuildIndex and

CAO ET AL.: PRIVACY-PRESERVING MULTI-KEYWORD RANKED SEARCH OVER ENCRYPTED CLOUD DATA

229

Trapdoor is relatively small compared to the whole cost.
Besides, BuildIndex is a one-time computation for the
whole scheme.

4.5 MRSE_II_TF
Here, although some entries in Di have been changed from
binary value 1 to normalized term frequency, the scale
analysis attack presented in Section 4.3 still partially works
in the known background model. With similar setting in the
previous section, the first query contains two keywords as
fK1 ; K2 g while the second query contains three keywords
as fK1 ; K2 ; K3 g. Given three documents as an example, the
first keyword K1 appears in two documents as F1 and F2 ,
and the second keyword K2 appears in document F1 . Note
that there are some differences between this attack and
previous one. If the third keyword K3 appears in each of
these three documents as shown in Table 1, such equiva-
lence relationship as shown in (3) does not exist among
these documents here. Here we only consider the case that
the third keyword K3 does not appear in any of these three
documents. The final similarity scores are shown in (6).
that
Recall
the scale analysis attack presented in
Section 4.3,
it is caused by the fixed value of random
in each data vector Di which remains same
variable "i
here. From (6),
the cloud server can still deduce the
equivalence relationship as presented in (3). As a result,
the document frequency could be exposed to cloud server
and further used to identify this keyword in the known
background model. To this end, we can employ the same
solution as presented in MRSE_II
to build the new
8>>>>>>>>>>>>>>>>>>>>>>><



mechanism as MRSE_II_TF where more dummy key-
words instead of only one are inserted into data vectors



1 þ ln f1;1
 ln 1 þ m
y1 ¼ r
jF1 j




f1
þ 1 þ ln f1;2
þ t;
þ "1
 ln 1 þ m
jF1 j
f2
1 þ ln f2;1



þ t;
þ "2
 ln 1 þ m
y2 ¼ r
>>>>>>>>>>>>>>>>>>>>>>>:
jF2 j
f1
y3 ¼ r"3 þ t;



1 þ ln f1;1
1 ¼ r0
 ln 1 þ m
y0
jF1 j




f1
þ 1 þ ln f1;2
 ln 1 þ m
þ t0 ;
þ "1
jF1 j
f2
2 ¼ r0 1 þ ln f2;1
þ "2
þ t0 ;
 ln 1 þ m
y0
jF2 j
f1
3 ¼ r0 "3 þ t0 :
y0
4.6 Supporting Data Dynamics
After the data set is outsourced to the cloud server, it may
be updated in addition to being retrieved [34]. Along with
the updating operation on data documents, supporting the
score dynamics in the searchable index is thus of practical
importance. While we consider three dynamic data opera-
tions as inserting new documents, modifying existing
documents, and deleting existing documents, correspond-
ing operations on the searchable index includes generating
new index, updating existing index, and deleting existing
index. Since dynamic data operations also affect
the
document frequency of corresponding keywords, we also
need to update the dictionary W .

ð6Þ

For the operation of inserting new documents in the data
set, there may be some new keywords in new documents
which need to be inserted in the dictionary W . Remember
that every subindex in our scheme has fixed dimension as
same as the number of keywords in the old dictionary, so
the straightforward solution is to retrieve all the subindexes
from the cloud server, and then decrypt, rebuild, and
encrypt
them before outsourcing to the cloud server.
However, this approach introduces much cost on computa-
tion and communication for both sides which is impractical
in the “pay-as-you-use” cloud paradigm. To reduce such
great cost, we preserve some blank entries in the dictionary
and set corresponding entries in each data vector as 0. If the
dictionary needs to index new keywords in the case of
inserting new documents, we just replace the blank entries
in the dictionary by new keywords, and generate sub-
indexes for new documents based on the updated dic-
tionary. The other documents and their subindexes stored
on the cloud server are not affected and therefore remain
the same as before. The number of preserved entries
functions as a tradeoff parameter to balance the storage
cost and the system scalability.
When existing documents are modified, corresponding
subindexes are also retrieved from the cloud server and
then updated in terms of
the term frequency before
outsourcing. If new keywords are introduced during the
modification operation, we utilize the same method which
is proposed in the previous insertion operation. As a
special case of modification,
the operation of deleting
existing documents introduce less computation and com-
munication cost since it only requires to update the
document frequency of all the keywords contained by
these documents.

5 PERFORMANCE ANALYSIS

In this section, we demonstrate a thorough experimental
evaluation of the proposed technique on a real-world data
set: the Enron Email Data Set [35]. We randomly select
different number of e-mails to build data set. The whole
experiment system is implemented by C language on a
Linux Server with Intel Xeon Processor 2.93 GHz. The
public utility routines by Numerical Recipes are employed
to compute the inverse of matrix. The performance of our
technique is evaluated regarding the efficiency of four
proposed MRSE schemes, as well as the tradeoff between
search precision and privacy.

5.1 Precision and Privacy
As presented in Section 4, dummy keywords are inserted
into each data vector and some of them are selected in
every query. Therefore, similarity scores of documents will
be not exactly accurate. In other words, when the cloud
server returns top-k documents based on similarity scores
of data vectors to query vector, some of real top-k relevant
documents for the query may be excluded. This is because
either their original similarity scores are decreased or the
similarity scores of some documents out of the real top-k
are increased, both of which are due to the impact of
dummy keywords inserted into data vectors. To evaluate
the purity of the k documents retrieved by user, we define
a measure as precision Pk ¼ k0 =k where k0 is number of
real top-k documents that are returned by the cloud server.

230

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 1,

JANUARY 2014

Fig. 3. With different choice of standard deviation  for the random
variable ", there exists tradeoff between (a) Precision, and (b) Rank
Privacy.

Fig. 4. Time cost of building index. (a) For the different size of data set
with the same dictionary, n ¼ 4;000. (b) For the same data set with
different size of dictionary, m ¼ 1;000.

the precision in MRSE scheme is
Fig. 3a shows that
evidently affected by the standard deviation  of
the
random variable ". From the consideration of effectiveness,
standard deviation  is expected to be smaller so as to
obtain high precision indicating the good purity of
retrieved documents.
However, user’s rank privacy may have been partially
leaked to the cloud server as a consequence of small . As
described in Section 3.2, the access pattern is defined as the
sequence of ranked search results. Although search results
cannot be protected (excluding costly PIR technique), we
e
can still hide the rank order of retrieved documents as
much as possible. To evaluate this privacy guarantee, we
pi ¼ jri   r0
i j=k, where ri
first define the rank perturbation as
is the rank number of document Fi in the retrieved top-k
documents and r0
e
i is its rank number in the real ranked
P e
e
documents. The overall rank privacy measure at point k is
pi for every document
then defined as the average of all the
Pk ¼
i in the retrieved top-k documents, denoted as
pi =k.
Fig. 3b shows the rank privacy at different points with two
standard deviations  ¼ 1 and  ¼ 0:5, respectively.
From these two figures, we can see that small  leads to
higher precision of search result but lower rank privacy
guarantee, while large  results in higher rank privacy
guarantee but lower precision. In other words, our scheme
provides a balance parameter for data users to satisfy their
different requirements on precision and rank privacy.

5.2 Efficiency

5.2.1 Index Construction
To build a searchable subindex Ii for each document Fi in
the data set F , the first step is to map the keyword set
extracted from the document Fi
to a data vector Di ,
followed by encrypting every data vector. The time cost of
mapping or encrypting depends directly on the dimension-
ality of data vector which is determined by the size of the
dictionary,
i.e., the number of indexed keywords. And
the time cost of building the whole index is also related to
the number of subindex which is equal to the number of
documents in the data set. Fig. 4a shows that, given the
same dictionary where jW j ¼ 4;000,
the time cost of
building the whole index is nearly linear with the size of
data set since the time cost of building each subindex is

fixed. Fig. 4b shows that the number of keywords indexed
in the dictionary determines the time cost of building a
subindex. As presented in the Section 4.2,
the major
computation to generate a subindex in MRSE_I includes
the splitting process and two multiplications of a ðn þ 2Þ 
ðn þ 2Þ matrix and a ðn þ 2Þ-dimension vector where
n ¼ jW j, both of which have direct relationship with the
size of dictionary. The dimensionality of matrices in
is ðn þ U þ 1Þ  ðn þ U þ 1Þ so that
MRSE_II
its index
construction time with complexity Oðmðn þ U Þ2 Þ is bigger
than that in MRSE_I with complexity Oðmn2 Þ as shown in
Figs. 4a and 4b. Both the MRSE_I_TF and the MRSE_II_TF,
presented in Sections 4.4 and 4.5, respectively, introduce
more computation during the index construction since we
need to collect the term frequency information for each
keyword in every document and then perform the normal-
ization calculation. But, as shown in both figures, such
additional computation in the TF  IDF weighting rule is
insignificant considering much more computation are
caused by the splitting process and matrix multiplication.
Although the time of building index is not a negligible
overhead for the data owner, this is a one-time operation
before data outsourcing. Besides, Table 3 lists the storage
overhead of each subindex in two MRSE schemes within
different sizes of dictionary. The size of subindex is
absolutely linear with the dimensionality of data vector
which is determined by the number of keywords in the
dictionary. The sizes of subindex are very close in the two
MRSE schemes because of
trivial differences in the
dimensionality of data vector.

5.2.2 Trapdoor Generation
Fig. 5a shows that the time to generate a trapdoor is greatly
affected by the number of keywords in the dictionary. Like
index construction, every trapdoor generation incurs two
multiplications of a matrix and a split query vector, where
the dimensionality of matrix or query vector is different

TABLE 3
Size of Subindex/Trapdoor

CAO ET AL.: PRIVACY-PRESERVING MULTI-KEYWORD RANKED SEARCH OVER ENCRYPTED CLOUD DATA

231

Fig. 5. Time cost of generating trapdoor. (a) For the same query
keywords within different sizes of dictionary, t ¼ 10. (b) For different
numbers of query keywords within the same dictionary, n ¼ 4;000.

Fig. 6. Time cost of query. (a) For the same query keywords in different
sizes of data set, t ¼ 10. (b) For different numbers of query keywords in
the same data set, m ¼ 1;000.

in two proposed schemes and becomes larger with the
increasing size of dictionary. Fig. 5b demonstrates the
trapdoor generation cost
in the MRSE_II scheme with
complexity Oððn þ U Þ2 Þ is about 10 percent larger than that
in the MRSE_I scheme w ith comp lex ity Oðn2 Þ. The
MRSE_I_TF and MRSE_II_TF have similar difference where
the additional logarithm computation accounts for very
small proportion of the whole trapdoor generation. Like the
subindex generation, the difference of costs to generate
trapdoors is mainly caused by the different dimensionality
of vector and matrices in the two MRSE schemes. More
importantly, it shows that the number of query keywords
has little influence on the overhead of trapdoor generation,
which is a significant advantage over related works on
multi-keyword searchable encryption.

5.2.3 Query
Query execution in the cloud server consists of computing
and ranking similarity scores for all documents in the data
set. The computation of similarity scores for the whole data
collection is OðmnÞ in MRSE_I and MRSE_I_TF, and the
computation increases to Oðmðn þ U ÞÞ in MRSE_II and
MRSE_II_TF. Fig. 6 shows the query time is dominated by
the number of documents in the data set while the number
of keywords in the query has very slight impact on it like
the cost of trapdoor generation above. The two schemes in
the known ciphertext model as MRSE_I and MRSE_I_TF
have very similar query speed since they have the same
dimensionality which is the major factor deciding the
computation cost in the query. The query speed difference
between MRSE_I and MRSE_I_TF or between MRSE_II and
MRSE_II_TF is also caused by the dimensionality of data
vector and query vector. With respect to the communication
cost in Query, the size of the trapdoor is the same as that of
the subindex listed in the Table 3, which keeps constant
given the same dictionary, no matter how many keywords
are contained in a query. While the computation and
communication cost in the query procedure is linear with
the number of query keywords in other multiple-keyword
search schemes [16], [18], our proposed schemes introduce
nearly constant overhead while increasing the number of
query keywords. Therefore, our schemes cannot be com-
promised by timing-based side channel attacks that try to
differentiate certain queries based on their query time.

6 RELATED WORK
6.1 Single Keyword Searchable Encryption
Traditional single keyword searchable encryption schemes
[7], [8], [9], [10], [11], [12], [13], [14], [15], [25], [26] usually
build an encrypted searchable index such that its content
is hidden to the server unless it
is given appropriate
trapdoors generated via secret key(s) [4]. It is first studied
by Song et al.
in the symmetric key setting, and
[7]
improvements and advanced security definitions are given
in Goh [8], Chang et al. [9], and Curtmola et al. [10]. Our
early works [25], [26] solve secure ranked keyword search
which utilizes keyword frequency to rank results instead
of returning undifferentiated results. However, they only
supports single keyword search. In the public key setting,
Boneh et al. [11] present the first searchable encryption
construction, where anyone with public key can write to
the data stored on server but only authorized users with
private key can search. Public key solutions are usually
very computationally expensive however. Furthermore,
the keyword privacy could not be protected in the public
key setting since server could encrypt any keyword with
public key and then use the received trapdoor to evaluate
this ciphertext.

6.2 Boolean Keyword Searchable Encryption
To enrich search functionalities, conjunctive keyword
search [16], [17], [18], [19], [20] over encrypted data have
been proposed. These schemes incur large overhead caused
by their fundamental primitives, such as computation
cost by bilinear map, for example, [18], or communication
cost by secret sharing, for example, [17]. As a more general
search approach, predicate encryption schemes [21], [22],
[23] are recently proposed to support both conjunctive and
disjunctive search. Conjunctive keyword search returns
“all-or-nothing,” which means it only returns those docu-
ments in which all the keywords specified by the search
query appear; disjunctive keyword search returns undiffer-
entiated results, which means it returns every document
that contains a subset of the specific keywords, even only
one keyword of interest. In short, none of existing Boolean
keyword searchable encryption schemes support multiple
keywords ranked search over encrypted cloud data while
preserving privacy as we propose to explore in this paper.
Note that, inner product queries in predicate encryption
only predicates whether two vectors are orthogonal or not,

232

IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, VOL. 25, NO. 1,

JANUARY 2014

i.e., the inner product value is concealed except when it
equals zero. Without providing the capability to compare
concealed inner products, predicate encryption is not
qualified for performing ranked search. Furthermore, most
of these schemes are built upon the expensive evaluation of
pairing operations on elliptic curves. Such inefficiency
disadvantage also limits their practical performance when
deployed in the cloud. Our early work [1] has been aware of
this problem, and provides solutions to the multi-keyword
ranked search over encrypted data problem. In this paper,
we extend and improve more technical details as compared
to [1]. We propose two new schemes to support more search
semantics which improve the search experience of the
MRSE scheme, and also study the dynamic operation on the
data set and index which addresses some important yet
practical considerations for the MRSE design. On a different
the research on top-k retrieval
front,
[31]
in database
community is also loosely connected to our problem.
Besides, Cao et. al. proposed a privacy-preserving graph
containment query scheme [36] which solves the search
problem with graph semantics.

7 CONCLUSION

In this paper, for the first time we define and solve the
problem of multi-keyword ranked search over encrypted
cloud data, and establish a variety of privacy requirements.
Among various multi-keyword semantics, we choose the
efficient similarity measure of “coordinate matching,” i.e., as
many matches as possible,
to effectively capture the
relevance of outsourced documents to the query keywords,
and use “inner product similarity” to quantitative ly
evaluate such similarity measure. For meeting the challenge
of supporting multi-keyword semantic without privacy
breaches, we propose a basic idea of MRSE using secure
inner product computation. Then, we give two improved
MRSE schemes to achieve various stringent privacy require-
ments in two different threat models. We also investigate
some further enhancements of our ranked search mechan-
ism, including supporting more search semantics, i.e., TF 
IDF, and dynamic data operations. Thorough analysis
investigating privacy and efficiency guarantees of proposed
schemes is given, and experiments on the real-world data
set show our proposed schemes introduce low overhead on
both computation and communication.
In our future work, we will explore checking the
integrity of the rank order in the search result assuming
the cloud server is untrusted.

ACKNOWLEDGMENTS

This work was supported in part by the US National Science
Foundation under grants CNS-1116939, CNS-1217889, CNS-
1218085, and CNS-1262277, and an ECS grant CityU138513
from the Research Grants Council of Hong Kong.

REFERENCES
[1] N. Cao, C. Wang, M. Li, K. Ren, and W. Lou, “Privacy-Preserving
Multi-Keyword Ranked Search over Encrypted Cloud Data,” Proc.
IEEE INFOCOM, pp. 829-837, Apr, 2011.
[2] L.M. Vaquero, L. Rodero-Merino, J. Caceres, and M. Lindner, “A
Break in the Clouds: Towards a Cloud Definition,” ACM
SIGCOMM Comput. Commun. Rev., vol. 39, no. 1, pp. 50-55, 2009.

[4]

[6]

[14]

[3] N. Cao, S. Yu, Z. Yang, W. Lou, and Y. Hou, “LT Codes-Based
Secure and Reliable Cloud Storage Service,” Proc. IEEE INFO-
COM, pp. 693-701, 2012.
S. Kamara and K. Lauter, “Cryptographic Cloud Storage,” Proc.
14th Int’l Conf. Financial Cryptograpy and Data Security, Jan. 2010.
[5] A. Singhal, “Modern Information Retrieval: A Brief Overview,”
IEEE Data Eng. Bull., vol. 24, no. 4, pp. 35-43, Mar. 2001.
I.H. Witten, A. Moffat, and T.C. Bell, Managing Gigabytes:
Compressing and Indexing Documents and Images. Morgan Kaufmann
Publishing, May 1999.
[7] D. Song, D. Wagner, and A. Perrig, “Practical Techniques for
Searches on Encrypted Data,” Proc. IEEE Symp. Security and
Privacy, 2000.
[8] E.-J. Goh, “Secure Indexes,” Cryptology ePrint Archive, http://
eprint.iacr.org/2003/216. 2003.
[9] Y.-C. Chang and M. Mitzenmacher, “Privacy Preserving Keyword
Searches on Remote Encrypted Data,” Proc. Third Int’l Conf.
Applied Cryptography and Network Security, 2005.
[10] R. Curtmola, J.A. Garay, S. Kamara, and R. Ostrovsky, “Searchable
Symmetric Encryption: Improved Definitions and Efficient Con-
structions,” Proc. 13th ACM Conf. Computer and Comm. Security
(CCS ’06), 2006.
[11] D. Boneh, G.D. Crescenzo, R. Ostrovsky, and G. Persiano, “Public
Key Encryption with Keyword Search,” Proc. Int’l Conf. Theory and
Applications of Cryptographic Techniques (EUROCRYPT), 2004.
[12] M. Bellare, A. Boldyreva, and A. ONeill, “Deterministic and
Efficiently Searchable Encryption,” Proc. 27th Ann. Int’l Cryptology
Conf. Advances in Cryptology (CRYPTO ’07), 2007.
[13] M. Abdalla, M. Bellare, D. Catalano, E. Kiltz, T. Kohno, T. Lange, J.
Malone-Lee, G. Neven, P. Paillier, and H. Shi, “Searchable
Encryption Revisited: Consistency Properties, Relation to Anon-
ymous Ibe, and Extensions,” J. Cryptology, vol. 21, no. 3, pp. 350-
391, 2008.
J. Li, Q. Wang, C. Wang, N. Cao, K. Ren, and W. Lou, “Fuzzy
Keyword Search Over Encrypted Data in Cloud Computing,”
Proc. IEEE INFOCOM, Mar. 2010.
[15] D. Boneh, E. Kushilevitz, R. Ostrovsky, and W.E.S. III, “Public Key
Encryption That Allows PIR Queries,” Proc. 27th Ann.
Int’l
Cryptology Conf. Advances in Cryptology (CRYPTO ’07), 2007.
[16] P. Golle, J. Staddon, and B. Waters, “Secure Conjunctive Keyword
Search over Encrypted Data,” Proc. Applied Cryptography and
Network Security, pp. 31-45, 2004.
[17] L. Ballard, S. Kamara, and F. Monrose, “Achieving Efficient
Conjunctive Keyword Searches over Encrypted Data,” Proc.
Seventh Int’l Conf. Information and Comm. Security (ICICS ’05),
2005.
[18] D. Boneh and B. Waters, “Conjunctive, Subset, and Range Queries
on Encrypted Data,” Proc. Fourth Conf. Theory Cryptography (TCC),
pp. 535-554, 2007.
[19] R. Brinkman, “Searching in Encrypted Data,” PhD thesis, Univ. of
Twente, 2007.
[20] Y. Hwang and P. Lee, “Public Key Encryption with Conjunctive
Keyword Search and Its Extension to a Multi-User System,”
Pairing, vol. 4575, pp. 2-22, 2007.
J. Katz, A. Sahai, and B. Waters, “Predicate Encryption Supporting
Disjunctions, Polynomial Equations, and Inner Products,” Proc.
27th Ann.
Int’l Conf. Theory and Applications of Cryptographic
Techniques (EUROCRYPT), 2008.
[22] A. Lewko, T. Okamoto, A. Sahai, K. Takashima, and B. Waters,
“Fully Secure Functional Encryption: Attribute-Based Encryption
and (Hierarchical) Inner Product Encryption,” Proc. 29th Ann.
Int’l Conf. Theory and Applications of Cryptographic Techniques
(EUROCRYPT ’10), 2010.
[23] E. Shen, E. Shi, and B. Waters, “Predicate Privacy in Encryption
Systems,” Proc. Sixth Theory of Cryptography Conf. Theory of
Cryptography (TCC), 2009.
[24] M. Li, S. Yu, N. Cao, and W. Lou, “Authorized Private Keyword
Search over Encrypted Data in Cloud Computing,” Proc. 31st
Int’l Conf. Distributed Computing Systems (ICDCS ’10), pp. 383-
392, June 2011.
[25] C. Wang, N. Cao, J. Li, K. Ren, and W. Lou, “Secure Ranked
Keyword Search over Encrypted Cloud Data,” Proc. IEEE 30th Int’l
Conf. Distributed Computing Systems (ICDCS ’10), 2010.
[26] C. Wang, N. Cao, K. Ren, and W. Lou, “Enabling Secure and
Efficient Ranked Keyword Search over Outsourced Cloud Data,”
IEEE Trans. Parallel and Distributed Systems, vol. 23, no. 8, pp. 1467-
1479, Aug. 2012.

[21]

CAO ET AL.: PRIVACY-PRESERVING MULTI-KEYWORD RANKED SEARCH OVER ENCRYPTED CLOUD DATA

233

[27] W.K. Wong, D.W. Cheung, B. Kao, and N. Mamoulis, “Secure
kNN Computation on Encrypted Databases,” Proc. 35th ACM
SIGMOD Int’l Conf. Management of Data (SIGMOD), pp. 139-152,
2009.
[28] S. Yu, C. Wang, K. Ren, and W. Lou, “Achieving Secure, Scalable,
and Fine-Grained Data Access Control in Cloud Computing,”
Proc. IEEE INFOCOM, 2010.
[29] C. Wang, Q. Wang, K. Ren, and W. Lou, “Privacy-Preserving
Public Auditing for Data Storage Security in Cloud Computing,”
Proc. IEEE INFOCOM, 2010.
[30] S. Zerr, E. Demidova, D. Olmedilla, W. Nejdl, M. Winslett, and
S. Mitra, “Zerber:
r-Confidential
Indexing for Distributed
Documents,” Proc. 11th Int’l Conf. Extending Database Technology
(EDBT ’08), pp. 287-298, 2008.
[31] S. Zerr, D. Olmedilla, W. Nejdl, and W. Siberski, “Zerber+r: Top-k
Retrieval
from a Confidential
Index,” Proc. 12th Int’l Conf.
Extending Database Technology (EDBT ’09), pp. 439-449, 2009.
[32] Y. Ishai, E. Kushilevitz, R. Ostrovsky, and A. Sahai, “Crypto-
graphy from Anonymity,” Proc. IEEE 47th Ann. Symp. Foundations
of CS, pp. 239-248, 2006.
J. Zobel and A. Moffat, “Exploring the Similarity Space,” ACM
SIGIR Forum, vol. 32, pp. 18-34, 1998.
[34] C. Wang, Q. Wang, K. Ren, N. Cao, and W. Lou, “Toward Secure
and Dependable Storage Services in Cloud Computing,” IEEE
Trans. Services Computing, vol. 5, no. 2, pp. 220-232, Apr.-June 2012.
[35] W.W. Cohen, “Enron Email Data Set,” http://www.cs.cmu.edu/
~enron/, 2013.
[36] N. Cao, Z. Yang, C. Wang, K. Ren, and W. Lou, “Privacypreser-
ving Query over Encrypted Graph-Structured Data in Cloud
Computing ,” Proc . Distr ibuted Computing Systems (ICDCS) ,
pp. 393-402, June, 2011.

[33]

Ning Cao received the BE and ME degrees in
computer science from Xi’an Jiaotong Univer-
sity, China, and the PhD degree in electrical and
computer engineering from Worcester Polytech-
the
nic Institute. He is currently working at
Walmart Labs. He previously worked at
the
Research and System Infrastructure, Google
Inc. His research interests include the areas of
security, privacy, and reliability in Cloud Com-
puting, with current focus on search and storage.
He is a member of IEEE and a member of ACM.

Cong Wang received the BE and ME degrees
from Wuhan University, and the PhD degree
from Il linois Institute of Technology, al l
in
electrical and computer engineering. He is an
assistant professor in the Computer Science
Department at City University of Hong Kong.
He worked at the Palo Alto Research Center in
the summer of 2011. His research interests
include the areas of cloud computing security,
with current focus on secure data outsourcing
and secure computation outsourcing in public cloud. He is a member of
the ACM and a member of the IEEE.

Ming Li (S’08—M’11) received the PhD degree
in electrical and computer engineering from
Worcester Polytechnic Institute,
the ME and
BE degrees in electronic and information en-
gineering from Beihang University, China. He
joined the Computer Science Department, Utah
State University, as an assistant professor in
2011. His research interests include the general
areas of cyber security and privacy, with current
emphases on data security and privacy in cloud
computing, security in wireless networks and cyber-physical systems.
He is a member of IEEE and ACM.

Kui Ren received the PhD degree from Worce-
ster Polytechnic Institute. He is currently an
associate professor of computer science and
engineering department at SUNY Buffalo. In the
past, he has been an associate/assistant pro-
fessor in the Electrical and Computer Engineer-
i n g D e p a r t m e n t a t
I l l i n o i s I n s t i t u t e o f
Technology. His research interests inc lude
Cloud Security, Wireless Security, and Smart-
phone-enabled Crowdsourcing Systems. His
research has been supported by the US National Science Foundation
(NSF), the US Department of Energy (DoE), AFRL, and Amazon. He is a
recipient of the NSF CAREER Award in 2011. He received the Best
Paper Award from IEEE ICNP 2011. Kui serves as an associate editor
for IEEE Transactions on Information Forensics and Security, IEEE
Wireless Communications, IEEE Transactions on Smart Grid, IEEE
Internet of Th ings Journal,
IEEE Commun ications Surveys and
Tutorials, Elsevier Pervasive and Mobile Computing, and the Journal
of Communications and Networks. He is a senior member of the IEEE, a
member of the ACM, and a past board member of Internet Privacy Task
Force, State of Illinois. He is a senior member of the IEEE.

Wenjing Lou received the PhD degree in
electrical and computer engineering,
the Uni-
versity of Florida, 2003. She is currently an
assoc iate professor at V irg in ia Po lytechn ic
Institute and State University. Prior to joining
Virginia Tech in 2011, she was on the faculty of
Worcester Polytechnic Institute from 2003 to
2011. Her current research interests include
cyber security, with emphases on wireless
network security and data security and privacy
in c loud comput ing . She was a rec ip ien t o f
the US Na t iona l
Science Foundation CAREER award in 2008. She is a senior member
of the IEEE.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

