The Case for a Single-Chip Multiprocessor

Kunle Olukotun,

Basem A. Nayfeh,

Lance Hammond,

Ken Wilson,

and Kunyung

Chang

Computer Systems
University
Stanford

Laboratory

Stanford,

CA 94305-4070

http:llwww-hydra.

stanford.edu

Abstract

in IC processing
Advances
design
allow for more microprocessor
gate density and cost of wires in advanced
options. The increasing
require that we look for new ways to
technologies
integrated circuit
This paper shows that
use their capabilities
effectively.
in advanced
technologies
it
is possible to implement
a single-chip multiproces-
sor in the same area as a wide issue superscalar processor. We find
that
for applications
with little parallelism the performance
of
the
two microarchitectures
with large
For applications
is comparable.
amounts of parallelism at both the fine and coarse grained levels,
outperforms
microarchitectnre
the multiprocessor
the superscrdar
architecture
by a significant
margin.
Single-chip multiprocessor
localized
they offer
have the advantage in that
architectures
imple-
sequential
for
of a high-clock
mentation
rate processor
inherently
applications
and low latency interprocessor
communication
for par-
allel applications.

1

Introduction

technology
have fueled microproces-
Advances in integrated circuit
fifteen years. Each increase in
the last
sor performance
growth for
rates and offers new
density
integration
clock
for higher
allows
Both of
innovation.
opportunities
for microarchitecturrd
these are
performance
required to maintain microprocessor
growth. Microar-
recent microprocessors
employed
innovations
chitectural
by
include multiple
instruction
issue, dynamic
scheduling,
speculative
the trend seems to
In the future,
caches.
execution and non-blocking
instruction
be towards CPUS with wider
issue and support
for larger
In this paper, we argue against
amounts of speculative
execution.
this trend. We show that, due to fundamental
circuit
limitations
and
limited
amounts
of
instruction
level parallelism,
the superscrrlrrr
returns in performance
diminishing
execution model will
provide
this situation,
for
a
increasing
issue width.
Faced with
building
complex wide issue superscalar CPU is not
the most efficient use of
silicon
resources. We present
the case that a better use of silicon
area is a multiprocessor microarchitecture
constructed from simpler
processors.

Permission to make digitalhard
copy of part or all of this work for personal
or classroom use is granted without
fee provided that copies are not made
or distributed for profit or commercial advantage,
the
the mpyright notice,
title of the publication and its date appear, and notice is given that
COpyin(l is by permission of ACM,
to
Inc. To copy otherwise,
to republish,
post on servers, or to redistribute to lists, requires prior specific permission
andlor a fee.

ASPLOS Vll 10/96 MA, USA
Q 1996 ACM 0-89791 -767-719610010...$3.50

2

between wide-issue
To understand the performance
pro-
trade-offs
way, we com-
in a more quantitative
cessors and multiprocessors
scheduled
a six-issue
performance
pare
the
of
dynamically
superscalar
processor with
a 4 x two-issue multiprocessor.
Our
has a number of unique features. First, we accurately
comparison
the cache hit
account
for and justify
the latencies, especially
time,
associated with the two microarchitectures.
Second, we develop
floor-plans
and carefully
allocate resources to the two microarchi-
tectures so that
they require an equal amount of die area. Third, we
floating
evaluate these architectures with a variety of
point
integer,
and multiprogramming
applications
running in a realistic
operating
system environment.

The results show that on applications
that cannot be parallelized,
the superscalar microarchitecture
performs
than one
3070 better
the multiprocessor
processor of
architecture. On applications
with
parallelism the multiprocessor microarchi-
fine grained thread-level
tecture can exploit
this parallelism so that
the superscalar microar-
chitecture is at most 10% better. On applications with large grained
the mul-
parallelism and multiprogramming
thread-level
workloads
tiprocessor microarchitecture
performs
50–1 00% better
than the
wide superscalar micro architecture.

In Section 2,
is organized
as follows.
The remainder
this paper
of
of superscalar
limits
design from a
we discuss the performance
In Section 3, we make
perspective.
technology
and implementation
the case for a single chip multiprocessor
from an applications
per-
In Section 4, we develop floor plans for a six-issue super-
spective.
scalar microarchitecture
and a 4 x two-issue multiprocessor
and
examine their area requirements. We describe the simulation meth-
in Section 5,
odology used to compare these two microarchitectures
and in Section 6 we present
the results of our performance
compar-
ison. Finally, we conclude in Section 7.

2

The Limits

of the Superscalar

Approach

A recent
trend in the microprocessor
industry
has been the design
to execute
issue and the ability
of CPUS with multiple
instruction
dynamic
out of program order. This
instructions
ability,
called
first appeared in the CDC 6600 [21]. Dynamic
scheduling,
schedul-
ing uses hardware to track register dependencies
between instruc-
out of program order, as
is executed, possibly
tions; an instruction
In the CDC 6600 the
its dependencies
soon as all of
are satisfied.
structure
checking was done with a hardware
register dependency
called the scoreboard. The IBM 360/9 1 used register
renaming
to
improve the efficiency
of dynamic
scheduling
using hardware struc-

Instruction
Fetch

Issue snd
Retirement

and

4-

+

R::e~r

+

Instruction
Issue
Queues

PInstruction
@il
Fetch &
Decode
AInstruction
c1Data
Cache
Cache
1. A dynamic
Figure
CPU
reservation
tures called
to design a
is possible
It
[3].
stations
using reserva-
dynamically
superscalar microprocessor
scheduled
tion stations; Johnson gives a thorough description
this approach
of
of dynamic
[13]. However,
super-
implementations
recent
the most
to the one shown in
scalar processors have used a structure similar
and physical
renaming between architectural
Figure 1. Here register
registers is done explicitly,
and instruction
scheduling
and register
dependency
tracking
between
instructions
are performed
in an
of microprocessors
instruction
designed in
issue queue. Examples
R1OOOO [24] and the HP
this manner are the MIPS Technologies
queue is actually
In these processors the instruction
PA-8000
[14].
implemented
as multiple
instruction
queues for different
classes of
load/store). The three major
floating point,
integer,
instructions
(e.g.
superscalar machine
in a dynamic
phases of
instruction
execution
In the
are also shown in Figure 1. They are fetch,
issue and execute.
thk section we describe these phases and the limiti~tions
rest of
that
will arise in the design of a very wide instruction
issue CPU.

superscalar

the CPU with a
The goal of
the rest of
the fetch phase is to present
Three factors
large and accurate window of decoded instructions.
instruction mis-
constrain instruction
fetch: mispredicted
branches,
to predict branches cor-
and cache misses. The ability
alignment,
to establishing
is crucial
rectly
a large,
accurate window
of
by using a moderate amount of memory
instructions.
Fortunately,
such as the selective branch predictor
branch predictors
(64Kbit),
proposed by McFarling
are able to reduce misprediction
rates to
good branch predic-
[15]. However,
under 590 for most programs
is also necessary to
tion is not enough. As Conte pointed
it
out,
[7]. When the issue
the decoder
instructions
align a packet of
for
width is wider
than four
instructions
there is a high probability
that
it will be necessary to fetch across a branch for a single packet of
one in every five instruc-
in integer programs,
instructions
since,
from two cache
tions is a branch [12]. This will
require fetching
to form a single
lines at once and merging the cache lines together
packet of
instructions.
Conte describes a number of methods
for

3

achieving
this. A technique
that divides
the instruction
cache into
too expensive
banks at once is not
banks and fetches from multiple
to implement
and provides performance
that
is within
3% of a per-
fect scheme on an 8-wide issue machine. Even with good branch
prediction
and alignment
a significant
cache miss rate will
limit
the
an adequate window of
ability
instruc-
to maintain
the fetcher
of
such as large logic simula-
tions. There are still some applications
that
processing
transactions
tions,
and
the OS kernel
have
cache miss rates even with fairly
significant
instruction
large 64 KEt
two way set-associative
caches [19]. Fortunately,
it
is possible to
in a dynamically
cache miss latency
hide some of
the instruction
scheduled processor by executing
in
instructions
that are already
window. Rosenblum et. al. have shown that over
the instruction
60% of
the instruction
cache miss latency can be hidden on a data-
instruction
base benchmark with a 64KB two way set associative
alignment
and instruction
cache [19]. Given good branch prediction
the fetch phase of a wide-issue
it
is likely
that
dynamic
superscalar
processor will not
limit performance.

In the issue phase, a packet of renamed instructions
is inserted into
the instruction
is issued for execution
issue queue. An instruction
once all of its operands are ready. There are two ways to implement
renaming. One could use an explicit
table for mapping architectural
registers to physical
registers,
this scheme is used in the R1 0000
buffer/instruction
or one could use a combination
[24],
reorder
the mapping
[14]. The advantage of
queue as in the PA-8000
table
register
are required for
is that no comparisons
renaming. The dis-
advantage of
the mapping
table is that
the number of access ports

required by the mapping table structure is O x W, where O is the

and W is the issue width of the
number of operands per instruction
three operands per
issue machine with
machine. An eight-wide
instruction
requires a 24 port mapping table.
Implementing
renami-
It requires
ng with a reorder buffer has its own set of drawbacks,

n x Q x O x W l-bit

comparators

to determine which physical

reg-

isters should supply

operands

for a new packet of

instructions,

where rr is the number of bits required to encode a register

identi-

fier and Q is the size of

the instruction

issue queue. Clearly,

the

the instruction
number of comparators grows with the size of
queue
queue, all
is in the instruction
and issue width. Once an instruction
instructions
that
issue must
update
their
dependencies.
This

requires another set of n x Q x O x W comparators. For example, a
three operand instructions,
machine with eight wide issue,
a 64-
requires 9,216 l-bit
queue, and 6-bit comparisons
entry instruction
comparators. The net effect of all
the comparison
logic and encod-
ing associated with the instruction
issue queue is that
it takes a large
is a four-
amount of area to implement. On the PA-8000, which
the instruc-
issue queue entries,
issue machine with 56 instruction
the die area. In addition,
tion issue queue takes up 20% of
as issue
widths increase,
larger windows
of
instructions
are required to find
that can issue in parallel
instmctions
independent
the
and maintain
increase in the size of
is a quadratic
issue bandwidth.
full
The result
the instruc-
the instruction
issue queue. Moving
to the circuit
level,
tion issue queue uses a broadcast mechanism to communicate
the
tags of
the instructions
that are issued, which
requires wires that
In future advanced integrated
span the length of
cir-
the structure.
long delays rel-
these wires will have increasingly
technologies
cuit
them [9]. Given
drive
to the gates
ative
that
this
situation,
ultimately,
the instruction
issue queue will
limit
the cycle time of
issue
the instruction
the processor. For these reasons we believe that

fundamentally
queue will
superscalar machines.

limit

the performance

of wide

issue

In the execution phase, operand values are fetched from the register
file or bypassed from earlier
instructions
to execute on the func-
tional units. The wide superscalar execution model will encounter
file,
in the register
performance
in the bypass logic and in the
limits
issue requires a larger window of
units. Wider
functional
instruction
which implies more register
instructions,
renaming. Not only must
the register
file be larger
to accommodate more renamed registers,
the number of ports required to satisfy the full
but
issue
instruction
this causes a qua-
also grows with issue width. Again,
bandwidth
the register
dratic increase in the complexity
of
tile with increases
in issue width. Farkas et. al. have investigated
the effect of register
file complexity
on performance
[10]. They find that an eight-issue
than a four-issue machine when
machine only performs 20’%0 better
estimates,
the effect of cycle-time
is included
in the performance
with
the bypass logic also grows quadratically
The complexity
of
number of execution
units; however, a more limiting
factor
is the
units. As far as
the execution
the wires that
delay of
interconnect
units themselves are concerned,
the execution
the arithmetic
func-
tional units can be duplicated
to support
the issue widtb, but more
ports must be added to the primary data cache to provide the neces-
sary load/store
bandwidth.
The cheapest way to add ports to the
a banked cache [20], but
data cache is by building
the added muM-
required to implement
plexing and control
a banked cache increases
the access time of
the cache. We investigate this issue in more detail
in Section 4.2.

3

The Case for a Single-Chip Multiprocessor

a single chip multiprocessor
for building
The motivation
comes
push and an application
there is a technology
from two sources;
issues, especially
pull. We have already argued that
the
technology
files, will
issue queue and multi-port
delay of
the complex
register
returns
limit
the performance
from a wide superscalar
execution
model. This motivates
the need for a decentralized microarchitec-
ture to maintain
the performance
growth of microprocessors.
From
the microarchitecture
the applications
that works best
perspective,
depends on the amount and characteristics
of
the parallelism in the
applications.

studies of
has performed
Wall
comprehensive
the most
one of
parallelism [22]. The results of his study indicate
application
that
in two classes. The first class consists of applica-
fall
applications
under
low to moderate
tions with
amounts
of parallelism;
ten
instructions
per cycle with aggressive branch prediction
and large,
are integer
infinite window sizes. Most of these applications
but not
with large
The second class consists of applications
applications.
per cycle
than forty
greater
amounts of parallelism,
instructions
sizes. The
with
aggressive
branch
prediction
and large window
majority
of
these applications
are floating
point applications
and
parallelism.
most of the parallelism is in the form of loop-level

arises
a single-chip multiprocessor
The application
towards
pull
execu-
because these two classes of applications
require different
in the first class work best on processors
tion models. Applications
that are moderately
superscalar
(2 issue) with very high clock rates
because there is little parallelism to exploit, To make this more con-
crete we note that a 200 MHz MIPS R5000, which is a single issue
machine when running
integer programs,
achieves a SPEC95 inte-

the rating of a 200 MHz MIPS R1OOOO,
ger rating which is 70% of
have the same
[6], Both machines
which is a four-issue machine
the R5000 has a blocking
size data and instruction
caches, but
data
cache, while the R1OOOO has a non-blocking
data cache. Applica-
tions in the second class have large amounts of parallelism and see
performance
benefits from a variety of methods designed to exploit
parallelism such as superscalrrr, VLIW or vector processing. How-
compilers make a multiproces-
ever, the recent advances in parallel
the parallelism in these
and flexible way to exploit
sor an efficient
programs
[1]. Single-chip
multiprocessors,
designed
so that
the
processors are simple and achieve very high clock rates,
individual
will work well on integer programs
in the first class. The addition of
between processors on the same chip
low latency
communication
also allows the multiprocessor
to exploit
the parallelism of the float-
ing point programs
in the second class.
In Section 6 we evaluate
the performance
these two
for
multiprocessor
of a single-chip
application
classes.

the
Today,
There are a number of ways to use a multiprocessor.
to
processes in parallel
use is to execute multiple
most common
environment
in a multiprogramming
increase throughput
the
under
system. We note that
aware operating
control of a multiprocessor
operating
there are a number of commercially
available
systems
that have this capability
(e.g. Silicon Graphics
IRIX, Sun Solaris,
the increasingly
Microsoft Windows NT). Furthermore,
widespread
applications
use of visualization
and multimedia
tends to increase
tbe number of active processes or independent
threads on a desktop
machine or server at a particular
point
in time.

is to execute multiple
Another way to use a multiprocessor
threads
examples are
llvo
that come from a single application.
in parallel
scien-
and hand parallelized
processing
transaction
floating
point
tific applications
[23].
In this case the threads communicate
using
shared memory, and these applications
are designed to run on paral-
latencies in the hundreds of CPU
lel machines with communication
in a very
the threads do not communicate
clock cycles;
therefore,
parallelized
of manually
example
tine grained manner. Another
applications
are fine-grained
thread-level
integer
applications.
exhibit mod-
Using the results from Wall’s
these applications
study,
erate amounts of parallelism when the instruction
window size is
very large and the branch prediction
is perfect because the parallelism that exists is widely
distributed. Due to the large window size
and the perfect branch prediction
this
for
be very difficult
it will
parallelism could be extracted with a superscalar execution model.
that understands
it
However,
is possible
for a programmer
the
the parallelism in the application
nature of
to parallelize
the application into multiple
threads. The parallelism exposed in this manner
by a conventional multiprocessor is fine-grained
and cannot be exploited architecture. The only way to exploit
this type of parallelism
is with a single-chip multiprocessor
architecture.

A third way to use a multiprocessor
is to accelerate the execution of
this requires
intervention;
without manual
applications
sequential
this automatic
technology. Recently,
parallelization
automatic
par-
was shown to be effective
technology
allelization
on scientific
applications
[2], but
it
is not yet ready for general purpose integer
applications,
integer
parallelized
Like the manually
applications.
benefits
performance
significant
could
these applications
derive
from the low-latency
interprocessor
communication
provided
by a
single-chip multiprocessor.

4

#of CPUS

Degret srrpcrscalm

#of architectural registers

#of physical registers

#of

integer functional units

#of

floating pt. functional units

#of

loarfk.tore ports

BTB size

Retarn stack size

Irrstraction issue queue size

I cache

D cache

LI hit time

LI cache interleaving

Unified L2 cache

L2 hit time/ L1 penatty

Memory latency / L2 penalty

6.way SS

1

6

32int 132fp

lrihrt

/ 160fp

3

3

8 (one per bank)

2048 entries

32 entries

128 entries

32 KB, 2-way S. A.

32 ICE, 2-way S. A.

2 cycles (4 ns)

8 banka

256 KB, 2-way S. A.

4 cycles (8 ns)

50 cycles (100 ns)

4x2.way MP

4

4x2

4 x 32int 132fp

4x40hrt/40fp

4X1

4X1

4X1

4x512 entries

4 x 8 entries

4 x 8 entries

4 x 8 KB, 2-way S. A.

4 x 8 KB, 2-way S. A.

1 cycle (2 rrs)

NIA

256 KB, 2-way S. A.

5 cycles (10 ns)

50 cycles (100 ns)

Table 1. Key characteristics

of the two microarchitectures

4

Two Microarchitectures

design
superscalar
To compare
and multiprocessor
the wide
for
two
the microarchitectures
approaches, we have developed
the art in processor design
machines that will
the state of
represent
a few years from now. The superscalar microarchitecture
(SS) is a
the current R1OOOO superscalar design, wid-
logical
extension
of
issue implemen-
issue to a six-way
four-way
ened from the current
tation. The multiprocessor
microarchitecture
(MP),
is a four-way
identical
composed of four
single-chip multiprocessor
2-way super-
identical
to fit
scalar processors.
In order
four
processors cm a die of
the same size, each individual
processor
is comparable to the Alpha
21064, which became available in 1992 [8].

different microarchitectures
These two extremely
have nearly iden-
tical die sizes when built
process technologies. The pro-
in identical
is based upon the kinds of processor chips that
cessor size we select
advances in silicon processing technology will allow in the next
few
in a 0.25 Lm process, which should be
years. When manufactured
possible by the end of 1997, each of
the chips will have im area of

than leading-edge microprocessors
about 30% larger
430 mm2 —
represents
being shipped today.
llig
typical
die size growth over
the course of a few years among the largest,
fastest microprocessors
[11].

the simpler
We have argued that
two-issue CPU used in (the multi-
will have a higher clock rate than the
processor microarchitecture
six issue CPU; however,
this comparison we
the purposes of
for
have assumed that
the two processors have the same clock rate. To
achieve the same clock rate the wide superscalar architecture would
require deeper pipelining
due to the large amount of
instruction
issue logic
in the critical
path. For simplicity,
we ignore latency
due to the degree of pipelining.
variations between the architectures
We assume the clock frequency
of both machines
is 500 MHz. At
500 MHz the main memory
latencies experienced by the processor
are large. We have modeled the main memory as a 50-cycle, 100 ns

delay for both architectures,
values in a workstation
typical
with 60 ns DRAMs
and 40 ns of delays due to buffering
DRAM controller
chips [25].

today
in the

the two architectures, We
Table 1 shows the key characteristics
in the following
these characteristics
explain and justify
sections.
The integer
functional
and floating
point
unit
result and repeat
Iatencies are the same as the RIOOOO [24]

of

4.1

6-Way

Superscalar

Architecture

is a logicrd extension of the cur-
The 6-way superscalar architecture
2 and the area break-
in F@re
rent R1OOOOdesign. As the floorplarr
the logic
down
in Table 2 indicate,
necessary
for out-of-order
the area of the chip, due
instruction
dominates
issue and scheduling
6-way instruction
to the quadratic area impact of supporting
issue.
buffers by
First, we increased the number of ports in the instruction
50% to support 6-way issue instead of 4-way,
increasing
the area of
each buffer by about 30-40%. Second, we increased the number of
from 48 to 128 entries
instruction
the processor
so that
buffers
for ILP to keep the execu-
examines a larger window of instructions
tion units busy. This large instruction window also compensates
for
the fact
that
the simulations
do not execute code that
is optimized
window
instruction
superscalar machine. The larger
for a 6-way
area increase of
issue width causes a quadratic
size and wider
the
size. Alto-
its original
instruction
sequencing
logic
to 3-4 times
gether,
the logic necessary to handle out-of-order
instruction
issue

occupies about 120 mm2 —

about 30% of

the die.

In comparison,

the actual execution units only occupy about 70 mm2 —
of
the die is required
to build triple R1OOOO execution
0.25 ~m process.

just 18%
units in a

are issued, we also
Due to the increased rate at which instructions
the size of
enhanced the fetch logic by increasing
the branch target
stack to 32 entries. This
to 2048 entries and the call-return
buffer
increases the branch prediction
accuracy of
the processor and pre-

5

Instruction
Cache
(32 KB)

TLB

2

z
x
UI

g
2
v
G

Data
Cache
(32 KB)

Instruction
::ti:::
Fetch
Inst. Decode &
Rename
hReorder Buffer,
Instruction Queues,
and Out-of-Order
Logic
Floating Point
Unit
for
2. Floorplan
the six-issue dynamic
microprocessor.
a bottleneck
fetch mechanism from becoming
vents the instruction
since the 6-way execution
engine requires a much higher
instruc-
tion fetch bandwidth
than the 2-way processors used in the MP
architecture.

.~
3
&
m
al
g

Figure

superscalar

hierarchy
The on-chip memory
a
to the Alpha 21164 —
is similar
small,
cache backed up by a large on-chip level
level one (Ll)
fast
two (L2) cache. The wide issue width requires the L1 cache to sup-
port wide instruction
fetches from the instruction
cache and multi-
ple loads from the data cache during each cycle. The two-way
set
32 KB L1 data cache is banked eight ways into eight
associative
4 KB cache banks each of which
small, single-ported,
independent
handling
one access every 2 ns processor cycle. However,
the addi-
logic and crossbar
the bank control
tional overhead of
required to
sharing the 8 data cache
the multiple
arbitrate
between
requests
cycle to the latency
banks adds another
of
the L1 cache, and
increases the area by 25%. Therefore,
our modeled L1 cache has a
hit
time of 2 cycles. Backing up the 32 KB L1 caches is a large, uni-
takes 4 cycles to access. These latencies
fied, 256 KB L2 cache that
the times obtained
are simple extensions
of
for
the L1 caches of
current Alpha microprocessors
[4], using a 0.25 ~m process tech-
nology

4

21 mm

F

>-cache

#1 (8K)

l-Cache #2 (8K)

Processor
#1

Processor
#2

2

m

D-Cache #1 (8K) D-Cache #2 (8K)
D-Cache #3 (8K) D-Cache #4 (8K)

Processor
#3

Processor
#4

External
Intetiace

a
n
Ur(n
$

s
o.-
G
0
.-
S
z
E
G
N
-1

6?
x
w
g

2
0

G
5
Q
z
y

6

]-Cache #3 (8X)

l-Cache #4 (8K)

Figure

3. FloorPlan
the four-way
for
multiprocessor.

single-chip

the 6-way SS processor, as
sors is less than one-fourth
the size of
shown in Table 3. The number of execution units actually
increases
in the MP because the 6-way processor had three units of each type,
while the 4-way MP must have four
one for each CPU. On the
smaller, due to the
the issue logic becomes dramatically
other hand,
number
buffer
decrease in instruction
ports and the smaller
of
buffer. The scaling factors of
entries in each instruction
these two
units balance each other out,
leaving the entire processor very close
to one-fourth
of the size of
the 6-way processor.

the multiprocessor
The on-chip cache hierarchy
is significantly
of
the 6-way superscalar proces-
from the cache hierarchy of
different
sor. Each of
the 4 processors has its own single-banked
and single-
ported 8 KB instruction
and data caches that can both be accessed
in a single 2 ns cycle. Since each cache can only be accessed by a
single processor with a single load/store
unit, no additional
over-
head is incurred to handle arbitration
among independent memory-
access units. However,
since the four processors now share a single
latency during every
that cache requires an extra cycle of
L2 cache,
and crossbar
interprocessor
access to allow time for
arbitration
L2 delay by penalizing
delay. We model
this additional
the MP an
additional
cycle on every L2 cache access, resulting
in a 5 cycle L2
time.
hit

4.2

4 x 2-way
Architecture

Superscalar

Multiprocessor

5

Simulation Methodology

four 2-way superscalar proces-
is made up of
The MP architecture
by a crossbar
sors interconnected
that allows the processors to share
the four processors are arranged in a grid
the L2 cache. On the die,
with the L2 cache at one end, as shown in Figure 3. Internally,
each
of
the processors has a register
renaming
buffer
that
is much more
limited
than the one in the 6-way architecture,
since each CPU only
buffer. We also quartered the size of
has an 8-entry instruction
the
in the fetch units,
prediction mechanisms
branch
to 512 BTB
the area adjustments
entries and 8 call-return
stack entries. After
caused by these factors are accounted for, each of
the four proces-

the two microarchitec-
of
the performance
evahrating
Accurately
in which we
tures requires a way of simulating
the environment
to be used in real systems,
would expect
these architectures
In this
6ection we describe the simulation
environment
and the applica-
tions used in this study.

5.1 Simulation

Environment

in the SimOS simulation
We execute the applications
environment
and I/O devices
[18]. SimOS models the CPUS, memory hierarchy

6

—
CPU Component

256K Orr-Cfdp L2 Cache a

8-bank D Cache (32 KB)

8-bank I Cache (32 KB)

TLB Mechanism

External Interface Unit

Instruction Fetch Unit and BTB

Irrshuction Decode Section

Instruction Queues

Reorder Buffer

Integer Functional Units

FP Functional Units

Clncking & Overhead

TotaJ Size

0.35Pm R1OK
Original Size (mm*)

Size Extrapolated
to 0.25prn (mmz)

% Growth Due to
New Functionality

New Size (mmz)

% Area

219

26

28

10

27

18

21

28

17

20

24

73

112

13

14

5

14

9

11

14

9

10

12

37

o%

25%

25%

200%

0%

200%

250%

250%

300%

200%

200%

o%

.

112

17

18

15

14

28

38

50

34

31

37

37

430

26%

4%

4%

3%

3%

6%

9%

12%

9%

7%

9%

9%

100%

Table 2. Size extrapolations

for

the 6-way superscalar

from the MIPS R1OOOOprocessor

CPU Component

D Cache (8 KB)

I Cache (8 KB)

TLB Mechanism

Instruction Fetch Unit nnd BTB

Instruction Decode Section

Inso’uction Queues

Reorder Buffer

IrWger Functional Units

FP Functional Units

Per-CPU Subtotal

256K On-Chip L2 Cache’

External Interface Unit

Crossbnr Between CPUS

Clocking & Overhead

Total Size

0.351un R1OK
Original Size (mmz)

Size Extrapolated
to 0.25vrrr (mmz)

% Growth Due to
New Functionality

26

28

10

1s

21

28

17

20

24

219

27

73

13

14

5

9

11

14

9

10

12

112

14

37

-75%

-75%

o%

-25%

-50%

-70%

-80%

o%

0%

o%

o%

o%

.

New Size (mmz)
~

4

5

7

5

4

2

10

12

53

112

14

50

37

424

% Area
(of CPU /of entire
chip)

6% /3%

7%13%

9% I 5%

13%/7%

10% /5%

8% 14%

3%12%

20% / 10%

23%/12%

100% / 50%

,

26%

3%

12%

9%

100%

Table 3. Size extrapolations

in the 4 x 2-way MP from the MIPS R1OOOOprocessor.

a. estimated from current L] caches

of uniprocessor
and multiprocessor
systems in sufficient
detail
to
system. SimOS uses the
boot and run a commercial
operating
lRIX
instruction
MIPS-2
set and runs the Silicon Graphics
5.3
perfor-
system which has been tuned for multiprocessor
operating
mance. SimOS actually
simulates
the operating
system;
therefore,
systemi and the
the memory
all
references made by the operating
are generated. This feature is particularly
applications
important
for
workloads where the time spent
the study of multiprogramming
executing
kernel code makes up a significant
fraction
of
the non-
idle execution time.

A unique feature of SimOS that makes studies such as this, feasible
that use a common
is that SimOS supports multiple CPU simulators
set architecture.
instruction
This
allows
trade-offs
to be made
between the simulation
speed and accuracy. The fastest CPU simu-

lator,
called Embra,
uses binary-to-binary
translation
techniques
system and positioning
and is used for booting
the
the operating
regions of execution.
so that we can focus on interesting
workload
is two
called Mipsy,
CPU simulator,
The medium performance
orders of magnitude
slower
than Embra. Mipsy
is an instruction
set
instructions with a one cycle result
that models all
simulator
latency
interprets all user and privileged
rate. Mipsy
and a one cycle repeat
references to a memory
instructions
and feeds memory
system sim-
ulator. The slowest, most detailed CPU simulator
is MXS, which
supports dynamic
scheduling,
speculative execution and non-block-
ing memory
four orders of magnitude
references. MXS is over
slower
than Embra.

system component
The cache and memory
is com-
of our simulator
to the SimOS processor model
and interfaces
pletely event-driven

7

—
—
—
—
—
—
—
—
—
—
compress

eqntott
I m88ksim
MPsim

applu

apsi

swim

tomcatv

1

I

,

Integer

applications

compresses and uncompressed file in memory

translates logic equations into truth tables

Motorola

88000 CPU simulator

VCS compiled Verilog simulation

of a multiprocessor

Floating

point applications

solver

for parabolic/elliptic

partial differential

equations

solves problems of temperature, wind, velocity, and distribution

of pollutants

shallow water model with 1K x 1K grid

mesh-generation

with Thompson

solver

Multiprogramming

application

pmake

I parallel make of gnuchess using c compiler

Table 4. The applications.

I

which drives it, Processor memory
references
cause threads to be
the state of each memory
generated which keep track of
reference
system. A call-back mecha-
and the resource usage in the memory
the status of all outstanding
nism is used to inform the processor of
references,
and to inform the processor when a reference
com-
allow for very detailed cache and mem-
pletes. These mechanisms
cycle accurate measures of
system models, which
ory
include
contention
and resource usage throughout
the system.

5.2

Applications

is used to evaluate
of nine realistic
The performance
applications
the nine applications
Table 4 shows that
the two microarchitectures.
benchmarks
are made up of
two SPEC95
integer
(compress,
m88ksim),
one SPEC92
integer
benchmark
(eqntott),
one other
point bench-
four SPEC95
(MPsim),
integer
floating
application
marks (applu, apsi, swim,
tomcatv),
and a multiprogramming
appli-
cation (pmake).

in different ways to run on the MP
are parallelized
The applications
on both the SS and
Compress
rnicroarchitecture.
is run unmodified
the MP archi-
using only one processor of
MP microarchitectures;
tecture. Eqntott
is parallelized manually
by modifying
a single bit
vector comparison
routine that
is responsible
for 9070 of
the execu-
[16]. The CPU simulator m88ksim is
tion time of
the application
into three threads using the SUIF com-
rdso parallelized manually
the three threads is allowed to be in a
runtime system. Each of
piler
different
phase of simulating
a different
instruction
at
the same
to the overlap of
is very similar
time. This style of parallelization
execution
instruction
that
occurs
in hardware
pipelining.
The
is a Verilog model of a bus based multiprocessor
MPsim application
(Chrono-
compiled
running under a multi-threaded
code simulator
logic VCS-MT).
The multiple
threads are specified manually
by
hierarchy
the model
assigning
threads. The
to different
parts of
coupled threads; one for each
MPsim application
uses four closely
of the processors in the model. The parallel versions of
the SPEC95
generated by the SUIF
floating point benchmarks
are automatically
compiler
system [2]. The pmake application
is a program develop-
the Modified
that consists of
ment workload
phase of
the compile
is executed
[17]. The same pmake application
Andrew Benchmark
on both microarchitectures;
however,
tbe OS takes advantage of
the

extra processors in the MP microarchitecture
lations in parallel.

to run multiple

compi-

problem that arises when comparing
of
the performance
A difficult
they do the same amount of
processors is ensuring
that
different
work. The solution is not as easy as comparing
the execution times
on each machine. Due to the slow simulation
of each application
used to collect
speed of
the detailed CPU simulator
(MXS)
these
results it would take far too long to run the applications
to comple-
tion. Our solution
is to compare the two microarchitectures
over a
called representative
using a technique
the application
portion
of
execution windows
[5].
In most
compute
intensive
applications
there is a steady state execution region that consists of a single outer
loop or a set of
loops that makes up the bulk of
the execution time.
to sample a small number of
It
these loops
of
iterations
is sufficient
the execution time behavior
as a representative
execution window if
of the entire program. Simu-
of the window is indeed representative
lation results show that
for most applications
the cache miss rates
and the number of
instructions
executed in the window deviates by
the entire program.
less than 1% from the results for

taken with the
procedure begins with a checkpoint
The simulation
starts with the
Embra simulator.
Simulation
from the checkpoint
and the full memory
instruction
level
simulator Mipsy
system.
simulator
the caches are warmed
After
the Mipsy
by running
the sim-
execution window at least once,
through the representative
ulator
is switched to the detailed simulator, MXS,
to collect
the per-
formance results presented in this paper.

We use the technique
for all
execution windows
representative
of
except pmake. Pmake does not have a well defined
the applications
execution region that
is representative
of the application
as a whole.
Therefore,
the results for pmake are collected by running the entire
application with MXS.

6

Performance

Comparison

We begin
by examining
the performance
of
the
superscalar
the multiprocessor microar-
and one processor of
microarchitecture
the IPC, branch
chitecture.
Table 5 shows
prediction
rates and
the MP, Table 6 shows the
cache miss rates for one processor of
IPC, branch
prediction
rates, and cache miss
rates for
the SS

8

microarchitecture.
The cache miss rates are presented in the tables
in terms of misses per completed
instruction
(MPCI);
including
that complete in kernel and user mode. When the issue
instructions
width
is increased
from two to six we see that
the actual
IPC
increases by less than a factor of 1.6 for all of the integer and multi-
programming
applications.
For
the floating
point applications
the
varies from a factor of 1.6 for torncatv to
improvement
performance

2.4 for swim.,

Program

compress

eqntott

m88kaim

MPsim

applu

apsi

swim

tomcatv

pmakc

IPC

0.9

1.3

1.4

0.8

0.9

0.6

0.9

0.8

1.0

BP Rate
%

I cache
%MPC1

D cache
%MPCI

G! cache
%MPCI

85.9

79.8

91.7

78.7

79.2

95.1

99.7

99.6

86.2

0.0

0.0

2.2

5.1

0.0

1.0

0.0

0,0

2,3

3.5

0.8

0.4

2,3

2.0

4.1

1.2

7.7

2.1

1.0

0.7

0.0

2.3

1.7

2.1

1,2

2.2

0.4

i

Table 5. Performance

of a single 2-issue superscalar

processor.

Pcogram

comDcess

cqntott

m88k.im

MPsim

applu

apsi

swim

tomcatv

pmakc

IPC

1.2

1.8

2.3

1.2

1.7

1,2

2.2

1,3

1,4

BP Rate
%

I cache
%MPC1

D cache
%MPCI

L2 cache
%MPCI

86.4

80,0

92.6

81.6

79.7

95.6

99.8

99.7

82.7

0.0

0.0

0.1

3.4

0.0

0.2

0.0

0.0

0.7

3.9

1.1

0.0

1.7

2.8

3.1

2.3

4.2

1.0

4

1.1

1.1

0.0

2,3

2.8

2.6

2.5

a

4.3

0.6

1+

Table 6. Performance

of

the 6-issue superscalar

processor.

the major causes of processor stalls in a superscrdar proces-
One of
sor is cache misses. However,
cache misses in a dynamically
sched-
speculative
uled superscalar
and non-
execution
processor with
caches are not straightforward
blocking
to characterize. The cache
in a single issue in-order processor are net neces-
misses that occur
sarily the same as the misses that will occur
in the speculative out-
of-order processor.
In speculative
processors there are misses that
that never complete. With
are caused by speculative
instructions
non-blocking
caches, misses may also occur
to lines which already
have outstanding misses. Both types of misses tend to inflate the
cache miss rate of a speculative out-of-order
processor,
lle
second
the higher L2 cache miss
responsible
type of miss is mainly
for
rates of
the 6-issue processor
compared
to the 2-issue processor,
even though the cache sizes are equal.

the MP
for one processor of
Figure 4 shows the IPC breakdown
microarchitecture
In addition to the actual
IPC of two.
with an ideal
IPC achieved, we show the loss in IPC due to data and instruction
stalls. We see that a large percentage of
cache stalls, and pipeline
the IPC loss is due to data cache stall
time, This is caused by the
small size of the primary data cache. Mk88ksim, MPsim ad pmake

cache stall
have significant
time which
instruction
these applications.
set size of
large instruction working
processes and significant
has multiple
kernel execution
further
increases the instruction
cache miss rate.

is due to the
Pmake also
time which

F@me 4.

IPC Breakdown

for a single 2-issue processor.

D Cache Stall

I Cache Stall

Pipelina Stall

Actual IPC

6-

5-

4-

f/3:

2-

1-

0-

Figure

5.

IPC Breakdown

for

the 6-issue processor.

Figure 5 shows the IPC breakdown
the SS microarchitecture.
for
amount of IPC is lost due to pipeline stalls.
We see that a significant
stalls relative to the two-issue processor
The increase in pipeline
is
ILP in the applications
due to limited
and the 2-cycle L1 data cache
hit
time. The larger
instruction
cache in the SS microarchitecture
the stalls due to instruction misses for all of
eliminates most of
the
the SPEC95 float-
except MPsim and pmake. Although
applications
amount of ILP,
ing point applications
have a significant
their perfor-
mance is limited
on the SS microarchitecture
due to data cache
stalls which consume over one-half of the available IPC

Table 7 shows cache miss rates for
given
the MP microarchitecture
in terms of MPCI. To reduce miss-rate effects caused by the idle
loop and spinning due to synchronization,
the number of completed
the single 2-issue processor. Comparing
instructions
are those of
for eqntott, m88ksim and apsi
Table 5 and Table 7 shows that
the
higher data cache miss rates
MP microarchitecture
has significantly
than the single 2-issue processor. This is due primarily
to the high-

9

q Ss
n MP

4

3.5

3 1

-

Figure

6. Performance

comparison

of SS and MP.

that cannot be parallelized
Our results show that on applications
the
than one proces-
performs 30% better
superscalar
rnicroarchitecture
architecture. On applications
the multiprocessor
sor of
tine
with
microarchitec-
parallelism the multiprocessor
grained thread-level
rnicroarchi-
the superscalar
ture can exploit
this parallelism so that
tecture is at most 109to better, even at
the same clock
rate. We
the higher clock rates possible with simpler CPUS in
anticipate that
difference.
this small performance
will eliminate
the multiprocessor
large grained
with
On applications
thread-level
parallelism
and
the multiprocessor microarchitecture
workloads
multiprogramming
performs 50-1 00% better
than the wide superscalar
tnicroarchitec-
ture.

Acknowledgments

We would like to thank Edouard Bugnion, Mendel Rosenblum, Ben
their help with SimOS, Doug Will-
Verghese and Steve Herrod for
group for use
the SUIF compiler
iams for his assistance with MXS,
of
their applications,
and the reviewers
for
their
insightful
com-
ments. This work was supported by DARPA contracts DABT63-95-
C-0089 and DABT63-94-C-O054.

Application
.
.
commess
eqntott

m88tilm

I MPsim

I

apsi

swim

tomcatv

pmakc

I cache
%MPCI

D cache
%MPCI

L2 cache
%MPCI

0.0

0.6

2.3

4.8

2,7

0.0

0.0

2.4

I

3.5

5.4

3.3

2.5

6.9

1.2

7.8

4.6

I

I

1.0

1.2

0.0

3.4

2.0

1.5

2.5

0.7

Table 7. Performance

of

the 4 x 2-issue processor.

Although
degree of communication
in these applications.
present
an increase in the data cache miss rate,
pmake also exhibits
is
it
in the MP
caused by process migration
from processor
to processor
micro architecture.

between the SS and
comparison
Figure 6 shows the performance
performance
is measured
as the
MP microarchitectures.
The
relative to the single 2-issue pro-
speedup of each microarchitectnre
cessor. On compress, an application with little parallelism,
the MP
even though three of
is able to achieve 75% of
the SS performance
the four processors are idle. Neither microarchitecture
shows sig-
nificant
improvement
over the 2-issue processor, however.

parallelism and high-communi-
For applications
with fine-gmined
the MP and SS are simi-
cation, such as eqntott, m88ksim and apsi,
parallelism,
lar. Both architectures
are able to exploit
tine-grained
although
in different ways. The SS microarchitectnre
relies on the
ILP from a single thread of control. The MP
dynamic extraction
of
ILP and can, unlike con-
can take advantage of moderate levels of
thread-level
ventional multiprocessors,
exploit
fine-grained
paral-
lelism. Both the SS and MP approaches provide
a 30% to 100%
performance
increase over the 2-issue processor.

allow the MP
large
of parallelism
amounts
Applications
with
to take advantage of coarse-grained
microarchitecture
parallelism
to fine-grained
in addition
parallelism and ILP. For
these applica-
outperform the SS microarchi-
the MP is able to significantly
tions,
tecture, whose ability
to dynamically
extract parallelism is limited
by the 128 instruction window.

7

Conclusions

integrated
The characteristics
technologies
circuit
of advanced
large numbers of gates
require us to look for new ways to utilize
delays. We have dis-
and mitigate
the effects of high interconnect
cussed the details of implementing
both a wide, dynamically
sched-
The
and a single chip multiprocessor.
processor
uled superscalar
and
issue mechanisms
complexity
implementation
of
the dynamic
with increasing
files scales quadraticrdly
size of
the register
issue
width and ultimately
impacts
the cycle time of
the machine. The
which is composed of
rnicroarchitecture,
alternative multiprocessor
the same
in approximately
simpler processors, can be implemented
the multiprocessor
area. We believe that
rnicroarchitecture
will be
easier to implement
and will
reach a higher clock rate.

10

SimOS
approach, ”
IEEE
Parallel
vol. 4, no. 3, 1995.
Technology,

and Distributed

[19] M.

S. Herrod, E. Witchel,
E. Bugnion,
and A.
Rosenblum,
trends on operating
impact of architectural
Gupta,
“The
of
system performance, ”
Proceedings
15th
ACM
symposium on Operating
Colorado,
Systems Principles,
December,
1995.

Data Memory
Bandwidth
[20] G. Sohi and M. Franklin,
“High
Proceedings
Systems for Superscalar Processors: ’
of 4th
Int.
Con$
Architectural
Support
for
Programming
Lunguages and Operating Systems (ASPLOS-IV),
pp. 53-
62, April,
1991.

[21]

J. E. Thornton,
Proceedings

operation in the Control Data 6600~’
“Parallel
of Spring Joint Computer Conference,
1964.

[22] D. W. Wall,
“Limits
of Instruction-Level
Parrdlelism~’
Digital
Western Research Laboratory, WRL Research Report 93/
6, November
1993.

[23]

[24]

[25]

S. C. Woo, M. Ohara~o&a~:,
J.P. Singh and A. Gupta,
Characterization
SPLASH-2
Methodological
Considerations”,
22nd Annual
Computer
Santa Margherita,
Architecture,
1995,

“The
and
Int. Symp.
June
Italy,

“R1OOOO Superscalar Microprocessor, ”
K. Yeager
et. al.,
presented at Hot Chips VII, Stanford, CA, 1995.

J. Zurawski,
verification
workstation:’
89-99, 1995.

J. Murray
of
Digital

and P. Lemmon,
AlphaStation
the
Technical
Journal,

design and
“The
600
5-series
vol. 7, no. 1, pp.

References

[1]

[2]

[3]

J. M. Anderson, M. S, Lam, and C.-W.
S. P. Amarasinghe,
the SUIF compiler
overview of
Tseng,
“An
for scalable
the Seventh SIAM
Proceedings
parallel machines;
of
Conference
on
Parallel
Processing
for
Scientific
1995.
Compiler, San Francisco,

S. Amarasinghe
future hot chips, ”
for
compilers
“Hot
et.al.,
presented at Hot Chips WI, Stanford, CA, 1995.

D. W. Anderson, F. J. Sparacio, and R. M. Tomasulo,
philosophy
IBM System/360 model
91: Machine
instruction-handling;
IBM Journal
of Research
Development,
vol. 11, pp. 8-24,1967.

“The
and
and

64b quad-issue CMOS
300MHz
“A
[4] W. Bowhill
et. al.,
IEEE International
Solid-State Circuits
microprocessor; ’
Conference Digest of Technical Papers, pp. 182-1183, San
Francisco, CA, 1995.

[5]

[6]

[7]

[8]

and M,
J. Anderson, T. Mowry, M. Rosenbhrm,
E. Bugnion,
Page
for
Lam.
Coloring
“Compiler-Directed
International
Proceedings
Seventh
Multiprocessors: ’
Symp.
Programming
Support
Architectural
for
Languages
and Operating
Systems
(ASPLOS
VII),
October 1996.

“Chart watch: RISC processors, ” Microprocessor
10, no. 1, p. 22, January, 1996.

Report, vol.

“Optimization
T. Conte, K. Menezes, P. Mills, and B. Patel,
of
issue
for
fetch mechanisms
instmction
rates, ”
high
Symposium
International
of the 22nd Annual
Proceedings
Architecture,
Computer
on
pp.
333-344,
Santa
Mrrrgherita
Ligure,
Italy, June, 1996.

“A 200-MHz
D. Dobberpuhl
et. al.,
IEEE Journal
microprocessor, ”
VO1. 27, Pp. 1555–1557,
1992.

64-b dual-issue CMOS
of Solid-State
Circuits,

[9]

Don

IEEE
nightmare;”
interconnect
‘The
Drappper,
International
Solid-State Circuits Conference Digest of
Technical Papers, p. 278, San Francisco, CA, 19!~6.

[10]

K.

Farkas,
N.
considerations
Proceedings
Computer
February,

“Register
file
and P. Chow,
Jouppi,
in dynamically
processors, ”
scheduled
the 2nd Int. Symp. on High-Per@nnance
of
Architecture,
San Jose, CA,
40-51,
pp.
1996.

[11 ]

[12]

J, Hennessy
architecture
Magazine,

technolc)gy
and
“Computer
and N.
Jouppi,
an evolving
IEEE Computer
interaction,”
vol. 24, no, 1, pp. 18-29, 1991.

J. L. Hennessy and D. A. Patterson, Computer Architecture
A
San Francisco,
Approach
Quantitative
2nd Edition.
California
Morgan Kaufman Publishers,
Inc., 1996.

[13] M. Johnson, Superscalar Microprocessor
Cliffs, NJ: Prentice Hall,
Inc., 1991

Design. Englewood

[14]

“A auad issue
J. Lotz. G. Lesartre. S. Naffzinszer. and D. Kism.
S;lid-State
out-of-order M’SC CPU, ”
~EEE Interna;i%al
Circuits Conference Digest of Technical Papers,
lpp. 210-
211, San Francisco, CA, 1996.

[15]

s.

[16]

B.

branch
McFarling,
WRL
predictors: ’
“Combining
Technicrd Note TN-36, Digital
Equipment Corporation,
1993.

and K. Olukohm,
L. Hammond,
“Evaluating
A. Nayfeh,
microprc~cessor~ ’
a multiprocessor
for
alternatives
of 23rd Int. Symp. Computer Architecture,
Proceedings
pp. 66-77, Philadelphia,
PA, 1996.

[17]

J. Ousterhout,
aren’t
faster as
getting
systems
operating
“Why
fast as hardware?;
Summer 1990 USENIX Conference,
pp. 247-256,
June 1990.

[18] M. Rosenblum,

S. Herrod, E. Witchel,

and A. Gupta,

“The

11


.

continues to have a bias for the past in that it

focuses on desktop and server applications. In

Christoforos E. Kozyrakis
David A. Patterson
University of California, Berkeley
e
r
bersquare
a
u
A New Direction for
q
s
r
e
b
y
C
A
Computer
Architecture Research
Current computer architecture research 
dvances  in  integrated  circuit  technology will  soon
allow the  integration of one billion transistors on a
single chip. This is an exciting opportunity for com-
puter architects and designers; their challenge will be
to propose microprocessor designs that use this huge
transistor budget efﬁciently and meet the requirements
of future applications.
The real question is, just what are these future appli-
cations? We contend that current computer architec-
ture research continues to have a bias for the past in
that it focuses on desktop and server applications. In
our view, a different computing domain—personal
mobile computing—will play a signiﬁcant role in dri-
ving technology in the next decade. In this paradigm,
the  basic  personal  computing  and  communicating
devices will be portable and battery operated, and will
support  multimedia  tasks  like  speech  recognition.

BILLION-TRANSISTOR PROCESSORS
Computer recently  produced  a  special  issue  on
“Billion-Transistor Architectures.”1 The ﬁrst three arti-
cles discussed problems and trends that will affect future
processor design. Seven articles from academic research
groups  proposed  microprocessor  architectures  and
implementations for billion-transistor chips. These pro-
posals covered a wide architecture space, ranging from
out-of-order designs to reconﬁgurable systems. At about
the same time, Intel and Hewlett-Packard presented the
basic characteristics of their next-generation IA-64 archi-
tecture, which is expected to dominate the high-perfor-
mance processor market within a few years.2

These devices will pose a different set of requirements
for microprocessors and could redirect the emphasis
of computer architecture research.

for microprocessors and could redirect the

emphasis of computer architecture research.

our view, a different computing domain—personal

mobile computing—will play a signiﬁcant role in

driving technology in the next decade. This

domain will pose a different set of requirements

24

Computer

0018-9162/98/$10.00 © 1998 IEEE

.

Table  1.  Number  of  memor y  transistors  in  the  billion-transistor  micro -
processors.

It is no surprise that most of these proposals focus
on the computing domains that have shaped proces-
sor architecture for the past decade:

• The uniprocessor desktop running technical and
scientiﬁc applications, and
• the multiprocessor server used for transaction
processing and ﬁle-system workloads.

Table 1 summarizes the basic features of the pro-
posed architectures presented in the Computer special
issue. We also include two other processor implemen-
tations not in the special issue, the Trace and IA-64.
Developers  of  these  processors  have  not  presented
implementation details. Hence we assume that they will
have billion-transistor implementations and speculate
on the number of transistors they devote to on-chip
memory or caches. (For descriptions of these proces-
sors, see the “Using a Billion Transistors” sidebar.)
Table 1 reports the number of transistors used for
caches and main memory  in each billion-transistor
processor. The amount varies  from almost half  the
transistor budget to 90 percent. Interestingly, only one
design, Raw, uses that budget as part of the main sys-
tem memory. The majority use 50 to 90 percent of their
transistor budget on caches, which help mitigate the
high latency and low bandwidth of external memory.
In other words, the conventional vision of future
computers spends most of the billion-transistor bud-

Architecture
Advanced superscalar

Superspeculative

Trace

Simultaneous 
multithreading

Chip multiprocessor

IA-64

Raw

No. of memory 
transistors (millions)
910

Key idea
Wide-issue superscalar 
processor with speculative 
execution; multilevel on-chip 
caches
Wide-issue superscalar 
processor with aggressive data 
and control speculation; multilevel, 
on-chip caches
Multiple distinct cores that 
speculatively execute program 
traces; multilevel on-chip caches
Wide superscalar with support 
for aggressive sharing among 
multiple threads; multilevel on-chip 
caches
Symmetric multiprocessor;
system shared second-level cache
VLIW architecture with support 
for predicated execution and long-
instruction bundling
Multiple processing tiles with 
reconfigurable logic and memory 
interconnected through a 
reconfigurable network

820

600

810

450

600

670

get on redundant, local copies of data normally found
elsewhere in the system. For applications of the future,
is this really our best use of a half-billion transistors?

Using a Billion Transistors
The ﬁrst two architectures in Computer’s
survey—the  advanced  superscalar  and
superspeculative—have very similar char-
acteristics. The basic idea is a wide super-
scalar organization with multiple execution
units or functional cores. These architectures
use multilevel caching and aggressive pre-
diction of data, control, and even sequences
of instructions (traces) to use all the avail-
able instruction level parallelism (ILP). Due
to their similarity, we group them together
and call them wide superscalar processors.
The trace processor consists of multiple
superscalar processing cores, each execut-
ing a trace issued by a shared instruction
issue unit. It also employs trace and data
prediction, and shared caches.
The simultaneous multithreading (SMT)
processor uses multithreading at the gran-
ularity of instruction issue slot to maximize
the use of a wide-issue, out-of-order super-
scalar processor. It does so at the cost of

additional complexity in the issue and con-
trol logic.
The chip multiprocessor (CMP) uses the
transistor budget by placing a symmetric
multiprocessor on a single die. There will
be eight uniprocessors on the chip, all sim-
ilar  to  current  out-of-order  processors.
Each uniprocessor will have separate ﬁrst-
level caches but share a large second-level
cache and the main memory interface.
IA-64 can be considered a recent com-
mercial  reincarnation  of  architectures
based on the very long instruction word
(VLIW), now renamed explicitly parallel
instruction computing. Based on the infor-
mation announced thus far, its major inno-
vations  are  the  instruction  dependence
information attached to each long instruc-
tion and the support for bundling multiple
long instructions. These changes attack the
problem of scaling and low code density
that  often  accompanied  older  VLIW
machines.  IA-64 also  includes hardware

checks for hazards and interlocks, which
helps  to  maintain  binary  compatibility
across chip generations. Finally, it supports
predicated  execution  through  general-
purpose  predication  registers,  which
reduces control hazards.
The Raw machine is probably the most
revolutionary architecture proposed, as it
incorporates reconﬁgurable logic for gen-
eral-purpose  computing.  The  processor
consists of 128 tiles, each of which has a
processing  core,  small  first-level  caches
backed  by  a  larger  amount  of  dynamic
memory (128 Kbytes) used as main mem-
ory, and a reconﬁgurable functional unit.
The tiles interconnect in a matrix fashion
via a reconﬁgurable network. This design
emphasizes  the  software  infrastructure,
compiler, and dynamic event support. This
infrastructure handles the partitioning and
mapping of programs on the tiles, as well
as the conﬁguration selection, data rout-
ing, and scheduling.

November 1998

25

.

Table  2.  Evaluation  of  billion-transistor  processors  for  the  desktop/ser ver  domain.  “Wide  superscalar”  includes  the  advanced 
superscalar and superspeculative processors.

Characteristic
SPECint04 per formance
SPECfp04 per formance
TPC-F per formance
Software effort
Physical-design complexity

Wide superscalar
+
+
0
+
-

Trace
+
+
0
+
0

Simultaneous multithreading Chip multiprocessor
0
+
+
+
+
+
0
0
-
0

IA-64
+/0
+
0
0
0

Raw
0
0
-
-
+

THE DESKTOP/SERVER DOMAIN
In optimizing processors and computer systems for
the desktop and server domain, architects often use the
popular SPECint95, SPECfp95, and TPC-C/D bench-
marks. Since this computing domain will likely remain
signiﬁcant as billion-transistor chips become available,
architects  will  continue  to  use  similar  benchmark
suites. We playfully call such future benchmark suites
“SPECint04’’ and “SPECfp04’’ for technical/scientiﬁc
applications, and “TPC-F’’ for database workloads.
Table 2 presents our prediction of how these proces-
sors will perform on these benchmarks. We use a grad-
ing system of + for strength, 0 for neutrality, and - for
weakness.

Desktop
For the desktop environment, the wide superscalar,
trace,  and  simultaneous  multithreading  processors
should deliver the highest performance on SPECint04.
These architectures use out-of-order and advanced pre-
diction  techniques  to  exploit most  of  the  available
instruction level parallelism (ILP) in a single sequen-
tial program. IA-64 will perform slightly worse because
very long instruction word (VLIW) compilers are not
mature enough to outperform the most advanced hard-
ware  ILP  techniques—those which exploit  runtime
information. The chip multiprocessor (CMP) and Raw
will  have  inferior  performance  since  research  has
shown that desktop applications are not highly paral-
lelizable. CMP will still beneﬁt from the out-of-order
features of its cores.
For ﬂoating-point applications, on the other hand,
parallelism and high memory bandwidth are more
important  than  out-of-order  execution;  hence  the
simultaneous multithreading (SMT) processor and
CMP will have some additional advantage.

Server
In the server domain, CMP and SMT will provide
the best performance, due to their ability to use coarse-
grained  parallelism  even  with  a  single  chip.  Wide
superscalar,  trace,  or  IA-64  systems  will  perform
worse, because current evidence indicates that out-of-

order execution provides only a small beneﬁt to online
transaction processing (OLTP) applications.3 For the
Raw architecture, it is difﬁcult to predict any poten-
tial success of its software to map the parallelism of
databases  on  reconfigurable  logic  and  software-
controlled caches.

Software effort
For any new architecture to gain wide acceptance,
it must run a signiﬁcant body of software.4 Thus the
effort needed to port existing software or develop new
software is very important. In this regard, the wide
superscalar, trace, and SMT processors have the edge,
since  they  can  run  existing  executables.  The  same
holds for CMP, but this architecture can deliver the
highest performance only if applications are rewrit-
ten in a multithreaded or parallel fashion. As the past
decade has taught us, parallel programming for high
performance is neither easy nor automated.
In the same vein, IA-64 will supposedly run exist-
ing executables, but signiﬁcant performance increases
will  require  enhanced  VLIW  compilers.  The  Raw
machine relies on the most challenging software devel-
opment. Apart from the requirements for sophisti-
cated routing, mapping, and runtime-scheduling tools,
the Raw processor will need new compilers or libraries
to make this reconﬁgurable design usable.

Complexity
One last issue is physical design complexity, which
includes the effort devoted to the design, veriﬁcation,
and testing of an integrated circuit.
Currently, advanced microprocessor development
takes  almost  four  years  and  a  few  hundred  engi-
neers.1,5 Testing complexity as well as functional and
electrical veriﬁcation efforts have grown steadily, so
that these tasks now account for the majority of the
processor development effort.5 Wide superscalar and
multithreading processors exacerbate both problems
by  using  complex  techniques—like  aggressive
data/control prediction, out-of-order execution, and
multithreading—and  nonmodular  designs  (that  is,
they use many individually designed multiple blocks).

26

Computer

.

With the IA-64 architecture, the basic challenge is the
design and veriﬁcation of the forwarding logic among
the multiple functional units on the chip.
Designs  for  the  CMP,  trace  processor,  and  Raw
machine  alleviate  the  problems  of  physical-design
complexity by being more modular. CMP  includes
multiple copies of the same uniprocessor. Yet, it car-
ries on the complexity of current out-of-order designs
with support for cache coherency and multiprocessor
communication. The trace processor uses replicated
processing elements to reduce complexity. Still, trace
prediction and issue involve intratrace dependence
checking, register remapping, and intraelement for-
warding. Such features account for a signiﬁcant por-
tion of the complexity in wide superscalar designs.
Similarly, the Raw design requires the design and
replication of  a  single processing  tile  and network
switch. Veriﬁcation of a reconﬁgurable organization is
trivial in terms of the circuits, but veriﬁcation of the
mapping software is also required, which is often not
trivial.
The conclusion we can draw from Table 2 is that
the proposed billion-transistor processors have been
optimized for such a computing environment, and that
most of them promise impressive performance. The
only concern for the future is the design complexity
of these architectures.

A NEW TARGET: PERSONAL MOBILE COMPUTING
In the past few years, technology drivers changed
signiﬁcantly. High-end systems alone used to direct
the evolution of computing. Now, low-end systems
drive technology, due to their large volume and atten-
dant proﬁts. Within this environment, two important
trends have evolved that could change the shape of
computing.
The ﬁrst new trend is that of multimedia applica-
tions. Recent improvements in circuit technology and
innovations in software development have enabled the
use of real-time data types like video, speech, anima-
tion, and music. These dynamic data  types greatly
improve the usability, quality, productivity, and enjoy-
ment of PCs.6 Functions like 3D graphics, video, and
visual imaging are already included in the most pop-
ular applications, and it is common knowledge that
their inﬂuence on computing will only increase:

• 90  percent  of  desktop  cycles  will  be  spent  on
“media” applications by 2000;7
• multimedia workloads will continue to increase
in importance;1 and
• image, handwriting, and speech recognition will
pose other major challenges.5

The  second  trend  is  the  growing  popularity  of
portable  computing  and  communication  devices.

Figure 1. Future 
personal mobile-
computing devices 
will incorporate the
functionality of several
current portable
devices.

Inexpensive gadgets that are small enough to ﬁt in a
pocket—personal digital assistants (PDA), palmtop
computers, Web phones,  and digital  cameras—are
joining  the  ranks  of  notebook  computers,  cellular
phones, pagers, and video games.8 Such devices now
support a constantly expanding range of functions,
and multiple devices are converging into a single unit.
This naturally leads to greater demand for comput-
ing power, but at the same time, the size, weight, and
power consumption of these devices must remain con-
stant. For example, a typical PDA is ﬁve to eight inches
by three inches, weighs six to twelve ounces, has two to
eight Mbytes of memory (ROM/RAM), and is expected
to run on the same set of batteries for a few days to a
few weeks.8 Developers have provided a large software,
operating system, and networking infrastructure (wire-
less modems, infrared communications, and so forth)
for such devices. Windows CE and the PalmPilot devel-
opment environment are prime examples.8

A new application domain
These  two  trends—multimedia  applications  and
portable electronics—will lead to a new application
domain and market in the near future.9 In the personal
mobile-computing environment, there will be a single
personal  computation  and  communication  device,
small enough to carry all the time. This device will
incorporate the functions of the pager, cellular phone,
laptop computer, PDA, digital camera, video game,
calculator, and remote shown in Figure 1.
Its most important feature will be the interface and
interaction with the user: Voice and image input and

November 1998

27

.

Most of the billion-
transistor 
architectures offer
only limited support
for personal mobile
computing.

output (speech and pattern recognition) will be
key functions. Consumers will use these devices
to take notes, scan documents, and check the
surroundings for speciﬁc objects.9 A wireless
infrastructure  for  sporadic connectivity will
support services like networking (to the Web
and  for  e-mail),  telephony,  and  global  posi-
tioning system (GPS) information. The device
will be fully functional even in the absence of
network connectivity.
Potentially, such devices would be all a per-
son needs to perform tasks ranging from note
taking to making an online presentation, and from
Web browsing to VCR programming. The numerous
uses of such devices and the potentially large volume
lead many to expect that this computing domain will
soon become at least as signiﬁcant as desktop com-
puting is today.

A different type of microprocessor
The  microprocessor  needed  for  these  personal
mobile-computing devices is actually a merged gen-
eral-purpose processor and digital-signal processor
(DSP) with the power budget of the latter. Such micro-
processors must meet four major requirements:

• high performance for multimedia functions,
• energy and power efﬁciency,
• small size, and
• low design complexity.

The basic characteristics of media-centric applica-
tions that a processor needs to support were speciﬁed
by Keith Diefendorff and Pradeep Dubey:6

• Real-time response. Instead of maximum peak
performance, processors must provide worst case
guaranteed performance that is sufﬁcient for real-
time qualitative perception in applications like
video.
• Continuous-media data types. Media functions
typically involve processing a continuous stream
of input (which is discarded once it is too old)
and continuously sending the results to a display
or speaker. Thus, temporal locality in data mem-
ory accesses—the assumption behind 15 years of
innovation in conventional memory systems—no
longer holds. Remarkably, data caches may well
be an obstacle to high performance for continu-
ous-media data types. This data is typically nar-
row (pixel images and sound samples are 8 to 16
bits wide, rather than the 32- or 64-bit data of
desktop machines). The capability  to perform
multiple operations on such data types in a sin-
gle wide data path is desirable.
• Fine-grained parallelism. Functions like image,

voice, and signal processing require performing
the same operation across sequences of data in a
vector or SIMD (single-instruction, multiple-data)
fashion.
• Coarse-grained parallelism. In many media appli-
cations, a pipeline of functions process a single
stream of data to produce the end result.
• High instruction reference locality. Media func-
tions usually have small kernels or loops that dom-
inate the processing time and demonstrate high
temporal and spatial locality for instructions.
• High memory bandwidth. Applications like 3D
graphics  require  huge memory  bandwidth  for
large data sets that have limited locality.
• High network bandwidth. Streaming data—like
video or images from external sources—requires
high network and I/O bandwidth.

With a budget of much less than two watts for the
whole device, the processor must be designed with a
power target of less than one watt. Yet, it must still
provide high performance for functions like speech
recognition. Power budgets close to those of current
high-performance microprocessors (tens of watts) are
unacceptable for portable, battery-operated devices.
Such devices should be able to execute functions at the
minimum possible energy cost.
After energy efﬁciency and multimedia support, the
third main requirement for personal mobile comput-
ers is small size and weight. The desktop assumption
of several chips for external cache and many more for
main memory is infeasible for PDAs; integrated solu-
tions that reduce chip count are highly desirable. A
related matter is code size, because PDAs will have
limited memory to minimize cost and size. The size of
executables is therefore important.
A ﬁnal concern is design complexity—a concern in
the desktop domain as well—and scalability. An archi-
tecture should scale efﬁciently not only in terms of per-
formance but also in terms of physical design. Many
designers consider long interconnects for on-chip com-
munication to be a limiting factor for future proces-
sors. Long interconnects should therefore be avoided,
since only a small region of the chip will be accessible
in a single clock cycle.10

Processor evaluation
Table 3 summarizes our evaluation of the billion-
transistor architectures with respect to personal mobile
computing.
As the table shows, most of these architectures offer
only limited support for personal mobile computing.
Real-time response. Because they use out-of-order
techniques and caches, these processors deliver quite
unpredictable performance, which makes it difﬁcult to
guarantee real-time response.

28

Computer

.

Table 3. Evaluation of billion-transistor processors for the personal mobile-computing domain. “Wide superscalar” includes the advanced
superscalar and superspeculative processors.

Characteristic
Real-time 
response

Continuous
data types

Fine-grained
parallelism

Coarse-grained 
parallelism
Code size

Memory 
bandwidth

Energy/power 
efficiency

Physical-design 
complexity
Design scalability

Wide 
superscalar
-

Trace
-

Simultaneous 
multithreading
0

Chip 
multiprocessor
0

IA-64
0

Raw
0

0

0

0

0

0

-

-

-

0

0

0

0

0

-

0

0

0

0

+

0

0

-

-

-

0

0

+

0

0

0

0

0

0

0

0

-

0

0

0

0

0

+

+

0

0

-

+

0

Comments
Out-of-order execution, branch
prediction, and/or caching 
techniques make execution 
unpredictable. 
Caches do not efficiently 
support data streams with 
little locality.
MMX-like extensions are 
less efﬁcient than full vector 
support. Reconﬁgurable logic 
can use ﬁne-grained parallelism.

For the first four architectures, 
the use of loop unrolling and 
software pipelining may 
increase code size. IA-64’s 
VLIW instructions and Raw’s 
hardware configuration bits 
lead to larger code sizes.
Cache-based designs interfere 
with high memory bandwidth 
for streaming data.
Out-of-order execution 
schemes, complex issue logic, 
forwarding, and reconfigurable 
logic have power penalties.

Long wires—for forwarding 
data or for reconfigurable inter-
connects—limit scalability.

Continuous-media data types. Hardware-controlled
caches complicate support for continuous-media data
types.
Parallelism. The  billion-transistor  architectures
exploit ﬁne-grained parallelism by using MMX-like
multimedia extensions or reconfigurable execution
units. Multimedia extensions expose data alignment
issues to the software and restrict the number of vec-
tor or SIMD elements operated on by each  instruc-
tion.  These  two  factors  limit  the  usability  and
scalability of multimedia extensions. Coarse-grained
parallelism, on the other hand, is best on the simul-
taneous multithreading, CMP, and Raw architectures.
Code size. Instruction reference locality has tradi-
tionally  been  exploited  through  large  instruction

caches. Yet designers of portable systems would pre-
fer  reduced  code  size,  as  suggested  by  the  16-bit
instruction  sets  of Mips  and ARM. Code  size  is  a
weakness for IA-64 and any other architecture that
relies heavily on loop unrolling for performance, as
such code will  surely be  larger  than  that of 32-bit
RISC machines. Raw may also have code size prob-
lems, as programmers must “program’’ the reconﬁg-
urable  portion  of  each  data  path.  The  code  size
penalty of the other designs will likely depend on how
much they exploit loop unrolling and in-line proce-
dures to achieve high performance.
Memory bandwidth. Cache-based architectures have
limited  memory  bandwidth.  Limited  bandwidth
becomes a more acute problem in the presence of multi-

November 1998

29

.

Table 4. Evaluation of vector IRAM for two computing domains.
Characteristics
Domain
SPECint04
Desktop/server computing
SPECfp04
TPC-F
Software effort
Physical-design complexity
Real-time response
Continuous data types
Fine-grained parallelism
Coarse-grained parallelism
Code size
Memory bandwidth
Energy/power efficiency
Design scalability

Personal mobile computing

Rating
-
+
0
0
0
+
+
+
0
+
+
+
0

ple data sequences (with little locality) streaming through
a system—exactly the case with most multimedia data.
The potential use of streaming buffers and cache
bypassing would help sequential bandwidth, but such
techniques fail to address the bandwidth requirements
of indexed or random accesses. In addition, it would
be embarrassing to rely on cache bypassing for per-
formance when a design dedicates 50 to 90 percent of
the transistor budget to caches.
Energy/power efﬁciency. Despite the importance to
both portable and desktop domains,1 most designs
do  not  address  the  energy/power  efficiency  issue.
Several characteristics of the billion-transistor proces-
sors increase the energy consumption of a single task
and the power the processor requires. 
These characteristics include redundant computa-
tion  for  out-of-order  models,  complex  issue  and
dependence analysis logic, fetching a large number of
instructions for a single loop, forwarding across long
wires, and the use of typically power-hungry recon-
ﬁgurable logic.
Design scalability. As for physical design scalability,
forwarding results across large chips or communica-
tion among multiple cores or tiles is the main prob-
lem  of  most  billion-transistor  designs.  Such  com-
munication already requires multiple cycles in several
high-performance,  out-of-order  designs.  Simple
pipelining of  long  interconnects  is not a  suf ficient
solution as  it exposes  the  timing of  forwarding or
communication to the scheduling logic or software. It
also increases complexity.
The conclusion to draw from Table 3 is that the pro-
posed processors  fail  to meet many of  the require-
ments of the new computing model. Which begs the
question, what design will?

30

Computer

VECTOR IRAM
Vector IRAM,1 the architecture proposed by our
research group, is our ﬁrst attempt to design an archi-
tecture and implementation that match the require-
ments of the mobile personal environment.
We base vector IRAM on two main ideas: vector
processing and the integration of logic and DRAM on
a single chip. The former addresses many demands of
multimedia processing, and the latter addresses the
energy efﬁciency, size, and weight demands of portable
devices. Vector IRAM is not the last word on com-
puter  architecture  research  for mobile multimedia
applications, but we hope it proves a promising ini-
tial step.
The vector IRAM processor consists of an in-order,
dual-issue superscalar processor with ﬁrst-level caches,
tightly integrated with a vector execution unit that
contains eight pipelines. Each pipeline can support
parallel operations on multiple media types and DSP
functions. The memory system consists of 96 Mbytes
of DRAM used as main memory. It is organized in a
hierarchical  fashion with  16  banks  and  eight  sub-
banks per bank, connected to the scalar and vector
unit through a crossbar. This memory system provides
sufﬁcient sequential and random bandwidth even for
demanding applications.
External I/O is brought directly to the on-chip mem-
ory through high-speed serial lines (instead of paral-
lel buses), operating in the Gbit/s range.
From a programming point of view, vector IRAM
is comparable to a vector or SIMD microprocessor.

Comparison with billion-transistor architectures
We asked reviewers—a group that included most of
the architects of the billion-transistor architectures—
to grade vector IRAM. Table 4 presents the median
grades they gave vector IRAM for the two computing
domains—desktop/server and mobile personal com-
puting. We requested reviews, comments, and grades
from all the architects of the processors in Computer’s
September 1997 special issue, and although some were
too busy, most were kind enough to respond.
Obviously, vector IRAM is not competitive within
the desktop/server domain. Indeed, this weakness in
the conventional computing domain is probably the
main reason some are skeptical of the importance of
merged  logic-DRAM  technology.  As  measured  by
SPECint04, we do not expect vector processing to ben-
eﬁt integer applications. Floating-point intensive appli-
cations, on the other hand, are highly vectorizable. All
applications would still beneﬁt from the low memory
latency and high memory bandwidth.
In the server domain, we expect vector IRAM to per-
form poorly due to its limited on-chip memory. A poten-
tially different evaluation for the server domain could
arise  if we  examine workloads  for  decision  support

.

instead of online transaction processing.11 In decision
support, small code loops with highly data-parallel oper-
ations dominate execution time. Architectures like vec-
tor IRAM and Raw should thus perform signiﬁcantly
better on decision support than on OLTP workloads.
In terms of software effort, vectorizing compilers
have been developed and used in commercial envi-
ronments for decades. Additional work is required to
tune such compilers for multimedia workloads and
make DSP features and data types accessible through
high-level languages. In this case, compiler-delivered
MIPS/ watt is the proper ﬁgure of merit.
As for design complexity, vector IRAM is a highly
modular design. The necessary building blocks are the
in-order scalar core, the vector pipeline (which is repli-
cated eight times), and the basic memory array tile.
The lack of dependencies within a vector instruction
and the in-order paradigm should also reduce the ver-
iﬁcation effort for vector IRAM.
The open questions are what complications arise
from merging high-speed logic with DRAM, and what
is  the  resulting  impact  on  cost,  yield,  and  testing.
Many  DRAM  companies  are  investing  in  merged
logic-DRAM fabrication lines, and many companies
are exploring products in this area. Also, our project
sent a nine-million-transistor test chip to fabrication
this fall. It will contain several key circuits of vector
IRAM in a merged logic-DRAM process. We expect
the answer to these questions to be clearer in the next
year. Unlike the other billion-transistor proposals, vec-
tor IRAM’s challenge is the implementation technol-
ogy rather than the microarchitectural design.

Support for personal mobile computing
As mentioned earlier, vector IRAM is a good match
to the personal mobile-computing model. The design
is in-order and does not rely on data caches, making
the delivered performance highly predictable—a key
quality in supporting real-time applications. The vec-
tor model is superior to limited, MMX-like, SIMD
extensions for several reasons:

• It provides explicit control of the number of ele-
ments each instruction operates on.
• It allows scaling of the number of elements each
instruction  operates  on  without  changing  the
instruction set architecture.
• It does not expose data packing and alignment
to  software,  thereby  reducing  code  size  and
increasing performance.
• A single design database can produce chips of
varying cost-performance ratios.

Since most media-processing functions are based on
algorithms working on vectors of pixels or samples,
it’s not surprising that a vector unit can deliver the

Vector IRAM’s
challenge is in the
implementation 
technology rather
than the microarchi-
tectural design.

highest  performance.  The  presence  of  short
vectors in some applications does not pose a
performance problem  if the start-up time of
each vector instruction is pipelined and chain-
ing  (the  equivalent  of  forwarding  in  vector
processors) is supported, as we intend.
The code size of programs written for vec-
tor  processors  is  small  compared  to  that  of
other architectures. This compactness is possi-
ble because a single vector instruction can spec-
ify whole loops.
Memory bandwidth, both sequential and random,
is available from the on-chip hierarchical DRAM.
Vector IRAM includes a number of critical DSP fea-
tures like high-speed multiply accumulates, which it
achieves through instruction chaining. Vector IRAM
also provides auto-increment addressing  (a special
addressing mode often found in DSP chips) through
strided vector memory accesses.
We designed vector IRAM to be programmed in
high-level languages, unlike most DSP architectures.
We did this by avoiding features like circular or bit-
reversed addressing and  complex  instructions  that
make DSP processors poor compiler targets.12 Like all
general-purpose processors, vector IRAM provides
virtual memory support, which DSP processors do not
offer.
We expect vector IRAM to have high energy efﬁ-
ciency as well. Each vector instruction speciﬁes a large
number of independent operations. Hence, no energy
needs to be wasted for fetching and decoding multiple
instructions or checking dependencies and making
various predictions. In addition, the execution model
is strictly in order. Vector processors need only lim-
ited forwarding within each pipeline (for chaining)
and do not require chaining to occur within a single
clock cycle. Hence, designers can keep  the control
logic simple and power efﬁcient, and eliminate most
long wires for forwarding.
Performance comes from multiple vector pipelines
working in parallel on the same vector operation, as
well as from high-frequency operation. This allows
the same performance at lower clock rates—and lower
voltages—as long as we add more functional units. In
CMOS logic, energy increases with the square of the
voltage, so such trade-offs can dramatically improve
energy efﬁciency. DRAM has been traditionally opti-
mized for low power, and the hierarchical structure
provides the ability to activate just the sub-banks con-
taining the necessary data.
As for physical-design scalability, the processor-mem-
ory crossbar is the only place were vector IRAM uses
long wires. Still, the vector model can tolerate latency
if sufﬁcient ﬁne-grained parallelism is available. So deep
pipelining is a viable solution without any hardware or
software complications in this environment.

November 1998

31

.

F or almost two decades, architecture research
has focused on desktop or server machines. As
a  result,  today’s  microprocessors  are  1,000
times faster for desktop applications. Nevertheless,
in doing so, we are designing processors of the future
with a heavy bias  toward  the past. To design  suc-
cessful processor architectures for the future, we ﬁrst
need to explore future applications and match their
requirements in a scalable, cost-effective way.
Personal mobile computing offers a vision of the
future with a much richer and more exciting set of
architecture research challenges than extrapolations
of the current desktop architectures and benchmarks.
Vector IRAM is an initial approach in this direction.
Put another way, which problem would you rather
work  on:  improving  performance  of  PCs  running
FPPPP—a  1982  Fortran  benchmark  used 
in
SPECfp95—or  making  speech  input  practical  for
PDAs? It is time for some of us in the very successful
computer architecture community to investigate archi-
tectures with a heavy bias for the future. v

Acknowledgments
The ideas and opinions presented here are from dis-
cussions within the IRAM group at UC Berkeley. In
addition, we thank the following for their useful feed-
back, comments, and criticism on earlier drafts, as
well as the grades for vector IRAM: Anant Agarwal,
Jean-Loup Baer, Gordon Bell, Pradeep Dubey, Lance
Hammond, Wang Wen-Hann, John Hennessy, Mark
Hill, John Kubiatowicz, Corinna Lee, Henry Levy,
Doug  Matzke,  Kunle  Olukotun,  Jim  Smith,  and
Gurindar Sohi.
This research is supported by DARPA (DABT63-
C-0056), the California State MICRO program, NSF
(CDA-9401156),  and  by  research  grants  from  LG
Semicon,  Hitachi,  Intel,  Microsoft,  Sandcraft,
SGI/Cray, Sun Microsystems, Texas Instruments, and
TSMC.

References
1. D. Burger and J. Goodman, “Billion-Transistor Archi-
tectures,” Computer, Sept. 1997, pp. 46-47.
2. J.  Crawford  and  J.  Huck,  “Motivations  and  Design
Approach for the IA-64 64-bit Instruction Set Architec-
ture,” Microprocessor Forum, Micro Design Resources,
1997.
3. K. Keeton et al., “Performance Characterization of the
Quad Pentium Pro SMP Using OLTP Workloads,” Proc.
1998 Int’l Symp. Computer Architecture, IEEE CS Press,
Los Alamitos, Calif., 1998, pp. 15-26.
4. J. Hennessy and D. Patterson, Computer Architecture: A
Quantitative Approach, 2nd ed., Morgan Kaufmann,

San Francisco, 1996.
5. J. Wilson et al., “Challenges and Trends in Processor
Design,” Computer, Jan. 1998, pp. 39-50.
6. K. Diefendorff and P. Dubey, “How Multimedia Work-
loads Will Change Processor Design,” Computer, Sept.
1997, pp. 43-45.
7. W. Dally, “Tomorrow’s Computing Engines,” keynote
speech, Fourth Int’l Symp. High-Performance Computer
Architecture, Feb. 1998.
8. T. Lewis, “Information Appliances: Gadget Netopia,”
Computer, Jan. 1998, pp. 59-66.
9. G. Bell and J. Gray, Beyond Calculation: The Next 50
Years of Computing, Springer-Verlag, Feb. 1997.
10. D. Matzke, “Will Physical Scalability Sabotage Perfor-
mance Gains?” Computer, Sept. 1997, pp. 37-39.
11. P. Trancoso et al., “The Memory Performance of DSS
Commercial  Workloads  in  Shared-Memory  Multi-
processors,” Proc. Third Int’l Symp. High-Performance
Computer Architecture, IEEE CS Press, Los Alamitos,
Calif., 1997, pp. 250-260.
12. J.  Eyre  and  J.  Bier,  “DSP  Processors  Hit  the  Main-
stream,” Computer, Aug. 1998, pp. 51-59.

Christoforos E. Kozyrakis is currently pursuing a PhD
degree in computer science at the University of Cali-
fornia,  Berkeley.  Prior  to  that,  he  was  with  ICS-
FORTH, Greece, working on the design of single-chip
high-speed network  routers. His  research  interests
include microprocessor architecture and design, mem-
ory hierarchies, and digital VLSI systems. Kozyrakis
received a BSc degree in computer science from the
University of Crete, Greece. He is a member of the
IEEE and the ACM.

David A. Patterson teaches computer architecture at
the University of California, Berkeley, and holds the
Pardee Chair of Computer Science. At Berkeley, he
led the design and implementation of RISC I, likely
the ﬁrst VLSI reduced instruction set computer. He
was also a leader of the Redundant Arrays of Inex-
pensive Disks (RAID) project, which led to the high-
performance  storage  systems  produced  by  many
companies. Patterson received a PhD in computer sci-
ence from the University of California, Los Angeles.
He is a fellow of the IEEE Computer Society and the
ACM, and is a member of the National Academy of
Engineering.

Contact the authors at the Computer Science Divi-
sion, University of California, Berkeley, CA 94720-
1776; {kozyraki, patterson}@cs.berkeley.edu.

32

Computer

21st Century Computer Architecture 

A community white paper 
 
May 25, 2012 

1. Introduction and Summary 

Information  and  communication 
including 
transforming  our  world, 
is 
technology  (ICT) 
healthcare,  education,  science,  commerce,  government,  defense,  and  entertainment.  It  is  hard 
to  remember  that  20  years  ago  the  first  step  in  information  search  involved  a  trip  to  the  library, 
10  years  ago  social  networks  were  mostly  physical,  and  5  years  ago  “tweets”  came  from 
cartoon characters. 

Importantly, much  evidence  suggests  that  ICT  innovation  is  accelerating with many  compelling 
visions moving from science fiction toward reality1. Appendix A both touches upon these visions 
and  seeks  to  distill  their  attributes.  Future  visions  include  personalized medicine  to  target  care 
and  drugs  to  an  individual,  sophisticated  social  network  analysis  of  potential  terrorist  threats  to 
aid homeland security, and  telepresence  to  reduce  the greenhouse gases spent on commuting. 
Future applications will  increasingly  require processing on  large, heterogeneous data sets  (“Big 
Data”2),  using  distributed  designs,  working  within  form-factor  constraints,  and  reconciling  rapid 
deployment with efficient operation. 

Two  key—but  often  invisible—enablers  for  past  ICT  innovation  have  been  semiconductor 
technology and computer architecture. Semiconductor  innovation has repeatedly provided more 
transistors  (Moore’s  Law)  for  roughly  constant  power  and  cost  per  chip  (Dennard  Scaling). 
Computer  architects  took  these  rapid  transistor  budget  increases  and  discovered  innovative 
techniques to scale processor performance and mitigate memory system  losses. The combined 
effect of technology and architecture has provided ICT  innovators with exponential performance 
growth at near constant cost.  

Because most technology and computer architecture  innovations were (intentionally)  invisible to 
higher layers, application and other software developers could reap the benefits of this progress 
without  engaging  in  it.  Higher  performance  has  both  made  more  computationally  demanding 
applications  feasible  (e.g.,  virtual  assistants,  computer  vision)  and  made  less  demanding 
applications easier to develop by enabling higher-level programming abstractions (e.g., scripting 
languages  and  reusable  components).  Improvements  in  computer  system  cost-effectiveness 
enabled  value  creation  that  could  never  have  been  imagined  by  the  field’s  founders  (e.g., 
distributed web search sufficiently inexpensive so as to be covered by advertising links). 

                                                
1 PCAST, “Designing a Digital Future: Federally Funded Research and Development Networking and Information 
Technology, Dec. 2010 (http://www.whitehouse.gov/sites/default/files/microsites/ostp/pcast-nitrd-report-2010.pdf). 
2 CCC, “Challenges and Opportunities with Big Data," Feb. 2012 (http://cra.org/ccc/docs/init/bigdatawhitepaper.pdf). 

1 

The  wide  benefits  of  computer  performance  growth  are  clear.  Recently,  Danowitz  et  al.3 
apportioned  computer  performance  growth 
technology  and 
roughly  equally  between 
architecture,  with  architecture  credited  with  ~80×  improvement  since  1985.  As  semiconductor 
technology approaches its “end-of-the-road” (see below), computer architecture will need to play 
an increasing role in enabling future ICT innovation.  But instead of asking, “How can I make my 
chip run faster?,” architects must now ask, “How can I enable the 21st century infrastructure, 
from  sensors  to  clouds,  adding  value  from  performance  to  privacy,  but  without  the 
benefit of near-perfect  technology scaling?”. The challenges are many, but with appropriate 
investment,  opportunities  abound.    Underlying  these  opportunities  is  a  common  theme  that 
future  architecture  innovations will  require  the  engagement  of  and  investments  from  innovators 
in other ICT layers. 

1.1 The Challenges: An Inflection Point 
The  semiconductor  technology  enabler  to  ICT  is  facing  serious  challenges  that  are  outlined  in 
Table  1  below.  First,  although  technologists  can  make  more  and  smaller  transistors  (Moore’s 
Law),  these  transistors  are  not  altogether  “better”  as  has  been  true  for  four  decades.  Second, 
the  power  per  transistor  is  no  longer  scaling  well  (Dennard  Scaling  has  ended).  Since  most 
products—sensors,  mobile,  client,  and  data  center—cannot 
tolerate  (repeated)  power 
increases,  we  must  consider  ways  to  mitigate  these  increases.  Third,  fabrication  variations  of 
nano-scale  features  (e.g.,  gate  oxides  only  atoms  thick)  reduce  transistors’  long-term  reliability 
significantly  compared  to  larger  feature  sizes.  Fourth,  communication  among  computation 
elements must be managed through locality to achieve goals at acceptable cost and energy with 
new  opportunities  (e.g.,  chip  stacking)  and  new  challenges  (e.g.,  data  centers).  Fifth,  one-time 
costs  to  design,  verify,  fabricate,  and  test  are  growing,  making  them  harder  to  amortize, 
especially  when  seeking  high  efficiency  through  platform  specialization  (e.g.,  handhelds, 
laptops, or servers). 

Table 1: Technology's Challenges to Computer Architecture 

Late 20th Century 

The New Reality 

Moore’s Law — 2× transistors/chip every 
18-24 months 

Transistor count still 2× every 18-24 months, 
but see below 

Dennard Scaling — near-constant 
power/chip 

Gone. Not viable for power/chip to double (with 2× 
transistors/chip growth) 

The modest levels of transistor 
unreliability easily hidden (e.g., via ECC)  

Transistor reliability worsening, no longer easy to hide  

Focus on computation over 
communication   

Restricted inter-chip, inter-device, inter-machine 
communication (e.g. Rent's Rule, 3G, GigE); 
communication more expensive than computation 

One-time 
engineering) 
(non-recurring 
costs  growing,  but  amortizable  for  mass-
market parts 

Expensive  to  design,  verify,  fabricate,  and  test,  especially 
for specialized-market platforms 

 

                                                
3 Danowitz, et al., “CPU DB: Recording Microprocessor History”, CACM 04/2012. 

2 

1.2 The Opportunities: 21st Century Computer Architecture 

With CMOS technology scaling weakening as an enabler of ICT innovation, computer architects 
must  step  up  their  role  even  further.  21st  century  computer  architecture,  however,  needs  to  be 
different  from  its 20th century predecessor  to embrace  this new  role. We see  three  fundamental 
differences,  highlighted  in  Table  2  below.  These  differences  form  the  basis  of  the  future 
research agenda described in Section 2. 

Table 2: Computer Architecture’s Past and Future 
21st Century Architecture 
20th Century Architecture 

 

Single-chip performance 

Architecture as infrastructure:  
from sensors to clouds 
●  Chips to systems 
●  Performance plus security, privacy, 
availability, programmability, … 

Performance through software-
invisible instruction level parallelism 
(ILP) 

Energy first 
●  Parallelism 
●  Specialization 
●  Cross-layer design 

 
 
 
 
 
Cross-cutting 
implication: 
 
Break current 
layers with new 
interfaces 

Tried and tested technologies: 
CMOS, DRAM, disks with rapid but 
predictable improvements 

New technologies: non-volatile memory, 
near-threshold voltage operation, 3D chips, 
photonics, … 
Rethink 
●  Memory+storage 
●  Reliability 
●  Communication 
●  … 

Architecture  as  Infrastructure:  From  Sensors  to  Clouds:  Past  architecture  research  often 
focused  on  a  chip  (microprocessor)  or  stand-alone  computer  with  performance  as  its  main 
optimization  goal.  Moving  forward,  computers  will  be  a  key  pillar  of  the  21st  century  societal 
infrastructure.  To  address  this  change,  computer  architecture  research  must  expand  to 
recognize  that  generic  computers  have  been  replaced  by  computation  in  context  (e.g.,  sensor, 
mobile,  client,  data  center)  and  many  computer  systems  are  large  and  geographically 
distributed.4 This  shift  requires more  emphasis  on  the  system  (e.g.,  communication  becomes  a 
full-fledged  partner  of  computation),  the  driver  application  (e.g.,  dealing  with  big  data),  and 
human-centric  design  goals  beyond  performance  (e.g.,  programmability,  privacy,  security, 
availability, battery life, form factor).  

Energy  First:    Past  computer  architecture  optimized  performance,  largely  through  software 
invisible  changes.  21st  century  architecture  confronts  power  and  energy  as  the  dominant 
constraints,  and  can  no  longer  sustain  the  luxury  of  software  invisible  innovation.  We  see 
parallelism, specialization, and cross-layer design as key principles in an energy-first era, but all 
three  require  addressing  significant  challenges.  For  example,  while  parallelism  will  abound  in 
future applications  (big data = big parallelism), communication energy will  outgrow computation 
                                                
4 Luiz André Barroso and Urs Hölzle, “The Datacenter as a Computer”, Morgan-Claypool, 2009 

3 

energy  and  will  require  rethinking  how  we  design  for  1,000-way  parallelism.  Specialization  can 
give  100×  higher  energy  efficiency  than  a  general-purpose  compute  or  memory  unit,  but  no 
known  solutions  exist  today  for  harnessing  its  benefits  for  broad  classes  of  applications  cost-
effectively.  Cross-layer  design  (from  circuit  to  architecture  to  run-time  system  to  compiler  to 
application)  can  wring  out  waste  in  the  different  layers  for  energy  efficiency,  but  needs  a  far 
more concerted effort among many layers to be practical. 

New  Technologies:  Past  computer  architecture  has  relied  on  the  predictable  performance 
improvements of stable technologies such as CMOS, DRAM, and disks. For  the first time  in the 
careers of many ICT professionals, new technologies are emerging to challenge the dominance 
of  the  “tried-and-tested,”  but  sorely  need  architectural  innovation  to  realize  their  full  potential. 
For example, non-volatile memory technologies (e.g., Flash and phase change memory) drive a 
rethinking  of  the  relationship  between  memory  and  storage.  Near-threshold  voltage  operation 
has tremendous potential to reduce power but at the cost of reliability, driving a new discipline of 
resiliency-centered  design.  Photonics  and  3D  chip  stacking  change  communication  costs 
radically enough to affect the entire system design.  

Underlying  all  of  the  above  is  a  cross-cutting  theme  of  innovations  that  are  exposed  to  and 
require  interaction  with  other  ICT  layers.  This  change  is  dramatic  in  that  it  will  impact  ICT 
innovators  in  other  layers,  similar  to,  but  potentially  greater  than,  the  recent  shift  to  multicore 
processors.  Collaboration  with  other-layer  innovators  will  empower  architects  to  make  bolder 
innovations with commensurate benefits, but it will also require significant investment and strong 
leadership to provide the richer inter-layer interfaces necessary for the 21st century.  

2. Research Directions 

This  section  discusses  important  research  directions  for  computer  architecture  and  related 
communities,  but  begins  with  two  comments.  First,  the  ideas  below  represent  good  ideas  from 
those who  contributed  to  this  document  and  complement  other  recent  documents.5  6  7 They  do 
not  represent  an  exhaustive  list  nor  a  consensus  opinion  of  the  whole  community,  which  are 
infeasible to gather in the process of creating this white paper. 

Second, if we have been convincing that the directions presented have value to society, there is 
a  need  for  pre-competitive  research  funding  to  develop  them. Even  highly-successful  computer 
companies  lack  the  incentive  to  do  this  work  for  several  reasons.  First,  the  technologies 
required  will  take  years  to  a  decade  to  develop.  Few  companies  have  such  staying  power. 
Second,  successful  work  will  benefit  many  companies,  disincentivizing  any  one  to  pay  for  it. 
Third,  many  of  the  approaches  we  advocate  cross  layers  of  the  system  stack,  transcending 
industry-standard  interfaces  (e.g.,  x86)  and  the  expertise  of  individual  companies.  Finally,  we 
need  to  educate  the  next  generation  of  technical  contributors,  which  is  perhaps  academia’s 
most important form of technology transfer.  

                                                
5 ACAR-1, "Failure is not an Option: Popular Parallel Programming," Workshop on Advancing Computer  
Architecture Research, August 2010 (http://www.cra.org/ccc/docs/ACAR_Report_Popular-Parallel-Programming.pdf). 
6 ACAR-2, "Laying a New Foundation for IT: Computer Architecture for 2025 and Beyond," Workshop on Advancing 
Computer Architecture Research, September 2010 (http://www.cra.org/ccc/docs/ACAR2-Report.pdf). 
7 Fuller and Millett, "The Future of Computing Performance: Game Over or Next Level?," The National Academy 
Press, 2011 (http://books.nap.edu/openbook.php?record_id=12980&page=R1). 

4 

2.1. Architecture as Infrastructure: Spanning Sensors to Clouds 

Until  recently,  computer systems were  largely  beige  boxes  nestled  under a  desk  or  hidden  in  a 
machine  room, and computer architecture  research  focused primarily on  the design of desktop, 
workstation  and  server  CPUs.  The  new  reality  of  21st-century  applications  calls  for  a  broader 
computer  architecture  agenda  beyond  the  beige  box.  The  emerging  applications  described  in 
Appendix  A  demand  a  rich  ecosystem  that  enables  ubiquitous  embedded  sensing/compute 
devices that feed data to “cloud servers” and warehouse-scale facilities, which can process and 
supply information to edge devices, such as tablets and smartphones.  Each class of computing 
device  represents  a  unique  computing  environment  with  a  specific  set  of  challenges  and 
opportunities, yet all three share a driving need for improved energy efficiency (performance per 
watt)  as  an  engine  for  innovation.  Moreover,  future  architecture  research  must  go  beyond 
optimizing  devices  in  isolation,  and  embrace  the  challenges  of  cross-environment  co-design  to 
address the needs of emerging applications.  

Smart  Sensing  and  Computing.  In  the  smart  sensors  space,  the  central  requirement  is  to 
compute  within  very  tight  energy,  form-factor,  and  cost  constraints.    The  need  for  greater 
computational  capability  is  driven  by  the  importance  of  filtering  and  processing  data where  it  is 
generated/collected  (e.g.,  distinguishing  a  nominal  biometric  signal  from  an  anomaly),  because 
the energy required to communicate data often outweighs that of computation. This environment 
brings  exciting  new  opportunities  like  designing  systems  that  can  leverage  intermittent  power 
(e.g.,  from  harvested  energy),  extreme  low-voltage  (near-threshold  and  analog)  design,  new 
communication  modalities  (e.g.,  broadcast  communication  from  building  lighting),  and  new 
storage  technologies  (e.g.,  NVRAM).    As  sensors  become  critical  to  health  and  safety,  their 
security  and  reliability  must  also  be  assured  (e.g.,  consider  recent  demonstrations  of  remote 
hacking  of  pacemakers),  including  design  correctness  of  hardware  and  software  components. 
Additionally, given that sensor data is inherently approximate, it opens the potential to effectively 
apply  approximate  computing  techniques,  which  can  lead  to  significant  energy  savings  (and 
complexity reduction). 

Portable  Edge  Devices.  The  portable  computing  market  has  seen  explosive  growth,  with 
smartphone  sales  recently  eclipsing  the PC market.8   And  yet,  current  devices  still  fall  far  short 
of  the  futuristic  capabilities  society  has  envisioned,  from  the  augmented  reality  recently 
suggested  by  “Google  Glasses,”  to  the  medical  tricorder  posited  by  Star  Trek  nearly  50  years 
ago  and  recently  resurrected  in  the  form  of  an  X  Prize  challenge  competition9.  Enriching 
applications  in  this  environment  will  need  orders  of  magnitude  improvement  in  operations/watt 
(from  today’s  ~10  giga-operations/watt),  since  user  interfaces  seem  to  have  a  significant 
appetite  for  computation  (e.g.,  multi-touch  interfaces,  voice  recognition,  graphics,  holography, 
and  3D  environment  reconstruction)  and  even  mobile  applications  are  becoming  data  and 
compute  intensive.  As  discussed  in  later  sections,  such  systems  clearly  need  both  parallelism 
and specialization (the latest generation iPad’s key chip has multiple cores and dedicates half of 
its  chip  area  for  specialized  units).  Given  the  proximity  with  the  user,  such  devices  motivate 
ideas  that  bring  human  factors  to  computer  design,  such  as  using  user  feedback  to  adjust 
voltage/frequency  to save  energy,  focusing  computation  on where  the  user  is  looking,  reducing 
the  image to salient features only, or predicting and prefetching for what  the user  is  likely  to do. 
This environment also motivates features beyond raw performance, such as security/privacy.  

                                                
8 e.g., http://mashable.com/2012/02/03/smartphone-sales-overtake-pcs/ and 
http://www.canalys.com/newsroom/smart-phones-overtake-client-pcs-2011  
9 http://www.xprize.org/x-prize-and-qualcomm-announce-10-million-tricorder-prize 

5 

The Infrastructure—Cloud Servers. Many of the most exciting emerging applications, such as 
simulation-driven  drug  discovery,  or  interactive  analysis  of  massive  human  networks,  can  only 
be achieved with  reasonable  response  times through  the combined efforts of tens of thousands 
of processors acting as a single warehouse-scale computer.   Internet search has demonstrated 
the  societal  importance  of  this  computing  paradigm.    However,  today’s  production  search 
systems  require  massive  engineering  effort  to  create,  program,  and  maintain,  and  they  only 
scratch  the  surface  of  what  might  be  possible  (e.g.,  consider  the  potential  of  IBM’s  Watson).  
Systems  architects  must  devise  programming  abstractions,  storage  systems,  middleware, 
operating  system,  and  virtualization  layers  to  make  it  possible  for  conventionally  trained 
software engineers to program warehouse-scale computers.  

While  many  computing  disciplines—operating  systems,  networking,  and  others—play  a  role  in 
data  center  innovations,  it  is  crucial  for  computer  architects  to  consider  the  interface  designs 
and  hardware  support  that  can  best  enable  higher-level  innovations.      In  addition,  a  key 
challenge lies in reasoning about locality and enforcing efficient locality properties in data center 
systems,  a  burden  which  coordination  of  smart  tools,  middleware  and  the  architecture  might 
alleviate.  A  further  challenge  lies  in  making  performance  predictable;  as  requests  are 
parallelized  over  more  systems,  infrequent  tail  latencies  become  performance  critical  (if  100 
systems must  jointly  respond  to a  request, 63% of  requests will  incur  the 99-percentile delay of 
the  individual  systems  due  to  waiting  for  stragglers10);  architectural  innovations  can  guarantee 
strict  worst-case  latency  requirements.  Memory  and  storage  systems  consume  an  increasing 
fraction  of  the  total  data  center  power  budget,  which  one  might  combat  with  new  interfaces 
(beyond the JEDEC standards), novel storage technologies, and 3D stacking of processors and 
memories. 

Putting It All Together—Eco-System Architecture.  There is a need for runtime platforms and 
virtualization  tools  that  allow  programs  to  divide  effort  between  the  portable  platform  and  the 
cloud  while  responding  dynamically  to  changes  in  the  reliability  and  energy  efficiency  of  the 
cloud uplink. How should computation be split between the nodes and cloud infrastructure? How 
can  security  properties  be  enforced  efficiently  across  all  environments?  How  can  system 
architecture  help  preserve  privacy  by  giving  users more  control  over  their  data? Should we  co-
design compute engines and memory systems? 

The  research  directions  outlined  above  will  push  architecture  research  far  beyond  the  beige 
box.  The  basic  research  challenges  that  relate  all  of  these  opportunities  are  improving  energy 
efficiency  dramatically  and  embracing  new  requirements  such  as  programmability,  security, 
privacy  and  resiliency  from  the  ground  up.  Given  the  momentum  in  other  areas  (e.g.,  HCI, 
machine  learning,  and  ubiquitous  computing)  now  is  the moment  to  explore  them. Success will 
require  significant  investment  in  academic  research  because  of  the  need  for  community-scale 
effort  and  significant  infrastructure.  While  mobile  and  data  centers  are  relatively  new  to  the 
menagerie  of  computing  devices,    the  research  challenges  we  discuss  will  likely  also  apply  to 
other devices yet to emerge.  

2.2. Energy First 

The shift from sequential to parallel (multicore) systems has helped  increase performance while 
keeping  the  power  dissipated  per  chip  largely  constant.  Yet  many  current  parallel  computing 
systems  are  already  power  or  energy  constrained.  At  one  extreme,  high-end  supercomputers 
and  data  centers  require  expensive  many-megawatt  power  budgets;  at  the  other,  high-

                                                
10 J. Dean. “Achieving Rapid Response Times in Large Online Services.” Talk in Berkeley, CA, Mar. 2012.  

6 

functionality  sensors  and  portable  devices  are  often  limited  by  their  battery’s  energy  capacity. 
Portable  and  sensor  devices  typically  require  high  performance  for  short  periods  followed  by 
relatively  long  idle  periods.    Such  bimodal  usage  is  not  typical  in  high-end  servers,  which  are 
rarely  completely  idle  and  seldom  need  to  operate  at  their  maximum  rate.    Thus,  power  and 
energy solutions  in  the server space are  likely  to differ from  those  that work best  in the portable 
device  space.    However,  as  we  demand more  from  our  computing  systems—both  servers  and 
sensors—more  of  them will  be  limited  by  power,  energy,  and  thermal  constraints. W ithout  new 
approaches to power- and energy-efficient designs and new packaging and cooling approaches, 
producing  ICT  systems  capable  of  meeting  the  computing,  storage  and  communication 
demands  of  the  emerging  applications  described  in  Appendix  A  will  likely  be  impossible.  It  is 
therefore urgent to invest in research to make computer systems much more energy efficient. 

As  the  next  subsections  describe,  energy must  be  reduced  by  attacking  it  across many  layers, 
rethinking parallelism, and with effective use of specialization. 

Energy Across the Layers 

Electronic  devices  consume  energy  as  they  do work  (and,  in  the  case  of  CMOS,  just  by  being 
powered  on).  Consequently,  all  of  the  layers  of  the  computing  stack  play  a  role  to  improve 
energy  and  power  efficiency:  device  technologies,  architectures,  software  systems  ( including 
compilers), and applications. Therefore, we believe  that a major  interdisciplinary  research effort 
will  be  needed  to  substantially  improve  ICT  system  energy  efficiency. We  suggest  as  a  goal  to 
improve  the  energy  efficiency  of  computers  by  two-to-three  orders  of  magnitude,  to  obtain,  by 
the  end  of  this  decade,  an  exa-op  data  center  that  consumes  no  more  than  10  megawatts 
(MW), a peta-op departmental server  that consumes no more than 10 kilowatts  (KW), a tera-op 
portable  device  that  consumes  no  more  than  10  watts  (W),  and  a  giga-op  sensor  system  that 
consumes  no more  than  10 milliwatts  (mW).  Such  an  ambitious  plan  can  only  be  attained  with 
aggressive changes in all layers of the computing stack.  

At  the  Circuit/Technology  Level,  we  need  research  in  new  devices  that  are  fundamentally 
more  energy  efficient,  both  in  CMOS  and  in  emerging  device  technologies.  Research  is  also 
needed on new technologies that can improve the energy efficiency of certain functions, such as 
photonics  for  communication,  3D-stacking  for  integration,  non-resistive  memories  for  storage, 
and  efficient  voltage  conversion.  Novel  circuit  designs  are  also  needed:  circuits  that  work  at 
ultra-low supply  voltages,  circuits  that carry out efficient power distribution, circuits  that perform 
aggressive  power  management,  and  circuits  that  are  able  to  support  multiple  voltage  and 
frequency domains on chip. 

At the Architecture Level, we need to find more efficient, streamlined many-core architectures. 
We  need  chip  organizations  that  are  structured  in  heterogeneous  clusters,  with  simple 
computational  cores  and  custom,  high-performance  functional  units  that  work  together  in 
concert. We  need  research  on  how  to  minimize  communication,  since  energy  is  largely  spent 
moving data. Especially  in portable and sensor systems,  it  is often worth doing  the computation 
locally  to  reduce  the  energy-expensive  communication  load.  As  a  result,  we  also  need  more 
research on synchronization support, energy-efficient communication, and in-place computation. 

At  the  Software  Level,  we  need  research  that  minimizes  unnecessary  communication.  We 
require runtimes that manage the memory hierarchy and orchestrate fine-grain multitasking. We 
also need  research on compilation systems and  tools  that manage and enhance  locality. At  the 
programming-model level, we need environments that allow expert programmers to exercise full 
machine control, while presenting a simple model of localities to low-skilled programmers. At the 

7 

application  level,  we  need  algorithmic  approaches  that  are  energy-efficient  via  reduced 
operation  count,  precision, memory  accesses,  and  interprocessor  communication,  and  that can 
take advantage of heterogeneous systems.  At the compiler level, we need to find ways to trade 
off  power  efficiency  and  performance,  while  also  considering  the  reliability  of  the  resulting 
binary. Overall,  it  is  only  through  a  fully-integrated  effort  that  cuts  across  all  layers  that we  can 
make revolutionary progress. 

Exploiting Parallelism to Enable Future Applications 
Throughout  the  20th  century,  the  growth  in  computing  performance  was  driven  primarily  by 
single-processor  improvements.  But  by  2004,  diminishing  returns  from  faster  clock  speeds  and 
increased transistor counts resulted in serious power (and related thermal) problems. By shift ing 
to  multiple  cores  per  chip,  our  industry  continued  to  improve  aggregate  performance  and 
performance  per  watt.  But  just  replicating  cores  does  not  adequately  address  the  energy  and 
scaling challenges on chip, nor does  it take advantage of the unique features and constraints of 
being  on  chip.    Future  growth  in  computer  performance  must  come  from  massive  on-chip 
parallelism  with  simpler,  low-power  cores,  architected  to  match  the  kinds  of  fine-grained 
parallelism available in emerging applications. 

Unfortunately,  we  are  far  from  making  parallel  architectures  and  programming  usable  for  the 
large  majority  of  users.  Much  of  the  prior  research  has  focused  on  coarse-grained  parallelism 
using  standard  processor  cores  as  the  building  block.    Placing massive  parallelism  on  a  single 
chip  offers  new  opportunities  for  parallel  architecture  and  associated  programming  techniques. 
To unlock the potential of parallel computing in a widely-applicable form, we may need at least a 
decade  of  concerted,  broad-based  research  to  address  emerging  application  problems  at  all 
levels  of  parallelism.  To  ensure  that  computing  performance  growth  will  continue  to  fuel  new 
innovations,  and  given  the  magnitude  of  the  technical  challenges  and  the  stakes  involved,  we 
need major funding investments in this area. 

Reinventing Computing Stack for Parallelism: We recommend an ambitious, interdisciplinary 
effort  to  re-invent  the  classical  computing  stack  for  parallelism—programming  language, 
compiler  and  programming  tools,  runtime,  virtual  machine,  operating  system,  and  architecture. 
Along with  performance  goals,  energy  considerations must  be  a  first-class  design  constraint  to 
forge  a  viable  path  to  scalable  future  systems. The  current  layers  have  been  optimized  for  uni -
processor systems and act as barriers  to change. Since a single, universal programming model 
may  not  exist, we  recommend  exploring multiple models  and  architectures. Moreover,  different 
solutions are clearly needed  for experts, who may  interact with  the  innards of the machine, and 
the  large  majority  of  programmers,  who  should  use  simple,  sequential-like  models  perhaps 
enabled by domain-specific languages.  

Applications-Focused  Architecture  Research:  We  recommend  an  application-focused 
approach to parallel architecture research, starting from chips with few to potentially hundreds of 
cores,  to  distributed  systems,  networking  structures  at  different  scales,  parallel  memory 
systems,  and  I/O  solutions. The  challenge  is  to  consider  application  characteristics,  but without 
overfitting  to  particular  specifics;  we  must  leverage  mechanisms  (including  programming 
languages)  that  will  perform  well  and  are  power  efficient  on  a  variety  of  parallel  systems. 
Fundamental  architecture  questions  include  the  types  of  parallelism  (e.g.,  data  or  task),  how 
units  should  be  organized  (e.g.,  independent  cores  or  co-processors),  and  how  to  synchronize 
and  communicate.    Solutions  may  differ  depending  on  the  inherent  application  parallelism  and 
constraints on power, performance, or system size. 

8 

Hardware/Software Co-Design: Current challenges call  for  integrative  research on parallelism 
that  spans  both  software  and  hardware,  from  applications  and  algorithms  through  systems 
software  and  hardware.  As  discussed  below,  we  see  a  need  to  break  through  existing 
abstractions  to  find  novel  mechanisms  and  policies  to  exploit  locality  and  enable  concurrency, 
effectively  support  synchronization,  communication  and  scheduling,  provide  platforms  that  are 
programmable,  high-performance,  and  energy  efficient,  and  invent  parallel  programming 
models,  frameworks,  and  systems  that  are  truly  easy  to  use.  Given  the  large  range  of  issues, 
we recommend a broad and inclusive research agenda. 

Enabling Specialization for Performance and Energy Efficiency 

For  the past  two or more decades, general-purpose computers have driven  the  rapid advances 
in  and  society’s  adoption  of  computing.  Yet  the  same  flexibility  that  makes  general-purpose 
computers applicable to most problems causes them to be energy inefficient for many emerging 
tasks.  Special-purpose  hardware  accelerators,  customized  to  a  single  or  narrow-class  of 
functions,  can  be  orders  of  magnitude  more  energy-efficient  by  stripping  out  the  layers  of 
mechanisms and abstractions that provide flexibility and generality. But current success stories, 
from  medical  devices  and  sensor  arrays  to  graphics  processing  units  (GPUs),  are  limited  to 
accelerating  narrow  classes  of  problems. Research  is  needed  to  (1)  develop  architectures  that 
exploit  both  the  performance  and  energy-efficiency  of specialization while  broadening  the  class 
of  applicable  problems  and  (2)  reduce  the  non-recurring  engineering  (NRE)  costs  for  software 
and hardware that limit the utility of customized solutions. 

Higher-level  Abstractions  to  Enable  Specialization.  General-purpose  computers  can  be 
programmed  in a range of higher-level  languages with sophisticated tool chains that translate to 
fixed 
functional  and  performance  portability  across  a  wide  range  of 
ISAs,  providing 
architectures.    Special-purpose  accelerators,  in  contrast,  are  frequently  programmed  using 
much  lower-level  languages  that often directly map  to hardware  (e.g., Verilog), providing  limited 
functional  or  performance  portability  and  high  NRE  costs  for  software.  As  further  discussed 
below,  research  is  needed  to  develop  new  layers  and  abstractions  that  capture  enough  of  a 
computation’s  structure  to  enable  efficient  creation  of  or mapping  to  special-purpose  hardware, 
without placing undue burden on  the programmer. Such systems will enable  rapid development 
of  accelerators  by  reducing  or  eliminating  the  need  to  retarget  applications  to  every  new 
hardware platform. 

Energy-Efficient  Memory  Hierarchies.  Memory  hierarchies  can  both  improve  performance 
and  reduce  memory  system  energy  demands,  but  are  usually  optimized  for  performance  first. 
But  fetching  the  operands  for  a  floating-point  multiply-add  can  consume  one  to  two  orders  of 
magnitude  more  energy  than  performing  the  operation.11  Moreover,  current  designs  frequently 
either  seek 
to  minimize  worst-case  performance 
to  maximize  generality  or  sacrifice 
programmability  to  maximize  best-case  performance.  Future  memory-systems  must  seek 
energy  efficiency  through  specialization  (e.g.,  through  compression  and  support  for  streaming 
data)  while  simplifying  programmability  (e.g.,  by  extending  coherence  and  virtual  memory  to 
accelerators  when  needed).  Such  mechanisms  have  the  potential  to  reduce  energy  demands 
for  a  broad  range  of  systems,  from  always-on  smart  sensors  to  data  centers  processing  big 
data. 

Exploiting  (Re-)configurable  Logic  Structures.  The  increasing  complexity  of  silicon  process 
technologies  has  driven  NRE  costs  to  prohibitive  levels,  making  full-custom  accelerators 

                                                
11 Steve Keckler, "Life After Dennard and How I Learned to Love the Picojoule,” Keynote at Micro 2011.  

9 

infeasible  for  all  but  the  highest-volume  applications.  Current  reconfigurable  logic  platforms 
(e.g.,  FPGAs)  drive  down  these  fixed  costs,  but  incur  undesirable  energy  and  performance 
overheads  due  to  their  fine-grain  reconfigurability  (e.g.,  lookup  tables  and  switch  boxes). 
Research  in  future  accelerators  will  improve  energy  efficiency  using  coarser-grain  semi-
programmable  building  blocks 
(reducing 
internal 
inefficiencies)  and  packet-based 
interconnection (making more efficient use of expensive wires). Additional efficiencies will come 
from  emerging  3D  technologies  such  as  silicon  interposers,  which  allow  limited  customizat ion 
(e.g.,  top-level  interconnect)  to  configure  systems  at  moderate  cost.  Such  techniques  coupled 
with  better  synthesis  tools  can  reduce  NRE  costs,  thereby  enabling  rapid  development  and 
deployment of accelerators in a broad range of critical applications. 
 
The  past  scaling  of  processor  performance  has  driven  advances  both  within  computing  and  in 
society at large. Continued scaling of performance will be largely limited by the improvements in 
energy  efficiency  made  possible  by  reconfigurable  and  specialized  hardware.  Research  that 
facilitates the design of reconfigurable and special-purpose processors will enable corporations, 
researchers,  and  governments  to  quickly  and  affordably  focus  enormous  computing  power  on 
critical problems. 

2.3. Technology Impacts on Architecture 

Application  demands  for  improved  performance,  power  and  energy  efficiency,  and  reliability 
drive  continued 
investment 
in 
technology  development.  As  standard  CMOS  reaches 
fundamental  scaling  limits,  the  search  continues  for  replacement  circuit  technologies  (e.g., 
sub/near-threshold  CMOS,  QWFETs,  TFETs,  and  QCAs)  that  have  a  winning  combination  of 
density,  speed,  power  consumption,  and  reliability.      Non-volatile  storage  (i.e.,  flash  memory) 
has  already  starting  to  replace  rotating  disks  in  many  ICT  systems,  but  comes  with  its  own 
design  challenges  (e.g.,  limited  write  endurance).    Other  emerging  non-volatile  storage 
technologies  (e.g.,  STT-RAM,  PCRAM,  and  memristor)  promise  to  disrupt  the  current  design 
dichotomy  between  volatile  memory  and  non-volatile,  long-term  storage.    3D  integration  uses 
die  stacking  to  permit  scaling  in  a  new  dimension,  but  substantial  technology  and  electronic 
design automation (EDA) challenges remain. Photonic interconnects can be exploited among or 
even on chips. 

In  addition  to  existing  efforts  to  develop  new  ICT  technologies  (e.g.,  through  NSF’s MRSECs), 
significant architectural advancements—and thus significant investments—are needed to exploit 
these technologies.  

Rethinking  the  Memory/Storage  Stack.  Emerging  technologies  provide  new  opportunities  to 
address  the  massive  online  storage  and  processing  requirements  of  “big  data”  applications. 
Emerging  non-volatile  memory  technologies  promise  much  greater  storage  density  and  power 
efficiency,  yet  require  re-architecting  memory  and  storage  systems  to  address  the  device 
capabilities (e.g., longer, asymmetric, or variable latency, as well as device wear out).  

Design  Automation  Challenges.    New  technologies  drive  new  designs  in  circuits,  functional 
units, microarchitectures,  and  systems.    Such  new  approaches  also  require  investment  in  new 
EDA  tools,  particularly  for mixed-signal  and  3D  designs.    New  design  tools must  be  tailored  to 
the  new  technologies:    functional  synthesis,  logic  synthesis,  layout  tools,  and  so  on.  
Furthermore,  heterogeneous  computing  greatly  taxes  our  ability  to  perform  pre-RTL  system 
modeling,  particularly  as  the  diversity  of  architectures  and  accelerators  explodes.  Hence,  new 
verification,  analysis  and  simulation  tools  will  be  needed:    tools  for  verifying  correct  operation, 
performance,  power  and  energy  consumption,  reliability  (e.g.,  susceptibility  to  soft  error  and 

10 

aging  effects),  and  security  (avoiding  power  “footprints,”  providing  architectural  support  for 
information flow tracking).   

3D  Integration. Die  stacking  promises  lower  latency,  higher  bandwidth,  and  other  benefits,  but 
brings  many  new  EDA,  design,  and  technology  challenges.    Computer  architects  have  started 
this investigation with the stacking of DRAM memories on top of cores, but future designs will go 
much  further  to  encompass  stacking  of  nonvolatile  memories,  of  custom  computational 
components  realized  with  a  non-compatible  technology,  of  new  interconnection  technologies 
(e.g.,  photonics),  of  sensors  and  the  analog  components  that  go  with  them,  of  RF  and  other 
analog  components,  of  energy  providers  and  cooling  systems  (e.g.,  MEMs  energy  harvesting 
devices,  solar  cells,  and  microfluidic  cooling).    To  realize  the  promise  of  the  new  technologies 
being  developed  by  nano-materials  and  nano-structures  researchers,  further  investment  is 
needed so that computer architects can turn these nanotechnology circuits into ICT systems. 

2.4. Cross-Cutting Issues and Interfaces 

As computers permeate more aspects of everyday life, making a computer system better means 
much more than being faster or more energy efficient. Applications need architectural support to 
ensure  data  security  and  privacy,  to  tolerate  faults  from  increasingly  unreliable  transistors,  and 
to  enhance  programmability,  verifiability  and  portability.  Achieving  these  cross-cutting  design 
goals—nicknamed  the  “Ilities”—requires  a  fundamental  rethinking  of  long-stable  interfaces  that 
were defined under extremely different application requirements and technological constraints. 

Security, Programmability, Reliability, Verifiability and Beyond.  

Applications  increasingly  demand  a  richer  set  of  design  “Ilities”  at  the  same  time  that  energy 
constraints  make  them  more  expensive  to  provide.  Fortunately,  the  confluence  of  new  system 
architectures  and  new  technologies  creates  a  rare  inflection  point,  opening  the  door  to  allow 
architects to develop fundamentally more energy-efficient support for these design goals. 

Verifiability  and  Reliability.  Ensuring  that  hardware  and  software  operate  reliably  is  more 
important  than  ever;  for  implantable  medical  devices,  it  is  (literally)  vital.  At  the  same  time, 
CMOS  scaling  trends  lead  to  less-reliable  circuits  and  complex,  heterogeneous  architectures 
threaten  to  create  a  “Verification  Wall”.    Future  system  architectures  must  be  designed  to 
facilitate  hardware  and  software  verification;  for  example,  using  co-processors  to  check  end-to-
end  software  invariants.    Current  highly-redundant  approaches  are  not  energy  efficient;  we 
recommend  research  in  lower-overhead  approaches  that  employ  dynamic  (hardware)  checking 
of  invariants  supplied  by  software.    In  general,  we  must  architect  ways  of  continuously 
monitoring  system  health—both  hardware  and  software—and  applying  contingency  actions. 
Finally,  for  mission-critical  scenarios  (including  medical  devices),  architects  must  rethink 
designs to allow for failsafe operation. 

Security  and  Privacy.  Architectural  support  for  security  dates  back  decades,  to  paging, 
segmentation  and  protection  rings.    Recent  extensions  help  to  prevent  buffer  overflow  attacks, 
accelerate  cryptographic  operations,  and  isolate virtual machines. However,  it  is  time  to  rethink 
security and privacy from the ground up and define architectural interfaces that enable hardware 
to  act  as  the  “root  of  trust”,  efficiently  supporting  secure  services.  Such  services  include 
information  flow  tracking  (reducing  side-channel  attacks)  and  efficient  enforcement  of  richer 
information  access  rules  (increasing  privacy).  Support  for  tamper-proof  memory  and  copy-
protection  are  likewise  crucial  topics.    Finally,  since  security  and  privacy  are  intrinsically 
connected to reliable/correct operation, research in these areas dovetails well. 

11 

  
Improving  Programmability.  Programmability  refers  to  all  aspects  of  producing  software  that 
reaches  targets  for  performance,  power,  reliability,  and  security,  with  reasonable  levels  of 
design and maintenance effort. The past decades have largely focused on software engineering 
techniques—such as modularity and  information hiding—to  improve programmer productivity at 
the  expense  of  performance  and  power.12  As  energy  efficiency  and  other  goals  become  more 
important,  we  need  new  techniques  that  cut  across  the  layers  of  abstraction  to  eliminate 
unnecessary inefficiencies.  

Existing  efforts  to  improve  programmability,  including  domain-specific  languages,  dynamic 
scripting  languages,  such  as  Python  and  Javascript,  and  others,  are  pieces  of  the  puzzle.  
Facebook’s HipHop, which dynamically compiles programs written  in scripting  language, shows 
how  efficiency  can  be  reclaimed  despite  such  abstraction  layers.  In  addition  to  software  that 
cuts  through  abstraction  layers,    we  recommend  cross-cutting  research  in  hardware  support  to 
improve  programmability.  Transactional  memory  (TM)  is  a  recent  example  that  seeks  to 
significantly simplify parallelization and synchronization  in multithreaded code. TM research has 
spanned  all  levels  of  the  system  stack,  and  is  now  entering  the  commercial  mainstream.  
Additional  research  is  required  on  topics  like  software  debugging,  performance  bottleneck 
analysis, resource management and profiling, communication management, and so on.   

Managing the interactions between applications also present challenges.  For example, how can 
applications express Quality-of-Service targets and have the underlying hardware, the operating 
system and the virtualization layers work together to ensure them?  Increasing virtualization and 
introspection  support  requires  coordinated  resource  management  across  all  aspects  of  the 
hardware  and  software  stack,  including  computational  resources,  interconnect,  and  memory 
bandwidth. 

Crosscutting Interfaces 

Current  computer  architectures  define  a  set  of  interfaces  that  have  evolved  slowly  for  several 
decades.  These  interfaces—e.g.,  the  Instruction  Set  Architecture  and  virtual  memory—were 
defined  when  memory  was  at  a  premium,  power  was  abundant,  software  infrastructures  were 
limited,  and  there  was  little  concern  for  security.    Having  stable  interfaces  has  helped  foster 
decades  of  evolutionary  architectural  innovations.  We  are  now,  however,  at  a  technology 
crossroads, and these stable interfaces are a hindrance to many of the innovations discussed in 
this document.  
  
Better  Interfaces  for  High-Level  Information. Current  ISAs  fail  to  provide  an  efficient means 
of  capturing  software-intent  or  conveying  critical  high-level  information  to  the  hardware.  For 
example,  they  have  no  way  of  specifying  when  a  program  requires  energy  efficiency,  robust 
security, or a desired Quality of Service (QoS) level. Instead, current hardware must try  to glean 
some  of  this  information  on  its  own—such  as  instruction-level  parallelism  or  repeated  branch 
outcome  sequences—at  great  energy  expense.  New,  higher-level  interfaces  are  needed  to 
encapsulate  and  convey  programmer  and  compiler  knowledge  to  the  hardware,  resulting  in 
major efficiency gains and valuable new functionality.  
  
Better  Interfaces  for  Parallelism.  Developing  and  supporting  parallel  codes  is  a  difficult  task. 
Programmers  are  plagued  by  synchronization  subtleties,  deadlocks,  arbitrary  side  ef fects,  load 
imbalance and unpredictable communication, unnecessary non-determinism, confusing memory 
models,  and  performance  opaqueness.  We  need  interfaces  that  allow  the  programmer  to 
                                                
12 James Larus, Spending Moore’s Dividend, Communications of the ACM, May 2009 5(52). 

12 

express  parallelism  at  a  higher  level,  expressing  localities,  computation  dependences  and  side 
effects,  and  the  key  sharing  and  communication  patterns.  Such  an  interface  could  enable 
simpler  and  more  efficient  hardware,  with  efficient  communication  and  synchronization 
primitives that minimize data movement. 
  
Better  Interfaces  for  Abstracting  Heterogeneity.  Supporting  heterogeneous  parallelism 
demands  new  interfaces.  From  a  software  perspective,  applications  must  be  programmed  for 
several  different  parallelism  and  memory  usage  models;  and  they  must  be  portable  across 
different  combinations  of  heterogeneous  hardware.  From  a  hardware  perspective,  we  need  to 
design  specialized  compute  and  memory  subsystems. We  therefore  need  hardware  interfaces 
that  can  abstract  the  key  computation  and  communication  elements  of  these  hardware 
possibilities. Such  interfaces need  to be at a high enough  level  to serve as a  target for portable 
software and at a low-enough level to efficiently translate to a variety of hardware innovations.  
  
Better Interfaces for Orchestrating Communication. Traditional computers and programming 
models  have  focused  heavily  on  orchestrating  computation,  but  increasingly  it  is  data 
communication  that must  be  orchestrated  and  optimized. We  need  interfaces  that more  clearly 
identify  long-term data and program dependence  relationships, allowing hardware and software 
schedulers to dynamically identify critical paths through the code.  W ithout the ability to analyze, 
orchestrate,  and  optimize  communication,  one  cannot  adhere  to  performance,  energy  or  QoS 
targets.  Data management  becomes  even more  complex  when  considering  big-data  scenarios 
involving  data  orchestration  between  many  large  systems.    Current  systems  lack  appropriate 
hardware-software abstractions for describing communication relationships. 
  
Better Interfaces for Security and Reliability.  Existing protection and reliability models do not 
address  current  application  needs.  We  need  interfaces  to  specify  fine-grain  protection 
boundaries among modules within a single application,  to treat security as a first class prope rty, 
and  to  specify  application  resilience  needs  or  expectations.  Some  parts  of  the  application may 
tolerate  hardware  faults,  or  may  be  willing  to  risk  them  to  operate  more  power -efficiently.  All 
these  interfaces  can  benefit  from  appropriate  hardware  mechanisms,  such  as  information-flow 
tracking, invariants generation and checking, transactional recovery blocks, reconfiguration, and 
approximate data types. The result will be significant efficiencies. 

3. Closing 

This  white  paper  surveys  the  challenges  and  some  promising  directions  for  investment  in 
computer architecture research to continue to provide better computer systems as a key enabler 
of the information and communication innovations that are transforming our world.  

 

13 

4. About this Document 

This  document  was  created  through  a  distributed  process  during  April  and  May  2012.  
Collaborative  writing  was  supported  by  a  distributed  editor.    We  thank  the  Computing 
Community Consortium13 (CCC), including Erwin Gianchandani and Ed Lazowska, for guidance, 
as  well  as  Jim  Larus  and  Jeannette W ing  for  valuable  feedback.  Researchers  marked  with  “*” 
contributed prose while “**” denotes effort coordinator. 

 
Sarita Adve, University of Illinois at Urbana-Champaign * 
David H. Albonesi, Cornell University 
David Brooks, Harvard 
Luis Ceze, University of Washington * 
Sandhya Dwarkadas, University of Rochester 
Joel Emer, Intel/MIT 
Babak Falsafi, EPFL 
Antonio Gonzalez, Intel and UPC 
Mark D. Hill, University of W isconsin-Madison *,** 
Mary Jane Irwin, Penn State University * 
David Kaeli, Northeastern University * 
Stephen W. Keckler, NVIDIA and The University of Texas at Austin 
Christos Kozyrakis, Stanford University 
Alvin Lebeck, Duke University 
Milo Martin, University of Pennsylvania 
José F. Martínez, Cornell University 
Margaret Martonosi, Princeton University * 
Kunle Olukotun, Stanford University 
Mark Oskin, University of Washington 
Li-Shiuan Peh, M.I.T. 
Milos Prvulovic, Georgia Institute of Technology 
Steven K. Reinhardt, AMD Research 
Michael Schulte, AMD Research and University of W isconsin-Madison 
Simha Sethumadhavan, Columbia University 
Guri Sohi, University of Wisconsin-Madison 
Daniel Sorin, Duke University 
Josep Torrellas, University of Illinois at Urbana Champaign * 
Thomas F. Wenisch, University of Michigan * 
David Wood, University of W isconsin-Madison * 
Katherine Yelick, UC Berkeley, Lawrence Berkeley National Laboratory * 

                                                
13 http://www.cra.org/ccc/ 

14 

Appendix A. Emerging Application Attributes 

Much  evidence  suggests  that  ICT  innovation  is  accelerating  with  many  compelling  visions 
moving  from  science  fiction  toward  reality.  Table  A.1  below  lists  some  of  these  visions,  which 
include  personalized  medicine  to  target  care  and  drugs  to  an  individual,  sophisticated  social 
network  analysis  of  potential  terrorist  threats  to  aid  homeland  security,  and  telepresence  to 
reduce the greenhouse gases spent on commuting and travel. Furthermore, it is likely that many 
important applications have yet  to emerge. How many of us predicted social networking even a 
few years before it became ubiquitous? 
 
While  predicted  and  unpredicted  future  applications  will  have  var ied  requirements,  it  appears 
that many share  features  that were  less common  in earlier applications.   Rather  than center on 
the  desktop,  today  the  centers  of  innovation  lie  in  sensors,  smartphones/tablets,  and  the  data-
centers  to  which  they  connect.    Emerging  applications  share  challenging  attributes,  many  of 
which arise because the applications produce data faster  than can be processed within current, 
limited  capabilities  (in  terms  of  performance,  power,  reliability,  or  their  combination).  Table  A.2 
lists  some  of  these  attributes,  which  include  processing  of  vast  data  sets,  using  distributed 
designs,  working  within  form-factor  constraints,  and  reconciling  rapid  deployment  with  efficient 
operation. 

Table A.1: Example Emerging Applications  

Data-centric  Personalized  Healthcare  -  Future  health  systems  will  monitor  our  health  24/7,  employing 
implantable,  wearable,  and  ambient  smart  sensors.  Local  on-sensor  analysis  can  improve  functionality 
and  reduce  device  power  by  reducing  communication,  while  remote  (i.e.,  c loud-based)  systems  can 
aggregate across  time and patient populations.   Such  systems will allow  us  to query  our own health data 
while  enabling  medical  providers  to  continuously  monitor  patients  and  devise  personalized  therapies. 
New  challenges  will  emerge  in  devising  computing  fabrics  that  meet  performance,  power,  and  energy 
constraints,  in  devising  appropriate  divisions  between  on-device  and  in-cloud  functionality,  as  well  as 
protecting this distributed medical information in the cloud.  

Computation-driven  Scientific  Discovery  -  Today’s  advanced  computational  and  visualization 
capabilities  are 
to  carry  out  simulation -driven 
increasingly  enabling  scientists  and  engineers 
experimentation  and  mine  enormous  data  sets  as  the  primary  drivers  of  scientific  discove ry.  Key  areas 
that  have  already  begun  to  leverage  these  advances  are  bio-simulation,  proteomics,  nanomaterials,  and 
high-energy  physics.   Just  as  the scientific  research community begins  to  leverage  real data,  issues with 
security and reproducibility become critical challenges. 

Human  Network  Analytics  -  Given  the  advances  in  the  Internet  and  personal  communication 
technologies,  individuals  are  interacting  in  new  ways  that  few  envisioned  ten  years  ago.    Human 
interaction  through  these  technologies  has  generated  significant  volumes  of  data  that  can  allow  us  to 
identify  behaviors  and  new  classes  of  connections  between  individuals.    Advances  in  Network  Science 
and Machine  Learning  have  greatly  outpaced  the  ability  of  computational  platforms  to  effectively  analyze 
these  massive  data  sets.  Efficient  human  network  analysis  can  have  a  significant  impact  on  a  range  of 
key application areas including Homeland Security, Financial Markets, and Global Health.  

Many  More  -  In  addition  to  these  three  examples,  numerous  problems  in  the  fields  of  personalized 
learning,  telepresence,  transportation,  urban  infrastructure,  machine  perception/inference  and  enhanced 
virtual  reality  are  all  pushing  the  limits  of  today’s  computational  infrastructure.    The  computational 
demands  of  these  problem  domains  span  a  wide  range  of  form  factors  and  architectures  including 
embedded sensors, hand-held devices and entire data centers.   

15 

Table A.2: Attributes of Emerging Applications  

The  three  example  applications  in  Table  A.1  share  at  least  four  key  attributes  that  present  barriers  that 
require additional research to overcome. 

Big  Data  -  W ith  the  ubiquity  of  continuous  data  streams  from  embedded  sensors  and  the  voluminous 
multimedia  content  from  new  communication  technologies,  we  are  in  the  midst  of  a  digital  information 
explosion.    Processing  this  data  for  health,  commerce  and  other  purposes  requires  efficient  balancing 
between computation, communication,  and storage.   Providing sufficient  on -sensor capability  to  filter and 
process  data  where  it  is  generated/collected  (e.g.,  distinguishing  a  nominal  biometric  signal  from  an 
anomaly),  can  be  most  energy-efficient,  because  the  energy  required  for  communication  can  dominate 
that for computation. Many streams produce data so rapidly that it is cost -prohibitive to store, and must be 
processed  immediately.  In  other  cases,  environmental  constraints  and  the  need  to  aggregate  data 
between sources impacts where we carry out these tasks.  The rich tradeoffs motivate the need for hybrid 
architectures that can efficiently reduce data transfer while conserving energy.  

Always Online  - To protect our borders, our environments and ourselves, computational  resources must 
be  both  available  and  ready  to  provide  services  efficiently.    This  level  of  availability  place s  considerable 
demands  on  the  underlying  hardware  and  software  to  provide  reliability,  security  and  self -managing 
features  not  present  on most  systems.   While  current mainframes  and medical  devices  strive  for  five  9’s 
or  99.999%  availability  (all  but  five  minutes  per  year),  achieving  this  goal  can  cost  millions  of  dollars.  
Tomorrow’s solutions demand this same availability at the many levels, some where the cost is only a few 
dollars. 

Secure  and  Private  -  As  we  increase  our  dependence  on  data,  we  grow more  dependent  on  balancing 
computational  performance  with  providing  for  available,  private,  and  secure  transactions.    Information 
security  is  a  national  priority,  and  our  computational  systems  are  presently  highly  vulnerable  to  attacks. 
Cyber  warfare  is  no  longer  a  hypothetical,  and  numerous  attacks  across  the  Internet  on  government 
websites  have  already  been  deployed.  New  classes  of  hardware  systems,  architectures,  firmware  and 
operating  systems  are  required  to  provide  for  the  secure  delivery  of  distributed  information  for  the  range 
of applications described above. 

 

16 

AFIPS spring joint computer conference(cid:0)	

Validity of the single processor approach to achieving large scale
computing capabilities

Gene M(cid:0) Amdahl

IBM Sunnyvale(cid:1) California



INTRODUCTION

For over a decade prophets have voiced the contention that the organization of a single computer

has reached its limits and that truly signi(cid:0)cant advences can be made only by interconnection of a

multiplicity of computers in such a manner as to permit cooperative solution(cid:1) Variously the proper

direction has been pointed out as general purpose computers with a generalized interconnection

of memories(cid:2) or as specialized computers with geometrically related memory interconnections and

controlled by one or more instruction streams(cid:1)

Demonstration is made of the continued validity of the single processor approach and of the

weaknesses of the multiple processor approach in terms of application to real problems and their

attendant irregularities(cid:1)

The arguments presented are based on statistical characteristics of computation on computers

over the last decade and upon the operational requirements within problems of physical interest(cid:1)

An additional reference will be one of the most thorough analyses of relative computer capabilities

currently published (cid:3)Changes in Computer Performance(cid:1)(cid:4) Datamation(cid:2) September 	(cid:2) Professor

Kenneth F(cid:1) Knight(cid:2) Stanford School of Business Asministration(cid:1)

The (cid:0)rst characteristic of interest is the fraction of the computational load which is associated

with data management housekeeping(cid:1) This fraction has been very nearly constant for about ten

years(cid:2) and accounts for  (cid:10) of the executed instructions in production runs(cid:1) In an entirely dedicated

special purpose environment this might be reduced by a factor of two(cid:2) but it is highly improbably

that it could be reduced by a factor of three(cid:1) The nature of this overhead appears to be sequential

so that it is unlikely to be amenable to parallel processing techniques(cid:1) Overhead alone would then

place an upper limit on throughput of (cid:0)ve to seven times the sequential processing rate(cid:2) even if the

This paper is retyped as the present form by Guihai Chen(cid:0) He wishes you would enjoy reading this historical

paper(cid:0)



housekeeping were done in a separate processor(cid:1) The non housekeeping part of the problem could

exploit at most a processor of performance three to four times the performance of the housekeeping

processor(cid:1) A fairly obvious conclusion which can be drawn at this point is that the e(cid:11)ort expended

on achieving high parallel processing rates is wasted unless it is accompanied by achievements in

sequential processing rates of very nearly the same magnitude(cid:1)

Data management housekeeping is not the only problem to plague oversimpli(cid:0)ed approaches to

high speed computation(cid:1) The physical problems which are of practical interest tend to have rather

signi(cid:0)cant complications(cid:1) Examples of these complications are as follows(cid:12) boundaries are likely to

be irregular(cid:13) interiors are inhomogeneous(cid:13) computations required may be dependent on the states

of the variables at each point(cid:13) propagation rates of di(cid:11)erent physical e(cid:11)ects may be quite di(cid:11)erent(cid:13)

the rate of convergence(cid:2) or convergence at all may be strongly dependent on sweeping through the

array along di(cid:11)erent axes on succeeding passes(cid:2) etc(cid:1) The e(cid:11)ect of each of these complications is

very severe on any computer organization based on geometrically related processors in a paralleled

processing system(cid:1) Even the existence of regular rectangular boundaries has the interesting property

that for spatial dimension of N there are N di(cid:11)erent point geometries to be dealt with in a nearest

neighbor computation(cid:1)

If the second nearest neighbor were also involved(cid:2) there would be N

di(cid:11)erent point geometries to contend with(cid:1) An irregular boundary compounds this problem as does

an inhomogeneous interiors(cid:1) Computations which are dependent on the states of variables would

require the processing at each point to consume approximately the same computational times as

the sum of computations of all physical e(cid:11)ects within a large region(cid:1) Di(cid:11)erences of changes in

propagation rates may a(cid:11)ect the mesh point relationships(cid:1)

Ideally the computation of the action of the neighboring points upon the point under consid(cid:16)

eration involves their values at a previous time proportional to the mesh spacing and inversely

proportional to the propagation rate(cid:1) Since the time step is normally kept constant(cid:2) a faster prop(cid:16)

agation rate for some e(cid:11)ects would imply interactions with more distant points(cid:1) Finally the fairly

common practice of sweeping through the mesh along di(cid:11)erent axes on succeeding passes posed

problems of data management which a(cid:11)ects all processors(cid:2) however it a(cid:11)ects geometrically related

processors more severely by requiring transposing all points in storage in addition to the revised

input(cid:16)output scheduling(cid:1) A realistic assessment of the e(cid:11)ect of these irregularities on a simpli(cid:0)ed

and regularized abstraction of the problem yields a degradation in the vicinity of one(cid:16)half to one

order of magnitude(cid:1)

To sum up the e(cid:11)ects of data management housekeeping and of problem irregularities(cid:2) the

author has compared three di(cid:11)erent machine organizations involving approximately equal amounts



of hardware(cid:1) Machine A has thirty two arithmetic execution units controlled by a single instruction

stream(cid:1) Machine B has pipelined arithmetic execution units with up to three overlapped operations

on vectors of eight elements(cid:1) Machine C has the same pipelined execution units(cid:2) but initiation of

individual operations at the same rate as Machine B permitted vector element operations(cid:1) The

performance of these three machines are plotted in Figure  as a function of the fraction of the

number of instructions which permit parallelism(cid:1) The probable region of operation is centered

around a point corresponding to (cid:10) data management overhead and  (cid:10) of the problem operations

forced to be sequential(cid:1)

Figure  1

The historic performance versus cost of computers has been explored very thoroughly by Profes(cid:16)

sor Knight(cid:1) The carefully analyzed data he presents re(cid:18)ects not just execution times for arithmetic

operations and cost of minimum of recommended con(cid:0)gurations(cid:1) He includes memory capacity ef(cid:16)

fects(cid:2) input(cid:16)output overlap experienced(cid:2) and special functional capabilities(cid:1) The best statistical (cid:0)t

obtained corresponds to a performance proportional to the square of cost at any technological level(cid:1)

This result very e(cid:11)ectively supports the often invoked (cid:3)Grosch(cid:19)s Law(cid:4)(cid:1) Utilizing this analysis(cid:2) one

can argue that if twice the amount of hardware were exploited in a single system(cid:2) one could expect

to obtain four times the performance(cid:1) The only di(cid:20)culty is involved in knowing how to exploit this

additional hardware(cid:1) At any point in time it is di(cid:20)cult to foresee how the precious bottlenecks in

a sequential computer will be e(cid:11)ectively overcome(cid:1) If it were easy they would not have been left as

bottlenecks(cid:1) It is true by historical example that the successive obstacles have been hurdled(cid:2) so it is

appropriate to quote the Rev(cid:1) Adam Clayton Powell(cid:21)(cid:3)Keep the faith(cid:2) baby(cid:22)(cid:4) If alternatively one

decided to improve the performance by putting two processors side by side with shared memory(cid:2)

one would (cid:0)nd approximately (cid:1) times as much hardware(cid:1) The additional two tenths in hardware

accomplished the crossbar switching for the sharing(cid:1) The resulting performance achieved would

be about (cid:1)(cid:1) The latter (cid:0)gure is derived from the assumption of each processor utilizing half of



the memories about half of the time(cid:1) The resulting memory con(cid:18)icts in the shared system would

extend the execution of one of two operations by one quarter of the execution time(cid:1) The net result

is a price performance degradation to  (cid:1) rather than an improvement to (cid:1)  for the single larger

processor(cid:1)

Comparative analysis with associative processor is far less easy and obvious(cid:1) Under certain

condition of regular formats there is a fairly direct approach(cid:1) Consider an associative processor de(cid:16)

signed for pattern recognition in which decisions within individual elements are forwarded to some

set of other elements(cid:1) In the associative processor design the receiving elements would have a set of

source addresses which recognize by associative techniques whether or not it was to receive the de(cid:16)

cision of the currently declaring element(cid:1) To make a corresponding special purpose non(cid:16)associative

processor one would consider a receiving element and its source addresses as an instruction(cid:2) with

binary decision maintained in registers(cid:1) Considering the use of the (cid:0)lm memory(cid:2) an associative

cycle would be longer than a non(cid:16)destructive read cycle(cid:1) In such a real analogy the special purpose

non(cid:16)associative processor can be expected to take about one(cid:16)fourth as many memory cycles as the

associative version and only about one sixth of the time(cid:1) These (cid:0)gures were computed on the full

recognition task with somewhat di(cid:11)ering ratios in each phase(cid:1) No blanket claim is intended here(cid:2)

but rather that each requirement should be investigated from both approaches(cid:1)

 Notes by Guihai Chen

(cid:0) The very famous Amdahl(cid:19)s Law(cid:2) presented as in the following formula(cid:2) is deprived from this

paper(cid:1) However(cid:2) Amdahl gave only a literal description which was paraphrased by latecomers

as follows(cid:12)


rs (cid:25) rp
n
where rs (cid:25) rp (cid:24)  and rs represents the ratio of the sequential portion in one program(cid:1)

S peedup (cid:24)

(cid:0) Only a small part of this paper(cid:2) exactly the fourth paragraph(cid:2) contributes to the Amdahl(cid:19)s

Law(cid:1) This paper also discussed some other important problems(cid:1) For example(cid:2) Amdahl had

forseen many negative factors plaguing the parallel computation of irregular problems(cid:2) such

as (cid:26) boundaries are likely to be irregular(cid:13) (cid:26) interiors are inhomogeneous(cid:13) (cid:26) computations

required may be dependent on the states of the variables at each point(cid:13) (cid:26) propagation rates

of di(cid:11)erent physical e(cid:11)ects may be quite di(cid:11)erent(cid:13) (cid:26) the rate of convergence(cid:2) or convergence

at all may be strongly dependent on sweeping through the array along di(cid:11)erent axes on

succeeding passes(cid:2) etc(cid:1)



Computer Architecture and Organization: From Software to
Hardware

Mano j Franklin
University of Maryland, College Park

c(cid:13)Mano j Franklin 2007

Preface

Introduction

Welcome! Bienvenidoes! Bienvenue! Benvenuto! This book provides a fresh introduction
to computer architecture and organization. The sub ject of computer architecture dates back
to the early periods of computer development, although the term was coined more recently.
Over the years many introductory books have been written on this fascinating sub ject, as the
sub ject underwent many changes due to technological (hardware) and application (software)
changes. Today computer architecture is a topic of great importance to computer science,
computer engineering, and electrical engineering. It bridges the yawning gap between high-
level language programming in computer science and VLSI design in electrical engineering.
The spheres of in(cid:13)uence exercised by computer architecture has expanded signi(cid:12)cantly in
recent years. A fresh introduction of the sub ject is therefore essential for modern computer
users, programmers, and designers.

This book is for students of computer science, computer engineering, electrical engineer-
ing, and any others who are interested in learning the fundamentals of computer architecture
in a structured manner. It contains core material that is essential to students in all of these
disciplines.
It is designed for use in a computer architecture or computer organization
course typically o(cid:11)ered at the undergraduate level by computer science, computer engineer-
ing, electrical engineering, or information systems departments. On successful completion
of this book you will have a clear understanding of the foundational principles of computer
architecture. Many of you may have taken a course in high-level language programming
and in digital logic before using this book. We assume most readers will have some fa-
miliarity with computers, and perhaps have even done some programming in a high-level
language. We also assume that readers have had exposure to preliminary digital logic de-
sign. This book will extend that knowledge to the core areas of computer architecture,
namely assembly-level architecture, instruction set architecture, and microarchitecture.

The WordReference dictionary de(cid:12)nes computer architecture as \the structure and or-
ganization of a computer’s hardware or system software." Dictionary.com de(cid:12)nes it as \the
art of assembling logical elements into a computing device; the speci(cid:12)cation of the rela-
tion between parts of a computer system." Computer architecture deals with the way in
which the elements of a computer relate to each other. It is concerned with all aspects of
the design and operation of the computer system. It extends upward into software as a
system’s architecture is intimately tied to the operating system and other system software.
It is almost impossible to design a good operating system without knowing the underlying
architecture of the systems where the operating system will run. Similarly, the compiler
requires an even more intimate knowledge of the architecture.

It is important to understand the general principles behind the design of computers, and
to see how those principles are put into practice in real computers. The goal of this book is

2

to provide a complete discussion of the fundamental concepts, along with an extensive set
of examples that reinforce these concepts. A few detailed examples are also given for the
students to have a better appreciation of real-life intricacies. These examples are presented
in a manner that does not distract the student from the fundamental concepts. Clearly, we
cannot cover every single aspect of computer architecture in an introductory book. Our goal
is to cover the fundamentals and to lay a good foundation upon which motivated students
can easily build later. For each topic, we use the following test to decide if it should get
included in the text:
is the topic foundational? If the answer is positive, we include the
topic.

Almost every aspect of computer architecture is replete with trade-o(cid:11)s, involving char-
acteristics such as programmability, software compatibility, portability, speed, cost, power
consumption, die size, and reliability. For general-purpose computers, one trade-o(cid:11) drives
the most important choices the computer architect must make: speed versus cost. For
laptops and embedded systems, the important considerations are size and power consump-
tion. For space applications and other mission-critical applications, reliability and power
consumption are of primary concern. Among these considerations, we highlight programma-
bility, performance, cost, and power consumption throughout the text, as they are funda-
mental factors a(cid:11)ecting how a computer is designed. However, this coverage is somewhat
qualitative, and not intended to be quantitative in nature. Extensive coverage of quantita-
tive analysis is traded o(cid:11) in favor of qualitative explanation of issues. Students will have
plenty of opportunity to study quantitative analysis in a graduate-level computer architec-
ture course. Additional emphasis is also placed on how various parts of the system are
related to real-world demand and technology constraints.

Performance and functionality are key to the utility of a computer system. Perhaps one
of the most important reasons for studying computer architecture is to learn how to extract
the best performance from a computer. As an assembly language programmer, for instance,
you need to understand how to use the system’s functionality most e(cid:11)ectively. Speci(cid:12)cally,
you must understand its architecture so that you will be able to exploit that architecture
during programming.

Coverage of Software and Hardware

Computer architecture/organization is a discipline with many facets, ranging from trans-
lation of high-level language programs through design of instruction set architecture and
microarchitecture to the logic-level design of computers. Some of these facets have more
of a software luster whereas others have more of a hardware luster. We believe that a
good introduction to the discipline should give a broad overview of all the facets and their
interrelationships, leaving a non-specialist with a decent perspective on computer architec-
ture, and providing an undergraduate student with a solid foundation upon which related
and advanced sub jects can be built. Traditional introductory textbooks focussing only on
software topics or on hardware topics do not ful(cid:12)ll these ob jectives.

3

Our presentation is unique in that we cover both software and hardware concepts. These
include high-level language, assembly language programming, systems programming, in-
struction set architecture design, microarchitecture design, system design, and digital logic
design.

There are four legs that form the foundation of computer architecture: assembly-level
architecture, instruction set architecture, microarchitecture, and logic-level architecture.
This book is uniquely concerned about all four legs. Starting from the assembly-level
architecture, we carry out the design of the important portions of a computer system all
the way to the lower hardware levels, considering plausible alternatives at each level.

Structured Approach

In an e(cid:11)ort to systematically cover all of these fundamental topics, the material has been
organized in a structured manner, from the high-level architecture to the logic-level archi-
tecture. Our coverage begins with a high-level language programmer’s view|expressing
algorithms in an HLL such as C|and moves towards the less abstract levels. Although
there are a few textbooks that start from the digital logic level and work their way to-
wards the more abstract levels, in our view the fundamental issues of computer architec-
ture/organization are best learned starting with the software levels, with which most of the
students are already familiar. Moreover, it is easier to appreciate why a level is designed
in a particular manner if the student knows what the design is supposed to implement.
This structured approach|from abstract software levels to less abstract software levels to
abstract hardware levels to less abstract hardware levels|is faithfully followed through-
out the book. We make exceptions only in a few places where such a deviation tends to
improve clarity. For example, while discussing ISA (instruction set architecture) design
options in Chapter 5, we allude to hardware issues such as pipelining and multiple-issue,
which in(cid:13)uence ISA design.

For each architecture level we answer the following fundamental questions: What is
the nature of the machine at this level? What are the ways in which its building blocks
interact? How does the machine interact with the outside world? How is programming
done at this level? How is a higher-level program translated/interpreted for controlling the
machine at this level? We are con(cid:12)dent that after you have mastered these fundamental
concepts, building upon them will be quite straightforward.

Example Instruction Set

As an important goal of this book is to lay a good foundation for the general sub ject of com-
puter architecture, we have refrained from focusing on a single architecture in our discussion
of the fundamental concepts. Thus, when presenting concepts at each architecture level,
great care is taken to keep the discussion general, without tailoring to a speci(cid:12)c architecture.
For instance, when discussing the assembly language architecture, we discuss register-based

4

approach as well as a stack-based approach. When discussing virtual memory, we discuss a
paging-based approach as well as a segmentation-based approach. In other words, at each
stage of the design, we discuss alternative approaches, and the associated trade-o(cid:11)s. While
one alternative may seem better today, technological innovations may tip the scale towards
another in the future.

For ease of learning, the discussion of concepts is peppered with suitable examples. We
have found that students learn the di(cid:11)erent levels and their inter-relationships better when
there is a continuity among many of the examples used in di(cid:11)erent parts of the book. For
this purpose, we have used the standard MIPS assembly language [ref ] and the standard
MIPS Lite instruction set architecture [ref ], a subset of the MIPS-I ISA [ref ]. We use MIPS
because it is very simple and has had commercial success, both in general-purpose com-
puting and in embedded systems. The MIPS architecture had its beginnings in 1984, and
was (cid:12)rst implemented in 1985. By the late 1980s, the architecture had been adopted by
several workstation and server companies, including Digital Equipment Corporation and
Silicon Graphics. Now MIPS processors are widely used in Sony and Nintendo game ma-
chines, palmtops, laser printers, Cisco routers, and SGI high-performance graphics engines.
More importantly, some popular texts on Advanced Computer Architecture use the MIPS
architecture. The use of the MIPS instruction set in this introductory book will therefore
provide good continuity for those students wishing to pursue higher studies in Computer
Science or Engineering.

In rare occasions, I have changed some terminology, not to protect the innocent but
simply to make it clearer to understand.

Organization and Usage of the Book

This book is organized to meet the needs of several potential audiences. It can serve as an
undergraduate text, as well as a professional reference for engineers and members of the
technical community who (cid:12)nd themselves frequently dealing with computing. The book
uses a structured approach, and is intended to be read sequentially. Each chapter builds
upon the previous ones. Certain sections contain somewhat advanced technical material,
and can be skipped by the reader without loss in continuity. These sections are marked
with an asterisk. We recommend, however, that even those sections be skimmed, at least
to get a super(cid:12)cial idea of their contents.

Each chapter is followed by a \Concluding Remarks" section and an \Exercises" section.
The exercises are particularly important. They help master the material by integrating a
number of di(cid:11)erent concepts. The book also includes many real-world examples, both his-
torical and current, in each chapter. Instead of presenting real-world examples in isolation,
such examples are included while presenting the ma jor concepts.

This book is organized into 9 chapters, which are grouped into 3 parts. The (cid:12)rst part
provides an overview of the sub ject. The second part covers the software levels, and the
third part covers the hardware levels. The coverage of the software levels is not intended

5

to make the readers pro(cid:12)cient in programming in these levels, but rather to help them
understand what each level does, how programs at immediately higher level are converted
to this level, and how to design this level in a better way.

A layered approach is used to cover the topics. Each new layer builds upon the previous
material to add depth and understanding to the reader’s knowledge.

Chapter 1 provides an overview of .... It opens with a discussion of the expanding role
of computers, and the trends in technology and software applications. It brie(cid:13)y introduces
..... Chapter 2 ... Chapter 3 ..... Most of the material in Chapter 3 should be familiar
to readers with a background in computer programming, and they can probably browse
through this chapter. Starting with Chapter 4, the material deals with the core issues in
computer architecture. Chapter 4 ..... Chapter 5 ..... Chapter 6 .....

The book can be tailored for use in software-centric as well as hardware-centric courses.
For instance, skipping the last chapter (or the last 3 chapters) makes the book becomes suit-
able for a software-centric course, and skipping chapter 2 makes it suitable for a hardware-
centric course.
\If you are planning for a year, sow rice;
if you are planning for a decade, plant trees;
if you are planning for a lifetime, educate people."
| Chinese Proverb

\Therefore, since brevity is the soul of wit, And tediousness the limbs and outward
(cid:13)ourishes, I wil l be brief "
| Wil liam Shakespeare, Hamlet

6

Soli Deo Gloria

Contents

1 Introduction

1.1 Computing and Computers

. . . . . . . . . . . . . . . . . . . . . . . . . . .

1.1.1 The Problem-Solving Process . . . . . . . . . . . . . . . . . . . . . .

1.1.2 Automating Algorithm Execution with Computers . . . . . . . . . .

1.2 The Digital Computer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.2.1 Representing Programs in a Digital Computer: The Stored Program
Concept . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.2.2 Basic Software Organization . . . . . . . . . . . . . . . . . . . . . . .

1.2.3 Basic Hardware Organization . . . . . . . . . . . . . . . . . . . . . .

1.2.4

Software versus Hardware . . . . . . . . . . . . . . . . . . . . . . . .

1.2.5 Computer Platforms . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.3 A Modern Computer System . . . . . . . . . . . . . . . . . . . . . . . . . .

1.3.1 Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.3.2

1.3.3

Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Starting the Computer System: The Boot Process

. . . . . . . . . .

1.3.4 Computer Network . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.4 Trends in Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.4.1 Hardware Technology Trends . . . . . . . . . . . . . . . . . . . . . .

1.4.2

Software Technology Trends . . . . . . . . . . . . . . . . . . . . . . .

1.5 Software Design Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.6 Hardware Design Issues

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.6.1 Performance

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.6.2 Power Consumption . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.6.3 Price . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

1

3

3

5

9

10

12

13

15

16

17

17

19

20

21

22

22

23

25

25

25

26

27

8

CONTENTS

1.6.4

1.6.5

Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.7 Theoretical Underpinnings . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.7.1 Computability and the Turing Machine

. . . . . . . . . . . . . . . .

1.7.2 Limitations of Computers . . . . . . . . . . . . . . . . . . . . . . . .

1.8 Virtual Machines: The Abstraction Tower . . . . . . . . . . . . . . . . . . .

1.8.1 Problem De(cid:12)nition and Modeling Level Architecture . . . . . . . . .

1.8.2 Algorithm-Level Architecture . . . . . . . . . . . . . . . . . . . . . .

1.8.3 High-Level Architecture . . . . . . . . . . . . . . . . . . . . . . . . .

1.8.4 Assembly-Level Architecture

. . . . . . . . . . . . . . . . . . . . . .

1.8.5

Instruction Set Architecture (ISA) . . . . . . . . . . . . . . . . . . .

1.8.6 Microarchitecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.8.7 Logic-Level Architecture . . . . . . . . . . . . . . . . . . . . . . . . .

1.8.8 Device-Level Architecture . . . . . . . . . . . . . . . . . . . . . . . .

1.9 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.10 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

I PROGRAM DEVELOPMENT | SOFTWARE LEVELS

2 Program Development Basics

2.1 Overview of Program Development . . . . . . . . . . . . . . . . . . . . . . .

2.1.1 Programming Languages . . . . . . . . . . . . . . . . . . . . . . . . .

2.1.2 Application Programming Interface Provided by Library . . . . . . .

2.1.3 Application Programming Interface Provided by OS . . . . . . . . .

2.1.4 Compilation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.1.5 Debugging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 Programming Language Speci(cid:12)cation . . . . . . . . . . . . . . . . . . . . . .

2.2.1

2.2.2

Syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3 Data Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3.1 Constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3.2 Variables

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3.3

IO Streams and Files . . . . . . . . . . . . . . . . . . . . . . . . . . .

27

27

27

27

28

30

33

33

37

38

38

39

39

40

41

41

43

45

46

47

50

50

50

50

50

50

50

50

51

52

58

CONTENTS

2.3.4 Data Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3.5 Modeling Real-World Data . . . . . . . . . . . . . . . . . . . . . . .

2.4 Operators and Assignments . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.5 Control Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.5.1 Conditional Statements

. . . . . . . . . . . . . . . . . . . . . . . . .

2.5.2 Loops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.5.3

2.5.4

Subroutines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Subroutine Nesting and Recursion . . . . . . . . . . . . . . . . . . .

2.5.5 Re-entrant Subroutine . . . . . . . . . . . . . . . . . . . . . . . . . .

2.5.6 Program Modules

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.5.7

Software Interfaces: API and ABI

. . . . . . . . . . . . . . . . . . .

2.6 Library API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.7 Operating System API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.7.1 What Should be Done by the OS? . . . . . . . . . . . . . . . . . . .

2.7.2

Input/Output Management . . . . . . . . . . . . . . . . . . . . . . .

2.7.3 Memory Management

. . . . . . . . . . . . . . . . . . . . . . . . . .

2.7.4 Process Management . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.8 Operating System Organization . . . . . . . . . . . . . . . . . . . . . . . . .

2.8.1

System Call Interface

. . . . . . . . . . . . . . . . . . . . . . . . . .

2.8.2 File System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.8.3 Device Management: Device Drivers . . . . . . . . . . . . . . . . . .

2.8.4 Hardware Abstraction Layer (HAL)

. . . . . . . . . . . . . . . . . .

2.8.5 Process Control System . . . . . . . . . . . . . . . . . . . . . . . . .

2.9 Ma jor Issues in Program Development . . . . . . . . . . . . . . . . . . . . .

2.9.1 Portability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.9.2 Reusability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.9.3 Concurrency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.10 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.11 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Assembly-Level Architecture | User Mode

3.1 Overview of User Mode Assembly-Level Architecture . . . . . . . . . . . . .

3.1.1 Assembly Language Alphabet and Syntax . . . . . . . . . . . . . . .

9

60

60

64

65

65

66

67

68

68

69

69

69

70

72

72

73

74

74

76

76

77

78

78

80

80

80

80

80

80

81

82

83

10

CONTENTS

3.1.2 Memory Model

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.3 Register Model

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.4 Data Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.5 Assembler Directives . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.6

Instruction Types and Instruction Set . . . . . . . . . . . . . . . . .

3.1.7 Program Execution . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.8 Challenges of Assembly Language Programming . . . . . . . . . . .

3.1.9 The Rationale for Assembly Language Programming . . . . . . . . .

3.2 Assembly-Level Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2.1 Assembly-Level Interface Provided by Library . . . . . . . . . . . . .

3.2.2 Assembly-Level Interface Provided by OS . . . . . . . . . . . . . . .

3.3 Example Assembly-Level Architecture: MIPS-I . . . . . . . . . . . . . . . .

3.3.1 Assembly Language Alphabet and Syntax . . . . . . . . . . . . . . .

3.3.2 Register Model

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3.3 Memory Model

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3.4 Assembler Directives . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3.5 Assembly-Level Instructions . . . . . . . . . . . . . . . . . . . . . . .

3.3.6 An Example MIPS-I AL Program . . . . . . . . . . . . . . . . . . .

3.3.7

SPIM: A Simulator for the MIPS-I Architecture

. . . . . . . . . . .

3.4 Translating HLL Programs to AL Programs . . . . . . . . . . . . . . . . . .

3.4.1 Translating Constant Declarations . . . . . . . . . . . . . . . . . . .

3.4.2 Translating Variable Declarations . . . . . . . . . . . . . . . . . . . .

3.4.3 Translating Variable References . . . . . . . . . . . . . . . . . . . . .

3.4.4 Translating Conditional Statements

. . . . . . . . . . . . . . . . . .

3.4.5 Translating Loops

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.4.6 Translating Subroutine Calls and Returns . . . . . . . . . . . . . . .

3.4.7 Translating System Calls

. . . . . . . . . . . . . . . . . . . . . . . .

3.4.8 Overview of a Compiler . . . . . . . . . . . . . . . . . . . . . . . . .

3.5 Memory Models: Design Choices . . . . . . . . . . . . . . . . . . . . . . . .

3.5.1 Address Space: Linear vs Segmented . . . . . . . . . . . . . . . . . .

3.5.2 Word Alignment: Aligned vs Unaligned . . . . . . . . . . . . . . . .

3.5.3 Byte Ordering: Little Endian vs Big Endian . . . . . . . . . . . . . .

3.6 Operand Locations: Design Choices

. . . . . . . . . . . . . . . . . . . . . .

83

85

87

89

90

93

94

95

96

97

97

97

97

98

101

102

103

104

107

107

108

110

118

119

122

123

127

128

129

129

130

131

131

CONTENTS

3.6.1

Instruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.6.2 Main Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.6.3 General-Purpose Registers . . . . . . . . . . . . . . . . . . . . . . . .

3.6.4 Accumulator

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.6.5 Operand Stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.7 Operand Addressing Modes: Design Choices . . . . . . . . . . . . . . . . . .

3.7.1

Instruction-Residing Operands: Immediate Operands . . . . . . . . .

3.7.2 Register Operands . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.7.3 Memory Operands . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.7.4

Stack Operands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.8 Subroutine Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.8.1 Register Saving and Restoring

. . . . . . . . . . . . . . . . . . . . .

3.8.2 Return Address Storing . . . . . . . . . . . . . . . . . . . . . . . . .

3.8.3 Parameter Passing and Return Value Passing . . . . . . . . . . . . .

3.9 De(cid:12)ning Assembly Languages for Programmability . . . . . . . . . . . . . .

3.9.1 Labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.9.2 Pseudoinstructions . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.9.3 Macros

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.10 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.11 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Assembly-Level Architecture | Kernel Mode

4.1 Overview of Kernel Mode Assembly-Level Architecture . . . . . . . . . . . .

4.1.1 Privileged Registers

. . . . . . . . . . . . . . . . . . . . . . . . . . .

4.1.2 Privileged Memory Address Space . . . . . . . . . . . . . . . . . . .

4.1.3

IO Addresses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.1.4 Privileged Instructions . . . . . . . . . . . . . . . . . . . . . . . . . .

4.2 Switching from User Mode to Kernel Mode . . . . . . . . . . . . . . . . . .

4.2.1

Syscall Instructions: Switching Initiated by User Programs

. . . . .

4.2.2 Device Interrupts: Switching Initiated by IO Interfaces . . . . . . . .

4.2.3 Exceptions: Switching Initiated by Rare Events . . . . . . . . . . . .

4.3

IO Registers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.3.1 Memory Mapped IO Address Space

. . . . . . . . . . . . . . . . . .

11

131

131

132

132

133

136

137

137

138

140

141

142

143

145

146

146

146

146

147

147

149

150

151

152

152

152

153

154

156

157

158

158

12

CONTENTS

4.3.2

Independent IO Address Space . . . . . . . . . . . . . . . . . . . . .

4.3.3 Operating System’s Use of IO Addresses . . . . . . . . . . . . . . . .

4.4 Operating System Organization . . . . . . . . . . . . . . . . . . . . . . . . .

4.4.1

System Call Layer

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.4.2 File System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.4.3 Device Management: Device Drivers . . . . . . . . . . . . . . . . . .

4.4.4 Process Control System . . . . . . . . . . . . . . . . . . . . . . . . .

4.5 System Call Layer for a MIPS-I OS . . . . . . . . . . . . . . . . . . . . . . .

4.5.1 MIPS-I Machine Speci(cid:12)cations for Exceptions . . . . . . . . . . . . .

4.5.2 OS Usage of MIPS-I Architecture Speci(cid:12)cations . . . . . . . . . . . .

4.6

IO Schemes Employed by Device Management System . . . . . . . . . . . .

4.6.1

Sampling-Based IO . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.6.2 Program-Controlled IO . . . . . . . . . . . . . . . . . . . . . . . . .

4.6.3

Interrupt-Driven IO . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.6.4 Direct Memory Access (DMA)

. . . . . . . . . . . . . . . . . . . . .

4.6.5

IO Co-processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.6.6 Wrap Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.7 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.8 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Instruction Set Architecture (ISA)

5.1 Overview of Instruction Set Architecture . . . . . . . . . . . . . . . . . . . .

5.1.1 Machine Language . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.1.2 Register, Memory, and IO Models

. . . . . . . . . . . . . . . . . . .

5.1.3 Data Types and Formats

. . . . . . . . . . . . . . . . . . . . . . . .

5.1.4

Instruction Types and Formats . . . . . . . . . . . . . . . . . . . . .

5.2 Example Instruction Set Architecture: MIPS-I

. . . . . . . . . . . . . . . .

5.2.1 Register, Memory, and IO Models

. . . . . . . . . . . . . . . . . . .

5.2.2 Data Types and Formats

. . . . . . . . . . . . . . . . . . . . . . . .

5.2.3

Instruction Types and Formats . . . . . . . . . . . . . . . . . . . . .

5.2.4 An Example MIPS-I ML Program . . . . . . . . . . . . . . . . . . .

5.3 Translating Assembly Language Programs to Machine Language Programs

5.3.1 MIPS-I Assembler Conventions . . . . . . . . . . . . . . . . . . . . .

158

160

162

164

164

165

167

168

168

170

173

173

174

177

181

182

183

183

183

185

186

186

189

189

189

190

190

190

190

191

191

192

CONTENTS

5.3.2 Translating Decimal Numbers . . . . . . . . . . . . . . . . . . . . . .

5.3.3 Translating AL-speci(cid:12)c Instructions and Macros

. . . . . . . . . . .

5.3.4 Translating Labels . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.3.5 Code Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.3.6 Overview of an Assembler . . . . . . . . . . . . . . . . . . . . . . . .

5.3.7 Cross Assemblers . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.4 Linking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.4.1 Resolving External References

. . . . . . . . . . . . . . . . . . . . .

5.4.2 Relocating the Memory Addresses

. . . . . . . . . . . . . . . . . . .

5.4.3 Program Start-Up Routine

. . . . . . . . . . . . . . . . . . . . . . .

5.5

Instruction Formats: Design Choices . . . . . . . . . . . . . . . . . . . . . .

5.5.1 Fixed Length Instruction Encoding . . . . . . . . . . . . . . . . . . .

5.5.2 Variable Length Instruction Encoding . . . . . . . . . . . . . . . . .

5.6 Data Formats: Design Choices and Standards . . . . . . . . . . . . . . . . .

5.6.1 Unsigned Integers: Binary Number System . . . . . . . . . . . . . .

5.6.2

Signed Integers: 2’s Complement Number System . . . . . . . . . . .

5.6.3 Floating Point Numbers: ANSI/IEEE Floating Point Standard . . .

5.6.4 Characters: ASCII and Unicode

. . . . . . . . . . . . . . . . . . . .

5.7 Designing ISAs for Better Performance . . . . . . . . . . . . . . . . . . . . .

5.7.1 Technological Improvements and Their E(cid:11)ects

. . . . . . . . . . . .

5.7.2 CISC Design Philosophy . . . . . . . . . . . . . . . . . . . . . . . . .

5.7.3 RISC Design Philosophy . . . . . . . . . . . . . . . . . . . . . . . . .

5.7.4 Recent Trends

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.8 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.9 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

II PROGRAM EXECUTION | HARDWARE LEVELS

6 Program Execution Basics

6.1 Overview of Program Execution . . . . . . . . . . . . . . . . . . . . . . . . .

6.2 Selecting the Program: User Interface

. . . . . . . . . . . . . . . . . . . . .

6.2.1 CLI Shells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.2.2 GUI Shells

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

13

192

192

193

195

196

196

197

198

198

198

199

201

203

203

204

205

206

212

213

214

215

215

217

217

218

219

221

221

222

223

224

14

CONTENTS

6.2.3 VUI Shells

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.3 Creating the Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.4 Loading the Program . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.4.1 Dynamic Linking of Libraries . . . . . . . . . . . . . . . . . . . . . .

6.5 Executing the Program . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.6 Halting the Program . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.7

Instruction Set Simulator

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.7.1

6.7.2

Implementing the Register Space . . . . . . . . . . . . . . . . . . . .

Implementing the Memory Address Space . . . . . . . . . . . . . . .

6.7.3 Program Loading . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.7.4

Instruction Fetch Phase . . . . . . . . . . . . . . . . . . . . . . . . .

6.7.5 Executing the ML Instructions . . . . . . . . . . . . . . . . . . . . .

6.7.6 Executing the Syscall Instruction . . . . . . . . . . . . . . . . . . . .

6.7.7 Comparison with Hardware Microarchitecture . . . . . . . . . . . . .

6.8 Hardware Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.8.1 Clock . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.8.2 Hardware Description Language (HDL)

. . . . . . . . . . . . . . . .

6.8.3 Design Speci(cid:12)cation in HDL . . . . . . . . . . . . . . . . . . . . . . .

6.8.4 Design Veri(cid:12)cation using Simulation . . . . . . . . . . . . . . . . . .

6.8.5 Hardware Design Metrics

. . . . . . . . . . . . . . . . . . . . . . . .

6.9 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.10 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7 Microarchitecture | User Mode

7.1 Overview of User Mode Microarchitecture . . . . . . . . . . . . . . . . . . .

7.1.1 Dichotomy: Data Path and Control Unit

. . . . . . . . . . . . . . .

7.1.2 Register File and Individual Registers . . . . . . . . . . . . . . . . .

7.1.3 Memory Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.1.4 ALUs and Other Functional Units

. . . . . . . . . . . . . . . . . . .

7.1.5

Interconnects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.1.6 Processor and Memory Subsystems . . . . . . . . . . . . . . . . . . .

7.1.7 Micro-Assembly Language (MAL)

. . . . . . . . . . . . . . . . . . .

7.2 Example Microarchitecture for Executing MIPS-0 Programs . . . . . . . . .

225

227

227

228

230

231

231

233

234

235

235

235

236

237

237

237

237

238

238

238

239

239

241

243

243

244

245

246

247

249

249

251

CONTENTS

7.2.1 MAL Commands . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.2.2 MAL Operation Set

. . . . . . . . . . . . . . . . . . . . . . . . . . .

7.2.3 An Example MAL Routine . . . . . . . . . . . . . . . . . . . . . . .

7.3

Interpreting ML Programs by MAL Routines . . . . . . . . . . . . . . . . .

7.3.1

7.3.2

7.3.3

7.3.4

7.3.5

Interpreting an Instruction | the Fetch Phase . . . . . . . . . . . .

Interpreting Arithmetic/Logical Instructions

. . . . . . . . . . . . .

Interpreting Memory-Referencing Instructions . . . . . . . . . . . . .

Interpreting Control-Changing Instructions . . . . . . . . . . . . . .

Interpreting Trap Instructions . . . . . . . . . . . . . . . . . . . . . .

7.4 Memory System Organization . . . . . . . . . . . . . . . . . . . . . . . . . .

7.4.1 Memory Hierarchy: Achieving Low Latency and Cost

. . . . . . . .

7.4.2 Cache Memory: Basic Organization . . . . . . . . . . . . . . . . . .

7.4.3 MIPS-0 Data Path with Cache Memories

. . . . . . . . . . . . . . .

7.4.4 Cache Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.4.5 Address Mapping Functions . . . . . . . . . . . . . . . . . . . . . . .

7.4.6 Finding a Word in the Cache . . . . . . . . . . . . . . . . . . . . . .

7.4.7 Block Replacement Policy . . . . . . . . . . . . . . . . . . . . . . . .

7.4.8 Multi-Level Cache Memories

. . . . . . . . . . . . . . . . . . . . . .

7.5 Processor-Memory Bus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.5.1 Bus Width . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.5.2 Bus Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.6 Processor Data Path Interconnects: Design Choices . . . . . . . . . . . . . .

7.6.1 Multiple-Bus based Data Paths . . . . . . . . . . . . . . . . . . . . .

7.6.2 Direct Path-based Data Path . . . . . . . . . . . . . . . . . . . . . .

7.7 Pipelined Data Path: Overlapping the Execution of Multiple Instructions

.

7.7.1 De(cid:12)ning a Pipelined Data Path . . . . . . . . . . . . . . . . . . . . .

7.7.2

Interpreting ML Instructions in a Pipelined Data Path . . . . . . . .

7.7.3 Control Unit for a Pipelined Data Path . . . . . . . . . . . . . . . .

7.7.4 Dealing with Control Flow . . . . . . . . . . . . . . . . . . . . . . .

7.7.5 Dealing with Data Flow . . . . . . . . . . . . . . . . . . . . . . . . .

7.7.6 Pipelines in Commercial Processors

. . . . . . . . . . . . . . . . . .

7.8 Wide Data Paths: Superscalar and VLIW Processing . . . . . . . . . . . . .

7.9 Co-Processors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

15

253

253

253

254

256

257

258

259

260

260

260

263

264

264

264

267

268

269

269

270

270

272

272

273

274

275

279

279

280

284

286

287

288

16

CONTENTS

7.10 Processor Data Paths for Low Power . . . . . . . . . . . . . . . . . . . . . .

7.11 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.12 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8 Microarchitecture | Kernel Mode

8.1 Processor Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.1.1

Interpreting a System Call Instruction . . . . . . . . . . . . . . . . .

8.1.2 Recognizing Exceptions and Hardware Interrupts . . . . . . . . . . .

8.1.3

Interpreting an RFE Instruction . . . . . . . . . . . . . . . . . . . .

8.2 Memory Management: Implementing Virtual Memory . . . . . . . . . . . .

8.2.1 Virtual Memory: Implementing a Large Address Space . . . . . . . .

8.2.2 Paging and Address Translation . . . . . . . . . . . . . . . . . . . .

8.2.3 Page Table Organization . . . . . . . . . . . . . . . . . . . . . . . . .

8.2.4 Translation Lookaside Bu(cid:11)er (TLB)

. . . . . . . . . . . . . . . . . .

8.2.5

Software-Managed TLB and the Role of the Operating System in
Virtual Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.2.6

Sharing in a Paging System . . . . . . . . . . . . . . . . . . . . . . .

8.2.7 A Real-Life Example: a MIPS-I Virtual Memory System . . . . . . .

8.2.8

Interpreting a MIPS-I Memory-Referencing Instruction . . . . . . .

8.2.9 Combining Cache Memory and Virtual Memory . . . . . . . . . . .

8.3

IO System Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.3.1

8.3.2

Implementing the IO Address Space: IO Data Path . . . . . . . . .

Implementing the IO Interface Protocols: IO Controllers . . . . . . .

8.3.3 Example IO Controllers . . . . . . . . . . . . . . . . . . . . . . . . .

8.3.4 Frame Bu(cid:11)er: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.3.5

IO Con(cid:12)guration: Assigning IO Addresses to IO Controllers . . . . .

8.4 System Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.4.1

Single System Bus . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.4.2 Hierarchical Bus Systems

. . . . . . . . . . . . . . . . . . . . . . . .

8.4.3

Standard Buses and Interconnects

. . . . . . . . . . . . . . . . . . .

8.4.4 Expansion Bus and Expansion Slots . . . . . . . . . . . . . . . . . .

8.4.5

IO System in Modern Desktops . . . . . . . . . . . . . . . . . . . . .

8.4.6 Circa 2006

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.4.7 RAID . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

288

290

291

293

293

294

295

297

297

297

301

304

306

309

312

312

319

320

321

322

323

324

325

329

332

332

333

341

352

354

355

356

CONTENTS

8.5 Network Architecture

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.5.1 Network Interface Card (NIC)

. . . . . . . . . . . . . . . . . . . . .

8.5.2 Protocol Stacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.6

Interpreting an IO Instruction . . . . . . . . . . . . . . . . . . . . . . . . . .

8.7 System-Level Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.8 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.9 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9 Register Tranfer Level Architecture

9.1 Overview of RTL Architecture

. . . . . . . . . . . . . . . . . . . . . . . . .

9.1.1 Register File and Individual Registers . . . . . . . . . . . . . . . . .

9.1.2 ALUs and Other Functional Units

. . . . . . . . . . . . . . . . . . .

9.1.3 Register Transfer Language . . . . . . . . . . . . . . . . . . . . . . .

9.2 Example RTL Data Path for Executing MIPS-0 ML Programs

. . . . . . .

9.2.1 RTL Instruction Set . . . . . . . . . . . . . . . . . . . . . . . . . . .

9.2.2 RTL Operation Types . . . . . . . . . . . . . . . . . . . . . . . . . .

9.2.3 An Example RTL Routine . . . . . . . . . . . . . . . . . . . . . . . .

9.3

Interpreting ML Programs by RTL Routines

. . . . . . . . . . . . . . . . .

17

357

358

359

359

359

360

360

361

362

362

363

363

364

366

367

368

369

9.3.1

9.3.2

9.3.3

9.3.4

9.3.5

Interpreting the Fetch and PC Update Commands for Each Instruction 369

Interpreting Arithmetic/Logical Instructions

. . . . . . . . . . . . .

Interpreting Memory-Referencing Instructions . . . . . . . . . . . . .

Interpreting Control-Changing Instructions . . . . . . . . . . . . . .

Interpreting Trap Instructions . . . . . . . . . . . . . . . . . . . . . .

9.4 RTL Control Unit: An Interpreter for ML Programs . . . . . . . . . . . . .

9.4.1 Developing an Algorithm for RTL Instruction Generation . . . . . .

9.4.2 Designing the Control Unit as a Finite State Machine . . . . . . . .

9.4.3

9.4.4

Incorporating Sequencing Information in the Microinstruction . . . .

State Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9.5 Memory System Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9.5.1 A Simple Memory Data Path . . . . . . . . . . . . . . . . . . . . . .

9.5.2 Memory Interface Unit . . . . . . . . . . . . . . . . . . . . . . . . . .

9.5.3 Memory Controller . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9.5.4 DRAM Controller

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

371

372

373

374

375

375

377

380

381

382

383

383

383

383

18

CONTENTS

9.5.5 Cache Memory Design . . . . . . . . . . . . . . . . . . . . . . . . . .

9.5.6 Cache Controller: Interpreting a Read/Write Command . . . . . . .

9.6 Processor Data Path Interconnects: Design Choices . . . . . . . . . . . . . .

9.6.1 Multiple-Bus based Data Paths . . . . . . . . . . . . . . . . . . . . .

9.6.2 Direct Path-based Data Path . . . . . . . . . . . . . . . . . . . . . .

9.7 Pipelined Data Path: Overlapping the Execution of Multiple Instructions

.

9.7.1 De(cid:12)ning a Pipelined Data Path . . . . . . . . . . . . . . . . . . . . .

9.7.2

Interpreting ML Instructions in a Pipelined Data Path . . . . . . . .

9.7.3 Control Unit for a Pipelined Data Path . . . . . . . . . . . . . . . .

9.8 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9.9 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10 Logic-Level Architecture

10.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.1.1 Multiplexers

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.1.2 Decoders

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.1.3 Flip-Flops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.1.4 Static RAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.1.5 Dynamic RAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.1.6 Tri-State Bu(cid:11)ers . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.2 Implementing ALU and Functional Units of Data Path . . . . . . . . . . . .

10.2.1 Implementing an Integer Adder . . . . . . . . . . . . . . . . . . . . .

10.2.2 Implementing an Integer Subtractor

. . . . . . . . . . . . . . . . . .

10.2.3 Implementing an Arithmetic Over(cid:13)ow Detector . . . . . . . . . . . .

10.2.4 Implementing Logical Operations . . . . . . . . . . . . . . . . . . . .

10.2.5 Implementing a Shifter . . . . . . . . . . . . . . . . . . . . . . . . . .

10.2.6 Putting It All Together: ALU . . . . . . . . . . . . . . . . . . . . . .

10.2.7 Implementing an Integer Multiplier . . . . . . . . . . . . . . . . . . .

10.2.8 Implementing a Floating-Point Adder

. . . . . . . . . . . . . . . . .

10.2.9 Implementing a Floating-Point Multiplier . . . . . . . . . . . . . . .

10.3 Implementing a Register File . . . . . . . . . . . . . . . . . . . . . . . . . .

10.3.1 Logic-level Design . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.3.2 Transistor-level Design . . . . . . . . . . . . . . . . . . . . . . . . . .

383

384

384

385

387

390

390

393

393

394

395

397

397

399

400

402

403

404

405

405

406

415

416

419

419

419

420

425

425

425

426

429

CONTENTS

10.4 Implementing a Memory System using RAM Cells

. . . . . . . . . . . . . .

10.4.1 Implementing a Memory Chip using RAM Cells

. . . . . . . . . . .

10.4.2 Implementing a Memory System using RAM Chips . . . . . . . . . .

10.4.3 Commercial Memory Modules . . . . . . . . . . . . . . . . . . . . . .

10.5 Implementing a Bus

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.5.1 Bus Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.5.2 Bus Arbitration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.5.3 Bus Protocol: Synchronous versus Asynchronous . . . . . . . . . . .

10.6 Interpreting Microinstructions using Control Signals

. . . . . . . . . . . . .

10.6.1 Control Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.6.2 Control Signal Timing . . . . . . . . . . . . . . . . . . . . . . . . . .

10.6.3 Asserting Control Signals in a Timely Fashion . . . . . . . . . . . .

10.7 Implementing the Control Unit . . . . . . . . . . . . . . . . . . . . . . . . .

10.7.1 Programmed Control Unit: A Regular Control Structure . . . . . . .

10.7.2 Hardwired Control Signal Generator: A Fast Control Mechanism . .

10.7.3 Hardwired versus Programmed Control Units . . . . . . . . . . . . .

10.8 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.9 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A MIPS Instruction Set

B Peripheral Devices

B.1 Types and Characteristics of IO Devices . . . . . . . . . . . . . . . . . . . .

B.2 Video Terminal

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.2.1 Keyboard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.2.2 Mouse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.2.3 Video Display . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.3 Printer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.4 Magnetic Disk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.5 Modem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

431

431

431

432

433

434

435

435

439

439

442

442

443

443

446

449

449

450

451

453

453

454

455

456

457

458

459

460

20

CONTENTS

Chapter 1

Introduction

Let the wise listen and add to their learning, and let the discerning get guidance

Proverbs 1: 5

We begin this book with a broad overview of digital computers This chapter serves
as a context for the remainder of this book.
It begins by examining the nature of the
computing process.
It then discusses the fundamental aspects of digital computers, and
moves on to recent trends in desktop computer systems. Finally, it introduces the concept
of the computer as a hierarchical system. The ma jor levels of this hierarchical view are
introduced. The remainder of the book is organized in terms of these levels.

\The computer is by al l odds the most extraordinary of the technological clothing
ever devised by man, since it is an extension of our central nervous system. Beside
it the wheel is a mere hula hoop...."
| Marshal l McLuhan. War and Peace in the Global Vil lage

Born a few years back, digital computer technology, in cohort with telecommunication
technology, has ushered us into the information age and is exerting a profound in(cid:13)uence
on almost every facet of our daily lives1 . Most of us spend a substantial time every day in
front of a computer (most of it on the internet or on some games!). Rest of the time, we are
on the cell phone or some other electronic device with one or more computers embedded
within. On a more serious note, we are well aware of the critical role played by computers
in (cid:13)ying modern aircraft and spacecraft; in keeping track of large databases such as airline
reservations and bank accounts; in telecommunications applications such as routing and
controlling millions of telephone calls over the entire world; and in controlling power stations
and hazardous chemical plants. Companies and governmental agencies are virtually crippled

1This too shall pass .....

1

2

Chapter 1.

Introduction

when their computer systems go down, and a growing number of sophisticated medical
procedures are completely dependent on computers. Biologists are using computers for
performing extremely complex computations and simulations. Computer designers are using
them extensively for developing tomorrow’s faster and denser computer chips. Publishers
use them for typesetting, graphical picture processing, and desktop publishing. The writing
of this book itself has bene(cid:12)tted substantially from desktop publishing software, especially
Latex. Thus, computers have taken away many of our boring chores, and have replaced
them with addictions such as chatting, browsing, and computerized music.

What exactly is a computer? A computer science de(cid:12)nition would be as follows: a com-
puter is a programmable symbol-processing machine that accepts input symbols, processes
it according to a sequence of instructions called a computer program, and produces the
resulting output symbols. The input symbols as well as the output symbols can represent
numbers, characters, pictures, sound, or other kinds of data such as chess pieces. The most
striking property of the computer is that it is programmable, making it a truly general-
purpose machine. The user can change the program or the input data according to speci(cid:12)c
requirements. Depending on the software run, the end user \sees" a di(cid:11)erent machine; the
computer user’s view thus depends on the program being run on the computer at any given
instant. Suppose a computer is executing a chess program. As far as the computer user
is concerned, at that instant the computer is a chess player because it behaves exactly as
if it were an electronic chess player2 . Because of the ability to execute di(cid:11)erent programs,
a computer is a truly general-purpose machine. The same computer can thus perform a
variety of information-processing tasks that range over a wide spectrum of applications|
for example, as a word processor, a calculator, or a video game|by executing di(cid:11)erent
programs on it; a multitasking computer can even simultaneously perform di(cid:11)erent tasks.
The computer’s ability to perform a wide variety of tasks at very high speeds and with high
degrees of accuracy is what makes it so ubiquitous.

\The computer is only a fast idiot, it has no imagination; it cannot originate action.
It is, and wil l remain, only a tool to man."
| American Library Association’s reaction to the UNIVAC computer exhibit at the
1964 New York World’s Fair

2 In the late 1990s, a computer made by IBM called Deep Thought even defeated the previous World Chess
Champion Gary Kasparov. It is interesting to note, however, that if the rules of chess are changed even
slightly (for example, by allowing the king to move two steps at a time), then current computers will have
a di(cid:14)cult time, unless they are reprogrammed or reconstructed by humans. In contrast, even an amateur
human player will be able to comprehend the new rules in a short time and play a reasonably good game
under the new rules!

1.1. Computing and Computers

3

1.1 Computing and Computers

The notion of computing (or problem solving) is much more fundamental than the notion
of a computer, and predates the invention of computers by thousands of years.
In fact,
computing has been an integral aspect of human life and civilization throughout history.
Over the centuries, mathematicians developed algorithms for solving a wide variety of math-
ematical problems. Scientists and engineers used these algorithms to obtain solutions for
speci(cid:12)c problems, both practical and recreational. And, we have been computing ever since
we entered kindergarten, using (cid:12)ngers, followed later by paper and pencil. We have been
adding, subtracting, multiplying, dividing, computing lengths, areas, volumes and many
many other things. In all these computations, we follow some de(cid:12)nite, unambiguous set of
rules that have been established. For instance, once the rules for calculating the area of a
complex shape have been established|divide it into non-overlapping basic shapes and add
up the areas of the shapes|we can calcuate the area of any complex shape.

A typical modern-day computing problem is much more complex, but works on the same
fundamental principles. Consider a metropolitan tra(cid:14)c control center where tra(cid:14)c video
images from multiple cameras are being fed, and a human operator looks at the images
and takes various tra(cid:14)c control decisions. Imagine automating this process, and letting a
computer do the merging of the images and taking various decisions! How should we go
about designing such a computer system?

1.1.1 The Problem-Solving Process

Finding a solution to a problem, irrespective of whether or not we use a computer, involves
two important phases, as illustrated in Figure 1.1:

(cid:15) Algorithm development

(cid:15) Algorithm execution

We shall take a detailed look at these two phases.

1.1.1.1 Algorithm Development

The (cid:12)rst phase of computing involves the development of a solution algorithm or a step-
by-step procedure that describes how to solve the problem. When we explicitly write down
the rules (or instructions) for solving a given computation problem, we call it an algorithm.
An example algorithm is the procedure for (cid:12)nding the solution of a quadratic equation.
Informally speaking, many of the recipes, procedures, and methods in everyday life are
algorithms.

What should be the granularity of the steps in an algorithm? This depends on the
sophistication of the person or machine who will execute it, and can vary signi(cid:12)cantly from

4

Chapter 1.

Introduction

Problem

Algorithm Development

Input Data

Algorithm

Algorithm Execution

Output Data (Results)

Figure 1.1: The Problem Solving Process

one algorithm to another; a step can be as complex as (cid:12)nding the solution of a sub-problem,
or it can be as simple as an addition/subtraction operation. Interestingly, an addition step
itself can be viewed as a problem to be solved, for which a solution algorithm can be
developed in terms of 1-bit addition with carry-ins and carry-outs. It should also be noted
that one may occasionally tailor an algorithm to a speci(cid:12)c set of input data, in which case
it is not very general.

Algorithm development has always been done with human brain power, and in all likeli-
hood will continue like that for years to come! Algorithm development has been recorded as
early as 1800 B.C., when Babylonian mathematicians at the time of Hammurabi developed
rules for solving many types of equations [4]. The word \algorithm" itself was derived from
the last name of al-Khw^arizm^i, a 9th century Persian mathematician whose textbook on
arithmetic had a signi(cid:12)cant in(cid:13)uence for more than 500 years.

1.1.1.2 Algorithm Execution

Algorithm execution|the second phase of the problem-solving process|means applying
a solution algorithm on a particular set of input values, so as to obtain the solution of
the problem for that set of input values. Algorithm development and execution phases
are generally done one after the other; once an algorithm has been developed, it may be
executed any number of times with di(cid:11)erent sets of data without further modi(cid:12)cations.
However, it is possible to do both these phases concurrently, in a lock-step manner! This
typically happens when the same person performs both phases, and is attempting to solve
a problem for the (cid:12)rst time.

The actions involved in algorithm execution can be broken down into two parts, as
illustrated in Figure 1.2.

(cid:15) Sequencing through the algorithm steps: This part involves selecting from the algo-
rithm the next step to be executed.

1.1. Computing and Computers

5

(cid:15) Executing the next step of the algorithm, as determined by the sequencing part.

Algorithm

Input Data

Determine Next Step

Step

Data

Execute the Step

Output Data (Results)

Figure 1.2: The Algorithm Execution Process

For hundreds of years, people relied mainly on human brain power for performing both of
these parts. As centuries went by (and the gene pool deteriorated), a variety of computing
aids were invented to aid human brains in executing the individual steps of solution algo-
rithms. The Chinese abacus and the Japanese soroban were two of the earliest documented
aids used for doing the arithmetic calculations speci(cid:12)ed in algorithm steps. The slide rule
was a more sophisticated computing aid invented in the early 1600s by William Oughtred,
an English clergyman; it helped to perform a variety of computation operations including
multiplication and division. Later examples of computing aids included Pascaline, the
mechanical adder built in 1642 by the French mathematician Blaise Pascal (to assist his
father in adding long columns of numbers in the tax accounting o(cid:14)ce) and the stepped-
wheel machine of Gottfried Wilhelm Leibniz in 1672 (which could perform multiplication
and division in addition to addition and subtraction).

As problems increased in complexity, the number of steps required to solve them also
increased accordingly. Several mechanical and electrical calculators were commercially pro-
duced in the 19th century to speed up speci(cid:12)c computation steps. The time taken by a
calculator to perform a computation step was in the order of a few milliseconds, in contrast
to the several seconds or minutes taken by a person to perform the same step. It is impor-
tant to note that even after the introduction of calculators, the sequencing part of algorithm
execution was still done by people, who punched in the numbers and the operations. It is
also important to note that the granularity of the steps in an algorithm is related to the
capabilities and sophistication of the calculating aids used. Thus, a typical calculator lets
us specify algorithm steps such as multiplication and square root, for instance, whereas an
abacus can perform only more primitive computation steps.

1.1.2 Automating Algorithm Execution with Computers

We saw that calculators and other computing aids allowed an algorithm’s computation
steps to be executed much faster than what was possible without any computing aides.
However, the algorithm execution phase still consumed a signi(cid:12)cant amount of time for

6

Chapter 1.

Introduction

the following reasons: (i) the sequencing process was still manual, and (ii) the execution of
each computation step involved manual inputting of data into the calculating aid. Both of
these limitations can be overcome if the sequencing process is automated by means of an
appropriate machine, and the data to be processed is stored in the machine itself. This is
the basic idea behind computers.

\Stripped of its interfaces, a bare computer boils down to little more than a pocket
calculator that can push its own buttons and remember what it has done."
{ Arnold Penzias.
Ideas and Information.

One of the earliest attempts to automate algorithm execution was that of Charles Bab-
bage, a 19th century mathematics professor. He developed a mechanical computing ma-
chine called Difference Engine. This computer was designed to execute only a single
algorithm|the method of ((cid:12)nite) di(cid:11)erences using polynomials. Although this algorithm
used only addition and subtraction operations, it permitted many complex and useful func-
tions to be calculated.
(Chapter 1 of [2] provides a good description of the use of this
algorithm in calculating di(cid:11)erent functions.) The Difference Engine performed the se-
quencing process automatically, in addition to performing the operation speci(cid:12)ed in each
computation step. This is a ma jor advantage because it allows the algorithm execution
phase to be performed at machine speeds, rather than at the speed with which it can be
done manually. One limitation of executing a single algorithm, however, is that only a few
problems can be solved by a single algorithm; such a computing machine is therefore not
useful for general-purpose computing.

After a few years, Babbage envisioned the Analytical Engine, another massive brass,
steam-powered, mechanical (digital) computing machine. The radical shift that it intro-
duced was to have the machine accept an arbitrary solution algorithm (in punched card
format), and execute the algorithm by itself. This approach allows arbitrary algorithms to
be executed at the speed of the machine, making the machine a general-purpose computer.
The radical idea embodied in the Analytical Engine was the recognition that a machine
could be \programmed" to perform a long sequence of arithmetic and decision operations
without human intervention.

\What if a calculating engine could not only foresee but could act on that foresight?"
{ Charles Babbage. November 1834.

The Analytical Engine served as a blueprint for the (cid:12)rst real programmable computer,
which came into existence a century later3 . The basic organization proposed by Babbage
is given in Figure 1.3. The main parts are the mill, the store, the printer and card punch,
the operation cards, and the variable cards. The instructions were given to the machine on
punch cards, and the input data was supplied through the variable cards. Punched cards had

3Primitive forms of \programmable" machines had existed centuries ago, dating back to Al-Jazari’s
musical automata in the 13th century and even to Heron’s mobile automaton in the 1st century.

1.1. Computing and Computers

7

been recently invented by Jacquard for controlling weaving looms. Augusta Ada, Countess
of Lovelace as well as a mathematician, was one of the few people who fully understood
Babbage’s vision. She helped Babbage in designing the Analytical Engine’s instruction set,
and in describing, analyzing, and publicizing his ideas.
In an article published in 1843,
she predicted that such a machine might be used to compose complex music, to produce
graphics, and would be used for both practical and scienti(cid:12)c use. She also created a plan
for how the engine might calculate a mathematical sequence known as Bernoulli numbers.
This plan is now regarded as the (cid:12)rst \computer program," and Ada is credited as the (cid:12)rst
computer programmer.

\The Analytical Engine has no pretensions whatever to originate anything. It can
do whatever we know how to order it to perform."

\Supposing, for instance, that the fundamental relations of pitched sounds in the
science of harmony and of musical composition were susceptible of such expression
and adaptations, the engine might compose elaborate and scienti(cid:12)c pieces of music
of any degree of complexity or extent."
| Countess Ada Lovelace

Mill

(Arithmetic/
Logic Unit)

Data

Store

(Main
Memory)

Printer and
Card Punch

(Output Unit)

Instructions

Operation
Cards

Variable
Cards

Program

Figure 1.3: Basic Organization of Babbage’s Analytical Engine

Automated algorithm execution has two side-e(cid:11)ects that we need to keep in mind. First,
it forces the algorithm development and algorithm execution phases to happen one after
the other. It also implies that the algorithm must allow for the occurrence of all possible
inputs. Hence computer algorithms are seldom developed to solve just a single instance
of a problem; rather they are developed to handle di(cid:11)erent sets of input values. Thus, in
moving from the manual approach to the automated approach, we are forced to sacri(cid:12)ce
the versatility inherent in the concurrent development and execution of an algorithm. The
big gain, however, is in the speed and storage capabilities o(cid:11)ered by the computer machine.

Another side-e(cid:11)ect of automated algorithm execution is that for a machine to follow
an algorithm, the algorithm must be represented in a formal and detailed manner: the less

8

Chapter 1.

Introduction

sophisticated the follower, the more detailed the algorithm needs to be! Detailed algorithms
written for computers are called computer programs. By de(cid:12)nition, a computer program
is an expression of an algorithm in a computer programming language, which is a precise
language that can be made understandable to a computer. Because of the extensive e(cid:11)orts
involved in developing a computer program to make it suitable for execution in a computer,
the program itself is often developed with a view to solve a range of related problems rather
than a single problem. For instance, it may not be pro(cid:12)table to develop a computer program
to process a single type of bank transaction; instead, it is pro(cid:12)table to develop the program
with the ability to process di(cid:11)erent types of transactions.

In spite of these minor side-e(cid:11)ects, the idea of using computers to perform automated
algorithm execution has been found to have tremendous potential. First of all, once an
algorithm has been manually developed to solve a particular problem, computers can be used
to execute the algorithm at very high speeds. This makes it possible to execute long-running
algorithms that require billions of operations, which previously could never be executed in
a reasonable period of time4 . In fact, a lion’s share of computer development took place in
the 1930s and 1940s, mostly in response to computation problems that arose in the WW
II e(cid:11)ort, such as ballistic computations and code-breaking. The ability to execute complex
algorithms in real-time is the leading reason for the acceptance of computers in many
embedded applications, such as automobiles and aircraft. Secondly, the same computing
machine can be used to execute di(cid:11)erent algorithms at di(cid:11)erent times, thus having a truly
general-purpose computing machine. Thirdly, computers are immune to emotional and
physical factors such as distraction and fatigue, and can provide accurate and reliable results
almost all the time5 . Finally, embedded applications often involve working in hazardous
environments where humans cannot go, and computers are good candidates for use in such
environments.

At this stage, it is instructive to contrast the computing machine against other types of
machines such as clocks, which predate the computer by hundreds of years. Such machines
are constructed to perform a speci(cid:12)c sequence of internal actions to solve a speci(cid:12)c problem.
For instance, the hands of a clock go around at (cid:12)xed speeds; this is in fact a mechanical
implementation of an algorithm to keep track of time. A digital clock keeps track of time
using a quartz crystal and digital circuitry. Such machines can only do the one thing they
are constructed to do. A computing machine, on the other hand, is general-purpose in that
it can perform a large variety of widely di(cid:11)ering functions, based on the algorithm that
it is operating upon at any given time. Because the algorithm can be changed, di(cid:11)erent
functions can be implemented by acquiring a single hardware system and then developing
di(cid:11)erent algorithms to perform di(cid:11)erent functions in the hardware. Thus, by executing a

4 Interestingly, even now, at a time when computers have become faster by several orders of magnitude,
there are prodigies like Sakuntala Devi [] who have demonstrated superiority over computers in performing
certain kind of complex calculations!
5We should mention that computers are indeed susceptible to some environmental factors such as elec-
trical noise and high temperatures. Modern computers use error-correcting codes and other fault tolerance
measures to combat the e(cid:11)ect of electrical noise and other environmental e(cid:11)ects.

1.2. The Digital Computer

9

computer program for keeping track of time, a computer can implement a clock! This feature
is the crucial di(cid:11)erence between general-purpose computing machines and special-purpose
machines that are geared to perform speci(cid:12)c functions.

We have described some of the landmark computers in history. Besides the few comput-
ers mentioned here, there are many other precursors to the modern computer. Extensive
coverage of these computers can be found in the IEEE Annals of the History of Computing,
now in its 28th volume [ref ].

\Computers in the future wil l weigh no more than 0.5 tons."
| Popular Mechanics: Forecasting Advance of Science, 1949

1.2 The Digital Computer

We saw how computers play a ma jor role in executing algorithms or programs to obtain
solutions for problems. Solving a problem involves manipulating information of one kind
or other. In order to process information, any computer|mechanical or electrical|should
internally represent information by some means. Some of the early computers were analog
computers, in that they represented information by physical quantities that can take values
from a continuum, rather than by numbers or bit patterns that represent such quantities.
Physical quantities can change their values by an arbitrarily small amount; examples are the
rotational positions of gears in mechanical computers, and voltages in electrical computers.
Analog quantities represent data in a continuous form that closely resemble the information
they represent. The electrical signals on a telephone line, for instance, are analog-data
representations of the original voices. Instead of doing arithmetic or logical operations, an
analog computer uses the physical characteristics of its data to determine solutions. For
instance, addition could be done just by using a circuit whose output voltage is the sum of
its input voltages.

Analog computers were a natural outcome of the desire to directly model the smoothly
varying properties of physical systems. By making use of di(cid:11)erent properties of physical
quantities, analog computers can often avoid time-consuming arithmetic and logical opera-
tions. Although analog computers can nicely represent smoothly changing values and make
use of their properties, they su(cid:11)er from the di(cid:14)culty in measuring physical quantities pre-
cisely, and the di(cid:14)culty in storing them precisely due to changes in temperature, humidity,
and so on. The subtle errors introduced to the stored values due to such noise are di(cid:14)cult
to detect, let alone correct.

The 20th century saw the emergence of digital computers, which eventually replaced
analog computers in the general-purpose computing domain. Digital computers represent
and manipulate information using discrete elements called symbols. A ma jor advantage of
using symbols to represent information is resilience to error. Even if a symbol gets distorted,
it can still be recognized, as long as the distortion does not cause it to appear like another

10

Chapter 1.

Introduction

symbol. This is the basis behind error-correcting features used to combat the e(cid:11)ects of
electrical noise in digital systems. Representing information in digital format has a side-
e(cid:11)ect, however. As we can only have a limited number of bits, only a (cid:12)nite number of values
can be uniquely represented. This means that some of the values can be represented with
high degree of precision, whereas the remaining ones will need to be approximated.

Electronic versions of the digital computer are typically built out of a large collection
of electronic switches, and use distinct voltage states (or current states) to represent dif-
ferent symbols. Each switch can be in one of two positions, on or o(cid:11); designing a digital
computer will therefore be a lot simpler if it is restricted to handling just two symbols.
So most of the digital computers use only two symbols in their alphabet and are binary
systems, although we can design computers and other digital circuits that handle multiple
symbols with multiple-valued logic. The two symbols of the computer alphabet are usu-
ally represented as 0 and 1; each symbol is called a binary digit or a bit. Computers often
need to represent di(cid:11)erent kinds of information, such as instructions, integers, (cid:13)oating-point
numbers, and characters. Whatever be the type of information, digital computers repre-
sent them by concatenations of bits called bit patterns, just like representing information in
English by concatenating English alphabets and puctuation marks. The (cid:12)nite number of
English alphabets and puctuation marks do not impose an inherent limit on how much we
can communicate in English; similarly the two symbols of the computer alphabet do not
place any inherent limits on what can be communicated to the digital computer. Notice,
however, that information in the computer language won’t be as cryptic as in English, just
like inofmration in English is not as cryptic as in Chinese (which has far more symbols).

\Even the most sophisticated computer is real ly only a large, wel l-organized volume
of bits."
| David Harel. Algorithmics: The Spirit of Computing

By virtue of their speed and other nice properties, these electronic versions completely
replaced mechanical and electromechanical versions. At present, the default meaning of the
term \computer" is a a general-purpose automatic electronic digital computer.

1.2.1 Representing Programs in a Digital Computer: The Stored Pro-
gram Concept

We saw that a computer solves a problem by executing a program with the appropriate set
of input data. How is this program conveyed to the computer from the external world? And,
how is it represented within the computer? In the ENIAC system developed at University of
Pennsylvania in early 1940s, for instance, the program was a function of how its electrical
circuits were wired, i.e., the program was a function of the physical arrangement of the
cables in the system. The steps to be executed were speci(cid:12)ed by the connections within the
hardware unit. Every time a di(cid:11)erent program needed to be executed, the system had to be
rewired. Conveying a new program to the hardware sometimes took several weeks! Other

1.2. The Digital Computer

11

early computers used plug boards, punched paper tape, or some other external means to
represent programs. Developing a new program involved re-wiring a plugboard, for instance.
And, loading a program meant physically plugging in a patch board or running a paper
tape through a reader.

A marked change occurred in the mid-1940s when it was found that programs could be
represented inside computers in the same manner as data, i.e., by symbols or bit patterns.
This permits programs to be stored and transfered like data. This concept is called the
stored program concept, and was (cid:12)rst described in a landmark paper by Burks, Goldstein,
and von Neumann in 1946 [1].
In a digital computer implementing the stored program
concept, a program will be a collection of bit patterns. When programs are represented
and stored as bit patterns, a new program can be conveyed to the hardware very easily.
Moreover, several programs can be simultaneously stored in the computer’s memory. This
makes it easy not only to execute di(cid:11)erent programs one after the other, but also to switch
from one program to another and then back to the (cid:12)rst, without any hardware modi(cid:12)cation.
Stored program computers are truly \general-purpose," as they can be easily adapted to do
di(cid:11)erent types of computational and information storage tasks. For instance, a computer
can instantaneously switch from being a word processor to a telecommunications terminal,
a game machine, or a musical instrument! Right from its inception, the stored program
concept was found to be such a good idea that it has been the basis for virtually every
general-purpose computer designed since then. In fact it has become so much a part of the
modern computer’s functioning that it is not even mentioned as a feature!

In a stored program computer, the program being executed can even manipulate another
program as if it were data|for example, load it into the computer’s memory from a storage
device, copy it from one part of memory to another, and store it back on a storage device.
Altering a program becomes as easy as modifying the contents of a portion of the computer’s
memory. The ability to manipulate stored programs as data gave rise to compilers and
assemblers that take programs as input and translate them into other languages.

The advent of compilers and assemblers have introduced several additional steps in
solving problems using modern digital computers. Figure 1.3 depicts the steps involved in
solving a problem using today’s computers. First, an algorithm, or step-by-step procedure,
is developed to solve the problem. Then this algorithm is expressed as a program in a
high-level programming language by considering the syntax rules and semantic rules of the
programming language. Some of the common high-level languages are C, FORTRAN, C++,
Java, and VisualBasic.

The source program in the high-level language is translated into an executable program
(in the language of the machine) using programs such as compilers, assemblers, and linkers.
During this compilation process, syntax errors are detected, which are then corrected by the
programmer. Once the syntax errors are corrected, the program is re-compiled. Once all
syntax errors are corrected, the compiler produces the executable program. The executable
program can be executed with a set of input values on the computer to obtain the results.
Semantic errors manifest as run-time errors, and are corrected by the programmer.

12

Chapter 1.

Introduction

Problem

Algorithm Development

Human

Algorithm

Program Development

Human

Source Program

Program Translation

Program

Syntax
Errors

Semantic
Errors

Input Data

Executable Program

Program Execution

Hardware

Output Data (Results)

Figure 1.4: Basic Steps in Solving a Problem using a Computer

1.2.2 Basic Software Organization

As discussed in the previous section, today’s computers use the stored program concept.
Accordingly the software consists of symbols or bit patterns that can be stored in storage
devices such as CD-ROMs, hard disks, and (cid:13)oppy disks. A program consists of two parts|
instructions and data|both of which are represented by bit patterns. The instructions
indicate speci(cid:12)c operations to be performed on individual data items. The data items can
be numeric or non-numeric.

It is possible to write stand-alone programs that can utilize and manage all of the system
resources, so as to perform the required task. This is commonly done in controllers and
embedded computers, which typically store a single program in a ROM (read-only memory),
and run the same program forever. In the mid and higher end of the computer spectrum,
starting with some embedded computers, a dichotomy is usually practiced, however, for
a variety of reasons.
Instead of writing stand-alone programs that have the ability to
access and control all of the hardware resources, the access and control of many of the
hardware resources (typically IO devices) are regulated through a supervisory program
called the operating system. When a program needs to access a regulated hardware resource,
it requests the operating system, which then provides the requested service if it is legitimate
request. This dichotomy has led to the development of two ma jor kinds of software|user
programs and kernel programs|as shown in Figure 1.4.

The operating system is one of the most important pieces of software to go into a modern
computer system. It provides other programs a uniform software interface to the hardware

1.2. The Digital Computer

13

User programs

Kernel programs

(Application software)

(Operating system)

Figure 1.5: Basic Software Organization in a Digital Computer

resources. In addition to providing a standard interface to system resources, in multitasking
environments, the operating system enables multiple user programs to share the hardware
resources in an orderly fashion6 . This sharing increases overall system performance, and
ensures security and privacy for the individual programs. To do this sharing in a safe and
e(cid:14)cient manner, the operating system is the software that is \closest to the hardware". All
other programs use the OS as an interface to shared resources and as a means to support
sharing among concurrently executing programs.

A hardware timer periodically interrupts the running program, allowing the processor to
run the operating system. The operating system then decides which of the simultaneously
active application programs should be run next on the processor; it takes this decision with
a view to minimize processor waste time. Peripheral devices also interrupt the running
program, at which times the operating system intervenes and services the devices.

For similar reasons, memory management and exception handling functions are also
typically included in the operating system. Memory management involves supporting a large
virtual memory address space with a much smaller physical memory, and also sharing the
available physical memory among the simultaneously active programs. Exception handling
involves dealing with situations that cause unexpected events such as arithmetic over(cid:13)ow
and divide by zero.

1.2.3 Basic Hardware Organization

Even the most complex software, with its excellent abstraction and generality features, is
only like the mental picture an artist has before creating a masterpiece. By itself it does
not solve any problem. For it to be productive, it must be eventually executed on suitable
hardware with proper data, just like the artist executing his/her mental picture on a suitable
canvas with proper paint. The hardware is thus an integral part of any computer system.

\You’l l never plow a (cid:12)eld by turning it over in your mind."
| An Irish Proverb

While nearly every class of computer hardware has its own unique features, from a func-

6Even in multitasking computers, hardware diagnostic programs are often run entirely by themselves,
with no intervention from the operating system.

14

Chapter 1.

Introduction

tional point of view (i.e, from the point of view of what the ma jor parts are supposed to
do), the basic organization of modern computers|given in Figure 1.5|is still very similar
to that of the Analytical Engine proposed in the 19th century. This organization consists
of three functionally independent parts: the CPU (central processing unit), the memory
unit, and the input/output unit. The actions performed by the computer are controlled
and co-ordinated by the program that is currently being executed by the CPU. The in-
put/output unit is a collection of diverse devices that enable the computer to communicate
with the outside world. Standard input/output devices include the keyboard, the mouse,
the monitor, and so on. Programs and data are brought into the computer from the external
world using the input devices and their controllers. The input unit’s function is to accept
information from human users or other machines, through devices such as the keyboard, the
mouse, the modem, and the actuators. The results of computations are sent to the outside
world through the output unit. Some of the input/output devices are storage devices, such
as hard disks, CD-ROMs, and tapes, which can store information for an inde(cid:12)nite period
of time.

CPU

Data Path

Control Unit

Memory

Input

Output

Storage

Figure 1.6: Basic Hardware Organization of a Digital Computer

When the program and data are ready to be used, they are copied into the memory unit
from either the external environment or a storage device. The memory unit stores two types
of information: (i) the instructions of the program being executed, and (ii) the data for the
program being executed. The CPU executes a memory-resident program by reading the
program instructions and data from the memory. The execution of the program is carried
out by the CPU’s control unit, which reads each instruction in the program, decodes the
instruction, and causes it to be executed in the data path. The control unit is the brain of
the system, and behaves like a puppeteer who pulls the right strings to make the puppets
behave exactly as needed.

It is interesting to compare and contrast this organization with that of a human being’s
information processing system, which among other things, involves the brain. The main
similarity lies in the way information is input and output. Like the digital computer, the
human information processing system obtains its inputs through its input unit (the sense
organs), and provides its outputs through its output unit by way of speech and various

1.2. The Digital Computer

15

motions. The dissimilarity lies both in the organization of the remaining parts and in the
way information is stored and processed. All of the information storage and processing
happens in a single unit, the brain. Again, the brain stores information not as 0s and 1s in
memory elements, but instead by means of its internal connectivity. Information processing
is done in the brain on a massively parallel manner. This is in contrast to how information
processing is done in a digital computer, where the information is stored in di(cid:11)erent memory
units from where small pieces are brought into the CPU and processed 7 .

1.2.4 Software versus Hardware

Software consists of abstract ideas, algorithms, and their computer representations, namely
programs. Hardware, in contrast, consists of tangible ob jects such as integrated circuits,
printed circuit boards, cables, power supplies, memories, and printers. Software and hard-
ware aspects are intimately tied together, and to achieve a good understanding of computer
systems, it is important to study both, especially how they integrate with each other.
Therefore, the initial portions of this book deal with software and programming, and the
latter portions deal with the hardware components. This introductory chapter introduces a
number of software and hardware concepts, and gives a broad overview of the fundamental
aspects of both topics. More detailed discussions follow in subsequent chapters.

The boundary between the software and the hardware is of particular interest to systems
programmers and compiler developers.
In the very (cid:12)rst computers, this boundary|the
instruction set architecture|was quite clear; the hardware presented the programmer with
an abstract model that took instructions from a serial program one at a time and executed
them in the order in which they appear in the program. Over time, however, this boundary
blurred considerably, as more and more hardware features are exposed to the software, and
hardware design itself involves software programming techniques. Nowadays, it is often
di(cid:14)cult to tell software and hardware apart, especially at the boundary between them. In
fact, a central theme of this book is:
Hardware and software are logical ly equivalent.

Any operation performed by software can also be built directly into the hardware. Em-
bedded systems, which are more specialized than their general-purpose counterpart, tend to
do more through hardware than through software. In general, new functionality is (cid:12)rst in-
troduced in software, as it is likely to undergo many changes. As the functionality becomes
more standard and is less likely to change, it is migrated to hardware.

\Hardware is petri(cid:12)ed software."
| ????

Of course, the reverse is also true: Any instruction executed by the hardware can also

7Research is under way to develop computers made from quantum circuits, and even biological circuits.
In the next decade, we may very well have computers made with such \hardware", and working with di(cid:11)erent
computation models!

16

Chapter 1.

Introduction

be simulated in software. Suppose an end user is using a computer to play a video game.
It is possible to construct an electronic circuit to directly handle video games, but this is
seldom done. Instead a video game program is executed to simulate a video game. The
decision to put certain functions in hardware and others in software is based on such factors
as cost, speed, reliability, and frequency of expected changes. These decisions change with
trends in technology and computer usage.

1.2.5 Computer Platforms

Classi(cid:12)cation is fundamental to understanding anything that comes in a wide variety. Auto-
mobiles can be classi(cid:12)ed according to manufacturer, body style, pickup, and size. Students
often classify university faculty based on their teaching style, sense of humor, and strictness
of grading. They classify textbooks according to price, contents, and ease of understanding.
Likewise, computers come in various sizes, speeds, and prices, from small-scale to large-
scale. Table 1.1 gives a rough categorization of today’s computers. This categorization is
somewhat idealized. Within each category, there is wide variability in features and cost; in
practice the boundary between two adjacent categories is also somewhat blurred. The ap-
proximate price (cid:12)gures in the table are only intended to show order of magnitude di(cid:11)erences
between di(cid:11)erent categories. All computers are functionally similar, irrespective of where
they line up in the spectrum. The general principles of computer architecture and organi-
zation are the same for the entire computer spectrum, from workstations to multiprocessors
and distributed computer systems.

Category

Price Typical applications

Disposable computer
Embedded computer
Entertainment PC
Desktop or laptop PC
Server
Collection of workstations
Mainframe
Supercomputer

$1 Greeting cards, watches
$10 Home appliances, cars
$100 Home video games
$1000 Word processing, CAD design
$10,000 Network server
$100,000 LAN
$1,000,000 Bank accounts, airline reservations
$10,000,000 Weather forecast, oil exploration

Table 1.1: Di(cid:11)erent categories of currently available computers

At one end of the spectrum we have disposable computers like the ones used in greeting
cards, inexpensive watches, and other similar applications. These are quite inexpensive
because they use a single chip with small amounts of memory, and are produced in large
quantities. Then there are a wide variety of embedded computers, used in applications such
as automobiles and home appliances. The entertainment PCs are computer systems that
are optimized for games, personal communications, and video playback. They typically

1.3. A Modern Computer System

17

have high-quality graphics, video, and audio so as to support high clarity and realism.
Desktop computers and laptop computers are typically intended for a single user to run
applications such as word processing, web browsing, and receiving/sending email. These
computers come with di(cid:11)erent features and costs.
In the immediately higher category,
we have servers. A server is a high-performance computer that serves as a gateway in a
computer network. At the other end of the spectrum, we have the supercomputers, which
are used for applications involving very complex calculations, such as weather prediction
and nuclear explosion modeling. The lower end of the spectrum often provides the best
price/performance ratio, and the decision on which system to purchase is often dictated by
such issues as software and ob ject code compatibility.

1.3 A Modern Computer System

As discussed earlier, computers come in various sizes and kinds. Among these, perhaps the
most commonly seen one and one that comes to mind vividly when one thinks of a computer,
is a desktop computer. Desktop computers are designed to be truly general-purpose. For
these reasons, we provide a detailed description of a typical desktop computer system in
this section. In fact, many of the issues discussed here are applicable to all members of the
computer spectrum.

1.3.1 Hardware

Figure 1.6 shows a typical desktop computer. It has a system unit which is the case or
box that houses the motherboard, other printed circuit boards, the storage devices, and the
power supply. The system unit is generally designed in such a way that it can be easily
opened to add or replace modules. The di(cid:11)erent components in the system unit are typically
connected together using a bus, which is a set of wires for transferring electrical signals. Each
printed circuit board houses a number of chips, some of which are soldered and the rest are
plugged into the board. The latter permits the user to upgrade the computer components.
Circuits etched into the boards act like wires, providing a path for transporting data from
one chip to another.

Figure 1.7: Photograph of a Typical Desktop Computer System

Processor: The processor, also called the central processing unit (CPU), is perhaps the
most important part of a computer. It carries out the execution of the instructions of a
program.

18

Chapter 1.

Introduction

Chip Sets: The chipsets provide hardware interfaces for the processor to interact with
other devices, such as DRAM and graphics cards.

Motherboard: The motherboard is the main printed circuit board, and holds the com-
puter’s processor chip(s), ROM chips, RAM chips, and several other key electronic com-
ponents. The processor is an important part of a computer, and can be a single chip or
a collection of chips. ROM chips typically contain a small set of programs that start the
computer, run system diagnostics, and control low-level input and output activities. These
programs are collectively called BIOS (basic input output system) in PCs. The instructions
in the ROM chips are permanent, and the only way to modify them is to reprogram the
ROM chips. RAM chips are volatile and hold program and data that is temporary in na-
ture. A battery powered real-time clock chip keeps track of the current date and time. The
motherboard also typically contains expansion slots, which are sockets into which expansion
cards such as video card, sound card, and internal modem, can be plugged in. An expansion
card has a card edge connector with metal contacts, which when plugged into an expansion
slot socket, connect the circuitry on the card to the circuitry on the motherboard. The
number of expansion slots in the motherboard determines its expandability.

Figure 1.8: Photograph of a Motherboard

Storage Devices: The commonly used storage devices are (cid:13)oppy disk drives, hard disk
drives, CD-ROM drives, and ZIP drives. A (cid:13)oppy disk drive is a device that reads and
writes data on (cid:13)oppy disks. A typical (cid:13)oppy disk drive uses 3 1
2 -inch (cid:13)oppy disks each of
which can store up to 1.44 MB. A hard disk drive can store billions of bytes on a non-
removable disk platter. A CD-ROM drive is a storage device that uses laser technology to
read data from a CD-ROM. The storage devices are typically mounted in the system unit.
The ones involving removable media such as the (cid:13)oppy disk drive, the CD-ROM drive, and
the ZIP drive are mounted on the front side of the system unit, and the hard disk drives
are typically mounted inside the system unit.

Input/Output Devices: Two of the commonly used input devices in a desktop computer
are the keyboard and the mouse. A computer keyboard looks similar to that of a typewriter,
with the addition of number keys, as well as several additional keys that control computer-
speci(cid:12)c tasks. The mouse is useful in manipulating ob jects depicted on the screen. Other
commonly used input device is the microphone. The primary output device in a desktop
computer is the monitor, a display device that forms an image by converting electrical
signals from the computer into points of colored light on the screen. Its functioning is very
to a television picture tube, but has a much higher resolution so that a user sitting at close
quarters can clearly see computer-generated data such as text and images. Other frequently
used output devices are the printer and the speakers.

1.3. A Modern Computer System

19

Device Controllers: Each device|keyboard, mouse, printer, monitor, etc|requires spe-
cial control ler circuitry for transferring data from the processor and memory to the device,
and vice versa. A device controller is designed either as a chip which is placed in the
motherboard or as a printed circuit board which is plugged into an expansion slot of the
motherboard. The peripheral devices are connected to their respective controllers in the
system unit using special cables to sockets called expansion ports. The ports are located
on the backside of the system unit and provide connections through holes in the back of
the system unit. Parallel ports transfer several bits simultaneously and are commonly used
to connect printers to the computer. Serial ports transfer a single bit at a time, and are
commonly used to connect mice and communication equipment to the computer. Device
controllers are very complex. Each logical command from the processor must typically be
decomposed into long sequences of low-level commands to trigger the actions to be per-
formed by the device and to supervise the progress of the operation by testing the device’s
status. For instance, to read a word from a disk, the disk controller generates a sequence of
commands to move the read/write arm of the disk to the correct track, await the rotational
delay until the correct sector passes under the read/write arm, transfer the word, and check
for a number of possible error conditions. A sound card contains circuitry to convert digital
signals from the computer to sounds that play through speakers or headphones that are
connected to the expansion ports of the card. A modem card connects the computer to the
telephone system so as to transport data from one computer to another over phone lines.
A network card, on the other hand, provides the circuitry to connect a computer to other
computers on a local area network.

1.3.2 Software

A desktop computer typically comes with pre-installed software. This software can be
categorized into two categories|application software and systems software.

Application Software: Application programs are designed to satisfy end-user needs by
operating on input data to perform a given job, for example, to prepare a report, update
a master payroll (cid:12)le, or print customer bills. Application software may be packaged or
custom. Packaged software includes programs pre-written by professional programmers,
and are typically o(cid:11)ered for sale in a (cid:13)oppy disk or CD-ROM. Custom software includes
programs written for a highly specialized task.

Systems Software: Systems software enables the application software to interact with
the computer, and helps the computer manage its internal and external resources. Systems
software is required to run applications software; however, the converse is not true. Systems
software can be classi(cid:12)ed into three types|utility programs, language translators, and the
operating system. Utility programs are generally used to support, enhance, or expand the
development of application programs. Examples consist of editors and programs for merging

20

Chapter 1.

Introduction

(cid:12)les. A language translator or compiler is a software program that translates a program
written in a high-level language such as C into machine language, which the hardware
can directly execute. Thus a compiler provides the end user with the capability to write
programs in a high-level language.

Operating System: The operating system is a ma jor component of the systems software.
Desktop operating systems allocate and control the use of all hardware resources:
the
processor, the main memory, and the peripheral devices. They also add a variety of new
features, above and beyond what the hardware provides. Running the shell provides the end
user with a more \capable" machine, in that the computer system provides direct capability
to specify commands by typing them on a keyboard. The GUI (graphical user interface)
goes one step further by providing the user with a graphical view of the desktop, and letting
the user enter commands by clicking on icons. The multitasking feature of the OS provides
the user with the capability to run multiple tasks \concurrently". The (cid:12)le system of the OS
provides the user with a structured way of storing and accessing \permanent" information.
The operating system is thus an important part of most computer systems because it exerts
a ma jor in(cid:13)uence on the overall function and performance of the entire computer. Normally,
the OS is implemented in software, but there is no theoretical reason why it could not be
implemented in hardware!

Device Driver (Software Driver): Most application programs need to access input/output
devices and storage devices such as disks, terminals, and printers. Allowing these programs
to perform the low-level IO activity required to directly control an input/output device
is not desirable for a variety of reasons. First, most application programmers would (cid:12)nd
it extremely di(cid:14)cult to do the intricate actions required to directly control an IO device.
Second, inappropriate accesses of the IO devices by amateur or malicious programmers can
wreck plenty of havoc. The standard solution adopted in computer systems is therefore to
provide a more abstract interface to the application programmer, and let an interface pro-
gram perform the required low-level IO activity. This interface program is called a device
driver or software driver. Each device requires speci(cid:12)c device driver software, because
each device has its own speci(cid:12)c commands whereas an application program uses generic
commands. The device driver receives generic commands from the application program
and converts them into the specialized commands for the device, and vice versa.

1.3.3 Starting the Computer System: The Boot Process

Now that you have a good understanding of the role of an operating system in a modern
computer, it would be interesting to learn how the operating system is activated each time
a computer is turned on. When a computer is turned o(cid:11), the data in the registers and
memory are lost. Thus when the computer is turned on, the OS program is not residing in
the main memory, and needs to be brought into main memory from a storage device such as

1.3. A Modern Computer System

21

a diskette or hard disk. In modern computers, this copying is done by executing a program
called the bootstrap program or boot program for short. How can the computer execute
this copy program if the memory contains no useful contents? To solve this dilemma, a
portion of the memory is implemented using non-volatile memory devices such as a read-
only memory (ROM). This memory contains the boot program. When the computer is
turned on, it starts executing instructions from the starting address of the boot program.
The boot program contains code to perform diagnostic tests of crucial system components
and load the operating system from a disk to the main memory. This bootstrap loader may
be comprehensive enough to copy the nucleus of the operating system into memory. Or it
may (cid:12)rst store a more comprehensive loader that, in turn, installs the nucleus in memory.
Once loaded, the OS remains in main memory until the computer is turned o(cid:11).

For copying the OS from a disk drive to the RAM, the computer needs to know how
the disk has been formatted, i.e., the number of tracks and sectors and the size of each
sector.
If information about the hard disk were stored in the ROM, then replacing the
hard disk becomes a di(cid:14)cult proposition, because the computer will not be able to access
the new hard disk with information about the old disk. Therefore, a computer must have
a semi-permanent medium for storing boot information, such as the number of hard disk
drive cylinders and sectors. For this purpose, it uses CMOS (complementary metal oxide
semiconductor) memory, which requires very little power to retain its contents and can
therefore be powered by battery. The battery helps the CMOS memory to retain vital
information about the computer system con(cid:12)guration, even when the computer is turned
o(cid:11). When changing the computer system con(cid:12)guration, the information stored in the CMOS
memory must be updated, either by the user or by the plug-and-play feature.??

1.3.4 Computer Network

Till now we were mostly discussing stand-alone computers, which are not connected to any
computer network. Most of today’s desktop computers are instead connected to a network,
and therefore it is useful for us to have a brief introduction to this topic. A computer network
is a collection of computers and other devices that communicate to share data, hardware,
and software. Each device on a network is called a node. A network that is located within
a relatively limited area such as a building or campus is called a local area network or
LAN, and a network that covers a large geographical area is called a wide area network
or WAN. The former is typically found in medium-sized and large businesses, educational
institutions, and government o(cid:14)ces. Di(cid:11)erent types of networks provide di(cid:11)erent services,
use di(cid:11)erent technology, and require users to follow di(cid:11)erent procedures. Popular network
types include Ethernet, Token Ring, ARCnet, FDDI, and ATM.

Give a (cid:12)gure here

A computer connected to a network can still use all of its local resources, such as hard
drive, software, data (cid:12)les, and printer.
In addition, it has access to network resources,
which typically include network servers and network printers. Network servers can serve as

22

Chapter 1.

Introduction

a (cid:12)le server, application server, or both. A (cid:12)le server serves as a common repository for
storing program (cid:12)les and data (cid:12)les that need to be accessible from multiple workstations|
client nodes|on the network. When an individual client node sends a request to the (cid:12)le
server, it supplies the stored information to the client node. Thus, when the user of a client
workstation attempts to execute a program, the client’s OS sends a request to the (cid:12)le server
to get a copy of the executable program. Once the server sends the program, it is copied
into the memory of the client workstation, and the program is executed in the client. The
(cid:12)le server can also supply data (cid:12)les to clients in a similar manner. An application server, on
the other hand, runs application software on request from other computers, and forwards
the results to the requesting client.

In order to connect a computer to a network, a network interface card (NIC) is required.
This interface card sends data from the workstation out over the network and collects
incoming data for the workstation. The NIC for a desktop computer can be plugged into
one of the expansion slots in the motherboard. The NIC for a laptop computer is usually
a PCMCIA card. Di(cid:11)erent types of networks require di(cid:11)erent types of NICs.

A computer network requires a network operating system to control the (cid:13)ow of data,
maintain security, and keep track of user accounts. Commonly used operating systems such
as UNIX, Windows XP, and Windows Vista already include networking capability. There
are also software packages such as Novel l Netware that are designed exclusively for use as
network operating system. A network operating system usually has two components: server
software and client software. The server software is installed in the server workstation; it
has features to control (cid:12)le access from the server hard drive, manage the print queue, and
track network user data such as userids and passwords. The client software is installed
on the local hard drive of each client workstation; it is essentially a device driver for the
NIC. When the client workstation boots up, the network client software is activated and
establishes the connection between the client and the other devices on the network.

1.4 Trends in Computing

Computer systems have undergone dramatic changes since their inception a few decades
ago. It is di(cid:14)cult to say whether it is the hardware that drives the software or if it is the
other way around. Both are intimately tied to each other; trends in one do a(cid:11)ect the other
and vice versa. We shall (cid:12)rst discuss trends in hardware technology.

1.4.1 Hardware Technology Trends

Ever since transistors began to be integrated in a large scale, producing LSI and VLSI (Very
Large Scale Integration) circuits, there have been non-stop e(cid:11)orts to continually reduce the
transistor size. Over the last three decades, the feature size has decreased nearly by a factor
of 100, resulting in smaller and smaller transistors. This steady decrease in transistor sizes,

1.4. Trends in Computing

23

coupled with occasional increases in die sizes, have resulted in more and more transistors
being integrated in a single chip. This has translated .......

In 2008, Intel r(cid:13)released the (cid:12)rst processor chip that integrates more than 2 billion
transistors|the quad-core Tukwila. As of 2008, it is also the biggest microprocessor made,
with a die size of 21.5 (cid:2) 32.5mm2 .
Below we highlight some of the main trends we see in hardware technology today:

(cid:15) Clock speed: Clock speed had been steadily increasing over the last several decades;
however, the current trend hints more of a saturation.
In 2007, IBM released the
dual-core Power6 processor, which operates at an astonishing 4.7 GHz clock.

(cid:15) Low-power systems: In the late 1990s, as the number of transistors as well as the
clock speed steadily increased, power consumption|especially power density|began
to increase at an alarming rate. High power densities translated to higher temper-
atures, necessitating expensive cooling technologies. Today, power consumption has
become a (cid:12)rst-order design constraint, and power-aware hardware designs are com-
monplace.

(cid:15) Large memory systems: Memory size has always been increasing steadily, mir-
roring the downward trend in price per bit. However, memory access time increases
with size, necessitating the use of cache memories to reduce average memory latencies.
Nowadays, it is commonplace to see multi-level cache organizations in general-purpose
computers.

(cid:15) Multi-core processors: The current trend is to incorporate multiple processor cores
on the same die. These cores parallely execute multiple threads of execution.

(cid:15) Embedded systems: Although embedded systems have been around for a while,
their popularity has never been as high as it is today. Cell phones, automobile controls,
computer game machines | you name it! | all have become so sophisticated, thanks
to advances in embedded system technology.

Some of these trends become clear when we look at the microprocessors developed over
the last four decades for desktop systems by one of the ma jor processor manufacturers,
Intel r(cid:13). Table 1.2 succinctly provides various features of these processors.

1.4.2 Software Technology Trends

As processors and memory became smaller and faster | providing the potential for signi(cid:12)-
cant boosts in performance | application programs and operating systems strived to o(cid:11)set
that bene(cid:12)t by becoming bigger and sluggish. The time to boot up a computer, for instance,
has remained steady|if not increased|over the years. This does not mean that software

24

Chapter 1.

Introduction

Processor

Word Intro

Feature

Die Number of

Clock

Name
4004
8008
8080
8085
8086
80286
Intel386T M
Intel486T M
Pentium r(cid:13)
Pentium r(cid:13)Pro
Pentium r(cid:13)II
Pentium r(cid:13)III
Pentium r(cid:13)4
Pentium r(cid:13)D

size Year
1971
4
1972
8
1974
8
8
1976
1978
16
1982
16
1985
32
32
1989
1993
32
1995
32
1997
32
32
1999
2000
32
64
2005

size ((cid:22)m)
10
10
6
3
3
1.5
1.5
1
0.8
0.35
0.35
0.18
0.18
0.09

area (mm2 ) Transistors
Freq.
2300
13.5
108 KHz
3500
200 KHz
6000
2 MHz
6500
2 MHz
29K 4.77 MHz
134K
6 MHz
16 MHz
275K
1.2M
25 MHz
3.1M
66 MHz
5.5M 200 MHz
7.5M 300 MHz
28M 733 MHz
42M
2 GHz
3.2 GHz
230M

81
294
195
131
106
217
206

Table 1.2: Progression of Intel r(cid:13)Microprocessors Designed for Desktop Systems

technology has made no progress. On the contrary, there have been tremendous improve-
ments in software application development. The driving philosophy in software technology
development has also been performance and e(cid:14)ciency, but of the programmer!.

Below we highlight a few of the current trends in software applications:

(cid:15) Application Nature:

{ Multimedia

{ Graphics

{ Bioinformatics

{ Web-based

(cid:15) Programming Methodology and Interface:

{ Ob ject-oriented programming: Java, C#

{ Visual programming: Scripting, HTML

{ Multi-threading

{ Application Programming Interface (API): POSIX, Windows API

(cid:15) Operating System:

{ Linux

1.5. Software Design Issues

25

{ Windows Vista
{ Mac OS X

(cid:15) Security:

1.5 Software Design Issues

A good software piece is not one that just works correctly. Modularity, simplicity,

1.6 Hardware Design Issues

Among the two phases in the life of a program | its development and execution | hardware
designers are concerned with the execution phase. As we will see in Section 1.8, hardware
design is carried out in various stages, at di(cid:11)erent levels of abstraction. When designing
hardware, the factors that stand at the forefront are performance, power consumption, size,
and price; well designed hardware structures are those that have adequate performance and
long battery life (if running o(cid:11) a battery), and are compact and a(cid:11)ordable. Other issues
that become important, depending on the computing environment, are binary compatibility,
reliability, and security.

1.6.1 Performance

The speed with which computer systems execute programs has always been a key design
parameter in hardware design. We can think of two di(cid:11)erent metrics when measuring the
speed of a computer system: response time and throughput. Whereas response time refers
to how long the computer takes to do an activity, throughput refers to how much the com-
puter does in unit time. The response time is measured by time elapsed from the initiation
of some activity until its completion. A frequently used response time metric is program
execution time, which speci(cid:12)es the time the computer takes for executing the program once.
The execution time of a program, ET , can be expressed as the product of three quantities:
(i) the number of instructions executed or instruction count (I C ), (ii) the average number
of clock cycles required to execute an instruction or cycles per instruction (C P I ), and (iii)
the duration of a clock cycle or cycle time (C T ). Thus,
ET = I C (cid:2) C P I (cid:2) C T
Although this simple formula seems to provide a good handle on program execution time,
and therefore on computer performance, the reality is not so simple! The instruction count
of a program may vary depending on the data values supplied as input to the program. And,
the cycles per instruction obtained may vary depending on what other programs are simul-
taneously active in the system8 . Finally, we have computer systems that dynamically adjust

8Nowadays, almost all computer systems execute multiple programs at the same time.

26

Chapter 1.

Introduction

the clock cycle time|dynamic frequency scaling|in order to reduce power consumption.

While reporting program execution time, the standard practice used to deal with the
(cid:12)rst problem is to measure the execution time with a standard set of input data. The
second and third problems are avoided by not running only one benchmark program at a
time and by not exercising dynamic frequency scaling.

Throughput, the other metric of performance, speci(cid:12)es the number of programs, jobs,
or transactions the computer system completes per unit time.
If the system completes
C programs during an observation period of T seconds, its throughput X is measured as
C/T programs/seconds. For processors, a more commonly used throughput measure is the
number of instructions executed in a clock cycle, referred to as its IPC (instructions per
cycle).

Although throughput and response time are related, improving the throughput of a
computer system does not necessarily result in reduced response time. For instance, the
throughput of a computer system improves when we incorporate additional processing cores
and use these cores for executing independent tasks, but that does not decrease the execution
time of any single program. On the other hand, replacing a processor with a faster one would
invariably decrease program execution time as well as improve throughput.

1.6.2 Power Consumption

After performance, power consumption is perhaps the biggest design issue to occupy the
hearts and minds of computer designers.
In fact, in some application domains, power
consumption has edged out performance as the most important design factor. Why is
power such an important issue? This is because it directly translates to heat production.
Most of the integrated circuits will fail to work correctly if the temperature rises beyond a
few degrees.

Again, the designer’s goal is to reduce the power consumption occurring during program
execution, as program development can be carried out in a di(cid:11)erent system where power
consumption may not be burning issue.

Power consumption has two components: dynamic and static. Dynamic power relates to
power consumed when there is switching activity (or change of state) in a system, whereas
static power relates to power consumed even when there is no switching activity in the
system. Dynamic power is directly proportional to the extent of switching activity in the
system and the clock frequency of operation. It is also proportional to the capacitance in
the circuits and wires, and to the square of the supply voltage of the circuits.

Static power consumption occurs due to leakage currents in the system. With continued
scaling in transistor technology|reduction in transistor sizes|static power consumption
is becoming comparable to dynamic power consumption. Static power consumption is also
related to the supply voltage. Therefore, to develop low-power systems, computer hardware
designers strive to reduce the supply voltage of the circuits as well as reduce the amount of

1.7. Theoretical Underpinnings

27

hardware used to perform the required functionality.

1.6.3 Price

Price is an important factor that makes or breaks the success of any computer system. Be-
tween two comparable computer systems, all things being equal, price will be an important
factor. The ma jor factors a(cid:11)ecting the price are design cost, manufacturing cost, and pro(cid:12)t
margin, all of which may be impacted by the sales volume. In general, price increases expo-
nentially with the complexity of the system. Therefore, it is imperative to reduce hardware
complexity at all costs.

1.6.4 Size

Size is an important design consideration, especially for laptops and embedded systems.

1.6.5 Summary

From the above discussion, it is apparent that design the computer hardware is a complex
process, where one has to focus on several factors at the same time. Often, focusing on one
factor comes at the expense of others. For instance, attempting to improve performance by
using substantial amounts of hardware generally results in high power consumption as well
as size and price. A good design will attempt to achieve good performance without increase
in hardware complexity, thereby conserving power, and reducing the size and price as well.

1.7 Theoretical Underpinnings

1.7.1 Computability and the Turing Machine

The (cid:13)edgling days of computers saw them only solving problems of a numerical nature;
soon they began to process various kinds of information. A question that begs an answer is:
What kinds of problems can a computer solve? The answer, as per computer science theory,
is that given enough memory and time, a computer can solve all problems for which a (cid:12)nite
solution algorithm exists. One of the computer pioneers who de(cid:12)ned and formalized compu-
tation was the British mathematician Alan Turing. While a graduate student at Princeton
University in 1936, Turing published a seminal paper titled \On Computable Numbers
with an Application to the Entscheidungsproblem," which laid a theoretical foundation for
modern computer science. In that paper he envisioned a theoretical machine, which later
became known as a Turing Machine, that could read instructions from a punched paper tape
and perform all the critical operations of a computer. One of Turing’s remarkable achieve-
ments was to prove that a universal Turing machine|a simulator of Turing machines|can

28

Chapter 1.

Introduction

perform every reasonable computation [?].
If given a description of a particular Turing
machine TM, the universal Turing machine simulates all the operations performed by TM.
It can do anything that any real computer can do, and therefore serves as an abstract model
of all general-purpose computers. Turing’s paper also established the limits of computer
science by mathematically demonstrating that some problems do not lend themselves to
algorithmic representations, and therefore cannot be solved by any kind of computer.

1.7.2 Limitations of Computers

\Computers are useless. They can only give you answers."
| Pablo Picasso (1881 - 1973).

For the computer to solve a problem, it is imperative to (cid:12)rst develop a solution algorithm,
or step-by-step procedure, for the problem. Although a general-purpose computer can be
used to solve a wide variety of problems by executing appropriate algorithms, there are
certain classes of problems that cannot be solved using a computer, either in principle or
in practice! Such problems may be grouped into three categories:

(cid:15) Undecidable or non-computable problems

(cid:15) Unsolvable problems

(cid:15) Intractable problems

Well−defined problems

Computable

Noncomputable

Undecidable or Non-computable
Problems: This category includes
problems that have been proven not
to have (cid:12)nite solution algorithms.
Kurt G(cid:127)odel, a famous mathemati-
cian, proved in the 1930s his famous
incompleteness theorem [ref ]. An
important consequence of G(cid:127)odel’s
theorem is that there is a limit
on our ability to answer questions
about mathematics.
If we have a
mathematical model as complex as
the set of integers, then there is no
algorithmic way by which true state-
ments can be distinguished from
false ones.
In practical terms, this
means that not all problems have an
algorithmic solution, and therefore a computer cannot be used to solve any arbitrary prob-
lem. In particular, a computer cannot (cid:12)nd proofs in su(cid:14)ciently complex systems. A. M.

Figure 1.9: Categorization of Well-De(cid:12)ned Problems
based on Computability

Partially
computable

Tractable

NP−complete

Unsolvable

1.7. Theoretical Underpinnings

29

Turing and Alonzo Church demonstrated a set of undecidable problems in 1936. One of
these is what has become known as the Turing machine halting problem, which states that
no algorithm exists to determine if an arbitrary Turing machine with arbitrary input data
will ever halt once it has started working. A practical implication of this result is that given
a su(cid:14)ciently complex computer program with loops, it is impossible to determine if under
certain inputs the program will ever halt. Since the 1930s, a number of other problems
have been proven to be undecidable. It will never be possible in a logically consistent sys-
tem to build a computer, however powerful, that by manipulating symbols can solve these
undecidable problems in a (cid:12)nite number of steps!

Unsolvable Problems: This category includes well-de(cid:12)ned problems that have not been
proved to be undecidable, but for which no (cid:12)nite algorithm has yet been developed. An
example is Goldbach’s conjecture, formulated by the 18th century mathematician Christian
Goldbach. The conjecture states that every even integer greater than 2 is the sum of exactly
two prime numbers. Although this conjecture has been veri(cid:12)ed for a large number of even
integers, it has not yet been proved to be true for every even integer, nor has any (cid:12)nite
algorithm been developed to prove this conjecture. An algorithm that examines all even
integers is not (cid:12)nite, and therefore will not terminate. An unsolved problem may eventually
be solved or proved to be undecidable.

Intractable Problems: This category includes problems that have a (cid:12)nite solution al-
gorithm, but executing the best available algorithm requires unreasonable amounts of time,
computer memory, and/or cost.
In general, this is the case when the complexity of the
best available algorithm grows exponentially with the size of the problem. An example of
an intractable problem is the traveling salesman problem. The ob jective in this problem is
to (cid:12)nd a minimum-distance tour through a given set of cities. The best available solution
algorithms for this problem are exponential in n, where n is the number of cities in the tour.
This means that the execution time of the algorithm increases exponentially as n increases.
For reasonably large values of n, executing such an algorithm becomes infeasible. Many
problems that occur in real life are closely related to the traveling salesman problem; two
common examples are the scheduling of airline (cid:13)ights, and the routing of wires in a VLSI
chip. An intractable problem becomes more tractable with technological advances that
make it feasible to design faster computers. Algorithm developers often tackle intractable
problems by devising approximate or inexact solution algorithms. These approximate al-
gorithms often involve the use of various heuristics, and are near-optimal most of the time.
Simulated annealing is an example of such an algorithm. Recent research seems to indicate
that quantum computing has the potential to solve many of the intractable problems more
e(cid:14)ciently.

30

Chapter 1.

Introduction

1.8 Virtual Machines: The Abstraction Tower

If we look at the computer as a physicist would do, we will see that a digital computer
executes an algorithm by controlled movement of electrons through silicon substrates and
metal wires. A complete description of a computer could be given in terms of all of its silicon
substrates, impurity dopings, wire connections, and their properties. Such a view, although
very precise, is too detailed even for computer hardware designers, let alone the program-
mers. Hardly any programs would have been written in all these years if programmers were
given such a speci(cid:12)cation!

Like many other machines built today, computers are incredibly complex. The functions
involved in developing programs and in designing the hardware to execute them are so
diverse and complex that it is di(cid:14)cult for a user/programmer/designer to have mastery
of all of the functions. A practical technique for dealing with complexity in everyday
life is abstraction9 . An automobile driver, for instance, need not be concerned with the
details of how exactly the automobile engine works. This is possible because the driver
works with a high-level abstract view of the car that encapsulates the essentials of what
is required for driving the vehicle. The car mechanic, on the other hand, has a more
detailed view of the machine.
In a similar manner, abstraction is used to deal with the
complexity of computers. That is, computer software and hardware can be viewed as a
series of architectural abstractions or virtual machines. Di(cid:11)erent users see di(cid:11)erent (virtual)
machines depending on the level at which they use the computer. For instance, a high-
level language programmer sees a virtual machine that is capable of executing statements
speci(cid:12)ed in a high-level language. An assembly language programmer, on the other hand,
sees a di(cid:11)erent machine with registers and memory locations that can execute instructions
speci(cid:12)ed in an assembly language. Thus, the study of a computer system is (cid:12)lled with
abstractions. There is yet another advantage to viewing the computer at several abstraction
levels. Programs developed for a particular abstraction level can be executed in di(cid:11)erent
platforms|which di(cid:11)er in speed, cost, and power consumption|that implement the same
abstract machine.

The user who interacts with a computer system at a particular abstraction level has
a view of what its capabilities are at this level, and this view results directly from the
functions that the computer can perform at this level. Conceptually, each architectural
abstraction is a set of rules that describes the logical function of a computer as observable by
a program running on that abstract machine. The architecture does not specify the details of
exactly how its functions will be performed; it only speci(cid:12)es the architecture’s functionality.
Implementation issues related to the functionality are left to the lower-level machines. The

9An abstraction is a representation that hides details so that one can focus on a few concepts at a time.
Formal abstractions have a well-de(cid:12)ned syntax and semantics. Hence, they provide a way of conveying the
information about a system in a consistent way that can be interpreted unambiguously. This abstraction (or
speci(cid:12)cation) is like a contract: it de(cid:12)nes how the system behaves. The abstraction de(cid:12)nes how the outside
world interacts with the system. The implementation, on the other hand, de(cid:12)nes how the system is built,
as seen from the inside.

1.8. Virtual Machines: The Abstraction Tower

31

entire point of de(cid:12)ning each architectural abstraction is to insulate programmers of that
level from those details. The instruction set architecture, for instance, provides a level of
abstraction that allows the same (machine language) program to be run on a family of
computers having di(cid:11)erent implementations (i.e., microarchitectures).

\‘There are many paths to the top of the mountain,
but the view is always the same."
| Chinese Proverb

In order to make it easier for comprehension purposes, we have organized the computer
virtual machines along a single dimension as an abstraction tower, with one machine \above"
the other. Each virtual machine except the one at the lowest level is implemented by the
virtual machine below it. This approach is called hierarchical abstraction. By viewing the
computer as a hierarchy of abstractions, it becomes easier to master the complexity of
computers and to design computer systems in a systematic, organized way.

Appropriate interfaces are used to specify the interaction between di(cid:11)erent abstractions.
This implementation is done by translating or interpreting the steps or instructions of one
level using instructions or facilities from the lower level. A particular computer designer
or user needs to be familiar only with the level at which he/she is using the computer.
For instance, a programmer writing a C program can assume that the program will be
executed on a virtual machine that directly executes C programs. The programmer need
not be concerned about how the virtual machine implements C’s semantics. Similarly,
in a multitasked computer system, each active program sees a separate virtual machine,
although physically there may be only a single computer! Some of the machine levels
themselves can be viewed as a collection of multiple abstractions. One such breakdown
occurs in the assembly-level language machine where we further break it into User mode
and Kernel mode.

Figure 1.10 depicts the principal abstraction levels present in modern computers.
In
the (cid:12)gure the planes depict the abstract machines. For each machine level, we can write
programs speci(cid:12)c to that level to control that machine. The solid blocks depict the peo-
ple/software/hardware who transform a program for one machine level to a program for the
machine level below it. For the sake of clarity and to put things in proper perspective, the
(cid:12)gure also includes a few levels (at the top) that are currently implemented by humans. It
is important to note that these abstract machines are somewhat di(cid:11)erent from the virtual
machines seen by end users when they run di(cid:11)erent programs on a computer system. For
instance, when you run a MIPS assembler program on a host computer system, you do not
\see" that host as a MIPS assembly-level machine or a MIPS ISA-level machine. Instead,
your view of the host is simply that of a machine capable of taking a MIPS assembly lan-
guage program as input, and churning out an equivalent MIPS machine language program
as output! You can even run that assembler program without knowing anything about the
MIPS assembly language or machine language. The person who wrote the MIPS assem-
bler, on the other hand, does see the MIPS assembly-level architecture as well as the MIPS

32

Chapter 1.

Introduction

ISA. Finally, the MIPS assembler program may have been originally written in an HLL,
for example C, in which case its developer also sees an abstract C machine that takes as
commands C statements!

The Use of a Language for an Abstraction: Each virtual machine level provides an
abstraction that is suitable for the computer user/designer working at that level. In order to
make use of a particular abstraction, it must be possible to specify commands/instructions
to that virtual machine. Without a well-de(cid:12)ned language, it becomes di(cid:14)cult to specify a
program of reasonable size and complexity. Most of the abstraction levels therefore provide
a separate language to enable the user at that level to specify the actions to be performed
by the virtual machine. The language speci(cid:12)es what data can be named by a program at
that level, what operations can be performed on the named data, and what ordering exists
among the operations. The language must be rich enough to capture the intricacies of the
corresponding virtual machine. When describing each of the abstraction levels, we will show
how the language for that level captures the essence of that level, and how it serves as a
vehicle to represent the commands speci(cid:12)ed by the user of that level.

\The limits of my language mean the limits of my world."
| L. Wittgenstein. Tractatus Logicio-Philosophicus

Translators, Interpreters, Emulators, and Simulators: An important aspect of the
layered treatment of computers is that, as already mentioned, the commands speci(cid:12)ed at
each abstraction level need to be converted to commands speci(cid:12)c to the immediately lower
level. Such a transformation makes the computer behave as a di(cid:11)erent machine than the
one for which the original program was written. This transformation process can be done by
translation or interpretation. In computer parlance, the term translation indicates taking a
static program or routine, and producing a functionally equivalent static program, usually
at a lower level. A static program is one that is not being executed currently. Thus,
translation of a loop involves translating each command of the loop exactly once. The term
interpretation, on the other hand, indicates taking individual steps of a dynamic program
and producing an equivalent sequence of steps, usually at a lower level. A dynamic program
is one that is in the process of being executed. Thus, interpretation of a loop involves
interpreting each command of the loop multiple times, depending on the number of times
the loop gets executed. Because of this dynamic nature, an interpreter essentially makes
one machine (the host machine) appear as another (the target machine). A translator is
almost always implemented in software, whereas an interpreter is implemented in software
or hardware. A software interpreter is often called a simulator, and a hardware interpreter is
often called an emulator. Of course, it is possible to build interpreters that use a combination
of software and hardware techniques. A simulator is often used to illustrate the working of
a virtual machine. It is also used to allow programs compiled for one machine to execute
on another machine. For instance, a simulator can execute programs written for an older

1.8. Virtual Machines: The Abstraction Tower

33

machine on a newer machine.

Advantages of using interpretation include (i) the ability to execute the (source) program
on di(cid:11)erent platforms, without additional compilation steps, and (ii) the ease of carrying
out interactive debugging. The main disadvantage is performance.

1.8.1 Problem De(cid:12)nition and Modeling Level Architecture

\At the highest level, the description is greatly chunked, and takes on a completely
di(cid:11)erent feel, despite the fact that many of the same concepts appear on the lowest
and highest levels."
| Douglas R. Hofstadter, in G(cid:127)odel, Escher, Bach: An Eternal Golden Band

The highest abstraction level that we can think of is the level at which problem de(cid:12)nition
and modeling are done. At this level, we can view the computer system as a machine that can
solve well-de(cid:12)ned computer problems. We loosely de(cid:12)ne a well-de(cid:12)ned computer problem
as one that can be represented and manipulated inside a computer.

The problem modeling person, therefore, takes complex real-life problems and precisely
formulates them so as to be solved on the computer. This process involves representing
the real-life problem’s data by some form of data that can be manipulated by a computer.
This process is called abstraction or modeling| creating the right model for the problem
so as to make it possible to eventually develop an appropriate algorithm to solve it. Notice
that in this context, modeling often implies simpli(cid:12)cation, the replacement of a complex
and detailed real-world situation by a comprehendable model within which we can solve a
problem. That is, the model captures the essence of the problem, \abstracting away" the
details whose e(cid:11)ect on the problem’s solution is nil or minimal.

Almost any branch of mathematics or science may be utilized in the modeling process.
Problems that are numerical in nature are typically modeled by mathematical concepts
such as simultaneous linear equations (e.g., (cid:12)nding currents in electrical circuits, or (cid:12)nding
stresses in frames made of connected beams) and di(cid:11)erential equations (e.g., predicting
population growth, or predicting the rate at which chemicals will react). Several problems
can be modeled as graph theoretical problems. Symbol and text processing problems can
be modeled by character strings and formal grammars. Once a problem is formalized, the
algorithm developer at the lower level can look for solutions in terms of a precise model and
determine if an algorithm already exists to solve that problem. Even if there is no known
solution, knowledge of the model properties might aid in developing a solution algorithm.

1.8.2 Algorithm-Level Architecture

The architecture abstraction below the problem de(cid:12)nition level is the algorithm-level archi-
tecture. At this level, we see the computer system as a machine that is capable of executing

34

Chapter 1.

Introduction

algorithms. An algorithm, as we saw earlier, is a step-by-step procedure that can be carried
out mechanically so as to get a speci(cid:12)c output from a speci(cid:12)c input. A key feature of com-
puter algorithms is that the steps are precisely de(cid:12)ned so as to be executed by a machine.
In other words, it describes a process so unambiguously that it becomes mechanical, in the
sense that it does not require much intelligence, and can be performed by rote or a machine.
Computer scientists also require that an algorithm be (cid:12)nite, meaning that (i) the number
of steps must be (cid:12)nite so that it terminates eventually, and (ii) each step must require only
(cid:12)nite time and computational resources.

The basic actions involved in a computer algorithm are:

(cid:15) Specify data values (using abstract data types)

(cid:15) Perform calculations and assign data values

(cid:15) Test data values and select alternate courses of actions including repetitions

(cid:15) Terminate the algorithm

The algorithm-level architecture supports abstract data types and abstract data struc-
tures; the algorithm designer formulates suitable abstract data structures and develops an
algorithm that operates on the data structures so as to solve the problem. Providing ab-
stract data types enables the algorithm designer to develop more general algorithms that
can be used for di(cid:11)erent applications involving di(cid:11)erent data types. This is often called
algorithm abstraction. For instance, a sorting algorithm that has been developed without
specifying the data types being sorted, can be programmed to sort a set of integers or a
set of characters. Similarly, when considering a data structure, such as an array, it is often
more productive to ignore certain details, such as the exact bounds of its indices. This is
often called data abstraction.

Algorithm E(cid:14)ciency: Computer theorists are mainly concerned with discovering the
most e(cid:14)cient algorithms for a given class of problems. The algorithm’s e(cid:14)ciency relates its
resource usage, such as execution time or memory consumption, to the size of its input data,
n. The e(cid:14)ciency is stated using the \Big O" notation, O(n). For example, if an algorithm
takes 4n - 2n + 2 steps to solve a problem of size n, we can say that the number of steps is
O(n2 ). Programmers use their knowledge of well-established algorithms and their respective
complexities to choose algorithms that are best suited to the circumstances. Examples of
such algorithms are quick-sort for sorting data (which has an an (n log n) average running
time), and binary search for searching through sorted data (which has an O(log 2n) time).
Algorithms can be speci(cid:12)ed in di(cid:11)erent ways. Two common methods are pseudocode
descriptions and (cid:13)owchart diagrams. A pseudocode description uses English, mathematical
notations, and a limited set of special commands to describe the actions of an algorithm. A
(cid:13)owchart diagram provides the same information graphically, using diagrams with a (cid:12)nite
set of symbols in the place of the more elegant features of the pseudocode. A computer

1.8. Virtual Machines: The Abstraction Tower

35

cannot directly understand either pseudocode or (cid:13)owcharts, and so algorithm descriptions
are translated to computer language programs, most often by human programmers. Thus,
a computer program is an embodiment of an algorithm; strictly speaking, an algorithm is
a mental concept that exists independently of any representation.

1.8.2.1 Computation Models

Another important tenet of an algorithm-level architecture is the computational model
it supports. A computational model conceptualizes computation in a particular way by
specifying the kinds of primitives, relationships, and events that can be described in an
algorithm. A computational model will generally have the following features:

(cid:15) Primitives: They represent the simplest ob jects that can be expressed in the model.
Examples of primitives found in most of the computation models are constants and
variables.

(cid:15) Methods of combination: They specify how the primitives can be combined with
one another to obtain compound expressions.

(cid:15) Methods of abstraction: They specify how compound ob jects can be named and
manipulated as units.

The computational model determines the kind of computations that can be speci(cid:12)ed by
an algorithm. For example, if we consider a geometric computational model that supports
only ruler and compass construction primitives, then we can specify algorithms (rules)
for bisecting a line segment, bisecting an angle, and other similar problems. We cannot,
however, specify an algorithm to trisect an angle. For solving this problem, we require
additional primitives such as a protractor. For arithmetic computation we can use models
incorporating di(cid:11)erent primitives such as an abacus, a slide rule, or even a calculator. With
each of these computation models, the type of arithmetic problems that can be solved is
di(cid:11)erent. The algorithms for solving a speci(cid:12)c problem would also be di(cid:11)erent.

Algorithm development is always done for a speci(cid:12)c algorithm-level architecture having
an underlying computational model. Three basic computational models are currently in
use, some of them being more popular than the others: imperative, functional, and logic.
These models of computation are equivalent in the sense that, in principle, any problem
that has a solution in one model is solvable in every one of the other models also.

Imperative Model: The imperative model of computation is based on the execution
of a sequence of instructions that modify storage called state. The basic concept is the
notion of a machine state (comprising variables in the high-level architecture, or registers
and memory locations in the assembly-level architecture). Program development consists
of specifying a sequence of state changes to arrive at the solution. An imperative program

36

Chapter 1.

Introduction

would therefore consist of a sequence of statements or instructions and side-e(cid:11)ect-prone
functions; the execution of each statement (or instruction) would cause the machine to
change the value of one or more elements of its state, thereby taking the machine to a new
state. A side-e(cid:11)ect-prone function is one whose execution can result in a change in the
machine state. Historically, the imperative model has been the most widely used model;
most computer programmers start their programming career with this computational model.
It is the closest to modeling the computer hardware. This tends to make it the most e(cid:14)cient
model in terms of execution speed. Commonly used programming languages such as C,
C++, FORTRAN, and COBOL are based on this computational model.

Applicative (Functional) Model: The functional model has its foundation in mathe-
matical logic. In this model, computing is based on recursive function theory (RFT), an
alternative (and equivalent) model of e(cid:11)ective computability. As with the Turing machine
model, RFT can express anything that is computable. Two of the prominent computer
scientists who pioneered this computational model are Stephen Kleene and Alonso Church.
The functional model consists of a set of values, functions, and the application of side-e(cid:11)ect-
free functions. A side-e(cid:11)ect-free function is one in which the entire result of computation is
produced by the return value(s) of the function. Side-e(cid:11)ect-free functions can only access
explicit input parameters; there are no global variables in a fully functional model. And, in
the purest functional models, there are no assignment statements either. Functions may be
named and may be composed with other functions. Functions can take other functions as
arguments and return functions as results. Programs consist of de(cid:12)nitions of functions, and
computations are application of functions to values. A classic example of a programming
language that is built on this model is LISP.

Rule-based (Logic) Model: The logic model of computation is a formalization of the
logical reasoning process. It is based on relations and logical inference. An algorithm in
this model involves a collection of rules, in no particular order. Each rule consists of an
enabling condition and an action. The execution order is determined by the order in which
the enabling conditions are satis(cid:12)ed. The logic model is related to relational data bases and
expert systems. A programming language designed with this model in mind is Prolog.

Computational Model Extensions: Apart from the three popular computational mod-
els described above, many other computational models have been proposed. Many exten-
sions have also been proposed to computational models to improve programmer e(cid:14)ciency or
hardware e(cid:14)ciency. Two important such extensions are the ob ject-oriented programming
model and the concurrent programming model.

(cid:15) Ob ject-Oriented Model: In this model, an algorithm consists of a set of ob jects
that compute by exchanging messages. Each ob ject is bound up with a value and
a set of operations that determine the messages to which it can respond. Functions

1.8. Virtual Machines: The Abstraction Tower

37

are thus designed to operate on ob jects. Ob jects are organized hierarchically. That
is, complex ob jects are designed as extensions of simple ob jects; the complex ob ject
will \inherit" the properties of the simple ob ject. The ob ject-oriented model may be
implemented within any of the other computational models. Imperative programming
languages that use the ob ject-oriented approach are C++ and Java.

(cid:15) Concurrent Model: In this model, an algorithm consists of multiple processes or
tasks that may exchange information. The computations may occur concurrently or
in any order. The model is primarily concerned with methods for synchronization and
communication between processes. The concurrent model may also be implemented
within any of the other computational models. Concurrency in the imperative model
can be viewed as a generalization of control. Concurrency is particularly attractive
within the functional and logic models, as subexpression evaluation and inferences
may then be performed concurrently. Hardware description languages (HDLs) such
as Verilog and VHDL use the concurrency model, as they model hardware components,
which tend to operate concurrently.

1.8.3 High-Level Architecture

The abstraction level below the algorithm-level architecture is the high-level architecture.
This is the highest level that we study in this book, and is de(cid:12)ned by di(cid:11)erent high-level
languages, such as C, C++, FORTRAN, Java, LISP, Prolog, and Visual Basic. This level
is used by application programmers and systems programmers who take algorithms and
formally express them in a high-level language. HLL programmers who develop their own
algorithms often perform both these steps concurrently. That is, the algorithm development
is done side by side with HLL program development.

To the HLL programmer the computer is a machine that can directly accept programs
written in a high-level language that uses alphabets as well as symbols like +, (cid:0), etc. It
is de(cid:12)nitely possible to construct a computer hardware that directly executes a high-level
language; several LISP machines were developed in the 1970s and 1980s by di(cid:11)erent vendors
to directly execute LISP programs. Directly running high-level programs on hardware is not
commonplace, however, as the hardware can run only programs written in one speci(cid:12)c high-
level language. More commonly, programs written in high-level languages are translated to
a lower level by translators known as compilers. We shall see more details of the high-level
architecture in Chapter 2.

For a computer to solve a problem, the algorithm must be expressed in an unambiguous
manner, so that computers can faithfully follow it. This implies espressing the algorithm
as a program as per the syntax and semantics of a programming language.

38

Chapter 1.

Introduction

1.8.4 Assembly-Level Architecture

The next lower level, called the assembly-level architecture, implements the high-level ar-
chitecture. The architecture at this level has a notion of storage locations such as registers
and memory. Its instructions are also more primitive than HLL statements. An instruction
may, for instance, add two registers, move data from one memory location to another, or
determine if a data value is greater than zero. Primitive instructions such as these are
su(cid:14)cient to implement high-level language programs. The language used to write programs
at this level is called an assembly language. In reality, an assembly language is a symbolic
form for the language used in the immediately lower level, namely the instruction set ar-
chitecture. Often, the assembly-level architecture also includes some instructions that are
not present in the instruction set architecture.

The assembly-level architecture and the instruction set architecture are usually hybrid
levels in that each of these architectures typically includes at least two modes|the User
mode and the Kernel mode. Both modes have many common instructions; however, each
mode also has a few instructions of its own. The extra instructions in the Kernel mode
include, for instance, those for reading or writing to IO addresses, managing memory al-
location, and creating multiple processes. The extra instructions in the User mode are
called system cal l instructions. In the microarchitecture, these instructions are interpreted
by executing an interpreter program in the Kernel mode at the ISA level. This interpreter
program is called the operating system kernel. Notice that the operating system itself may
have been originally written in a high-level language, and later translated to the lower
levels. The instructions that are common to both modes are interpreted directly by the
microarchitecture, and not by the OS. Thus, the system call instructions of the User mode
are interpreted by the OS and the rest are interpreted directly by the microarchitecture.

1.8.5

Instruction Set Architecture (ISA)

The next lower level is called instruction set architecture (ISA). The language used to
specify programs at this level is called a machine language. The memory model, IO model,
and register model in the ISA are virtually identical to the ones in the assembly-level
architecture. However, when specifying register and memory addresses in machine language,
they are speci(cid:12)ed in binary encoding. The instructions in the ISA are also mainly binary
encodings of the instructions present in the assembly-level architecture. There may be a
few minor di(cid:11)erences in the instruction set. Usually, the assembly-level architecture has
more instructions than what is available in the ISA. Moreover, most assembly languages
permit programmers to de(cid:12)ne their own macros. These enhancements make it much easier
to program in an assembly language, compared to a machine language. Programs written in
an assembly language are translated to machine language using a program called assembler.

The instruction set architecture is sometimes loosely called architecture. Di(cid:11)erent ISAs

1.8. Virtual Machines: The Abstraction Tower

39

di(cid:11)er in the number of operations, data types, and addressing modes they specify. ISAs that
include fewer operations and addressing modes are often called RISC (Reduced Instruction
Set Computer) ISAs. Those with a large repertoire of operations and addressing modes are
often called CISC (Complex Instruction Set Computer) ISAs. The most commonly found
ISA is the IA-32 ISA|more often known by its colloquial name, x86|introduced by Intel
Corporation in 1979. Other ISAs that are in use today are IA-64, MIPS, Alpha, PowerPC,
SPARC, and PA-RISC.

1.8.6 Microarchitecture

The microarchitecture is the abstraction level immediately below the ISA; it serves as a
platform for interpreting machine language instructions. A microarchitecture speci(cid:12)cation
includes the resources and techniques used to realize the ISA speci(cid:12)cation, along with the
way the resources are organized to realize the intended cost and performance goals. At this
level the viewer sees hardware ob jects such as instruction fetch unit, register (cid:12)les, ALUs,
latches, cache memory, memory systems, IO interfaces, and interconnections. A register (cid:12)le
is a collection of registers from which a single register can be read or written by specifying
a register number. An ALU (Arithmetic Logic Unit) is a combinational logic circuit that is
capable of performing simple arithmetic and logical operations that are speci(cid:12)ed in machine
language instructions. The register (cid:12)le, the ALU, and the other components are connected
together using bus-type or point-to-point interconnections to form a data path. The basic
operation of the data path consists of fetching an instruction from main memory, decoding
its bit pattern to determine what it speci(cid:12)es, and to carry out its execution by fetching the
required operands, using the ALU to operate on the operand values, and storing back the
result in the speci(cid:12)ed register or memory location.

The actual interpretation of the machine language instructions is done by a control unit,
which controls and coordinates activities of the data path. It issues commands to the data
path to fetch, decode, and execute machine language instructions one by one. There is a
fundamental break at the instruction set architecture. Whereas the architectures above it
are usually implemented by translation, the ISA and the architectures below it are always
implemented by interpretation.

1.8.7 Logic-Level Architecture

Descending one level lower into the hardware, we get to the logic-level architecture. This
architecture is an abstraction of the electronic circuitry of a computer, and refers to the
actual digital logic and circuit designs used to realize the computer microarchitecture. The
designer of this architecture uses gates, which accept one or more digital inputs and produce
as output some logical function of the inputs. Several gates can be connected to form a mul-
tiplexer, decoder, PLA, or other combinational logic circuits such as an adder. The outputs
of an adder, subtractor, and other functional units can be passed through a multiplexer

40

Chapter 1.

Introduction

to obtain an ALU. Similarly, a few gates can be connected together with some feedback
arrangement to form a (cid:13)ip-(cid:13)op or 1-bit memory, which can be used to store a 0 or a 1.
Several (cid:13)ip-(cid:13)ops can be organized to form a register, and several registers can be organized
to form a register (cid:12)le. Memory systems are built in a similar manner, but on a much larger
scale. Thus, we use a hierarchical approach for implementing the individual blocks of the
microarchitecture in the logic-level architecture. We will examine gates and the logic-level
architecture in detail in Chapter 9 of the book. This is the lowest architecture abstraction
that we will study in detail in this book.

Synchronous vs Asynchronous (self-timed): Currently digital computers are typi-
cally designed as synchronous or clocked sequential circuits, meaning that they use clock
signals to co-ordinate and synchronize the di(cid:11)erent activities in the computer. Changes in
the machine state occur only at discrete times that are co-ordinated by the clock signals.
Thus, the basic speed of a computer is determined by the time of one clock period. The
clock speed of the processor used in the original IBM PC was 4.77 MHz. The clock speeds
in current state-of-the-art computers range from 1 to 3.8 GHz. If all other speci(cid:12)cations
are identical, higher clock speeds mean faster processing. An alternative approach is to use
asynchronous or self-timed sequential circuits.

1.8.8 Device-Level Architecture

For the sake of completeness, we mention the existence of a machine level below the logic-
level architecture, called device-level architecture. The primitive ob jects at this level are
devices and wires. The prevalent devices in today’s technologies are transistors. The de-
signer of this architecture uses individual transistors and wires to implement the digital
logic circuits speci(cid:12)ed in the logic-level architecture. The designer also speci(cid:12)es how the
transistors and wires should be laid out. With today’s technology, millions and millions of
transistors can be integrated in a single chip; such a design is called VLSI (Very Large Scale
Integration). Accordingly, device-level architecture is also called VLSI architecture.

One possible way of designing a device-level architecture involves taking the logic-level
architecture and implementing it as follows: connect a few transistors together to form
device-level circuitry that implement logic gates such as inverter, AND, OR, NAND, and
NOR. Then, implement each logic gate in the logic-level architecture by the equivalent
device-level cicuitry. In current practice, a di(cid:11)erent approach is taken. Instead of attempting
to implement the logic-level architecture, the VLSI designer takes the microarchitecture, and
implements the functional blocks in the microarchitecture with device-level circuitry. By
bypassing the logic-level architecture, this approach leads to a more e(cid:14)cient design.

Di(cid:11)erent types of transistor devices are available: BJT (Bipolar Junction Transistor),
MOSFET (Metal Oxide Semiconductor Field E(cid:11)ect Transistor), etc. Digital computer
applications invariably use MOSFETs. Again, di(cid:11)erent design styles are available with
MOSFETs. The most prevalent style is the CMOS (Complementary Metal Oxide Semicon-

1.9. Concluding Remarks

41

ductor) approach, which uses a PMOS network and a complementary NMOS network.

If we want to study the design of transistors, that leads us into solid-state physics, which
deals with low-level issues such as electrons, holes, tunneling, and quantum e(cid:11)ects. At this
low abstraction level the machine looks more analog than digital! This level is clearly
outside the scope of this book.

1.9 Concluding Remarks

We have barely scratched the surface of computing, but we have laid a solid foundation
for computing and computers. This chapter began with the general topic of computing
and the role of computers in computing applications. Perhaps now we can answer the
question: what exactly is a computer? To a large extent, the answer depends on the level
at which we view the computer. At the high-level view, it is a machine that accepts high-
level language programs, and directly executes them. At the logic-level view, computers are
digital electronic circuits consisting of di(cid:11)erent types of gates that process 0s and 1s. Viewed
in the above light, we arrive at a de(cid:12)nition of computer architecture as being concerned
with the design and application of a series of virtual machines, starting from high-level
architecture to logic-level architecture.

Besides the abstractions described in the previous section, a computer can have addi-
tional abstractions, such as user interfaces and data communication facilities. The key thing
to remember is that computers are generally designed as a series of architectural abstrac-
tions, each one implementing the one immediately above it. Each architecture represents
a distinct abstraction, with its own unique ob jects and operations. By focusing on one
architecture at a time, we are able to suppress irrelevant details, thereby making it easier
to master this complex sub ject. All of the architectures are important for mastery of the
sub ject; this textbook studies four architectures in detail: the assembly-level architecture,
the instruction set architecture, the microarchitecture, and the logic-level architecture. A
synthesis of these four architectures will give a depth and richness of understanding that
will serve well, irrespective of whether your main interest is in computer science, computer
engineering, or electrical engineering.

1.10 Exercises

1. Explain the role played by the operating system in a computer.

2. What is meant by a virtual machine in the context of computers?

3. Explain what is meant by the stored program concept.

Chapter 1.

Introduction

42

Problem−Level Architecture

Algorithm−Level Architecture

Problem

Algorithm Developer

Algorithm

HLL Programmer

High−Level Architecture
main() {
int a, b;
Application Program (HLL)
a = read(0, &b, 1);

Library Program (HLL)

OS Program (HLL)

Compiler (Translator)

Assembly−Level Architecture

User Mode

Kernel Mode

main:

LW  R4, 0(R29)
ADD R1, R3, R4
SYSCALL

User Program (AL)

OS Program (AL)

Assembler (Translator)

Machine−Level Architecture
(Instruction Set Architecture)

User Mode

Kernel Mode

Designed by Instruction Set Architect

User Program (ML)

Interpretation
by OS code

OS Program (ML)

Control Unit (Interpretor)

Microarchitecture

PC     MAR

Microarchitectural       Data Path
(RFs, Caches,      ALUs, Buses)

Designed by Microarchitect

Microinstruction

Microsequencer (Interpretor)

Logic −Level Architecture
PC_out, MAR_in

Logic−Level         Data Path
(Gates, Flip−flops, MUXes, ROMs)

Designed by Logic Designer

Control Signals

Device Control Inputs (Implementor)

Device−Level Architecture

Device −Level            Data Path
(Transistors, Wires, Layouts)

Designed by VLSI Designer

Figure 1.10: Machine Abstractions relevant to Program Development and Execution, along
with the Ma jor Components of each Abstract Machine.

Part I

PROGRAM DEVELOPMENT |
SOFTWARE LEVELS

Finish your outdoor work and get your (cid:12)elds ready; after that, build your house.

Proverbs 24: 27

This part deals with the software levels of computers.
In particular, it discusses the
high-level language (HLL)-level architecture, the assembly-level architecture, and the in-
struction set architecture (ISA). Chapter 2 gives a brief overview of program development.
This discussion is focused primarily on the high-level architecture. The detailed discus-
sion of computer architecture begins in this chapter with background information on the
high-level architecture, which is usually covered in a pre-requisite course to computer
architecture. This material is included in the book for completeness and to highlight
some of the software issues that are especially critical to the design of computer systems.
Chapter 3 provides a detailed treatment of the assembly-level architecture. In particular,
it describes the memory model, the register model, instruction types, and data types. It
also discusses programming at this level. Chapter 4 covers the Kernel mode, and di(cid:11)er-
ent ways of carrying out IO operations. Chapter 5 discusses ISA. It covers instruction
encoding, data encoding, translation from assembly language to machine language, and
di(cid:11)erent approaches to instruction set design.

44

Chapter 2

Program Development Basics

Let the wise listen and add to their learning, and let the discerning get guidance

Proverbs 1: 5

Software development is a fundamental aspect in computing; without software, comput-
ing would be limited to a few (cid:12)xed algorithms that have been hardwired into the hardware
system. The phenominal power of computers is due to their ability to execute di(cid:11)erent
programs at di(cid:11)erent times or even concurrently. Most of today’s program development
| programming | is done in one of the high-level languages. Therefore, much of the
discussion in this chapter is focused on high-level languages. These languages languages
abstract away the hardware details, making it possible to develop portable programs, i.e.,
programs that are not tied to any speci(cid:12)c hardware platform and can therefore be made
to execute on di(cid:11)erent hardware platforms. It is this high degree of abstraction that gives
them the name \high-level languages." Programming at a high level allows programmers
not to be concerned with the detailed machine-level speci(cid:12)cations, which in turn improves
their e(cid:14)ciency (if not the e(cid:14)ciency of the code!).

Many high-level languages are popular today: C, C++, Java, FORTRAN, VisualBA-
SIC, etc. Our ob jective in this chapter is not to teach programming; we assume that you
are already familiar with at least one high-level language, and have done some entry-level
programming. Our intent is to review important concepts that are common to program
development | irrespective of the language | and to lay a foundation for the material
presented in the subsequent chapter, which deals with the assembly-level architecture and
translation of programs from high-level languages to assembly languages.
In that vein,
we touch upon basic issues in software engineering as well; however, advanced software
engineering concepts are clearly out of the scope of this book.

45

46

Chapter 2. Program Development Basics

2.1 Overview of Program Development

\Programs should be written for people to read, and only incidentally for machines to
execute."

| Structure and Interpretation of Computer Programs by Harold Abelson and Gerald
Jay Sussman

There is no single way to develop computer programs; programmers di(cid:11)er quite a bit
when it comes to how they develop programs. A software engineering approach to pro-
gramming .................. We shall start with an overview of the important aspects in program
development. Below we highlight these aspects.

(cid:15) Problem modeling: The model is created by understanding the complete problem
to be solved, and making a formal representation of the system being designed.

(cid:15) Algorithm development: Once a formal model of the problem is developed, the
next step is to develop an appropriate algorithm for solving the problem. Algorithm
development involves de(cid:12)ning the following:

{ Data structures: the format and type of data the program will represent and
manipulate.

{ Inputs: the kind of data the program will accept.

{ Outputs: the kind of data the program will output.

{ User interface: the design of the screen the end user will see and use to enter
and view data.

{ Algorithm: the methods of manipulating the inputs and determining the outputs.

Algorithm development includes a lion’s share of the problem solving e(cid:11)ort. It is a
creative process, and has not yet been automated! The reason for this, of course, is
that for automating something, an algorithm has to be developed for performing it,
which means that we would require an algorithm for writing algorithms! Therein lies
the di(cid:14)culty. An array of guidelines have been developed, however, to make it easier
for an algorithm developer to come up with an algorithm for a new problem. Some
of these guidelines are given below:

{ See if any standard techniques (or \tricks") can be used to solve the problem.

{ See if the problem is a slight variation of a problem for which an algorithm has
already been developed. If so, try to adapt that algorithm.

{ Divide-and-conquer approach: See if the problem can be broken into subprob-
lems.

{ Develop a simpli(cid:12)ed version of the problem, and develop an algorithm for the
simpli(cid:12)ed problem. Then adapt the algorithm to (cid:12)t the original problem.

2.1. Overview of Program Development

47

A problem can often be solved by more than one functionally correct algorithm.
Choosing between two algorithms often depends on the requirements of a particular
application. For instance, one algorithm may be good for small data sets, whereas
the other may be good for large data sets.

(cid:15) Programming:

(cid:15) Debugging: Virtually all programs have defects in them called bugs, and these need
to be eliminated. Bugs can arise from errors in the logic of the program speci(cid:12)cation
or errors in the programming code created by a programmer. Special programming
tools assist the programmer in (cid:12)nding and correcting bugs. Some bugs are di(cid:14)cult
to locate and (cid:12)xing them is like solving a complex puzzle.

(cid:15) Testing: Alpha and beta testing. Alpha testing is a small scale trial of the program.
The application is given to a few expert users to assess whether it is going to meet their
needs and that the user interface is suitable. Bugs and missing features due to the
application being un(cid:12)nished will be found. Any errors in the code and speci(cid:12)cation
will be corrected at this stage. Beta testing is a more wide-ranging trial where the
application is given to a selection of users with di(cid:11)erent levels of experience. This
is where the bulk of the remaining bugs are found; some may remain undetected or
un(cid:12)xed.

(cid:15) Software delivery: The completed software is packaged with full documentation
and delivered to the end users. When they use the software, bugs that were not found
during testing may appear. As these new bugs are reported an updated version of the
software with the reported bugs corrected is shipped as a replacement.

Program development, as we saw in Chapter 1, .........

2.1.1 Programming Languages

Programming languages are the vehicle we use to express the tasks a computer must per-
form. It serves as a framework within which we organize our ideas about computer processes.

What makes a programming language powerful? A powerful programming language is
more than just a means for instructing a computer to perform various tasks. The power
of the language depends on the means it provides for combining simple ideas to form more
complex ideas. Every powerful language has three mechanisms for accomplishing this:

* primitive expressions, which represent the simplest entities the language is concerned
with,

* means of combination, by which compound elements are built from simpler ones, and

* means of abstraction, by which compound elements can be named and manipulated
as units.

48

Chapter 2. Program Development Basics

In programming, we deal with two kinds of primitives: instructions and data. Informally,
data is \stu(cid:11) " that we want to manipulate, and instructions are descriptions of the rules
for manipulating the data. Thus, any powerful programming language should be able to
describe primitive data and primitive instructions, and should have methods for combining
and abstracting instructions and data.

A computer program is nothing but an algorithm expressed in the syntax of a program-
ming language. For executing an algorithm, it is imperative that it be (cid:12)rst expressed in a
formal language. The familiar hello, world! program given below, when executed on a
computer, will display the words \hello, world!" on the display. This program uses the
syntax and semantics of the C programming language.

Program 1 The Hello World! program in C.

main() {
// Display the string
printf("hello, world!");

}

The same program, when expressed in a di(cid:11)erent programming language, will have some
di(cid:11)erences, but the underlying algorithm will be the same. For instance, when expressed in
Java, the same algorithm may look as follows:

Program 2 The Hello World! program in Java.

class helloworld {
public static void main(String[] args) {
// Display the string
System.out.println("hello, world!");

}

}

The features supported by a programming language form an important aspect of pro-
gramming, as the programmer expresses the entire algorithm by means of the programming
language chosen. In this section, we discuss features that are common to many high-level
programming languages.

(cid:15) Alphabet: High-level languages generally use a rich alphabet, such as the ones used
in natural languages, along with many of the symbols used in mathematics. Common
languages such as C and Java use the English alphanumerical character set as the
alphabet.

2.1. Overview of Program Development

49

(cid:15) Syntax: Syntax speci(cid:12)es the rules for the structure of programs.
It speci(cid:12)es the
delimiters, keywords, etc, and the possible combinations of symbols. Programming
languages can be textual or graphical. Textual languages use sequences of text includ-
ing words, numbers, and punctuation, much like written natural languages. Graphical
languages use symbols and spatial relationships between symbols to specify programs.
A languages syntax can be formalized by a grammar or syntax chart.

(cid:15) Semantics: While the syntax of a language refers to the appearance of programs
written in that language, semantics refers to the meanings of those programs. The
semantics of a language specify the meaning of di(cid:11)erent constructs in the language,
and therefore the behavior of the program when executed. The semantics of a lan-
guage draw upon linguistics and mathematical logic, and have a connection with the
computational model(s) supported by the language.

(cid:15) Data types and data abstraction: All programming languages provide a set of
basic data types such as integers, (cid:13)oating-point numbers, and characters. A data
type speci(cid:12)es the set of values a variable of that type can have. It also de(cid:12)nes how
operations such as + and (cid:0) will be carried out on variables of that type. In Pascal, for
instance, the expression i + j indicates integer addition if i and j are de(cid:12)ned to be
integers, and (cid:13)oating-point addition if they are de(cid:12)ned to be reals. Type checking
is supported by Pascal to ensure that such operations are applied to data of the same
type; more weakly typed languages such as C relax this restriction somewhat. Ob jects
are organized hierarchically

Most of the programming languages allow the programmer to de(cid:12)ne complex data
types out of simpler ones. Examples are the record data type in Pascal and the
struct data type in C. Ob ject-oriented languages such as C++ and Java extend
this concept further, by allowing the programmer to de(cid:12)ne a set of operations for
each of the newly de(cid:12)ned data type. The data type, along with the associated set of
operations, is called an object. Program statements are only allowed to manipulate
data ob jects according to the operations de(cid:12)ned for that ob ject.

Finally, most modern programming languages allow the programmer to de(cid:12)ne abstract
data types, thereby creating an extended language. An abstract data type is a data
type that is de(cid:12)ned in terms of the operations that it supports and not in terms
of its structure or implementation. In the context of programming languages, data
abstraction means hiding the details concerning the implementation of data values
in computers. Data abstraction thus makes it possible to have a clear separation
between the properties of a data type (which are visible to the user interface) and its
implementation details. Thus, abstraction forms the basic platform for the creation
of user-de(cid:12)ned ob jects.

If a programming language does not directly support data abstraction, the program-
mer may explicitly design and use abstract data types, by using appropriate coding.

(cid:15) Control abstraction:

50

Chapter 2. Program Development Basics

(cid:15) Library API:

(cid:15) OS API:

In the discussion that follows, we will provide example code snippets in both C and
Java. We selected these two languages because of their popularity. The reader who is not
familiar with any of these languages should not be distracted by this choice; the syntax
and semantics of these languages are easy to understand and are similar to those of other
high-level languages.
In any case, we will restrict our discussion to simple constructs in
these languages.

2.1.2 Application Programming Interface Provided by Library

2.1.3 Application Programming Interface Provided by OS

2.1.4 Compilation

2.1.5 Debugging

\If builders built houses the way programmers built programs, the (cid:12)rst woodpecker
to come along would destroy civilization."
| Gerald Weinberg

\Do not look where you fell, but where you slipped." African proverb

2.2 Programming Language Speci(cid:12)cation

2.2.1 Syntax

The syntax of a language refers to ....
It a(cid:11)ects the readability of the program. It also
impacts the ease with which a compiler can parse the program.

2.2.2 Semantics

2.3 Data Abstraction

Declaration and manipulation of data values is at the heart of computer algorithms. The
data types and structures used by algorithms are somewhat abstract in nature. A ma jor part
of the programming job involves implementing these abstractions using the more primitive
data types and features provided by the programming language. All programming languages
provide several primitive data types, and means to combine primitive data types into data
structures. Let us look at these primitive data types.

2.3. Data Abstraction

2.3.1 Constants

51

We shall start our discussion of data types with constants. Constants are ob jects whose
values do not change during program execution. Many calculations in real-world problems
involve the use of constants. Although a constant can be represented by declaring a variable
and initializing it to the appropriate value, this may not be the most e(cid:14)cient way from the
execution point of view. Most assembly languages do not treat constant and variable data in
the same manner. Assembly languages support a special immediate addressing mode that
lets a constant value to be directly speci(cid:12)ed as part of an instruction rather than storing that
constant’s value in a memory location and accessing it as a variable. By understanding how
constants are represented at the assembly language and machine language levels, constants
may be appropriately presented in the HLL source code to produce smaller and faster
executable programs.

2.3.1.1 Literal Constants and Program E(cid:14)ciency

High-level programming languages and most modern assembly languages allow you to spec-
ify constant values just about anywhere you can legally read the value of a memory variable.

2.3.1.2 Manifest Constants

A manifest constant is a constant value associated with a symbolic name. During program
translation, the translator will directly substitute the value everywhere the name appears
within the source code. Manifest constants allow programmers to attach meaningful names
to constant values so as to create easy-to-read and easily maintained programs.

2.3.1.3 Read-Only Memory Ob jects

C++ programmers may have noticed that the previous section did not discuss the use of
C++ const declarations. This is because symbols you declare in a C++ const statement
aren’t necessarily manifest constants. That is, C++ does not always substitute the value
for a symbol wherever it appears in a source (cid:12)le. Instead, C++ compilers may store that
const value in memory and then reference the const ob ject as it would a static variable.
The only di(cid:11)erence, then, between that const ob ject and a static variable is that the C++
compiler doesn’t allow you to assign a value to the const ob ject at runtime.

C++ sometimes treats constants you declare in const statements as read-only variables
for a very good reasonit allows you to create local constants within a function that can
actually have a di(cid:11)erent value each time the function executes (although while the function
is executing, the value remains (cid:12)xed). Therefore, you cannot always use such "constants"
within constant expressions in C++ and expect the C++ compiler to precompute the
expression’s value.

52

Chapter 2. Program Development Basics

2.3.1.4 Enumerated Types

Well-written programs often use a set of names to represent real-world quantities that
don’t have an explicit numeric representation. An example of such a set of names might be
various car manufacturers, such as GM, Ford, and Chrysler. Even though the real world
does not associate numeric values with these manufacturers, they must be must encoded
with numerical values if they are to be represented in a computer system. (Of course, it
is possible to represent them as \text" by representing each character in the name using
ASCII, but that would slow down program execution.) The internal value associated with
each symbol can be arbitrary; the important point is that the same unique value is used
every time a particular symbol is used. Many programming languages provide a feature
known as the enumerated data type that will automatically associate a unique value with
each name in a list. The use of enumerated data types helps the programmer to specify the
data using meaningful names rather than \magic" numbers such as 0, 1, and 2.

2.3.1.5 Boolean Constants

Many high-level programming languages provide Boolean or logical constants that can rep-
resent the values True and False. Because there are only two possible Boolean values, their
representation requires only a single bit at the machine language. However, because most
machine languages do not permit storage allocation at the granularity of a single bit, most
programming languages use a whole byte or even a larger ob ject to represent a Boolean
value. The behavior of the unused bits in a Boolean ob ject depends on the programming
language. Many languages treat the Boolean data type as an enumerated type.

2.3.2 Variables

Irrespective of the speci(cid:12)c high-level language used, the programmer sees an abstract ma-
chine that supports data structures and operations that can be performed on the data
structures. In most high-level languages, the data structures are declared a certain type.
The type indicates both the characteristics of ob jects that can be represented and the kinds
of operations that can be performed on the ob jects.

As mentioned earlier, manipulation of data values is at the heart of every computer pro-
gram. It is therefore of utmost importance that high-level languages provide programmers
an e(cid:14)cient and easy way of specifying data values that can be modi(cid:12)ed. Most HLLs allow
the programmer to refer to a data value symbolical ly by a name. Variables are used for a
variety of data values including input values, output values, loop counts, and intermediate
results of computation. Consider a simple program|one that counts the number of words
in a text (cid:12)le. This program would need to know the name of the (cid:12)le|information the
program end user would need to supply. The program would need a variable to keep track
of the number of words counted so far.

2.3. Data Abstraction

53

Declaring and manipulating variables is a central concept in HLL programming. The
HLL variables have some similarity to the variables used in algebra and other branches
of mathematics, although there are a few notable di(cid:11)erences.
In addition to specifying
the name of a variable, a variable declaration includes specifying the type, scope, and
storage class of the variable. The position of a variable declaration statement in a program
implicitly speci(cid:12)es the scope of the variable. The declaration is for the bene(cid:12)t of the
compiler, which must know how much space to allocate for each variable. Di(cid:11)erent variable
types require di(cid:11)erent amounts of space. For example, C permits di(cid:11)erent variable types
such as integers, characters, and (cid:13)oats. The declaration of a variable is accomplished by
specifying its name and type. For example, in C the declaration

int n;

declares an integer variable named n.

Most high-level languages allow a variable to be initialized at the time of declaration.
In C, the declaration

int n = 5;

declares an integer variable named n, and calls for initializing its value to 5. Once a variable
has been assigned a value, it retains that value until it is modi(cid:12)ed, either by a direct
assignment or an indirect assignment through a pointer.

2.3.2.1 Data Type

The data type of a variable de(cid:12)nes the set of values that the variable may ever assume, and
the semantics of possible arithmetic/logical operations on those values. In other words, a
variable type is a formally speci(cid:12)ed set of values and operations. For example, a variable
of type boolean (or logical) can assume either the value true or the value false, but no
other value. Logical operations such as fand, or, notg, and the assignment operation can be
performed on it. In addition, the data type indirectly speci(cid:12)es the number of bytes occupied
by the variable, and the methodology to be used for carrying out arithmetic operations on
the variable. Some of the commonly used data types are discussed next.

Signed Integers and Unsigned Integers: These are fundamental data types; virtually
every HLL supports them. Most HLLs support di(cid:11)erent word sizes for integer variables.
For instance, C has short and int variable types for representing 16-bit integers and 32-bit
integers, which can take positive as well as negative values. C also lets the programmer
declare unsigned integer types by adding the pre(cid:12)x unsigned before short or int.

\Good things, when short, are twice as good" Baltasar Gracian, The Art of Worldly
Wisdom

54

Chapter 2. Program Development Basics

double value; /* or your money back! */
short changed; /* so triple your money back! */

| Larry Wal l (the Perl Guy) in cons.c from the Perl source code

Floating Point Numbers (for Increased Range): The range of signed integers rep-
resentable in a 32-bit (cid:12)xed-point format is approximately (cid:0)2:15 (cid:2) 10 9 to 2:15 (cid:2) 109 . Many
computation problems require the ability to represent numbers that are of much greater
or smaller magnitude than the integers in this range. Examples are Avogadro’s number,
6:02 (cid:2) 1023 ; mass of a proton, 1:673 (cid:2) 10(cid:0)24 g; and the US National Debt a few years back,
$17,383,444,386,952.37. In order to represent very large integers and very small fractions,
most high-level languages support (cid:13)oating-point (FP) variable types, in which the e(cid:11)ective
position of the radix point can be changed by adjusting an exponent. The radix point is
said to (cid:13)oat, and the numbers are called (cid:13)oating-point numbers. This distinguishes them
from (cid:12)xed-point numbers, whose radix point is always in the same position. An FP number
is written on paper as follows:
(Sign)Signi(cid:12)cand (cid:2) BaseExponent

The base is the radix of the FP number, the signi(cid:12)cand identi(cid:12)es the signi(cid:12)cant digits of
the FP number, and the sign identi(cid:12)es the overall sign of the FP number. The exponent,
along with the base, determines the scale factor, i.e., the factor by which the signi(cid:12)cand is
multiplied to get the actual value. In C, (cid:13)oating-point variables can be declared as follows:

float f;
double d;

/* single precision floating-point */
/* double precision floating-point */

Character: Textual information has become one of the frequently utilized forms of infor-
mation for both storage and manipulation. This seems counterintuitive, given that comput-
ers have historically been used to \compute," or perform calculations. However, when we
consider the facts that programs are input in text form, that compilers operate on strings
of characters, and that computation answers are generally provided via some type of tex-
tual information, then the amount of textual information processed by computers begins
to be appreciated. Furthermore, the preparation of letters, reports, and other documents
has become a ma jor application of computers. The basic unit of textual information is a
character. Most high-level languages provide variable type(s) to represent character and/or
strings of characters. In C, a character variable can be declared as follows:

char c;

If you lost wealth, you lost nothing
If you lost health, you lost something
If you lost character, you lost everything.

| An old proverb

2.3. Data Abstraction

55

Pointer: The last data type that we will discuss in this section is the pointer. A pointer
is a variable used to hold the (memory) address of another variable. In de(cid:12)ning a pointer,
the high-level language assumes a limited knowledge of the memory model of the lower
level assembly-level architecture that implements it. Only some high-level languages sup-
port pointers. Example are Pascal and C. Pointers are helpful for building complex data
structures such as linked lists and trees. A pointer variable that points to a character can
be declared in C as follows:

char *cptr;

In this declaration, cptr is the pointer variable; the implicit assumption is that whatever
cptr is pointing to should be interpreted as a data item of type char.

Array: This is a data structure, i.e., a collection of variables.

Structure: This is a data structure, i.e., a collection of variables.

2.3.2.2 Scope

Another important piece of information speci(cid:12)ed in a variable declaration is the variable’s
scope, which de(cid:12)nes where and when it is active and available in the program.

2.3.2.3 Static Scoping

With static scoping, a variable always refers to its nearest enclosing binding. Because
matching a variable to its binding only requires analysis of the program text, this type of
scoping is sometimes also called lexical scoping. Static scope allows the programmer to
reason as if variable bindings are carried out by substitution. Static scoping also makes it
much easier to make modular code and reason about it, since its binding structure can be
understood in isolation. Correct implementation of static scope in languages with (cid:12)rst-class
nested functions can be subtle, as it requires each function value to carry with it a record
of the values of the variables that it depends on. When (cid:12)rst-class nested functions are not
used or not available (such as in C), this overhead is of course not incurred. Variable lookup
is always very e(cid:14)cient with static scope, as the location of each value is known at compile
time.

Most high-level languages that have static scoping allow at least the following two scopes
for variables:

(cid:15) global
(cid:15) local

56

Chapter 2. Program Development Basics

A global variable can be accessed throughout the program (that is, by all modules or
functions in the program). Because of this, declaring too many global variables makes it
di(cid:14)cult to debug and track variable values.

A local variable can be accessed only within the block in which it is declared. When
a local variable has the same name as that of a global variable, the global variable is not
visible in the block where the local variable is visible.

In some high-level languages, the scope of a variable is not explicitly declared; instead,
it is implicitly de(cid:12)ned by where exactly the variable is declared.

int i, j;

/* global variables; static storage class */

main()
f

int i, *iptr;
static int s;

/* local variables; automatic storage class */
/* local variable; static storage class */

iptr = (int *)malloc(40); /* dynamic storage class */

g

2.3.2.4 Dynamic Scoping

In dynamic scoping, each identi(cid:12)er has a global stack of bindings.
Introducing a local
variable with name x pushes a binding onto the global x stack (which may have been
empty), which is popped o(cid:11) when the control (cid:13)ow leaves the scope. Evaluating x in any
context always yields the top binding. Note that this cannot be done at compile time
because the binding stack only exists at runtime, which is why this type of scoping is called
dynamic scoping.

ince a section of code can be called from many di(cid:11)erent locations and situations, it can
be di(cid:14)cult to determine at the outset what bindings will apply when a variable is used.
This can be bene(cid:12)cial; application of the principle of least knowledge suggests that code
avoid depending on the reasons for (or circumstances of ) a variable’s value, but simply use
the value according to the variable’s de(cid:12)nition. This narrow interpretation of shared data
can provide a very (cid:13)exible system for adapting the behavior of a function to the current
state (or policy) of the system. However, this bene(cid:12)t relies on careful documentation of all
variables used this way as well as on careful avoidance of assumptions about a variable’s
behavior, and does not provide any mechanism to detect interference between di(cid:11)erent parts
of a program. As such, dynamic scoping can be dangerous and many modern languages do
not use it. Some languages, like Perl and Common Lisp, allow the programmer to choose
lexical or dynamic scoping when (re)de(cid:12)ning a variable. [edit]

Implementation

Dynamic scoping is extremely simple to implement. To (cid:12)nd an identi(cid:12)er’s value, the

2.3. Data Abstraction

57

program traverses the runtime stack, checking each activation record (each function’s stack
frame) for a value for the identi(cid:12)er. This is known as deep binding. An alternate strategy
that is usually more e(cid:14)cient is to maintain a stack of bindings for each identi(cid:12)er; the stack
is modi(cid:12)ed whenever the variable is bound or unbound, and a variable’s value is simply
that of the top binding on the stack. This is called shallow binding. Note that both of
these strategies assume a last-in-(cid:12)rst-out (LIFO) ordering to bindings for any one variable;
in practice all bindings are so ordered. [edit]

Example

int x = 0; int f () return x;

int g () int x = 1; return f();

With static scoping, calling g will return 0 since it has been determined at compile time
that the expression x in any invocation of f will yield the global x binding which is una(cid:11)ected
by the introduction of a local variable of the same name in g.

With dynamic scoping, the binding stack for the x identi(cid:12)er will contain two items when
f is invoked: the global binding to 0, and the binding to 1 introduced in g (which is still
present on the stack since the control (cid:13)ow hasn’t left g yet). Since evaluating the identi(cid:12)er
expression by de(cid:12)nition always yields the top binding, the result is 1.

2.3.2.5 Lifetime

A variable’s lifetime or storage class determines the period during which that variable exists.
Some variables exist brie(cid:13)y, some are repeatedly created and destroyed, and others exist for
the entire program execution. A variable’s storage class determines if the variable loses its
value when the block that contains it has completed execution. Most high-level languages
support the following three storage classes:

(cid:15) automatic
(cid:15) static
(cid:15) dynamic
(cid:15) persistent

Automatic variables begin to exist when control reaches their block, and lose their values
when execution of their block completes. Examples are the local variables declared within
subroutines1 . Static variables, on the other hand, begin to exist when the program starts
running, and continue to retain their values till the termination of the program. Dynamic
variables are implicit variables pointed to by pointer variables, and do not exist when
the program starts running. They are created during the execution of the program, and
continue to exist and retain their values until they are explicitly destroyed by the program.
In C, a dynamic variable is created during program execution by calling the library function

1Some high-level languages permit a local variable to retain its value between invocations of the subroutine
by declaring the local variable as a static variable.

58

Chapter 2. Program Development Basics

malloc() or calloc(), which returns the starting address of the memory block assigned
to the variable. The allotted memory locations are explicitly freed by calling the library
routine free(). Once some memory locations are freed, those locations should no longer
be accessed. Doing so may cause a protection fault or, worse yet, corrupt other data in the
program without indicating an error. The following example code illustrates the creation
and destruction of a dynamic variable using malloc() and free(), respectively.

char *cptr;

/* allocate a dynamic char array having size elements */
cptr = (char *)malloc(size * sizeof(char));
...
free(cptr);

/* free the block of memory pointed by cptr */

A persistent variable is one that keeps its value after program execution, and that has
an initial value before program execution. The most common persistent variables are (cid:12)les.

2.3.3

IO Streams and Files

The variable types that we saw so far manage data that originate within the program. If
a program operates only on internally generated data, then it is geared to solve only a
particular instance of a problem (i.e., solving a problem for a particular set of input values
only), and is not likely to be very useful. Instead, if the program accepts external inputs,
then it can solve di(cid:11)erent instances of a problem. Thus, it is important for programs to
have the ability to accept input data. On a similar note, the moment a program completes
its execution, its variables cease to exist, and the variable values disappear, without leaving
a trace of the computation results. For the purposes of performing input and output,
high-level languages provide data types that deal with IO streams and (cid:12)les.

In C, the stdio library supports IO streams and (cid:12)les. Each program can access a
collection of virtual IO devices (stdin, stdout, and stderr) that may be controlled by
a simple set of library functions. The main reason for including IO routines in a library
is their complexity: rather than force every application programmer to write these com-
plex routines, simple economics suggest including them in the library software. Example
library functions in C that deal with IO streams are getchar(), putchar(), scanf(), and
printf(). getchar() allows application programs to read a single character from standard
input, and putchar() allows application programs to write a single character to standard
output; scanf() and printf() are useful for performing formatted IO operations with
standard input and standard output, respectively 2 .

2The behavior of these and other similar functions is precisely de(cid:12)ned by the ANSI C standard. Standards
have been developed for high-level languages by national and international organizations such as ANSI
(American National Standards Institute), IEEE (Institute of Electrical and Electronic Engineers), and ISO

2.3. Data Abstraction

59

For clari(cid:12)cation, we present below a simple C program to copy standard input to
standard output, one character at a time, using the C library routines getchar() and
putchar().

#include <stdio.h> /* contains definitions such as EOF */

main()
f

int c;

while ((c = getchar()) != EOF)
putchar(c);

g

Although IO streams (standard input and standard output) can be used to supply
external input to programs and to obtain the results of computation, they are cumbersome
to handle large amounts of data. Inputting large amounts of data through a keyboard every
time a program is executed is impractical. Moreover, the results sent to the standard output
do not leave a \permanent" record. For these reasons, most high-level languages provide a
data type called (cid:12)le, which provides a permanent way of storing data. Unlike other data
structures provided by an HLL, (cid:12)les may be present before the execution of a program, and
do not vanish when a program terminates; in other words, they persist.

\The palest ink is better than the best memory."
| Chinese proverb

The HLL application programmer is provided with an abstraction of a uniform space
of named (cid:12)les. Thus, HLL application programmers do not concern themselves with any
speci(cid:12)c IO devices; instead they can rely on a single set of (cid:12)le-manipulation OS routines
for (cid:12)le management (and IO device management in an indirect manner). This is sometimes
referred to as device-independent IO. For example, a character may be printed on a
printer by writing the character to the \printer (cid:12)le".

Application programs generally read (formatted) data from one or more (cid:12)les, process the
data, and then write the result to one or more other (cid:12)les. For example, an accounts payable
program reads a (cid:12)le containing invoices and another containing purchase orders, correlates
the data, and then prints a check and writes to a (cid:12)le to describe the expenditures. A
compiler reads an HLL source program (cid:12)le, translates the program into machine language,
and writes the machine language program into an executable (cid:12)le.

The actions that an HLL program is allowed to perform on a (cid:12)le are restricted in certain
ways. First of all, before accessing a (cid:12)le, it has to opened. Secondly, most of the HLLs permit

(International Standards Organization). Adherence to a standard facilitates the development of portable
programs, which can run in di(cid:11)erent computer systems with little or no change. Portability is particularly
important, given that software is a ma jor investment for many computer users.

60

Chapter 2. Program Development Basics

only sequential access to the data present in a (cid:12)le. After completing all accesses to a (cid:12)le,
the (cid:12)le is closed. The activities of opening, closing, reading from, and writing to (cid:12)les are
done using special function calls, which are also typically implemented as part of the library
routines. Example library functions in C that deal with (cid:12)le accesses are fopen(), fclose(),
fscanf(), and fprintf().

Instead of specifying a (cid:12)le’s name every time it is accessed, C provides a (cid:12)le pointer data
type. Unlike other pointer variables, which point to the starting address of a variable, a (cid:12)le
pointer does not point to the (cid:12)le, but rather to a data structure that contains information
about the (cid:12)le. The following C code illustrates how a (cid:12)le pointer is declared, initialized,
and used.

#include <stdio.h>

main()
f

FILE *fp;
int c;

/* special data type for accessing a file */

fp = fopen("fname", "r"); /* open file "fname" in read mode */
c = getc(fp); /* read next character from file pointed by fp */
fclose(fp);
/* close file pointed by fp */

g

HLLs such as C provide a uniform interface for both IO streams and (cid:12)les. The generic
functions used to interact with the IO streams are similar to those provided for the ma-
nipulation of (cid:12)les. In fact, if sequential access to (cid:12)les is assumed, there is practically no
di(cid:11)erence between a virtual device and a (cid:12)le, and hence virtual devices may be manipulated
using the same library routines as those used to access (cid:12)les.

The library routines can be linked to the application program statically at link time or
dynamically at run time.

2.3.4 Data Structures

2.3.5 Modeling Real-World Data

As we saw in Chapter 1, problem solving using a computer involves executing an algorithm
with appropriate input data. For a digital computer to solve real-world problems, the
real-world data has to be converted to a digital form that can be easily represented and
manipulated inside the computer. To begin with, all of the analog data has to be converted
to digital data, possibly resulting in some loss of precision depending on the number of
bits used to represent the digital data. The digital data itself can be represented in many

2.3. Data Abstraction

61

di(cid:11)erent ways. In fact, one of the most important steps in developing a computer algorithm
is to carefully consider what real-world data the algorithm needs to process and then choose
an appropriate internal representation for that data. For some type of data, an internal
representation is fairly obvious; for other types of data, many alternatives exist.

Depending on the architectural level at which programming is done, the details of data
representation will di(cid:11)er.
In this section, we concern ourselves only with how data is
represented at the algorithm development level, arguably the highest level that we can
think of.

2.3.5.1

Images

Images have become an important type of data these days, especially with the popularity
of the internet.
Images form an important part of many documents and presentations.
Images vary greatly, based on size, color, textures, and shapes of ob jects. Di(cid:11)erent formats
are used to represent images, depending on these characteristics as well as processing and
performance requirements. There are two fundamentally di(cid:11)erent ways the computer stores
and manipulates images.

(cid:15) Vector image | Draw-type: In this approach, an image is viewed as a collection of
lines, shapes, and ob jects. Lines and curves can be easily de(cid:12)ned by mathematical
ob jects called vectors. Geometrically de(cid:12)nable shapes can be easily represented as
mathematical ob jects by a small number of parameters. For example, a line can be
completely speci(cid:12)ed by noting the co-ordinates of its end points. A circle can similarly
be speci(cid:12)ed by noting the co-ordinates of its center and the length of its radius.

A draw-type image, often referred to as a vector or scalable image, contains a set
of ob jects whose characteristics are stored mathematically. Each individual ob ject
within the image retains its own characteristics, such as the co-ordinates of its vertices
(corners), the thickness and colour of its outline, the color of its interior, etc. This
makes it possible to target editing actions at speci(cid:12)c elements of an image. Vector
graphics are resolution independent and can be scaled to any size and printed at any
resolution without losing clarity. Vector graphics are best for type and graphics that
must retain crisp lines when scaled to various sizes. Examples of commonly used
software for producing vector images include: CorelDRAW!, Adobe Illustrator, Aldus
Freehand, Microsoft Draw, and AutoDesk AutoCAD.

(cid:15) Raster image (pixelmap, bitmap) | Paint-type: In this approach, an image is viewed
as a rectangular grid of tiny squares called pixels. Each pixel has a speci(cid:12)c location
and color value. When viewed in this manner, the viewer does not \see" ob jects or
shapes! As a result, a raster image can lose detail and appear jagged if viewed at
a high magni(cid:12)cation or printed at too low a resolution. Raster images are best for
representing subtle gradations of shades and color such as in photographs. Raster
images can either be created entirely by computer, or can be sampled initially from

62

Chapter 2. Program Development Basics

other sources, for example, scanning a photograph or capturing a frame of video.
When displaying on computer monitors, all images | vector images and raster images
| are displayed as pixels on-screen.

Image Formats

Several di(cid:11)erent formats are used to represent images. Some of these are lossless in that
they retain all of the information that has been captured about an image. Others are lossy
and approximate some of the information so as to reduce the amount of data required to
store and manipulate the image.

Bitmap (BMP):
It is the Microsoft Windows standard for image (cid:12)les. The bitmap
format stores the image in uncompressed form, and so it is lossless. As the image is not
compressed, it renders good images; however, the data (cid:12)le is likely to be quite large. What
you store is pretty much what you see! Although they do have the ability to support up to
16.7 million colors, there really is no reason why the average user should need to use this
format for image manipulation. Moreover, the bitmap format does not support animation,
transparency, and interlacing.

Graphics Interchange Format (GIF): The GIF is a lossless image format that is the
most common format found on the web. Due to having a 256 maximum color range this
format is ideal for making small icons, buttons, or other graphics that have large blocks of
the same color, but not for images that are required to be photographic quality. Therefore,
if you are working with images from a digital camera, do not use GIF as the (cid:12)le format.
This (cid:12)le format supports transparency, interlace, and can be animated, which makes it a
excellent format for putting images on a website. To reduce the data size, GIF does use a
non-lossy compression algorithm. This means that the compressed image can be converted
back to the original image with no loss of detail. The algorithm works by noting sections of
the image that are the same color, and works quite well for images that have large areas of
solid color with little to no texture. The GIF format is an excellent choice for images that
are cartoonlike in appearance such as banner ads, logos, and text on a solid background.
It is a poor choice for real-life depictions such as photographs of nature or people which
tend to have lots of detailed variations. CompuServe Graphics Interlaced Format. Designed
for transmission over modem links, GIF (cid:12)les are compact images that can be stored in an
interlaced format. This means that one line of pixels in every four will be decoded (cid:12)rst,
allowing the user to see what the image will look like before the whole (cid:12)le is downloaded.
It has a maximum of 8-bit (256 colours). This type of image is used on the World Wide
Web.

Joint Photographic Experts Group (JPEG): The JPEG format was developed by
the Joint Photographic Experts Group committee. It is a lossy (cid:12)le format that was designed

2.3. Data Abstraction

63

with photographic images in mind. A JPEG is capable of storing millions of colors, making
it a great format for saving digital camera photographs and capturing the proper hues and
color that we see in real life. JPEG does support compression, but the more you compress
this type of image, the more loss of detail will occur. The predominant uses for JPEGs are
for photographic images on websites and for storing pictures from digital cameras. Though
the TIFF format is a higher quality format, the size of the resulting TIFF image, makes the
JPEG the more practical choice. This format does not support does not support animation
or transparency, but can be interlaced. It is a lossy compression meaning the compressed
image will not render in as much detail as the original from which it was created. The
JPEG format is an excellent choice for photograpic images which depict the real world such
as nature or people and also for complex backgrounds with lots of texture and detailed
variation.

MPEG: An MPEG (cid:12)le uses a complex algorithm like a JPEG (cid:12)le does { it tries to elim-
inate repetition between frames to signi(cid:12)cantly compress video information. In addition,
it allows a soundtrack (which animated GIFs do not). Because a typical sequence has
hundreds or thousands of frames, (cid:12)le sizes can still get quite large.

Shockwave: Shockwave provides a vector-based animation capability. Instead of speci-
fying the color of every pixel, a Shockwave (cid:12)le speci(cid:12)es the coordinates of shapes (ob jects
like lines, rectangles, circles, etc.) as well as the color of each shape. Shockwave (cid:12)les can be
extremely small. They allow animation and sound. The images are also scalable; because
they are vector-based, the image can be enlarged and it will still look great.

Tagged Image File Format (TIFF): The TIFF format is a lossless image format that
is the considered the best choice for photographic image quality. Most digital cameras give
the option of using TIFF as the format it saves (cid:12)les in. The main problem with this (cid:12)le
format is that most applications do not compress the TIFF (cid:12)les, and so they can be quite
large. This is not much of a problem for storing the pictures on a computer, but with
limited (cid:13)ash memory sizes for cameras it could limit the amount of pictures that can be
stored on one card. If you have the storage, then the TIFF format is highly recommended,
but if you do not have the space, then go with a JPEG as you most likely will not notice
a di(cid:11)erence in image quality. This format does not support animation, transparency, and
can not be interlaced.

Encapsulated PostScript (EPS): Adobe PostScript is the industry standard page de-
scription language. EPS (cid:12)les can contain both paint and draw type information, and can
often become extremely large.

Name Extension Compressed Loss Animated Max Colors Transparency Interlaced

64

Chapter 2. Program Development Basics

Graphics Interchange Format .GIF Yes Lossless Yes 256 Yes Yes Joint Photographic
Experts Group .JPG Yes Lossy No 16.7 Million No Yes Portable Network Graphics .PNG
Yes Lossless Yes 256/16.7 Mil Yes Yes Bit-Map .BMP Rarely Lossless No 16.7 Million No
No Tagged Image File Format .TIFF or .TIF Yes Lossless No 16.7 Million No No

Figure 4: Graphic Formats and their Attributes

2.3.5.2 Video

Video data is a natural extension of graphical data. It is a sequence of still images that
depict how the scene changes with the passage of time. If these still images are displayed
at the rate of 30 or more images per second, then, to the human eye, they appear as a
continuous motion picture.

2.3.5.3 Audio

Another real-world entity that we often represent and process inside a computer is audio.
Although real-world audio is analog in nature, inside the computer it is stored in digital form,
called digital audio. Digitization is done by electronically sampling the analog waveform at
regular time intervals; the time intervals should be small enough to capture every nuance in
the analog signal. Each sample is then approximated to one of the allowable discrete values
using an analog-to-digital (A/D) converter. These discrete values are then represented in a
binary format.

2.4 Operators and Assignments

Apart from providing di(cid:11)erent data types, high-level languages also provide a set of arith-
metic operators and logical operators that allow the programmer to specify manipulation
of variables. Operators act on the values assigned to variables. Variables, along with the
operators, allow the HLL programmer to express the computation speci(cid:12)ed in the algorithm
to be implemented.

The assignment statement in C involves the evaluation of expressions composed of op-
erators, variables, and constants. In C, as in most HLLs, all operators are either monadic
or dyadic, i.e, involve one or two operands.

Another operator available in C and other languages that support pointers is the address-
of operator, \&", to take the address of a static variable.

2.5. Control Abstraction

65

2.5 Control Abstraction

What distinguishes a computer from a simple calculator is its ability to make decisions:
based on the input data and the values created during the computation, di(cid:11)erent instruc-
tions are executed.

2.5.1 Conditional Statements

I shal l be tel ling this with a sigh
Somewhere ages and ages hence:
Two roads diverged in a wood, and I
I took the one less traveled by,
And that has made al l the di(cid:11)erence.

Robert Frost

I have always known
That at last I would
Take this road, but yesterday
I did not know that it would be today."

| The Road Not Taken by

of Ise) by Ariwara no Narihira (9th century Japan)

| Ise Monogatari (The Tales

if-else: Often it is necessary to have the program execute some statements only if a
condition is satis(cid:12)ed. All high-level languages provide if constructs to program conditional
execution. The C if statement has the syntax

if (expr)
stmt;

The semantics of this statement is: if expr evaluates to true, execute stmt, otherwise skip
stmt. Sometimes, a program may need to choose one of two alternative (cid:13)ows of control,
depending on the value of a variable or expression. For this purpose, the C language provides
an if-else construct, which has the syntax

if (expr)
stmt1;

else

stmt2;

The semantics of this statement is:
execute stmt2.

if expr evaluates to true, execute stmt1, otherwise

66

Chapter 2. Program Development Basics

switch: High-level languages also generally provide a construct for specifying multi-way
decisions. The C language provides a switch construct, which compares the value of an
expression against a list of supplied constant integer values, and depending on the match
selects a particular set of statements. Its syntax is given below.

switch (expr)
f

case const-expr1:
stmt1;
break;
case const-expr2:
stmt2;
break;
default:
stmt3;

g

The semantics of this switch statement is: if expr when evaluated matches the evaluated
value of const-expr1 or const-expr2, then control branches to stmt1 or stmt2, respectively.
If there is no match, then control branches to stmt3, which follows the default label. After
executing the statements associated with the matching case, control falls through to the
next case unless control is explicitly taken out of the switch construct using statements
such as break, goto, return, or exit().

2.5.2 Loops

while: Most programs require some action to be repeated a number of times, so long as
a particular condition is satis(cid:12)ed. Although it may be possible to include the same code
the required number of times, that would be quite tedious and result in longer programs.
Moreover, this may not be possible in cases where we do not know at programming time
the number of times the action needs to be repeated. High-level languages provide loop
constructs to express such code in a concise manner. The C while loop statement has the
syntax

while (expr)
stmt;

The semantics of this statement is: if expr evaluates to a non-zero value, execute stmt, and
repeat this process until expr evaluates to zero. stmt normally modi(cid:12)es a variable that is
contained in expr so that expr will eventually become false and therefore terminate the loop.

for: Although a while loop is su(cid:14)cient for implementing any kind of loops, most HLLs
provide a for loop construct to express some loops in a more elegant manner. The C for

2.5. Control Abstraction

statement has the syntax

for (initial expr; expr2; update expr)
stmt;

67

The semantics of this for loop can be easily understood when considering its equivalent
while loop, given below:

initial expr;
while (expr2)
f

stmt;
update expr;

g

In this loop, initial expr is the initializing statement; expr2 calculates the terminating con-
dition; and update expr implements the update of the loop variables.

2.5.3 Subroutines

The next topic that we like to discuss is the subroutine, procedure, function, or method
concept, an important innovation in the development of programming languages.
In a
given program, it is often necessary to perform a speci(cid:12)c task on di(cid:11)erent data values.
Such a repeated task is normally implemented as a subroutine, which can be called from
di(cid:11)erent places in a program. For example, a subroutine may evaluate the mathematical
sine function or sort a list of values into increasing or decreasing order. At any point in
the program the subroutine may be invoked or cal led. That is, at that point, the computer
is instructed to suspend the execution of the cal ler and execute the subroutine. Once the
subroutine’s execution is completed, control returns to the point from where the call was
made.

The two principal reasons for the use of subroutines are economy and modularity. A
subroutine allows the same piece of code to be called from di(cid:11)erent places in the program.
This is important for economy in programming e(cid:11)ort, and for reducing the storage require-
ments of the program. Subroutines also allow large programming tasks to be subdivided into
smaller units. This use of modularity greatly eases the programming task. The following C
code illustrates the use of subroutines in constructing a modular program.

main()
f

float a, b, c, d;

b = bigger(a, b);

68

Chapter 2. Program Development Basics

d = bigger(c, d);

g

bigger(float p, float q)
f

if (p >= q)
return p;

else

return q;

g

In this code, the function bigger() is used to determine which of its two input pa-
rameters (p or q) is bigger. This function is called twice from the function main().
If
the language does not support subroutines/functions, then most of the body of function
bigger() will need to be repeated twice in main().

The subroutine concept thus permits control abstraction; the code fragment within the
subroutine can be referred by the subroutine name at the calling place, where it is thought in
terms of its function rather than its implementation. In structured programming languages,
subroutines (and macros) are the main mechanism for control abstraction.

2.5.3.1 Parameter Passing

2.5.4 Subroutine Nesting and Recursion

If a subroutine calls a subroutine (either itself or another subroutine), then that is called
subroutine nesting. Conceptually, subroutine nesting can be carried out to any depth. The
(cid:12)rst subroutine to complete will be the one that was called last, causing control to return
to the one that called it.

If nesting involves calling the same subroutine itself, either directly or through other
subroutines, then we call that recursion. Recursion is an important concept in computer
science, analogous to the concept of induction in mathematics. Mathematical problems that
can be explained by induction invariably have an elegant recursive solution algorithm.

A rose is a rose is a rose

| Gertrude Stein

2.5.5 Re-entrant Subroutine

In a multitasked environment, many programs (or processes) can be simultaneously active
in a computer system. Many of the concurrently active processes may share a few routines,
especially those belonging to libraries and the OS. Consider the scenario where a context

2.6. Library API

69

switch happens when a process is in the middle of the execution of a subroutine. Before
the control is restored to this process, it is quite possible for another process to execute
the same subroutine. In order for a subroutine to be executed by another process in this
manner, the subroutine must be re-entrant, i.e., it must not update global variables.

2.5.6 Program Modules

We can let a subroutine to call subroutines that are physically present in other modules.
The caller and callee subroutines may be written by di(cid:11)erent programmers, at di(cid:11)erent
times! This calls for de(cid:12)ning and using speci(cid:12)c interfaces for each subroutine that is likely
to be called from other modules.

2.5.7 Software Interfaces: API and ABI

Application programs do not directly implement all of the functionality required. Instead,
they frequently invoke the services of library routines and OS routines.

2.5.7.1 Application Binary Interface (ABI)

An application binary interface (ABI) speci(cid:12)es the machine language interface provided by
an execution environment, which is usually a hardware platform along with the operating
system running on it (e.g., Linux ABI for the ARM Architecture). Thus, the ABI refers to
the speci(cid:12)cations to which an executable should conform in order to execute in a speci(cid:12)c
execution environment. The ABI includes the user mode instruction set architecture (the
memory address space, the number, sizes and reserved uses of registers, and the instruction
set), the ISA-level system call interface supported by the operating system (including the
system call numbers and how an application should make system calls to the operating
system), and the binary format of executable (cid:12)les. ABIs also cover other details of system
calls, such as the calling convention, which tells how functions’ arguments are passed and
return values retrieved. ABIs deal with run-time compatibility; a program binary targeted
to an ABI can run (without relinking or recompilation) on any system that supports the
same ABI. Application binary interfaces are also known as Abstract Machine Interface.

2.6 Library API

A practical extension of the above modularization concept is the use of library modules. A
library is a group of commonly used subroutines or functions bundled into a single module.
The basic libraries contain routines that read and write data, allocate and deallocate mem-
ory, and perform complex operations such as square root calculation and sorting. Other
libraries contain routines to access a database or manipulate terminal windows. Apart from

70

Chapter 2. Program Development Basics

language-dependent library such as the C stdio which provides IO routines, we also have
language-independent but OS-dependent libraries such as the Solaris thread library which
provides support functions for multithreading, and language- and platform-independent li-
braries such as the MPI (which supports the message-passing model of parallel processing)
and the OpenGL (which supports advanced graphics functions).

2.7 Operating System API

A hardware system, augmented by an operating system, cannot do much unless application
programs are loaded in it. Application programs are what the end users run when .............
In modern computers, application programs never run in a \vacuum" either. They rely
heavily on support from pre-written software such as library routines and the operating
system. When the application program wants to carry out a functionality that has already
been implemented in a library routine or in the OS, it simply invokes the functionality and
does not directly code the functionality.

An application programming interface (API) speci(cid:12)es a language and message format
used by an application program to communicate with a systems program (library, OS) that
provides services to it. Three commonly used OS APIs are the POSIX API for UNIX,
Linux, and MAC OS X systems; the Win32 API for Windows systems; and the Java API
for the Java virtual machine. It is the interface by which an application gains access to
operating systems.

The application program interface (API) de(cid:12)nes the calls that can be made from an
application to the operating system. Notice that adherence to an API does not ensure
runtime compatibility.

An API speci(cid:12)es a set of calling conventions that de(cid:12)nes how a service is invoked through
a software package. The calls, subroutines, interrupts, and returns that comprise a doc-
umented interface so that a higher-level program such as an application can make use of
the services of another application, operating system, network operating system, driver, or
other lower-level software program.

In the software (cid:12)eld, APIs are structured abstraction layers that conceal the gory details
of an individual application, operating system or hardware item and the world outside that
software or hardware.

All programs using a common API will have similar interfaces. This makes it easier to
port programs across multiple systems and for users to learn new programs.

An application program interface or application programming interface (API) is the
speci(cid:12)c method prescribed by a computer operating system or by an application program
by which a programmer writing an application program can make requests of the operating
system or another application.

We just saw the use of library routines in supporting IO streams and (cid:12)les. The library

2.7. Operating System API

71

routines do not directly access the IO devices, however. As we saw in Section 1.2.2, access to
and control of many of the hardware resources are regulated through the operating system.
This is because the low-level details of the system’s operation are often of no interest to
the computation at hand, and it is better to free the application programmer and the
library developer from dealing with the low-level details. The problem is further aggravated
because the program must be able to deal with a multitude of di(cid:11)erent IO interfaces, not
only for di(cid:11)erent device types but also for di(cid:11)erent models within the same class of devices.
It would not be practical to require each programmer to know the operational details of
every device that will be accessed by a program. Moreover, the number and type of devices
may change in course of time.
It is important that such changes remain transparent to
application programs.

To satisfy the preceding requirements of device independence, the operating system pro-
vides an abstract interface to the application programmer. This interface, called application
programming interface (API) of the OS, presents a greatly simpli(cid:12)ed view of the hardware
resources of the computing environment. The OS’ API is simply a set of commands that
can be issued by the application program to the operating system. If the OS performs these
jobs, it simpli(cid:12)es the job of the application programmer, who need not get involved with
the details of hardware devices. Further, when an application program uses the API, it is
shielded from changes in the computer hardware as new computers are developed. To be
speci(cid:12)c, the operating system and its device drivers can be changed to support new com-
puter hardware while preserving the API unchanged and allowing application programs to
run unchanged on the new computer. The API provided by the OS is formally de(cid:12)ned by
a set of human readable function call de(cid:12)nitions, including call and return parameters.

API

Application Programs

Library Routines
System Calls

System Call Interface

Operating System Routines

Figure 2.1: Illustration of API

Applications programs written with a particular API in mind can be run only on systems
that implement the same API. The APIs de(cid:12)ned by most operating systems are very similar,
and di(cid:11)er only in some minor aspects. This makes it easy to port application programs
developed for one OS to another. Examples of APIs are the Portable Operating System

72

Chapter 2. Program Development Basics

Interface (POSIX) and the Win32 API. The POSIX standard is based on early UNIX
systems and is widely used in UNIX-based operating systems such as FreeBSD and Linux.
Many non-UNIX systems also support POSIX. The Win32 API is the one implemented in
a Microsoft Windows environment. It supports many more functions (about 2000) than
POSIX, as it includes functions dealing with the desktop window system also.

2.7.1 What Should be Done by the OS?

(cid:15) Simple vs Complex Function

(cid:15) Speci(cid:12)c vs Generic Function

(cid:15) Security

The commands supported by an OS’ API can be classi(cid:12)ed into 3 classes:

(cid:15) Input/Output management

(cid:15) Memory management

(cid:15) Process management

2.7.2

Input/Output Management

The most commonly used part of the OS’ API is the part that deals with input/output.
The basic abstraction provided for application programmers and library programmers to
perform input and output operations is called a (cid:12)le. The (cid:12)le abstraction supported by the
API is more basic than the one supported by library routines in that it is simply a sequence
of bytes (with no formatting) in an IO device. This de(cid:12)nition of the (cid:12)le as a stream
of bytes imposes little structure on a (cid:12)le; any further structure is up to the application
programs, which may interpret the byte stream as they wish. The interpretation has no
bearing on how the OS stores the data. Thus, the syntax for accessing the data in a (cid:12)le is
de(cid:12)ned by the API, and is identical for all application programs, but the semantics of the
data are imposed by the application program. For instance, the text formatting program
LaTeX expects to (cid:12)nd \new-line" characters at the end of each line of text, and the system
accounting program acctcom expects to (cid:12)nd (cid:12)xed-length records in the (cid:12)le. Both programs
use the same API commands to access the data in the (cid:12)le as a byte stream, and internally,
they parse the stream into the appropriate format.

The (cid:12)le abstraction provided by the API is also device-independent like the one pro-
vided by the standard library. That is, it hides all device-speci(cid:12)c aspects of (cid:12)le manipulation
from HLL application and library programmers, and provides instead an abstraction of a
simple, uniform space of named (cid:12)les.

2.7. Operating System API

2.7.2.1 Example

73

For illustration, let us consider the UNIX API. In this API, all input and output operations
are done by reading or writing (cid:12)les, because all IO devices|including the user program’s
terminal|are considered as (cid:12)les. This means that a single, homogeneous interface handles
all communication between a program and peripheral devices.

The lowest level of IO in the UNIX API provides no bu(cid:11)ering or any other services. All
input and output operations are done by two API functions called read and write, which
specify a (cid:12)le descriptor and the number of bytes to be transferred. The second argument
is a bu(cid:11)er in the application program’s memory space where the data is to come from or
go to. The third argument is the number of bytes to be transferred. The calls are done as
follows:

numbytes read = read(fd, buf, numbytes);
numbytes written = write(fd, buf, numbytes);

Each of these system calls returns a byte count indicating the number of bytes actually
transferred. When reading, the number of bytes returned may be less than the number
asked for. A return value of zero indicates end of (cid:12)le, and a (cid:0)1 indicates an error 3 .

2.7.2.2 A Trace of a System Call

When an application program executes a read() system call to read data from a (cid:12)le, a
set of OS functions are called, which may eventually result in calling an appropriate device
driver. Figure 2.2 illustrates a situation in which an application program calls the OS twice
for reading from standard input. During the (cid:12)rst time, it calls the scanf() library function,
which calls the read() system call de(cid:12)ned in the API. During the second time, it directly
calls the read() system call.
In both cases, the read() routine calls the keybd read()
device driver to perform the actual read operation.

If a di(cid:11)erent keyboard is used in the future, only the keyboard device drivers need to be
changed; the application program, the library routines, and the device-independent part of
the OS require no change.

2.7.3 Memory Management

High-level languages that support pointers naturally allow dynamic allocation (and deal-
location) of memory. Applications programmers typically do allocation and deallocation
using library functions such as malloc() and free(), as we already saw in Section 2.1.
The malloc(n) function, for instance, returns a pointer to an unused block of n bytes.

3More details of read and write system calls can be obtained in a UNIX/Linux system by typing man 2
read and man 2 write, respectively.

74

Chapter 2. Program Development Basics

Library
Interface

API

scanf()
{

read();

}

main()
{

scanf();

read();

}

read()
{

keybd_read();

keybd_read()
{

}

}

Application Program

Library Routines

OS Kernel
(Device−Independent IO)

Device Drivers
(Device−Dependent IO)

Figure 2.2: A Trace of Routines Executed when Calling API Function read()

obtains large chunks of memory address space from the OS using the sbrk() system call
provided by the API, and manages them. The system call sbrk(b) returns a pointer to b
more bytes of memory.

2.7.4 Process Management

process creation

context switch

process control block

2.8 Operating System Organization

In modern computers, the operating system is the only software component that runs in the
Kernel mode. It behooves us therefore to consider the structure and implementation of an
operating system. In particular, it is important to see how the system calls speci(cid:12)ed in the
application programming interface (API) (provided to user programs and library functions)

2.8. Operating System Organization

75

are implemented by operating systems4 . The exact internal details of an operating system
vary considerably from one system to another. It is beyond the scope of this book to discuss
di(cid:11)erent possibilities. Figure 4.7 gives a possible block diagram, which is somewhat similar
to that of a standard UNIX kernel.
In the (cid:12)gure, the kernel mode software blocks are
shown shaded. The main components of the OS software include a system call interface,
(cid:12)le system, process control system, and the device management system (device drivers).
This organization uses a layered approach, which makes it easier to develop the OS and to
introduce modi(cid:12)cations at a later time. This also makes it easier to debug the OS code,
because the e(cid:11)ect of bugs may be restricted to a single layer. The (cid:12)gure also shows the
relationship of the OS to user programs, library routines, and the hardware.

HCI

User Mode
Software

ABI

OS Interface

Application Programs and Shell

Dynamically Linked Libraries
System Calls

System Call Layer

Device−
Independent

File System

Kernel Mode
Software

IO Management
Character−
Network−
Oriented
Oriented

Block−
Oriented

Process Control System

Inter−process
Communication

Scheduler

Memory
Management

User Mode
Instructions

Device−
Dependent

Device Drivers

OS Kernel

s
t
n
e
v
E
 
l
a
n
o
i
t
p
e
c
x
E

ISA

Kernel Mode

User Mode

Device Controllers

Hardware

IO Registers

IO Control Logic

IO Devices

Privileged Registers and Memory User Mode Registers and Memory
Control Unit, ALU, and Memory Controllers

Device Interrupts

Program Control Transfers Initiated by Software

Program Control Transfers Initiated by Hardware

Hardware Accessed/Controlled

Hardware Buses

Hardware Connections

Figure 2.3: Block Diagram Showing How a UNIX-like Kernel Implements the API

4Operating systems typically play two roles|controlling the environment provided to the end user and
controlling the environment provided to application programs. The former involves tasks such as maintaining
a (cid:12)le structure and supporting a graphical user interface. The latter involves taks such as reading a speci(cid:12)ed
number of bytes from a (cid:12)le on behalf of an application program. In this book, we are concerned only with
the latter role, as it is closer to computer architecture.

76

Chapter 2. Program Development Basics

2.8.1 System Call Interface

The system call interface provides one or more entry points for servicing system call instruc-
tions and exceptions, and in some cases device interrupts also. The system call interface
code copies the arguments of the system call and saves the user process’ context. It then
uses the system call type to look up a system cal l dispatch vector to determine the ker-
nel function to be called to implement that particular system call, interrupt, or exception.
When this kernel function completes, the system call interface restores the user process’
context, and switches to User Mode, transferring control back to the user process. It also
sends the return values and error status to the user program.

We can summarize the functions performed by the system call interface:

(cid:15) Determine type of syscall

(cid:15) Save process context

(cid:15) Call appropriate hander

(cid:15) Restore process context

(cid:15) Return to user program

2.8.2 File System

The API provided to application programs by the operating system, as we saw earlier, in-
cludes device-independent IO. That is, the interface is the same, irrespective of the physical
device that is involved in the IO operation. The (cid:12)le abstraction part of the API is sup-
posed to hide all device-speci(cid:12)c aspects of (cid:12)le manipulation from application programmers,
and provide them with an abstraction of a simple, uniform space of named (cid:12)les. Thus,
application programmers can rely on a single set of (cid:12)le-manipulation OS routines for (cid:12)le
management (and IO device management in an indirect manner). This is sometimes referred
device-
independent
to as device-independent IO.
IO

As we saw in Section 3.4.7, application programs access IO (i.e., (cid:12)les) through read and
write system call instructions. The read and write system call instructions (of the User
mode) are implemented in the Kernel mode by the (cid:12)le system part of the OS, possibly with
the help of appropriate device drivers.

Files of a computer installation may be stored on a number of physical devices, such as
disk drives, CD-ROM drives, and magnetic tapes, each of which can store many (cid:12)les. If the
IO device is a storage device, such as a disk, the (cid:12)le can be read back later; if the device is
a non-storage device such as a printer or monitor, the (cid:12)le cannot be read back. Di(cid:11)erent
(cid:12)les may store di(cid:11)erent kinds of data, for example, a picture, a spreadsheet, or the text of
a book chapter. As far as the OS is concerned, a (cid:12)le is simply a sequence of bytes written
to an IO device.

2.8. Operating System Organization

77

The OS partitions each (cid:12)le into blocks of (cid:12)xed size. Each block in a (cid:12)le has an address
that uniquely tells where within the physical device the block is located. Data is moved
between main memory and secondary storage in units of a single block, so as to take
advantage of the physical characteristics of storage devices such as magnetic disks and
optical disks.

File management related system calls invoked by application programs are interpreted
by the (cid:12)le system part of the OS, and transformed into device-speci(cid:12)c commands. The
process of implementing the open system call thus involves locating the (cid:12)le on disk, and
bringing into main memory all of the information necessary to access it. The OS also
reserves for the (cid:12)le a bu(cid:11)er space in its memory space, of size equal to that of a block.
When an application program invokes a system call to write some bytes to a (cid:12)le, the (cid:12)le
system part of the OS writes the bytes in the bu(cid:11)er allotted for the (cid:12)le. When the bu(cid:11)er
becomes full, the (cid:12)le system copies it into a block in a storage device (by invoking the
device’s device driver); this block becomes the next block of the (cid:12)le. When the application
process invokes the close system call for closing a (cid:12)le, the (cid:12)le system writes the (cid:12)le’s bu(cid:11)er
as the (cid:12)nal block of the (cid:12)le, irrespective of whether the bu(cid:11)er is full or not, prior to closing
the (cid:12)le. Closing a (cid:12)le involves freeing up the table space used to hold information about
the (cid:12)le, and reclaiming the bu(cid:11)er space allotted for the (cid:12)le.

2.8.3 Device Management: Device Drivers

The device management part of the OS is implemented as a collection of device drivers.
Most computers have input/output devices such as terminals and printers, and storage
devices such as disks. Each of these devices requires speci(cid:12)c device driver software, which
acts as an interface between the device controller and the (cid:12)le system part of the OS kernel.
A device driver is needed because each device has its own speci(cid:12)c commands instead of
generic commands. A printer device driver, for instance, contains all the software that is
speci(cid:12)c to a prticular type of printer such as a Postscript printer. Thus, the device drivers
form the device-dependent part of the IO software. By partitioning the kernel mode software
into device-independent and device-dependent components, the task of adding a new device
to the computer is greatly simpli(cid:12)ed.

The device drivers form a ma jor portion of the kernel mode software. Each device driver
itself is a collection of routines, and can have multiple entry points. The device driver
receives generic commands from the OS (cid:12)le system and converts them into the specialized
commands for the device, and vice versa. To the maximum extent possible the driver
software hides the unique characteristics of a device from the OS (cid:12)le system.

Device drivers can be fairly complex. Many parameters may need to be set prior to
starting a device controller, and many status bits may need to be checked after the comple-
tion of each device operation. Many device drivers such as the keyboard driver are supplied
as part of the pre-installed system software. Device drivers for other devices need to be
installed as and when these devices are installed.

78

Chapter 2. Program Development Basics

The routines in a device driver can be grouped into three kinds, based on functionality:

(cid:15) Autocon(cid:12)guration and initialization routines

(cid:15) IO initiation routines

(cid:15) IO continuation routines (interrupt service routinestem Autocon(cid:12)guration and initial-
ization routines

(cid:15) IO initiation routines

(cid:15) IO continuation routines (interrupt service routines)

The autocon(cid:12)guration routines are called at system reboot time, to check if the corre-
sponding device controller is present, and to perform the required initialization. The IO
initiation routines are called by the OS (cid:12)le system or process control system in response to
system call requests from application programs. These routines check the device status, and
initiate IO requests by sending commands to the device controller. If program-controlled
IO transfer is used for the device, then the IO initiation routines perform the IO transfers
also. By contrast, if interrupt-driven IO transfer is used for the device, then the actual IO
transfer is done by the interrupt service routines when the device becomes ready and issues
an interrupt.

2.8.4 Hardware Abstraction Layer (HAL)

The hardware abstraction layer provides a slightly abstract view of the hardware to the
OS kernel and the device drivers. By hiding the hardware details, it provides a consistent
hardware platform for the OS. This makes it easy to port an OS across a family of hardware
platforms that have the same user mode ISA, but di(cid:11)er in the kernel mode ISA (such as
di(cid:11)erent MMU architectures).

2.8.5 Process Control System

2.8.5.1 Multi-Tasking

When a computer system supports multi-tasking, each process sees a separate virtual ma-
chine, although the concurrent processes are sharing the same physical resources. Therefore,
some means must be provided to separate the virtual machines from each other at the phys-
ical level. The physical resources that are typically shared by the virtual machines are the
processor (including the registers, ALU, etc), the physical memory, and the IO interfaces.
Of these, the processor and the IO interfaces are typically time-shared between the pro-
cesses (temporal separation), and the physical memory is partitioned between the processes

2.8. Operating System Organization

79

(spatial separation)5 . To perform a context switch of the virtual machines, the time-shared
resources must be switched from one virtual machine to the next. This switching must be
managed in such a way that the virtual machines do not interact through any state infor-
mation that may be present in the physically shared resources. For example, the ISA-visible
registers must be saved and restored during a context switch so that the new context cannot
access the old context’s register state.

Decisions regarding time-sharing and space-sharing are taken in the Kernel mode by the
operating system, which is responsible for allocating the physical resources to the virtual
machines. If a user process is allowed to make this decision, then it could possibly encroach
into another process’ resources, and tamper with its execution. The operating system’s
decisions, however, need to be enforced when the system is in the User mode. This enforce-
ment is done using special hardware (microarchitectural) support so that the enforcement
activity does not reduce performance.

2.8.5.2 Multi-Programming

Some applications can be most conveniently programmed for two or more cooperating pro-
cesses running in parallel rather than for a single process. In order for several processes to
work together in parallel, certain new Kernel mode instructions are needed. Most modern
operating systems allow processes to be created and terminated dynamically. To take full
advantage of this feature to achieve parallel processing, a system call to create a new pro-
cess is needed. This system call may just make a clone of the caller, or it may allow the
creating process to specify the initial state of the new process, including its program, data,
and starting address. In some cases, the creating (parent) process maintains partial or even
complete control over the created (child) processes. To this end, Kernel mode instructions
are added for a parent to stop, restart, examine, and terminate its children.

5Time-sharing the entire physical memory is not feasible, because it necessitates saving the physical
memory contents during each context switch.

80

Chapter 2. Program Development Basics

2.9 Ma jor Issues in Program Development

2.9.1 Portability

2.9.2 Reusability

2.9.3 Concurrency

2.10 Concluding Remarks

2.11 Exercises

Chapter 3

Assembly-Level Architecture |
User Mode

He who scorns instruction wil l pay for it, but he who respects a command is rewarded.

Proverbs 13: 13

This is the (cid:12)rst of several chapters that address core issues in computer architecture.
The previous chapter discussed high-level architectures. This chapter is concerned with the
immediately lower-level architecture that is present in essentially all modern computers: the
assembly-level architecture. This architecture deals with the way programs are executed
in a computer from the assembly language programmer’s viewpoint. We describe ways in
which sequences of instructions are executed to implement high-level language statements.
We also discuss commonly used techniques for addressing memory locations and registers.
A proper understanding of these concepts is an essential part of the study of computer
architecture, organization, and design. We introduce new concepts in machine-independent
terms to emphasize that they apply to all computers.

The vast ma jority of today’s programs are written in a high-level language such as C,
FORTRAN, C++, and Java. Before the introduction of high-level languages, early pro-
grammers and computer architects were using languages of a di(cid:11)erent type, called assembly
languages. The main purpose of discussing assembly-level architectures in this book is
to provide an adequate link to instruction set architectures and microarchitectures, which
provide a closer view on how computers are built and how they operate. To execute any
high-level program, it must (cid:12)rst be translated into a lower level program (most often by a
compiler and occasionally by assembly language programmers). Knowledge of the assembly-
level architecture is a must, both for compiler writers and for assembly language program-
mers. The relationship between high-level, assembly, and machine language features is a key

81

82

Chapter 3. Assembly-Level Architecture | User Mode

consideration in computer architecture. Much of the discussion in this chapter is applicable
to both the assembly-level architecture and the instruction set architecture, as the former
is a symbolic representation of the latter.

The ob jective of this chapter on assembly-level architecture is not to make you pro(cid:12)cient
in assembly language programming, but rather to help you understand what this virtual
machine does, and how high-level language programs are converted to assembly language
programs. We include a number of code fragments that are short and useful for clari(cid:12)cation.
These code fragments are meant to be conceptual, rather than to be cut and pasted into
your application programs. If you like to write intricate assembly language programs, it is
better to follow up this material with a good book on assembly language programming.

3.1 Overview of User Mode Assembly-Level Architecture

We shall (cid:12)rst present an overview of the basic traits of an assembly language machine. You
will notice that these basic traits closely resemble those of the generic computer organization
described in Section 1.2. An assembly language machine is designed to support a variety of
high-level languages, and is not tailored to a particular high-level language. Thus, programs
written in di(cid:11)erent high-level languages can be translated to the same assembly language.

It is also interesting to note that many of the popular assembly-level architectures are
quite similar to each other, just like the case of many popular high-level architectures.
Therefore once you master one, it is easy to learn others. This similarity occurs because
any given assembly-level architecture closely follows an instruction set architecture (ISA),
and ISA design is driven by hardware technology and application requirements. Di(cid:11)erent
ISAs have many things in common, because they target similar application domain, and
are interpreted by hardware machines built using similar hardware technologies.

\Real programmers can write assembly code in any language. :-)"
{ Larry Wal l (the Perl guy)

\Al l people smile in the same language."
{ Author Unknown

As mentioned in Section 1.8, an architecture speci(cid:12)es what data can be named by a
program written for that architecture, what operations can be performed on the named
data, and what ordering exists among the operations. When writing an assembly language
program, the locations that can be named are the (virtual) memory address space, and
the registers. The value returned by a read to an address is the last value written to that
address. In most languages, sequential order is implied among the instructions. That is,
instructions are to be executed one after the other, in the order in which they are speci(cid:12)ed
in the program.

3.1. Overview of User Mode Assembly-Level Architecture

83

3.1.1 Assembly Language Alphabet and Syntax

The programmer must tell the computer, through the statements/instructions in the pro-
gramming language used, everything that the computer must do. We shall look at the
di(cid:11)erent facets of an assembly language.

3.1.1.1 Alphabet

High-level languages, as we saw in Chapter 2, are somewhat close to natural languages,
and are substantially removed from the underlying hardware levels. An assembly language
also uses symbols and words from natural languages such as English, but is closer to the
underlying hardware1 . Formally speaking, an assembly language consists of a set of symbolic
names and a set of rules for their use. The symbolic names are called mnemonics (which
mean \aid to memory" in the Greek language).

3.1.1.2 Syntax

The syntax of a language is the set of rules for putting together di(cid:11)erent tokens to produce
a sequence of valid statements or instrucions. In the case of assembly languages, this deals
with the rules of using the mnemonics in the speci(cid:12)cation of complete instructions and
assembly language programs. The syntax followed by assembly languages is quite di(cid:11)erent
from that followed by high-level languages. Instructions are generally speci(cid:12)ed in a single
line by an opcode mnemonic followed by zero or more operand mnemonics. The opcode (cid:12)eld
may be preceded by a label, and the instruction may be followed by a comment, which starts
with a character such as \;" or \#". Most of the assembly languages require the label to
start in the (cid:12)rst column of the line, and instructions to start only from the second column
or later. You might wonder why the syntax of an assembly language is so restrictive. The
reason is to simplify the assembler, which was traditionally written in assembly language to
occupy very little space in memory. Apart from instructions, an assembly language program
also contains assembler directives, which separate data values from instructions and specify
information regarding data values.

3.1.2 Memory Model

An assembly-level architecture de(cid:12)nes a memory model consisting of a large number of
locations, each of which is a (cid:12)xed-size group of storage cel ls. Each cell can store a single bit

1Speci(cid:12)cally, an assembly language is a symbolic representation of the machine language|which uses
only bit patterns (1s and 0s) to specify information. It uses a richer set of symbols (including the English
alphabet) instead of bits, and gives symbolic names to commonly occurring bit patterns, such as opcodes
and register speci(cid:12)ers, which make it easier for humans to read and comprehend them. For example,
in assembly language, we use instructions such as add $at, $v0, $v1 in place of bit patterns such as
000000000010001000011000000000000.

84

Chapter 3. Assembly-Level Architecture | User Mode

of information|a 0 or a 1. Most assembly languages consider a location to store 8 bits, or
a byte. Because a single bit represents a very small amount of information, bits are seldom
handled individually; the usual approach is to read or write one or more locations at a time.
Most assembly languages provide instructions for manipulating data items of di(cid:11)erent sizes,
such as 8 bits, 16 bits, 32 bits, and 64 bits.

For the purpose of reading or writing memory locations, each memory location is given a
unique address. The collection of all memory locations is often called the memory address
space. Although assembly languages have provision to deal with numerical addresses, it is
customary to use labels to refer to memory locations. As labels can be constructed using
alphabetical letters (in addition to numerical digits if required), they are easier for the
programmer to keep track of.

The instructions and data of an assembly language program are strongly tied to the
memory address space. Each instruction or data item is viewed as occupying one location
(or a contiguous set of locations) in the memory address space. Although the assembly
language programmer can assign instructions and data items to memory locations in a
random manner, for functional reasons it is better to organize the locations into a few
sections, much like how the ma^(cid:16)tre d’h^otel organizes a restaurant dining area into smoking
and non-smoking sections. Each section holds a chunk of code or data that logically belongs
together. Some of the sections commonly used by assembly language programs are text
(code), data, heap, and stack, as illustrated in Figure 3.1.

Memory Address Space

Addresses
0x00000000

Text

Data

Stack

Direction of growth
at run−time

Figure 3.1: Organizing the Memory Address Space as Multiple Sections

3.1. Overview of User Mode Assembly-Level Architecture

85

The text section, as its name implies, is used to allocate instructions, and is read-
only from the application program’s point of view. The operating system can still write
to that section, and uses this ability when loading an application program into memory.
Another point to note is that in machines that allow self-modifying code, the text section
is read-write.

The data section is generally used to store data items that are required throughout the
activation of the program. Such items include, for instance, statically allocated global data
items and dynamically allocated data items. This section is allowed to grow at run-time as
and when allocating new data items dynamically.

The stack section is generally used to store data items that are required only during
the activation of a subroutine; such items include local variables and parameters to be
passed to other subroutines. The stack section is therefore empty at programming time. At
program run-time, it starts empty, and then grows and shrinks as subroutines are called and
exited. Every time a subroutine is called, the stack section grows by an amount called stack
frame or activation record. The stack frame thus constitutes a private work space for the
subroutine, created at the time the subroutine is called and freed up when the subroutine
returns. Historically, most machines assume the stack frames to grow in the direction of
decreasing memory addresses, although a few machines assume the opposite.

3.1.3 Register Model

Most assembly-level architectures include a few registers to store information that needs
to be accessed frequently. In the lower-level hardware implementations, these registers are
implemented in a manner that permits faster access to them compared to accessing the
main memory. This is because of the following reasons, which stem from having only a few
registers:

(cid:15) In a microarchitecture, the decoder or selector used to select a register will be much
smaller than the one used to select a memory location.

(cid:15) In a microarchitecture, the registers are typically implemented inside the processor
chip, and so no o(cid:11)-chip access is required to access a register.

(cid:15) In a device-level architecture, the technology used to implement registers ((cid:13)ip-(cid:13)ops
or SRAMs) is faster than the one used to implement memories (DRAMs), which are
typically designed for achieving high density.

Apart from these access time advantages, there is yet another advantage at the lower level:
a register can be speci(cid:12)ed with just a few bits in a machine language. This is much less than
the 32 or 64 bits required to specify a memory location. For example, in an instruction
set architecture that speci(cid:12)es 32 registers, only log 2 32 = 5 bits are needed to specify a
register. Notice that all of these advantages of registers would be lost, if too many registers
are speci(cid:12)ed.

86

Chapter 3. Assembly-Level Architecture | User Mode

Registers are used for a variety of applications, and generally have names that denote
their function. Below, we give the names of commonly used registers and a brief description
of their function. Notice that not all machines may have every one of these registers.

(cid:15) Program Counter (PC): This register is speci(cid:12)ed in virtually every machine, and is
used to store the address of the memory location that contains the next instruction
to be executed.

(cid:15) Accumulator (ACC): Many of the early machines speci(cid:12)ed a special register called the
accumulator to store the result of all arithmetic and logical operations.

(cid:15) Stack Pointer (SP): This register is used to store the address of the topmost location
of the stack section of the memory.

(cid:15) Link Register: This register is used to store the return address when calling a sub-
routine.

(cid:15) General-Purpose Registers (GPRs): Modern architectures invariably specify a number
of GPRs to store key local variables and the intermediate results of computation.
Examples are the AX, BX, CX, and DX registers in the IA-32 architecture, and registers $0
through $31 in the MIPS-I architecture. Most architectures specify separate registers
for holding integer numbers and (cid:13)oating-point numbers. On some architectures, the
GPRs are completely symmetric and interchangeable. That is, to hold a temporary
result, the compiler can equally use any of the GPRs; the choice of register does not
matter. On other architectures some of the GPRs may have some special functions
too. For example, in the IA-32 architecture, there is a register called EDX, which can
be used as a GPR, but which also receives half the product in a multiplication and
half the dividend in a division. Similarly, in the MIPS-I architecture, register $31
is a GPR, but is used to store the return address when executing a subroutine call
instruction.

(cid:15) Flags Register: If speci(cid:12)ed, this register stores various miscellaneous bits of informa-
tion (called (cid:13)ags or condition codes), which re(cid:13)ect di(cid:11)erent properties of the result
of the most recent arithmetic or logical operation, and are likely to be needed by
subsequent instructions. Typical condition code bits include:
N | set if the previous result was negative
Z | set if the previous result was zero
V | set if the previous result caused an over(cid:13)ow 2

2Over(cid:13)ow occurs in a computer because of using a (cid:12)xed number of bits to represent numbers. When
the result of an arithmetic operation cannot be represented by the (cid:12)xed number of bits allotted for the
result, then an over(cid:13)ow occurs. The over(cid:13)ow event can be handled in 3 ways: (i) the semantics of the
instruction that generated the over(cid:13)ow may include speci(cid:12)cations on how the over(cid:13)ow is treated. (ii) the
over(cid:13)ow triggers an exception event, transferring control to the operating system, which then handles the
exception event. (iii) the V (cid:13)ag is set to 1 so as to permit subsequent instructions of the application program
to monitor the V (cid:13)ag and take appropriate action.

3.1. Overview of User Mode Assembly-Level Architecture

87

C | set if the previous result caused a carry out of the most signi(cid:12)cant bit (MSB)
A | set if the previous result caused a carry out of bit 3 (auxiliary carry)
P | set when the previous result had even parity.
Flags are set implicitly by certain arithmetic and logical instructions. For example,
after a compare instruction is executed, the Z (cid:13)ag is used to indicate if the two num-
bers are equal, and the N (cid:13)ag is used to indicate if the second number is bigger than
the (cid:12)rst number. A subsequent instruction can test the value of these (cid:13)ags, and take
appropriate action. Similarly, the C (cid:13)ag is useful in performing multiple-precision
arithmetic. The required multiple-precision addition is done in several steps, with
each step doing a single-precision addition. The C (cid:13)ag generated in one step serves
as a carry input for the next step. The A (cid:13)ag is useful for performing arithmetic
operations on packed decimal numbers.

3.1.4 Data Types

In Chapter 2 we saw that declaring and manipulating variables were key concepts in high-
level languages; each variable has a type associated with it. By contrast, the assembly-
level architecture does not have a notion of variables!
Instead, the assembly language
programmer considers the contents of a register, a memory location, or a contiguous set of
memory locations as a data item, and manipulates the contents of these storage locations
using instructions.

When a compiler translates an HLL program into an assembly language program, it
maps HLL variables to memory locations in the assembly-level architecture. The number of
memory locations allocated depends on the variable’s type. The memory section or region
in which locations are allocated depends on the variable’s storage class.

Assembly-level architectures support a variety of data types, such as characters, signed
integers, unsigned integers, and (cid:13)oating-point numbers. Support for a particular data type
comes primarily in the form of instruction opcodes that interpret a bit pattern as per
the de(cid:12)nitions of that data type. For example, an assembly language may provide two
di(cid:11)erent ADD instructions|add and fadd|one that interprets bit patterns as integers, and
one that interprets them as (cid:13)oating-point numbers. Remember that in an assembly-level
architecture, the data in a particular storage location is not self-identifying. That is, the
bits at that storage location do not specify a data type, and therefore have no inherent
meaning. The meaning is determined by how an instruction uses them.
It is up to the
assembly language programmer (or compiler) to use the appropriate opcodes to interpret
the bit patterns correctly. Thus, it is the job of the assembly language programmer (or
the compiler) to ensure that bit patterns representing integer variables are added together
using an integer ADD instruction.

To illustrate this further, we shall use an example. Figure 3.2 shows how two di(cid:11)erent
ADD instructions can produce two di(cid:11)erent results when adding the same two bit patterns
01010110 and 00010100. In the (cid:12)rst case, an integer add instruction treats the two patterns

88

Chapter 3. Assembly-Level Architecture | User Mode

as (binary encoded) integers 86 and 20, and obtains the result pattern 01101010, which has
a decimal value 106 when interpreted as an integer. In the second case, a BCD 3 add instruc-
tion treats the two patterns as (BCD encoded) numbers 56 and 14, and obtains a di(cid:11)erent
result pattern of 01110000, which represents the decimal number 70 when interpreted as a
BCD number.

Binary Number System
(Unsigned Integers)

Binary Coded Decimal System
(BCD Integers)

01010110
00010100
01101010

(86)
(20)
(106)

1

01010110
00010100
01110000

(56)
(14)
(70)

Decimal

Decimal

Figure 3.2: An Example Illustrating how the same two Bit Patterns can be added di(cid:11)erently
to yield Di(cid:11)erent Results

Below, we highlight the data types that are typically supported in assembly-level archi-
tectures.

(cid:15) Unsigned Integer and Signed Integer: Integers are among the most basic data
types, and all machines support them. Some machines provide support for unsigned
integers as well as signed integers. This support comes primarily in the way of in-
structions that have a di(cid:11)erent semantic for recognizing arithmetic over(cid:13)ows.

(cid:15) Floating-Point Number: Most machines support (cid:13)oating-point data type by in-
cluding speci(cid:12)c instructions for performing (cid:13)oating-point arithmetic. Many machines
also have separate registers for holding integer values and (cid:13)oating-point values. As
mentioned in Chapter 2, an FP number is written on paper as follows:
(S ign)S ignif icand (cid:2) B aseE xponent
The base in the above equation is the radix of the system, which is a constant for a
particular assembly-level architecture, and is usually chosen as 2. The signi(cid:12)cand is
used to identify the signi(cid:12)cant digits of the FP number; the number of bits allotted
for the signi(cid:12)cand determines the precision.

(cid:15) Decimal Number: Some HLLs, notably COBOL, allow decimal numbers as a data
type. Assembly-level architectures that were designed to be COBOL-friendly often
directly support decimal numbers, typically by encoding a decimal digit in 4 bits
and then packing two decimal digits per byte (BCD format). However, instead of

3A BCD number is a binary coded decimal number. The BCD format is explained later in this section.

3.1. Overview of User Mode Assembly-Level Architecture

89

providing arithmetic opcodes that work correctly on these packed decimal numbers,
they typically provide special decimal-arithmetic-correction opcodes that can be used
after an integer addition to obtain the correct BCD answer! These opcodes use the
carry out of bit 3, which is available in the A (auxiliary carry) (cid:13)ag.

(cid:15) Character: Most assembly-level architectures support non-numeric data types such
as characters. It is not uncommon for an assembly-level architecture to have special
opcodes that are intended for handling character strings, that is, consecutive runs
of characters. These opcodes can perform copy, search, edit, and other functions on
strings.

If an architecture does not support a particular data type, and an arithmetic operation
needs to be performed for that data type, the assembly language programmer (or compiler)
may have to synthesize that operation using the available instructions. For example, if an
architecture does not support the (cid:13)oating-point data type, and there is a need to add two
bit patterns as if they are stored in the (cid:13)oating-point format, then the programmer needs
to write a routine that separates the exponents and signi(cid:12)cands, equalizes the exponents by
modifying the signi(cid:12)cands, and then performs the addition and re-normalization operations.
Similarly, if the architecture does not support the BCD (binary coded decimal) data type,
and if BCD arithmetic needs to be performed on a bit pattern, then the assembly language
programmer (or compiler) needs to synthesize that arithmetic operation using sequences of
existing instructions.

3.1.5 Assembler Directives

An assembly language program instructs the assembly-level machine two things: (i) how to
initialize its storage locations (registers and memory), and (ii) how to manipulate the values
in its storage locations. The (cid:12)rst part is conveyed by means of statements called assembler
directives, and the second part is conveyed by means of assembly-level instructions. (Apart
from directives that are used to initialize storage locations, there are some other directives
that are executed by the assembler and are not assembled. There are also some directives
that serve as programming tools, to simplify the process of writing the program.) We shall
discuss directives in this section. Instructions will be discussed in the next section.

Without initialization, the values in registers and memory locations would be unde(cid:12)ned.
The bulk of the initialization done by the directives involves the memory space and those
registers that point to di(cid:11)erent sections in the memory space, such as the program counter,
the global pointer, and the stack pointer. Speci(cid:12)cally, they indicate which static sections
the program will use (.text, .rdata, .data, etc 4 ), how big these sections are, where their
boundaries are, and what goes into the di(cid:11)erent data sections.

4Dynamic sections such as heap and stack are created during execution, and are therefore not speci(cid:12)ed
by directives.

90

Chapter 3. Assembly-Level Architecture | User Mode

For example, to place the subsequent statements of the program in the .data section of
memory, we can use a directive such as:

.data
And, to initialize the next 4 bytes to 58, we can use a directive such as:
.word 58

When an assembly language program is translated to machine language, the directives
are translated to form the machine language program’s header, section headers, and various
data sections. A directive therefore does not translate to machine language instructions. At
the execution time of a machine language program, the initialization task is performed by
the loader part of the OS, which reads the program’s header, section headers, and sections
to do so.

3.1.6

Instruction Types and Instruction Set

Apart from assembler directives, an assembly language program includes assembly-level
instructions also. In fact, the instructions form the crux of the program; a program without
instructions would only be initializing the registers and memory! As mentioned before,
instructions manipulate the values present in registers and memory locations. For example,
to copy the contents of memory location A to register $t0, we can use an instruction such
as:

lw
$t0, A
Here, the mnemonic lw is an abbreviation for load word. Similarly, to add the contents of
registers $t1 and $t2 and to place the result in register $t3, we can use an instruction such
as:

add

$t3, $t1, $t2

A typical program involves performing a number of functionally di(cid:11)erent steps, such
as adding two numbers, testing for a particular condition, reading a character from the
keyboard, or sending a character to be displayed on a video screen. Each assembly-level
architecture has its own set of instructions, called its instruction set. In practice, many
of the instruction sets are quite similar. The instructions in an instruction set can be
functionally classi(cid:12)ed into four categories:

(cid:15) Data transfer

(cid:15) Data manipulation

(cid:15) Program sequencing and control

(cid:15) Trap or syscall

Data Transfer: Data transfer instructions copy data from one storage location to an-
other, without changing the data stored in the source location. Typical transfers of data

3.1. Overview of User Mode Assembly-Level Architecture

91

are between registers and main memory locations; between registers and stack locations; or
or between registers themselves. Variables declared in a high-level language are generally
allocated locations in main memory (some are allocated in general-purpose registers), and
so most of the data reside initially in main memory. From the main memory, data is often
copied to general-purpose registers or the stack, prior to operating on them. Data transfer
instructions are quite useful in this endeavor. Some of the commonly used data transfer
instructions in di(cid:11)erent assembly languages and their semantics are given in Table 3.1.

Mnemonic

MOV
LOAD
STORE
PUSH
POP
XCH

Semantics
Copy data from one register/memory location to another
Copy data from a memory location to a register
Copy data from a register to a memory location
Copy data from a register/memory location to top of stack
Copy data from top of stack to a register/memory location
Exchange the contents of two register/memory locations

Table 3.1: Common Data Transfer Instructions in Di(cid:11)erent Assembly Languages

Data Manipulation: Data manipulation instructions perform a variety of operations
on data and modify them. These are instrumental for implementing the operators and
assignments of HLL programs. There are three types of data manipulation instructions:
arithmetic, logical, and shift. Some of the commonly used data manipulation instructions
in di(cid:11)erent assembly languages and their semantics are given in Table 3.2. The input
operands for these instructions are speci(cid:12)ed as part of the instruction, or are available in
storage locations such as memory locations, registers, or stack. The result of the instruction
is also stored in a storage location. Recently, many assembly languages have included
instructions that are geared for speeding up multimedia applications.

Program Sequencing and Control: Normally, the instructions of a program are exe-
cuted one after the other, in a straightline manner. Control-changing instructions are used
to deviate from this straightline sequencing. A conditional branch instruction is an instruc-
tion that causes a control (cid:13)ow deviation, if and only if a speci(cid:12)c condition is satis(cid:12)ed. If
the condition is not satis(cid:12)ed, instruction sequencing proceeds in the normal way, and the
next instruction in sequential address order is fetched and executed. The condition can
be the value stored in a condition code (cid:13)ag, or the result of a comparison. Conditional
branch instructions are useful for implementing if statements and loops. Besides condi-
tional branches, instruction sets also generally provide unconditional branch instructions.
When an unconditional branch is encountered, the sequencing is changed irrespective of any
condition. Finally, assembly languages also include cal l instructions. and return instructions

92

Chapter 3. Assembly-Level Architecture | User Mode

Mnemonic
ADD
SUB
MULT
DIV

INC

DEC

ABS
AND
OR
XOR
LSHIFT
RSHIFT
LROT
RROT

Semantics
Add two operand values and store the result value
Subtract one operand value from another and store the result value
Multiply two operand values and store the result value
Divide an operand value by another and store the quotient
remainder values
Increment an operand value and store the result value
in the same location
Decrement an operand value and store the result value
in the same location
Find the absolute value of the operand value and store the result
And two operand values and store the result value
Or two operand values and store the result value
Exor two operand values and store the result value
Left shift one operand value by another and store the result value
Right shift one operand value by another and store the result value
Left rotate one operand value by another and store the result value
Right rotate one operand value by another and store the result value

Table 3.2: Common Data Manipulation Instructions in Di(cid:11)erent Assembly Languages

to implement subroutine calls and returns. Some of the commonly used program control
and sequencing instructions in di(cid:11)erent assembly languages and their semantics are given
in Table 3.3. The (cid:12)rst group of instructions in the table are conditional branch instructions;
the second group are unconditional branches, and the third group deal with subroutine calls
and returns.

Trap or Syscall Instructions: The instructions we saw so far cannot do IO operations
or terminate a program. For performing such operations, an application program needs
to call the services of the operating system (OS). Trap or syscall instructions are used
to transfer control to the OS, in order for the OS to perform some task on behalf of the
application program. That is, this instruction is used to invoke an OS service. Once the OS
completes the requested service, it can return control back to the interrupted application
program by executing an appropriate Kernel mode instruction. If a single trap instruction
is provided for specifying di(cid:11)erent types of services, then the required service is speci(cid:12)ed as
an operand of the instruction.

3.1. Overview of User Mode Assembly-Level Architecture

93

Mnemonic
JZ
JNZ
JC
JNC
BEQ
BNE
BLT
JMP
B
JR
CALL
JAL
RETURN

Semantics
Jump if Z (cid:13)ag is set
Jump if Z (cid:13)ag is not set
Jump if C (cid:13)ag is set
Jump if C (cid:13)ag is not set
Branch if both operand values are equal
Branch if both operand values are not equal
Branch if one operand value is less than the other
Jump unconditionally
Branch unconditionally
Jump to address in speci(cid:12)ed register
Call speci(cid:12)ed subroutine
Jump and link to speci(cid:12)ed subroutine
Return from subroutine

Table 3.3: Some of the Common Program Sequencing and Control Instructions in Di(cid:11)erent
Assembly Languages

3.1.7 Program Execution

Consider a simple assembly language program to add the contents of memory locations 1000
and 1004, and store the result in memory location 1008.

.text
start: lw
lw
add
sw
sys halt

$t0, 1000
$t1, 1004
$t1, $t1, $t0
$t1, 1008

The programmer’s view is that when this program is being executed, it is present in the
computer’s main memory. The (cid:12)rst line in the program, namely .text, is a directive
indicating that the program be placed in the .text section of main memory. This line
is called an assembler directive, and is not part of the executed program. The next line,
which contains the (cid:12)rst instruction, begins with the label
start. This label indicates the
memory address of the (cid:12)rst instruction. Figure 3.3 shows how this small program might be
loaded in the .text section of the memory space, when it is about to be executed. The (cid:12)ve
instructions of the program have been placed in successive memory locations, in the same
start. Notice that the label
order as that in the program, starting at location
start
does not explicitly appear in the program stored in memory, nor does the directive .text
in the (cid:12)rst line. The comment portions of the statements also do not appear in memory.

94

Chapter 3. Assembly-Level Architecture | User Mode

Address
0

__start

__start

Address

Main Memory

lw  $t0, 1000
lw  $t1, 1004
add $t1, $t1, $t0
sw  $t1, 1008
sys_halt

.text
section

.data
section

1000
1004
1008

M  −1

Data

pc

$t0
$t1
$t2
$t3
$t4
$t5

Figure 3.3: A Possible Placement of the Sample Program in Memory

Let us consider how this program is executed. To begin executing the program, the
address of its (cid:12)rst instruction (P ) is placed into pc. This instruction is executed and the
contents of pc are advanced to point to the next instruction. Then the second instruction
is executed, and the process is continued until the computer encounters and executes the
sys halt instruction. The last instruction transfers control to the OS, and tells it to
terminate this program. As you can see, instructions are executed in the order of increasing
addresses in memory. This type of sequencing is called straight-line sequencing.

3.1.8 Challenges of Assembly Language Programming

Because an assembly language is less abstract than a high-level language, programming in
assembly is considerably more di(cid:14)cult than programming in a high-level language. The
lack of abstraction manifests itself in di(cid:11)erent ways:

(cid:15) First, the storage resources are more concrete at the assembly level, which has the
notion of registers and memory locations, as opposed to variables. The programmer
must manage the registers and memory locations at every step of the way. In machines
with condition codes, the programmer must keep track of the status of condition codes
and know what instructions a(cid:11)ect them before executing any conditional branches.
This sounds tedious, if not di(cid:14)cult.

3.1. Overview of User Mode Assembly-Level Architecture

95

(cid:15) Another important di(cid:11)erence is that the data items in an assembly language program
are not typed, meaning they are not inherently speci(cid:12)ed as belonging to a particular
type such as integer or (cid:13)oating-point number. Assembly languages provide di(cid:11)erent
instructions for manipulating a data item as an integer or another data type. Thus,
it is the responsibility of the programmer to use the appropriate instructions when
manipulating data items. Even then, the number of data types supported in an
assembly-level architecture is fewer than that in the high-level architecture; only a
few simple data types such as integers, (cid:13)oating-point numbers, and characters are
supported. Thus, all of the complex data types and structures supported at the
HLL level must be implemented by the assembly language programmer using simple
primitives.

(cid:15) In assembly language programs, most of the control (cid:13)ow changes must be imple-
mented with branch instructions whose semantics are similar to those of the \go to"
statements used in HLLs.

(cid:15) The amount of work speci(cid:12)ed by an assembly language instruction is generally smaller
than that speci(cid:12)ed by an HLL statement5 . This means that several assembly language
instructions are usually needed to implement the equivalent of a typical HLL state-
ment.

(cid:15) Assembly language programs take much longer to debug, and are much harder to
maintain.

(cid:15) Finally, it is di(cid:14)cult for assembly language novices, particularly those with high-level
language experience, to think at a low enough level.

All of these factors make it di(cid:14)cult to program in assembly language. Apart from these
di(cid:14)culties, there is a practical consideration: an assembly language program is inherently
tied to a speci(cid:12)c instruction set, and must be completely rewritten to run on a machine
having a di(cid:11)erent instruction set. Because of these reasons, most of the programming done
today is in a high-level language.

3.1.9 The Rationale for Assembly Language Programming

Under the above circumstances, why would anyone want to program in assembly language
today? There are at least two reasons:

1. Speed and code size

2. Access to the hardware

5Exceptions to this general case are the vector instructions and the MMXT M instructions.

96

Chapter 3. Assembly-Level Architecture | User Mode

First of all, an expert assembly language programmer can often produce code that is much
smaller and much faster than the code obtained by compiling an equivalent HLL program.
For some applications, speed and code size are critical. Many embedded applications, such
as the code in a smart card, the code in a cellular telephone, the code in an anti-lock brake
control, device drivers, and the inner loops of performance-critical applications fall in this
category. Second, some functions need complete access to the hardware features, something
usually impossible to specify in high-level languages. Some hardware features have no ana-
logues in high-level languages. The low-level interrupt and trap handlers in an operating
system, and the device drivers in many embedded real-time systems fall into this category.
Similarly, an assembly language programmer may be able to make better use of special in-
structions, such as string copy instructions, pattern-matching instructions, and multimedia
instructions such as the MMXT M instructions. Many of these special instructions do not
have a direct equivalent in high-level languages, thereby forcing the HLL programmer to
use loops. Compilers, in most cases, cannot determine that such a loop can be replaced by
a single instruction, whereas the assembly language programmer can easily determine this.

3.2 Assembly-Level Interfaces

The assembly-level architecture attributes that we saw so far pertain to the assembly lan-
guage speci(cid:12)cation part of the architecture.
In many contexts, by assembly-level archi-
tecture, we just mean this assembly language speci(cid:12)cation. For serious programming in
assembly, we need to enhance the architecture by the following two additional parts:

(cid:15) assembly-level interface provided by libraries

(cid:15) assembly-level interface provided by the OS

Assembly Language
Programmer

Assembly Language
Specification
(User Mode)

Assembly−Level
Interface
(Library)

Assembly−Level
Interface
(OS)

Figure 3.4: Three Di(cid:11)erent Parts of Assembly-Level Architecture

3.3. Example Assembly-Level Architecture: MIPS-I

97

3.2.1 Assembly-Level Interface Provided by Library

3.2.2 Assembly-Level Interface Provided by OS

3.3 Example Assembly-Level Architecture: MIPS-I

We shall next take a more detailed look at assembly-level architectures. It is easier to do
so using an example architecture. The example architecture that we use is the well-known
MIPS-I assembly-level architecture [ref ]. We use the MIPS-I architecture because it is one
of the simplest architectures that has had good commercial success, both in the general-
purpose computing world and in the embedded systems world. The MIPS instruction set
architecture had its beginnings in 1984, and was (cid:12)rst implemented in 1985. By the late
1980s, this architecture had been adopted by several workstation and server companies,
including Digital Equipment Corporation and Silicon Graphics. Today MIPS processors are
widely used in Sony and Nintendo game machines, palmtops, laser printers, Cisco routers,
and SGI high-performance graphics engines. Although it is not popular anymore in the
desktop computing world, the availability of sophisticated MIPS-I simulators such as SPIM
makes it possible for us to develop MIPS-I assembly language programs and simulate their
execution. All of these features makes MIPS-I an excellent architecture for use in Computer
Architecture courses. Below, we look at some of the important aspects of the MIPS-I
architecture; interested readers may refer to [refs] for a more detailed treatment.

3.3.1 Assembly Language Alphabet and Syntax

The MIPS-I assembly language format is line oriented; the end of a line delimits an in-
struction. Each line can consist of up to four (cid:12)elds, as shown in Figure 3.5: a label (cid:12)eld,
an opcode (cid:12)eld, an operand (cid:12)eld, and a comment (cid:12)eld. The language is free format in the
sense that any (cid:12)eld can begin in any column, but the relative left-to-right ordering of the
(cid:12)elds must be maintained. For the sake of clarity, we will align the (cid:12)elds in each of our
code snippets.

Label

Opcode

Destination
Operand

Source
Operands

Comment

label1:

add

$t3, $t1, $t2

# add contents of $t1 and $t2, and put result in $t3

label2:

.word 20

# call next loc as label2, initialize 4−byte word to 20

Assembler Directive

Figure 3.5: Format of a MIPS-I Assembly Language Statement

98

Chapter 3. Assembly-Level Architecture | User Mode

A label is a symbolic name used to identify a memory location that is explicitly referred
to in the assembly language program. A label consists of any sequence of alphanumerical
characters, underscores ( ), dollar signs ($), or periods (.), as long as the (cid:12)rst character is
not a digit. A label must be followed by a colon. Labels are particularly useful for speci-
fying the target of a control-changing instruction and for specifying the memory location
corresponding to a variable.

After the optional label (cid:12)eld, the next (cid:12)eld speci(cid:12)es an opcode or an assembler directive.
An opcode speci(cid:12)es the operation to be done by the instruction.

The operand (cid:12)eld in an assembly language statement speci(cid:12)es the destination operand
and source operand(s) of the instruction. Operands are separated by commas, and the
destination operand (if present) appears in the leftmost position in the operand (cid:12)eld, except
for store instructions. For instance, in the assembly language instruction \add $t3, $t1,
$t2," the source operands are registers $t1 and $t2, and the destination operand is register
$t3. Numbers are interpreted as decimal, unless preceded by 0x or succeeded by H, either
of which denotes a hexadecimal number. Multiple instructions can be written in a single
line, separated by semicolons.

The comment (cid:12)eld, which comes last, begins with a sharp sign (#), and terminates at
the end of the line. Thus, all text from a # to the end of the line is a comment 6 . Just
as in high-level languages, the comments are only intended for human comprehension, and
are ignored by the assembler. A good comment helps explain a non-intuitive aspect of one
or more instructions. By providing additional insight, such a comment provides important
information to a future programmer who wants to modify the program.

3.3.2 Register Model

The MIPS-I assembly-level architecture models 32 general-purpose 32-bit integer registers
named $0 - $31, 32 general-purpose 32-bit (cid:13)oating-point registers named f0 - f31, and
three special integer registers named pc, hi, and lo. Two of the general-purpose integer
registers ($0 and $31) are also somewhat special. Register $0 always contains the value
0, and can never be changed. If an instruction speci(cid:12)es $0 as the destination register, the
register contents will not change when the instruction is executed. Register $31 can be used
as a general-purpose register, but it has an additional use as a link register for storing a
subroutine return address, as we will see in Section 3.4.6.

The (cid:13)oating-point arithmetic instructions use the FP registers as the source as well
as the destination. However, they can specify only the 16 even-numbered FP registers,
$f0, $f2, $f4, ...., $f30. When specifying an even-numbered FP register, if the operand
is a double-precision FP number, the remaining 32 bits of the number are present in the
subsequent odd-numbered FP register. For instance, when $f0 is speci(cid:12)ed for indicating a

6Although a line can contain nothing other than a comment, starting a comment from the very (cid:12)rst
column of a line is not a good idea. This is because most assemblers invoke the C preprocessor cpp, which
treat them as preprocessor commands.

3.3. Example Assembly-Level Architecture: MIPS-I

99

31

PC

0

zero
at
v0
v1
a0
a1
a2
a3
t0
t1
t2
t3
t4
t5
t6
t7
s0
s1
s2
s3
s4
s5
s6
s7
t8
t9
k0
k1
gp
sp
s8/fp
ra

lo

hi

$0
$1
$2
$3
$4
$5
$6
$7
$8
$9
$10
$11
$12
$13
$14
$15
$16
$17
$18
$19
$20
$21
$22
$23
$24
$25
$26
$27
$28
$29
$30
$31

fv0

fv1

ft0

ft1

ft2

ft3

fa0

fa1

ft4

ft5

fs0

fs1

fs2

fs3

fs4

fs5

FpCond

$f0
$f1
$f2
$f3
$f4
$f5
$f6
$f7
$f8
$f9
$f10
$f11
$f12
$f13
$f14
$f15
$f16
$f17
$f18
$f19
$f20
$f21
$f22
$f23
$f24
$f25
$f26
$f27
$f28
$f29
$f30
$f31

Figure 3.6: MIPS-I User Mode Register Name Space

double-precision operand, the 64-bit number is present in the register pair f$f0, $f1g.

Explain HI and LO registers

MIPS-I Assembly Language Conventions for Registers

The MIPS-I assembly language follows some conventions regarding the usage of registers.
These conventions are not part of the assembly-level architecture speci(cid:12)cations. This means
that if you write stand-alone assembly language programs that do not adhere to these
conventions, they are still guaranteed to be assembled and executed correctly. However,
if you do not adhere to these conventions, you cannot use the standard libraries and the
standard operating system, because they have been already compiled with the MIPS-I
compiler, which follows these conventions. Some of the important conventions are given
below:

100

Chapter 3. Assembly-Level Architecture | User Mode

(cid:15) Register $at ($1) is reserved for use by the assembler for computing certain memory
addresses.

(cid:15) Register $sp ($29) is reserved for use as the stack pointer.

(cid:15) The (cid:12)rst four integer parameters of a subroutine are passed through registers $a0-$a3
($4-$7). Thus, the subroutines in the standard libraries and operating system have
been developed with the assumption that their (cid:12)rst 4 parameters will be present in
these registers. Similarly, the (cid:12)rst two (cid:13)oating-point parameters are passed through
FP registers $fa0 and $fa1 ($f12 and $f14). The remaining parameters are passed
through the stack frame.

(cid:15) The integer return values of a subroutine are passed through registers $v0 and $v1
($2 and $3).

(cid:15) Register $v0 ($2) is used to specify the exact action required from the operating
system (OS) when executing a syscall instruction.

(cid:15) Registers $k0 and $k1 ($26 and $27) are reserved for use by the operating system.

Table 3.4 gives the names by which the MIPS registers are usually known. These names
are based on the conventional uses for the di(cid:11)erent registers, as explained above.

Register
Number

$0
$1
$2-$3
$4-$7
$8-$15
$16-$23
$24-$25
$26-$27
$28
$29
$30
$31
$f0, $f2
$f4, $f6, $f8, $f10
$f12, $f14
$f16, $f18
$f20, $f22, $f24, $f26, $f28, $f30

Register
Name
$zero
$at
$v0-$v1
$a0-$a3
$t0-$t7
$s0-$s7
$t8-$t9
$k0-$k1
$gp
$sp
$s8/$fp
$ra
fv0-fv1
ft0-ft3
fa0-fa1
ft4-ft5
fs0-fs5

Typical Use

Zero constant, destination of nop instruction
Assembler temporary; reserved for assembler
Values returned by subroutines
Arguments to be passed to subroutines
Temporaries used by subroutines without saving
Saved by subroutines prior to use
Temporaries used by subroutines without saving
Kernel uses for interrupt/trap handler
Global pointer
Stack pointer
Saved by subroutines, frame pointer
Return address for subroutines
Values returned by subroutines
Temporaries used by subroutines without saving
Arguments to be passed to subroutines
Temporaries used by subroutines without saving
Saved by subroutines prior to use

Table 3.4: Conventional Names and Uses of MIPS-I User Mode Registers

3.3. Example Assembly-Level Architecture: MIPS-I

101

3.3.3 Memory Model

The MIPS-I assembly-level architecture models a linear memory address space (i.e., a (cid:13)at
address space) of 231 memory locations that are accessible to user programs. These locations
have addresses ranging from 0 to 0x7fffffff, as indicated in Figure 3.7. Each address refers
to a byte, and so a total of 2 Gbytes of memory can be addressed. Although each address
refers to a single byte, MIPS-I provides instructions that simultaneously access one, two,
three, or four contiguous bytes in memory. The most common access size is 4 bytes, which
is equal to the width of the registers. Although a location can be speci(cid:12)ed by its address,
the common form of speci(cid:12)cation in assembly language is by a label or by an index register
along with an o(cid:11)set. In the latter case, the memory address is given by the sum of the
register contents and the o(cid:11)set.

Memory Address Space

Addresses
0x00000000

0x00400000

0x10000000

Reserved

.text

.data

Heap

Direction of growth
at run−time

Stack

0x7fffffff

Figure 3.7: Organization of MIPS-I User Memory Address Space

MIPS-I Assembly Language Conventions for Memory

Like the case with registers, there are MIPS-I assembly language conventions for the memory
address space also. Again, these conventions are not part of the assembly-level architecture
speci(cid:12)cations. Figure 3.7 indicates the conventional organization of the MIPS-I memory
address space, in terms of where the di(cid:11)erent sections of the memory start. The conventional
starting points for the .text and .data sections are at addresses 0x400000 and 0x10000000,

102

Chapter 3. Assembly-Level Architecture | User Mode

respectively. The user run-time stack starts at address 0x7fffffff, and grows towards the
lower memory addresses. General-purpose register $sp is generally used as the stack pointer;
stack operands are accessed by specifying an o(cid:11)set value that is to be added to the stack
pointer. Thus, operands that are buried within the stack can also be accessed. Local
variables of a subroutine are allocated on the stack. If a subroutine needs the stack to grow,
for allocating local variables and temporaries, it decrements $sp by the appropriate amount
at the beginning, and increments $sp by the same amount at the end.

3.3.4 Assembler Directives

The MIPS-I assembly-level architecture supports several assembler directives, all of which
are written with a dot as their (cid:12)rst character. Below, we describe some of the commonly
used directives.

(cid:15) .rdata: indicates that the subsequent items are to be stored in the read-only data
section.

(cid:15) .data: indicates that the subsequent items are to be stored in the .data section.

(cid:15) .text: indicates that the subsequent items are to be stored in the .text section.

(cid:15) .comm, .lcomm: are used to declare uninitialized, global data items. These direc-
tives are commonly used when the initial value of a variable is not known at program-
ming time. An item declared with the .comm directive can be accessed by all modules
that declare it. (The linker allocates memory locations for such an item in the .bss
or .sbss section.) An item declared with the .lcomm directive is a global variable
that is accessible within a single module. (The assembler allocates memory locations
for such an item in the .bss section or in the .sbss section.)

(cid:15) .byte, .half, .word: These directives are used to set up data items that are 1, 2,
and 4 bytes, respectively. In contrast to the .comm directive, these directives provide
the ability to initialize the data items. Example declarations using these directives
are given below. The last two declarations correspond to arrays, and are extensions
of the one used for specifying a single item.

b:

h:

w:

.byte

.half

.word

5

5

5

ba:

.byte

0:5

wa:

.word

1:2, 4

# Allocate an 1-byte item with initial value 5
# at next memory location, and name it b
# Label next memory location as h;
# allocate next 2 bytes for a 2-byte item with initial value 5
# Label next memory location as w;
# allocate next 4 bytes for a 4-byte item with initial value 5
# Label next memory location as ba;
# allocate next 5 bytes to 5 1-byte items with initial value 0
# Label next memory location as wa;
# allocate next 12 bytes to 3 4-byte items with initial values 1, 1, 4

3.3. Example Assembly-Level Architecture: MIPS-I

103

In these directives, if no integer value is speci(cid:12)ed after .byte, .half, .word, then the
item is initialized to zero.

(cid:15) .(cid:13)oat:
is used to specify a 4-byte data item that is initialized to a single-precision
(cid:13)oating-point number.

(cid:15) .ascii and .asciiz: are used to declare ASCII strings, without and with a terminating
null character, respectively. In the following example, both directives de(cid:12)ne the same
string.

a:
z:

.ascii "goodn0"
.asciiz "good"

# Place string \good" in memory at location a
# Place string \good" in memory at location z

(cid:15) .space: is used to increment the current section’s location counter by a stipulated number of
bytes. This directive is useful to set aside a speci(cid:12)ed number of bytes in the current section.

s:

.space 40

# Label current section’s location counter as s
# and increment it by 40 bytes

(cid:15) .globl:
indicates that the subsequent variable name or label is globally accessible,
and can be referenced in other (cid:12)les. For example,

.data
.globl g
.word
0

# Subsequent items are stored in the data section
# Label g (in this case, a variable) is global
# Declare a 4-byte item named g with initial value 0

.text
.globl f
subu
$sp, 24

# Subsequent items are stored in the text section
# Label f (in this case, the beginning of a function) is global
# Decrement stack pointer register by 24

g:

f:

(cid:15) .align: is used to specify an alignment. The alignment is speci(cid:12)ed as a power of 2.

3.3.5 Assembly-Level Instructions

We have already seen some of the instructions and addressing modes of the MIPS-I assembly-
level architecture. The MIPS-I architecture supports the following addressing modes: reg-
ister direct, memory direct, register-relative, immediate, and implicit. Let us now examine
the MIPS-I instruction set in a more comprehensive manner 7 .
The MIPS-I architecture is a load-store architecture, which means that only load
instructions and store instructions can access main memory. The commonly used lw (load
word) instruction fetches a 32-bit word from memory. The MIPS-I architecture also provides

7The assembly language examples in this book use only a subset of the MIPS-I assembly-level instruction
set. A complete description of the MIPS-I assembly-level instruction set is available in Appendix *.

104

Chapter 3. Assembly-Level Architecture | User Mode

separate load instructions for loading a single-byte (lb opcode) and a 2-byte half-word (lh
opcode).

As we saw earlier, conventional MIPS assembly language designates a portion of the
memory address space as a stack section. Special stack support such as push and pop
instructions are not provided, however. Locations within the stack section of the memory
space are accessed just like the remaining parts of memory.

Arithmetic and Logical Instructions: MIPS-I provides many instructions for per-
forming arithmetic and logical operations on 32-bit operands. All arithmetic instructions
and logical instructions operate on register values or immediate values. While the register
operands are always 32 bits wide, the immediate operands can be shorter. These instruc-
tions explicitly specify a destination register, and two source operands, of which one is a
register and the other is a register or immediate value.

Control-changing Instructions: The MIPS-I architecture includes several conditional
as well as unconditional branch instructions. The conditional branch instructions base their
decisions on the contents of one or two registers.

System Call Instructions: The MIPS-I architecture provides a syscall instruction for
user programs to request a variety of services from the operating system (OS). The speci(cid:12)c
service requested is indicated by a code value stored in register $v0. The list of services
supported and the speci(cid:12)c code for each service varies from one OS to another 8 .

SPIM’s ABI | System Calls Supported by SPIM: The SPIM simulation tool
includes a very simple operating system that supports a small set of system calls. These
system calls are listed in Table 3.5. If you are familiar with the standard calls supported
by di(cid:11)erent UNIX-style and Windows-style operating systems, you will notice that these
system calls are somewhat di(cid:11)erent. Apart from the much smaller set of calls supported,
the functionality provided by the SPIM calls is not very primitive and is more at the level
of library routines.

3.3.6 An Example MIPS-I AL Program

We are now familiar with the syntax of the MIPS-I assembly language. Next, we shall put
together some of the concepts we learned by writing a simple MIPS-I assembly language

8The di(cid:11)erences in the range of services and system call codes between di(cid:11)erent OSes imply that if a user
program uses syscall instructions to directly request the OS for services|instead of going through library
functions|then the program may not be portable across di(cid:11)erent OSes. Thus, a syscall-laden assembly
language program targeted speci(cid:12)cally for the SPIM OS will most likely not run correctly on an ULTRIX
OS host machine.

3.3. Example Assembly-Level Architecture: MIPS-I

105

OS Service Code in $v0
1
print int
2
print float
3
print double
4
print string
5
read int
6
read float
7
read double

read string

sbrk
exit

8

9
10

Arguments
$a0: integer to be printed
$fa0: (cid:13)oat to be printed
$fa0: double to be printed
$a0: address of string to be printed

Return Value

$v0: integer read
$fv0: (cid:13)oat read
$fv0: double read

$a0: address for placing read string;
$a1: number of bytes
$a0: amount

$v0: address

Table 3.5: System Calls Supported by SPIM

program. Let us write an assembly language program that prints the familiar \hello,
worldnn" string, whose C code and Java code are given below.

Program 4 The Hello World! program in Java.

main() {
// Display the string
printf("hello, world!");

}

class helloworld {
public static void main(String[] args) {
// Display the string
System.out.println("hello, world!");

}

}

For writing the MIPS AL program, we use the ABI (application binary interface) sup-
ported by SPIM. This will help motivated students to ‘execute’ this program using any of
the fspim, xspim, PCSpimg tool set. One point to note when using these tools to ‘execute’
assembly language programs is that prior to execution, the tools assemble the program into
the equivalent machine language program. Thus, the tools directly simulate the execution
of machine language instructions, and not the assembly language instructions. The mem-

106

Chapter 3. Assembly-Level Architecture | User Mode

ory map displayed by these tools therefore indicates machine language instructions, and not
assembly language instructions.

##################################################################################
# data section
##################################################################################
# Store subsequent items in the .data section
.data
# Label next location as string1
string1:
.asciiz "hello, worldnn" # Allocate subsequent bytes and store string "hello, worldnn"

##################################################################################
# text section
##################################################################################
# Store subsequent items in the .text section
.text
#
.globl
# Program execution begins here
start:
# Place the memory address labeled string1 in $a0
la
# Place code for print string system call in $v0
li
# Call OS to print the string
syscall

$a0, string1
$v0, 4

start

li
syscall

$v0, 10

# Place code for exit system call in $v0
# Call OS to terminate the program

This MIPS-I AL program contains only 2 sections: .data and .text. The .data section
has a single declaration|that of the \hello, worldnn" string, which is declared using the
.asciiz directive, and given the label string1. The string therefore starts at label string1.

The .text directive tells the assembler to place the subsequent items in the .text
section. The
start label indicates that execution of the program should begin at that
point. Although we have placed the
start label just before the very (cid:12)rst instruction in
this program, this is not a rule; the
start label can be placed anywhere within the .text
section. In order to print the string, we have to use the services of the operating system
via a syscall instruction.
In this example code, we have used the syscall convention
followed by SPIM, a simulator for the MIPS-I architecture. The address of the string (i.e.,
label string1) is placed in register $a0, and the code for the print string system call in
SPIM (which is 4) is placed in register $v0, prior to the syscall instruction. Notice that
this program will not run on a standard MIPS host, because standard OSes use a di(cid:11)erent
ABI. The standard ABI does not support the print string system call, and instead uses
the code 4 for the write system call.

Finally, after printing \hello, worldnn", the program should terminate. It does so, via
another syscall instruction, after placing in register $v0 the value 10, which is the code
for the exit system call in SPIM. (The standard ABI de(cid:12)nes a code value of 1 for the exit
system call, in contrast to the value of 10 used by SPIM’s ABI.)

3.4. Translating HLL Programs to AL Programs

107

3.3.7 SPIM: A Simulator for the MIPS-I Architecture

Let us elaborate on the SPIM simulator, as it is a very useful tool for learning MIPS-I
assembly language programming. This simulator was developed by Prof. James Larus at
University of Wisconsin-Madison, and is available to the public from an ftp site at that
University. The simulator comes in 3 di(cid:11)erent (cid:13)avors: spim, xspim, and PCSpim. Among
these, spim is the most basic one, providing a terminal-style interface on Unix/Linux hosts,
and a DOS interface or console interface on Windows hosts. The xspim and PCSpim tools
are fancier and provide graphical user interfaces (GUI). xspim runs on Unix/Linux hosts
and provides an X window interface, whereas PCSpim runs on Windows hosts and provides
a Windows interface.

To execute the above program with xspim in a Unix/Linux host, type the command:
xspim -notrap &. The -notrap option tells SPIM not to add its own start-up code, and
to begin execution at the start label. An xspim window pops up, as shown in Figure 3.8.
You can use the load button in this window to read and assemble your MIPS-I assembly
language program. If any errors are generated during assembly, they are indicated in the
bottom portion of the xspim window. After (cid:12)xing the bugs in the AL program (cid:12)le, you can
reload the program (cid:12)le into xspim by (cid:12)rst clearing the memory and register contents|using
the memory & registers option in the clear button|and then using the load button to
reload the program.

If SPIM has successfully assembled your program, then you can use the run or step
buttons in the xspim window to execute your program.
In this case, a console window
will be automatically opened to display hello world. xspim provides many other useful
features such as breakpoints and a debugger, which make it even more attractive than a
real MIPS host for developing MIPS-I assembly language programs. You are encouraged to
consult the SPIM manual for learning and using these features.

3.4 Translating HLL Programs to AL Programs

We just saw the rudiments of writing a complete program in MIPS-I assembly language, and
‘executing’ it on the SPIM simulator. We shall next take a look at how a high-level language
program is translated to assembly language. After all, most of today’s programming is done
in a high-level language. We shall revisit direct programming in assembly language when
we discuss device drivers and exception handlers in the next chapter.

The translation of a program from a high-level language to the assembly language is
typically done by a program called a compiler. A detailed treatment of the algorithms
used by a compiler is beyond the scope of this book; therefore we restrict our discussion
to a sketch of the important ideas. For illustrating the translation process, we again use
the MIPS-I assembly language. We illustrate the utility of various instruction types with
practical examples of translation from C language to MIPS-I assembly language.

108

Chapter 3. Assembly-Level Architecture | User Mode

Figure 3.8: A Sample xspim Window

3.4.1 Translating Constant Declarations

Constants are ob jects whose values do not change during program execution. Translating a
constant declaration typically involves assigning memory location(s) for the constant. The
number of locations required for a constant depends on its size. The memory location(s)
will hold the constant’s value (a pattern of 0s and 1s) during the execution of the program.
Notice that in machines that support only aligned words, integers and (cid:13)oating-point con-

3.4. Translating HLL Programs to AL Programs

109

stants need to start on a word boundary. The standard practice is to allocate them in the
.rdata section of memory.

#define const 32000

Sample Constant Declaration

One way of translating this HLL constant declaration to assembly language is given
below. In this translation, we have used a label to specify a memory address that will store
the value of the constant. For clarity, we have named this label with the same name as
the HLL constant name. The .rdata directive tells that the subsequent items need to be
placed in the .rdata (read-only data) section 9 . The HLL constant const is allocated space
in memory with the .word directive, which allocates 4 bytes. Subsequent instructions that
use this constant value will read it from memory.

# Assign memory locations for the constants and initialize them
# Subsequent items are stored in the .rdata section
.rdata
# Label next memory location as const;
const:.word
# allocate next 4 bytes for constant const

32000

It is important to note here that a compiler may not always translate constants in this
manner. Small constants are often not often allocated to memory locations or registers.
Instead, the compiler explicitly speci(cid:12)es the value of the constant in instructions that use
that constant. Consider the following code snippet.

#define small 2
main()
f

var i, j;
i = small;
j = small * small;

g

Declaration and Uses of a Small Constant

This code can be re-written as follows:

main()
f

g

var i, j;
i = 2;
j = 2 * 2;

9Certain run-time tables are also allocated in the same section of memory as the constants. Such an
example is available in page 122, where we discuss jump tables for translating switch statements.

110

Chapter 3. Assembly-Level Architecture | User Mode

Substituting Occurrences of a Small Constant by its Value

References to small constants can often be directly speci(cid:12)ed in an instruction itself as
immediate operands. This is discussed in Section 3.7.1.

||||||||||||||||||||||||{

Floating-Point Constants: Floating-point constants take up many bits even if the value
they represent is small. In modern architectures, the minimum number of bits required to
represent a (cid:13)oating-point number is 32 bits. Therefore, (cid:13)oating-point constants are seldom
explicitly speci(cid:12)ed within instructions as immediate values. Instead, they are allocated to
memory locations just like what is done for large integer constants. Consider, for example,
the following C program:

#include <stdlib.h>
#include <stdio.h >
int main( int argc, char **argv, char **envp )
{

static int j;
static double i = 1.0;
static double a[8] = {0,1,2,3,4,5,6,7};

j = 0;
a[j] = i + 1.0;

}

|||||||||||||||||||||||||{

3.4.2 Translating Variable Declarations

High-level language programs typically have a number of variable declarations, both global
and local. Global variables are visible throughout the program, whereas local variables are
visible only when the block in which they are declared are active. Translation of a vari-
able declaration typically involves assigning memory location(s) for the variable; optimizing
compilers may allocate some variables to registers in order to speed up the program’s exe-
cution. The number of locations required for a variable depends on its type. The memory
location(s) or register(s) will hold the variable’s value (a pattern of 0s and 1s) during the
execution of the program. Notice that in machines that support only aligned words, integers
and (cid:13)oating-point numbers need to start on a word boundary.

3.4. Translating HLL Programs to AL Programs

111

Variables declared in a high-level language are generally allocated locations in one of
three sections in memory: the .data section, the run-time stack section, and the heap
section. Variables that persist across function invocations, such as global variables and
static variables, are allocated memory locations in the .data section. Variables that do not
persist across function invocations, such as local variables, are generally allocated locations
in the stack section. Dynamic variables created at execution time and accessed through
pointers are allocated memory locations in the heap section.

3.4.2.1 Global Variables

Consider the following global variable declarations in C.

int i = 0;
int a = 12;
struct f
char name[6];
int length;
g record;
float f = 0.5;
char *cptr;

Sample Global Variable Declarations

One way of translating these HLL variable declarations to assembly language is given
below. Again, we use labels to specify memory addresses that correspond to variables. For
clarity, in our examples, the label assigned to the memory location corresponding to an
HLL variable generally has the same name as the HLL variable name. The .data directive
tells that the subsequent items need to be placed in the .data section. We have allocated a
contiguous space in memory for HLL variables i, a, record, f, and cptr. Variables i and
a require four bytes each, and are allocated memory using the .word directive. The struct
variable record has 2 (cid:12)elds: name and length. The (cid:12)eld name, an array of six characters,
requires one byte per character. The (cid:12)eld record.length is an integer, and therefore starts
at the next word boundary. Thus, a total of 12 bytes are allocated for the HLL variable
record. The HLL variable f of type float is allocated with the .float directive, and
occupies 4 bytes in memory. Finally, the HLL pointer variable cptr is allocated space
in memory with the .word directive. Notice that in an assembly language program, the
memory location assigned to an HLL variable does not identify the data type. Here, the
same directive is used, for instance, for allocating integers as well as pointers.

# Assign memory locations for the global variables and initialize them
# Subsequent items are stored in the .data section
.data
# Label next memory location as i;
.word
# allocate next 4 bytes for int variable i

0

i:

112

Chapter 3. Assembly-Level Architecture | User Mode

a:

.word

12

record:
.byte
.word

0:6
0

f:

.float 0.5

# Label next memory location as a;
# allocate next 4 bytes for int variable a

# Label next memory location as record
# Allocate next 6 bytes for record.name
# Allocate next 4 bytes for record.length

# Label next memory location as f;
# allocate next 4 bytes for (cid:13)oat variable f

cptr: .word

NULL

# Label next memory location as cptr;
# allocate next 4 bytes for pointer variable cptr

For the HLL variables, we have allocated memory locations in sequential order in the
.data section, but the exact addresses are not speci(cid:12)ed. When this program is translated
to machine language by the assembler, speci(cid:12)c addresses will be assigned. You can verify
this by loading the above code in SPIM, and looking at the DATA display of the xspim
window. Figure 3.9 gives one such memory allocation assuming these data items to start at
location 0x10010000. The assembler has allocated a contiguous space in memory for all of
the items declared in the .data portion of the assembly code given above. Thus, variables
i and a, which require four bytes each, are mapped to locations 0x10010000-0x10010003
and 0x10010004-0x10010007, respectively. The locations corresponding to struct variable
record begin at address 0x10010008, with the (cid:12)rst 6 locations corresponding to the 6
characters in record.name. Because of the automatic alignment of the .word directive on
a word boundary, the memory locations corresponding to record.length start at the next
word boundary, 0x10010010. Locations 0x1001000e and 0x1001000f are therefore unused.
Had we placed the .align 0 directive after the .data directive in the above code, then
record.length would have been mapped to locations 0x1001000e-0x10010011. However,
all accesses to record.length then become complicated, as we will see later.

In the above assignment in a machine language, all of the initialized variables (i, a, and
f) and the uninitialized variables (record and cptr) were allocated together in the same
section of memory. When a machine language program is shipped, the .text section as well
as the initialized .data section need to be shipped. The assembly language programmer
can help to reduce the size of the shipped program by allocating uninitialized data items
using the .comm or .lcomm directives instead of the .word directive. The assembler and
linker would then allocate memory locations for such data items in a separate data section,
called .bss. This uninitialized data section need not be included in the shipped program.
Notice that the .comm directive is not supported by SPIM; so, you cannot try this with
SPIM.

3.4. Translating HLL Programs to AL Programs

113

AL Sections

AL Labels

Memory Address Space HLL Variables

.data

i:
a:
record:

f:
cptr:

0
0

0
12

0
0

0
0

0
0.5
NULL

0
0

i
a

name

length
f
cptr

record

Figure 3.9: A Memory Map View of the Assembly Language Program Snippet Implementing
Global Variables

3.4.2.2 Local Variables

Next let us turn our attention to local variables (or automatic variables), which are de(cid:12)ned
inside a subroutine, and are active only when the subroutine is active. Two tpes of allocation
are possible for local variables: static allocation and global allocation.

Static Allocation:
In this type of allocation, the local variables are assigned memory
locations in the .data section, just like the globals. This avoids the overhead of creation
and destruction of a work space for every subroutine instance. However, this is possible
only if the subroutine is non-recursive and non-reentrant, such as in Fortran 77 (an earlier
version of Fortran). In these languages only one instance of a given subroutine can be active
at a time.

Dynamic Allocation: Although we can assign them memory locations along with the
globals, such an approach has some drawbacks:

(cid:15) The local variables of all subroutines will be occupying memory space throughout the
entire execution of the program, irrespective of whether they are active or not.

(cid:15) More importantly, if a subroutine is recursive, then multiple instances of a local vari-
able de(cid:12)ned in that subroutine will map to the same memory location. The newer

114

Chapter 3. Assembly-Level Architecture | User Mode

instances of the subroutine may therefore overwrite the values stored by the earlier,
but still active, instances of the subroutine.

In the ensuing discussion, we only consider dynamic allocation. Conceptually, the local
variables of a subroutine instance should be \created" only when the subroutine instance
comes into existence, and should be \destroyed" when the instance (cid:12)nishes. To do this, the
allocation of these variables should happen at run-time as opposed to during translation.
Thus, when a subroutine calls another, a new set of local variables are created for the callee,
although the caller’s local variables are still active. The (cid:12)rst subroutine to complete will
be the one that is called last, and the local variables to be destroyed (cid:12)rst are the ones that
were created last in the nested call sequence. That is, the local variables are created and
destroyed in a last-in (cid:12)rst-out (LIFO) order. This suggests that the local variables could
be allocated at run-time in a stack-like structure; the LIFO nature of stack-like structures
(cid:12)ts naturally with the LIFO nature of subroutine calls and returns.

A natural place to allocate the local variables, then, is in the stack section discussed
in Section 3.1.2. A convenient way of doing this allocation is to designate for each active
subroutine a contiguous set of locations in the stack section called a stack frame or
activation record. The stack frame constitutes a private work space for the subroutine,
created when entering the subroutine and freed up when returning from the subroutine. If
the subroutine requires more space for its local variables, it can obtain the space it needs
by raising the top of stack.

We shall illustrate the use of stack frames using an example code. Consider the following
C code. We have included only the main() function for simplicity. This function declares
two local variables, x and y.

int i = 0;
int a = 12;

main()
f

int x = 5;
int y;

...

g

Example Global Variable and Local Variable Declarations

One way of translating these variable declarations to assembly language is given below. A
stack frame of 12 bytes is created for the main subroutine by the subu $sp, 12 instruction,
which forms the subroutine prologue code. The local variables x and y are allocated 4 bytes

3.4. Translating HLL Programs to AL Programs

115

each in the stack section. Unlike the global variables, the exact addresses assigned to
these local variables are not determined when generating the equivalent machine language
program. These addresses will be determined only at run-time, based on where the stack
frame for main() gets allocated. Figure 3.10 illustrates this memory allocation. Part
(a) of the (cid:12)gure shows the memory map prior to entering main(), and part (b) shows
the same after entering main() and executing the subu $sp, 12 instruction. The newly
created stack frame for main() is deleted prior to leaving main() by executing the addu
$sp, 12 instruction, which forms the subroutine epilogue code. You can load this assembly
language program in xspim and ‘execute’ it to see the allocation of local variables. When
running programs having a main label, it is better to run xspim without the -notrap option,
allowing SPIM to append a start-up code at the beginning of the program. The start-up
code performs some initialization, and then calls the main subroutine.

# Assign memory locations for the global variables
# Initialize memory locations if necessary
# Store subsequent items in the data section
.data
# Allocate a 4-byte item with initial value 0
.word
# at next memory location, and label it i
# Allocate a 4-byte item
# at next memory location, and label it a

.word

0

12

i:

a:

.text
.align 2
.globl main
.ent
main

main:

# Entry point of subroutine main (optional)

# Assign memory locations for the local variables of main()
# Decrement $sp to allocate a 12-byte stack frame
subu
$sp, 12
.frame $sp, 12, $ra # Stack frame is accessed with $sp; frame size is 12 bytes;
# return address is in $ra

li
sw
...
...
addu
jr
.end

$t1, 5
$t1, 8($sp) # Initialize local variable x to 5

$sp, 12
$ra
main

# Increment $sp to delete the current stack frame
# Return from subroutine main
# End point of subroutine main (optional)

3.4.2.3 Dynamic Variables

A global variable is assigned a (cid:12)xed memory location (or set of neighboring locations) at
compile time, and remains (cid:12)xed throughout the execution of the program. Such variables
are assigned memory locations within the .data section. A local variable, on the other
hand, begins to exist from the time the corresponding subroutine is called. It is disposed

116

Chapter 3. Assembly-Level Architecture | User Mode

AL Sections

AL Labels

Memory Address Space

HLL Variables

AL Sections

AL Labels

Memory Address Space

HLL Variables

.data

i:
a:

0
12

i
a

.data

i:
a:

0
12

i
a

global variables

stack

Stack frame
main()
for

stack

y
x

local variables
of
main()

Register

sp

Register

sp

(a) Memory Map Before Entering main()

(b) Memory Map After Entering main()

Figure 3.10: A Memory Map View of the Assembly Language Program Snippet Implement-
ing Local Variables

of when control is passed back to the calling routine. A convenient place to allocate such a
variable is in the stack frame that will be created when the subroutine is called.

Finally, a dynamic variable (pointed to by a pointer) is created during program execution
(by calling a library function such as malloc() or calloc() in C, which returns the memory
address assigned to the variable).
Such a variable continues to exist until the allotted
memory locations are explicitly freed (by calling a library routine such as free()). Like
local variables, dynamic variables are also not assigned memory locations at compile time,
and are instead assigned memory locations at run-time as and when they are created. Unlike
local variables, however, the run-time allocation is done not in the stack section, but in the
heap section. This is because these data items need to be active even after the subroutines
in which they were created (cid:12)nish execution.

We shall illustrate the allocation of dynamic variables using an example code. Consider
the following C code. We have included only the main() function for simplicity. This
function declares two local variables, x and y.

int i = 0;

3.4. Translating HLL Programs to AL Programs

117

main()
f

g

int x, *y;

y = (int *)malloc(8);
...

Example Global Variable, Local Variable, and Dynamic Variable Declarations

# Assign memory locations for the global variables
# Initialize memory locations if necessary
# Store subsequent items in the data section
.data
# Allocate a 4-byte item with initial value 0
.word
# at next memory location, and label it i
# Allocate a 4-byte item
# at next memory location, and label it a

.word

0

12

i:

a:

.text
.align 2
.globl main
main
.ent

main:

# Entry point of subroutine main

# Assign memory locations for the local variables of main()
# Decrement $sp to allocate a 12-byte stack frame
subu
$sp, 12
# for storing $ra and for variables x and y
.frame $sp, 12, $ra # Stack frame is accessed with $sp; frame size is 12 bytes
# return address is in $ra
$ra, 0($sp) # Save register $ra’s contents on stack
$a0, 8
# Call subroutine malloc to allocate a 4-byte item
malloc
$v0, 4($sp) # Update variable y with the address of dynamic variable

$ra, 0($sp) # Load register $rawith previously saved value
# Increment $sp to delete the current stack frame
$sp, 12
$ra
main

# End point of subroutine main

sw
li
jal
sw
...
...
lw
addu
jr
.end

3.4.2.4 Symbol Table

Keeping track of the mapping between the HLL variables and memory locations can be quite
tedious for an assembly language programmer, but not for the compiler. Most assembly

118

Chapter 3. Assembly-Level Architecture | User Mode

AL Sections

AL Labels

Memory Address Space

HLL Variables

AL Sections

AL Labels

Memory Address Space

HLL Variables

.data

i:
a:

0
12

Stack frame
main()
for

stack

i
a

y
x

i:
a:

.data

heap

0
12

i
a

global variables

dynamic variable

Stack frame
main()
for

stack

y
x

local variables
of
main()

Register

sp

Register

sp

(a) Memory Map After Entering main()

(b) Memory Map After Executing malloc()

Figure 3.11: A Memory Map View of the Assembly Language Program Snippet Implement-
ing a Dynamic Variable

languages provide the ability to symbolically specify memory locations by labels. Each
symbolic name in the assembly language program is eventually replaced by the appropriate
memory address during the assembly process. The compiler typically keeps track of variable
allocations through a data structure called symbol table. Each entry of the symbol table
corresponds to one variable, and has enough information for the compiler to remember
the memory locations or registers allocated for the variable. As and when the compiler
encounters a variable declaration, it creates a new entry for the variable in the symbol
table. The symbol table information is useful for displaying the addresses of symbols during
debugging or analyzing a program.

3.4.3 Translating Variable References

The compiler (or assembly language programmer) needs to generate the proper sequence of
data movement operations whenever an HLL variable is referenced. For doing this, it has
to remember where in memory (or register) the variable has been allocated. For variables
encompassed in complex data structures such as array elements and structure elements,
accessing the variable also involves calculating the correct address of the variable. Consider

3.4. Translating HLL Programs to AL Programs

119

the following assignment statements involving the previously declared global variables.

record.length = i;
cptr = &(record.name[3]);
record.name[5] = *cptr;

Some machines provide instructions to perform memory-to-memory arithmetic. In such
machines we may be able to translate arithmetic expressions without copying the variable
values to registers. However, in load-store machines such as the MIPS-I, we must (cid:12)rst
load the variable values into registers from their corresponding memory locations. Let us
translate the above assignment statements to MIPS-I assembly code. Notice that these
statements cannot be executed in SPIM without including the appropriate .data declara-
tions.

.text
lw
la
sw
addu
sw
lw
lw
sw

# Store subsequent items in the text section
# Copy the value in memory location named i into $t1
$t1, i
$t2, record # Copy the memory address named record into $t2
$t1, 8($t2) # Copy $t1 into memory location allocated to record.length
$t3, $t2, 3 # Calculate address of record.name[3]
# Store the address into memory location named cptr
$t3, cptr
# Copy the value in memory location cptr to $t3
$t3, cptr
$t4, 0($t3) # Copy the value in mem location pointed to by cptr to $t4
$t4, 5($t2) # Store $t4 into memory location allocated to record.name[5]

3.4.4 Translating Conditional Statements

All high-level languages support if statements, which cause some statements to be executed
only if a condition is satis(cid:12)ed. Consider the following C code:

if (i < a)
cptr = record.name;

else

cptr = &(record.name[1]);

To translate this if statement to assembly language, it is easier to (cid:12)rst rewrite this code
using goto statements, because assembly languages usually do not provide if-else con-
structs. The above code can be rewritten as follows:

if (i >= a)
goto else1;
cptr = record.name;
goto done;
else1: cptr = &(record.name[1]);
done:
...

120

Chapter 3. Assembly-Level Architecture | User Mode

We can implement this modi(cid:12)ed C code in assembly language with the use of condi-
tional branch (or conditional jump) instructions, which permit the skipping of one or more
instructions. The exact manner in which a conditional branch checks conditions depends
on the machine. Some machines provide condition codes that are set by arithmetic instruc-
tions and can be tested by conditional branch instructions. Some others like MIPS-I do not
provide condition codes, and instead let conditional branches check the value in a register.

A translation for the above C code to MIPS-I assembly code is given below. In this code,
the if condition evaluation is done using a bge instruction. Notice that in the high-level
language, when an if condition is satis(cid:12)ed, the statement(s) following the if statement is
(are) executed. In the assembly language, by contrast, when a branch condition is satis(cid:12)ed,
the instruction(s) following the branch instruction is (are) skipped. Therefore, we need to
use the complement of the if condition as the branch condition. In this example, the C code
checks for the condition if (i < a); the assembly code checks for the branch condition
bge (branch if greater or equal). Also notice that in the high-level language, once execution
goes to the then portion, the else portion is automatically skipped. However, the assembly
language does not provide such a support, and so an unconditional branch instruction is
used just before the else portion to skip the else portion whenever control goes to the then
portion.

.text
lw
lw
bge
la
sw
b
else: la
addu
sw
done: ...

# Copy the value in memory location i into $t1
$t1, i
# Copy the value in memory location a into $t2
$t2, a
$t1, $t2, else # Branch to label else if $t1 (i) (cid:21) $t2 (a)
# Copy the memory address named record into $t1
$t1, record
# Copy the value in $t1 into memory location named cptr
$t1, cptr
# Branch to label done (to skip the else portion)
done
# Copy the memory address named record into $t1
$t1, record
# Increment $t1 so as to obtain address of record.name[1]
$t1, 1
# Copy the value in $t1 into memory location named cptr
$t1, cptr

A series of if-else statements based on a single variable can be expressed as a multi-
way branching statement using the C switch statement. Consider the following C code. It
contains a switch statement, with 5 di(cid:11)erent cases including the default case.

switch (record.length)
f
case 0:
case 1:

*record.name = ’L’;
break;

case 2:
case 3:

3.4. Translating HLL Programs to AL Programs

121

*record.name = ’M’;
break;
default:
*record.name = ’H’;

g

A trivial way of translating this switch statement is to (cid:12)rst express it as a series of if-
else statements, and then translate them as we just did. However, such an approach may
be very ine(cid:14)cient, especially if there are many cases to consider. A more e(cid:14)cient translation
can be performed by incorporating a software structure called jump table. A jump table
is an array that stores the starting addresses of the code segments corresponding to the
di(cid:11)erent cases of a switch statement. It is indexed by the value of the switch statement’s
control variable.

In the next page we show an assembly language translation of the switch statement
given above. This code begins with the jump table declaration, which is stored in the read-
only data section. The jump table starts at label JT, and contains 4 entries, corresponding
to values 0-3 for record.length. These 4 entries are initialized to 4 labels that are declared
in the .text section.

The .text portion (cid:12)rst reads the value of record.length from memory. Then the
default case is handled by checking if the value of this variable is greater than or equal
to 4.
If the
If this condition is satis(cid:12)ed, then control is transferred to label default.
condition is not satis(cid:12)ed, then we need to index into the jump table. The jump table index
is calculated by scaling the value of record.length by 4, as each table entry occupies 4
bytes. For instance, if record.length has a value of 3, then the mul instruction scales it by
4 to obtain an index of 12. The subsequent lw instruction adds this o(cid:11)set to the jump table
starting address JT, and loads the target address into $t6. The next instruction performs
an unconditional jump to the target address stored in $t6.

# Store subsequent items in the read only data section
.rdata
#####################################################################
# Begin jump table here, and initialize it with target addresses
JT:

case0
.word
case1
.word
case2
.word
.word
case3
#####################################################################

# Store subsequent items in the text section
.text
# Align next item on a 22 byte (32-bit word) boundary
.align 2
# Load starting address of variable record in $t1
la
$t1, record
# Copy contents of memory location record.length to $t6
$t6, 8($t1)
lw
$t6, 4, default # Branch to label default if record.length (in $t6) (cid:21) 4
bge

122

Chapter 3. Assembly-Level Architecture | User Mode

mul
lw
j

# Scale by 4 to obtain the jump table index
$t6, $t6, 4
$t6, JT($t6) # Copy jump target address from jump table to $t6
$t6

# case 0
# case 1

$t15, ’L’
$t15, 0($t1) # *record.name = ’L’
# break
done

# case 2
# case 3

$t15, ’M’
$t15, 0($t1) # *record.name = ’M’
# break
done

# default

$t15, ’H’
$t15, 0($t1) # *record.name = ’H’

case0:
case1:
li
sb
b

case2:
case3:
li
sb
b

default:
li
sb

done:

3.4.5 Translating Loops

Loops form a ma jor portion of most programs. Therefore, it is important to translate
them in an e(cid:14)cient manner. Consider the following C code that adds the elements of
record.name, and stores the result in the memory location pointed by variable cptr. This
code uses the for loop construct.

*cptr = 0;
for (i = 0; i < record.length; i++)
*cptr += record.name[i];

Most assembly languages do not provide a single instruction that can directly implement
complex HLL constructs such as for loops. We can, however, rewrite the above loop in
terms of an if statement with an embedded goto, as follows:

loop:

*cptr = 0;
i = 0;
if (i < record.length)
f

*cptr += record.name[i];
i++;

3.4. Translating HLL Programs to AL Programs

123

goto loop;

g

One possible translation for this rewritten loop is given below. In this assembly language
program, register $t2 is used to store the current value of memory word pointed by cptr,
and register $t3 is used to store the latest value of i.

# Store subsequent items in the text section
.text
# Align next item on a 22 byte (32-bit word) boundary
.align 2
$t1, record # Load starting address of variable record in $t1
la
# Initialize copy of *cptr in $t2
li
$t2, 0
# Initialize copy of i in $t3
$t3, 0
li
$t4, 8($t1) # Copy contents of memory location record.length to $t4
lw
$t3, $t4, done# Branch to label done if i (in $t3) (cid:21) copy of record.length
loop: bge
$t5, 0($t1) # Copy record.name[i] (pointed by $t1) from memory to $t5
lw
$t2, $t2, $t5 # Add copy of record.name[i] in $t5 to running total in $t2
add
# Increment $t1 to point to next element of record.name[]
$t1, 4
addu
# i++; do this on copy of i in $t3
$t3, 1
add
# Branch to label loop
loop
b
# Store copy of i in $t3 to memory location named i
done: sw
$t3, i
# Copy pointer value stored in mem addr cptr to $t4
$t4, cptr
lw
$t2, 0($t4) # Store calculated sum ($t2) to memory location pointed by cptr
sw

This loop causes a straight-line sequence of instructions to be executed repeatedly, as
many times as needed (record.length in this case). The loop starts at location loop and
ends at the instruction b loop. During each pass through this loop, record.name[i]’s
address is (cid:12)rst determined by adding 4 to $t1, and then record.name[i] is fetched and
added to the running total present in $t2. Thus, conditional branch instructions are very
useful in implementing the if statements and loops present in high-level programs.

In the above loop, a form of indirect addressing (indexed addressing, to be precise) is
used in the loop to access record.name[i]. Indirect addressing permits a di(cid:11)erent memory
address to be speci(cid:12)ed during each iteration of the loop, by varying the contents of the index
register ($t1 in this case). If an assembly language does not support any form of indirect
addressing, then the only way to change the memory address during each iteration is to use
self-modifying code, i.e., the program considers portions of itself as data, and modi(cid:12)es itself
during execution.

3.4.6 Translating Subroutine Calls and Returns

We just saw how control (cid:13)ow changes introduced by if statements and loops can be im-
plemented in assembly language. Next we discuss a more complicated type of control (cid:13)ow
change, the one involving subroutine calls and returns. To implement the special type of
branching required to implement subroutines, two basic instructions are used: a cal l instruc-
tion that transfers control from the calling program to the subroutine’s starting location,

124

Chapter 3. Assembly-Level Architecture | User Mode

and a return instruction that returns control from the subroutine to the place from which
it was called, as illustrated in Figure 3.12. In MIPS-I AL, these two instructions are called
jal and jr $ra, respectively. Implementing subroutines in an assembly language is more
complicated because of the following reasons:

Calling
Program

jal P

jal P

Called
Subroutine

P:

jr  $ra

Figure 3.12: Transfer of Control during Subroutine Linkage

(cid:15) The HLL subroutine may have local variables declared within it, for which storage
space needs to be allocated by the assembly language program. Because of the possi-
bility of recursion, each instance of a subroutine requires a new set of storage locations
for its local variables.

(cid:15) Temporary values created within an assembly language subroutine may need to be
stored in memory locations and registers. We cannot specify a (cid:12)xed set of memory
locations for a subroutine, again because of recursion. Conceptually, each run-time
instance of a subroutine requires a few \fresh" memory locations and registers.

(cid:15) A subroutine may be called from di(cid:11)erent places in a program. When the subroutine
(cid:12)nishes execution, control must return to the instruction that immediately follows the
call instruction that passed control to the subroutine.

(cid:15) Most subroutines need the calling program to pass parameters to them at the time
they are called. Also, often, a subroutine may need to pass a return value to the
calling program.

We shall take a detailed look at each of these issues and the solutions developed to
handle them. One important aspect that guides these solutions is that the development of
an assembly language subroutine|be it by a programmer or a compiler|is often done in
isolation to the development of its calling routine(s). This means that the calling routines as
well as the subroutines must adhere to a set of well-de(cid:12)ned speci(cid:12)cations or conventions. If
the calling routine was developed with a particular convention in mind, and the subroutine
was developed with another, then the program as a whole may not guarantee correct results.

3.4. Translating HLL Programs to AL Programs

125

The crux of the (cid:12)rst two problems mentioned above is that each invocation of a subrou-
tine needs a separate working environment. This environment consists of a set of registers
and memory locations that can be used for allocating local variables, and storing temporary
values. To solve the last two problems, we need to provide a well-de(cid:12)ned communication
mechanism between the working environments of the caller and the callee.

We already saw in Section 3.4.2 how local variables of a HLL program are allocated
storage space in the corresponding assembly language program. The conventional method
is to build a new stack frame every time a subroutine is called, and to allocate speci(cid:12)c
locations within the stack frame for each local variable. The stack frames are created within
the stack section of the memory address space, and are organized as a LIFO structure. The
provision of an independent stack frame for each active subroutine enables each subroutine
instance to have its own set of storage locations for allocating its local variables. In the
following subsections, we will see how the stack frame concept has become the backbone
for solving all of the above mentioned problems associated with implementing subroutines.
Frame layouts vary between assembly languages and compilers. If a frame pointer is used,
then the frame pointer of the previous frame is stored in the current frame, and restored
when returning from a subroutine.

Return Address Storing

Because a subroutine may be called from di(cid:11)erent places in a program, provision must be
made for returning to the appropriate location in the program after the subroutine completes
execution. To do this, the return address must be saved somewhere before transferring
control to the subroutine. The MIPS-I assembly-level architecture has earmarked register
$ra for storing the return address. The semantics of a call instruction (named jal for jump
and link) specify that the return address is automatically stored in general-purpose register
$ra. If a subroutine needs to call another, then the assembly language programmer (or
compiler) writing the caller routine should include instructions to save the contents of $ra
on the stack frame (before performing the call) and to restore the contents of $ra (after
returning from the callee). Typically, these instructions are placed at the beginning and
end, respectively, of the calling subroutine.

Consider the MIPS-I AL program that we saw in page 115. In this program, the routine
main() calls subroutine malloc(). Prior to the jal malloc instruction, the return address
of main() is saved in main()’s stack frame using the instruction sw $ra, 8($sp). When
executing the jal malloc instruction, the contents of $ra will be overwritten with the
return address of malloc(). Therefore, prior to returning from main(), the correct return
address of main() is restored into $ra using the instruction lw $ra, 8($sp).

126

Chapter 3. Assembly-Level Architecture | User Mode

Parameter Passing and Return Value Passing

When calling a subroutine, the calling routine must provide to the callee the parameters,
that is, the operands or their addresses, to be used in the computation. Later, the sub-
routine may return the results of the computation to the calling routine. This exchange of
information between a calling routine and a subroutine is referred to as parameter passing.
The MIPS-I AL convention stipulates the programmer to place the parameters in registers
$a0 through $a3, where they can be accessed by the instructions of the subroutine. Simi-
larly, the programmer should place the return values in registers $v0 and $v1. If more than
4 parameters are present, the rest of the parameters are passed through the stack frame.
Figure 3.18 shows a parameter being passed to subroutine P through register $a0. The
return value is passed by the subroutine back to the calling program through register $v0.

Calling
Routine

li
jal

$a0, 1
P

# place argument in $a0
# call subroutine P
# use return value stored in $v0

Called
Subroutine

P:

# use parameter stored in $a0

# place return value in $v0
$ra # return

jr

Figure 3.13: Passing Parameters and Return Value through Registers

Register Saving and Restoring

As discussed, the development of an assembly language subroutine|whether by a program-
mer or a compiler|is often done in isolation to the development of the calling program.
This means that at the time of subroutine development, it is di(cid:14)cult to identify the registers
that are not used in the calling environment, and are therefore available for its use. If the
subroutine developer blindly uses an arbitrary register for storing a temporary value, there
is a possibility of overwriting useful information belonging to the calling environment.

The approach followed in MIPS-I is to let the programmer temporarily save the values
pertaining to the caller, prior to the callee using them. Once the callee has completed its
usage of a register set, the programmer restores their original values. Again, a convenient
place to temporarily save the register values is the stack frame. Figure 3.17 shows the layout
of a stack frame in which space has been set apart for saving general-purpose registers as
well as (cid:13)oating-point registers. This is the approach followed in the MIPS-I ALA.

Performing the saving (and restoring) of the register values at the proper times is the
responsibility of the assembly language programmer. This saving (and restoring) can be
done by the caller (cal ler save), by the callee (cal lee save), or by a combination of the two.

3.4. Translating HLL Programs to AL Programs

127

It is important to note that it is not necessary to save the entire set of registers in the name
space. The MIPS-I AL earmarks some of the registers for caller save, and some others for
callee save.

Lower Addresses

Current Stack Frame

$sp

Space for storing
callee arguments

Space for storing
FPRs

Space for storing
GPRs

Space for storing
Temporary Values

Space for allocating
Local Variables

Higher Addresses

Figure 3.14: Layout of a Typical Stack Frame

Notice that the need for saving and restoring .... or memory locations, the MIPS-I
compiler avoids this problem by utilizing a separate stack frame for each active subroutine.

3.4.7 Translating System Calls

Consider the C program given in page 53 (and reproduced below), for copying characters
from standard input to standard output. This program directly calls the operating system
services using the read() and write() system calls, instead of going through library rou-
tines. These system calls can be implemented in an assembly language with the use of trap
or syscall instructions. The syscall instruction is quite di(cid:11)erent from a library function call,
although from the assembly language programmer’s point of view there may not be a big
di(cid:11)erence. Consider the following C code which uses the system calls read() and write()
to read a sequence of characters from stdin and write them to stdout.

main()

128

Chapter 3. Assembly-Level Architecture | User Mode

f

g

char c;

while (read(0, &c, 1) > 0) /* read one char from stdin */
write(1, &c, 1);
/* write one char to stdout */

A MIPS-I AL translation of the above C code is given below. This code uses the syscall
instruction to request the OS to perform the read() and write() system calls. Most
commercial operating systems use the same set of values for read code and write code
(3 and 4, respectively). Notice that this AL program cannot be ‘executed’ on a SPIM
simulator, because SPIM does not support these system calls.

.text
.globl main

main: add
li
loop: li
li
syscall

$a1, $sp, 4
$a2, 1
$a0, 0
$v0, 3

# Place the address of c in $a1
# Place the number of bytes to be read (i.e., 1) in $a2
# Place the (cid:12)le descriptor (0) in $a0
# Place the code for read() system call in $v0
# Call OS routine to perform the read

blez
li
li
syscall
b

$v0, done
$a0, 1
$v0, 4

loop

# Break out of while loop if syscall returned zero
# Place the (cid:12)le descriptor (1) in $a0
# Place the code for write() system call in $v0
# Call OS routine to perform the write
# Go back to while loop

done: jr

$ra

# Return from main() function

3.4.8 Overview of a Compiler

[This section needs to be written.]

The compilation process involves a series of phases.

(cid:15) Lexical analyzer (or lexer)

(cid:15) Syntax analyzer (or parser)

(cid:15) Intermediate code generator

(cid:15) Code optimizer: Code optimization is an important phase because for most of the
applications, once a program is developed, the same program is executed thousands
or millions of times.

3.5. Memory Models: Design Choices

129

(cid:15) Code generator

3.4.8.1 Just-In-Time (JIT) Compiler

The oldest implementation of Java is the interpreter. Every Java command is interpreted
to an equivalent sequence of host machine instructions, and is run in the order in which it
arrives. This is a really slow way of doing things.

Then came the JIT (just in time) compiler. Every time the Java execution runtime
environment would run into a new class|classes are functional groups within the Java
program|the JIT compiler would compile it right there. Once something is compiled, it
runs with native commands, and it is fast. Spending a little bit of time up front can save
a lot of execution time later. That did improve matters, but it still did not get the top
performance, because some things that would only run once could take longer to compile
than it would take to run them with the interpreter. This means you could wind up with
a net loss.

With that observation came the dynamic compiler, which compiles only those things
that matter and leaves the rest alone. The dynamic compiler decides whether to compile
each class. It has two weapons in its arsenal: an interpreter and a JIT, and it makes an
intelligent decision on a class-by-class basis whether to use the one weapon or the other.
The dynamic compiler makes that decision by "pro(cid:12)ling," or letting the code run a few
internal loops before deciding whether or not to compile that section of code. The decision
may be wrong, but statistically the dynamic compiler is right much more often than not;
in fact, the longer you run the code, the more likely it is to get it right.

3.5 Memory Models: Design Choices

We have seen the basics of translating high-level language programs to assembly language
programs. Much of this discussion was centered around the MIPS-I ALA, a somewhat simple
architecture. A wide variety of architectures have been in use over the last several decades.
They di(cid:11)er in terms of the register model, the memory model, and/or the instruction set.
In this section, we shall focus on the memory model. In particular, we shall investigate
di(cid:11)erent options for various facets of the memory model, such as the address space, the
endian-ness, and the support for word alignment.

3.5.1 Address Space: Linear vs Segmented

We have already seen that the memory address space is a collection of unique addresses,
each of which corresponds to a location that can store a single word of information. The
address space can be organized in more than one way. The organization we saw so far
is the linear (one-dimensional array) organization, in which consecutive numbers ranging

130

Chapter 3. Assembly-Level Architecture | User Mode

from 0 to M (cid:0) 1 (where M is the number of memory locations) form the addresses of
successive memory locations. The M addresses (0 ! M (cid:0) 1) thus constitute a linear address
space (a.k.a. (cid:13)at address space) for the computer. Thus, the memory model adopted by
conventional assembly language (and machine language) architectures is a linear memory
model. To specify a memory address in this model, log 2M bits are required. For example,
if the address space of an assembly-level architecture includes 2 32 memory locations, then
32 bits are required to specify a memory address in that machine. For this model, we will
use the notation (X) to denote the contents of memory location X.

In some architectures, this memory is organized in a 2-dimensional manner as a collection
of segments, each of which is a linear address space for storing logically related words. The
segments are typically of di(cid:11)erent sizes. One segment may hold the program, another may
hold the data, and so on.
In this memory model, a memory location is referred to by
specifying a segment address and a displacement within the segment.

Expand, saying the adv and disadv of segmented memory model

3.5.2 Word Alignment: Aligned vs Unaligned

Many assembly-level architectures require words to be aligned to their natural boundaries;
that is, an N -bit word may (cid:12)t within a memory location, but not across two locations. A
sketch of the linear memory model, along with an illustration of aligned words and unaligned
words is given in Figure 3.15.

Address

log M
2

Aligned

N −bit word

Unaligned

N −bit word

0
4

M−4

N bits

N

Data

Figure 3.15: Linear Memory Model and Alignment of Words in Memory. Some Assembly-
Level Architectures specify words to be aligned in Memory

3.6. Operand Locations: Design Choices

131

3.5.3 Byte Ordering: Little Endian vs Big Endian

In an assembly-level architecture specifying a byte-addressable memory address space, a
word typically spans multiple (adjacent) locations in memory. Each byte within such a
word will then have a distinct memory address, although the word is usually referenced by
a single address (either the lowest address or the highest address in the range). Two options
are available for placing the bytes within the word in memory. In the (cid:12)rst option, called
little-endian organization, the least signi(cid:12)cant byte of the word is assigned the lowest (byte)
address. In the second option, called big-endian organization, the most signi(cid:12)cant byte is
assigned the lowest address.

3.6 Operand Locations: Design Choices

In this section we will study the di(cid:11)erent options that exist for holding the operands of
an instruction. The MIPS-I architecture limited itself to using the main memory, the
general-purpose registers, a few special registers, and even part of the instruction for holding
the operands; for data manipulation instructions, operand storage was limited to general-
purpose registers. Many other machines specify additional locations for operand storage,
such as accumulator and operand stack. We will see each of these options in detail.

3.6.1

Instruction

Small constants are frequently used as instruction operands. A convenient place for storing
these operands is within the instruction itself. Such operands are called immediate operands
because they are available for use immediately after the instruction has been fetched from
memory. Many small constants such as 0, 1, (cid:0)1, 2, and 4 occur frequently in programs, and
the immediate operand approach o(cid:11)ers a convenient way to store such an operand within
the instruction itself. A MIPS-I instruction using an immediate operand is given below.

li

$t1, 5

# 5 is an immediate operand

3.6.2 Main Memory

Main memory is the default location for maintaining the values of the variables declared in
a program. For example, when a C compiler encounters the variable declaration
int c;
it assigns a memory location for variable c. The values of the variables are initialized/updated
by writing values to the relevant memory locations. Before manipulating a data item, it is
often copied to other storage locations such as registers and operand stack.

132

Chapter 3. Assembly-Level Architecture | User Mode

3.6.3 General-Purpose Registers

Besides memory locations, registers are another common place for storing operands.
In
load-store architectures such as MIPS-I, operands have to be copied to registers before they
can be used in arithmetic and logical operations. Registers are also commonly used for
storing temporary values and frequently used values. The more the number of registers
de(cid:12)ned in the architecture, the easier it is to keep the frequently used values in registers.

Modern computers invariably support a number of general-purpose registers at the as-
sembly language and ISA levels|typically 8 to 64|which can be used to temporarily store
frequently used operands. When operands are present in registers, it is possible to specify
multiple operands explicitly in an instruction, because a register can be speci(cid:12)ed by just a
few bits. Therefore, we can have instructions such as

add
add

# an x86 AL add instruction
BX, CX
$t0, $t1, $t2# a MIPS-I AL add instruction

Because of these multiple operand or address instructions, machines that use general-
purpose registers are often called multiple address machines. Among the multiple ad-
dress machines, some permit data manipulation only on register operands (as well as im-
mediate operands).
In such machines, all memory operands must be copied to registers
using explicit LOAD instructions prior to using them for data manipulation. Similarly, the
results of data manipulation instructions are also stored only in registers, from where they
are copied to memory locations (if needed) using explicit STORE instructions. Machines of
this kind are called load-store architectures (to re(cid:13)ect the fact that only LOAD and STORE
instructions can access memory) or register-register architectures (to re(cid:13)ect the fact
that all data manipulation instructions use only register operands (immediate operands as
well) and store the result in a register). Examples of load-store architectures are MIPS-I
and Alpha.

Machines that permit data manipulation instructions to access register operands as well
as memory operands are called register-memory architectures. An example is the IA-
32 machine. Notice that a register-memory machine can have instructions that use only
register operands.

3.6.4 Accumulator

The early computers made very judicious use of registers, because registers required a non-
trivial amount of hardware. In these machines, one of the registers, called the Accumulator,
was designated as the one that would be utilized in all arithmetic and logic operations.
On machines that operate in this manner, instructions requiring a single operand, such
as COMPLEMENT and INCR, (cid:12)nd the operand in the Accumulator. The result is also writ-
ten to the Accumulator.
Instructions requiring two operands also use the value in the
Accumulator as one of the operands. The other operand is identi(cid:12)ed by a single address in

3.6. Operand Locations: Design Choices

133

the instruction; hence machines that always use the Accumulator are often called single-
address machines. For example, the single-address instruction

add

X;

# Add the contents of mem loc X to ACC

means: \Add the contents of memory location X to the contents of accumulator and place
the sum into accumulator." To move the contents of memory location X into accumulator,
we can specify a single address instruction as follows:

lw

X;

# Copy the contents of mem loc X to ACC

Similarly, to move the contents of Accumulator to memory location Y, we can specify a
single address instruction as follows:

sw

Y;

# Copy the contents of ACC to mem loc Y

Using only single address instructions, we can add the contents of memory locations X
and Y, and place the result in memory location Z by executing the following sequence of
instructions:

lw
add
sw

X
Y
Z

Note that the operand speci(cid:12)ed in the operand (cid:12)eld may be a source or a destination,
depending on the opcode of the instruction. The lw instruction explicitly speci(cid:12)es the source
operand, memory address X, and implicitly speci(cid:12)es the destination, the Accumulator. On
the other hand, the sw instruction explicitly speci(cid:12)es the destination, memory location Z,
and implicitly speci(cid:12)es the source, the Accumulator. A single address machine built in
the mid-1960s that enjoyed wide popularity was the PDP-8, made by Digital Equipment
Corporation.

3.6.5 Operand Stack

The approaches we saw so far use some type of registers|general-purpose registers or an
accumulator|in addition to memory locations to hold instruction operands. A radically
di(cid:11)erent approach is to use an operand stack to hold the operands. An operand stack
is di(cid:11)erent from the stack frames that we saw earlier. It is a storage structure in which
accesses are allowed only to the top location of the stack (sometimes to the topmost two
locations)10 . Only two types of accesses are permitted to the top of the stack: push and

10 In computer science and engineering (cid:12)elds, this type of storage mechanisms are also known by the term
last-in (cid:12)rst-out (LIFO); at any time the (cid:12)rst data item that will be taken out of the stack will be the
last one that was placed in the stack.

134

Chapter 3. Assembly-Level Architecture | User Mode

pop. These operations are analogous, respectively, to the store and load operations de(cid:12)ned
on the memory address space. The push operation places a new data item to the top of
stack, causing the stack to \grow". The pop operation removes the top item from the stack,
causing the stack to \shrink". Thus, the push and pop operations cause a change in the size
of the operand stack structure. This is unlike the memory address space, which permits the
usual load and store operations, which cause no change to the size of the structure. This
peculiar nature of the operand stack can be clari(cid:12)ed with a real-world example. Consider
a pile of trays in a cafeteria: clean trays are added (pushed) to the pile at the top, causing
it to grow; and customers pick up (pop) trays from the top of the pile, causing it to shrink.

Example push and pop instructions are given below:

push
pop

X
X

# Copy contents of mem loc X to top of operand stack
# Copy contents of top of operand stack to mem loc X

Apart from these push and pop operations, a machine that supports an operand stack typi-
cally provides many arithmetic and logical instructions that operate on operands stored in
the operand stack. One such instruction could be
add

which means pop the contents of the two topmost locations of the operand stack, add them

together, and push the result to the new top of the operand stack. This instruction is
interesting in that it does not explicitly specify any operands, but the operands are implied
to be on the top of the operand stack. Because only the topmost item(s) of the operand
stack are accessible, it is important to push data items to the operand stack in the proper
order; otherwise it becomes di(cid:14)cult to use instructions that manipulate data stored in the
operand stack.

Only a few architectures support an operand stack. By contrast, almost all architectures
support stack frames for allocating the local variables of subroutines. Some of the architec-
tures that support an operand stack de(cid:12)ne it as a separate address space (as illustrated in
Figure 3.16(i)), whereas others de(cid:12)ne it as part of the memory address space. In the latter
case, the operand stack of a subroutine is usually implemented on top of the subroutine’s
stack frame, as illustrated in Figure 3.16(ii). The FP register points to the bottom of the
current stack frame, and the SP register points to the top of the operand stack, which is
implemented on top of the current stack frame. The Java Virtual Machine (JVM) de(cid:12)nes
an operand stack in this manner.

A pure stack machine takes the operand stack approach to the extreme. It does not
de(cid:12)ne any general-purpose registers, and performs arithmetic and logical operations only
with operands present in (the top one or two locations of ) the operand stack. It does specify
a memory model, along with push and pop instructions to move data between the operand
stack and memory locations. None of the data manipulation instructions in such a machine
speci(cid:12)es operands in an explicit manner. Because of these \zero address instructions",
machines that perform data manipulation solely on the operand stack are often called zero

3.6. Operand Locations: Design Choices

135

Stack Space

Memory Space

Direction of
stack growth

TOS

BOS

Direction of
stack growth

Operand
Stack

Stack Frame

SP (TOS)

FP

BOS

(i)

(ii)

Figure 3.16: Di(cid:11)erent Ways of Implementing an Operand Stack: (i) Separate Stack Address
Space; (ii) Within Memory Address Space, on Top of Current Stack Frame

address machines.

Example: Write an assembly language program to evaluate the following C language expres-
sion in a zero address machine.
D = (A + B + C ) (cid:2) (A + B )
Assume that variables A, B , C , and D have been declared as int.

A:
B:
C:
D:

.data
.word
.word
.word
.word
.text
push
push
add
push
add
push
push
add
mult
pop

5
10
8
0

A
B

C

A
B

D

# Copy contents of mem location A to TOS
# Copy contents of mem location B to TOS

# Copy contents of mem location C to TOS

# Copy contents of mem location A to TOS
# Copy contents of mem location B to TOS

The pure stack approach does have some drawbacks:

(cid:15) If the operand stack is implemented in a memory structure outside the processor,
all instructions that access the operand stack require o(cid:11)-chip access to fetch/store

136

Chapter 3. Assembly-Level Architecture | User Mode

operands, and this results in poor performance. The solution often adopted in hard-
ware implementations of stack-based instruction sets is to incorporate the top portion
of the operand stack as microarchitectural registers inside the processor; the rest of
the operand stack is incorporated in memory outside the processor chip. The hard-
ware registers that incorporate the top of the stack can be accessed in a single cycle.
As more and more data items are pushed onto the operand stack, these hardware
registers get (cid:12)lled up, necessitating the transfer of its bottom portion to the stack
in memory. The microarchitecture (and not the assembly language programmer) is
responsible for performing this transfer. The assembly language programmer is not
even aware of these hardware registers.

(cid:15) A second drawback of using an operand stack is the inability to reuse temporary val-
ues created during computations. Consider the expression that you evaluated just
now:

F = (A + B + C ) (cid:2) (A + B )
The A + B portion of this expression needs to be computed only once if the result is
stored in a general-purpose register, whereas it needs to be computed twice if operands
are stored only on the operand stack, unless the calculated value of A + B is popped
into a memory location, and pushed back to the top of stack when needed again.

(cid:15) Lastly, when all instructions use a common resource such as the top of the operand
stack,
it becomes di(cid:14)cult to execute multiple instructions in parallel
in a high-
performance processor implementation.

The proponents of pure stack machines counter these arguments. Their main claims are
that the stack machine approach is clean, simple, elegant! These features make it an easy
target for compilers. Thus, in the last decade, Sun Microsystems introduced a stack-based
machine called Java Virtual Machine (JVM), which has become popular in the world wide
web and embedded computing applications.

3.7 Operand Addressing Modes: Design Choices

An assembly language instruction speci(cid:12)es the assembly-level machine to do a speci(cid:12)c op-
eration on one or more operands. The operands can be present in general-purpose registers,
memory, accumulator, stack, or the instruction itself. The exact location of the operands
depends on the addressing modes speci(cid:12)ed in the instruction. An addressing mode speci(cid:12)es
a rule for interpreting or modifying an operand address (cid:12)eld to obtain the operand. For
instance, the operand or its location can be assumed, as in the CLA (clear accumulator)
instruction, or the operand location can be identi(cid:12)ed in the instruction itself, as in the \add
$t1, $t2, $t3" instruction.

We have already used several addressing modes when writing MIPS-I assembly-level
programs. Some instruction sets are equipped with even more powerful addressing modes,

3.7. Operand Addressing Modes: Design Choices

137

which give more (cid:13)exibility to the assembly language programmer. These addressing modes
include capabilities such as pointers to memory, counters for loop control, indexing of data,
and relocation of programs. In this section we will examine the common addressing modes
for specifying operands in assembly languages. It is important to note that many of these
addressing modes are not present in the instruction set architecture. Additional addressing
modes are provided in the assembly-level architecture to make it easier to program at the
assembly level. Fewer addressing modes are supported by the instruction set architecture
so as to make it easier to implement the architecture in hardware. Addressing modes that
are unsupported by the instruction set architecture are synthesized using the supported
addressing modes.

3.7.1

Instruction-Residing Operands: Immediate Operands

We shall start with addressing mode(s) for operands speci(cid:12)ed within the instruction (im-
mediate operands). In the MIPS-I instruction

li

$t1, 5

# 5 is an immediate operand

the operand 5 is speci(cid:12)ed using the immediate addressing mode. Almost all machines pro-
vide the immediate addressing mode, for specifying small integer constants. If an instruction
set does not support the immediate addressing mode, it would be di(cid:14)cult to specify such
constants. One option would be to hardwire a few memory locations or registers with fre-
quently required constants. The rest of the constants will then need to be synthesized from
the hardwired constants. Many of the newer machines use such a hardwired register for
storing the constant zero, and use the immediate addressing mode for specifying the re-
maining constants. We have already seen that the MIPS-I architecture has such a register,
called $zero.

3.7.2 Register Operands

In register-based architectures such as MIPS-I, many of the operands reside in registers. The
common method for specifying register operands is to employ the register addressing
mode, which is perhaps the most frequently used addressing mode in a register-based
machine. In this addressing mode, the name or address of the register is speci(cid:12)ed in the
instruction. We have already used this addressing mode several times in this chapter. For
example, the following MIPS-I instruction uses the register addressing mode to specify 2
operands|a source operand and a destination operand.

move

$t1, $t2

# The operand addresses are registers $t1 and $t2

In some architectures such as the IA-32, some of the registers are special. Often, when
using such as Apart from the register addressing mode,

138

Chapter 3. Assembly-Level Architecture | User Mode

3.7.3 Memory Operands

Memory operands have the largest variety of addressing modes, including ones with indi-
rection. We shall look at the commonly used ones here.

Memory Direct Addressing:
In this addressing mode, the entire address of the memory
operand is given explicitly as part of the instruction. Example instructions that use the
memory direct addressing mode to fetch a memory operand are given below:

lw
lw

# Source operand is in memory location label1
$t1, label1
$t1, 0x10000000# Source operand is in memory location 0x10000000

In the second instruction, the source operand is present in memory location whose address
is 0x10000000. This address is explicitly speci(cid:12)ed in the instruction. Memory direct ad-
dressing has two limitations: (i) At the ISA level, the entire address of the memory operand
has to be encoded in the instruction, which makes the instruction long. (ii) The address
must be determined and (cid:12)xed at the time of programming or the assembly process. Once
(cid:12)xed, this address cannot be changed when the program is being run, unless the architecture
permits self-modifying code11 . Therefore, memory direct addressing is limited to accessing
global variables whose addresses are known at the time of programming or the assembly
process.

Register Indirect Addressing:
In this addressing mode, the instruction speci(cid:12)es a
register as in register direct addressing, but the speci(cid:12)ed register contains the address of
the memory location where the operand is present. Thus, the e(cid:11)ective address of the
memory operand is in the register whose name or number appears in the instruction. This
addressing mode is useful for implementing the pointer data type of high-level languages.
For example,

lw

$t1, ($t2)

# Memory address of source operand is in register $t2

The big advantage of register indirect addressing is that it can reference memory without
paying the price of specifying a full memory address in the instruction. Second, by modifying
the contents of the speci(cid:12)ed register, it is possible to access di(cid:11)erent memory words on
di(cid:11)erent executions of the instruction. The utility of indirect addressing was demonstrated
in the example loop code in Section 3.4.5, which involved (cid:12)nding the sum of the elements
of an array. In that program, the add $t1, 4 instruction (which uses register addressing)
causes the address speci(cid:12)ed by the lw instruction (which uses a form of indirect addressing)

11 In an architecture that permits self-modifying code, the program is allowed to use the .text section as
data as well. The program can thus modify some of its instructions at run time, by writing to the memory
locations allotted to those instructions. Self-modifying code was common in the early days of computer
programming, but is not in vogue any more because of debugging di(cid:14)culties.

3.7. Operand Addressing Modes: Design Choices

139

to point to the next element of the array. What is interesting to note is that the loop body
does not explicitly specify any memory addresses.

Autoincrement Addressing: This is an extension of register indirect addressing. When
this mode is speci(cid:12)ed, the speci(cid:12)ed register is incremented by a (cid:12)xed amount (usually 1),
after using the current value of the register for determining the e(cid:11)ective address of the
operand. For example,

lw

$t1, ($t2)+

# lw $t1, ($t2)
# addu $t2, 1

When this instruction is executed, the current value of $t2 is used for determining the
memory address of the operand, and then $t2 is incremented. This addressing mode is not
speci(cid:12)ed in the MIPS-I assembly language. It was common in earlier assembly languages,
particularly for use inside loops.

Indexed or Register Relative Addressing: This is an extension of register indirect
addressing. The e(cid:11)ective address of the memory operand is given by the sum of an index
register value and an o(cid:11)set value speci(cid:12)ed in the instruction. For example,

add
add
add

$t1, 100($t2) # Mem addr of operand is 100 + contents of $t2
$t1, label($t2)# Mem addr of operand is addr of label + contents of $t2
$t1, $t3($t2) # Mem addr is contents of $t2 + contents of $t3

The (cid:12)rst instruction uses a single index register ($t2) and an o(cid:11)set, which are added
together to obtain the memory address of the operand. Notice that the value of the index
register does not change. The second instruction speci(cid:12)es a label as the o(cid:11)set. This mode is
particularly useful for accessing arrays; the index register serves as an index into the array
starting at label1. The third instruction uses two index registers, the contents of which
are added together to obtain the memory address of the operand; this indexed addressing
mode is not supported in the MIPS-I assembly language. Some assembly languages even
permit the index register to be a special register, such as the PC. When register PC is used as
an index register, it is often called PC-relative addressing. Some machines dedicate one
register to function solely as an index register. This register is addressed implicitly when
an index mode is speci(cid:12)ed. In other machines, other special registers or GPRs can be used
as an index register. In such a situation, the index register must be explicitly speci(cid:12)ed in
the instruction.

Memory Indirect Addressing:
In this addressing mode, the instruction speci(cid:12)es the
memory address where the e(cid:11)ective address of the memory operand is present. This ad-
dressing mode is not supported in the MIPS-I assembly language. An example MIPS-like
instruction that uses the memory indirect addressing mode is given below.

140

Chapter 3. Assembly-Level Architecture | User Mode

lw

$t1, (label1) # Mem addr of operand is in mem location label1

3.7.4 Stack Operands

Finally, let us look at stack operands. In the \pure" stack machines that we saw in Section
3.6.5, a stack operand can be accessed only if it is currently at the top of the stack. In
such machines, the location of stack operands need not be speci(cid:12)ed explicitly, and instead
can be speci(cid:12)ed implicitly. This type of addressing is called implicit addressing mode. The
operand or its e(cid:11)ective address is implicitly speci(cid:12)ed by the opcode. An example of such
an instruction is given below.

add

# Both operands are on top of stack

Most of the register-based machines provide a stack model to the programmer, but use
a less strict access mechanism for stack operands. They have a program-managed stack
pointer register, which can be used as an index register to access stack operands that are
not necessarily at the top of the stack.

Registers

$t0
$t1
$t2
$t3
$sp
pc

200
200
300
500

Addr

200

Main
Memory
50

300

400

500

1000

500

200

Example: Consider the register map and memory gap given. Each of the memory locations
explicitly shown represents 4 bytes. What is the value of the operand that is written to $t1
in each of the following MIPS-like instructions?

1. move
$t1, $t2
This is register direct addressing, and the operand value is 200.

2. lw
$t1, 300
This is memory direct addressing, and the operand value is 500.

3. li
$t1, 300
This is immediate addressing, and the operand value is 300.

3.8. Subroutine Implementation

141

4. lw
$t1, ($t2)
This is register indirect addressing, and the operand value is the contents of memory
location 200, which is 50.

5. lw
$t1, $t2($t3)
This is register indexed addressing, and the operand value is the contents of memory
location 400 (obtained by adding the contents of $t2 and $t3), which is 1000.

6. lw
$t1, (500)
This is memory indirect addressing; the e(cid:11)ective address of the operand is the contents
of memory location 500, and the operand value is 50.

3.8 Subroutine Implementation

In structured programming languages, subroutines (and macros) are the main mechanism
for control abstraction, which permits associating a name with a potentially complex code
fragment that can be thought in terms of its function rather than its implementation. In
Section ***, we saw how a subroutine is implemented in the MIPS-I assembly language.
In this section, we take a broader look at this topic, which is at the core of structured
programming. The two things that (i) a subroutine can be called from di(cid:11)erent places
in the program, and (ii) after the completion of the subroutine, control returns to the
calling place. For the proper functioning of a subroutine, at the assembly language level, a
subroutine requires its own storage space for storing the following: its return address of the
subroutine, its local variables, links to variables in non-local scopes 12 , and temporary values
it produces. In addition, it may require register space to store frequently used values.

Speci(cid:12)cally, we discussed three sub-topics there: return address saving, parameter pass-
ing, and saving (and restoring) of registers. In this section, we shall look at some design
choices in these areas.
Important issues to consider in implementing subroutines in an
assembly language are:

(cid:15) The HLL subroutine may have local variables declared within it, for which storage
space needs to be allocated by the assembly language program. Because of the possi-
bility of recursion, each instance of a subroutine requires a new set of storage locations
for its local variables.

(cid:15) Temporary values created within an assembly language subroutine may need to be
stored in memory locations and registers. We cannot specify a (cid:12)xed set of memory
locations for a subroutine, again because of recursion. Conceptually, each run-time
instance of a subroutine requires a few \fresh" memory locations and registers.

12 Some high-level languages such as

142

Chapter 3. Assembly-Level Architecture | User Mode

(cid:15) A subroutine may be called from di(cid:11)erent places in a program. When the subroutine
(cid:12)nishes execution, control must return to the instruction that immediately follows the
call instruction that passed control to the subroutine.

(cid:15) Most subroutines need the calling program to pass parameters to them at the time
they are called. Also, often, a subroutine may need to pass a return value to the
calling program.

We shall take a detailed look at each of these issues and the solutions developed to
handle them. One important aspect that guides these solutions is that the development of
an assembly language subroutine|be it by a programmer or a compiler|is often done in
isolation to the development of its calling routine(s). This means that the calling routines as
well as the subroutines must adhere to a set of well-de(cid:12)ned speci(cid:12)cations or conventions. If
the calling routine was developed with a particular convention in mind, and the subroutine
was developed with another, then the program as a whole may not guarantee correct results.

The crux of the (cid:12)rst two problems mentioned above is that each invocation of a subrou-
tine needs a separate working environment. This environment consists of a set of registers
and memory locations that can be used for allocating local variables, and storing temporary
values. To solve the last two problems, we need to provide a well-de(cid:12)ned communication
mechanism between the working environments of the caller and the callee.

We already saw in Section 3.4.2 how local variables of a HLL program are allocated
storage space in the corresponding assembly language program. The conventional method
is to build a new stack frame every time a subroutine is called, and to allocate speci(cid:12)c
locations within the stack frame for each local variable. The stack frames are created within
the stack section of the memory address space, and are organized as a LIFO structure. The
provision of an independent stack frame for each active subroutine enables each subroutine
instance to have its own set of storage locations for allocating its local variables. In the
following subsections, we will see how the stack frame concept has become the backbone
for solving all of the above mentioned problems associated with implementing subroutines.

3.8.1 Register Saving and Restoring

As discussed, the development of an assembly language subroutine|whether by a program-
mer or a compiler|is often done in isolation to the development of the calling program.
This means that at the time of subroutine development, it is di(cid:14)cult to identify the regis-
ters that are not used in the calling environment, and are therefore available for its use. If
the subroutine developer blindly uses an arbitrary register for storing a temporary value,
there is a possibility of overwriting useful information belonging to the calling environment.
Notice that a similar problem is avoided for memory values by utilizing a separate stack
frame for each active subroutine. One possibility is to provide a similar arrangement for
registers. Sun Microsystems’ SPARC architecture does precisely that. It uses the concept
of multiple register windows. Each active subroutine has its own private register name

3.8. Subroutine Implementation

143

space called register window. Every time a subroutine is called, a new register window is
made available to the newly activated subroutine. When the subroutine (cid:12)nishes execution,
its register window ceases to exist.

A more conventional approach for furnishing registers to a subroutine is to let both the
caller and the callee use the same register set, but provide a means to temporarily save the
values pertaining to the caller, prior to the callee using them. Once the callee has completed
its usage of a register set, their original values are restored. Again, a convenient place to
temporarily save the register values is the stack frame. Figure 3.17 shows the layout of a
stack frame in which space has been set apart for saving general-purpose registers as well
as (cid:13)oating-point registers. This is the approach followed in the MIPS-I ALA.

Performing the saving (and restoring) of the register values at the proper times is the
responsibility of the assembly language programmer. This saving (and restoring) can be
done by the caller (cal ler save), by the callee (cal lee save), or by a combination of the two.
It is important to note that it is not necessary to save the entire set of registers in the
name space. Strictly speaking, in order to ensure program correctness, we need to save only
those registers that contain useful values (such registers are called live registers) and are
about to be overwritten. However, it is di(cid:14)cult for either the caller or the callee to verify
both of these requirements: liveness of a register as well as the de(cid:12)niteness of overwriting it.
The caller knows about the liveness of a register, but not its probability to be overwritten
by the callee. The callee, on the other hand, knows when the register will be overwritten,
but it does not know if the register is live! Thus, whether we use caller save or callee
save, some ine(cid:14)ciency is bound to occur. Assembly languages like MIPS-I AL incorporate
some conventions about register usage to trim this ine(cid:14)ciency: some of the registers are
earmarked for caller save, and some others are earmarked for callee save.

3.8.2 Return Address Storing

Because a subroutine may be called from di(cid:11)erent places in a program, provision must be
made for returning to the appropriate location in the program after the subroutine completes
execution. To do this, the return address must be saved somewhere before transferring
control to the subroutine. This saving can be done either by inserting extra instruction(s)
before the call instruction, or by specifying it as part of the call instruction’s semantics.
Most of the machines follow the latter approach. The way in which a computer supports
control (cid:13)ow changes to and from subroutines is referred to as its subroutine linkage
method.

What would be a good place to store the return address? A commonly used method
is to store it in a special register, called a link register. This provides a fast mechanism to
store and retrieve the return address. However, it does not allow subroutine nesting; i.e.,
one subroutine calling another. When a nested subroutine call is made, the return address
of the second call also gets stored in the link register, overwriting its previous contents
(the return address of the (cid:12)rst call). Hence it is essential to save the link register contents

144

Chapter 3. Assembly-Level Architecture | User Mode

Lower Addresses

Current Stack Frame

$sp

Space for storing
callee arguments

Space for storing
FPRs

Space for storing
GPRs

Space for storing
Temporary Values

Space for allocating
Local Variables

Higher Addresses

Figure 3.17: Layout of a Typical Stack Frame

somewhere else before calling another subroutine.

Conceptually, subroutine nesting can be carried out to any depth. At any point in time,
the (cid:12)rst subroutine to complete will be the last one to be called. Its return address is the
last one generated by the nested call sequence. That is, the return addresses are generated
and used in a last-in (cid:12)rst-out (LIFO) order. This suggests that it would be ideal to save
the return addresses in a stack-like structure; the LIFO nature of stack pushes and pops
(cid:12)ts naturally with the LIFO nature of subroutine calls and returns. Instead of de(cid:12)ning a
separate stack structure for storing the return addresses, however, we can conveniently store
a subroutine’s return address in its stack frame itself, as we saw for the MIPS-I assembly
language.

If a subroutine needs to call another, then the assembly language programmer (or com-
piler) writing the caller routine includes instructions to save the contents of $ra on the stack
frame (before performing the call) and to restore the contents of $ra (after returning from
the callee). Typically, these instructions are placed at the beginning and end, respectively,
of the calling subroutine.

3.8. Subroutine Implementation

145

3.8.3 Parameter Passing and Return Value Passing

Finally, when calling a subroutine, the calling routine must provide to the callee the pa-
rameters, that is, the operands or their addresses, to be used in the computation. Later,
the subroutine may return the results of the computation to the calling routine. This ex-
change of information between a calling routine and a subroutine is referred to as parameter
passing. Parameter passing may occur in several ways. The parameters may be placed in
registers or in the stack, where they can be accessed by the subroutine. Figure 3.18 shows
a parameter being passed to subroutine P through register $a0. The return value is passed
by the subroutine back to the calling program through register $v0.

Calling
Routine

li
jal

$a0, 1
P

# place argument in $a0
# call subroutine P
# use return value stored in $v0

Called
Subroutine

P:

# use parameter stored in $a0

# place return value in $v0
$ra # return

jr

Figure 3.18: Passing Parameters and Return Value through Registers

Many assembly languages have conventions about which registers are used for passing
parameters and return values. For instance, the MIPS-I AL, as we saw, designates 4 regis-
ters, $a0-$a3 for passing parameters and 2 registers, $v0 and $v1, for passing return values.
When one subroutine wants to call another, it may need to save the incoming parameters
(present in registers $a0-$a3) in the stack frame, and copy the outgoing parameters (for
the callee) onto the same registers. Assembly languages with register windows avoid this
overhead by overlapping a portion of two adjacent register windows.

Passing parameters through general-purpose registers is straightforward and e(cid:14)cient.
However, if many parameters are involved, there may not be enough general-purpose reg-
isters available for this purpose. In such a situation, the parameters may be placed on the
caller subroutine’s stack frame, from where the callee subroutine can access them. This is
depicted in the stack frame layout given in Figure 3.17. The stack frame provides a very
(cid:13)exible alternative, because it can handle a large number of parameters. Before calling the
subroutine, the calling program copies all of the parameters to the stack frame. The called
subroutine can access the parameters from the stack frame. Before returning to the calling
program, the return values can also be placed on the stack frame.

146

Chapter 3. Assembly-Level Architecture | User Mode

3.9 De(cid:12)ning Assembly Languages for Programmability

When assembly languages were (cid:12)rst introduced, they were very similar to the lower-level
machine language (ML) they corresponded to, except that they used alphanumeric symbols
instead of binary codes. Thus, instead of coding in a machine language the bit pattern
10001100010000010101010110000010, the programmer could code the same instruction
in an assembly language as lw R1, 0x5582(R2). The assembler would translate each AL
instruction into precisely one ML instruction. With improvements in assembler technology,
this strict correspondence to machine language became relaxed. We now have powerful
assembly languages that provide several additional features. We shall discuss some of these
features below.

3.9.1 Labels

In order to do this, the assembly language programmer has to keep track of the memory
locations that correspond to di(cid:11)erent variables. Keeping track of the memory locations
assigned to variables can be quite tedious for an assembly language programmer. Most
assembly languages therefore provide the ability to symbolically specify the memory location
corresponding to an HLL variable. Each symbolic name in the assembly language program
is eventually replaced by the appropriate memory address during the assembly process.

3.9.2 Pseudoinstructions

Pseudoinstructions are instructions that are present in the assembly-level architecture, but
not in the instruction set architecture. Such instructions are not implemented in the hard-
ware, but are synthesized by the assembler using sequences of instructions present in the
machine language. Modern assembly languages often support di(cid:11)erent types of pseudoin-
structions. For example, the assembly-level architecture may support addressing modes that
are not really present at the ISA level. By extending the instruction set in this manner, the
assembly language makes it easier to program at the assembly level, without adding any
complexity to the hardware. During assembly, each pseudoinstruction is synthesized using
one or more ML instructions.

3.9.3 Macros

Macros go one step beyond pseudoinstructions by allowing the programmer to de(cid:12)ne (or use
prede(cid:12)ned) parameterized sequences of instructions that will be expanded during assembly.
Macros are very helpful in reducing the source code size as well as in making it more
readable, without incurring the overhead of subroutine calls and returns. It is often the case
that a sequence of instructions is repeated several times within a program. An example
of a sequence of instructions might be the operation that pushes data onto the stack, or

3.10. Concluding Remarks

147

the operation that pops data o(cid:11) the stack. A mechanism that lets the assembly language
programmer de(cid:12)ne sequences of instructions, and associate these instructions with a key
word or phrase is called a macro. Macros allow the assembly language programmer to de(cid:12)ne
a level of abstraction.

Simple text-substitution macros can be easily incorporated into an assembly language
by using the C language’s #define construct. The assembler can invoke the C preprocessor
cpp to do the required text substitution prior to carrying out the assembly process. An
example for such a macro de(cid:12)nition and use is given below.

#define LEAF(fname) n
.text; n
.globl
.ent
fname:

fname; n
fname; n

LEAF(foo)

3.10 Concluding Remarks

3.11 Exercises

1. Explain the ma jor di(cid:11)erences between high-level languages and assembly languages.

2. Explain why local variables (the ones declared inside subroutines) are typically as-
signed memory locations in the stack, and not in the .data section of memory.

3. Explain why the memory direct addressing mode cannot be used for accessing data
from the stack and heap sections of memory.

4. An assembly-level architecture de(cid:12)nes directives, non-branch instructions, branch in-
structions, subroutine calls and returns, registers, memory address space, macros,
operand stack, and AL-speci(cid:12)c instructions. Which of these are non-essential from a
strictly functional point of view? Explain.

5. Explain why, during compilation, dynamically allocated variables of a HLL program
are typically assigned memory locations in the heap section of memory and not in the
stack, even if the dynamic allocation takes place inside a subroutine?

6. Consider the following variable declaration and assignment involving pointers in C.
Translate this C code into MIPS-I assembly language code.

148

Chapter 3. Assembly-Level Architecture | User Mode

int **ppn, n;

*ppn = &n;

Example Assignment Statement Involving Pointers

7. Consider the following C code snippet:

i:
j:
k:

.data
.word 24
.word 22
.word 0

.text
__start:la
lw
jal
sw

$t0, i
$a0, 0($t0)
foo
$v0, k

foo:

loop:

$t1, j
lw
li
$v0, 0
addi $v0, $v0, 2
addi $t1, $t1, 1
$t1, $a0, loop
bne
jr
$ra

li
$v0, 10
syscall

# Code for exit system call
# Call OS to exit

(a) Trace the execution of this MIPS-I program for 12 instructions. Tracing of an in-
struction involves showing what value that instruction writes to a register or memory
location. The 12 instructions that you trace must be written in the order in which
they are executed.

(b) How many memory data references will be made during the execution of these 12
instructions? That is, how many values will be transferred from the memory to the
processor?

(c) (2 points) Will this program exit by calling the OS, or will it stay in an in(cid:12)nite
loop? Explain.

Chapter 4

Assembly-Level Architecture |
Kernel Mode

Give instruction to a wise man, and he wil l be stil l wiser;
Teach a just man, and he wil l increase in learning.

Proverbs 9: 9

The previous chapter discussed at length the assembly-level architecture machine seen
by application programmers. We also saw the basics of translating high-level language
application programs to equivalent assembly language programs. As discussed in the last
part of chapter 2, high-level languages provide application programmers with an application
programming interface (API) for specifying system-speci(cid:12)c functions such as input/output
and memory management. The API is implemented by the operating system kernel that
resides in computer systems. When an application program invokes one of the system
call functions speci(cid:12)ed in the API, the control of the computer system is transferred from
the application program to the operating system (OS), which performs the function, and
transfers control back to the application program.

In order to perform the system functions in an adequate and e(cid:14)cient manner, the
machine needs to include special resources to support the OS. Such resources are generally
restricted to the OS, and typically include a few registers, a notable portion of the memory
address space, a few instructions, and direct access to all of the IO device controllers.
Application programs are not allowed to access these restricted resources. In order to enforce
the distinction between application programs (which can only access limited resources) and
OS programs (which can access all resources), computers can operate in at least two di(cid:11)erent
execution modes|the User mode and the Kernel mode, which is also called Supervisor
mode or Privileged mode; in this book we use the term Kernel mode. The Kernel mode is

149

150

Chapter 4. Assembly-Level Architecture | Kernel Mode

intended to execute instructions belonging to the OS, and the User mode is intended to
execute instructions belonging to application programs.

\Every mode of life has its conveniences."
| Samuel Johnson. The Id ler

It is important to note that many of the routines in the OS kernel are executed on behalf
of user programs. For example, the shell program executes in the User mode and invokes
a system call (syscall) instruction to obtain the characters entered by the computer user
on the terminal keyboard. This syscall instruction is implemented by the OS kernel, which
executes in the Kernel mode on behalf of the shell program, reads the characters typed on
the keyboard, and returns the characters to the shell. The shell then executes in User mode,
interprets the character stream typed by the user, and performs the set of actions speci(cid:12)ed
by the user, which might involve invoking other syscall instructions.

Thus, Computer operating systems are another classic example of event-driven programs
on at least two levels. At the lowest level, interrupt handlers act as direct event handlers
for hardware events, with the CPU hardware performing the role of the dispatcher. Op-
erating systems also typically act as dispatchers for software processes, passing data and
software interrupts to user processes that in many cases are programmed as event handlers
themselves.

The discussion so far might suggest that systems spend very little time in the kernel
mode, and that most of the time is spent in the user mode. The truth is just the opposite!
Many embedded systems never leave the kernel mode. A signi(cid:12)cant amount of code is
therefore developed for the kernel mode.

4.1 Overview of Kernel Mode Assembly-Level Architecture

In this chapter, we will study the Kernel mode aspects of the assembly-level architecture of
modern computers. The Kernel mode part is similar in many ways to the User mode part
that we saw in detail in Chapter 3. In particular, the register model, the memory model,
the data types, and the instruction types available to the Kernel mode assembly language
programmer are all quite similar to those available to the User mode assembly language
programmer. Therefore, it is more instructive to consider the di(cid:11)erences between the two
modes, the details of which, unfortunately, vary somewhat from one machine to another.
The main di(cid:11)erences are given below:

(cid:15) In addition to the register set available in the User mode, an additional set of registers
called privileged registers is available in the Kernel mode. One such register is the
processor status register, for instance. The register set available in the Kernel
mode is a superset of what is available in the User mode.

(cid:15) Like the case with the register set, an extended memory address space is usually

4.1. Overview of Kernel Mode Assembly-Level Architecture

151

available in the Kernel mode. The overall Kernel mode memory address space may be
divided between addresses that are accessible only in the Kernel mode and addresses
that are accessible in both modes. User mode programs can access only User mode
addresses. Kernel mode programs can access both Kernel mode and User mode ad-
dresses. For instance, in the MIPS-I assembly-level architecture, memory addresses
from 0x80000000 to 0xffffffff can be accessed only by the OS.

(cid:15) In the Kernel mode, the assembly language programmer is provided a set of IO ports.
These ports are either provided as a set of IO registers or as part of the privileged
memory address space.

(cid:15) In the Kernel mode, the program has access to special hardware structures for perform-
ing resource management functions. One such hardware structure is TLB (Translation
Lookaside Bu(cid:11)er), which is used to implement the virtual memory concept.

(cid:15) In addition to the instruction set available in the User mode, an additional set of in-
structions called privileged instructions is available in the Kernel mode. The privileged
instructions cannot be executed while in User mode. For example, a machine may
have an instruction that manipulates the processor status register. If a User program
uses this instruction, the computer will not execute it, and instead will signal an error.

4.1.1 Privileged Registers

In addition to the register set available in the User mode, an additional set of registers
called privileged registers is available in the Kernel mode. Table 4.1 lists the privileged
registers de(cid:12)ned in the MIPS-I kernel mode architecture, along with their names and uses.
The (cid:12)rst eight registers in the table are used for implementing memory management, and
are explained in Chapter 7. The next 3 privileged registers|sr, cause, and epc|are used
for processor management. sr contains bits that specify the current operating mode and
conditions of the processor, such as the current interrupt priority level. epc is used to
store the memory address of the interrupted instruction, and is useful for returning to the
interrupted program after handling the interrupt/exception.
Its contents can be copied
to a general-purpose register rt by executing the privileged instruction \mfc0 rt, epc".
Finally, the PRId register is used for storing the processor’s generic type number.

Some architectures include a privileged register to point to the current process’ process
control block, the block of memory in the privileged address space where the OS stores
information about the process. Some architectures provide a page table pointer for speeding
up translations of virtual memory addresses to physical memory addresses. In machines
using vectored interrupts, an interrupt vector register may be provided.

152

Chapter 4. Assembly-Level Architecture | Kernel Mode

Register Register
Number Name

Use

0
1
2
4
5
6
8
10
12
13
14
15

Index
Random
EntryLo
Context
PageMask
Wired
BadVaddr
EntryHi
SR
Cause
EPC
PRId

Status register
Store the cause of the most recent exceptional event
Exception PC; store PC value of interrupted instruction
Processor ID register; store this processor’s generic type number

Table 4.1: Names and Uses of MIPS-I Privileged Registers

4.1.2 Privileged Memory Address Space

4.1.3

IO Addresses

The IO models supported by the User mode assembly-level architecture and the Kernel
mode assembly-level architecture are quite di(cid:11)erent. As discussed in Chapters 2 and 3,
the IO model presented to application program developers is at the level of (cid:12)les, and is
somewhat abstract. Any operation on a (cid:12)le is accomplished by calling the operating system.
The IO model presented to operating system developers is more concrete, and involves a
collection of IO addresses, which are accessed by IO instructions. In other words, the IO
primitives provided by the kernel mode machine consist of an IO address space and a set
of (privileged) IO instructions. The exact nature of the IO address space depends on the
type of IO addressing used, and is discussed in Section 4.3.

4.1.4 Privileged Instructions

The Kernel mode architecture includes additional address spaces and registers, as we just
saw.
In order to provide exclusive access to these, an additional set of instructions are
also included in the Kernel mode ISA. These instructions are called privileged instructions.
Examples include instructions to access IO addresses, instructions to manage memory, and
instructions to manage processes. The Kernel mode instruction set is thus a superset of the
User mode instruction set, as pictorially depicted in Figure 4.1. The User mode instruction
set contains a set of syscall instructions, as well as non-syscall instructions for performing

4.2. Switching from User Mode to Kernel Mode

153

data transfer operations between registers and memory, arithmetic/logic operations, and
control (cid:13)ow change operations. At the microarchitecture level (which is two levels below
the assembly level), a non-syscall instruction is directly interpreted for execution, whereas
a syscall instruction is interpreted by invoking a prede(cid:12)ned OS service. That is, a syscall
instruction is interpreted by executing a sequence of instructions in the Kernel mode (some
of which will be privileged instructions), which are then directly interpreted for execution
in the underlying kernel mode microarchitecture.

User Mode

System Call
Instructions

Non−system Call
Instructions

Privileged
Instructions

Kernel Mode

Figure 4.1: Relation between the User Mode and Kernel Mode Instruction Sets

The privileged instructions|which are not available in the User mode|consist of IO
instructions, inter-process synchronization instructions, memory management instructions,
and instructions to enable and disable interrupts. IO instructions include instructions that
read from or write to IO registers. The reason for keeping the IO instructions privileged is
straightforward: if an application program is permitted to execute a privileged instruction,
then it could read con(cid:12)dential data stored anywhere in the system, write on other users’
data, erase all of the information on a disk, and, in general, become a threat to the security
of the system itself. So, what will happen if a programmer includes a privileged instruction
in an application program? When the program is being executed, an attempt to execute
that instruction will generate an exception1 , which causes control to be transferred to the
OS. The OS will most likely terminate that application program.

4.2 Switching from User Mode to Kernel Mode

We can think of three events that cause the execution mode to switch from User mode to
Kernel mode, causing control to transfer from user code to kernel code. They are: syscall
instructions, device interrupts, and exceptions. These three events are illustrated in Figure
4.2, and are discussed in detail in this section. When any of these events happen, the kernel
gets the control and performs the required action. Among these three events, only the
action to be done for syscall instructions is de(cid:12)ned in the API (Application Programming
Interface).

The (cid:12)rst thing the kernel does after getting the control is to disable all interrupts (i.e.,
set the processor state to not accept any more interrupts), and save the essential state of

1Most assemblers will (cid:13)ag this as an error during the assembly process.

154

Chapter 4. Assembly-Level Architecture | Kernel Mode

the interrupted process on the kernel stack2 . This state includes the contents of the process’
general-purpose registers, program counter, and process status word. Afterwards, the kernel
determines the cause of the interrupt, and calls the appropriate low-level handler routine
by looking up a dispatch table containing the addresses of these routines. This low-level
routine performs the functions for which the OS was speci(cid:12)cally called at that time. When
this low-level routine completes, the kernel restores the state of the process, and sets the
execution mode back to the previous value.

Device Interrupts

System Calls

Exceptions

User
Mode

Kernel
Mode

RFE − Returns

Figure 4.2: Events Causing Switching Between User Mode and Kernel Mode

4.2.1 Syscall Instructions: Switching Initiated by User Programs

When an application program is being executed, the program can voluntarily hand over the
machine’s control to the OS by executing a syscall instruction (sometimes called software
interrupt or programmed interrupt). Execution of a syscall instruction is a synchronous
event, because it occurs at the same point of execution when a program is run multiple
times. To the application programmer, a syscall instruction seems very much like a function
call; however, this control (cid:13)ow change causes the processor to switch to the Kernel mode
and to begin executing kernel code. The exact semantics of each system call are de(cid:12)ned
in the API, and can be di(cid:11)erent, at least theoretically, in di(cid:11)erent APIs. For producing
portable code, however, the semantics of each system call are kept more or less the same
across APIs. To be on the safe side, it is prudent for application programs not to directly
call the OS, but instead call an appropriate library routine. When porting to a platform
with a di(cid:11)erent API, all that is required then is to use a di(cid:11)erent set of library routines
that suit the new API.

When a syscall instruction is executed, the computer temporarily stops execution of the
current program, switches to Kernel mode, and transfers control to a special OS routine
called system call layer. Figure 4.3 illustrates how transfer of control takes place when
a syscall instruction is executed. As shown, the syscall instruction is treated very similar

2Some operating systems save the state in the interrupted process’ user stack. Some others save the state
in a global interrupt stack that stores the frames for those interrupt handlers that are guaranteed to return
without switching context.

4.2. Switching from User Mode to Kernel Mode

155

to a jal S (jump and link) instruction, where S is the address of the (cid:12)rst instruction of
the system call layer routine. Thereafter the machine executes the instructions of this part
of the OS. After executing this routine, the eret instruction at the end of the routine
causes control to return to the instruction that immediately follows the syscall instruction
in the application program. The system call layer routine is very much like a subroutine;
an important di(cid:11)erence, however, is that it executes in Kernel mode.

User Mode

Application
Program

syscall

Kernel Mode

System Call
Interface
Routine

S:

#enable interrupts

eret

Similar to ‘jal S’ instruction

Figure 4.3: Transfer of Control while Executing a System Call Instruction

It is important to see the ma jor actions speci(cid:12)ed by a syscall instruction. These functions
are described below:

(cid:15) Switch to kernel mode

(cid:15) Disable interrupts: One of the (cid:12)rst actions to be performed by the system call layer
when control transfers to it is to save the current register state and perform other
book-keeping functions.
If another interrupt is accepted during this period, there
is a potential to loose useful data. Therefore, it is prudent to temporarily disable
interrupts. After performing the book-keeping functions, the handler may enable
interrupts of higher priority.

(cid:15) Save return address: The syscall instruction is similar to a subroutine call in many
ways. One of the striking similarities is in the manner of control (cid:13)ow return. When
control returns to the program that contains the syscall instruction, execution contin-
ues from the next instruction onwards. In order to e(cid:11)ect such a control (cid:13)ow transfer,
the return address (the address of the instruction immediately after the syscall in-
struction in the static program) needs to be recorded. The MIPS-I architecture, for
instance, speci(cid:12)es a privileged register called epc (exception program counter) for
storing this return address.

(cid:15) Record the cause for this exceptional event: Once the syscall instruction is executed
and control is transfered to a handler, the system has no way of remembering the

156

Chapter 4. Assembly-Level Architecture | Kernel Mode

reason for activating the handler. This is especially the case if multiple exceptional
events transfer control to the same entry point, i.e., the same handler. For instance,
the MIPS-I architecture speci(cid:12)es the same entry point (0x80000080) for all but two of
the exceptional events. To identify the reason for transferring control to this memory
location, the MIPS-I architecture provides a privileged register called cause.

(cid:15) Update pc to point to the entry point associated with syscall instructions. For the
MIPS-I architecture, this entry point is 0x80000080.

4.2.2 Device Interrupts: Switching Initiated by IO Interfaces

The syscall instruction is useful when an application program needs some service from the
OS. Sometimes, the currently executing application program may not need any service from
the OS, but an IO device needs attention, requiring the OS to be executed. This requirement
has led to the provision of device interrupts (also called hardware interrupts) by which an
IO device can notify the computer when it requires attention. Unlike a system call, a device
interrupt is an asynchronous event, because it may not occur at the same point of execution
when a program is run multiple times.

In order to run the OS, the currently running program has to be temporarily stopped.
Therefore, when an interrupt is received, the computer temporarily stops execution of the
current program and transfers control to a special OS routine called interrupt service
routine (ISR) or interrupt handler.
If the machine was in the User mode at the
time of the interrupt, then it is switched to the Kernel mode, giving the operating system
privileged access to the machine’s resources. An ISR is very much like the subroutines that
we saw earlier; an important di(cid:11)erence, however, is that a subroutine performs a function
required by the program from which it is called, whereas the ISR may have nothing in
common with the program being executed at the time the interrupt request is received.
The exact manner in which the interrupting device is identi(cid:12)ed and the appropriate ISR is
called varies from one machine to another.

Figure 4.4 illustrates one way of transfering control to the ISR when an interrupt is
raised. Assume that an interrupt request arrives during the execution of instruction i. The
computer (cid:12)rst completes the execution of i. Then it transfers control to an OS routine called
interrupt hander interface. This routine identi(cid:12)es the interrupting device, and determines
the starting address of the appropriate ISR, namely P in the (cid:12)gure.
It then transfers
control to the ISR by means of a CALL instruction. Thereafter the machine executes the
instructions of the ISR. After executing the ISR, control is returned to the interrupt hander
interface. The interface routine is terminated by an ERET instruction, which causes control
to return to instruction i + 1 in the interrupted program. Along with this, the computer
also switches back to the User mode.

Because an interrupt is an unscheduled event, the ISR must save and restore any registers
that it modi(cid:12)es. A convenient place to save the registers is the kernel stack.

interrupt
service
routine,
interrupt
handler

4.2. Switching from User Mode to Kernel Mode

157

Interrupted
Program

Kernel Mode

Interrupt
Handler
Interface

Interrupt
Service
Routine

S:

P:

Interrupt occurs

i
i +1

eret

jr $ra

Equivalent to ‘jal S’ instruction

Figure 4.4: Transfer of Control while Servicing an Interrupt

In a multi-tasking environment, whenever the machine switches to the Kernel mode,
handing control over to the OS, it also performs process scheduling. That is, the OS decides
which application process should run next. For the OS to do this process scheduling, it
must run periodically. However, extended periods of time may elapse with the computer
being in the User mode and no syscall instructions or device interrupts. In order to perform
process scheduling in an adequate manner, the OS needs to gain control of the machine on
a periodic basis. Multi-tasking computers typically implement this by including as an IO
device a hardware timer, which issues a hardware interrupt at regular intervals (cid:12)xed by the
OS.

4.2.3 Exceptions: Switching Initiated by Rare Events

\The young man knows the rules, but the old man knows the exceptions".
| Oliver Wendel l Holmes, Sr (American Physician, Poet, Writer, Humorist and
Professor at Harvard, 1809-1894) in The Young Practitioner

An exception is an unexpected event generated from the program being executed. Ex-
amples are attempt to execute an unde(cid:12)ned instruction, arithmetic over(cid:13)ow, and divide
by zero. When an exception occurs, the machine switches to Kernel mode and generates
an exception vector depending on the type of exception. The exception vector indicates
the memory address from which the machine should start execution (in Kernel mode) after
it detected the exceptional event. In other words, after an exceptional event occurs, the
machine starts executing in Kernel mode from the address speci(cid:12)ed in the exception vector.
The machine may also record the cause of the exception, usually in a privileged register.
The rest of exception handling is similar to that of interrupt handling. That is, the return
address is saved, and control is transfered to an exception handler routine in the OS.

The exception handler routine at the exception vector performs the appropriate actions.
If that vector is used for di(cid:11)erent exception types, then the routine inspects the recorded
cause of the exception, and other relevant state information, and branches to an appropri-

158

Chapter 4. Assembly-Level Architecture | Kernel Mode

ate exception handler routine to handle the exception. After taking the necessary steps,
control may be returned to the application program that caused the exception, switching
the mode back to the User mode. Sometimes, exception handling may involve terminating
the application program that generated the exception, in which case the OS gives control
to another application program.

4.3

IO Registers

We saw in Section 4.1 that the IO model supported in the kernel mode consists of a set of
IO registers and a set of instructions to access them. The nature of these IO registers vary
considerably, depending on the addressing method used. Two types of addressing methods
are used for IO registers: memory mapped IO and independent IO (or IO mapped IO). We
shall discuss these two schemes in detail.

4.3.1 Memory Mapped IO Address Space

In memory mapped IO, each IO address refers to an IO register, an entity similar to a
memory location that can be read/written. Because of this similarity, the IO registers are
assigned locations within the memory address space itself. Thus, there is a single address
space for both memory locations and IO registers. Some portions of the memory address
space are assigned to IO registers; loads and stores to those addresses are interpreted as reads
and writes to IO registers. The main advantage with this approach is that no extensions are
required to the instruction set to support IO operations 3 . Another advantage is that it allows
for wider compatibility among IO speci(cid:12)cations across di(cid:11)erent computer families. The
MIPS-I architecture uses this approach; memory addresses from 0xa0000000 to 0xbfffffff
are available to the OS for use as IO registers. An example IO read instruction for the MIPS-
I architecture is

lw

$t1, keyboard status

which copies the contents of IO register labeled keyboard status to general-purpose register
$t1. At execution time, this label will refer to an address in the range 0xa0000000 -
0xbfffffff. This mapping is done either at assembly time by the assembler or at IO port
con(cid:12)guration time by the OS.

4.3.2

Independent IO Address Space

In this type of IO addressing, a separate IO address space is provided independent of the
memory address space. Thus, like the register space and the memory address space, there
is an IO address space also. When accessing an address in the IO address space, an IO

3At the microarchitectural level, the hardware has to distinguish IO operations from memory operations,
and treat them accordingly.

4.3.

IO Registers

159

address can be speci(cid:12)ed either by a new addressing mode, or by a new set of opcodes. It is
customary to use the latter approach | providing a separate set of opcodes speci(cid:12)cally for
manipulating IO addresses. An example IO read instruction for a MIPS-I-like architecture
would be

in

$t1, keyboard status

What do we gain by providing a separate address space for the IO (and a separate set
of opcodes as well)? On (cid:12)rst glance, there seems to be no apparent gain. Now consider this
scenario. When we have a separate IO address space, instead of organizing it as a linear
address space, we have the (cid:13)exibility of organizing it as a set of IO ports or IO programming
interfaces, entities that are more complex than IO registers and memory locations. That
is, each IO address refers to an IO port. Several instruction opcodes can be provided to
perform complex operations on an IO port. An example IO instruction that tests the status
of a port used for connecting a keyboard is
keyboard port
test

Examples of machines with independent IO are the Intel x86 and the IBM 370 computers.
Figure 4.5 illustrates the two types of IO address mapping. It is important to note that
the two types of IO addressing are not mutually exclusive.
In a machine that supports
independent IO, some of the memory addresses can still be mapped to IO registers. For
instance, a graphics display port is usually memory-mapped, to permit device drivers to
easily modify bit patterns in memory, which are then displayed on the screen.

Memory
address

Hard disk
Terminal
Printer

Memory address
space

Memory address
space

Memory
address

IO address space

Hard disk
Terminal
Printer

IO
address

IO address
space

Data

Data

Data

(i)

(ii)

Figure 4.5: Di(cid:11)erent IO Address Mappings: (i) Memory Mapped IO; (ii) Independent IO

160

Chapter 4. Assembly-Level Architecture | Kernel Mode

4.3.3 Operating System’s Use of IO Addresses

Irrespective of the type of IO addressing used, the OS views the IO address space as a
collection of IO ports or IO programming interfaces. The speci(cid:12)cations of di(cid:11)erent IO
ports can be di(cid:11)erent, and are determined based on the type of IO devices that are meant
to be connected to them. Most of the ports adhere to one standard or other, as we will see
in Chapter 8.

In an ISA that speci(cid:12)es IO registers, each IO port then encompasses several consecutive
IO registers. Depending on the characteristics of the port, some of its registers are used to
record status information related to the port (IO status registers); individual bits of a status
register may correspond to a di(cid:11)erent attribute of the port for example, the least signi(cid:12)cant
bit may specify whether a new character has been typed on the keyboard, the next bit may
specify whether an error has occurred, etc.). Other registers may be used to store data
to be transferred between IO ports and general-purpose registers or main memory. These
registers are called IO data registers. Some of the registers in a port may be viewed as
read-only, some may be write-only, and the rest may be read-write. The OS decides which
IO port needs to be accessed to access a particular (cid:12)le.

Notice that all that the OS program \sees" of an IO device is its port, which includes a
set of IO registers and speci(cid:12)cations for the operation of these registers. When an IO write
instruction writes a value to a status/control register in an IO port, the device controller
interprets it as a command to a particular IO device. When an OS routine wants to know
the status of an IO device, it uses an IO read instruction to read the status register of its IO
port. Similarly, an IO write instruction can be executed by an OS routine to send data to
the data register or status register of the IO port pertaining to an IO device. An example
IO write instruction for the MIPS-I architecture is
$t1, keyboard status
sw

where keyboard status is the address assigned to the status register of the IO port per-
taining to the keyboard.

The astute reader would have realized by now that the IO model presented to OS
programmers, although more concrete than the one presented to applications programmers,
still does not provide direct access to the IO devices. That is, the instructions in the OS
program do not directly access the IO device hardware. There are a variety of reasons for
this:

It would be impractical for the OS device
(cid:15) The IO devices are incredibly diverse.
drivers to incorporate the necessary functionality to directly control a wide range of
IO devices. For example, if the OS directly accesses a hard disk, to get a data item
from the disk, the device driver would need to execute many instructions that deal
with the intricate details of how exactly that disk works. In the future, if this disk is
replaced by a slightly di(cid:11)erent hard disk, then the device driver (to access the disk)
may also need to be changed.

4.3.

IO Registers

161

Interconnect

Registers

Main
Memory

Status/Control Register

Status/Control Register

Status/Control Register

Data Register

Data Register

Data Register

IO Port

IO Port

IO Port

Figure 4.6: Abstraction of IO Devices Presented to the OS

(cid:15) IO devices are often electromechanical devices whose manner of operation is di(cid:11)erent
from that of the rest of the machine, which are implemented using electronic devices.
Therefore, a conversion of signal values may be required.

(cid:15) The data transfer rate of IO devices is often much lower than that of the rest of the
system, necessitating special synchronization operations for correct transfer of data.

(cid:15) IO devices often use data formats and word lengths that are di(cid:11)erent from those
speci(cid:12)ed in the kernel mode.

Because IO ports are modeling electromechanical IO devices, the behavior of IO registers
can be quite di(cid:11)erent from that of ordinary registers and memory locations. An assembly
language systems programmer needs to be aware of these di(cid:11)erences. The following di(cid:11)er-
ences may come as surprises:

(cid:15) IO registers may be active elements, and not just passive storage elements. A write
to an IO register often has side e(cid:11)ects, such as initiating an activity by the IO device
connected to the port. A read to an IO register may also have side e(cid:11)ects, such as
clearing an interrupt request or clearing an error condition.

(cid:15) IO registers may have timing characteristics that are di(cid:11)erent from ordinary memory.
If a write to an IO register is expected to produce a visible change in that or some other
register, the device driver programmer may need to introduce a pause, for example
by executing nops, to give the device time to respond.

162

Chapter 4. Assembly-Level Architecture | Kernel Mode

(cid:15) A read to an IO register may not necessarily return the value that was last written
to it. This is because some bits do not exist (always zero or meaningless), and some
others do not store a value, but are only sensitive to the value conveyed to them
during a write operation. Sometimes the contents or meaning of a bit varies, based
on the contents of other IO registers. In some cases, a single IO register may serve
the dual purpose of being a command register as well as a status register. The device
driver writes to this IO register to send commands to the interface, and reads from
the same register to obtain status information. Depending on whether the register is
being read or written, the contents associated with the register are di(cid:11)erent 4 ! Finally,
an IO register may be updated by the IO device connected to the port.

The behavior of an IO register depends on the speci(cid:12)cs of the IO port to which it belongs.
The speci(cid:12)cations of an IO port include the characteristics of its IO registers. Often, it
is possible to program the behavior of IO registers by writing speci(cid:12)c commands in the
control/status registers of the same IO port. All of this depends on the port speci(cid:12)cations.
It is therefore important to study the device’s manual, and learn how it functions, before
writing device drivers that control that IO device.

Although the above discussion seems to imply that the grouping of IO registers into IO
ports is done at the time of OS development, that is rarely the case. The behavior of a
port is very much dependent on the IO device it models. At the time of writing an OS,
it is di(cid:14)cult to know which devices will be eventually connected to a particular computer
system that uses that OS. Therefore, in practice, the OS is split into two parts|the kernel
and the device drivers (or device handlers, or IO handlers, or software drivers). The kernel
consists of the parts that do not change from system to system. The device drivers are
speci(cid:12)c to each computer system, and are usually added on when each IO device is hooked
to the system.

Standard IO Ports: We will see later how this IO model is used for writing assem-
bly language device driver routines that can do speci(cid:12)c IO operations, such as reading a
character from a keyboard.

4.4 Operating System Organization

In modern computers, the operating system is the only software component that runs in the
Kernel mode. It behooves us therefore to consider the structure and implementation of an
operating system. In particular, it is important to see how the system calls speci(cid:12)ed in the
application programming interface (API) (provided to user programs and library functions)

4At the microarchitectural level, the IO interface module typically implements such a register by two
separate registers. It is reasonable to ask why the assembly-level architecture and the ISA de(cid:12)ne a single IO
register for dual functions. The reason is to conserve the IO address space, which was once a scare resource.

4.4. Operating System Organization

163

are implemented by operating systems. The exact internal details of an operating system
vary considerably from one system to another.
It is beyond the scope of this book to
discuss di(cid:11)erent possibilities. Figure 4.7 gives a possible block diagram, which is somewhat
similar to that of a standard UNIX kernel.
In the (cid:12)gure, the kernel mode software blocks
are shown shaded. The main components of the OS software include a system call layer,
(cid:12)le system, process control system, and the device management system (device drivers).
This organization uses a layered approach, which makes it easier to develop the OS and to
introduce modi(cid:12)cations at a later time. This also makes it easier to debug the OS code,
because the e(cid:11)ect of bugs may be restricted to a single layer. The (cid:12)gure also shows the
relationship of the OS to user programs, library routines, and the hardware.

HCI

User Mode
Software

ABI

OS Interface

Application Programs and Shell

Dynamically Linked Libraries
System Calls

System Call Layer

Device−
Independent

File System

Kernel Mode
Software

IO Management
Character−
Network−
Oriented
Oriented

Block −
Oriented

Process Control System

Inter−process
Communication

Scheduler

Memory
Management

User Mode
Instructions

Device−
Dependent

Device Drivers

OS Kernel

s
t
n
e
v
E
 
l
a
n
o
i
t
p
e
c
x
E

ISA

Kernel Mode

User Mode

Device Controllers

Hardware

IO Registers

IO Control Logic

IO Devices

Privileged Registers and Memory User Mode Registers and Memory
Control Unit, ALU, and Memory Controllers

Device Interrupts

Program Control Transfers Initiated by Software

Program Control Transfers Initiated by Hardware

Hardware Accessed/Controlled

Hardware Buses

Hardware Connections

Figure 4.7: Block Diagram Showing the Structure of a UNIX-like Operating System

164

Chapter 4. Assembly-Level Architecture | Kernel Mode

4.4.1 System Call Layer

The system call layer provides one or more entry points for servicing syscall instructions and
exceptions, and in some cases device interrupts also. The user program conveys the system
call type by placing the system call number on the user stack or in a register; the MIPS-I
assembly language convention is to use register $2 for this purpose. The system call layer
copies the arguments of the system call from the user stack (or registers) and saves the user
process’ context, possibly on the kernel stack. It then uses the system call number to look up
a system cal l dispatch vector to determine the kernel function to be called to implement that
particular system call, interrupt, or exception. It then calls that kernel function, sometimes
mapping or converting the arguments. When this kernel function completes, the system
call layer sets the return values and error status in the user stack (or registers), restores
the user process’ context, and switches to User Mode, transferring control back to the user
process.

We can summarize the functions performed by the system call layer:

(cid:15) Determine type of syscall

(cid:15) Save registers

(cid:15) Call appropriate hander

(cid:15) Restore registers

(cid:15) Return to user program

4.4.2 File System

The API provided to application programs by the operating system, as we saw earlier,
includes device-independent IO. That is, the interface is the same,
irrespective of the
physical device that is involved in the IO operation. The (cid:12)le abstraction part of the API
is supposed to hide all device-speci(cid:12)c aspects of (cid:12)le manipulation from HLL application
programmers, and provide them with an abstraction of a simple, uniform space of named
(cid:12)les. Thus, HLL application programmers can rely on a single set of (cid:12)le-manipulation OS
routines for (cid:12)le management (and IO device management in an indirect manner). This is
device-
independent
sometimes referred to as device-independent IO.
IO

As we saw in Section 3.4.7, application programs access IO (i.e., (cid:12)les) through read and
write system calls. The read and write system calls (of the User mode) are implemented
in the Kernel mode by the (cid:12)le system part of the OS, possibly with the help of appropriate
device drivers.

Files of a computer installation may be stored on a number of physical devices, such as
disk drives, CD-ROM drives, and magnetic tapes, each of which can store many (cid:12)les. If the
IO device is a storage device, such as a disk, the (cid:12)le can be read back later; if the device is

4.4. Operating System Organization

165

a non-storage device such as a printer or monitor, the (cid:12)le cannot be read back. Di(cid:11)erent
(cid:12)les may store di(cid:11)erent kinds of data, for example, a picture, a spreadsheet, or the text of
a book chapter. As far as the OS is concerned, a (cid:12)le is simply a sequence of bytes written
to an IO device.

The OS partitions each (cid:12)le into blocks of (cid:12)xed size. Each block in a (cid:12)le has an address
that uniquely tells where within the physical device the block is located. Data is moved
between main memory and secondary storage in units of a single block, so as to take
advantage of the physical characteristics of storage devices such as magnetic disks and
optical disks.

File management related system calls invoked by application programs are interpreted
by the (cid:12)le system part of the OS, and transformed into device-speci(cid:12)c commands. The
process of implementing the open system call thus involves locating the (cid:12)le on disk, and
bringing into main memory all of the information necessary to access it. The OS also
reserves for the (cid:12)le a bu(cid:11)er space in its memory space, of size equal to that of a block.
When an application program invokes a system call to write some bytes to a (cid:12)le, the (cid:12)le
system part of the OS writes the bytes in the bu(cid:11)er allotted for the (cid:12)le. When the bu(cid:11)er
becomes full, the (cid:12)le system copies it into a block in a storage device (by invoking the
device’s device driver); this block becomes the next block of the (cid:12)le. When the application
process invokes the close system call for closing a (cid:12)le, the (cid:12)le system writes the (cid:12)le’s bu(cid:11)er
as the (cid:12)nal block of the (cid:12)le, irrespective of whether the bu(cid:11)er is full or not, prior to closing
the (cid:12)le. Closing a (cid:12)le involves freeing up the table space used to hold information about
the (cid:12)le, and reclaiming the bu(cid:11)er space allotted for the (cid:12)le.

4.4.3 Device Management: Device Drivers

The device management part of the OS is usually implemented separate from the kernel,
to facilitate easy adding or removal of devices. It is actually a collection of device drivers.
Most computers have input/output devices and storage devices such as disks, terminals,
and printers. Each of these devices requires speci(cid:12)c device driver software, which acts
as an interface between the device controller and the (cid:12)le system part of the OS kernel.
A speci(cid:12)c device driver is important, because each device has its own speci(cid:12)c commands
instead of generic commands. Each device driver itself is a collection of routines, and can
have multiple entry points. The device driver receives generic commands from the OS (cid:12)le
system and converts them into the specialized commands for the device, and vice versa. To
the extent possible, the driver software hides the unique characteristics of a device from OS
(cid:12)le system.

A device driver, or a software driver is a speci(cid:12)c type of computer software, developed
to interact with hardware devices. This usually constitutes an interface for communicating
with the device, through the speci(cid:12)c computer bus or communications subsystem that the
hardware is connected to, providing commands to and/or receiving data from the device, and
on the other end, the requisite interfaces to the operating system and software applications.

166

Chapter 4. Assembly-Level Architecture | Kernel Mode

Because of its interfacing nature, it is speci(cid:12)c to the hardware device as well as to the
operating system.

The key design goal of device drivers is abstraction. Every model of hardware (even
within the same class of device) is di(cid:11)erent. Newer models also are released by manufac-
turers that provide more reliable or better performance and these newer models are often
controlled di(cid:11)erently.

The operating system cannot be expected to know how to control every device, both now
and in the future. To solve this problem, operating systems essentially dictate how every
type of device should be controlled. The function of the device driver is then to translate
these OS mandated function calls into device speci(cid:12)c calls. In theory a new device, which
is controlled in a new manner, should function correctly if a suitable driver is available.
This new driver will ensure that the device appears to operate as usual from the operating
systems’ point of view.

Device drivers can be fairly complex. Many parameters may need to be set prior to
starting a device controller, and many status bits may need to be checked after the comple-
tion of each device operation. Many device drivers such as the keyboard driver are supplied
as part of the pre-installed system software. Device drivers for other devices need to be
installed as and when these devices are installed.

The routines in a device driver can be grouped into three kinds, based on functionality:

(cid:15) Autocon(cid:12)guration and initialization routines

(cid:15) IO initiation routines

(cid:15) IO continuation routines (interrupt service routinestem Autocon(cid:12)guration and initial-
ization routines

(cid:15) IO initiation routines

(cid:15) IO continuation routines (interrupt service routines)

The autocon(cid:12)guration routines are called at system reboot time, to check if the corre-
sponding device controller is present, and to perform the required initialization. The IO
initiation routines are called by the OS (cid:12)le system or process control system in response to
system call requests from application programs. These routines check the device status, and
initiate IO requests by sending commands to the device controller. If program-controlled
IO transfer is used for the device, then the IO initiation routines perform the IO transfers
also. By contrast, if interrupt-driven IO transfer is used for the device, then the actual IO
transfer is done by the interrupt service routines when the device becomes ready and issues
an interrupt.

4.4. Operating System Organization

167

4.4.4 Process Control System

4.4.4.1 Multi-Tasking

When a computer system supports multi-tasking, each process sees a separate virtual ma-
chine, although the concurrent processes are sharing the same physical resources. Therefore,
some means must be provided to separate the virtual machines from each other at the phys-
ical level. The physical resources that are typically shared by the virtual machines are the
processor (including the registers, ALU, etc), the physical memory, and the IO interfaces.
Of these, the processor and the IO interfaces are typically time-shared between the pro-
cesses (temporal separation), and the physical memory is partitioned between the processes
(spatial separation)5 . To perform a context switch of the virtual machines, the time-shared
resources must be switched from one virtual machine to the next. This switching must be
managed in such a way that the virtual machines do not interact through any state infor-
mation that may be present in the physically shared resources. For example, the ISA-visible
registers must be saved and restored during a context switch so that the new context cannot
access the old context’s register state.

Decisions regarding time-sharing and space-sharing are taken in the Kernel mode by the
operating system, which is responsible for allocating the physical resources to the virtual
machines. If a user process is allowed to make this decision, then it could possibly encroach
into another process’ resources, and tamper with its execution. The operating system’s
decisions, however, need to be enforced when the system is in the User mode. This enforce-
ment is done using special hardware (microarchitectural) support so that the enforcement
activity does not reduce performance.

4.4.4.2 Multi-Programming

Some applications can be most conveniently programmed for two or more cooperating pro-
cesses running in parallel rather than for a single process. In order for several processes to
work together in parallel, certain new Kernel mode instructions are needed. Most modern
operating systems allow processes to be created and terminated dynamically. To take full
advantage of this feature to achieve parallel processing, a system call to create a new pro-
cess is needed. This system call may just make a clone of the caller, or it may allow the
creating process to specify the initial state of the new process, including its program, data,
and starting address. In some cases, the creating (parent) process maintains partial or even
complete control over the created (child) processes. To this end, Kernel mode instructions
are added for a parent to stop, restart, examine, and terminate its children.

5Time-sharing the entire physical memory is not feasible, because it necessitates saving the physical
memory contents during each context switch.

168

Chapter 4. Assembly-Level Architecture | Kernel Mode

4.5 System Call Layer for a MIPS-I OS

We just saw a functional organization of an operating system, and the important functions
performed by each ma jor block. To get a better appreciation of what the OS code looks
like, let us get our feet wet with a detailed real-life example. In this section, we discuss the
barebones of the system call layer of an OS for a MIPS-I machine. The system call layer
implements the interface for system calls, device interrupts, and exceptions. We restrict
ourselves to the system call layer for two reasons: (i) It is perhaps the smallest ma jor block
in an OS (and therefore manageable to be discussed in a book of this scope), but is detailed
enough re(cid:13)ect many of the idiosyncrasies that make OS routines di(cid:11)erent from application
programs. (ii) It is the block that directly interacts with application programs, which is
what many of the programmers care about.

4.5.1 MIPS-I Machine Speci(cid:12)cations for Exceptions

Software is always written for a speci(cid:12)c (abstract) machine speci(cid:12)cation. Before writing
assembly-level systems software for the system call layer of a MIPS-I OS, we need to know
how the MIPS-I machine speci(cid:12)es information regarding the occurrence of system calls,
device interrupts, and exceptions.
Interestingly, MIPS-I does not make a big distiction
between the 3 categories|system calls, device interrupts, and exceptions|when reporting
their occurrence. It treats them all as exceptions! To be speci(cid:12)c, it does 3 things when an
exceptional event occurs:

(cid:15) It modi(cid:12)es privileged register sr (status register) to disable device interrupts and
to re(cid:13)ect Kernel mode of operation.

(cid:15) It stores the restart instruction’s address in privileged register epc.

(cid:15) Generates an exception vector depending on the type of exception.

The MIPS-I architecture provides only 3 exception vectors, in contrast to many others that
provide a much larger set of exception vectors. These 3 vectors are stated below:

(cid:15) 0xbfc00000: This exception vector is speci(cid:12)ed at computer reset. Thus, after a reset,
the computer starts executing in the Kernel mode the program starting at memory
address 0xbfc00000.

(cid:15) 0x80000000: This exception vector is speci(cid:12)ed when a User TLB miss exception
occurs; Chapter 7 discusses this case in detail.

(cid:15) 0x80000080: This exception vector is speci(cid:12)ed when any other exceptional event
occurs.

4.5. System Call Layer for a MIPS-I OS

169

ExcCode Mnemonic Expansion
Interrupt
Int
0
1
Mod
TLB Modi(cid:12)cation Exception

2

3
4

5
6

7
8
9
10
11

12
13-15

TLBL

TLB Load Exception

TLBS
AdEL

AdES
IBE

DBE
Sys
Bp
RI
CpU

Ovf
(cid:0)

TLB Store Exception
Address Error Load Exception

Address Error Store Exception
Instruction Bus Error Exception

Data Bus Error Exception
Syscall
Breakpoint
Reserved Instruction Exception
Co-processor Unusable Exception

Over(cid:13)ow Exception
Reserved

Description
Device interrupt
Attempt to write to an address
marked as read-only
TLB miss for load
or instruction fetch
TLB miss for store
Address error for load
or instruction fetch
Address error for store
Bus error for
instruction fetch
Bus error for data reference
syscall instruction
break instruction
Illegal instruction
Software can emulate the
o(cid:11)ending instruction
Arithmetic over(cid:13)ow

Table 4.2: Exceptions Corresponding to Di(cid:11)erent Values of ExcCode Field of Cause register
in a MIPS-I Architecture

The third exception vector (0x80000080) corresponds to di(cid:11)erent types of exceptional
events, and so there must be some provision for the OS to know the exact cause of the
exceptional event whenever this exception vector is generated. The MIPS-I architecture
de(cid:12)nes a privileged register called Cause for recording the cause of the most recent excep-
tional event. This register has a 4-bit ExcCode (cid:12)eld, which holds an encoded bit pattern
corresponding to the exception cause. Thus, a maximum of 16 di(cid:11)erent exception types
can be uniquely speci(cid:12)ed by the machine in this (cid:12)eld. Table 4.2 shows the ExcCode values
for di(cid:11)erent types of exceptional events. Interestingly, a single value (0) corresponds to all
types of device interrupts; however, there is an IP (interrupt pending) (cid:12)eld in Cause which
helps to do some di(cid:11)erentiation between the interrupt sources. The OS can thus perform
selective polling (of appropriate IO registers) to determine the exact cause for the interrupt.
Similarly, a single value (8) corresponds to all types of syscall instructions; the OS can
determine the type of syscall by inspecting the contents of register $2.

170

Chapter 4. Assembly-Level Architecture | Kernel Mode

4.5.2 OS Usage of MIPS-I Architecture Speci(cid:12)cations

With the above background on the features provided in the MIPS-I architecture to support
exceptional events, let us turn our attention to the barebones of a typical system call layer
used in MIPS-I OSes. This interface code provides 3 entry points, corresponding to the 3
exception vectors: 0xbfc00000, 0x80000000, and 0x80000080. Figure 4.8 shows these 3
entry points and the placement of the exception handler code in the MIPS-I kernel address
space. The third entry point is common for a number of exceptional events, and so the
routine at that entry point checks the Cause register to determine what caused the event.
Depending on the contents of the ExcCode (cid:12)eld of Cause, an appropriate handler is called.
This checking is similar to that of implementing the C switch statement, and the standard
way for making this selection is by means of a jump table that contains the starting addresses
of the handler routines.

Kernel Address Space

Addresses
0x0000 0000

2 GB

User programs,
data

2 GB

0x8000 0000

0xffff fffc

UTLB Miss Handler

0x8000 0000

General Handler

0x8000 0080

Boot Program

0xbfc0 0000

Figure 4.8: Placement of the Exception Handler Code in the MIPS-I Kernel Address Space

In the skeleton code given below, the jump table containing handler addresses is called
handler table. It is placed in the kernel’s data section, and is initialized statically (i.e., at
assembly time as opposed to run time).

Let us take a closer look at the handlers themseleves, which are placed in the kernel’s
text section. The (cid:12)rst handler given here|the one starting at address 0x80000000|deals

4.5. System Call Layer for a MIPS-I OS

171

with user TLB miss exceptions, and is discussed in detail in Chapter 8. It is included here
just for the sake of completeness. The second handler, which starts at address 0x80000080,
deals with general exceptional events. The (cid:12)rst thing it does is to save the register values
of the interrupted process. In particular, registers s0-s7, k0, k1, sp, and epc need to be
saved. A good place to save them is the kernel stack 6 .
At the end of the handler code, we have a pair of instructions, jr $k1 and rfe, which
merit further discussion. The jr $k1 instruction speci(cid:12)es the transfer of control back to
the interrupted program. The rfe (restore from exception) instruction tells the machine to
restore the system’s mode and interrupt status to what it was prior to taking this exception;
thus, it puts the system back in the condition in which the interrupted program was running.
What is a good place to include the rfe instruction in the exception handler? If the rfe
instruction is placed before the jr instruction, then the machine may try to execute in user
mode the jr instruction, which is in the kernel code space. If, on the other hand, the rfe
instruction is placed after the jr instruction, then control is transfered to the interrupted
program, preventing the execution of the rfe instruction. What we really require is that
these two instructions must be executed atomical ly, meaning this two-instruction sequence
should be done together, without any interruptions 7 .

##################################################################################
# Handler Table
##################################################################################
# Store subsequent items in kernel data section
.kdata
.align 2
handler table:
.word
IntHandler
ModHandler
.word
TLBLHandler
.word
TLBSHandler
.word
.word
AdELHandler
AdESHandler
.word
IBEHandler
.word
DBEHandler
.word
.word
SysHandler

# Initialize to interrupt handler address
# Initialize to modi(cid:12)cation exception handler address
# Initialize to TLB load miss exception handler address
# Initialize to TLB store miss exception handler address
# Initialize to load address error exception handler address
# Initialize to store address error exception handler address
# Initialize to instruction bus error exception handler address
# Initialize to data bus error exception handler address
# Initialize to syscall handler address

6As with the user stack, most of the RISC machines do not provide direct support for a kernel stack
either. The kernel stack is therefore implemented within part of the memory address space. Like the user
stack, it is accessed by using the register-displacement addressing along with one of the general-purpose
registers (which serves as the kernel stack pointer). Therefore, the kernel cannot save onto the kernel stack
the value stored in this register by the interrupted program.
If the interrupted program was using this
register for its purposes, this presents a problem. The MIPS-I assembly language convention to solve this
dilemma is to reserve registers $k0 and $k1 (i.e., $26 and $27) for use by the OS.
7At the ISA level, the MIPS-I jr instruction uses the concept of delayed branching; i.e., the transfer of
control induced by the jr instruction takes e(cid:11)ect only after executing the immediately following instruction,
which in this case is the rfe instruction. The later versions of MIPS have a kernel mode eret (exception
return) instruction, which performs both actions in a single instruction.

172

Chapter 4. Assembly-Level Architecture | Kernel Mode

.word
.word
.word
.word

BpHandler
RIHandler
CpUHandler
OvHandler

# Initialize to breakpoint handler address
# Initialize to reserved instruction exception handler address
# Initialize to co-processor unusable exception handler address
# Initialize to arithmetic over(cid:13)ow exception handler address

mfc0

$k1, $epc

##################################################################################
# User TLB Miss Handler
##################################################################################
# Store subsequent items in kernel text section
.ktext 0x80000000
# starting at address 0x80000000
UTLBMiss Handler:
# Copy context register contents (i.e., kseg2 virtual
mfc0
$k0, $context
# address of required user PTE) into GPR k0
# Copy epc contents (address of faulting instruction)
# into GPR k1
# Load user PTE from kseg2 addr to GPR k0
# This load can cause a TLBMISS exception!
# Copy the loaded PTE into EntryLo register
# Write the PTE in EntryLo register into TLB
# at slot number speci(cid:12)ed in Random register
# Jump to address of faulting instruction
# Switch to user mode

$k0, $EntryLo

mtc0
tlbwr

lw

$k0, 0($k0)

jr
rfe

$k1

##################################################################################
# General Handler
##################################################################################
# Store subsequent items in kernel text section
.ktext 0x80000080
# starting at address 0x80000080
General Handler:

##################################################################################
# Save interrupted process’ s0 - s7 registers, k0, k1, sp, and epc on kernel stack
...
...

##################################################################################
# Determine cause for coming here, and jump to appropriate handler
# Copy Cause register to R26
$k0, $cause
mfc0
# Take out the ExcCode value
$k0, $k0, 0x3c
andi
$k0, handler table($k0)# Get starting address of appropriate handler
lw
# Call appropriate handler
$k0
jalr

##################################################################################
# Returned from handler
# Restore interrupted process’ s0 - s7 registers, k0, k1, sp, and epc
from kernel stack
#
...
...

##################################################################################
# Return to interrupted program
# Copy epc register to R26
mfc0
$k0, $epc

4.6.

IO Schemes Employed by Device Management System

173

jr
rfe

$k0

# Return to interrupted program
# Restore from exception

4.6

IO Schemes Employed by Device Management System

This section gives an overview of the device management part of a typical OS. A complete
treatment of the internal operation of the device management system of even a single OS
is beyond the scope of this book.

As can be imagined, there is a wide disparity in the nature of the various abstract IO
devices. Di(cid:11)erent schemes are available to accommodate this disparaging di(cid:11)erences in
speed and behavior. They are:

(cid:15) sampling

(cid:15) program-control led IO

(cid:15) interrupt-driven IO

(cid:15) direct memory access (DMA)

(cid:15) IO co-processing

As we go from the (cid:12)rst scheme to the last, more functionality is shifted from the device
driver to the IO interface. The scheme used by a device driver to perform IO transfers is,
of course, transparent to the user program, because it is not speci(cid:12)ed in the API. We shall
discuss each of these schemes in detail.

Central to all these schemes is the need to do appropriate synchronization between the
device driver and the device. IO devices tend to be signi(cid:12)cantly slower than the processor.
Moreover, their response times tend to have a wide variance. Because of these reasons, no
assumptions can be made about the response times of IO devices. The di(cid:11)erent IO transfer
schemes also di(cid:11)er in how synchronization is done between the device driver and the device.

4.6.1 Sampling-Based IO

This is the simplest of the IO data transfer methods.
In this scheme, the IO device is
treated like main memory; that is, the device is always ready to accept or to provide data,
as appropriate. No checking of device status is required. The programming interface is
extremely simple, consisting of just one or more data registers. Example IO devices that
can be communicated in this manner are simple devices such as digital input port and motor
port.

174

Chapter 4. Assembly-Level Architecture | Kernel Mode

4.6.2 Program-Controlled IO

With program-controlled IO, the device driver being executed on the machine has direct
control of the IO operation, including sensing the device status, sending a read or write
command, and transferring the data. Sensing the device status may involve executing
an IO read instruction to read the device controller’s status register into a general purpose
register (GPR), and then checking the appropriate bits of the GPR. This process of checking
the status by continuous interrogation of the status register associated with the device is
performed until the IO device becomes ready8 . For example, the status register of the DEC
LP11 line printer interface contains a done bit, which is set by the printer controller when
it has printed a character, and an error bit, which indicates if the printer is jammed or out
of paper. The bytes to be printed are copied from a general-purpose register to the data
register of the printer interface (by means of IO write instructions), one at a time. Each
byte is copied only after ensuring that the done bit has been set. The error bit is also
checked every time to determine if a problem has occurred in the printer.

A (cid:13)owchart of the device driver that carries out program-controlled IO is shown in
Figure 4.9. The (cid:13)owchart assumes that a sequence of words has to be read from an IO
device and stored in main memory. The device driver continually examines the status of
the device interface until the appropriate (cid:13)ag becomes 1. A word is then brought into a
GPR, and transferred to main memory. The entire process is repeated until all of the data
have been transferred.

Example: To review the basic concepts of program-controlled IO, consider the IO operations
involved in reading a character from the keyboard and copying it to a memory location. For
an application program, the convenient way to read a character is by invoking the operating
system (by means of a syscall instruction). A MIPS-I assembly language application pro-
gram for doing this IO operation is given below. Notice that the execution of the syscall
instruction causes the system to switch to kernel mode, and execute many instructions in
the kernel mode before returning to the user program.

.text
li
la
li
li
syscall

# Place the (cid:12)le descriptor for keyboard (0) in $a0
$a0, 0
$a1, buffer # Place the starting address of buffer in $a1
# Place the number of bytes to be read (i.e., 1) in $a2
$a2, 1
$v0, read code # Place the code for read in $v0
# Call OS routine to perform the read

Now, assume that the keyboard’s interface sets the LSB of its 8-bit status register to 1
whenever a new character is typed by the user. The device driver resets it to 0 after reading
that character. The display’s interface resets the next-to-LSB of its 8-bit status register to

8Because IO devices are slow, it is important to check their status prior to issuing a new command. For
instance, when a new command is sent to a device, the device may not be ready to accept it because it is
still working on the previous command. This situation can be recognized by checking its status.

4.6.

IO Schemes Employed by Device Management System

175

Start

Read status register
into a GPR

Busy wait

Check relevant status bit
in GPR

No

Is device ready?

Yes

Read data register
into a GPR

Copy data from GPR
to Memory

Yes

More
transfer?

No

Stop

Figure 4.9: A Flowchart Depicting the Working of a Device Driver that Reads Data from
an IO Device using Program-Controlled IO Transfer

0 whenever the display is ready to accept a new character. The device driver sets it to 1
when it writes a character to the display interface’s data register. Figure 4.10 illustrates
the interface provided by the IO devices, along with the speci(cid:12)c addresses given to the IO
registers.

Let us write an assembly language device driver routine that will be called when the
above syscall instruction is executed. First of all, when the syscall instruction is exe-
cuted, the execution mode switches to Kernel mode, and control passes over to the system
call layer part of the operating system. This routine saves the User process’ registers
and other relevant state on the Kernel stack. It then calls the SysHandler routine, which
subsequently calls the keyboard device driver. Below, we present an example code for a
keyboard device driver that reads from the keyboard up to as many characters as speci-
(cid:12)ed in register $a2. If $a2 contains zero or a negative number, then no character is read.
Similarly, reading stops when the end of line is reached, which is detected by checking the
read character against 0n0 . The number of characters actually read is placed in register $v0.
The read characters are placed in consecutive memory locations, starting from the address

176

Chapter 4. Assembly-Level Architecture | Kernel Mode

Interconenct

Keyboard Status

Monitor Status

CPU

MM

0xA0000000
Status
Data
0xA0000001
Keyboard Interface

0xA0000002
Status
Data
0xA0000003
Graphics Interface

0xA0000002
Status
Data
0xA0000003
Disk Interface

Figure 4.10: A Flowchart for an Assembly Language Device Driver to Read Data from an
IO Device using Program-Controlled IO Transfer

originally speci(cid:12)ed in register $a1. Notice that this simple device driver does not check for
error conditions, one of the tasks a real device driver must do.

##########################################################################
# Keyboard Read Device Driver: Called by OS File System
# $a1 contains address of buffer; $a2 contains number of bytes to read
##########################################################################

.ktext
keybd read:
li
blez
read loop:
lb
andi
beqz
lb
sb
andi
sb
addu
beq
addu
subu
bnez
read done:
jr

# Store subsequent items in kernel text section

$v0, 0
$a2, read done

# Initialize number of bytes read ($v0) to 0
# Go to label read done if no byte is to be read

# Read keyboard status register into R9
$9, keybd status
# Isolate the status bit for keyboard input
$10, $9, 1
# Branch back to label read loop if status bit is not set
$10, read loop
# Read the byte from keyboard data register
$11, keybd data
# Store the byte in the bu(cid:11)er
$11, 0($a1)
# Reset keyboard input status bit (LSB) to 0
$12, $9, 0xFE
$12, keybd status # Update keyboard status register
# Increment number of bytes read
$v0, 1
$11, ’nn’, read done# Go to label read done if end of line
# Increment address of bu(cid:11)er to store next byte
$a1, 1
# Decrement number of bytes to be read
$a2, 1
# Go to read loop if there are more bytes to be read
$a2, read loop

$ra

# Return control to OS (cid:12)le system

4.6.

IO Schemes Employed by Device Management System

177

The read loop in the above device driver constantly checks the keyboard status until the
next character has been typed. This type of continuous checking of the device status bits
to see if it is ready for the next IO operation is a hallmark of program-controlled IO. The
device controller places the information in one of its status registers, and the device driver
gets this information by reading this register. The device driver is in complete control of
the IO transfer.

The disadvantage of program-controlled IO is that it can waste a lot of time executing
the driver because the IO devices are generally slower compared to the other parts of the
system. The driver code may read the device status register millions of times only to (cid:12)nd
that the IO device is still working on a previous command, or that no new character has
been typed on the keyboard since the last time it was polled. For output devices, it has to
do this status checking until the output operation is completed to ensure that the operation
was successful. Ideally, it is preferable to execute a device driver only when a device is ready
for a new transfer and a transfer is required. For instance, the keyboard driver needs to be
executed only when a character is available in the input bu(cid:11)er of the keyboard interface.
Similarly, the printer driver needs to be executed only when the printer has completed
a previous print command, or is ready to accept a new command and a print request is
pending.

4.6.3

Interrupt-Driven IO

The overhead in program-controlled IO transfer was recognized long ago, leading to the use
of interrupt-driven IO for at least some of the peripheral devices. An interrupt is generated
by an IO port to signal that a status change has occurred. With interrupt-driven IO,
device driver routines need not be continuously executed to check the status of IO ports;
instead other useful programs can be executed until the IO device becomes ready. Indeed,
by using interrupts, such waiting periods can almost be eliminated, provided the system
has a su(cid:14)cient degree of multiprogramming.

When an interrupt is generated, the system acknowledges it when it is ready to pro-
cess the interrupt. At that time, the system enters the Kernel execution mode, and the
appropriate interrupt service routine (ISR) or interrupt handler is called with one or more
parameters that uniquely identify the interrupting device. The parameters are important,
because a single ISR may handle multiple devices of the same type. If the identity of the in-
terrupting device is not supplied, the ISR may need to poll all potential IO ports to identify
the interrupting one.

Data transfer by means of interrupts is illustrated in Figure 4.11 as a time line. Time
is depicted along the horizontal direction. Process A executes a read system call at time
T1. Control is then transferred to the syscall interface routine of the OS, from where
control is transfered to the (cid:12)le system, and then to the appropriate device driver. After the
device driver updates the relevant status information, control is transferred to the process
control system, which then allows process B to run (at time T2). The device becomes

178

Chapter 4. Assembly-Level Architecture | Kernel Mode

ready at time T3, and generates an interrupt. Control is transfered to the OS, which then
calls the ISR for the device. After the ISR completes, the process control system allows
process A to continue. Notice that during the time taken by the device to become ready
(T2 to T3), the system is utilized by executing process B.

l
l
a
c
 
m
e
t
s
y
s
 
d
a
e
r

Process A

Process B

OS Kernel

Device Driver

t
p
u
r
r
e
t
n
i
 
e
c
i
v
e
d

(ISR)

T1

T2

T3

T4

Time

Figure 4.11: A Timeline Depicting an Interrupt-driven IO Transfer on behalf of Process A

Example: The keyboard interface raises an interrupt when a character is typed on the
keyboard. When the interrupt is accepted, control goes to the system call layer of the OS,
which saves the interrupted process’ state and registers. It then calls the interrupt handler
routine, which subsequently calls the ISR part of the keyboard device driver. Below, we
present the ISR part of the keyboard device driver to read in a character from the keyboard
interface to memory location buffer. Again, we assume that the status register address is
named keybd status and the data register address is named keybd data.

# Store subsequent items in kernel text section
.ktext
# Update stack pointer ($sp)
keybd isr: subu $sp, 12
# Save value of R8 in stack before overwriting it
sw
$8, 0($sp)
# Copy processor status register to R8
mfc0 $8, C12
$8, $8, 0xFFFFXXFF # Code to disable lower priority interrupts
and
# Copy R8 to processor status register
mtc0 C12, $8
# Save value of R9 in stack before overwriting it
$9, 4($sp)
sw
# Save value of R10 in stack
sw
$10, 8($sp)
# Read keyboard interface’s data register
$9, keybd data
lb
# Store the byte in the bu(cid:11)er
sb
$9, buffer

4.6.

IO Schemes Employed by Device Management System

179

$9, keybd status # Read keyboard interface’s status register
lb
# Reset keyboard’s status bit (LSB) to 0
$9, $9, 0xFE
and
$9, keybd status # Update keyboard interface’s status register
sb
# Restore value of R9 from stack
$9, 4($sp)
lw
# Restore value of R10 from stack
lw
$10, 8($sp)
# Copy processor status register to R8
mfc0 $8, C12
# Code to enable lower priority interrupts
or
$8, $8, 0xXX00
# Copy R8 to processor status register
mtc0 C12, $8
# Restore value of R8 from stack
lw
$8, 0($sp)
# Restore stack pointer ($sp)
addu $sp, 12
# Return control to calling routine
jr
$ra

A number of actions are involved in processing an interrupt. Figure 4.12 clearly depicts
these actions.

Hardware

Software

I/O interface module
issues interrupt

Finish execution of
current instruction

Disable lower priority
interrupts;
Save registers

Acknowledge
interrupt

Save
return address

Service I/O device

Restore registers;
Enable lower priority
interrupts

Execute a CALL
to appropriate ISR

RETURN

Figure 4.12: A Flowchart Depicting the Steps Involved in Processing an Interrupt

Handling Multiple Interrupts

We now consider a computer system in which several interfaces are capable of raising
interrupts. Because these interfaces operate independently, there may not be a speci(cid:12)c
order in which they generate interrupts. For example, the keyboard interface may generate
an interrupt while an interrupt caused by a printer interface is being serviced, or all device
interfaces may generate interrupts at exactly the same time, in a pathological situation!

180

Chapter 4. Assembly-Level Architecture | Kernel Mode

The presence of multiple interrupt-raising device interfaces raises several questions:

1. How can the system identify the IO port that generated an interrupt?

2. Because di(cid:11)erent devices probably require di(cid:11)erent ISRs, how to obtain the starting
address of the appropriate ISR?

3. Should an ISR for one device be allowed to be interrupted (by another IO port)?

4. What should be done if multiple device interfaces generate interrupts at the same
time?

The ways in which these problems are resolved vary considerably from one machine to
another. The approach taken in any machine is an important consideration in determining
the machine’s suitability for speci(cid:12)c real-time applications. We shall discuss some of the
common techniques here.

Device Identi(cid:12)cation: Consider the case in which a device interface requests an in-
terrupt by activating an Interrupt Request line that is common to all device interfaces.
When a request is received over the common Interrupt Request line, additional infor-
mation is needed to identify the particular device that needs attention. One option is to
set appropriate bits in the status register of the interrupting device interface, so that the
interrupting device can be identi(cid:12)ed by pol ling. A better option is to use a scheme called
vectored interrupts, in which the interrupting device’s identi(cid:12)cation is sent as a special code
along with the interrupt.

Prioritizing Interrupts: Now let us consider the situation in which several interrupts
occur simultaneously. In this case, the system must decide which device to service (cid:12)rst.
A priority interrupt system establishes a priority over the various interrupt sources to de-
termine which interrupt request to service (cid:12)rst when two or more arrive simultaneously.
The system may also determine which interrupts are permitted to interrupt an ISR. Higher
levels of priority are assigned to requests that, if delayed or interrupted, could have serious
consequences. Devices with high-speed transfers such as magnetic disks are given high pri-
ority, and slow devices such as keyboards receive the lowest priority. When two devices raise
interrupt at the same time, the computer services the device with the higher priority (cid:12)rst.
Establishing the priority of simultaneous interrupts can be done by software or hardware.
Software uses a polling procedure to identify the interrupt source of higher priority. In this
method, there is one common routine to which the control is transferred upon receiving
an interrupt. This routine performs the polling of the interrupt sources in the order of
their priority. In the hardware priority scheme, the priority is implemented by some kind
of hardware.

4.6.

IO Schemes Employed by Device Management System

181

4.6.4 Direct Memory Access (DMA)

Both program-controlled IO and interrupt-driven IO are particularly suited for interacting
with low-bandwidth IO devices. Both schemes place the burden of moving data and manag-
ing the transfer on a device driver or OS ISR, which does so with the help of IO read/write
instructions such as

R1, data register
lw
Of course, an instruction to transfer input or output data is executed only after executing
other instructions that con(cid:12)rm that the IO interface is indeed ready for the transfer. In
either case, considerable overhead is incurred, because several instructions must be exe-
cuted to transfer each data word. The reason is that instructions are needed for performing
tasks such as incrementing the memory address and keeping track of the word count. This
overhead becomes intolerable for performing high-bandwidth transfers such as disk IO. For
such high-bandwidth devices, most of the transfers involve large blocks of data (hundreds to
thousands of bytes). So computer architects invented a mechanism for o(cid:11)-loading the device
driver, and letting the IO interface directly transfer data to or from main memory with-
out involving the device driver. This kind of transfer is called direct memory access (DMA).

Steps Involved in Setting Up a DMA Transfer:

Although the DMA mode of data transfer happens without interrupting the program
being executed, its operation is still initiated by a device driver routine. To initiate the
transfer of a block of words, instructions are executed by the device driver routine to convey
the following information to the DMA interface (IO interface capable of performing DMA
transfers):

(cid:15) the address or identity of the IO device

(cid:15) the starting address in the main memory

(cid:15) the number of words to be transferred

(cid:15) the direction of transfer (read/write)

(cid:15) the command to initiate the DMA operation

Once this information is conveyed to the DMA interface, the device driver stops execution,
and another program is executed. When the entire block has been transferred, the DMA
interface raises an interrupt. An interrupt service routine is then executed to interrogate
the DMA interface to verify that the entire operation indeed completed successfully. Figure
4.13 depicts the actions involved in a DMA transfer as a (cid:13)owchart.

182

Chapter 4. Assembly-Level Architecture | Kernel Mode

Send information to
DMA Interface

Device
Driver

Is IO device ready?

No

Yes

Transfer next word
to/from memory

Increment memory address

No

Are all words
transferred?

Yes

Raise interrupt to indicate
end of transfer

DMA
Interface

Execute ISR to check
if transfer was successful

Device
Driver

Figure 4.13: A Flowchart Depicting the Steps Involved in a DMA Transfer

4.6.5

IO Co-processing

The DMA scheme o(cid:11)ers signi(cid:12)cant performance improvements over interrupt-driven IO
whenever large blocks of data are to be transferred. Is it possible to do even better? To
answer this question, we must (cid:12)rst look at the limitations of the DMA scheme. First, each
DMA operation has to be initiated by a device driver, by executing several instructions.
Second, a DMA interface can handle only a single DMA operation at a time. To overcome
these limitations, we can use a scheme called IO co-processing, in which (complex) IO func-
tions are implemented by IO routines, which are executed (by IO processors or IO channels
or peripheral processing units (PPUs)9 ) in parallel with normal execution of programs (by
the main processor). The IO routines are also typically placed in the memory address space.
The execution of an IO routine is triggered by the device driver as in the case of a DMA

9An IO channel is itself a computer, albeit simple, which is capable of executing a small set of general-
purpose instructions along with many special-purpose IO instructions such as \read a track from the disk"
or \skip forward two blocks on the tape." More sophisticated IO processors such as a peripheral processing
unit (PPU) can do even more sophisticated IO functions. Similarly, a modern graphics processing unit
(GPU) does complex graphics functions such as rendering, and hidden surface calculation.

4.7. Concluding Remarks

183

operation.

4.6.5.1 Graphics Co-Processing

4.6.6 Wrap Up

As we can see, there is a trade-o(cid:11) between IO interface complexity and performance. When
we use a simple, inexpensive device interface, most of the functionality is implemented
in software (its device driver), and hence its performance will be low. A more expensive
interface, on the other hand, performs more functions in hardware, and is therefore faster.

4.7 Concluding Remarks

\Anybody who real ly knows computers spends a certain amount of time helping out
friends who have loaded up their computers with trash, viruses, and spyware."
| Howard Gilbert in Introduction to PC Hardware

4.8 Exercises

1. Consider a situation in which an OS routine calls a user mode library routine. What
mode | user mode or kernel mode | will the library routine execute?

2. Explain the role played by the system cal l layer in a MIPS-I operating system.

3. Explain why interrupts are disabled when handling an exception.

4. Consider a simple embedded computer with two IO devices|a transmitter and a
receiver. Describe the protocol Write a MIPS-I assembly language program (kernel
mode) to accept data from the receiver and send them to the transmitter.

5. Consider a computer system that supports only a single mode of operation, i.e., it does
not provide a separate user mode and a kernel mode. What would be the shortcomings
of such a computer system?

6. For the computer system mentioned in the previous problem, explain how the hard-
ware can provide some minimal functions traditionally carried out by the operating
system, such as resource allocation among the processes.

7. What does a computer system do when two IO devices simultaneously raise interrupts?
How does the computer recognize the identity of the devices?

184

Chapter 4. Assembly-Level Architecture | Kernel Mode

8. Explain the di(cid:11)erences between hardware interrupts, software interrupts, and excep-
tions. What do they have in common? Give an example of each.

9. A device driver programmer uses the following (cid:13)ow-chart for performing program-
controlled reads from a keyboard. Explain what could go wrong with this scheme.
Give the correct (cid:13)ow-chart.

Start

Read status register

Read data register

Transfer data to memory

Yes

More
transfer?

No

Stop

Chapter 5

Instruction Set Architecture (ISA)

Whoever loves instruction loves know ledge.

Proverbs 12: 1

The central theme of this book is to view a modern computer as a series of architectural
abstractions, each one implementing the one above it. We have already seen the high-level
architecture and the two distinct modes of the assembly-level architecture. This chapter
discusses the architectural abstraction immediately below the assembly-level architecture,
called the instruction set architecture (ISA). In principle, this architecture is de(cid:12)ned by how
the computer appears to a machine language programmer. We can also view it as a set of
rules that describe the logical function of a computer as observable by a (machine language)
program running on that machine. Historically, this architecture came into existence before
any of the other architectures we saw so far. The ISA has a special signi(cid:12)cance that makes it
important for system architects: it is the interface between the software and the hardware.
This architecture serves as the boundary between the hardware and the software in modern
computer systems, and thus provides a functional speci(cid:12)cation for the hardware part of a
computer system.

In the early days of computers, all of the programming used to be done in machine lan-
guage! This was found to be so tedious that computer scientists quickly invented assembly
languages, which used symbols and notations that were closer to the way people think and
communicate. Once assembly languages were introduced as a symbolic representation for
machine languages, programmers virtually stopped using machine languages to program.
And, in theory, we can build hardware units that directly interpret assembly language pro-
grams. It is reasonable to ask why we still maintain a separate virtual machine level at the
ISA level. The reason is that hardware that directly interprets assembly language programs
will be signi(cid:12)cantly more complex than one that directly interprets machine language pro-
grams. The main di(cid:14)culty arises from the use of labels and macros in assembly languages.

185

186

Chapter 5.

Instruction Set Architecture (ISA)

By de(cid:12)ning a lower-level abstraction at the ISA level, software in the form of assemblers
can be used to translate assembly language programs to machine language programs, which
can be more easily interpreted by hardware circuits.

The ISA does not specify the details of exactly how the hardware performs its functions;
it only speci(cid:12)es the hardware’s functionality. Thus, an ISA speci(cid:12)es general-purpose regis-
ters, special registers, a memory address space, and an instruction set 1 . The ISA provides a
level of abstraction that allows the same (machine language) program to be run on a family
of computers having di(cid:11)erent implementations (i.e., microarchitectures). An example is the
x86 ISA which is implemented in the 80386, 80486, Pentium, Pentium II, Pentium III, and
Pentium 4 processors by Intel Corporation, and in the K6 and Athlon processors by AMD.

5.1 Overview of Instruction Set Architecture

We shall begin this chapter with an overview of the basic traits of an instruction set archi-
tecture. The assembly-level architecture that we covered in the last two chapters is nothing
but a symbolic representation of the ISA. Many aspects of the ISA are therefore very similar
to that of the assembly-level architecture. In fact, both have very similar memory models,
register models, and IO models. The main di(cid:11)erences between the two abstraction levels
are in the two areas of language and instruction set. Let us take a detailed look into both
of these issues.

5.1.1 Machine Language

The language used to specify programs that are targeted to an ISA has been historically
called the machine language. The most notable feature of machine languages is their alpha-
bet, which is restricted to just two symbols (0 and 1). Apart from allowing only an extremely
frugal set of symbols for the language (as opposed to the richer set of alphanumeric charac-
ters available for an assembly language), the ISA also bans many of the luxuries permitted
by the assembly-level architecture, such as labels, macros, AL-speci(cid:12)c instructions, and
AL-speci(cid:12)c addressing modes.

5.1.1.1 Language Alphabet

\‘There are 10 types of people in this world |
those who understand binary, and those who don’t."
| J. Kincaid and P. Lewis

1 Issues related to implementation of the ISA (such as cache memory, ALUs, and the type of internal
connections) do not form part of the ISA. The entire point of de(cid:12)ning an ISA is to insulate the machine-
level programmer (and the assembler) from those details.

5.1. Overview of Instruction Set Architecture

187

A machine language alphabet has only two symbols: f0, 1g. This means that all of
the information in a machine language program|instructions as well as data|have to be
expressed using sequences of just 0s and 1s! The earliest programmers, who programmed
computers at the ISA level, did precisely this. Thus, opcodes (such as add and and) and
operands (such as $t0 and $sp) are all speci(cid:12)ed in bit patterns rather than in a natural
language-oriented symbolic form. A machine language program is therefore far less read-
able than an equivalent assembly language program. In addition, assembly languages permit
programmers to use labels to identify speci(cid:12)c memory addresses that hold data or form the
target of branch instructions. Assembly languages also provide a variety of other conve-
nience features that make programs at this level shorter and easier to write. For example,
data layout directives allow a programmer to describe data in a more concise and natural
manner than its binary representation. Similarly, macros and pseudoinstructions enable a
sequence of instructions to be represented concisely by a single macro or pseudoinstruction.

Under these circumstances, why would anyone other than ISA designers want to study
machine language? Certainly, not for writing application programs! The compelling reason
is that besides application programs, there are a lot of support programs such as compilers,
assemblers, disassemblers, and debuggers. The writers of these programs need to know the
ISA details. Microarchitecture designers also need to know the machine language to design
the hardware for executing machine language programs.

Information Representation with Bit Patterns: A machine language needs to rep-
resent two kinds of information: instructions and data. An assembly language had a large
set of symbols|alphanumeric characters and punctuation marks|to represent information.
By contrast, the alphabet used by a machine language for representing information has only
2 symbols|0 and 1. Thus, any kind of information can be represented at this abstraction
level only by bit patterns obtained by concatenating bits. The same sequence of bits can
have di(cid:11)erent meanings; the correct meaning depends on:

(cid:15) the instruction being executed and

(cid:15) the number systems and coding schemes adopted by the ISA designers

For example, consider the bit pattern 0100 1110 0111 0010 0000 0000 0000 0000 (i.e.,
4E320000H). If this is interpreted as an integer, the value is 2:106 (cid:2) 10 10 . If it is interpreted
as an ANSI/IEEE 754 (cid:13)oating-point number, the value is 1:015 (cid:2) 10 9 . If it is interpreted
as part of an ASCII character string, then it indicates the 4 characters N, r, Null, Null. If
it is interpreted as a Motorola 68000 instruction, then it indicates the STOP instruction if
the system is in the Kernel mode, and a syscall instruction, otherwise. What this shows
is that a bit pattern does not have an inherent meaning; we must know the context as
well as the rules concerning the interpretation of those bits. The rules for interpretating a
bit pattern are established at ISA design time, and will be e(cid:11)ective throughout the life of
the system. The assumptions about bit meaning will impact the design of the lower-level

188

Chapter 5.

Instruction Set Architecture (ISA)

hardware that manipulates the bits. The choice of a number system is based on available
hardware technology, desired range of values, and other system goals.

The ISA speci(cid:12)es exactly how di(cid:11)erent instruction types and data types are encoded as
bit patterns2 .

\A word aptly spoken is like apples of gold in settings of silver"
| Proverbs 25: 11

5.1.1.2 Syntax

A machine language program starts with a header, which provides a description of the
various sections that are present in the program and are mapped to various portions of
the memory address space. This description includes the name of the section, its size, its
location within the program (cid:12)le, the portion of memory address space it maps to, etc.
Table 5.1 lists the commonly found sections in machine language programs. Some of these
sections|.rdata, .text, .sdata, and .data|are de(cid:12)ned in the assembly language also,
as we already saw in the previous two chapters. The rest|.bss, .sbss, .lit8, .lit4, and
.comments|are limited to machine languages, and are created by the assemblers.

Section Name Explanation
Read-only data
.rdata
Code
.text
Uninitialized writable data
.bss
Uninitialized writable small data
.sbss
64-bit (cid:13)oating-point constants
.lit8
32-bit (cid:13)oating-point constants
.lit4
Writable small data
.sdata
Writable data
.data
Comments
.comment

Table 5.1: Typical Sections in a Machine Language Program

Unlike the case with assembly languages, each of the sections that are present appears
exactly once. Within each section, the bit patterns corresponding to contiguous memory
locations are stored contiguously. This is pictorially depicted in Figure 5.1. The order of
the sections within the machine language program is not critical.

2Some of this encoding information|in particular, the encoding of data types|is known to the assembly
language programmer too.

5.1. Overview of Instruction Set Architecture

189

Header .rdata

.text

.data

.lit8 .lit4 .sdata .sbss .comment

Figure 5.1: Organization of Header and Various Sections in a Machine Language Program

5.1.1.3 ELF Format

5.1.1.4 Portable Executable File Format

5.1.2 Register, Memory, and IO Models

To produce machine language code, machine language programmers and the developers of
assemblers and disassemblers need to know the speci(cid:12)cations of the instruction set archi-
tecture. These attributes include the memory model, the register model, the IO model, the
data types, and the instruction types. The collection of all this information is what de(cid:12)nes
the instruction set architecture.

The register model supported by an ISA is the same as that supported by the assembly-
level architecture for that computer. On a similar note, the ISA’s memory model is also
virtually the same as that supported by the assembly-level architecture. The only di(cid:11)erence
is that the ISA does not permit labels to be used to specify memory locations.

5.1.3 Data Types and Formats

A picture is worth 1,000 words, especially in memory requirements. And a video is worth
a million words!

5.1.4

Instruction Types and Formats

Perhaps the biggest di(cid:11)erence in the two architectures is in the area of the instruction set.
The instruction set supported by the ISA is generally a subset of that supported by the
assembly-level architecture. While the assembly-level instruction set is designed to facilitate
assembly language programming, the machine-level instruction set is designed to facilitate
lower-level implementations. Thus, there is a noticeable semantic gap between an assembly-
level architecture and its corresponding ISA. This is especially the case for RISC (reduced
instruction set computer) ISAs, as we will see later in this chapter.

In the case of the MIPS-I, the assembly-level instruction set has several opcodes that
permit 32-bit immediate operands. Examples are li and lw.
In order to encode these
instructions using bit patterns, we need more than 32 bits. However, the MIPS-I ISA
designers wanted (cid:12)xed length instruction formats in which all instructions are encoded
with 32 bits. Therefore, li is not included in the MIPS-I ISA and lw is allowed to have

190

Chapter 5.

Instruction Set Architecture (ISA)

only a 16-bit immediate operand. This begs the question of how we get a 32-bit data value
into a register, at the ISA level. The standard method is to split the 32-bit data value into
2 parts|a 16-bit upper half and a 16-bit lower half|and use a lui-ori sequence to load
the two halves.

Like the load instructions, the branch instructions are also limited to specifying a 16-bit
immediate value at the ISA level. As 16 bits are not su(cid:14)cient by themselves to specify
the branch target address, the solution adopted is to consider the 16-bit immediate value
as the instruction o(cid:11)set from the branch instruction. Thus, when executing the ISA-level
instruction represented symbolically as
beq $1, $2, 8
8 instructions will be skipped if the contents of registers $1 and $2 are equal.

5.2 Example Instruction Set Architecture: MIPS-I

5.2.1 Register, Memory, and IO Models

5.2.2 Data Types and Formats

5.2.3

Instruction Types and Formats

The designers of the MIPS-I ISA decided to use a (cid:12)xed length instruction format, so as
to make it easier for the lower level machine (microarchitecture) to quickly determine in-
struction boundaries to facilitate the overlapped execution of multiple instructions. Every
instruction is encoded in exactly 32 bits. In the MIPS-I ISA, the opcode implicitly speci(cid:12)es
the addressing modes for each of the operands, and therefore no addressing mode infor-
mation is explicitly recorded in the instruction bits. Considering all of the instructions
in the MIPS-I ISA, we have the following (cid:12)elds: opcode, rs, rt, rd, offset/immediate,
and target. Based on the (cid:12)elds used, we can classify the MIPS-I ISA instructions into 3
categories:

(cid:15) Instructions that specify an immediate/offset: Examples are: (i) addi rt, rs,
immediate, (ii) lw rt, offset(rs), and (iii) beq rs, rt, offset. These instruc-
tions are called I format instructions.

(cid:15) Instructions that specify 3 registers: An example is add rd, rs, rt. These instruc-
tions are called R format instructions.

(cid:15) Instructions that specify a target. An example is jal target. These instructions
are called J format instructions.

Among the di(cid:11)erent (cid:12)elds, the rs, rt, and rd (cid:12)elds require 5 bits each, in order to specify
one of 32 general-purpose registers. Based on an analysis of existing programs, the MIPS-I

5.3. Translating Assembly Language Programs to Machine Language Programs

191

ISA designers decided to use 16 bits to specify the offset/immediate (cid:12)eld. For the (cid:12)rst
category of instructions, this leaves 6 bits for the opcode. With 6-bit opcodes, the maximum
number of opcodes that can be supported by the ISA is 64. Incidentally, the MIPS-I ISA has
more than 64 opcodes. To accommodate more than 64 opcodes with a (cid:12)xed opcode (cid:12)eld,
we need to either reduce the number of bits for the offset/immediate (cid:12)eld, or use longer
instruction formats, both of which have undesired consequences. The solution adopted by
the MIPS-I ISA designers was to use variable length opcodes or expanding opcodes within a
(cid:12)xed-length instruction format. Instructions that cannot a(cid:11)ord to support a longer opcode
(cid:12)eld|the ones in the (cid:12)rst and third categories|are given a 6-bit opcode (cid:12)eld. Instructions
in the second category have to specify only 3 register addresses, requiring a total of 15 bits,
besides the opcode. Therefore, these instructions are given a longer opcode (cid:12)eld. For these
instructions, the standard opcode (cid:12)eld is set to the bit pattern 000000, and a separate 6-bit
func (cid:12)eld is used to specify the desired operation. The three main formats of the MIPS-I
ISA are given in Figure 5.2.

I format

opcode

6

R format

000000

rs

5

rs

5

rt

5

rt

5

J format

opcode

6

offset

16

addi
lw
beq

rt, rs, immediate
rt, offset(rs)
rs, rt, offset

rd

5

target

26

opcode

6

add

rd, rs, rt

j
jal

target
target

Figure 5.2: Instruction Formats Used in the MIPS-I ISA

5.2.4 An Example MIPS-I ML Program

5.3 Translating Assembly Language Programs to Machine
Language Programs

Programs written in an assembly language are translated into machine language programs
by a special program called an assembler. An assembly language program is usually
entered into the computer through a terminal and stored either in the main memory or
on a magnetic disk. At this point, the program is simply a set of lines of alphanumeric
characters, which are stored as patterns of 0s and 1s, most likely as per the ASCII chart.
When the assembler program is executed, it reads this assembly language program, and
generates the desired machine language program (object code (cid:12)le), which also consists of
patterns of 0s and 1s (but in a tightly encoded form). The ob ject code is a combination

192

Chapter 5.

Instruction Set Architecture (ISA)

of machine language instructions, data, and information related to placing the instructions
properly in memory. It cannot be executed directly. Prior to execution, it undergoes the
steps of linking and relocation, which transform it into a binary executable (cid:12)le.

Because the assembly language program is just a symbolic representation of a machine
language program, on (cid:12)rst appearance it might seem that an assembler is a simple software
program that reads one statement at a time, and translates it to machine language. In fact,
for the most part, there is a one-to-one correspondence between instructions in an assem-
bly language program and the corresponding machine language program. The assembler’s
complexity arises from the additional features that are supported in the assembly-level ar-
chitecture. For instance, most assembly languages support AL-speci(cid:12)c instructions and
macros. In addition, all assembly languages support the use of labels.

5.3.1 MIPS-I Assembler Conventions

Like the MIPS-I compiler, the MIPS-I assembler also follows some conventions. These
conventions deal with the addresses allocated for di(cid:11)erent sections in memory, for instance.
Again, these conventions are not part of the machine language speci(cid:12)cations. Some of the
important conventions are given below:

(cid:15) The .text section starts at memory address 0x400000, and grows in the direction of
increasing memory addresses.

(cid:15) The .data section starts at memory address 0x10000000; it also grows in the direction
of increasing memory addresses.

(cid:15) The stack section starts at memory address 0x7fffffff, but grows in the direction
of decreasing memory addresses.

5.3.2 Translating Decimal Numbers

5.3.3 Translating AL-speci(cid:12)c Instructions and Macros

Many assembly languages provide AL-speci(cid:12)c instructions, which are not present in the
corresponding ISA level. The availability of these additional instructions makes it eas-

5.3. Translating Assembly Language Programs to Machine Language Programs

193

ier to program for the assembly-level architecture. The assembler translates AL-speci(cid:12)c
instructions by using short sequences of instructions that are present in the ISA. For in-
stance, the MIPS-I assembly-level architecture provides the opcode li, which is not present
in the MIPS-I ISA. A MIPS-I sequence for translating the AL-speci(cid:12)c instruction li R1,
0x11001010 is given below.

lui
ori

# Enter upper 16 bits of address in R1; clear lower 16 bits
R1, 0x1100
R1, R1, 0x1010 # Bitwise OR the lower 16 bits of address to R1

A few other examples are given in Table 5.2. In the table, the AL-speci(cid:12)c instruction
li is synthesized with an addiu instruction or lui-ori instruction sequence, depending on
whether the immediate value lies between (cid:6)32K or not. The AL-speci(cid:12)c instruction la is
also synthesized with a lui-ori pair. The AL-speci(cid:12)c instruction lw has a slightly di(cid:11)erent
format than the ISA-level lw instruction in that it uses memory direct addressing whereas
the latter uses register-relative addressing. This conversion is also done by using the lui
instruction to load the upper 16 bits of the memory address to a register, and then using
the ML lw instruction with the lower 16 bits as the displacement. Notice that when the
displacement value is negative, a 1 needs to be added to the upper 16 bits.

li

R2, 0x55ffff

MIPS-I AL Pseudoinstruction Equivalent MIPS-I AL Sequence
R2, R0, -10
addiu
R2, -10
li
R2, R0, 0x8000
ori
li
R2, 0x8000
R2, 0x55
lui
ori
R2, R2, 0xffff
R2, 0x1000
lui
R2, R2, 0xffff
ori
R4, 0x1000
lui
lw
R4, 0x1010(R4)
R5, 0x1001
lui
lw
R5, 0xffff(R5)

R4, 0x10001010

R2, 0x1000ffff

lw

R5, 0x1000ffff

la

lw

Table 5.2: Some MIPS-I AL-speci(cid:12)c Instructions and Their Equivalent Sequence

Translation of the macros can be done using text substitution in a similar manner.
For example, the assembler can invoke the C preprocessor cpp to do the required text
substitution prior to doing the assembly process.

5.3.4 Translating Labels

Another feature that assembly languages have incorporated for improving programmabil-
ity is the use of labels. A label is a symbolic representation for a memory location that

194

Chapter 5.

Instruction Set Architecture (ISA)

corresponds to an instruction or data. For example, in the following code, the label var1
corresponds to a memory address allocated for data, and the label L1 corresponds to a
memory address allocated for an instruction.

.data
var1: .word
.text
beq
ori
lw

L1:

6

# Allocate a 4-byte item at memory location named var1

R1, R2, L1
R1, R1, 0x1010
R2, var1

# Branch to label L1 if the contents of R1 and R2 are same

# Load from memory location named var1 into R2

Translating a label involves substituting all occurrences of the label with the correspond-
ing memory address. Although this is a straightforward substitution process, the situation
is complicated by the fact that most assembly languages permit a label to be used prior to
declaring it. This is illustrated in the above example code, where label L1 is used by the
beq instruction before its declaration two instructions later. Thus, the assembler cannot
perform the substitution for var1 when it (cid:12)rst encounters the beq instruction. In order to
translate such labels, the assembler needs to go through a program in two passes. The (cid:12)rst
pass scans the assembly language program for symbol declarations, and builds a symbol
table, which contains the addresses associated with each symbol. The second pass scans
the program, and substitutes each occurrence of a symbol with the corresponding address.
This pass also translates each instruction to machine language, and generates the machine
language program, possibly along with the assembly listing.

5.3.4.1 First Pass: Creating the Symbol Table

The purpose of the symbol table is to store the mapping between the symbolic names
(labels) and their memory addresses. As noted earlier, the ob jective of the (cid:12)rst pass is
to scan the assembly language program, and expand the AL-speci(cid:12)c instructions as well as
build the symbol table. For each symbol de(cid:12)ned in a module, a memory address is assigned,
and an entry is created in the symbol table. The symbol table is a data structure, on which
two operations can be performed:
insertion and search. The (cid:12)rst pass of the assembler
performs only insertions. An e(cid:14)cient organization of the symbol table is important for fast
assembly, as the symbol table may be referenced a large number of times during the second
pass.

5.3.4.2 Second Pass: Substituting Symbols with Addresses

In the second pass through the program, the assembler substitutes symbols with the re-
spective addresses stored in the symbol table created in the (cid:12)rst pass. For each symbol
encountered, it looks up in the symbol table to obtain the address. Unde(cid:12)ned symbols, if
any, are marked as external references. A list of unresolved references is made available at
the end of this pass.

5.3. Translating Assembly Language Programs to Machine Language Programs

195

5.3.5 Code Generation

Code generation involves translating each instruction to its corresponding bit pattern as
per the ISA de(cid:12)nition. THis step can be done in the second pass, and mostly involves table
lookups.

5.3.5.1 Example Ob ject File Format

To clarify the assembly process, we shall use a detailed example. For convenience, we shall
use the assembly language code that we saw earlier in pages 112 and 123. This code is
reproduced below. For the global variables, we shall use the same memory allocation given
in Figure 3.5. In the code given below, the left hand side shows the assembly language code.
The right hand side shows the memory map of the translated machine language code, along
with deassembled code (in assembly language format). This example shows how AL-speci(cid:12)c
instructions (such as la and bge) are translated using 2-instruction sequences. The example
also shows how various symbols (variable names as well as labels) are translated. After the
translation, references to variables are speci(cid:12)ed using the appropriate memory addresses.
Notice that in the MIPS-I ISA, a branch instruction speci(cid:12)es a branch o(cid:11)set instead of an
absolute target address.

# Assign memory locations for the global variables
# Initialize memory locations if necessary
.data
.word
a:
b:
.word
record: .word

0
12
0:3

f:
cptr:

.word
.word

loop:

.text
.align 2
la
R1, record

li
li
lw
bge

lw
add
addi
addi
b

R2, 0
R3, 0
R4, 8(R1)
R3, R4, done

R5, 0(R1)
R2, R2, R5
R1, R1, 4
R3, R3, 1
loop

0x10001000: 0x0
0x10001004: 0xc
0x10001008: 0x0
0x1000100c: 0x0
0x10001010: 0x0
0x10001014:
0x10001018:

0x400000:
0x400004:
0x400008:
0x40000c:
0x400010:
0x400014:
0x400018:
0x40001c:
0x400020:
0x400024:
0x400028:
0x40002c:

# lui R1, 0x1000
# ori R1, R1, 0x1008
# add R2, R0, R0
# add R3, R0, R0
# lw
R4, 8(R1)
# slt R6, R3, R4
# beq R6, R0, 6
# lw
R5, 0(R1)
# add R2, R2, R5
# addi R1, R1, 4
# addi R3, R3, 1
# beq R0, R0, (cid:0)6

196

Chapter 5.

Instruction Set Architecture (ISA)

done:

sw

R3, a

lw
sw

R4, cptr
R2, 0(R4)

0x400030:
0x400034:
0x400038:
0x40003c:

# lui R6, 0x1000
# sw
R3, 0x1000(R6)
# lw
R4, 0x1018(R6)
# sw
R2, 0(R4)

One important aspect to note is that the target of the assembled code is the machine
language seen by the linker.

5.3.6 Overview of an Assembler

Summary

AL Program

ML Program

Expanded (by a pre-processor)
Macro
Do not appear explicitly
Directive
Comment
Ignored, or placed in the .comments section
Label declaration Assigned memory address(es) in the appropriate section
Label reference
Substituted by the address assigned to the label
Synthesized by an instruction or a sequence of instructions
Instruction

Table 5.3: Translation of AL features to ML

5.3.7 Cross Assemblers

We just saw how an assembler translates assembly language programs to machine language.
As the assembler is implemented in software, it has to be executed on a host computer
for it to perform the translation. Normally when we execute an assembler on a particular
host computer, the assembler translates the program to the machine language of the host
computer itself. We can say that the assembler’s target ML is the same as that of the host
ML. Sometimes we may want to translate a program to an ML that is di(cid:11)erent from the
host ML. A cross assembler is used to do such a translation. There are several reasons why
such a cross assembly is required: (i) The target computer may not have enough memory
to run the assembler. This is often the case in embedded systems. (ii) When a new ML is
being developed, no host computer exists for that ML. To analyze di(cid:11)erent trade-o(cid:11)s, the
designer often builds a software simulator for the target computer. In order to \execute"
programs on the simulator, programs need to be translated to the new ML on a di(cid:11)erent
host computer.

5.4. Linking

5.4 Linking

197

Many computer programs are very large, and may even have millions of lines of instructions
and data. In order to make it easier to develop such large programs, a single program is
often developed as a collection of modules (i.e., a logically related collection of subroutines).
Di(cid:11)erent modules may even be developed by di(cid:11)erent programmers, possibly at di(cid:11)erent
times. For instance, the library routines are written as a common set of routines before
the development of application programs, and rarely undergo changes after development.
Several programs may even share modules other than the libraries.

When an assembly language program is developed as a collection of several modules,
how should it be assembled? If all of the source modules are combined into a single AL
program, and then the assembly is performed, that entails re-assembling the entire AL
program every time a change is made to one of the modules. This might require a long
time to perform the assembly, besides wasting computing resources. An alternative is to
assemble each module independently of other modules, and store the translated output as
an ob ject module (cid:12)le (on the disk). A change in a single source module would then require
re-assembling only that module. On Microsoft systems such as DOS, Windows 95/98, and
Windows NT, an ob ject module is named with a .obj extension, and an executable ML
program is given the extension .exe. On UNIX systems, ob ject (cid:12)les have extension .o,
whereas executable ML programs have no extension.

An ob ject module cannot be directly executed as a ML program, because it is likely
to contain unresolved references. An ob ject module contains an unresolved reference to a
label, if its source module does not contain a de(cid:12)nition for that label. For instance, the
module may have a call instruction to label foo, and foo may be de(cid:12)ned in another module.
In order to generate an executable ML program, we need to combine all of the component
ob ject modules into a single program. This process of combining | called linking | is is
customarily done using a program called link editor or linker. The linking process has to
be repeated whenever one or more of the component modules is re-assembled.

The linker performs four important tasks:

(cid:15) Resolve references among multiple ob ject modules, i.e., patch the external references.

(cid:15) Search the program libraries to (cid:12)nd library routines used by the program.

(cid:15) Determine the memory locations that will be occupied by the instructions and data
belonging to each module, and relocate its instructions by adjusting absolute refer-
ences.

(cid:15) Append a start-up routine at the beginning of the program.

198

Chapter 5.

Instruction Set Architecture (ISA)

5.4.1 Resolving External References

A linker’s (cid:12)rst task is to resolve external references in the ob ject modules. The linker
does this by matching the unresolved references of each ob ject module and the external
symbols of every other ob ject module. Information about external symbols and unresolved
references is generated in the second pass of the assembly process, and stored as part of the
ob ject module. An external symbol in one ob ject module resolves a reference from another
ob ject module if both have the same name. An unmatched reference means that a symbol
was used, but not de(cid:12)ned in any of the ob ject modules. Unresolved references at this stage
in the linking process do not necessarily mean that there is a bug in the program. The
program could have referenced a library routine (such as printf) whose code was not in
the ob ject modules passed to the linker.

After attempting to resolve external references in the ob ject modules, the linker searches
the system library archives such as /lib/libc.a to (cid:12)nd prede(cid:12)ned subroutines referenced
by the ob ject modules. When the program uses a library routine, the linker extracts the
routine’s code from the library and incorporates it into the program text section. This new
routine, in turn, may call other library routines, so the linker continues to fetch other library
routines until no external references remain to be resolved, or a routine cannot be found.
A program that references an unresolved symbol that is not in any library is erroneous and
cannot be linked.

5.4.2 Relocating the Memory Addresses

If all external references are resolved, then the linker determines the memory locations that
each module will occupy. Because the modules were assembled in isolation, the assembler did
not know where a module’s instructions and data will be placed relative to other modules.
When the linker places a module in memory, all absolute references must be relocated to
re(cid:13)ect its true location. Because the linker is provided with relocation information that
identi(cid:12)es all relocatable references, it can e(cid:14)ciently (cid:12)nd and backpatch these references.

5.4.3 Program Start-Up Routine

Finally, the linker appends a start-up routine at the beginning of the program. The start-
up routine performs several book-keeping functions. One of the book-keeping functions
performed by MIPS-I start-up routines involves copying from the stack to the argument
registers ($a0-$a3) the (cid:12)rst four parameters (to be passed to the function main). After
performing the required initializing jobs, the start-up code calls the main routine of the
program. When the main routine completes execution, control returns to the start-up
routine, which then terminates the program with an exit system call.

.text

5.5.

Instruction Formats: Design Choices

199

.globl
start:
lw
addiu
addiu
mul
addu
jal
ori
syscall

start

# Program execution begins here
# Copy argc from stack to R4
$a0, 0($sp)
# Place stack address corresponding to argv in R5
$a1, $sp, 4
# Start calculating stack address of envp
$a2, $a1, 4
# Stack addr of envp = stack addr of argv + argc (cid:2) 4 + 4
$v0, $a0, 2
# Place stack address corresponding to envp in R6
$a2, $a2, $v0
# Call subroutine main
main
$v0, $0, exit code# Place code for exit system call in R2
# Call OS to terminate the program

main: add
li
loop: li
li
syscall

# Place the address of c in $a1
$a1, $sp, 4
# Place the number of bytes to be read (i.e., 1) in $a2
$a2, 1
# Place the (cid:12)le descriptor (0) in $a0
$a0, 0
$v0, read code # Place the code for read in $v0
# Call OS routine to perform the read

blez
li
li
syscall
b

# break out of while loop if syscall returned zero
$v0, done
# Place the (cid:12)le descriptor (1) in R4
$a0, 1
$v0, write code # Place the code for write in $v0
# Call OS routine to perform the write
# go back to while loop

loop

done: jr

$ra

# return from subroutine main to start-up code

The linker produces an executable (cid:12)le that can directly run on a computer system (that
has an appropriate loader). Typically, this (cid:12)le has the same format as an ob ject (cid:12)le, except
that it contains no unresolved references.
Its target is the instruction set architecture
seen by the loader. Notice that, unlike the assembly process, the linking process does not
represent a change of level in the virtual machine abstraction tower, because the linker’s
input and output are programs for the same virtual machine, and use the same language.
Some linkers also produce an ASCII representation of the symbol table. This can be used
during debugging for displaying the addresses of symbols used in both ob ject and executable
(cid:12)les.

5.5

Instruction Formats: Design Choices

Like an assembly language program, a machine language program also consists of a sequence
of instructions, each one instructing the machine to do a speci(cid:12)c operation. However, the
machine language alphabet has only two symbols, f0, 1g. Therefore bit patterns are the
sole means of specifying each instruction. For instance, the MIPS-I AL instruction
add $v0, $a1, $a2
is represented in machine language as:
00000000100001010001000000100000

200

Chapter 5.

Instruction Set Architecture (ISA)

Never mind how we get this bit pattern|we will show that a little later. This bit pattern
encodes all of the information needed to correctly interpret the instruction, including the
opcode and all 3 explicit operands. In the general case, the encoded information includes:

(cid:15) opcode | operation to be performed

(cid:15) address/value of each explicit operand

(cid:15) addressing mode for each operand (optional)

The addressing mode part is treated as optional because the opcode often implies the
addressing mode; i.e., the addressing mode of the operand is unambiguous, and need not
be explicitly speci(cid:12)ed. How about labels and comments? Are they also encoded in the
machine language? Consider the same add instruction, with a label and comment added as
shown below:
# calculate return value
add $v0, $a1, $a2
done:
It turns out that the encoding for this AL instruction is same as that given above for the
instruction without the label and the comment! The label (cid:12)eld is indeed translated to
a memory address, but this address is not endoded along with the instruction; instead,
arrangements are made to ensure that the encoded ML instruction will be placed in that
memory addrress when the program is loaded. The comment (cid:12)eld is ignored altogether,
and is not represented in machine language programs.

A key issue to consider in deciding an encoding for the instructions is the number of
bits to be used for encoding each instruction type. If too many bits are used to encode
an instruction, then the code size increases unnecessarily, resulting in increased memory
storage requirement3 as well as increased instruction fetch bandwidth requirements. If too
few bits are used, it might become di(cid:14)cult to add new instructions to the ISA in the future.
Some ISAs use a (cid:12)xed number of bits to encode all of the instructions in the ISA, and the
rest use a variable number of bits to encode all of the instructions. Newer ISAs such as
MIPS-I, SPARC, Alpha, and PowerPC use (cid:12)xed length instruction encoding, whereas older
ISAs such as IA-32 and VAX use variable length instruction encoding. There are good
reasons why the older ISAs went for variable length instructions. When computers were
(cid:12)rst introduced, memory address space was at a premium, and it was important to reduce
the size of programs. The use of variable length instructions, coupled with an encoding
technique called Hu(cid:11)man encoding4 , enabled these ISAs to promote the development of
machine language programs that required the minimum amount of memory.

When an ISA design team has to choose instruction formats for a new ISA, a number
of factors must be considered. If a particular computer becomes commercially successful,

3Memory storage requirement was a ma jor concern when memory was expensive. With memory prices
falling steadily, this has become less of a concern, especially for general-purpose computers. In the embedded
computing world, it is still a concern.
4 In this encoding, the most frequent instructions are encoded with the fewest bits and the least frequent
ones are encoded with the maximum number of bits. The words in the English language follow such an
encoding to a large extent.

5.5.

Instruction Formats: Design Choices

201

its ISA may survive for 20 years or more, like the Intel IA-32. The ability to add new
instructions and exploit other opportunities that arise over an extended period is of great
importance, if the ISA survives long enough for it to be a success.

Field-based Encoding: Apart from the number of bits used for instruction encoding,
there is another ma jor di(cid:11)erence in how information in the di(cid:11)erent (cid:12)elds are encoded. One
approach starts by listing all of the unique combinations of opcodes, operand speci(cid:12)ers, and
addressing modes. Once this listing is done, a unique binary value is assigned to each
unique combination. Such an encoding is called total encoding. An alternate approach
is to encode each (cid:12)eld|the opcodes, the operand speci(cid:12)ers, and the addressing modes|
separately. This approach is called (cid:12)eld-based encoding. Such an encoding makes it easier
for the lower level machine to decode instructions during execution. A related feature that
makes it even easier to decode instructions is (cid:12)xed (cid:12)eld encoding. In such an encoding,
a particular set of bits are devoted for specifying a particular part of an instruction, such
as its opcode or operands. This type of encoding helps to determine the operands or their
addresses even before (cid:12)guring out the opcode of the instruction.

5.5.1 Fixed Length Instruction Encoding

In this type of encoding, all instructions are encoded with the same number of bits, and
therefore have the same length. Using a (cid:12)xed number of bits for instruction encoding makes
it easier to determine where each instruction starts and ends. Although this might waste
some space because all instructions are as long as the longest one, it makes instruction
decoding easier for the lower-level microarchitecture, which interprets ML instructions.
The customary method is to divide the bits in an instruction word into groups called (cid:12)elds.
Consider the instruction add rd, rs, rt. When the assembler converts this assembly-
language instruction into a machine-language instruction, it produces a bit pattern that
encodes the information contained in add rd, rs, rt. Consider the format given in Figure
5.3.

Opcode

Operand 1 Operand 2

Opcode specifiers

Addr Mode specifiers

Register specifiers

0 0 0

1

0 0

0

0 0

1

0 0

0

0

1

0

Addr
Mode

Addr/
Value

Addr
Mode

Addr/
Value

0000 : COPY
0001 : ADD
0010 : SUB
0011 : AND

000 : Register direct
001 : Memory direct
010 : Register indirect
011 : Memory indirect
100 : Immediate

000 : AX
001 : BX
010 : CX
011 : DX

Figure 5.3: A Sample 16-bit Format to Represent the Instruction add rd, rs, rt

In this format, 4 bits are allocated to specify the opcode, thereby allowing up to 16
unique opcodes in the instruction set. The opcode ADD is encoded by the bit pattern 0001.
The register names are also given speci(cid:12)c 3-bit patterns. For instance, the bit pattern 001

202

Chapter 5.

Instruction Set Architecture (ISA)

has been used to represent BX, and the bit pattern 010 has been used to represent CX. Three
bits are used to specify an addressing mode. For instance, the bit pattern 000 is used to
denote register direct addressing.

Example: What is the bit pattern for ADD AX, BX in the sample encoding of Figure 5.3?

The bit pattern for the opcode, ADD, is 0001. The bit patterns for registers AX and BX are
000 and 001, respectively. The bit pattern for register direct addressing is 000. Therefore,
the bit pattern corresponding to the instruction ADD AX, BX is 0001000000000001.

What would be the bit pattern for COPY AX, M[32]? We can see that it is not possible
to represent the bit pattern for memory address 32 in the 3 bits allotted for specifying an
address. To accommodate long addresses in a (cid:12)xed length format, instruction sets use a
technique called expanding opcodes or variable length opcodes.

Let us illustrate the idea of expanding opcodes using the instruction encoding done in
PDP-8, a popular single-address machine of the 1960s. It was a 12-bit machine made by
DEC. Of the 12 bits allotted to an instruction, 3 bits were used for specifying the opcode.
The PDP-8 used 3 di(cid:11)erent instruction formats, as shown in Figure 5.4. Let us calculate
the maximum number of unique opcodes that PDP-8 can have, and the maximum number
of memory locations that it can specify directly.

Single address instructions

Opcode

3

Addr
Mode
2

Address

7

I/O instructions

1

1

0

I/O Address

I/O Opcode

6

3

Zero address instructions

1

1

1

Opcode

9

Figure 5.4: Instruction Formats Used in the PDP-8 ISA

Consider the (cid:12)rst format, namely the one for single-address instructions. 3 bits have been
allocated for encoding the opcode, and therefore, 2 3 = 8 unique bit patterns are possible.
Out of the 8, two are used for identifying IO instructions and zero-address instructions.
Thus, a maximum of 6 single-address instruction opcodes are possible. In the IO instruction
format, 3 bits are allocated for encoding the opcode, and therefore, a maximum of 8 IO
instruction opcodes are possible. In the zero-address format, 9 bits are allocated for the
opcode, allowing a maximum of 29 = 512 unique opcodes. Thus, a total of 6 + 8 + 512 =
526 unique opcodes are possible for the PDP-8.

The maximum number of bits allocated for directly encoding memory addresses in any
of the three formats is 7. Thus, a maximum of 27 = 128 memory locations can be speci(cid:12)ed.

5.6. Data Formats: Design Choices and Standards

203

5.5.2 Variable Length Instruction Encoding

Some instruction sets have a large number of instructions, with many di(cid:11)erent addressing
modes. With such instruction sets, it may be impractical to encode all of the instructions
within a (cid:12)xed length of reasonable size. To encode such an instruction set, many instruction
sets use variable length instructions.

In the IA-32 ISA, instruction lengths may vary from 1 byte up to 17 bytes. Figure 5.*
shows the instruction formats for some of the commonly used IA-32 instructions. The 1
byte format is typically used for instructions that have no opcodes and those that involve
specifying a single register operand. The opcode byte usually contains a bit indicating
if the operands are 8 bits or 32 bits. For some opcodes, the addressing mode and the
register are (cid:12)xed, and therefore need not be explicitly speci(cid:12)ed. This is especially the
case for instructions of the form opcode register, immediate. Other instructions use
a post-byte or extra opcode byte, labeled mod, reg, r/m, which contains the addressing
mode information. This is the format for many of the instructions that specify a memory
operand. The base plus scaled index mode uses a second post-byte labeled sc, index,
base.

5.6 Data Formats: Design Choices and Standards

We have seen how instructions are represented in a machine language using bit patterns.
Next, we will see how data values are represented in an ISA. The encoding used to repre-
sent data values has a signi(cid:12)cant e(cid:11)ect on the complexity of the lower-level hardware that
performs various arithmetic/logical operations on the values. The hardware for performing
an arithmetic operation on two numbers can be simple or complex, depending on the repre-
sentation chosen for the numbers! Therefore, it is important to choose representations that
enable commonly used arithmetic/logical operations to be performed in a speedy manner.
When using N bits to represent numbers, 2N distinct bit patterns are possible. The
following table summarizes the number of representable values for popular word sizes.

Machine language programs often need to represent di(cid:11)erent kinds of information|
instructions, integers, (cid:13)oating-point numbers, and characters. All of these are represented
by bit patterns. An important aspect regarding the typical usage of bit patterns is that:

Bit patterns have no inherent meaning

Stating this di(cid:11)erently, a particular bit pattern may represent di(cid:11)erent information in
di(cid:11)erent contexts. Thus, the same bit pattern may be the representation for an integer as
well as a (cid:13)oating-point number. We shall illustrate this concept with an example from the
English language. The word pen has several meanings in English: (i) a small enclosure for
animals, (ii) an instrument used for writing, or (iii) to write. Depending on the context,

204

Chapter 5.

Instruction Set Architecture (ISA)

Number of
Word Size
(in bits) Representable Values
16
4
8
256
65,536
16
4.29 (cid:2)109
32
1.41 (cid:2)1014
48
1.84 (cid:2)1019
64

ISAs/Machines

Intel 4004
Intel 8080, Motorola 6800
DEC PDP 11, Intel 8086, Motorola?? 32020
IBM 370, Motorola 68020, VAX 11/780
Unisys
Cray, DEC Alpha

Table 5.4: Number of Representable Values for Di(cid:11)erent Word Sizes

we are able to (cid:12)gure out the intended meaning of the word pen. Similarly, based on the
context, the computer correctly (cid:12)gures out the information type and the intended meaning
of a bit pattern. It is important to note that the information type could also be encoded in
the bit pattern using additional bits, but that is rarely done.

\Time (cid:13)ies like an arrow, but fruit (cid:13)ies like an orange."

5.6.1 Unsigned Integers: Binary Number System

First, consider the encoding of unsigned integers, which only have a magnitude and no
sign. The standard encoding used to represent unsigned integers at the ISA level is to use
the binary number system. This is a positional number system, and the value of an N -bit
pattern, bN (cid:0)1 bN (cid:0)2 :::b1 b0 , if interpreted as an unsigned integer, is given by

Vunsigned = bN (cid:0)1 (cid:2) 2N (cid:0)1 + bN (cid:0)2 (cid:2) 2N (cid:0)2 + bN (cid:0)3 (cid:2) 2N (cid:0)3 + ::::: + b0 (cid:2) 20

Example: The value of bit pattern 10001101, if interpreted as an unsigned binary integer,
is given by
27 + 23 + 22 + 20 = 128 + 8 + 4 + 1 = 141.
To convert the representation of an N -bit unsigned integer to a 2N -bit number system,
all that needs to be done is to append N zeroes to the left of the most signi(cid:12)cant bit (MSB).

Range of an N -bit Unsigned Number System:

0 ! 2N (cid:0) 1
Figure 5.5 depicts this range for a 32-bit number system.

5.6. Data Formats: Design Choices and Standards

205

0

0

1

1

.
0

. . . . .
1 2 3 4

. . . . . . . . .

. . . . . . . . . . . .

..

. .

32
2 −1

Figure 5.5: Range of Unsigned Integers Represented in a 32-bit Binary Number System

5.6.2 Signed Integers: 2’s Complement Number System

The most widely used number system for representing signed integers at the ISA level is the
2’s complement number system. In the 2’s complement number system, positive numbers
are represented in their binary form as before, but negative numbers are represented in 2’s
complement form. The 2’s complement number system is also a positional number system,
and the value of a bit pattern, bN (cid:0)1 bN (cid:0)2 :::b1 b0 , if interpreted as a signed integer, is given
by

Vsigned = (cid:0)bN (cid:0)1 (cid:2) 2N (cid:0)1 + bN (cid:0)2 (cid:2) 2N (cid:0)2 + bN (cid:0)3 (cid:2) 2N (cid:0)3 + ::::: + b0 (cid:2) 20
This expression is similar to the one for usigned integers, except for the negative sign in
front of the bN (cid:0)1 term.
To convert the representation of an N -bit signed integer to a 2N -bit number system, all
that needs to be done is to append N copies of the sign bit to the left of the MSB.

Example: The integer (cid:0)12 is represented in the 8-bit 2’s complement number system as
11110100. Its representationj in the 16-bit 2’s complement number system is 1111111111110100,
obtained by appending 8 copies of the sign bit (1) to the left of the most signi(cid:12)cant bit.

Range of an N -bit 2’s Complement Number System:

(cid:0)2N (cid:0)1 ! 2N (cid:0)1 (cid:0) 1
Figure 5.6 depicts this range for a 32-bit number system. Roughly half of the range is
on the positive side and the other half is on the negative side. On comparing this range
with the one given earlier for unsigned integers, we can see that the upper half of the range
of unsigned integers have been replaced by negative integers.

Table 5.5 gives the bit pattern for some positive and negative numbers in the 8-bit 2’s
complement number system.

The main advantages of using the 2’s complement number system are:

(cid:15) Only one representation for zero|allows an extra (negative) number to be represented.

206

Chapter 5.

Instruction Set Architecture (ISA)

Negative Numbers

Positive Numbers

1

Any bit pattern

0

Any bit pattern

.

. . . . .

. . . . . . . . .
−2
−4
−3
−1

. . . . . . . .
1 2
3 4
0

. . . .

..

. .

31

−2

31
+2  −1

1 1

1

0

0

10

1

Figure 5.6: Range of Signed Integers Represented in a 32-bit 2’s Complement Number
System

Bit pattern Decimal value Remarks
(cid:0)128
Most negative representable integer
10000000
(cid:0)127
10000001

11111111
00000000
00000001
00000010

01111110
01111111

(cid:0)1
0
1
2

126
127

Unique representation for zero

Largest representable integer

Table 5.5: Decimal Equivalents of 8-bit Patterns in 2’s Complement Number System

(cid:15) The same adder can be used for adding unsigned integers as well as signed integers,
thereby simplifying the design of the ALU at the digital logic level.

For instance, if two signed integers (e.g., 45 and (cid:0)62), expressed in the Sign-Magnitude
number system, are added as if they are unsigned integers, then the result will be incorrect
((cid:0)107), as shown below. On the other hand, if the same two numbers are expressed in the
2’s complement number system and added as unsigned integers, the result ((cid:0)17) will be
correct.

5.6.3 Floating Point Numbers: ANSI/IEEE Floating Point Standard

Our next ob jective is to come up with suitable representations for (cid:13)oating-point numbers.
We already saw in Section 3.1.4 that a (cid:13)oating-point number has 4 parts: the sign, the
signi(cid:12)cand, the base, and the exponent. Among these, the base is (cid:12)xed for a given ISA,

5.6. Data Formats: Design Choices and Standards

207

Sign-Magnitude System

2’s Complement System

0

1

0

0

1 0 1 1 0 1

(45)

1

1

1 1 1 0

(-62)

0

0

1 0 1 1 0 1

(45)

1 1 0 0 0 0 1 0

(-62)

11010111

(-107)

11110111

(-17)

Incorrect

Correct

Figure 5.7: Adding Two Signed Integers in Sign-Magnitude Number System and 2’s Com-
plement Number System

and need not be explicitly encoded or represented in the bit pattern for a (cid:13)oating-point
number. The remaining parts (the sign, the signi(cid:12)cand, and the exponent) need to be
explicitly stored in the bit pattern. In designing a format for (cid:13)oating-point numbers, the
obvious choice is to use (cid:12)eld-based encoding, i.e., partition the available bit positions into
(cid:12)elds, and pack the di(cid:11)erent parts of the FP number into di(cid:11)erent (cid:12)elds. Thus the items
to be decided at ISA design time are:

(cid:15) the base

(cid:15) the signing convention for the signi(cid:12)cand and the exponent

(cid:15) the number of bits for specifying the signi(cid:12)cand and the exponent

(cid:15) the ordering of the sign, signi(cid:12)cand, and exponent (cid:12)elds

Until about 1980, almost every ISA used a unique FP format. There was no consensus
for even the base; di(cid:11)erent powers of 2 such as 2, 4, 8, or 16 have been used as the base.
The lack of a standard made it di(cid:14)cult to exchange FP data among di(cid:11)erent computers.
Worse yet, some computers occasionally did (cid:13)oating-point arithmetic incorrectly because
of some subtleties in (cid:13)oating-point arithmetic. To rectify this situation, the IEEE set up
a committee in the late 1970s to standardize FP representation and arithmetic. The goal
was not only to permit FP data to be exchanged among di(cid:11)erent computers but also to
provide hardware designers with a model known to be correct. The resulting work led
to IEEE Standard 754. Almost all of the present day ISAs (including the IA-32, Alpha,
SPARC, and JVM) use the IEEE FP standard and have FP instructions that conform to
this standard.

The IEEE standard de(cid:12)nes three formats: single precision (32 bits), double precision
(64 bits), and extended precision (80 bits). The extended precision format is intended to
reduce roundo(cid:11) errors while performing arithmetic operations. It is primarily used inside
FP arithmetic units, and so we will not discuss it further. Both the single- and double-
precision formats use base 2, and excess code for exponents. These two formats are shown
below.

208

Chapter 5.

Instruction Set Architecture (ISA)

\The most valuable of all talents is that of never using two words when one will do"
Thomas Je(cid:11)erson

1

8

23

1

11

52

Sign

Exponent

Significand

Sign

Exponent

Significand

(i)

(ii)

Figure 5.8: IEEE Floating-Point Formats: (i) Single Precision (ii) Double Precision

Single-Precision:

(cid:15) Base of the FP number system is 2
(cid:15) Number of bits allotted for representing the signi(cid:12)cand is 23
(cid:15) Signi(cid:12)cand’s precision, m is 24 (because the MSB is not explicitly stored)
(cid:15) Number of bits allotted for representing the exponent, e is 8
(cid:15) Format of the exponent: excess 127 code

Double-Precision:

(cid:15) Base of the FP number system is 2
(cid:15) Number of bits allotted for representing the signi(cid:12)cand is 52
(cid:15) Signi(cid:12)cand’s precision, m is 53 (because the MSB is not explicitly stored)
(cid:15) Number of bits allotted for representing the exponent, e is 11
(cid:15) Format of the exponent: excess 1023 code

Notice that the value used for the base is not encoded in the bit pattern, but forms part
of the de(cid:12)nition of the number system.
In the single-precision format, the exponent is
represented in 8 bits. With 8 bits, the exponent can take values ranging from (cid:0)128 to 127.
Of these, the most negative value ((cid:0)128) is treated as special; when the exponent is (cid:0)128,
the signi(cid:12)cand part is used to represent special numbers such as +1, (cid:0)1, and NaN (Not
a Number). Because of this arrangement, the most negative exponent allowed in this FP
format is (cid:0)127.

Normalization of Signi(cid:12)cand

When using the (cid:13)oating-point notation, a number can be written in a myriad ways. For
example, 0:001012 (cid:2) 23 , 1:012 (cid:2) 20 , and 101002 (cid:2) 2(cid:0)4 all denote 1:012 . To reduce this
profusion of equivalent forms, a speci(cid:12)c form is de(cid:12)ned to give every FP number a unique
representation. This process is called normalization. Normalization (cid:12)xes the position of
the radix point such that it immediately follows the most signi(cid:12)cant non-zero bit of the
signi(cid:12)cand. For example, the normalized form of 0:00101 2 (cid:2) 23 is 1:012 (cid:2) 20 . The normalized
signi(cid:12)cand is a fraction with value between 1 and almost 2.

5.6. Data Formats: Design Choices and Standards

209

Speci(cid:12)cally, Signi(cid:12)candMax = 1:111111:::::12 = 210 (approx)
Signi(cid:12)candMin = 1:000000:::::02 = 110

With non-zero binary numbers, the most signi(cid:12)cant non-zero bit of the signi(cid:12)cand is always
a 1, and therefore need not be explicitly stored. By not explicitly storing this bit, an extra bit
of precision can be provided for the signi(cid:12)cand. Thus, the single-precision format e(cid:11)ectively
represents 24-bit signi(cid:12)cands, which provides approximately the same precision as a 7-digit
decimal value. This technique of not explicitly representing the most signi(cid:12)cant bit is often
called the hidden bit technique. With this technique, the signi(cid:12)cand 1.011, for instance,
will be represented as follows, with the leading 1 and the radix point omitted.

0 1 1

0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

00

What about the number zero? Can it be expressed in normalized form? It turns out that
the number zero cannot be represented in the normalized form, because its signi(cid:12)cand does
not contain any 1s! The closest representable number is 1:0 2 (cid:2) 2(cid:0)127 , which has the smallest
normalized signi(cid:12)cand and the smallest exponent. A similar representation problem arises
when attempting to represent FP numbers that have magnitudes smaller than 1:0 2 (cid:2) 2(cid:0)127 ,
the smallest normalized FP number that can be represented with the allowed precision.
To provide good precision for these small numbers and to represent zero, IEEE 754 allows
these numbers to be represented in unnormalized form. Thus, when the exponent value is
(cid:0)127, the signi(cid:12)cand part is considered to have a hidden 0 (instead of a hidden 1) along
with a radix point. This makes it possible to represent numbers with very small magnitudes
and the number zero, at the expense of not representing some numbers such as 2 (cid:0)127 . For
example, the number zero will be represented as follows:

0

Bit pattern
for −127

0 0 0

0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

00

Biasing of Exponent

The designers of the IEEE 754 standard also considered several hardware issues when com-
ing up with this standard. In particular, they wanted an integer comparator to be able to
compare two (cid:13)oating-point numbers also. This is one of the motivating reasons for placing
the sign bit in the most signi(cid:12)cant position, and the exponent before the signi(cid:12)cand. Plac-
ing the sign bit at the most signi(cid:12)cant position ensures that the bit pattern of a negative
FP number will seem smaller than that of a positive FP number, if both patterns are inter-
preted as signed integers (in the 2’s complement number system). When using normalized
signi(cid:12)cands, for FP numbers with positive exponents, the bit patterns of numbers with
smaller exponents are guaranteed to appear smaller than numbers with bigger exponents.

A problem occurs, however, with negative exponents when exponents are represented in
the 2’s complement number system or any other number system in which negative integers

210

Chapter 5.

Instruction Set Architecture (ISA)

have a 1 in the MSB. This is because an FP number with a negative exponent will look
like a very big binary number. For example, the number 1:01 2 (cid:2) 2(cid:0)1 , which has a negative
exponent, would be represented as
0 11111111 01000000000000000000000
The number 1:012 (cid:2) 21 , which has a positive exponent, would be represented as
0 00000001 01000000000000000000000
If these two bit patterns are compared using an integer comparator, the latter pattern’s
value will be considered to be smaller than that of the former!

The desired notation for FP numbers must therefore represent the most negative expo-
nent as 000000002 and the most positive exponent as 111111112 . This can be achieved by
using the excess 127 code for representing the exponents so that the most negative exponent
((cid:0)127) can be represented as the bit pattern 00000000. In an excess E code system, before
representing an integer I , the excess value of E is purposely added to I such that the integer
value of the bit pattern stored is given by
S = I + E

With the use of the excess 127 code for the exponents, the representations for the above
two numbers (1:012 (cid:2) 2(cid:0)1 and 1:012 (cid:2) 21 ) become
0 01111110 01000000000000000000000

and

0 10000000 01000000000000000000000
respectively, in which case an integer comparator is su(cid:14)cient to compare them. Expressing
the exponent in excess code thus allows simpler hardware to compare two (cid:13)oating-point
numbers. At the same time, the excess code system retains the bene(cid:12)ts of the 2’s com-
plement number system in that it can handle negative exponents as gracefully as positive
exponents. Notice that the range of representable exponents is still (cid:0)127 ! 127. Thus,
the excess code system neither allows more exponents to be represented nor changes the
range of representable exponents. It only changes the bit patterns of the exponents that
were representable before the excess was applied.

In order to place the di(cid:11)erent features of the IEEE 754 standard in the proper context
and to see how they all (cid:12)t together, let us look at an example.

Example: What is the decimal equivalent of the following bit pattern if interpreted as an
IEEE single precision (cid:13)oating point number?
0 10000110 00110000000000000000000

(cid:15) Step 1: Extract the 8-bit exponent (cid:12)eld (10000110) and subtract 127 from it. One
way to do is to add the bit pattern 10000001, which is the 2’s complement of the bit
pattern for 127. The resulting bit pattern gives the exponent in 8-bit 2’s complement
number system. Find the decimal equivalent of this exponent.

(cid:15) Step 2: Extract the 23-bit signi(cid:12)cand (00110000000000000000000). If the exponent’s
actual value is (cid:0)127, include a ‘0.’ before the signi(cid:12)cand; otherwise include a ‘1.’.

5.6. Data Formats: Design Choices and Standards

211

(cid:15) Step 3: Extract the sign bit.
signi(cid:12)cand.

If it is a 1, then attach a negative sign before the

(cid:15) Step 4: Find the decimal equivalent of the signi(cid:12)cand.

Interestingly, when representing the exponents with the excess 127 code, the represen-
tation of (cid:13)oating-point number 0.0 becomes the all-zero pattern
00000000000000000000000000000000. Because zero is a frequently encountered (cid:13)oating-
point number, the ability to quickly identify zero from its bit pattern is a clear advantage.

Figure 5.6 succinctly shows the range of numbers representable by the IEEE 754 single-
precision format, and how it represents di(cid:11)erent regions within this range. The (cid:12)gure shows
the real number line; the dots in the line denote the real numbers that are representable
in the IEEE format. One ma jor di(cid:11)erence between the set of real numbers and the set
of representable (cid:13)oating-point numbers is their density. Real numbers form a continuum,
whereas the representable (cid:13)oating-point numbers do not form one. If a real number cannot
be represented in (cid:13)oating-point format, then the obvious thing to do is to round that number
to the nearest expressible (cid:13)oating-point number.

Zero

0

0

0

0

0

Negative Numbers
in Normalized Form

Positive Numbers
in Normalized Form

1

Any bit pattern

0

Any bit pattern

.

.

. . . .

.

.

. .

. .
...............
.
.
.
... ... .. .. . . . . .
.

..

.

...

..

.

.

-Infinity

128
-2

-126
-2

0

-126
+2

128
+2

+Infinity

1 1

1 0

0

0

1

1 0

0

1

0

0

Any non-zero pattern

0

0

0

Any non-zero pattern

Negative Numbers
in Unnormalized Form

Positive Numbers
in Unnormalized Form

Any bit pattern with at least one 0 and at least one 1

1

1 Any non-zero pattern

Not a Number

Figure 5.9: Range of Numbers Representable by IEEE 754 Single Precision Format

As we can see from the (cid:12)gure, the spacing between adjacent expressible numbers is
not constant throughout. However, when the spacing is expressed as a percentage of
the expressed numbers, there is no systematic variation throughout the expressible range.
Therefore, the relative error introduced by rounding is approximately the same for small
numbers and large numbers.

212

Chapter 5.

Instruction Set Architecture (ISA)

Rounding: As discussed earlier, because of using a (cid:12)nite number of bits, only a (cid:12)nite
number of distinct real numbers can be represented in the computer. This means that
most of the in(cid:12)nitely many real numbers must be rounded o(cid:11) to the nearest representable
(cid:13)oating-point number, producing roundo(cid:11) errors. In order to reduce roundo(cid:11) errors during
computation, IEEE provides the 80-bit extended precision format. Expand.

5.6.4 Characters: ASCII and Unicode

The last type of data type we look at is the character. At the ISA level, characters, like
numeric information, can only be represented by bit patterns. And in a manner similar to
numeric data, the assumptions made about the format of characters is made at ISA design
time.

The characters that are usually encountered in computer applications are upper-case
alphabet (A-Z), lower-case alphabet (a-z), decimal digits (0-9), punctuation and formatting
symbols (full stop, comma, quotation marks, space, tab, parenthesis, etc), arithmetic sym-
bols (+, (cid:0), etc), and control characters (CR, LF, etc). Altogether, we have more than 64
characters, but fewer than 128. Therefore, we need a minimum of 7 bits for encoding these
characters.

A standard code has been developed for representing characters. This code has received
almost universal acceptance, and is called ASCII (American Standard Code for Information
Interchange). ASCII uses a 7-bit representation for each character, allowing up to 128
di(cid:11)erent characters to be represented. The normal method for storing a character is to place
the 7-bit pattern in an 8-bit (cid:12)eld called a byte. The 8th bit can be used as the parity bit if
required. Table 5.6 gives a sample of the bit patterns assigned to characters in the ASCII
code. The (cid:12)rst 32 codes (0x0 - 0x1F) are assigned to control characters: codes originally
intended not to carry printable information, but rather to control devices (such as printers)
that make use of ASCII, or to provide meta-information about data streams such as those
stored on magnetic tape. For example, the bit pattern 0x08 represents \backspace", and
the bit pattern 0x0A represents the \line feed" function (which causes a printer to advance
its paper).

The character digits \0"-\9" are represented with their values in binary pre(cid:12)xed with
0x3 (this means that converting a BCD number to ASCII code involves just taking each
BCD nibble separately and pre(cid:12)xing it with 0x3. The uppercase alphabets A-Z are assigned
consecutive patterns from 0x41 - 0x5A. The lowercase alphabets a-z are also assigned con-
secutive patterns. The bit patterns for the lowercase and uppercase letters di(cid:11)er only in
the most signi(cid:12)cant bit. This trivializes case conversion to a range test (to avoid converting
characters that are not letters) followed by a single bitwise operation.

A character string is represented by a concatenation of the ASCII codes for its component
characters. For example, the 11-character string \God is good", is encoded in ASCII by
the following bit pattern sequence:
G
o
d

d

o

g

o

i

s

5.7. Designing ISAs for Better Performance

213

1000111 1101111 1100100 0100000 1101001 1110011 0100000 1100111 1101111 1101111 1100100

ASCII Code
00H
08H
0AH
0DH
20H
21H
30H
31H
.
.
39H
41H
42H
.
.
5AH

Character
NULL
BACKSPACE
Line Feed
Carriage Return
SPACE
!
0
1
.
.
9
A
B
.
.
Z

ASCII Code Character

61H
62H
.
.
7AH

a
b
.
.
z

Table 5.6: ASCII Codes for a Sample of Characters

In terms of mere adoption, the ASCII standard is perhaps one of the most successful
software standards ever introduced. Clearly, the 7-bit ASCII representation is not su(cid:14)cient
for representing the characters of languages such as Chinese and Japanese, which have a
very large number of characters. For representing the characters of such languages, more
bits are required. Recently, a 16-bit character code known as Unicode has been developed
as an international standard. The characters of most of the natural languages are encoded
in Unicode. In fact, the ASCII standard has become embedded in Unicode, as the (cid:12)rst 128
characters.

5.7 Designing ISAs for Better Performance

\We stil l have judgment here, that we but teach bloody instructions,
which, being taught, return to plague th inventor"
| Act I, scene 7 of Macbeth, Wil liam Shakespeare

The last ma jor topic we discuss in this chapter is ISA design, that is, coming up with
the speci(cid:12)cations for a new ISA. Unlike the case with microarchitectures and other lower-
level architectures, ISAs are not changed frequently, because of the need to maintain binary
compatibility for existing machine language programs.

214

Chapter 5.

Instruction Set Architecture (ISA)

5.7.1 Technological Improvements and Their E(cid:11)ects

Instruction set architectures have evolved greatly since the late 1940s, when the (cid:12)rst ISAs
were implemented. Much of this evolution has been in(cid:13)uenced by continued improvements
in hardware technology and compiler technology, and also by changes in the application
domain. For instance, the physical dimensions of the devices used to implement switching
functions at the lower hardware levels have been shrinking at a rapid pace, ever since
semiconductor transistors were invented. Today’s technology allows millions of transistors
to be integrated in a single chip. How does this a(cid:11)ect ISA design? First of all, more
functionality can be incorporated in the processor, thereby facilitating complex instructions
in the ISA. Secondly, large memories can be built more easily, thereby facilitating large
memory address spaces in the ISA.

Along with improvements in hardware technology came advances in language translation
techniques.

There are many aspects of an ISA that are common to all ISAs. Common arithmetic
and logical instructions, control-changing instructions, .... An ISA designer must consider
several aspects of a potential feature to decide if it supports or con(cid:13)icts with the design
goals.

During these (cid:12)fty years of ISA development, a consensus has emerged about the im-
portance of some ISA features, for example, support of arithmetic and logical instructions,
conditional branch instructions, subroutine call and return instructions, indirect addressing
mode(s), constant addressing mode, etc. Almost all of the current ISAs include these; de-
cisions to exclude any of these features must be made very carefully. On other issues, there
has been and remains fundamental disagreement, for instance over the question of whether
the support or lack of complex instructions is better. No single set of value judgments has
yet emerged, because di(cid:11)erent ISAs have di(cid:11)erent goals and intended uses. DSP processors,
for instance, include features such as .....

Competing Design Goals:
In the previous two chapters we had identi(cid:12)ed di(cid:11)erent types
of instruction opcodes and addressing mechanisms in the assembly-level architecture. One
of the questions that must be addressed by a computer architect concerns the number and
complexity of instructions to be included at the ISA level. One option is to include a large
number of opcodes and addressing modes so as to reduce the total number of instructions
in a program. An ISA that uses this method is called a complex instruction set computer
(CISC). An alternative method is to reduce the complexity of the instruction set, and
thereby reduce the complexity of the hardware required for interpreting the instruction set.
An ISA of this type is called a reduced instruction set computer (RISC). In this section, we
will examine some of the issues involved in this decision process.

The earliest computers were very simple in their ISA and implementation, both be-
cause of lack of experience with computers and because the implementation technology of
those days mandated a simple machine. However, computer users wanted to solve rela-

5.7. Designing ISAs for Better Performance

215

tively complex problems, and high-level languages were developed that treated variables
and arithmetic at a level higher than that of the machine language. Compilers were used
to translate programs written in high-level languages to machine language programs. This
resulted in what has become known as the semantic gap, which is the gap between the
language of the programmer and the language of the hardware.

5.7.2 CISC Design Philosophy

One of the ob jectives of CISC designs is to reduce this semantic gap between the high-level
language statements and the machine language instructions.
It was felt that narrowing
the gap by means of complex instructions and addressing modes would lead to better per-
formance. Thus, computer architects began to make the ISAs more complex, with cor-
respondingly complex hardware implementations, made possible by advances in hardware
technology. CISC ISAs generally seek to reduce the compiler’s complexity by making the
ML instructions more closely conform to the operations of the high-level languages. Some
computer systems (e.g. SYMBOL) have carried this to the extreme by completely abol-
ishing the semantic gap between the high-level language and the machine language. That
is, the high-level language itself is used as the native machine language. However, this is a
rare practice.

Another reason for the popularity of complex instructions was that it reduced the size of
machine language programs. Program and data storage were at a premium in those days.
For instance, when Motorola introduced the M6800, 16 KB RAM chips cost $500, and 40
MB hard disk drives cost $55,000. When the MC68000 was introduced, 64 KB RAM chips
still cost several hundred dollars, and 10 MB hard drives cost $5,000.

If there is an overriding characteristic of the CISC concept, it is an approach to ISA
design that emphasizes doing more with each instruction. As a result, CISC machines have
a wide variety of addressing modes, 14 in the case of the MC68000, and 25 in its more
complex successor, the MC68020. Furthermore, CISC machines allow an instruction to
have variable number of operands, which can be present in registers and/or memory. For
instance, the VAX ADD instruction can have two or three operands, and any one of them
can be in a register or in memory. Another example of a CISC ISA is Intel’s IA-32 ISA
(more commonly known as the x86 ISA).

5.7.3 RISC Design Philosophy

By the early 1980s, compilers had advanced to such a level that almost all of the program-
ming began to be done in high-level languages. At that time, studies were conducted to
analyze the instruction set usage in contemporary CISC machines. These studies indicated
that compiler-generated ML programs, for the most part, were utilizing only a small subset
of all available instructions. An indication of this can be seen when inspecting the assembly
language programs we wrote in the last two chapters; instructions such as li, lw, sw, add,

216

Chapter 5.

Instruction Set Architecture (ISA)

sub, beq, and bne were used most of the time. Carrying this observation to the next logical
step, computer architects (cid:12)gured that computer performance could be enhanced by includ-
ing in the ISA only the frequently used instructions, and by making them execute as fast
as possible. The RISC approach, therefore, is to use a simpler ISA so as to permit a faster
clock than that possible in a CISC processor. With many commercial implementations this
premise is ful(cid:12)lled. Examples are the MIPS-I, Sparc, PowerPC, and Alpha ISAs, which
were implemented in several commercial processors. In practice, RISC-type ISAs permit a
number of strategies to be employed by the microarchitects so as to make use of a variety of
implementation features, such as pipelining and multiple instruction issue. In addition, they
help to free up space within the processor chip that can be usefully employed to incorporate
on-chip cache memory.

The complex statements of a high-level language would be translated to longer instruc-
tion sequences than corresponding CISC instruction sequences. The result is that a machine
language program for a RISC ISA is likely to have more instructions, each of which may
execute faster. The name RISC fsigni(cid:12)es the focus on reducing the number and complexity
of the instructions in the ISA. There are several other tenets common to RISC ISAs, and
these are described in the following paragraphs. You should be aware, however, that a
particular RISC ISA may not have all of these tenets.

Minimal Number of Opcodes and Addressing Modes: RISC ISAs usually limit
themselves to about three addressing modes: register addressing, register indexed address-
ing, and constant addressing. Other, more-complex addressing modes are synthesized in
software from the simple ones. only the instructions that are frequently executed, RISC
machines exclude infrequently used instructions. Simpler instructions permit shorter clock
cycles, because less work has to be done in a clock. The result is a smaller, faster proces-
sor, that is capable of executing more instructions in a given amount of time than a CISC
processor. Complicated addressing modes mean longer clock periods, because there is more
address calculation to perform.

Fixed Instruction Length and Simple Instruction Formats: Another common char-
acteristic of RISC ISAs is the use of a few, simple instruction formats. Instructions have
(cid:12)xed length and are aligned on word boundaries. Instructions are encoded in such a way
that (cid:12)eld locations, especially the opcode, are (cid:12)xed. This type of encoding has a number of
bene(cid:12)ts. First, with (cid:12)xed (cid:12)elds, register operand accesses can proceed prior to the comple-
tion of instruction decoding. Second, simpli(cid:12)ed instruction formats simplify the instruction
decoder circuitry, making it faster.

Only Load and Store Instructions Access Main Memory: Another important
characteristic of RISC ISAs is that arithmetic/logic instructions deal only with register
operands. Thus, only data transfer instructions, such as LOAD and STORE instructions, deal
with the memory address space. Restricting the operands of arithmetic/logic instructions

5.8. Concluding Remarks

217

to be in registers makes it easier to meet the above stated ob jective of single cycle execution.
The RISC approach relies on the observation that values stored in registers are likely to be
used several times before they are written to main memory.

The proponents of the RISC philosophy cite several reasons why the RISC approach to
ISA design can potentially lead to faster processors. Some of these reasons are given below.

1. Instruction decoding can be faster because of simpler instruction decoder (due to fewer
instructions in the ISA).

2. Memory access is faster because of the use of on-chip cache memory (cf. chapter
7), which is possible because of less hardware in the chip to do instruction decoding,
complex addressing modes, and complex instructions.

3. The use of simpler instructions makes it easier to use a hardwired control unit, which
is faster (cf. chapter 9).

4. Clock cycle time is potentially smaller because of less hardware in time-critical path(s).

5.7.4 Recent Trends

\The amount of money you make is directly proportional to your vocabulary."

Multimedia Extensions

Vector Instructions

VLIW and EPIC Instructions

5.8 Concluding Remarks

From a functional point of view, the ISA implements the assembly-level architecture. Al-
though our discussion was based primarily on this view, historically, the ISA is de(cid:12)ned
before de(cid:12)ning the assembly-level architecture. In other words, the assembly-level architec-
ture being a symbolic form of the ISA, is built on top of the ISA. Considering the historical
development of computers, the ISA came into existence before the assembly-level architec-
ture as well as the high-level architecture; in the early computers, programs were directly
written in machine language, in 0s and 1s!

218

Chapter 5.

Instruction Set Architecture (ISA)

5.9 Exercises

1. What is the decimal equivalent of 0xDD580000 if interpreted as an IEEE single preci-
sion (cid:13)oating-point number?

2. What is the decimal equivalent of the following bit pattern if interpreted as an IEEE
single precision (cid:13)oating-point number?
0 00000000 000000 .... 0

3. What are the decimal equivalents of the largest and smallest positive numbers that
can be represented in the IEEE single precision (cid:13)oating-point format?

4. Consider a (cid:13)oating-point number format that is similar to the IEEE format in all
respects, except that it does not use the hidden bit technique. That is, it explicitly
stores the most signi(cid:12)cant bit of the signi(cid:12)cand. Show that this number system can
represent only fewer (cid:13)oating-point numbers than the IEEE number system.

5. Explain why denormalization is done in the IEEE (cid:13)oating-point format for the smallest
exponent?

6. Both the assembly language program and the machine language program are stored
as \bit patterns" inside the main memory. Explain how the bit patterns of these two
programs are di(cid:11)erent.

7. An ISA de(cid:12)nes 2 di(cid:11)erent instruction formats. In the (cid:12)rst format, 6 bits are used
for the opcode, Among these opcode bit patterns, three are used as special patterns.
When any of these three patterns appear in the opcode (cid:12)eld, the instruction is encoded
using the second format, and the actual opcode is present in another 6-bit (cid:12)eld. How
many unique opcodes can this ISA support?

8. Design a variable-length opcode (but (cid:12)xed-length instruction) to allow all of the fol-
lowing to be encoded in 36-bit formats:
7 instructions with two 15-bit addresses and one 3-bit register number
500 instructions with one 15-bit addresses and one 3-bit register number
50 instructions with no addresses or registers.

Part II

PROGRAM EXECUTION |
HARDWARE LEVELS

By wisdom a house is built, and through understanding it is established; through
know ledge its rooms are (cid:12)l led with rare and beautiful treasures.

Proverbs 24: 3-4

220

The theme of this book is that a modern computer can be viewed as a series of archi-
tectural abstractions, each one implementing the one above it. Part II of the book gave
a perspective of the computer from the high-level language, assembly language, and ma-
chine language programmers’ points of view. These views relate to the abstract machine
levels that deal with program development, typically called the software levels.
We now turn our attention to the design of hardware for carrying out program execution|
moving electrons through wires and semiconductors to (cid:12)nd the results of executing a
program with a speci(cid:12)c set of inputs. Leaping from the \lofty" programs to the \lowly"
electrons is quite a dive, as you can imagine! Naturally, computer hardware design, like
computer programming, is carried out at multiple abstraction levels. Although such a
multi-layer implementation may incur some performance costs, it does have important
engineering advantages. The main advantage of adding extra implementation levels is
that it permits the complexity of the hardware to be tackled in a step-by-step manner.
Once an appropriate interface has been agreed upon between two adjacent levels, the
development of these levels can proceed more or less independently.

We will study three of these levels|the microarchitecture, the RTL architecture, and the
logic-level architecture|in this part of the book. Although these machine levels have
traditionally been implemented in hardware (for a variety of reasons), their real distiction
from the previously seen machine levels is that they relate to program execution, and
implement the machine levels immediately above them by interpretation rather than by
translation. One point will become apparent when you study the hardware abstraction
levels presented in this last part of the book: many of the traditional digital circuit design
techniques are of limited use while designing the computer hardware! For instance, in
theory, the digital computer can be viewed as a (cid:12)nite state machine. However, it is not
practical to design a digital computer as a single (cid:12)nite state machine, because of the
combinatorial explosion in the number of states. In practice, computer hardware design
is carried out by partitioning the hardware into di(cid:11)erent parts, based on functionality.

Program execution begins with copying the executable binary from a storage medium
into the computer’s memory. This process, called loading the program, is typically done
by the operating system. Loading, along with related topics such as dynamic linking, are
discussed in Chapter 6.
The microarchitecture|the (cid:12)rst virtual machine level we study in this part|implements
the instruction set architecture (ISA) by interpreting ML programs, as illustrated in Fig-
ure 1.10 on page 42. The details of the microarchitecture depend on the ISA being
implemented, the hardware technology available, and the cost-performance-power con-
sumption goals for the computer system. Microarchitecture-related issues are covered in
two chapters. In Chapter 7, we discuss microarchitectural aspects related to implementing
the user mode ISA. The initial ob jective is to present a microarchitecture that correctly
implements the user mode ISA and correctly executes user mode ML programs. Then we
move on to more advanced microarchitectures for the processor system and the memory
system. In this part of the chapter, we give special importance to achieving high perfor-
mance and low power. One of the important microarchitectural techniques discussed for
improving processor performance is pipelining. For memory systems, we discuss cache
memories in detail. Chapter 8 discusses microarchitectural aspects that are speci(cid:12)c to
implementing the kernel mode ISA. Topics covered include exceptions and interrupts,
virtual memory, and the IO system.
Chapter 9 discusses register transfer level architecture.

The logic-level architecture implements the microarchitecture using logic gates, (cid:13)ip-(cid:13)ops,
and other building blocks. This virtual machine level is covered in detail in Chapter 10.

Chapter 6

Program Execution Basics

Apply your heart to instruction and your ears to words of know ledge.

Proverbs 23: 12

A successfully developed program becomes useful only when it is executed, most likely
with a set of inputs. Program execution involves executing in the proper sequence the
instructions speci(cid:12)ed in the program. A typical computer system has ....

\The execution of the laws is more important than the making of them."
| Thomas Je(cid:11)erson

\I know no method to secure the repeal of bad or obnoxious laws
so e(cid:11)ective as their stringent execution."
| Ulysses S. Grant

6.1 Overview of Program Execution

In a small embedded computer system or a microcontroller that executes only a single
program, the program to be executed may be permanently stored in some kind of ROM
(read-only memory) that implements the memory address space of the computer. This
stand-alone program utilizes and manages all of the system resources by itself. Upon reset,
the program counter points to the beginning of the program. Thereafter, the system con-
tinues to execute the program until the next reset or power down. \Loading" the program
into the ROM | a very rare event | is usually done using a ROM programmer device that
has direct access to the ROM.

221

222

Chapter 6. Program Execution Basics

The situation is more complex for general-purpose systems such as desktops and laptops,
which are controlled by an OS and can execute multiple programs. The ma jority of the
memory address space in such systems is implemented by RAM (random access memory) to
make it easy for end-users to change the program to be executed. Several steps are involved
in the execution of a program in such a system:

(cid:15) Select the program

(cid:15) Create the process

(cid:15) Load the program

(cid:15) Dynamically link libraries (optional)

(cid:15) Execute the program

(cid:15) Stop and restart the program (optional)

(cid:15) Halt the program

This chapter discusses each of these steps in detail. All of the steps, except the interpretation
step, are typically done by the OS. The interpretation step, which forms the bulk of program
execution, is usually done directly by the hardware, for e(cid:14)ciency reasons. We can, however,
do this step also in software, and for pedagogic reasons, we include a discussion on software
interprters at the end of this chapter.

6.2 Selecting the Program: User Interface

In computer systems with an installed operating system, the operating system decides which
program should be executed and when. The end-user can specify the program to be executed
by giving the appropriate command to the operating system (OS). The part of the OS that
receives commands from the end-user is called a shel l. From the end-user’s perspective, the
shell is thus an important part of a computer system; it is the interface between the user
and the system.

Functionally, the shell behaves like an interpreter, and operates in a simple loop: accept
a command, interpret the command, execute the command, and then wait for another
command. Executing the command often involves requesting the kernel to create a new
process (called a child process) that performs the command. The child process is overlaid
with the program to be executed; once the program is completed, the child process is
terminated. The program to be executed is typically stored on a (cid:12)le system maintained by
the OS.

Over the years, many shells have been used and improved. Historically, they have a
(cid:13)avor heavily dependent on the OS they are attached to. We can classify shells into one of
three categories based on the primary input device used to enter commands:

6.2. Selecting the Program: User Interface

223

(cid:15) Text based | command line interface (CLI)

(cid:15) Graphics based | graphical user interface (GUI)

(cid:15) Voice based | voice user interface (VUI)

In a CLI environment, a command can be given, for instance, by typing the command in
a terminal via a keyboard. In a GUI environment, a command can be given, for instance,
by pointing the mouse at the program’s icon and clicking it.
In a VUI environment, a
command can be given, for instance, by speaking into a microphone.

6.2.1 CLI Shells

In a CLI environment, the user speci(cid:12)es a program to be executed by typing the command
in a terminal via a keyboard. A CLI shell displays a prompt whenever it is ready to
accept a new command. A command consists of a command name, followed by command
options (optional) and command arguments (optional). The command name, options, and
arguments, are separated by blank space. The basic form of a CLI command is:
commandname [-options] [arguments]

commandname is the name of the program the end-user wants the computer to execute.
options, usually indicated by a dash, alter the behavior of the command. arguments are
the names of (cid:12)les, directories, or programs that the program needs to access. The square
brackets ([ and ]) signify optional parts of the command, which may be omitted.

Example: The Unix command ls -l /tmp gives a long listing of the contents of the /tmp
directory. In this example, ls is the command name, -l is an option that tells ls to create
a long, detailed output, and /tmp is an argument naming the directory that ls should list.
A lot of typing on a keyboard can lead to repetitive strain injury (RSI) 1 , Newer CLI
shells reduce the amount of typing, with features such as auto-completion, showing the
(cid:12)les it would complete, and showing the current directory in the prompt. In addition, they
provide many facilities such as command aliasing and job control. Furthermore, a collection
of commands may be stored in a (cid:12)le, and the shell can be invoked to execute the commands
in that (cid:12)le. Such a (cid:12)le is known as a shel l script (cid:12)le. The language used in that (cid:12)le is called
shel l script language. Like other programming languages, it has variables and (cid:13)ow control
statements (e.g. if-then-else, while, for, goto). Most shells also permit the user to abort a
command being executed, by typing Control-C on the terminal.

1Repetitive Stress Injury, sometimes referred to as overuse syndrome, is pain and swelling that results from
performing a repetitive task, like text messaging and typing. The traditional input devices that computer
professionals use|the keyboard and the mouse|force the user to adapt their posture in order to adequately
operate the input devices. The static tension of the muscles that occurs while operating a standard keyboard
and mouse is thought to be one of the main causes of RSI for computer professionals.

224

Chapter 6. Program Execution Basics

Examples of CLI shells for Unix/Linux operating systems are Bourne shell (sh), Bourne-
Again shell (bash), C shell (csh), TENEX C shell (tcsh), and Korn shell (ksh). Examples
of non-Unix CLI shells are command.com (for DOS), cmd.exe (for OS/2 in text mode and
for Windows NT).

6.2.2 GUI Shells

In a GUI environment, the end-user speci(cid:12)es a program to be executed by pointing the
mouse at the program’s icon and clicking it. The system then responds by loading the
program into the physical memory. It also creates a new window on the display for the user
to interact with the program. The end user can also change the size, shape, and position
of the window.

Figure 6.1: Photo/Screen Shot of a GUI Desktop

The visual approach and the ease of use o(cid:11)ered by GUIs have made them much more
popular, particularly among the masses. The relative merits of CLI- and GUI-based shells
are often debated. From the performance perspective, a GUI would probably be more
appropriate for a computer used for image or video editing. On the other hand, for certain
other operations such as moving (cid:12)les, a CLI tends to be more e(cid:14)cient than a GUI, making it
a better choice for servers used for data transfers and processing with expert administration.
The best choice is really dependent on the way in which a computer will be used.

Graphical user interfaces do have some disadvantages. They are harder to develop and
require a lot more memory to run. The system may also need to have powerful graphic
video capability. Moreover, they do not free the end user from repetitive strain injury (RSI).
Although the amount of muscle movement required is small with a mouse (compared to a
keyboard), all of the work done with a mouse is done with one hand and mostly with the

6.2. Selecting the Program: User Interface

225

index (cid:12)nger, whereas the keyboard lets the user distribute the work between two hands
and multiple (cid:12)ngers. In laptops, the mouse is usually located as a pointer in the middle of
the keyboard or as a touch pad. These require excellent coordination and precision of the
(cid:12)ngers, making the muscles extra tense. It is therefore important to use an external mouse
whenever possible.

\In baiting a mousetrap with cheese, always leave room for the mouse."
| Saki, in The Infernal Parliament

Examples of GUI shells for Unix/Linux operating systems (X Windows based) are KDE,
GNOME, Blackbox, and CDE. Examples of GUI shells for Microsoft Windows environments
are Windows Explorer, Litestep, Geoshell, BB4Win, and Emerge Desktop. Modern versions
of Microsoft’s Windows operating system o(cid:14)cially support only Windows Explorer as their
shell. Explorer provides the familiar desktop environment, start menu, and task bar, as
well as the (cid:12)le management functions of the operating system. Older versions also include
Program Manager, which was the Shell for the 3.x series of Microsoft Windows. Macintosh
Finder is a GUI for Apple. The newer versions of GUI shells improve end-user performance
by providing advanced menu options.

6.2.3 VUI Shells

The VUI environment provides a new dimension for the end-user to select programs to run
and to interact with the computer. It permits the user to speak command(s) directly into
a noise-cancelling microphone, with no intermediate keying or mouse clicks. The voice is
converted to digital form by the sound card, and is then processed by speech recognition
software. Voice recognition can be viewed as a problem de(cid:12)ned by three axes: vocabulary
size, degree of speaker independence, and continuous speech capability. Some of the impor-
tant issues in a VUI environment are speaker independence, continuous speech capability,
and (cid:13)exible vocabulary.

(cid:15) Vocabulary size: A large vocabulary size is bene(cid:12)cial, as it lets the end-user specify
a large number of unique commands. Some systems also permit customization of the
vocabulary.

(cid:15) Degree of speaker independence: Speaker independence allows a VUI to accept
and recognize (with high accuracy) commands spoken by many users, including voices
that were not part of its training set. Speaker-independent VUIs require no prior
training for an individual user.
In contrast, a speaker-dependent system requires
samples of speech for each individual user prior to system use (i.e., the user has to
train the system). Speaker-independence is desirable, as it permits the same computer
to be used by multiple end-users, without training sessions in between.

226

Chapter 6. Program Execution Basics

(cid:15) Degree of connectedness: This parameter deals with the extent to which words can
be slurred together. If a good deal of connectedness is allowed, the user can talk nat-
urally without pauses between words. The other extreme would sound like a teacher
in a locution class, mouthing each word. Depending on the degree of connectedness,
VUIs can be categorized as isolated word, connected word, and continuous speech. Iso-
lated word recognisers are the simplest, but require a short pause of approximately
1/5 second between each word. Connected word recognizers recognize words spoken
continuously, so long as the words do not vary as they run together, i.e., they require
clear pronunciation. Continuous speech allows the user to speak words as normally
spoken in (cid:13)uent speech, and is the most desirable one to have.

The simplest of the VUIs support a small vocabulary of speaker-dependent words, that
must be uttered with distinct pauses between each. On the other extreme would be a VUI
that supports a large vocabulary, and caters to a large number of speakers, who tend to
have slurred speech.

A typical o(cid:14)ce environment, with a high amplitude of background speech, turns out to
be one of the most adverse environments for current speech recognition technologies. Large-
vocabulary systems with speaker-independence that are designed to operate within these
adverse environments have been observed to have signi(cid:12)cantly lower recognition accuracy.
The typical recognition rate achievable as of 2005 for large-vocabulary speaker-independent
systems is about 80%-90% for a clear environment, and about 50% for a noisy environment
such as with a cellular phone.

Examples of voice recognition software are Apple Speech Recognition, DragonDictate (a
large vocabulary, speaker-adaptive voice recognition dictation system capable of recognizing
up to 120,000 words. It allows free-form dictation into most text-based applications.
It
allows full mouse movement as well as text and numerical dictation and complete formatting,
all by voice, completely hands free. DragonDictate uses industry standard third-party sound
cards.), NaturallySpeaking software, Voice Xpress package, and ViaVoice Pro.

In the past decade, tremendous advances in automatic speech recognition have taken
place. A reduction in the word error rate by more than a factor of 5 and an increase
in recognition speeds by several orders of magnitude (brought about by a combination of
faster recognition search algorithms and more powerful computers), have combined to make
high-accuracy, speaker-independent, continuous speech recognition for large vocabularies
possible in real time, on o(cid:11)-the-shelf workstations. These advances promise to make speech
recognition technology readily available to the general public.

Although VUI was introduced to combat the RSI problem inherent with keyboard and
mouse, it is not a magic wand either. Intensive use of VUI can sub ject the vocal cords to
overuse, and associated injury (vocal loading).

6.3. Creating the Process

227

\When I tel l you in few years it wil l be possible for you to sketch in the air and have
the thing you sketch in the air come to your eye, solid and real, so that you can walk
around it, so that you can scrutinize it from any direction and any view point you
please. I am tel ling you the truth."
| Steven A. Coons.
in a panel discussion entitled: The past and future of design
by computer, 1968

6.3 Creating the Process

6.4 Loading the Program

When the OS receives a command to execute a program, it transfers control to a loader (a
part of the OS), which reads the executable program into memory and initiates its execution.
When the program is loaded into memory, it is not just copied into memory and executed;
there are a number of steps the loader takes to load the image correctly and set up things
in that memory image before it jumps to the code in that image. The UNIX OS uses the
following steps to do loading; steps 2 and 3 may make sense only after studying virtual
memory, described in Chapter 9.

1. Read the executable (cid:12)le’s header and determine the size of the text and data sections.

2. Create a new virtual address space for the program. Allocate enough physical memory
to hold the text, data, and stack sections.

3. Copy the instructions and the data from the executable (cid:12)le onto the allotted physical
memory. If there is not enough physical memory, then copy at least the portion of
the text containing the entry point of the program, i.e., the (cid:12)rst instruction to be
executed.

4. Link and load libraries if load-time dynamic linking is to be done.

5. Copy onto the stack any parameters that are to be passed to the main function of the
program. Such parameters include command line arguments supplied by the user to
the OS shell.

6. Initialize the general-purpose registers. In general, most registers are cleared, but the
SP register is assigned the address of the top of stack.

7. Jump to the start-up routine at the beginning of the program.

228

Chapter 6. Program Execution Basics

6.4.1 Dynamic Linking of Libraries

In Chapter 6, we saw that libraries are often statically linked to the application program
while forming the executable binary. While this produces a \self-contained" executable
program, static linking of libraries does have some drawbacks. First and foremost, libraries
are generally quite large, and linking them to the static executable can result in very large
executables. Another drawback is that the executable cannot take advantage of newer
versions of the library when they become available, without re-doing the static linking.
Examples of libraries that are traditionally designed to be statically linked include the
ANSI C standard library and the ALIB assembler library.
Dynamic linking is a solution adopted to deal with these drawbacks 2 . With this scheme,
the library code is not stitched into the program executable by the static linker. Instead,
the static linker only records which libraries the program needs and their index names
or numbers. The libraries themselves remain in a separate (cid:12)le on disk. The ma jority of
the work of linking is done at program execution time. Such library routines are called
dynamical ly linked library or dynamic link library (DLL) routines. In Microsoft Windows
environments, dynamic libraries use the (cid:12)lename extension .dll. Two di(cid:11)erent options
exist for the time at which dynamic linking is done: load-time dynamic linking and run-time
dynamic linking.

6.4.1.1 Load-time dynamic linking

In this case, dynamic linking is done by the dynamic linker at the time the application
is loaded. The dynamic linker (cid:12)nds the relevant libraries on disk and links them to the
executable; these libraries are loaded at the same time to the process’ memory space. The
dynamic linker itself may be part of the library (as in Linux), or may be part of the OS
kernel (as in Windows ..). In the former case, the OS maps the dynamic linker to a part
of the process’ address space, and execution begins with the bootstrap code in the linker.
Some operating systems support dynamic linking only at load time, before the process starts
executing.

Lazy Procedure Linkage: Executables that call dynamic libraries generally contain
calls to a lot of functions in the library. During a single execution, many of these functions
may never be called. Furthermore, each dynamic library may also contain calls to functions
in other libraries, even fewer of which will be called during a given execution. In order to
reduce the overhead during program loading, dynamically linked ELF programs use lazy
binding of procedure addresses. That is, the address of a procedure is not bound until the
(cid:12)rst time the procedure is called.

2Although dynamic linking libraries have gained popularity only recently, they date back to at least the
MTS (Michigan Terminal System), built in the late 1960s [8].

6.4. Loading the Program

229

6.4.1.2 Run-time dynamic linking

In this case, the linking of a library is done just when it is actually referenced during the
execution of the process. This type of dynamic linking is often called delay loading.
Figure 6.2 illustrates how this type of dynamic linking occurs. When the user program
wants to call library routine S.dll (from address 0x401000), it uses a syscall to convey
the request to the dynamic loader part of the OS, as shown in part (a) of the (cid:12)gure. It
will also pass the name of the routine (S) to the OS. The dynamic loader loads S.dll (at
address 0x402000), as it was not previously loaded. This is shown in part (b) of the (cid:12)gure.
The loader then transfers control to S.dll. After the execution of S.dll is over, control
passes back to the dynamic loader, which then transfers control back to the user program
(at address 0x401004).

0x        1000
0x        1200

syscall

S.dll

0x        1000
0x        1200

syscall

S.dll

0x    400000

0x    400000

0x    400000

0x    401000

syscall

.text

0x    401000

syscall

.text

0x    401000

syscall

.text

0x80000080

0x80000080

0x80000080

0x80001000

Dynamic
Loader

OS space

0x80001000

Dynamic
Loader

OS space

0x80001000

Dynamic
Loader

OS space

$pc

0x    401000

$pc

0x80001100

$pc

0x        1000

(a) User program requests OS to load S.dll

(b) S.dll has been loaded

(c) S.dll is being executed

Figure 6.2: Memory Map During Di(cid:11)erent Stages of Dynamic Linking

One wrinkle that the loader must handle is that the memory location of the actual
library code is not known until after the executable and all dynamically linked libraries
have been loaded into memory, because the memory locations assigned will depend on
which speci(cid:12)c DLLs have been loaded. In theory, it is possible to examine the program
and replace all references to data in the libraries with pointers to the appropriate memory
locations once all DLLs have been loaded. However, this would require a large amount of

230

Chapter 6. Program Execution Basics

time and memory. Therefore, most dynamic library systems take a di(cid:11)erent approach. At
compile time, a symbol table with blank addresses called the import directory is linked into
the program. At load time, this table is updated with the location of the library code/data
by the loader/linker. At run time, all references to library code/data pass through the
import directory.

The library itself contains a table of all the methods within it, known as entry points.
Calls into the library "jump through" this table, looking up the location of the code in
memory, then calling it. This introduces overhead in calling into the library, but the delay
is usually so small as to be negligible.

Specifying the Library Routine: A dynamic link library routine can be speci(cid:12)ed in a
program executable in two di(cid:11)erent ways. The (cid:12)rst option is to specify the path that locates
the library within the OS (cid:12)le system. Any change to the library naming or layout of the (cid:12)le
system will cause these systems to fail. In the second option | the more common one | only
the name of the library routine (and not the path) is speci(cid:12)ed in the executable, with the
operating system incorporating a mechanism to locate the library in the (cid:12)le system. Unix-
based systems have a list of \places to look" in a con(cid:12)guration (cid:12)le, and dynamic library
routines are placed in these places. On the downside this can make installation of new
libraries problematic, and these "known" locations quickly become home to an increasing
number of library (cid:12)les, making management more complex. Microsoft Windows will check
the Registry to determine the proper place to (cid:12)nd an ActiveX DLL, but for standard DLLs
it will check the current working directory; the directory set by SetDllDirectory(); the
System32, System, and Windows directories; and (cid:12)nally the PATH environment variable.

6.5 Executing the Program

After the loader loads the selected program into memory and completes all steps of dynamic
linking, it transfers control to the program. That is, the program counter is set to the entry
point of the program. Thereafter, the processor executes the loaded program by executing
the instructions (in the .text portion) of the program. Execution of these instructions
involves repeating two actions: fetch and execute. Fetch concerns obtaining the instruction
bit pattern from memory and decoding the bit pattern. Execution involves carrying out
the action speci(cid:12)ed in the instruction, including determining the next instruction to be
executed. The fetch and execute phases can be subdivided into the following sequence of
steps (some of which are optional):

6.6. Halting the Program

231

Executing the ML Program

(cid:15) Fetch Phase

1. Fetch instruction: Read the next instruction from the memory.

2. Decode instruction: Decode the instruction bit pattern so as to determine
the action speci(cid:12)ed by it.

(cid:15) Execution Phase

1. Fetch source operand values: Fetch source operands (if any) from regis-
ters, memory locations, or IO registers.

2. Process data: Perform arithmetic or logical operation on source operand
values, if required.

3. Write result operand value: Write the result of an arithmetic or logical
operation to a register, memory location, or IO register, if required.

4. Update program counter: Normally, the program counter needs to be in-
cremented so as to point to the immediately following instruction. When
a control (cid:13)ow change is required because of a branch instruction or the
like, the program counter is updated with the new target address deter-
mined in the execution phase.

6.6 Halting the Program

6.7

Instruction Set Simulator

We have seen the fundamental steps involved in executing a (machine language) program.
Although the operating system plays a ma jor role in setting up the stage, the bulk of
the work | interpreting the program | is usually carried out directly in hardware. A
hardware interpreter is more popular because of its superiority in performance and power
consumption. As pointed out in Chapter 1, however, the distinction between hardware and
software is somewhat blur. We can in fact implement an emulator in software; such an
emulator is generally called a software simulator. The SPIM simulator that we discussed
in Chapter 3 is a good example. It loaded MIPS assembly language programs, translated
them to MIPS machine language (ML) programs, and then executed them by interpreting
the ML instructions, one by one. Partly to prove this point, and more importantly, to
start with a simple emulator, we shall (cid:12)rst study a software emulator, and then move on to
hardware interpreters in the next chapter.

It is important to note that a software simulator program can perform its emulation

232

Chapter 6. Program Execution Basics

of a target ISA only when the simulator is executed on another microarchitecture (called
host machine), entailing an interpretation by the host machine’s control unit. Thus, imple-
menting a microarchitecture in software introduces an extra interpretation step for the ML
program. Why in any case, would anyone want to introduce extra interpretation steps, other
than for pedagogic reasons? Although software simulation may not make much sense for
ordinary users, software simulation is the de facto tool of the trade for computer architects
who are in the business of developing new ISAs and microarchitectures.

Another common situation that warrants the use of a software simulator is in executing
Java applets that are supplied over the web. Java applets are compiled and assembled into
bytecode, with the Java Virtual Machine (JVM) as the target ISA. Java bytecode
can be directly executed on hardware only if the hardware implements the JVM ISA. When
a web browser running on a di(cid:11)erent platform downloads a Java applet, it interprets the
bytecode using a software interpreter, as illustrated in Figure 6.3. This permits the web
server to supply the same bytecode irrespective of the platform from which web browsing
is done.

JVM
Bytecode

Java Class
Libraries

JVM

JVM Interpreter

Host OS

Host ISA

Host Hardware

Figure 6.3: Execution of a Java Application by Interpretation

It is important to note that this simulator uses a behavioral approach, and models only
the functional aspects of emulation, without doing a hardware microarchitecture design. A
functional simulator thus correctly executes machine language programs, without modeling
any hardware devices. Such a simulator has no notion of hardware-speci(cid:12)c features such as
clock cycles and buses. It just takes in as input the program to be executed (along with
the executed program’s inputs), and outputs (among other things) the outputs generated
by the executed program. Examples are the SPIM simulator and the JVM interpreter.

We shall develop a simple functional simulator here. For simplicity and ease of under-
standing, we restrict ourselves to a subset of the MIPS-I ML instruction set which we call

6.7.

Instruction Set Simulator

233

MIPS-0. The encodings of the MIPS-0 instructions and the semantics are same as that in
the MIPS-I ISA. The MIPS-0 instruction set, along with the instruction encodings, is given
in Figure 6.4.

LUI

rt, immed

001111

LW

rt, offset(rs)

100011

SW rt, offset(rs)

101011

ADDI

rt, rs, immed

001000

ADDU rd, rs, rt

000000

SUBU

rd, rs, rt

000000

ANDI

rt, rs, immed

001100

AND

rd, rs, rt

000000

ORI

rt, rs, immed

001101

OR

rd, rs, rt

000000

NOR

rd, rs, rt

000000

SLLV

rd, rs, rt

000000

BEQ rs, rt, offset

000100

BNE rs, rt, offset

000101

JALR

rd, rs

JR

rs

SYSCALL

000000

000000

000000

rs

rs

rs

rs

rs

rs

rs

rs

rs

rs

rs

rs

rs

rs

rs

rt

rt

rt

rt

rt

rt

rt

rt

rt

rt

rt

rt

rt

rt

immed

offset

offset

immed

immed

immed

offset

offset

100001

100011

100100

100101

100111

000100

001001

001000

001100

rd

rd

rd

rd

rd

rd

rd

Figure 6.4: The MIPS-0 Instruction Set, along with Encoding

6.7.1

Implementing the Register Space

Let us take a closer look at designing functional simulators. First, we will see how the register
model speci(cid:12)ed in the ISA is implemented in the simulator. This can be accomplished by
declaring an appropriate data structure (usually an array) in the simulator.

/* Allocate space for modeling the ISA-visible registers */
long R[32];
long PC;

234

Chapter 6. Program Execution Basics

Apart from the ISA-visible registers, many new \registers" may need to be de(cid:12)ned, for
storing temporary values.

6.7.2

Implementing the Memory Address Space

Conceptually, the memory address space of the target ISA can be implemented in a manner
similar to what we did for the register space. Notice that with this simple approach, either
the target’s memory space should be smaller than that of the host, or the entire memory
space of the target is not simulated.

Host’s User Memory Space

Text

Data

Registers

Target’s Simulated
Register Space

Target’s Simulated
Memory Space

Text

Data

Stack

Stack

Figure 6.5: Implementing the Register Space and Memory Space in a Functional Simulator

The simulator maintains the mapping between the target addresses and the host ad-
dresses. When the simulated program accesses a memory location, the simulator translates
that address to a host memory address.

/* Allocate space for modeling the 3 sections of the memory address space */
long text[0x800000];
long data[0x800000];

6.7.

Instruction Set Simulator

235

long stack[0x800000];

6.7.3 Program Loading

The binary program to be executed (i.e., the ML program whose execution needs to be
simulated) is typically stored as a (cid:12)le in the host machine’s (cid:12)le system. Prior to execution
by the simulator, this program needs to be loaded into the target’s instruction memory
which is modeled in the simulator. This loading job is quite similar to the job done by the
loader part of the OS when a user requests a program to be executed directly in a machine.
The simulator needs to have the functionality to perform this \loading".

6.7.4

Instruction Fetch Phase

/* Fetch the instruction from the text section of memory */
IR = text[PC - 0x400000];

/* Separate the different fields of the instruction */
opcode = [IR & 0xfc000000] >> 26;
rs
= [IR & 0x3e00000] >> 21;
= [IR & 0x1f0000] >> 16;
rt
= [IR & 0xf800] >> 11;
rd
func
= IR & 0x3f;
offset = IR & 0xffff;

/* Decode the instruction */
switch (opcode)
{

case 000000b:

case 000100b: /* beq */

case 000101b: /* bne */

case 001000b: /* addi */

}

6.7.5 Executing the ML Instructions

Once the program to be executed is loaded into the simulator’s memory, execution of the
program (i.e., simulation of the execution) can begin. The PC variable is initialized to the

236

Chapter 6. Program Execution Basics

program starting address. The simulator then goes through a loop, with each iteration of
the loop implementing the fetching and execution of an ML instruction.

The simulation of the machine is done by just a big function in the simulator. This
function understands the format of MIPS instructions and the expected behavior of those
instructions as de(cid:12)ned by the MIPS architecture. When the MIPS simulator is executing
a \user program", it simulates the behavior of a real MIPS CPU by executing a tight
loop, fetching MIPS instructions from the simulated machine memory and \executing"
them by transforming the state of the simulated memory and simulated machine registers
according to the de(cid:12)ned meaning of the instructions in the MIPS architecture speci(cid:12)cation.
Remember that the simulated machine’s physical memory and registers are data structures
in the simulator program.

6.7.6 Executing the Syscall Instruction

Execution of syscall instructions in the simulator is somewhat complex due to a variety of
reasons. First of all, the binary program being \executed" does not contain the Kernel code
that needs to be executed upon encountering a syscall instruction; this code is a part of the
operating system of the target machine. Therefore, instead of simulating the execution of
Kernel code on an instruction-by-instruction basis, the simulator directly implements the
functionality speci(cid:12)ed by the syscall instruction.

Secondly, many of the syscall instructions require intervention of the host machine’s
operating system. For instance, if a syscall instruction speci(cid:12)es a (cid:12)le to be opened (open()
system call), only the host machine’s operating system can open the (cid:12)le; remember that
the simulator is just an Application program (User mode program) that runs on the host
machine. Similarly, if a syscall instruction speci(cid:12)es another process to be created (fork()
system call), only the host machine’s operating system can create another process. For
such system calls, the simulator should act as an interface to the host machine’s operating
system. Some other system calls, such as brk() (which requests the operating system to
increase the size of the heap memory segment), on the other hand, do not require any
intervention from the host operating system. Instead, the simulator itself will act as the
operating system for the executed program.

The simulator’s \kernel" controls the simulated machine in the same way that a real
OS kernel controls a real machine. Like a real kernel on a real machine, the simulator’s
kernel can direct the simulated machine to begin executing code in user mode at a speci(cid:12)c
memory address. The machine will return control to the kernel if the simulated user program
executes a syscall or trap instruction, or if an interrupt or other machine exception occurs.

Like a real kernel, the simulator’s kernel must examine and modify the machine registers
and other machine state in order to service exceptions and run user programs. For example,
system call arguments and results are passed between a user program and the kernel through
the machine’s registers. The kernel will also modify page table data structures that are used
by the simulated machine for translating virtual addresses to physical addresses. From the

6.8. Hardware Design

237

perspective of the simulator’s kernel, all of the machine state { registers, memory, and page
tables { are simply data structures (most likely arrays) in the kernel address space??

Figure needed here.

6.7.7 Comparison with Hardware Microarchitecture

In the next chapter, we will study hardware microarchitectures, which interpret machine
language programs using hardware circuitry. It is informative to compare and contrast the
functionality performed in traditional microarchitectures and simulators.

HW Microarchitecture
Attribute
Hardware registers
Register space
Memory address space Hardware memory structures
Loader part of OS
Loading
Interpretation
Control unit hardware

SW Microarchitecture
Simulator program variables
Simulator program variables
Loader part of simulator

Table 6.1: A Succinct Comparison of Hardware and Software Microarchitectures

6.8 Hardware Design

6.8.1 Clock

Computer hardware, like other synchronous digital circuits, use a clock signal to coordinate
the actions of two or more circuit blocks. A clock signal oscillates between a high and a low
state, normally with a 50% duty cycle. In other words, the signal is a square wave. The
circuits using the clock signal for synchronization may become active at either the rising or
falling edge, or both, of the clock signal. Processor clock speed or clock rate is an indicator of
the speed at which the processor executes instructions. Every computer hardware contains
an internal clock that regulates the rate at which instructions are executed and synchronizes
all the various computer components. The processor requires several clock ticks (or clock
cycles) to execute each instruction. The faster the clock, the more instructions the processor
can generally execute per second. Clock speeds are expressed in megahertz (MHz) or
gigahertz (GHz).

6.8.2 Hardware Description Language (HDL)

The two ma jor HDLs are Verilog and VHDL (Very high speed integrated circuits HDL).
Both of these are equally popular and have roughly equal market presence.

238

Chapter 6. Program Execution Basics

Machine−Level Architecture
(Instruction Set Architecture)

User Mode

Kernel Mode Designed by Instruction Set Architect

User Program (ML)

Interpretation
by OS code

OS Program (ML)

Control Unit (Interpretor)

Microarchitecture

PC     MAR

Logic−Level Architecture
PC_out, MAR_in

Microarchitectural       Data Path
(RFs, Caches,      ALUs, Buses)

Designed by Microarchitect

Microinstruction

Microsequencer (Interpretor)

Logic−Level         Data Path
(Gates, Flip−flops, MUXes, ROMs)

Designed by Logic Designer

Control Signals

Device Control Inputs (Implementor)

Device−Level Architecture

Device−Level            Data Path
(Transistors, Wires, Layouts)

Designed by VLSI Designer

Figure 6.6: Machine Abstractions relevant to Program Execution, along with the Ma jor
Components of each Abstract Machine.

6.8.3 Design Speci(cid:12)cation in HDL

6.8.4 Design Veri(cid:12)cation using Simulation

6.8.5 Hardware Design Metrics

6.8.5.1 Technology Trends

Moore’s Law:

6.8.5.2 Performance

\A man with a watch knows what time it is.
A man with two watches is never sure."
| Segal’s Law

The performance of a computer depends on a number of factors, many of which are re-
lated to the design of the processor data path. Three of the most important factors are the
strength of the machine language instructions, the number of clock cycles required to fetch

6.9. Concluding Remarks

239

and execute a machine language instruction, and the clock speed. A powerful instruction
performs a complex operation and accomplishes more than what a simple instruction ac-
complishes, although its execution might take several additional clock cycles. The strength
of instructions is an issue that is dealt with at the ISA level (taking into consideration
microarchitectural issues), and is not under the control of the microarchitect.

Clock speed has a ma jor in(cid:13)uence on performance, and depends on the technology used
to implement the electronic circuits. The use of densely packed, small transistors to fabricate
the digital circuits leads to high clock speeds. Thus, implementing the entire processor on
a single VLSI chip allows much higher clock speeds than would be possible if several chips
were used. The use of simple logic circuits also makes it easier to clock the circuit faster.
Clock speed is an issue that is primarily dealt with at the design of logic-level architectures,
although microarchitectural decisions have a strong bearing on the maximum clock speeds
attainable.

The third factor in performance, namely the number of clock cycles required to fetch
and execute an instruction, is certainly a microarchitectural issue. In the last several years,
speed improvements due to better microarchitectures, while less amazing than that due to
faster circuits, have nevertheless been impressive. There are two ways to reduce the average
number of cycles required to fetch and execute an instruction: (i) reduce the number of
cycles needed to fetch and execute each instruction, and (ii) overlap the interpretation
of multiple instructions so that the average number of cycles per instruction is reduced.
Both involve signi(cid:12)cant modi(cid:12)cations to the processor data path, primarily to add more
connectivity and latches.

6.8.5.3 Power Consumption

6.8.5.4 Price

6.9 Concluding Remarks

6.10 Exercises

1. Explain the di(cid:11)erence between static, load-time, and lazy (fully dynamic) linking.
What are the advantages and disadvantages of each type of linking?

2. Write a software simulator program (in any high-level language) that can interpret
machine language programs written for the MIPS-0 ISA.

240

Chapter 6. Program Execution Basics

Chapter 7

Microarchitecture | User Mode

Listen to counsel and receive instruction, That you may be wise in your latter days.

Proverbs 19: 20

Executing a program involves several steps, as highlighted in the previous chapter. These
steps are carried out by di(cid:11)erent entities: program selection is typically done by the end user;
process creation, program loading, and process halting are typically done by the operating
system. Program execution | the bulk of the e(cid:11)ort | generally falls on the processor’s
shoulders1 . The processor is a hardware entity.
Our ob jective in this chapter is to implement the machine speci(cid:12)cation given in the
instruction set architecture (ISA). Because of the complexity of this machine, it is imprac-
tical to directly design a gate-level circuitry that implements the ISA. Therefore, computer
architects have taken a more structured approach by introducing one or more hardware
abstraction levels in between2 . The abstraction level directly below the instruction set ar-
chitecture is called the microarchitecture. This level is responsible for executing machine
language programs.

In this chapter, we study the organization and operation of the di(cid:11)erent components
that constitute the user mode microarchitecture, the machine that is responsible for imple-
menting the user mode instruction set architecture (ISA)|i.e., carrying out the execution
of user mode machine language programs. A microarchitectural view of a computer is re-
stricted to seeing the ma jor building blocks of the computer, such as register (cid:12)les, memory
units, ALUs and other functional units, and di(cid:11)erent buses.

1Even the steps carried out by the operating system involve work by the processor, as the operating
system itself is a process that is executed by the processor, in kernel mode.
2Even the design of simple digital circuits involves a somewhat structured approach. When designing a
circuit as a (cid:12)nite state machine, we (cid:12)rst construct the state transition diagram, and then implement the
state transition diagram.

241

242

Chapter 7. Microarchitecture | User Mode

A particular ISA may be implemented in di(cid:11)erent ways, with di(cid:11)erent microarchitec-
tures. An executable program developed for this ISA can be executed on any of these
microarchitectures without any change, regardless of their di(cid:11)erences. For example, the
Intel IA-32 ISA has been implemented in di(cid:11)erent processors such as Pentium III, Pentium
4, and Athlon. Some of these processors are built by Intel, and the others by competitors
such as AMD and Cyrix. Even when using the same processor type, computer vendors build
many di(cid:11)erent system microarchitectures by putting together processors, memory modules,
and IO interface modules in di(cid:11)erent ways. One microarchitecture might focus on high
performance, whereas another might focus on reducing the cost or the power consumption.
The ability to develop di(cid:11)erent microarchitectures for the same ISA allows processor ven-
dors and memory vendors to take advantage of new IC process technology, while providing
upward compatibility to the users for their past investments in software. Although our
discussion may seem to hint that the microarchitecture is always implemented in hardware,
strictly speaking, the microarchitecture only speci(cid:12)es an abstract model. This abstract ma-
chine can be implemented in software if needed. Examples are the SPIM simulator we saw
in Chapter 4 (which is a functional simulator) and the SimpleScalar simulator [?] (which is
a cycle-accurate simulator).

Like the design of the higher level architectures that we already saw, microarchitecture
design is also replete with trade-o(cid:11)s. The trade-o(cid:11)s at this level involve characteristics such
as speed, cost, power consumption, die size, and reliability. For general-purpose computers
such as desktops, one trade-o(cid:11) drives the most important choices the microarchitect must
make: speed versus cost. For laptops and embedded systems, the important considerations
are size and power consumption. For space exploration and other critical applications,
reliability is of primary concern.

Present day computer microarchitecture is incredibly complex. Fortunately, the two
principles that enabled computer scientists to develop high-level architectures that support
million-line programs|modularization and partitioning|are also available to computer en-
gineers to design complex microarchitectures that meet various requirements. By breaking
up a microarchitecture into multiple blocks, the design becomes much more tractable. For
simplifying the discussion, this chapter presents only microarchitectural features that are
speci(cid:12)c to implementing the user mode features of the ISA. Chapter 8 deals with microar-
chitectural aspects that are speci(cid:12)c to implementing the kernel mode features of the ISA.
The current chapter addresses some of the fundamental questions concerning user mode
microarchitecture, such as:

(cid:15) What are the building blocks in a computer microarchitecture, and how are they
connected together?

(cid:15) What steps should the microarchitecture perform to sequence through a machine
language program, and to execute (i.e., accomplish the work speci(cid:12)ed in) each machine
language instruction?

(cid:15) What are some simple organizational techniques that can reduce the number of time-

7.1. Overview of User Mode Microarchitecture

243

steps needed to execute each machine language instruction?

(cid:15) What are some techniques commonly used to reduce the latency of memory accesses?

Go to the ant, you sluggard; consider its ways and be wise! It has no commander, no
overseer or ruler, yet it stores its provisions in summer and gathers its food at harvest.
Proverbs 6: 6-8

7.1 Overview of User Mode Microarchitecture

A commonly used approach for designing hardware | especially small to medium size
circuits | is the (cid:12)nite state machine approach. Finite state machine-based design involves
identifying a suitable state variable, identifying all possible states, and then developing a
state transition diagram. Theoretically, the computer hardware can also be built in this
manner as a single (cid:12)nite state machine; it is, after all, a digital circuit. However, such a
(cid:12)nite state machine would have far too many states (a single bit change in a single memory
location changes the system state), making it extremely di(cid:14)cult to comprehend the complex
functionality, let alone design one in an e(cid:14)cient manner.

7.1.1 Dichotomy: Data Path and Control Unit

In order to tackle the above state explosion, microarchitects partition the computer hard-
ware into a data path and a control unit3 . The data path serves as the platform for executing
machine language (ML) programs, whereas the control unit serves as the interpreter of ML
programs, for execution on the data path. This point needs further clari(cid:12)cation. In order to
execute a machine language program on the data path, the program has to be interpreted.
This interpretation is done by the control unit. The relation between the interpreter and
the microarchitecture is pictorially depicted in Figure 7.1.

ML Program

Control Unit

(Interpreter)

Status

Control

Data Path

Figure 7.1: Relation between the Data Path and the Control Unit (Interpreter)

3The data path itself is partitioned into several smaller data paths (with their own miniature control
units) to keep the design even simpler. Thus, the memory unit is designed as a separate data path, with its
own memory controller.

244

Chapter 7. Microarchitecture | User Mode

The data path incorporates circuitry and interconnections that are required for executing
each of the instructions de(cid:12)ned in the instruction set architecture, and serves as a platform
for executing ML programs. It is a collection of storage components such as registers and
memories, functional units such as ALUs, multipliers, and shifters, as well as interconnects
that connect these components. The movement of data through the data path is controlled
by the control unit. Microarchitects use a language called micro-assembly language
(MAL) to indicate the control actions speci(cid:12)ed by the control unit. In order to execute
ML programs on the data path, ML programs are interpreted in terms of MAL commands.
This interpretation is performed by the control unit, which supplies the MAL commands
to the data path. Because a signi(cid:12)cant portion of the functionality has been shifted to
the data path, the control unit can be conveniently built as a (cid:12)nite state machine with a
manageable number of states. A change in a main memory value does not cause a state
transition in the control unit, for instance.

The sequence of operations performed in the data path is determined by commands
generated by the control unit. The control unit thus functions as the interpreter of machine
language programs.

The computer data path can be easily divided into ma jor building blocks and subsystems
based on how data (cid:13)ow is speci(cid:12)ed in the ISA. That is, we break the functionality speci(cid:12)ed
in the machine language into di(cid:11)erent parts, and use separate building blocks to implement
each of the parts in the data path. Thus at the system level, we see the computer hardware
in terms of ma jor building blocks and their interconnections.

In order to design a data path for implementing a user mode ISA we need to consider
what the user mode ISA requires for data to (cid:13)ow through the system. There are two aspects
to consider here:

(cid:15) The storage locations (register name space and memory address space) de(cid:12)ned in the
user mode ISA

(cid:15) The operations or functions de(cid:12)ned in the user mode ISA.

The data path of a computer is built from building blocks such as registers, memory
elements, arithmetic-logic units, other functional units, and interconnections. Assumptions
about the behavior of these building blocks become speci(cid:12)cations of the data path, which
are then conveyed to the logic-level designer of the data path.

7.1.2 Register File and Individual Registers

A microarchitecture de(cid:12)nes one or more register (cid:12)les, each consisting of several physical
registers. The register (cid:12)les are used to implement the integer and (cid:13)oating-point register
name spaces de(cid:12)ned in the ISA. Figure 7.2 depicts a register (cid:12)le. It has an address input
for specifying the register to be read from or written into. Any speci(cid:12)c register in the
register (cid:12)le can be read or written by placing the corresponding register number or address

7.1. Overview of User Mode Microarchitecture

245

in the address input lines. The data that is read or written is transmitted through the
data lines. The data lines can be bidirectional as shown in the (cid:12)gure, or can be two sets of
unidirectional lines.

Register Number

R

Register File

R
2   Registers

N

Data

Figure 7.2: An N -bit wide Register File containing 2R Registers

Apart from register (cid:12)les, several individual hardware registers are also included in the
microarchitecture. Some of these registers can be used to implement other ISA-de(cid:12)ned
registers such as pc, sp, and flags. Individual registers and register (cid:12)les that implement
registers de(cid:12)ned in the ISA are often called ISA-visible registers.

Register (cid:12)les as well as individual registers that are not ISA-visible are invisible to the
machine language programmer, and are called microarchitectural registers. These serve as
temporary storage for values generated or used in the midst of instruction execution in
the microarchitecture. The data path may also need to keep track of special properties of
arithmetic and logical operations such as condition codes; most data paths use a register
called flags4 . Notice that the total number of registers provided in the microarchitecture
is higher than the number of registers de(cid:12)ned in the ISA.

7.1.3 Memory Structures

A microarchitecture de(cid:12)nes one or more memory structures, each containing very large
numbers of memory locations. These memory structures are useful for implementing the
memory address space and sections de(cid:12)ned in the ISA. Each memory structure resembles a
very large register (cid:12)le. Like a register (cid:12)le, each memory structure has an address input for
specifying the location to be read from or written into. Thus, any speci(cid:12)c memory location
can be read or written by placing the corresponding memory address in the address input
lines. The di(cid:11)erent storage elements | memory structure(s) as well as register (cid:12)le(s) |
are pictorially shown in Figure 7.3.

4 In some architectures such as the IA-32, the flags register is visible at the ISA level (usually as a set
of condition codes). In MIPS-I, it is not visible at the ISA level.

246

Chapter 7. Microarchitecture | User Mode

PC

Register File

Registers

Memory

Memory Locations

Figure 7.3: Implementing the Storage Locations De(cid:12)ned in the User Mode ISA

7.1.4 ALUs and Other Functional Units

We have just looked at the storage elements present in a microarchitecture. Next, we shall
consider the elements that serve to perform the operations and functionalities speci(cid:12)ed in
the ISA. In order to do this, let us review the functions the microarchitecture must perform
so as to execute instructions. Some of the important functionalities are listed below.

(cid:15) Data transfer between memory locations and registers.

(cid:15) Data transfer between registers

(cid:15) Arithmetic/logical operations on data present in registers

Next, let us consider equipping the data path to perform arithmetic and logical oper-
ations, such as addition, subtraction, AND, OR, and shift. For carrying out operations,
the microarchitecture de(cid:12)nes hardware circuitry that can perform di(cid:11)erent operations on
bit patterns, and produce result bit patterns in the speci(cid:12)c data encodings used. Many
microarchitectures consolidate the hardware circuitry for performing arithmetic and logical
operations on integer data into a single multi-function block called arithmetic and logic unit
(ALU). An ALU takes data inputs as well as control inputs, as depicted in Figure 7.4. The
two main sets of data inputs are marked as X and Y . The control inputs indicate the spe-
ci(cid:12)c arithmetic/logic function to be performed, and the data type of the inputs. An ALU

7.1. Overview of User Mode Microarchitecture

247

typically performs functions such as addition and subtraction (for signed integers as well as
unsigned integers), AND, OR, NAND, NOR, left shift, logical right shift, and arithmetic
right shift.

Hardware circuitry for performing complex operations such as integer multiplication,
integer division, and all types of (cid:13)oating-point operations are usually not integrated into
the ALU; instead, these are typically built as separate functional units. This is because
these operations typically require much longer times than that required for integer addition,
integer subtraction, and all logical operations.
Integrating all of these operations into a
single ALU forces every operation to be as slow as the slowest one.

Carry out
Overflow

X

N

Y

N

N−bit
ALU

N

A

k

F
Data Type

Figure 7.4: An N -bit ALU Capable of Performing 2k Functions

7.1.5

Interconnects

It is not su(cid:14)cient to implement the storage locations and the functional units in a microar-
chitecture. For meaningful operations to be carried out in a data path, it is important
that the storage elements and the functional units be connected properly. To achieve a
reasonable speed of operation, most of these connections transfer a full word in parallel
over multiple wires. The connections can be either buses or direct paths.

A bus connects together a large number of blocks using a single set of wires, and is a
shared communication link. Multiple blocks can communicate over a single bus, at di(cid:11)erent
times. At any particular instant, data can be transferred from a single block to one or more
blocks. Thus a bus permits broadcasting of data to multiple destination blocks. In addition
to the wires that carry the data, additional wires may be included in a bus for addressing
and control purposes. One point to note here is that a bus allows only a single transfer at
a time, and so, if a data path has only a small number of buses, then only very few data
transfers can happen in parallel in that data path.

A direct path, unlike a bus, is a point-to-point connection that connects exactly two
blocks. A direct path-based interconnect therefore consists of a collection of point-to-point
connections between di(cid:11)erent blocks. Direct paths tend to be faster than buses, due to two
reasons. First, their wires have only two connections, and this results in less capacitance

248

Chapter 7. Microarchitecture | User Mode

than that in buses, which have many connections. Secondly, direct paths tend to be much
shorter than buses, and so have lower transmission delays. Direct paths are particularly
suited for connecting hardware blocks that are physically close and communicate frequently.
The downside is that a direct path based data path tends to have too many paths, requiring
a large chip area for the wires.

The di(cid:11)erent blocks in a microarchitecture can be interconnected in a variety of ways.
The type of interconnects and the connectivity they provide determine, to a large extent,
the time it takes to execute a machine language instruction. A typical data path has too
many hardware blocks for every block to be connected to every other block by point-to-
point connections. What would be a good interconnect to use here? The simplest type of
connection that we can think of is to use a single bus to connect all of them, as illustrated
in Figure 7.5. In this (cid:12)gure, all of the storage elements (including the microarchitectural
registers) and the ALU are connected together by a single bus called system bus.

PC

Register File

ISA−visible Registers

s
u
B
 
m
e
t
s
y
S

Memory

Memory Addresses

IR

flags

Microarchitectural Registers

ALU

Functional Units

Figure 7.5: Interconnecting the ISA-visible Registers, Microarchitectural Registers, and the
ALU using a Single Bus to form a Computer Data Path

7.1. Overview of User Mode Microarchitecture

249

7.1.6 Processor and Memory Subsystems

In the discussion so far, the registers, the memory structures, and the functional units are
all interconnected with a single bus, called the system bus. The use of a single bus has
several limitations:

(cid:15) It permits only a single data transfer between the di(cid:11)erent units at any given instant.

(cid:15) Some of the transfers may be very fast (those involving only registers), whereas some
others (those involving memory) are very slow. The di(cid:11)erence in transfer rates neces-
sitates an asynchronous protocol for the system bus, which introduces an overhead to
the register-only transfers, which are more frequent.

(cid:15) The bus may have high capacitance due to two reasons: (i) it is very long so as to
connect all devices in the system, and (ii) each device that is hooked up to the bus
adds to the capacitance. To reduce the capacitance, each device must have expensive
low-impedance drivers to drive the bus.

In order to improve performance, we typically organize the user mode microarchitecture
as two subsystems: the processor subsystem and the memory subsystem.
This
is depicted in Figure 7.6. In such an organization, the registers and functional units are
integrated within the processor subsystem, and the memory structures are integrated within
the memory subsystem. Because the functional units work with values stored in registers,
they are typically included in the processor subsystem where the registers are present. The
memory subsystem is traditionally placed outside the processor chip 5 .
Another notable change is that we now have two buses|a processor bus and a system
bus. The processor bus is used to interconnect the blocks within the processor subsystem,
and the system bus is used to connect the two subsystems. The processor subsystem also
includes a memory interface to act as a tie between the two buses. The use of two buses
enables multiple transfers to happen in parallel. For instance, data could be transferred
from the register (cid:12)le to the ALU, while data transfer is taking place between the memory
subsystem and the memory interface.

7.1.7 Micro-Assembly Language (MAL)

As mentioned earlier, the data path does not do anything out of its own volition; it merely
carries out the elementary instruction that it receives from the control unit. Thus, it is
the control unit that decides what function should be carried out by the processor data
path in a particular clock cycle. Likewise, the function to be performed by the execution

5As we will see later in this chapter, the memory subsystem of a typical computer includes multiple
levels of cache memories, of which the top one or two levels are often integrated into the processor chip.
With continued advancements in VLSI technology, a number of researchers are even working towards the
integration of the processor and memory subsystems into a single chip.

250

Chapter 7. Microarchitecture | User Mode

System Bus

Memory
Interface

PC

IR

s
u
B
 
r
o
s
s
e
c
o
r
P

Register File

Memory

ALU

flags

Processor Subsystem

Memory
Subsystem

Figure 7.6: Organizing the User Mode Microarchitecture as a Combination of Processor
and Memory Subsystems

unit in a clock cycle is also decided by the control unit. For ease of understanding, we
shall use an assembly-like language to express these elementary instructions sent by the
control unit to the data path. This language uses mnemonics similar to those used in the
assembly languages studied in Chapters 3 and 4, and is called a micro-assembly language
or MAL. Accordingly, an elementary instruction represented in this language is called a
micro-assembly language command or a MAL command for short. A MAL command may
be composed of one or more elementary operations called MAL operations, performed on
data stored in registers or in memory. A MAL operation can be as simple as copying data
from one physical register to another, or more complex, such as adding the contents of two
physical registers and storing the result in a third physical register.

We shall deal with the speci(cid:12)cs of the data transfer when we look at RTL architectures
in Chapter ??.

7.2. Example Microarchitecture for Executing MIPS-0 Programs

251

7.2 Example Microarchitecture for Executing MIPS-0 Pro-
grams

Designing a computer microarchitecture is better caught than taught. Accordingly, in this
chapter we will design several example microarchitectures, and illustrate the principles
involved. Although a microarchitecture can be designed in a generic manner to support
a variety of ISAs, such an approach is rarely taken.
In practice, each microarchitecture
is designed with a speci(cid:12)c ISA in mind6 . The primary reason for this is performance. If
we design a generic microarchitecture to support the idiosyncrasies of a number of ISAs
(like the XXL size T-shirts distributed during class reunion ceremonies), then that data
path is likely to be signi(cid:12)cantly slower than one that caters to a speci(cid:12)c ISA. In accordance
with this practice, our microarchitecture designs are also for a speci(cid:12)c ISA. For the sake of
continuity with the preceding chapters, these microarchitectures are designed for executing
MIPS ISA instructions7 . For simplicity and ease of understanding, we restrict ourselves to
a representative subset of the MIPS-I instruction set which we call MIPS-0. We will (cid:12)rst
discuss a simple example microarchitecture in detail, and later move on to more complex
microarchitectures. This simple microarchitecture will be used in the discussions of the
control unit as well.

In order to design a microarchitecture for the MIPS-0 ISA, we (cid:12)rst consider the ISA-
de(cid:12)ned storage locations that are typically implemented within the processor subsystem.
These include the general-purpose registers and the special registers. The MIPS ISA de(cid:12)nes
32 general-purpose registers, R0 - R31, and we use a 32-entry register (cid:12)le to implement
them. In each clock cycle, this register (cid:12)le can accept a single address input for specifying
the register to be read from or written into. A special register called PC is used to store the
memory address of the next instruction to be interpreted and executed. Apart from these
ISA-visible registers, we shall include the following microarchitectural register: flags to
store important properties of the result produced by the ALU. The contents of the flags
register can be used to perform selective updates to PC.

We shall use a single multi-function ALU (Arithmetic and Logic Unit) to perform the
arithmetic and logical operations speci(cid:12)ed in the MIPS-0 ISA. More advanced designs may
use a collection of specialized functional units.

We shall design the data path as a combination of a processor subsystem and a memory
subsystem, as discussed in Section 7.1.6. The processor subsystem includes the registers,
the ALU, and the interface to the memory subsystem. We shall use a single 32-bit processor
bus to interconnect the hardware structures inside the processor subsystem, and another

6This is in contrast to the practice followed in designing an assembly-level architecture, where the design
is not tailored for any particular high-level language.
7The microarchitecture presented in this section is somewhat generic, and is not necessarily close to the
ones present in any of the commercial MIPS processors.
In Sections 7.6.2 and 7.7, we present microar-
chitectures that are tailored for the MIPS ISA, and are therefore closer to commercial MIPS processor
microarchitectures.

252

Chapter 7. Microarchitecture | User Mode

bus | the system bus | to connect the processor subsystem to the memory subsystem.

Figure 7.7 pictorially shows one possible way of interconnecting the main building blocks
so as to perform the required functions. This (cid:12)gure suppresses information on how the
control unit controls the functioning of the blocks; these details will be discussed later. The
use of a bus permits a full range of paths to be established between the blocks connected
by the bus.
It is quite possible that some of these paths may never have to be used.
The direct path based data path that we will see later in this chapter eliminates all such
unnecessary paths by providing independent connections only between those blocks that
need to communicate.

System Address Bus

PC

ID

To Control Unit

Register Address

32

Register File
(RF)

Register Data

s
u
B
 
r
o
s
s
e
c
o
r
P

Memory
Interface

Memory
Structure

Memory Subsystem

System Data Bus

ALU

Flags

Processor Subsystem

Figure 7.7: A Microarchitecture for Implementing the MIPS-0 User Mode ISA

Finally, the data path includes the memory subsystem, which implements the mem-
ory address space de(cid:12)ned in the user mode ISA. There is a memory interface unit for
interfacing the processor bus to the memory subsystem.

7.2. Example Microarchitecture for Executing MIPS-0 Programs

253

7.2.1 MAL Commands

The process of executing a machine language (ML) instruction on the data path can be
broken down into a sequence of more elementary steps. We saw in Section 7.1 how we can
use a micro-assembly language (MAL) to express these elementary steps. We can de(cid:12)ne a
set of MAL operations for the MIPS-0 data path de(cid:12)ned in Figure 7.7.

If the interconnections of the data path are rich enough to allow multiple MAL opera-
tions in the same time period, these can be denoted by writing them in the same line, as
follows:

Fetch instruction;

Increment PC

speci(cid:12)es that in the same step, the next instruction is fetched from memory, and the contents
of PC are incremented by 4.

Finally, for MAL operations that are conditional in nature, an \if " construct patterned
after the C language’s \if " construct is de(cid:12)ned.
if (src1 == src2)

Update PC

indicates that if the zero (cid:13)ag is equal to 1, the contents of AOR is copied to PC; otherwise no
action is taken.

7.2.2 MAL Operation Set

Table 7.1 gives a list of useful MAL operations for the microarchitecture of Figure 7.7. The
MAL operations done (in parallel) in a single step form a MAL command.

7.2.3 An Example MAL Routine

Now that we are familiar with the syntax of MAL as well as the useful MAL operations for
the microarchitecture of Figure 7.7, we shall look at a simple sequence of MAL operations
called a MAL routine. We shall write a MAL routine that adds the contents of registers
speci(cid:12)ed in the rs and rt (cid:12)elds of the instruction, and writes the result into the register
speci(cid:12)ed in the rd (cid:12)eld of the instruction. The astute reader may have noticed that executing
this MAL routine amounts to executing the MIPS-I machine language instruction and rd,
rs, rt in the microarchitecture of Figure 7.7, provided the binary pattern of this instruction
has been fetched and decoded.

In theory, we can write entire programs in MAL that can perform tasks such as ‘printing
\hello, world!" ’ and more; we will, of course, need a special storage for storing the MAL
programs and a mechanism for sequencing through the MAL program. Having seen the
di(cid:14)culty of writing programs in assembly language and machine language, one can easily
imagine the nightmare of writing entire programs in MAL! However, developing a MAL
program may not be as bad as it sounds, given that we can develop translator software such
as assemblers that take machine language programs and translate them to MAL programs.

254

Chapter 7. Microarchitecture | User Mode

No.
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14

Comments
MAL Command
Instruction = Memory[PC]
Instruction
Fetch
Determine opcode, src1, src2, dest, offset
Instruction
Decode
Result = R[rs] + R[rt]
src1, src2
Add
src1, offset Result = R[rs] + sign-extended offset
Add
Result = R[rs] AND R[rt]
And
src1, src2
src1, offset Result = R[rs] AND sign-extended offset
And
Address = R[rs] + sign-extended offset
MemAddress
Compute
BranchTarget Target = PC + 4 + 4 (cid:2) sign-extended offset
Compute
Target = PC + 4 + 4 (cid:2) sign-extended offset
Compute
JumpTarget
R[rt]   Memory[Address]
Load
Memory[Address]   R[rt]
Store
Write result to register dest
Write
R[31]   PC
Save
PC   PC + 4
Increment
PC   Target
Update

PC
PC
PC

Table 7.1: A List of Useful MAL Operations for the Microarchitecture of Figure 7.7

Step MAL Command
Comments
registers Read registers rs and rt
0
Read
AND the operand values
1
Compute
result
2
Write result into destination register (rd) if it is non-zero
Write

Table 7.2: An Example MAL Routine for Executing an Instruction in the Microarchitecture
of Figure 7.7

The real di(cid:14)culty is that MAL programs will be substantially bigger than the corresponding
ML programs, thereby requiring very large amounts of storage. This is where interpretation
comes in. By generating the required MAL routines on-the-(cid:13)y at run-time, the storage
requirements are drastically reduced, as we will see next.

7.3

Interpreting ML Programs by MAL Routines

Having discussed the basics of de(cid:12)ning MAL routines, our next step is to investigate how
machine language programs can be interpreted by a sequence of MAL commands that are
de(cid:12)ned for a particular data path. To that end, we will take individual MIPS-0 instructions
and how MAL commands can be put together to carry out its execution in the data path.
Before developing the MAL command routines, let us brie(cid:13)y review the functions the data
path must perform so as to execute instructions one by one. As seen in Section 6.5, the

7.3.

Interpreting ML Programs by MAL Routines

255

interpretation process involves two ma jor functions:

(cid:15) Fetching the instruction: This concerns fetching the next instruction in the pro-
gram.

(cid:15) Executing the instruction: This concerns executing the fetched instruction.

These actions are typically done as a fetch-execute sequence. We also saw that these two
functions can be further divided into a sequence of steps (some of which are optional), as
indicated below.

Executing the next instruction

(cid:15) Fetch Phase

1. Fetch instruction: Read the next instruction from the main memory into
the processor data path.

2. Decode instruction: Decode the instruction bit pattern so as to determine
the action speci(cid:12)ed by it.

(cid:15) Execute Phase

1. Fetch source operand values: Fetch source operands (if any) from regis-
ters, main memory, or IO registers.

2. Process data: Perform arithmetic or logical operation on source operand
values, if required.

3. Write result operand value: Write the result of an arithmetic or logical
operation to a register, memory location, or IO register, if required.

4. Update instruction address: Determine the memory address of the next
instruction to be interpreted.

These 6 steps serve as directives for the data path in its attempt to carry out the
execution of each instruction. In order to execute an entire ML program, which is just a
sequence of instructions, the interpretation should also implement the sequencing among
instructions. That is, after the completion of the above 6 steps for a single instruction,
it should go back to the fetch phase to begin executing the next instruction in the ML
program. Thus, the fetch-execute sequence becomes a fetch-execute cycle.

All of these steps can be accomplished by a sequence of MAL commands called a MAL
routine. A MAL routine is allowed to freely use and modify any of the ISA-invisible
microarchitectural registers. The ISA-visible registers, however, can be modi(cid:12)ed only as
per the semantics of the instruction being interpreted.

256

Chapter 7. Microarchitecture | User Mode

7.3.1

Interpreting an Instruction | the Fetch Phase

When implementing ISAs with (cid:12)xed length instructions, the actions required to perform
the fetch phase of the interpretation are the same for all instructions. The MIPS-0 ISA is no
exception. We shall consider this phase of instruction interpretation (cid:12)rst. In the data path
of Figure 7.7, register PC keeps the memory address of the next instruction to be fetched.
To fetch this instruction from memory, we have to utilize the memory interface provided in
the data path. Table 7.3 gives a MAL routine that implements the fetch phase of executing
the instruction. This routine also includes the MAL command for incrementing PC after the
fetch phase so as to point to the sequentially following instruction, which is normally the
instruction executed after the execution of the current instruction. If the current instruction
is a branch instruction, then the PC value may be modi(cid:12)ed in the execute phase.

Step

0
1

2

Comments
MAL Command
Fetch and Decode
instruction
instruction Determine opcode, src1, src2, dest
PC increment
Increment PC by 4

PC

Fetch
Decode

Increment

Table 7.3: A MAL Routine for the Fetching a MIPS-0 ISA Instruction in the Microarchi-
tecture of Figure 7.7

The (cid:12)rst step involves getting the binary pattern corresponding to the instruction from
the memory8 . In this MAL routine, we consider the time taken to perform this MAL com-
mand as one unit; the exact number of cycles taken depends on the speci(cid:12)cs of the memory
system used and the RTL sequence used.
In step 1, the fetched instruction bit pattern
is decoded by the instruction decoding circuitry. The exact manner in which instruction
decoding is performed is not speci(cid:12)ed here; this will be relevant only to the lower level
of design. The opcode and funct (cid:12)elds of the fetched instruction uniquely identify the
instruction. Steps 0-1 thus constitute the MAL routine for the fetch phase of instruction
interpretation. Naturally, this portion is the same for every instruction in the MIPS ISA
because all instructions have the same size. Had the MIPS ISA used variable length in-
structions, this fetch process would have to be repeated as many times as the number of
words in the instruction.

After interpreting an instruction, the interpreter needs to go back to step 0 to interpret
the next instruction in the ML program. However, prior to that, it has to increment PC to
point to the next instruction; otherwise the same instruction gets interpreted repeatedly.
In this MAL routine, PC update is done immediately after completing the instruction fetch.

8We have used a single MAL command, namely Fetch instruction, to tell the microarchitecture to
fetch an instruction. At the register transfer level (RTL), this MAL command may be implemented by a
sequence of RTL instructions.

7.3.

Interpreting ML Programs by MAL Routines

257

Thus, in step 2, PC is incremented by 4 to point to the next instruction in the executed
machine language program. After step 2, the interpreter goes to the execute phase.

This MAL routine has 3 steps. We can, in fact, perform the Increment PC function in
parallel to the instruction fetch or decode functions, and reduce the total number of steps
required for the interpretation. Table 7.4 provides the modi(cid:12)ed MAL routine, which requires
only 2 steps. In this routine, in step 0, PC is incremented at the same time the instruction
is fetched from memory. It is important to note that to do multiple MAL operations in
parallel, the microarchitecture needs to have appropriate connectivity. In general, the more
the connectivity provided in a data path, the more the opportunities for performing multiple
MAL operations in parallel.

Step

0
1

Fetch
Decode

MAL Command
Fetch, Decode, and PC increment
instruction;
PC
Increment
instruction

Comments

Table 7.4: An Optimized MAL Routine for Fetching a MIPS-0 ISA Instruction in the
Microarchitecture of Figure 7.7

7.3.2

Interpreting Arithmetic/Logical Instructions

We just saw a MAL routine for the fetch phase of instruction interpretation. Next, let
us consider the execute phase of instructions. Unlike the actions in the fetch phase, the
actions in the execute phase are not identical for di(cid:11)erent instructions. Therefore, the MAL
routines for the execute phase are di(cid:11)erent for the di(cid:11)erent instructions. We shall con-
sider one instruction each from the four types of instructions: (i) data transfer instruction,
(ii) arithmetic/logical instruction, (iii) control (cid:13)ow changing instruction, and (iv) syscall
instruction.

Let us start by considering an arithmetic instruction. We shall put together a sequence
of MAL commands to interpret an arithmetic/logical instruction.

Example: Consider the MIPS ADDU instruction whose symbolic representation is addu
rd, rs, rt. Its encoding is given below.

000000

rs

rt

rd

ADD

The (cid:12)elds rs, rt, and rd specify register numbers; the (cid:12)rst two of these contain the data
values to be added together as unsigned intergers. The rd (cid:12)eld indicates the destination
register, i.e., the register to which the result should be written to, as long as it is not register
$0. In this data path, the addition of the two register values can be done in the ALU.

258

Chapter 7. Microarchitecture | User Mode

Table 7.5 speci(cid:12)es a sequence of MAL commands for carrying out the execute phase of
this instruction in the data path of Figure 7.7. Let us go through the working of this MAL
routine. We name the (cid:12)rst MAL command of the routine as step 2, as the execute phase is
a continuation of the fetch phase, which ended at step 2.

Step MAL Command Comments
Execute phase
src1, src2 Read operands R[rs] and R[rt]
Perform arithmetic operation
Write result to register rd, if rd is non-zero

Read
Add
Write

2
2
3

Table 7.5: A MAL Routine for Executing the MIPS-0 Instruction Represented Symbolically
as addu rd, rs, rt. This MAL Routine is for executing the instruction in the Data Path
of Figure 7.7

Step 2 begins the execution phase. First, the register operands must be fetched from the
register (cid:12)le. In step 2, the operand values present in general-purpose registers rs and rt
are read, and are added together in the ALU. In step 3, the result of the addition is written
to the destination register (rd), if rd is non-zero. By performing this sequence of MAL
commands in the correct order, the instruction addu rd, rs, rt is correctly interpreted.

7.3.3

Interpreting Memory-Referencing Instructions

Let us next put together a sequence of MAL commands to fetch and execute a memory-
referencing instruction. Because all MIPS instructions are of the same length, the MAL
routine for the fetch part of the instruction is the same as before; the di(cid:11)erences are only in
the execution part. Consider the MIPS load instruction whose symbolic representation is
lw rt, offset(rs). The semantics of this instruction are to copy to GPR rt the contents
of memory location whose address is given by the sum of the contents of GPR rs and sign-
extended offset. We need to come up with a sequence of MAL commands that e(cid:11)ectively
fetch and execute this instruction in the data path of Figure 7.7.

100011

rs

rt

offset

The interpretation of a memory-referencing instruction requires the computation of an
address. For the MIPS ISA, address calculation involves sign-extending the offset (cid:12)eld of
the instruction to form a 32-bit signed o(cid:11)set, and adding it to the contents of the register
speci(cid:12)ed in the rs (cid:12)eld of the instruction. In this data path, the address calculation is done
using the same ALU, as no separate adder has been provided. With this introduction, let
us look at the MAL routine given in Table 7.6 to interpret this lw instruction.

In step 2, the memory address is computed by reading the contents of register speci(cid:12)ed

7.3.

Interpreting ML Programs by MAL Routines

259

Step

MAL Command

Comments
Execute phase
MemAddress Memory address = R[rs] + sign-extended offset
Load from memory into register rt

2
3

Compute
Load

Table 7.6: A MAL Routine for the Execute Phase of the Interpretation of the MIPS-0 ISA
Instruction Represented Symbolically as lw rt, offset(rs). This MAL Routine is for
executing the instruction in the Data Path of Figure 7.7

in the rs (cid:12)eld and adding the sign-extended offset value to it. In step 3, the contents of
the memory location at the computed address is loaded into register rt. Recall that the
memory transfer takes place through the memory interface.

7.3.4

Interpreting Control-Changing Instructions

The instructions that we interpreted so far | addu and lw | do not involve control (cid:13)ow
changes that cause deviations from straightline sequencing in the ML program. Next let us
see how we can interpret control-changing instructions, which involve modifying PC, usually
based on a condition.

Example: Consider the MIPS-0 conditional branch instruction whose symbolic representa-
tion is beq rs, rt, offset. The semantics of this instruction state that if the contents
of GPRs rs and rt are equal, then the value offset (cid:2) 4 + 4 should be added to PC so as
to cause a control (cid:13)ow change9 ; otherwise, PC is incremented by 4 as usual. The encoding
of this instruction is given below:

000100

rs

rt

offset

Step

2
3
4

MAL Instruction
Comments
Execute phase
BranchTarget
src1, src2
IfEqual

Compute
Compare
Update

Compare operands R[rs] and R[rt]
Update PC if operands are equal

Table 7.7: A MAL Routine for Executing the MIPS-0 ISA Instruction Represented Sym-
bolically as beq rs, rt, offset. This MAL Routine is for executing the instruction in
the Data Path of Figure 7.7

9The actual MIPS ISA uses a delayed branch scheme; i.e., the control (cid:13)ow change happens only after
executing the instruction that follows the branch instruction in the program. We avoid delayed branches to
keep the discussion simple.

260

Chapter 7. Microarchitecture | User Mode

Table 7.7 presents a MAL routine to execute this instruction in the data path given in
Figure 7.7. The execute routine has 3 steps numbered 2-4. The (cid:12)rst step of the routine
(step 2) calculates the target address of the branch instruction by adding 4 times the sign-
extended offset value to the incremented PC value. In step 3, the contents of registers rs
and rt are compared, and the result of the comparison is stored in the flag register. In
the last step, PC is updated with the calculated target address, if the two operands were
equal. Thus, if the operands turned out to be not equal, then PC retains the incremented
value it obtained in step 1, which is the address of the instruction following the branch in
the machine language program being interpreted.

7.3.5

Interpreting Trap Instructions

We have seen the execute phase for all of the instruction types other than the syscall
instructions. Let us next look at how the data path performs syscall execution, which
is somewhat di(cid:11)erent from the previously seen ones. Like control-changing instructions,
syscall instructions also involve modifying the contents of pc. For the MIPS-I ISA, the
pc is updated to 0x80000080. In addition, the machine is placed in the kernel mode. We
shall take a detailed look at the corresponding MAL sequence in Section 8.1.1, along with
other kernel mode implementation issues.

7.4 Memory System Organization

The memory system is an integral component of the microarchitecture of a stored program
computer, as it stores the instructions and data of the program being executed. In this
section we look at the microarchitecture of this subsystem more closely. This section begins
by illustrating the need to organize the physical memory system as a hierarchy, in order to
achieve high speed without incurring high cost. Then it describes how di(cid:11)erent components
of the memory hierarchy work. The next section provides an in-depth treatment of cache
memories, and demonstrates how they help to increase the apparent speed of the memory
system.

\In a hierarchy every employee tends to rise to his level of incompetence."
| Dr. Laurence Peter, 1919-90, in The Peter Principle, 1969

7.4.1 Memory Hierarchy: Achieving Low Latency and Cost

The discussion we had so far may seem to indicate that the main memory is implemented
as a single structure in a computer microarchitecture. That is, a single memory structure
implements the entire address space, provides fast access, and is inexpensive. Unfortunately,
this ideal situation is feasible only in some small systems that have a small address space
and work with a slow processor. For general-purpose computers, it is impossible to meet

7.4. Memory System Organization

261

all three of these requirements simultaneously with today’s semiconductor technology. As
might be expected, there is a trade-o(cid:11) among these three key characteristics, because of the
following relationships:

(cid:15) larger a memory structure, slower its operation.

(cid:15) larger a memory structure, greater its cost

(cid:15) faster a memory structure, greater its cost

\Everyone who comes in here wants three things:
(1) They want it quick.
(2) They want it good.
(3) They want it cheap.
I tel l ’em to pick two and cal l me back."
| A sign on the back wal l of a smal l printing company

When we implement the entire address space using a single memory structure, a memory
access cannot be completed in 1 cycle, contrary to what we had in mind when writing
the MAL routines.
Instead, a memory access will take anywhere from 50-100 processor
clock cycles, because the memory structure is quite large and also requires o(cid:11)-chip access.
Furthermore, because the memory address space de(cid:12)ned in ISAs is often quite large, it may
not be cost-e(cid:11)ective to implement the entire address space using semiconductor memory.

Unlike the predicament of the printing company in the above quote, we do have a solution
for the memory subsystem to deal with the problems introduced by the three relationships
given above. To motivate this solution, let us consider a simple analogy that illustrates the
key principles and mechanisms well. Many of you may be using an Address Book that stores
information about frequently used telephone numbers or email addresses, in order to reduce
the time spent in going through a large telephone directory. Once this Address Book has
been initialized, chances are high that most of the time you will get the required telephone
number or address directly from this list, without going through the telephone directory.
Keeping the frequently accessed information as a separate list results in signi(cid:12)cant time
savings, compared to going through the telephone directory each time.

The same principle can be used to create the illusion of a large memory that can be
accessed as quickly as a small memory. Just like we do not access every number in the
telephone directory with equal probability at any given time, a program does not access all
of its program or data locations with equal probability at any given time. The principle of
locality underlies both the way in which we deal with telephone numbers and the way that
programs operate. This principle indicates that programs access a relatively small portion
of their address space at any instant of time. There are two di(cid:11)erent types of locality:

(cid:15) Temporal locality (locality in time): If an item is referenced now, it is likely to be
accessed again in the near future.

262

Chapter 7. Microarchitecture | User Mode

(cid:15) Spatial locality (locality in space): If an item is referenced now, items whose addresses
are close by are likely to be accessed in the near future.

Just as accesses to telephone numbers exhibit locality, locality in programs arises from sim-
ple and natural program structures. For example, most programs contain loops. Therefore,
instructions and data are likely to be accessed repeatedly, resulting in high amounts of tem-
poral locality. Analyses of program executions show that about 90% of the execution time
is generally spent on 10% of the code. This may be due to simple loops, nested loops, or
a few subroutines that repeatedly call each other. Also, because instructions are normally
accessed sequentially, instruction accesses show high spatial locality. Accesses to data also
exhibit a natural spatial locality. For example, accesses to elements of an array or a record
will naturally have high degrees of spatial locality.

Much work has gone into developing clever structures that improve the apparent speed
of the memory, without terribly increasing the cost, by taking advantage of the principle
of locality. The solution that has been widely adopted is not to rely on a single memory
component or technology, but to use a memory hierarchy. A memory hierarchy consists of
multiple levels of memory with di(cid:11)erent speeds and sizes, as illustrated in Figure 7.8. The
fastest memories are more expensive per bit than the slower ones, and are usually smaller.
The goal is to achieve a performance close to that of the fastest memory, and a cost per bit
close to that of the least expensive memory.

Speed

Cost per Bit

CPU

Memory

Memory

Memory

Memory

Figure 7.8: The Basic Structure of a Typical Memory Hierarchy

Size

A memory hierarchy can consist of multiple levels, but data is copied only between two
adjacent levels at a time. The fastest elements are the ones closest to the processor. The top
part of the hierarchy will supply data most of the time because of the principle of locality.
Memory hierarchies take advantage of temporal locality by keeping recently accessed data

7.4. Memory System Organization

263

items closer to the processor. They take advantage of spatial locality by moving blocks
consisting of multiple contiguous words to upper levels of the hierarchy, even though only
a single word was requested by the processor. If the fraction of times a memory request is
satis(cid:12)ed at the upper level is high, the memory hierarchy has an e(cid:11)ective access time close
to that of the highest (and fastest) level and an e(cid:11)ective size equal to that of the lowest
(and largest) level.

7.4.2 Cache Memory: Basic Organization

Over the years, processor speeds have been increasing at an astronomical rate. Main mem-
ory speeds, however, have not been increasing at a comparable rate. Today, it takes any-
where from 50-200 processor cycles to access a word from main memory. A common solution
adopted to bridge this gap in speeds is to insert a relatively small, high-speed memory, called
cache memory, between the processor and the main memory. The cache memory keeps a
copy of the frequently accessed memory locations. When the processor issues a request to
access a memory location, the cache memory responds if it contains the requested word. If
it does not contain the requested word, the request is passed on to main memory.

Conceptually, the operation of cache memory is very simple. The temporal aspect of
the locality of reference suggests that whenever the contents of a location is needed for the
(cid:12)rst time, this item should be brought into the cache so that the cache can supply the value
the next time the same location is accessed. The spatial locality suggests that instead of
bringing just one item from the main memory to the cache memory, it is wise to bring
several nearby items as well. We will use the term block to refer to a set of contiguous
addresses. Another term that is often used to refer to a block is line.

Consider the simple arrangement shown in the previous (cid:12)gure. When the processor
issues a Read request to a memory location for the (cid:12)rst time, the contents of the speci(cid:12)ed
location and the surrounding block of locations are transferred to the cache, one word after
another. Subsequently, when the program references any of the words in this block, the
requested word is read directly from the cache, and the request is not forwarded to main
memory.

Notice that cache memory is a microarchitectural feature to enhance performance, and
is not part of most instruction set architectures (ISAs). Thus, the machine language pro-
grammer does not need to know the existence of the cache. The processor simply issues
Read and Write requests using main memory addresses. The cache control circuitry deter-
mines if the requested word currently exists in the cache. If the cache contains the word, a
cache hit is said to have occurred. If it does not, then a cache miss is said to have occurred.

264

Chapter 7. Microarchitecture | User Mode

7.4.3 MIPS-0 Data Path with Cache Memories

7.4.4 Cache Performance

A basic question concerning a cache memory system is how to quantify its performance.
The (cid:12)gure of merit of interest to us is the average memory access time, considering the
cache memory and the main memory together as a single system. A simple formula for this
metric is given by:

TEF F = TCA + m (cid:2) TM iss
where TCA is the access time of the cache, TM iss is the time taken to service a cache miss,
and m is the miss ratio (i.e., the fraction of accesses that resulted in a cache miss).

Example: In order to improve the memory access time from 50 cycles, a system designer
decided to incorporate a cache memory, which has an access time of 4 cycles. The cache
was able to supply data for 80% of the memory references. Has the cache memory improved
the memory system’s performance or decreased it?

When the system did not use cache memory, each memory access took 50 cycles. When
a cache memory is introduced, the cache miss ratio is 20%. Therefore, the average memory
access time becomes 4 + 0.2 (cid:2) 50 = 14 cycles. Thus, in this case, the introduction of the
cache memory improves the average access time from 50 cycles to 14 cycles. If the cache
memory has a higher hit ratio and/or a lower access time, then the improvement will be
even more marked.

7.4.5 Address Mapping Functions

The cache memory is obviously much smaller than the main memory, which means that the
maximum number of blocks it can store at any time is less than the total number of blocks
present in the main memory. This has two ma jor implications:

(cid:15) We cannot use the straightforward linear addressing methodology for accessing the
cache memory. For instance, if we want to access memory address 1 billion, we cannot
just go to the 1 billionth location in the cache. Thus, there has to be some mapping
between the main memory blocks and the physical frames of the cache. In an Address
Book, for instance, this mapping is typically based on the (cid:12)rst letter of the person’s
name, as illustrated in Figure 7.9; the information pertaining to a person whose name
starts with \A" is entered in the page corresponding to letter \A". Below, we will
look at address mapping schemes that are typically used in cache memories.

(cid:15) Multiple main memory blocks can map to the same cache block frame, at di(cid:11)erent
times. This implies that there must be some mechanism to identify the main memory
block that is currently mapped to each cache clock frame. Again, in Figure 7.9, we
can see that two di(cid:11)erent names are mapped to the page corresponding to letter \A".

7.4. Memory System Organization

265

The name currently mapped to the page is identi(cid:12)ed by explicitly storing the name
| which serves as a tag | along with the address and phone number | which form
the data.

Tag

Tag

Adam First

1234 Garden Pkwy
Eden Gardens
123−456−6789

A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
P
Q
R
S
T
U
V
W
X
Y
Z

Data

A l i c e   L i d d e l l

2 2 1 B   R a b b i t ’ s   H o l e
W o n d e r l a n d
9 8 7 − 6 5 4 − 4 2 3 1

A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
P
Q
R
S
T
U
V
W
X
Y
Z

Data

Figure 7.9: An Alphabetically Organized Address Book Containing Two Di(cid:11)erent Name
Entries

Identi(cid:12)cation of the memory block that is currently mapped to a cache block frame is
typically achieved by including a Tag (cid:12)eld in each cache block frame. When a new memory
block is copied to a cache block frame, the Tag (cid:12)eld is updated appropriately. A Valid bit
is also included for indicating if the cache block frame currently has a valid main memory
block.

The correspondence between the main memory blocks and those present in the cache
is speci(cid:12)ed by a mapping function. When the cache is full and the processor accesses a
word that is not present in the cache, the cache control hardware must decide which block
should be removed to create space for the new block that contains the referenced word. The
collection of rules for making this decision constitutes the replacement algorithm.

To discuss possible methods for specifying where main memory blocks are placed in the
cache, we use a speci(cid:12)c small example. Consider a cache consisting of 8 blocks of 32 words
each, for a total of 256 words, and assume that the main memory is addressable by a 10-bit
address. The main memory has 1M words, which can be viewed as 32K blocks of 32 words
each.

The simplest way to map main memory blocks to cache block frames is the fully asso-
ciative mapping technique. In this mapping, a main memory block can be placed in any

266

Chapter 7. Microarchitecture | User Mode

cache block frame, as shown in the (cid:12)rst part of Figure 7.10. Therefore, a new memory block
will replace an existing block only if the cache is full. However, the cost of an associative
cache is high because of the need to search all cache block frames to determine if a given
main memory block is in the cache. A search of this kind is called an associative search.
For performance reasons, the tags must be searched in parallel.

Cache Block Frame No.

Tag

Cache Memory
Valid
Data

0
1
2
3
4
5
6
7
fully associative: MM block 12 can go anywhere in cache

0
1
2
3
4
5
6
7
direct mapped: MM block 12 can go only to one cache block frame

0
1
2
3
4
5
6
7

Set 0

Set 1

Set 2

Set 3

2−way set associative: MM block 12 can go anywhere in one set

Main Memory

MM Block No.
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

32K − 3
32K − 2
32K − 1

Figure 7.10: Possible Mappings between Main Memory and Cache Memory

An alternative approach is the direct mapping technique, depicted in the second part
of Figure 7.10.
In this technique, a particular main memory block maps to exactly one
cache block frame. The commonly used hash function is such that main memory block i
maps onto cache block frame i mod Num Cache Blocks. Thus, it is easy to determine if
a particular main memory block is present in the cache or not. However, because a main
memory block can be placed only in one particular cache block frame, contention may arise
for that position even if the cache is not full or if another cache block frame has an unused
main memory block.

\He who always puts things in place, is too lazy to look for them."
| Anonymous

7.4. Memory System Organization

267

Set associative mapping is a compromise that captures the advantages of both the
fully associative and the direct mapping approaches. Cache blocks are grouped into sets,
and the mapping allows a main memory block to reside in any block of a speci(cid:12)c cache set.
Hence, the contention problem of the direct mapping method is reduced by having some
choices for block replacement. At the same time, the hardware cost of fully associative
mapping is reduced by decreasing the size of the associative search.

7.4.6 Finding a Word in the Cache

How does the cache memory (cid:12)nd a memory word if it is contained in the cache? Given
a memory address, the cache (cid:12)rst (cid:12)nds the memory block number to which the address
belongs. This can be determined by dividing the memory address by the block size. Because
it is time-consuming to do an integer division operation, cache designers only use block sizes
that are powers of 2. The division, then, becomes taking out the upper bits of the memory
address. Figure 7.11 shows how a memory address bit pattern is split. The (cid:12)rst split
is between the Main Memory Block Number and the Offset within Block. The Main
Memory Block Number (cid:12)eld is further split into the Tag (cid:12)eld and the Cache Set Number
(cid:12)eld. The Offset within Block (cid:12)eld selects the desired word once the block is found, the
Cache Set Number (cid:12)eld selects the cache set, and the Tag (cid:12)eld is compared against the tag
values in the selected set to check if a hit occurs.

Main Memory Address

log

2

(no. Addrs in Main Memory)

Main Memory Block Number

Offset within Block

log

2

(no. Addrs in Block)

Tag

Cache Set Number

Offset within Block

log   (no. sets)
2

Figure 7.11: Splitting a Memory Address Bit Pattern for Accessing the Cache Memory

268

Chapter 7. Microarchitecture | User Mode

For a given total cache size, increasing the associativity reduces the number of cache
sets, thereby decreasing the size of the Cache Set Number (cid:12)eld and increasing the size of
the Tag (cid:12)eld. That is, the boundary between the Tag and Cache Set Number (cid:12)elds moves
to the right with increasing associativity, with the limit case of fully associative caches
having no Cache Set Number (cid:12)eld (because the entire cache is just one set).

Example: Consider a computer system that has a 4-way set-associative cache. The cache
receives 16-bit addresses, and splits it as follows: 5 bits for (cid:12)nding the Offset within
Block; 8 bits for (cid:12)nding the Cache Set Number.

1. How many sets are present in the cache memory?
Because 8 bits are decoded to determine the Cache Set Number, the cache has 2 8 =
256 sets.

2. How many words are present in a block?
Because 5 bits are decoded to determine the Offset within Block, each block has
25 = 32 words.

3. How many block frames are present in the cache memory?
The number of block frames in a cache memory is given by the product of the number
of sets it has and the number of blocks in each set. Therefore, this cache has 256 (cid:2) 4
= 1024 blocks.

4. How many words are present in the cache memory?
The number of words in a cache memory is given by the product of the number of
block frames it has and the number of words in each block. Therefore, this cache has
1024 (cid:2) 32 = 32K words.

5. What is the cache set number corresponding to hexadecimal address FBFC?
The hexadecimal address FBFC corresponds to bit pattern 1111101111111100. The
Cache Set Number is given by bits 12-5, which is the 8-bit pattern 11011111.
In
hexadecimal number system, this bit pattern is represented as DF.

7.4.7 Block Replacement Policy

When a new memory block is to be brought into a cache set, and all the block frames that
it may occupy are occupied by other memory blocks, the cache controller must decide which
of the old blocks should be replaced. In a direct mapped cache, there is exactly one cache
block frame where a particular main memory block can be placed; hence there is no need for
a replacement strategy. In fully associative and set-associative caches, on the other hand,
there is a choice in determining the block to be replaced. A good replacement strategy is
important for the cache memory to perform well. In general, the ob jective is to keep in the
cache those blocks that are likely to be re-referenced in the immediate future. However, it
is not easy to determine which blocks will be re-referenced in the future.

7.5. Processor-Memory Bus

269

\The best thing about the future is that it comes to us only one day at a time."
| Abraham Lincoln

The property of locality of references suggests that blocks that have been referenced
recently are very likely to be re-referenced in the near future. This is illustrated in the
following pattern of accesses to three memory addresses fX , Y , Z g which belong to three
di(cid:11)erent blocks that map to the same cache block frame.
XXX Y Y Y Y Y Y XXXX Z Z Z Z

In this pattern, except for the few times a transition happens from one address to
another, the memory block being referenced is the same as the one referenced during the
last access. Therefore, when a block is to be replaced in a set, it is sensible to replace the one
that has not been referenced for the longest time in that set. This block is called the least
recently used block (LRU), and this replacement policy is called the LRU replacement
policy.

The LRU replacement policy works well for most scenarios. It works poorly, however,
for some scenarios. Consider, for instance, a 2-way set associative cache in which accesses
to addresses fX , Y , Z g follow a di(cid:11)erent pattern:
X Y ZX Y ZX Y ZX Y ZX Y ZX Y

In this case, whenever an access to Z causes a miss, X ’s block is the LRU block. Inter-
estingly, this is the block to be retained in the cache, for the next access to be a hit. The
LRU replacement policy is a poor choice for this kind of access pattern, although it is a
repeating pattern.

7.4.8 Multi-Level Cache Memories

7.5 Processor-Memory Bus

The next topic that we look at is the interconnection between the processor and the memory
system, which includes the cache memories. For a variety of reasons, this connection is often
done via a bus called the processor-memory bus. (In Intel x86-based computers, this bus is
called the Front Side Bus or FSB.) A bus, as we saw earlier, is a shared communication
link, connecting multiple devices using a single set of wires. In addition to wires, a bus also
includes hardware circuitry for controlling access to the bus and the transfer of information
over the wires, as per speci(cid:12)c protocols. The processor-memory bus consists of 3 sets of
wires: address, data, and control. The address lines carry the address of the memory location
to be accessed. The data lines carry data information between the source and destination.
The control lines are used for transmitting signals that control data transfer as per speci(cid:12)c
protocols. They specify the times at which the processor and the memory interfaces may
place data on the bus or receive data from the bus.

In a desktop environment, the processor-memory bus is almost always contained within
the motherboard; several sockets are provided along the bus for inserting the memory

270

Chapter 7. Microarchitecture | User Mode

modules. The bus speed is generally matched to the memory system so as to maximize
processor-memory bandwidth and minimize memory latency. Processor-memory buses are
often ISA-speci(cid:12)c, because their structure is closely tied to the ISA.

Bus design and operation are su(cid:14)ciently complex sub jects that entire books have been
written about them. The ma jor issues in bus design are bus width, bus clocking, bus
arbitration, and bus operations. Each of these issues has a substantial impact on the speed
and bandwidth of the bus. We will brie(cid:13)y examine each of these.

7.5.1 Bus Width

Bus width is an important microarchitectural parameter. The wider the bus, the more
its potential bandwidth. The problem, however, is that wide buses require more wires
than narrow ones. They also take up more physical space (e.g., on the motherboard)
and need bigger connectors. All of these factors make the wider bus more expensive. To
cut costs many system designers tend to be shortsighted, with unfortunate consequences
later. There are two ways to increase the data bandwidth of a bus: decrease the bus cycle
time (more transfers/sec) or increase the data bus width (more bits/transfer). Speeding
up the bus is possible, but di(cid:14)cult because the signals on di(cid:11)erent lines travel at slightly
di(cid:11)erent speeds, a problem known as bus skew. The faster the bus, the more serious bus
skew becomes. Another problem with speeding up the bus is that doing this will not be
backward-compatible. Old boards designed for the slower bus will not work with the new
one. Therefore the usual approach to improving bus performance is to add more data lines.

To get around the problem of very wide buses, sometimes bus designers opt for a mul-
tiplexed bus. In this design, instead of having separate address and data lines, a common
set of lines is used for both address and data. At the start of a bus operation, the lines
are used for the address. Later on, they are used for data. Multiplexing the lines in this
manner reduces bus width (and cost), but results in a slightly slower system. Bus designers
have to carefully weigh all these options when making choices.

7.5.2 Bus Operations

The simplest bus operation involves a master and a slave. The master, which in this case
can be the processor or the cache controller, initiates a read/write request by placing the
address of the location in the address lines of the bus. For a write operation, in addition the
data to be written is placed on the data lines. The master also activates the control lines
to indicate if it is a read or write operation. When the slave becomes ready, it performs the
read/write to the location speci(cid:12)ed in the address lines.

Block Transfers or Burst Transfers: Normally, one word is transferred in one trans-
fer. The throughput of such a bus is limited by the protocol overhead associated with
each transaction. Each transfer requires arbitration for bus mastership, transmission of an

7.5. Processor-Memory Bus

271

address, and slave acknowledgement, in addition to the transfer of the actual data. This
type of transfer is particularly wasteful for transfering contiguous blocks of data, such as
what happens during cache re(cid:12)ll for servicing cache misses. In such situations, only the
starting address and word count need be speci(cid:12)ed by the master.
In order to improve
the throughput for such situations, processor-memory buses generally support block trans-
fer transactions that e(cid:11)ectively amortize the transaction overhead over larger data blocks.
When a block read operation is initiated, the bus master tells the slave how many words are
to be transferred, for example by initially placing the word count on the data lines. Instead
of returning a single word, the slave returns one word at a time until the count has been
exhausted.

Split Transaction:
In the bus operations discussed so far, the bus is tied up for the entire
duration of a data transfer, that is, from the time the request is initiated until the time the
transfer is accomplished. No other transfers can be done or even initiated during this period.
If a memory module has a 50-cycle access time, for example, a read transaction might take
a total of about 55 cycles|the ma jority of which constitute idle time. To improve the bus
e(cid:14)ciency under such circumstances, certain buses split conventional transfer operations into
two parts: a request transaction and a response transaction. The intent is to relinquish the
bus in between these two transactions, allowing it to be used for other transactions in the
interim. Such a bus is called a split-transaction bus. A variety of issues must be addressed to
make a split-transaction bus work. First, in general, the bus protocols must work even when
several pending operations are directed at a single slave module. This typically involves
pipelined operation of the slave itself.
In addition, if the bus is to support out-of-order
responses from slave modules, there must be some provision to tag each response with the
identity of the request it corresponds to.

Read-Modify-Write Operation: Other kinds of bus cycles also exist. For example,
to support synchronization in a multiprocessor system, many ISAs provide an instruction
like TEST AND SET (X). The semantics of this instruction is: read the value at memory
location X, and set it to 1, i(cid:11) the read value is 0. The read and write operations together
form an atomic operation. Executing this instruction requires a bus read cycle and a bus
write cycle. It is important that after the read cycle is performed, and before the write
cycle is performed, no other device reads or modi(cid:12)es memory location X. To implement this
instruction correctly, multiprocessor bus systems often have a special read-modify-write bus
cycle that allows a processor to read a word from memory, inspect and modify it, and write
it back to memory, all without releasing the bus. This type of bus cycle prevents competing
processors from being able to use the bus and thus interfere with the (cid:12)rst processor’s
operation.

272

Chapter 7. Microarchitecture | User Mode

7.6 Processor Data Path Interconnects: Design Choices

The previous sections discussed the basics of designing a data path for interpreting machine
language instructions. We purposely used a very simple data path for ease of understanding.
Although such a data path may not provide high performance, it may be quite su(cid:14)cient
for certain embedded applications. The data path of modern general-purpose computers,
on the other hand, is far more complex. In this section and the one following it, we will
introduce high-performance versions of the processor data path. The processor plays a
central role in a computer microarchitecture’s function. It communicates with and controls
the operation of other subsystems within the computer. The performance of the processor
therefore has a ma jor impact on the performance of the computer system as a whole.

7.6.1 Multiple-Bus based Data Paths

As discussed in Chapter 6, three important factors that determine the performance of a
computer are the strength of the machine language instructions, the number of clock cycles
required to fetch and execute a machine language instruction, and the clock speed. Among
these three, the number of cycles required to fetch and execute an instruction can be reduced
by redesigning the data path so that several micro-operations can be performed in parallel.
The more interconnections there are between the di(cid:11)erent blocks of a data path, the more
the data transfers that can happen in parallel in a clock cycle. The number of micro-
operations that can be done in parallel in a data path does depend on the connectivity it
provides. In a bus-based processor data path, the number of buses provided is probably the
most important limiting factor governing the number of cycles required to interpret machine
language instructions, as most of the microinstructions would need to use the bus(es) in
one way or other.

A processor data path with a single internal bus, such as the one given in Figure 7.7,
will take several cycles to fetch the register operands and to write the ALU result to the
destination register. This is because it takes one cycle to transfer each register operand to
the ALU, and one cycle to transfer the result from the ALU to a register. An obvious way
to reduce the number of cycles required to interpret instructions is to include multiple buses
in the data path, which makes it possible to parallelly transfer multiple register values to
the ALU. Before going into speci(cid:12)c multiple bus-based data paths, let us provide a word of
caution: buses can consume signi(cid:12)cant chip area and power. Furthermore, they may need
to cross at various points in the chip, which can make it di(cid:14)cult to do the layout at the
device level.

A 2-Bus Processor Data Path

The single-bus processor data path given in Figure 7.7 can be enhanced by adding one more
internal bus. The additional bus can be connected to the blocks in di(cid:11)erent ways. Figure

7.6. Processor Data Path Interconnects: Design Choices

273

7.12 pictorially shows one possible way of connecting the second bus. In the (cid:12)gure, the
new bus is shown in darker shade, and is used primarily for routing the ALU result back
to di(cid:11)erent registers. A unidirectional path is provided from the (cid:12)rst bus to the second.
Notice that to perform a register read and a register write in the same clock cycle, the

PC

MAR

System Address Bus

4

IR

)
s
u
B
 
d
n
a
r
e
p
O
(
 
1
 
s
u
B
 
r
o
s
s
e
c
o
r
P

)
s
u
B
 
t
l
u
s
e
R
(
 
2
 
s
u
B
 
r
o
s
s
e
c
o
r
P

Sign
Extend

rs

rt

rd

31

MUX

MUX

Register Write Addr

To Control Unit

Register File
(RF)

Register Read Addr

Register Data

Memory
Subsystem

System Data Bus

MUX

MDR

AIR

ALU

Flags

Processor Data Path

Figure 7.12: A 2-Bus Based Processor Data Path for Implementing the MIPS-0 ISA

register (cid:12)le requires two ports (a read port and a write port).

7.6.2 Direct Path-based Data Path

As we keep adding more buses to the processor data path, we eventually get a data path
that has many point-to-point connections or direct paths. If we are willing to use a large
number of such direct path connections, it is better to redesign the processor data path.

274

Chapter 7. Microarchitecture | User Mode

Figure 7.13 presents a direct path-based data path for the MIPS-0 ISA. In this data path,
instead of using one or more buses, point-to-point connections or direct paths are provided
between each pair of components that transfer data. This approach allows multiple data
transfers to take place simultanesously, thereby speeding up the execution of instructions,
as we will see in Chapter 9. For instance, the MAL command ComputeAddress requires
several data transfers, which can only be done sequentially in a single-bus based data path.
The use of direct paths probably makes the data path’s functioning easier to visualize,
because each interconnection is now used for a speci(cid:12)c transfer as opposed to the situation
in a bus-based data path where a bus transaction changes from clock cycle to clock cycle.
Notice, however, that a direct path based data path is even more tailored to a particular
ISA.

Notice that there are two paths emanating from the register (cid:12)le. If both paths are to
be active at the same time, then the register (cid:12)le needs to have two read ports.

7.7 Pipelined Data Path: Overlapping the Execution of Mul-
tiple Instructions

The use of multiple buses and direct paths in the processor data path enables us to reduce
the number of steps required to fetch and execute instructions. Can we further reduce the
number of steps by increasing the connectivity? It may be possible; however, it is very
di(cid:14)cult, because many of the steps are somewhat sequential in nature. Instead of reducing
the number of steps required to fetch and execute each instruction, we can, however, re-
duce the total number of steps required to fetch and execute a sequence of instructions by
overlapping the interpretation of multiple instructions. In order to overlap the execution
of multiple instructions, a commonly used technique is pipelining, similar in spirit to the
assembly-line processing used in factories.

In the data paths that we studied so far, after an instruction is fetched and passed onto
the instruction decoder, the fetch part of the processor sits idle until the execution of the
instruction is completed. By contrast, in a pipelined data path, the fetch part of the data
path utilizes this time to fetch the next several instructions. Similarly, after the instruction
decoder has decoded an instruction, it starts decoding the next instruction, without waiting
for the previous instruction’s execution to be completed.

It is important to understand that pipelining does not result in individual instructions
being executed faster; rather, it increases the rate at which instructions are interpreted.
With a four-stage pipeline, for instance, instruction execution can be completed up to four
times the rate of an unpipelined processor. Based on this discussion, it may appear that the
performance obtained with pipelining increases linearly with the number of pipeline stages.
Unfortunately, this is not the case. For a variety of reasons, a pipeline stage may not be
able to perform useful work during every clock cycle. This causes the pipelined data path’s
performance to be less than the maximum possible.

7.7. Pipelined Data Path: Overlapping the Execution of Multiple Instructions

275

PC

Instruction Address

+

Instruction
Memory
System

Instruction

Instruction
Decoder

31

Register File
(RF)

Return Address

ALU

+

ALU Result

Memory Address

Store Data

Data
Memory
System
Load Data

Processor Microarchitecture

Figure 7.13: A Direct Path-Based Processor Microarchitecture for Implementing the MIPS-
0 ISA

7.7.1 De(cid:12)ning a Pipelined Data Path

To begin with, we need to determine which hardware blocks in the data path are used
during every clock cycle, and make sure that the same hardware block is not used for two
di(cid:11)erent instructions during the same clock cycle. Ensuring this becomes di(cid:14)cult if the
same hardware resource is used in multiple cycles during the execution of an instruction.
This makes it di(cid:14)cult to use a bus-based data path, as a bus is generally used multiple times
during the execution of an instruction. Therefore, our starting point is the direct path-based
data path that we saw in Figure 7.13. What would be a natural way to partition this data
path into di(cid:11)erent stages? While we can think of many di(cid:11)erent ways to partition this data

276

Chapter 7. Microarchitecture | User Mode

path, a straightforward way to partition is as follows: place the hardware resources that
perform each of the MAL commands of the MAL routines (in Table ??) in a di(cid:11)erent stage.
Thus, we can think of a 5-stage pipeline as depicted in Figure 7.14. The 5 stages of the
pipeline are named fetch (F), read source registers (R), ALU operation (A), memory access
(M), and end (E).

F

R

A

M

E

F
R
A
M
E

− Fetch ML Instruction
− Read Source Registers
− ALU Operation
− Memory Access
− End

Figure 7.14: Illustration of a 5-Stage Pipelined Data Path

Next, let us partition the direct path-based data path of Figure 7.13 as per this 5-
stage framework. Figure 7.15 shows such a partitioning. In this pipelined data path, the
instruction fetch portion of the data path|which includes PC and its update logic|has
been demarcated as the F stage. IR, which stores the fetched instruction, forms a bu(cid:11)er
between the F stage and the R stage. The instruction decoder and the register (cid:12)le have
been placed in the R stage. Similarly, the ALU is placed in the A stage. AOR, which stores
the output of the ALU, forms a bu(cid:11)er between the A stage and the M stage, which houses
the logic for accessing the data memory. Finally, the E stage just consists of the logic for
updating the register (cid:12)le. Table 7.8 shows the primary resources used in each stage of the
pipeline.

Primary Resources
Stage
PC, Instruction memory
Fetch
Register (cid:12)le, Sign extension unit
Read register
ALU
ALU
Memory access Data memory, Register (cid:12)le

Table 7.8: Primary Resources Used in Each Stage of the Pipelined Data Path of Figure
7.15

In the pipelined data path being discussed, an instruction uses the ma jor hardware blocks
in di(cid:11)erent clock cycles, and hence overlapping the interpretation of multiple instructions
introduces relatively fewer con(cid:13)icts. Some of the hardware blocks in the direct path-based

7.7. Pipelined Data Path: Overlapping the Execution of Multiple Instructions

277

Instruction Address

PC

Incrementer

PC_R

L1
Instruction
Cache

Instruction
rs
IR

rt

rd

Instruction
Decoder
rs
rt
Register File
(RF)

Opcode

Fetch (F)

Register Read (R)

dest_A

Rrs_A

Rrt_A

Offset_A

PC_A

)
W
_
r
(
 
r
e
t
s
i
g
e
R
 
n
o
i
t
a
n
i
t
s
e
D

Jump Target

+

Branch Target

ALU (A)

ALU

dest_M

Flags

AOR

Rrt_M

PC_M

Target

L1
Data
Cache

Memory (M)

ALU Result

Return Address

MUX

Processor Data Path

Figure 7.15: A Pipelined Processor Microarchitecture for Implementing the MIPS-0 ISA

data path are used multiple times while interpreting some instructions. In order to avoid
con(cid:13)icts for these hardware blocks, they have been replicated in the pipelined data path.
For example, a single PC cannot store the addresses of two di(cid:11)erent instructions at the same
time. Therefore, the pipelined data path must provide multiple copies of PC, one for each
instruction that is being executed in the processor.

Pipelining has been used since the 1960s to increase instruction throughput. Since then,

278

Chapter 7. Microarchitecture | User Mode

a number of hardware and software features have been developed to enhance the e(cid:14)ciency
and e(cid:11)ectiveness of pipelining. Not all pipelined data paths have all of these characteristics;
the ones we discuss below are quite prevalent.

Multi-ported Register File: When multiple instructions are being executed in a pipelined
data path, the same hardware block cannot be used for two instructions in the same clock
cycle. This requirement is straightforward to meet if each hardware block is accessed at most
in one piepline stage. A prime example of this is the ALU. It is required for an instruction
only in the A stage. An inspection of Table 7.8 reveals that the register (cid:12)le, on the other
hand, may be used by instructions present in two di(cid:11)erent stages of the pipeline|stages
R and M. Thus, in any given clock cycle, when the instruction in the R stage is trying
to read from the register (cid:12)le, a previous instruction in the M stage may be attempting to
write to the register (cid:12)le. The solution adopted in our data path is to make the register (cid:12)le
multi-ported. That is, the register (cid:12)le permits multiple accesses to be made in the same
clock cycle10 .

Separate Instruction Cache and Data Cache: Just like the case with register (cid:12)le
accesses, memory accesses may also be performed from multiple pipeline stages | instruc-
tion fetch from the F stage and data access from the M stage. If a single memory structure
is used for storing both the instructions and the data, then the data access done by a
memory-referencing instruction (load or store) will happen at the same time as the instruc-
tion fetch for a succeeding instruction. The data path of Figure 7.15 used two di(cid:11)erent
memory structures|an L1 instruction cache and an L1 data cache|to avoid this problem.
Another option is to use a single dual-ported memory structure.

Interconnect: Pipelined data paths typically use the direct path-based approach dis-
cussed earlier, which uses direct connections between the various hardware blocks. A bus-
based data path is not suitable for pipelining, because only one data transfer can take place
in a bus in a clock cycle. The direct path approach allows data transfers to take place
simultanesously in each pipeline stage, which indeed they must if the goal of allowing many
activities to proceed simultanesously is to be achieved. In fact, we have to add even more
interconnections that in a non-pipelined data path.

Pipeline Registers or Latches: When the processing of an instruction shifts from one
pipeline stage to the next, the vacated stage is utilized for processing the next instruc-
tion. Thus, consecutive stages of the pipeline contain information pertaining to di(cid:11)erent
instructions.
If adequate bu(cid:11)ering is not provided between consecutive stages, informa-
tion pertaining to di(cid:11)erent instructions can mix together, leading to incorrect execution.

10Making a register (cid:12)le multi-ported increases its size as well as the time it takes to read from or write to
the register (cid:12)le.

7.7. Pipelined Data Path: Overlapping the Execution of Multiple Instructions

279

Therefore, it is customary to separate consecutive stages by latches or registers. Because an
instruction can \occupy" only one pipeline stage at any given time, whatever information
is needed to execute that instruction at a subsequent stage must be carried along until that
stage is reached. This is the case even if the information is not required at every stage
through which it passes. Each instruction must \carry its own load," so to speak. Such
information might include opcode, data values, etc. A convenient way of propagating this
information is to insert extra pipeline latches between the pipeline stages. For example, the
incremented PC value is carried along, with the help of pipeline registers PC R, PC A, and
PC M.

Additional Hardware Resouces: Pipelined data paths usually need additional hard-
ware resources to avoid resource con(cid:13)icts between simultaneously executed instructions. A
likely candidate is a separate incrementer to increment the PC, freeing the ALU for doing
arithmetic/logical operations in every clock cycle.

7.7.2

Interpreting ML Instructions in a Pipelined Data Path

The ob jective of designing a pipelined data path is to permit the interpretation of multiple
instructions at the same time, in a pipelined manner. The MAL routines used for inter-
preting each instruction is similar to those for the direct path-based data path. Table 9.13
illustrates how the MAL routines for multiple instructions are executed in parallel.

Step
No.
0
1
2
3
4

5

lw rt, offset(rs)

Fetch instruction
Read registers
Rrs E + Off E ! AOR
DM[AOR] ! R[r W]

MAL Command
addu rd, rs, rt

beq rs, rt, offset

Fetch instruction
Read
registers
Rrs E + Rrt E ! AOR
AOR ! R[r W]

instruction
Fetch
registers
Read
Rrs E == Rrt E ! Z
PC E + Off E (cid:2) 4 ! Target
if (Z) Target ! PC

Table 7.9: Overlapped Execution of MAL Routines for Fetching and Executing Three MIPS-
0 Instructions in the Pipelined Data Path of Figure 7.15

7.7.3 Control Unit for a Pipelined Data Path

Converting an unpipelined data path into a pipelined one involves several modi(cid:12)cations, as
we just saw. The control unit also needs to be modi(cid:12)ed accordingly. We shall brie(cid:13)y look
at how the control unit of a pipelined data path works. With a pipelined data path, in each
clock cycle, the control unit has to generate appropriate microinstructions for interpreting

280

Chapter 7. Microarchitecture | User Mode

the instructions that are present in each pipeline stage. This calls for some ma jor changes
in the control unit, perhaps even more than what was required for the data path. A simple
approach is to generate at instruction decode time all of the microinstructions required
to interpret that instruction in the subsequent clock cycles (in di(cid:11)erent pipeline stages).
These microinstructions are passed on to the pipelined data path, which carries them along
with the corresponding instruction by means of extended pipeline registers. Then, in each
pipeline stage, the appropriate microinstruction is executed by pulling it out of its pipeline
register.

An alternate approach is for each pipeline stage to have its own control unit, so to
speak. Depending on the opcode of the instruction that is corrently occupying pipeline
stage i, the ith control unit generates the appropriate microinstruction and passes it to
stage i. Designing a control unit for a pipelined data path requires sophisticated techniques
which are beyond the scope of this book.

7.7.4 Dealing with Control Flow

The pipelined data path discussed above will not work correctly when the executed program
deviates from straightline sequencing. Such deviations happen whenever a branch instruc-
tion is taken (i.e., its condition is satis(cid:12)ed and control is transferred to its target address),
or a jump instruction is encountered. System call instructions also cause a change in control
(cid:13)ow, in addition to switching the system to kernel mode. The pipelined data path that we
have been studying does have the ability to perform the control (cid:13)ow deviation dictated by
branches, jumps, and syscalls. The Target register is updated with the appropriate target
address, and this value can be selectively loaded into the PC register, so that instructions
can be fetched from the target address thereafter. The problem, however, is that prior to
fetching instructions from the target address, the data path would have already fetched
the three instructions that immediately follow the branch/jump/syscall instruction. These
instructions, if left unhampered, will wreak havoc in the data path by causing unintended
changes to the ISA-visible state | the register (cid:12)le, PC, and the memory.

Consider the following MIPS program snippet. It starts with a conditional branch in-
struction at address 0x400500. If the branch condition is not satis(cid:12)ed, control (cid:13)ow proceeds
in a straightline fashion to fall-through instructions 1, 2, 3, .... By contrast, if the condition
is satis(cid:12)ed, control (cid:13)ow is transferred to the sub instruction, skipping the 3 add instructions.

0x400500:

beq $1, $2, 3

0x400504:
0x400508:
0x40050c:
0x400510:

add $3, $1, $2
add $5, $1, $4
add $6, $2, $4
sub $7, $3, $4

# skip next 3 instructions
# if values in $1 and $2 are equal
# fall-through instruction 1
# fall-through instruction 2
# fall-through instruction 3
# target address instruction

Now, let us track the execution of this program snippet in our pipelined data path. When

7.7. Pipelined Data Path: Overlapping the Execution of Multiple Instructions

281

this snippet is about to be executed, PC contains address 0x400500. Once this address is
in place in PC, fetching of the branch instruction automatically happens in the F stage. In
addition, the PC is also updated to 0x400504 in the F stage. In the next clock cycle, the
fetched branch instruction moves to the R stage of the pipeline, where reading of its source
registers $1 and $2 occurrs. At the same time, the F stage continues its action by fetching
fall-through instruction 1 from address 0x400504. In the third clock cycle, evaluation of
the branch condition takes place in the A stage. Calculation of the branch target address
(0x400510) also happens in the A stage. The outcomes of these actions are recorded in
the flags register and Target register, which sit at the boundary between the A and M
stages.
If the branch condition is satis(cid:12)ed, updation of the PC register with the target
address happens in the 4th clock cycle, when the branch instruction moves into the M
stage of the pipeline. By this time, stages A, R, and F of the pipeline would be populated
by fall-through instructions 1, 2, and 3, respectively. The pipeline latches present at the
boundaries of these stages would accordingly be updated by information pertaining to these
instructions. All of this is perfectly (cid:12)ne if the branch condition is not satis(cid:12)ed and program
control should continue to these instructions as per straightline sequencing. If the branch
condition is satis(cid:12)ed, on the other hand, then allowing these three instructions to proceed
would result in incorrect execution of the program. Speci(cid:12)cally, the value of register $3 used
by the target address instruction would be the one produced by fall-through instruction 1.

There are several ways to deal with the above control hazard problem. The most
straightforward among these is to detect the presence of a control hazard, and then squash
the unwanted instructions from the pipeline, if there is a change in control (cid:13)ow. Squashing
an instruction amounts to converting the instruction into a nop instruction within the
pipeline. Notice that the squashed instructions would not have made any changes to our
data path’s ISA-visible state such as the register (cid:12)le and the memory locations. Figure
***** illustrates this dynamic squashing of instructions within a pipeline.

The squashing method discussed above results in a loss of 3 clock cycles, as three in-
structions were converted to nop instructions. We call this 3-cycle loss as the branch
penalty. In most of the real-life programs, a control (cid:13)ow change is not an infrequent event
| roughly one in 6 of the executed instructions can be a control-changing instruction.
Loosing 3 clock cycles every 6 cycles is not very appealing. Microarchitects reduce this
penalty by incorporating one or more of the following techniques:

(cid:15) Branch latency reduction

(cid:15) Branch outcome prediction

(cid:15) Branch e(cid:11)ect delaying

7.7.4.1 Branch Latency Reduction

In our pipelined data path, the outcome of a conditional branch instruction is determined
in the A stage; calculation of the target address of control-changing instructions is also done

282

Chapter 7. Microarchitecture | User Mode

in the A stage. We can in fact determine the branch outcome in the R stage itself, as the
branch conditions in the MIPS-I ISA are quite simple (testing the equality of two register
values, or determining if the (cid:12)rst value is greater than or less than the second value). An
early computation such as this would, of course, require an extra comparator to be included
in the R stage.

In a similar manner, calculation of the target address can also be performed in the R
stage itself, as the ingredients for this calculation, such as the PC value and the Offset
value, are available in the R stage itself. An important point is in order here: to complete
the target address calculation in a timely manner in the R stage, the calculation may have
to be started before knowing for sure if target address calculation is indeed required, i.e.,
before instruction decoding has been completed. Moreover, in the MIPS-I ISA, the target
address calculation is di(cid:11)erent for the branch instructions and the jump instructions. This
means that both types of calculation have to be done in parallel, such that the correct one
among them can be selected after instruction decoding is completed. Notice also that such
speculative actions increase the power consumption within the data path.

Performing both branch condition evaluation and target address calculation in the R
stage reduces the branch penalty to just one cycle! Being able to determine the branch
outcome as well as the target address in the R stage may seem to be quite an achievement;
not so for modern pipelined processor data paths, which tend to have long, multi-stage,
fetch engines.
In these machines, several pipeline stages may precede the R stage; even
if branch resolution is done in the R stage, several clock cycles are lost for every branch
instruction. Can branch resolution be done earlier in such pipelines? Yes and no. While
complete resolution of a branch instruction can be done only after the source registers have
been read, a speculative resolution can be done before knowing the source register values.
Next, let us look at this widely used scheme for reducing branch penalties in deeply pipelined
machines.

7.7.4.2 Branch Outcome Prediction

When a conditional branch instruction is executed, one of two possible outcomes can hap-
pen: either the branch is taken or it is not taken. Once this outcome is known, the data path
can take appropriate actions for dealing with the instructions that were fetched following
the branch. Can the data path somehow predict the outcome of a branch instruction well
before its outcome becomes known? If it can do so, then it can take appropriate actions
based on the predicted outcome. If the prediction turns out to be correct, then the data
path does not su(cid:11)er from an inordinate branch penalty. If the prediction turns out to be
incorrect, then the data path su(cid:11)ers a branch misprediction penalty, which may even
be higher than the normal branch penalty. If the branch predictions turn out to be correct
the ma jority of the time, then it is worth the gamble!

Are branch instruction outcomes very predictable? Branch behavior is an artifact of
the way programmers write high-level programs. Extensive studies by computer architects

7.7. Pipelined Data Path: Overlapping the Execution of Multiple Instructions

283

with real-life programs have conclusively shown that branch outcomes are very predictable.
There are several reasons for this predictability. First, branches that are used to implement
loops tend to be taken many times in a row before they end up being not taken. These
branches are highly biased, in that they are taken many more times than they are not
taken. Second, many branches serve the function of testing for error conditions, which
come into existence very rarely. Such branches are also highly biased. Even if a branch is
not highly biased one way or the other, it may still be very predictable. Some of them depict
alternating behavior: taken, not taken, taken, not taken, .... Some others have outcomes
that have more complex, yet predictable, patterns. Yet others show behavior that is strongly
correlated to the outcomes of branches that immediately precede it. A detailed treatment
of branch behavior and branch prediction techniques is beyond the scope of this textbook.
We shall, however, outline some of the branch behavior and branch prediction schemes that
exploit such behavior. Before that, we need to see how the data path decides to make a
prediction before knowing that the instruction being fetched is a branch instruction. The
key to determining this is, again, temporal locality|branches that are executed now are
most likely to have been encountered before.

Branch Target Bu(cid:11)er (BTB): The branch target bu(cid:11)er is a microarchitectural struc-
ture for storing that target address of previously encountered branch instructions.
It is
usually indexed by hashing the pc value of the instruction. Because multiple pc values can
hash to the same entry in the BTB, a pc (cid:12)eld is kept in each BTB entry to identify the
address of the branch instruction that is currently mapped to that entry. Figure 7.16 shows
the organization of a BTB. This BTB has two entries occupied with information about the
branches at addresses 0x400800 and 0x400020. Their target addresses are 0x400880 and
0x400008. While fetching an instruction from the instruction cache, the BTB is consulted
in parallel to determine if the instruction being fetched is a branch instruction (that has
been seen before), and if so, to obtain its target address. A miss in the BTB indicates that
the instruction is either not a branch instruction or a branch instruction that is unlikely
to have been encountered in the recent past. In either case, if there is a miss in the BTB,
pipelined execution proceeds as normal, as in straightline sequencing.

Loop-terminating branches: As mentioned earlier, branch instructions that terminate
a loop tend to be taken many more times than they are not taken. A simple way to
exploit this behavior is to speculatively start executing from the target address whenever
a loop-terminating branch is encountered. How can the microarchitecture identify a loop-
terminating branch? A simple heuristic to use is to consider all branches with negative
o(cid:11)sets to be loop-terminating branches.

Dynamic branch prediction: Branches that are not loop-terminating may not be heav-
ily biased in one direction; however, they may still have predictable behavior. The best way
to capture such behavior is to use a microarchitectural structure that keeps track of the

284

Chapter 7. Microarchitecture | User Mode

pc

target

0x400800

0x400880

0x400020

0x400008

Hash

Figure 7.16: Organization of a Branch Target Bu(cid:11)er (BTB)

recent history of branches, and predicts the outcome every time a branch instruction is
fetched. The scheme used for prediction could range from the simple Last Outcome based
prediction to the most complex Perceptron based prediction. The Last Outcome predictor
.........

7.7.4.3 Branch E(cid:11)ect Delaying

Branch prediction is a popular technique for reducing branch penalty in high-performance
processors. However, it calls for higher power consumption as well as hardware complexity.
The hardware structures used to store the past behavior of branches can be quite large,
taking up space in the processor chip. They can also consume a signi(cid:12)cant amount of elec-
trical power. Both of these factors can make branch prediction less attractive in embedded
systems where size and power consumption may be constrained. Moreover, real-time system
designers like to have very little uncertainities in the system; with branch prediction, it is
di(cid:14)cult to accurately estimate the running time of a program. In such a scenario, we can
employ yet another scheme for reducing branch penalty: rede(cid:12)ne the semantics of a branch
instruction to mean that a (cid:12)xed number of instructions after the branch instruction should
be executed irrespective of the outcome of the branch.

7.7.5 Dealing with Data Flow

The pipelined data path discussed above will work correctly as long as the instructions in
the pipeline do not need to read registers that are updated by earlier instructions that are
still present in the pipeline. There is indeed a problem when an instruction has to read a

7.7. Pipelined Data Path: Overlapping the Execution of Multiple Instructions

285

register that will be updated by an earlier instruction. Consider the following instruction
sequence:

add
$3, $1, $2
addi $4, $3, 16

The (cid:12)rst instruction updates register $3, which is the source register for the second instruc-
tion. When these two instructions are executed in a non-pipelined data path, fetching and
execution of the second instruction begins only after the execution of the (cid:12)rst instruction
is completed and its result is written in $3. By contrast, in a pipelined data path, the
execution of the two instructions overlap in time. Let us trace this overlapped execution
and see what happens. Figure 7.17 shows a timeline depicting the ma jor actions happening
in the pipeline.

0

1

2

3

Clock Cycles

add  $3, $1, $2

Fetch

Read $1,$2 Add

Update $3

addi $4, $3, 16

Fetch

Read $3

Add

Update $4

Reads incorrect value

Figure 7.17: An Example Illustrating Data Hazards in a Pipelined Processor

The (cid:12)rst instruction starts execution in clock cycle 0, and completes in clock cycle 3
when it writes its result to register $3. The second instruction starts execution in clock cycle
1, and reads register $3 in clock cycle 2. As this read operation happens before the write
operation of clock cycle 3, it obtains the value that was in register $3 prior to the execution
of the (cid:12)rst instruction. This clearly violates the semantic meaning of the program.

There are di(cid:11)erent ways to prevent such a data hazard from causing incorrect execu-
tion. The simplest among them is to detect the presence of data hazards, and temporarily
switch to unpipelined operation if and when a data hazard is detected. In the above exam-
ple, for instance, if the second instruction is delayed by 2 cycles as shown in Figure 7.18,
then its register read operation happens only after the (cid:12)rst instruction performs its register
write operation. Of course, we need to introduce additional hardware to detect the pres-
ence of data hazards. Whenever a register-reading instruction reaches the R stage of the
pipeline, this hardware should check if its source registers match the destination registers
of the instructions present in the A and M stages of the pipeline.

The additional hardware for detecting data hazards turns out to be straightforward to
design, once we list all data hazard scenarios. For the pipelined data path that is on our
plate, we can group the data hazards into the following scenarios:

286

Chapter 7. Microarchitecture | User Mode

Clock Cycles

0

1

2

3

4

5

6

I0:

add  $3, $1, $2

Fetch

Read $1,$2 Add

Update $3

I1:

addi $4, $3, 16

Fetch

Stall

Stall

Read $3

Add

Update $4

Pipeline Occupancy

I0

I1

I0

I1

I0

I1

I0

I1

I1

I1

Reads correct value

Figure 7.18: An Example Illustrating the use of Pipeline Stalls to deal with Data Hazards
in a Pipelined Processor

1. lw
rt, offset(rs) # any load instruction (register rt is destination)
# any R-format instruction or branch-type instruction
add rd, rs, rt
# (registers rs and rt are sources)

A data hazard exists if the rt value of lw is equal to the rs or rt values of add.

2. lw
rt, offset(rs) # any load instruction (register rt is destination)
addi rt, rs, immed
# any I-format arithmetc/logic instruction
# (register rs is source)

A data hazard exists if the rt value of lw is equal to the rs value of addi.

3. lw
...
add rd, rs, rt

rt, offset(rs) # any load instruction (register rt is destination)
# any instruction that doesn’t have register rt as source

4. lw
...
addi rt, rs, immed

rt, offset(rs) # any load instruction (register rt is destination)
# any instruction that does not have register rt as source
# any I-format arithmetc/logic instruction
# (register rs is source)

7.7.6 Pipelines in Commercial Processors

Pipelining has been in use for several decades. One of the earliest machines to use pipelining
was the CDC 6600. The MIPS R2000 and R3000 processors, which came out in 198* and
199*, respectively, used the 5-stage pipeline that we just looked at. Intel’s Pentium processor
also used a relatively short 5-stage pipeline. The Pentium Pro/II/III series of processors
used a deeper pipeline having 12 stages, whereas the recent Pentium IV uses a very deep
pipeline having 20 stages.

7.8. Wide Data Paths: Superscalar and VLIW Processing

287

jump
branch

Instruction Address

L1
Instr.
Cache

Instruction
rs

IR

rt

rd

Opcode
Func

MUX

PC

4

+

PC_D

Fetch (F)

31 0

MUX

rs
rt
Register File
(RF)

Sign
Extend

Register Read (R)

MUX

MUX

=
=
=
=

rd_E

Rrs_E

Rrt_E

Offset_E

PC_E

)
W
_
r
(
 
r
e
t
s
i
g
e
R
 
n
o
i
t
a
n
i
t
s
e
D

rd_M

MUX
Jump Target

ALU

+

Branch Target

MUX

Flags

AOR

Rrt_M

PC_M

Target

L1
Data
Cache

ALU Result

Return Address

MUX

Processor Data Path

ALU (A)

Memory (M)

Figure 7.19: A Pipelined Processor Data Path with Data Forwarding

7.8 Wide Data Paths: Superscalar and VLIW Processing

We have seen how pipelining the datapath overlaps the execution of multiple instructions,
thereby executing more instructions in unit time. The maximum instruction execution
rate that can be achieved with pipelining, however, is only one instruction per clock cycle;
notice that during every clock cycle, at most one new instruction is entering the pipeline (at

288

Chapter 7. Microarchitecture | User Mode

the fetch stage). Modern processors surpass this execution rate by widening the pipelined
data path. This means that every clock cycle, they fetch multiple instructions in parallel,
decode them in parallel, and execute them in parallel. This type of multiple-issue is done
in addition to the pipelining technique described above. For multiple-issue to bear fruit,
the processor data path should have multiple ALUs, or functional units.

Wide-issue datapaths have multiple instructions in each stage of the pipeline, and there-
fore overlap the execution of a large number of instructions. This exacerbates the control
hazards and data hazards problem.

Compared to a single-wide pipeline, wider pipelines overlap the execution of a larger
number of instructions, exacerbating the problem of control hazards and data hazards. For
instance, when a control hazard is detected, more instructions are (cid:13)ushed from the pipeline,
in general. The complexity of hazard detection and corrective action due to the hazards
increases in two ways:

(cid:15) More instructions are present in the pipelined datapath

(cid:15) Hazards may be present among the multiple instructions present in the same stage of
the pipeline

The (cid:12)rst one among these is similar to what happens in a deep pipeline, and calls for rigorous
application of the techniques discussed in Sections 7.7.4 and ?? for dealing with control
hazards and data hazards | techniques such as branch prediction and data forwarding.
Thus, we need branch predictors that are highly accurate, for instance. We also need many
more paths for forwarding data.

Hazards introduced due to the presence of multiple instructions in the same pipeline
stage ....

[This section needs to be expanded.]

7.9 Co-Processors

7.10 Processor Data Paths for Low Power

\The ultimate \computer," our own brain, uses only ten watts of power { one-tenth
the energy consumed by a hundred-watt bulb."
| Paul Valery, Poet and Essayist

All of this chapter’s discussion so far focussed primarily on performance. While perfor-
mance is one of the primary considerations in microarchitecture design, power consumption
is becoming an important design consideration. Power consumption in a hardware circuit
is the sum of the power consumed by its individual low-level components (primarily the
transistors). The dynamic power consumed by a transistor is directly proportional to the

7.10. Processor Data Paths for Low Power

289

activity in the transistor in unit time, which can be loosely quanti(cid:12)ed as the product of
number of times it switched in unit time, its capacitance, and the square of the supply
voltage. The static power consumed by a transistor is directly proportional to its supply
voltage and leakage current.

From the above discussion, it is clear that power consumption can be reduced by reducing
the supply voltage and the number of transistors. Reducing the supply voltage, although
feasible up to a point, does have the drawback of reducing the noise immunity of the circuit.
Reducing the number of transistors directly translates to reducing the hardware circuitry
in the design. In many situations, this is likely to hurt performance. Thus, there is oftan a
trade-o(cid:11) between performance and power consumption. Intelligent designs attempt to use
as little hardware as possible without reducing the performance.

Recent Processors Intel ***

Recent Processors

290

Chapter 7. Microarchitecture | User Mode

7.11 Concluding Remarks

7.12. Exercises

7.12 Exercises

291

1. Consider a non-MIPS ISA that has variable length instructions. One of the instruc-
tions in this ISA is ADD (ADDR), which means add the contents of memory location
ADDR to ACC register. This instruction has the following encoding; notice that it oc-
cupies two memory locations.

Bit Pattern
for ADD

Bit Pattern
for mem dir
addressing

Bit Pattern for ADDR

Specify a MAL routine to carry out the interpretation of this instruction in the data
path given in Figure 9.12.

Main

Memory

RD

WR

Addr

Data_out

Data_in

1

+/-

SP

IR

ID

IR<Addr2>

IR<Addr1>

ADD

AND

CU

F0
F1

Len

PC

MAR

MDR

ALU

ACC

WR

Internal Bus

CPU

Figure 7.20: A Single-Bus based Processor Data Path for Interpreting an ACC based ISA

2. Consider the non-MIPS instruction ADD (ADDR1), (ADDR2), where ADDR1 and ADDR2
are memory locations whose contents needed to be added. The result is to be stored
in memory location ADDR1. This instruction has the following encoding; notice that

292

Chapter 7. Microarchitecture | User Mode

addresses ADDR1 and ADDR2 are speci(cid:12)ed in the words subsequent to the word that
stores the ADD opcode.

Bit Pattern
for ADD

Bit Pattern
for mem dir
addressing

Bit Pattern
for mem dir
addressing

Bit Pattern for ADDR1

Bit Pattern for ADDR2

Word 0

Word 1

Word 2

Specify the data transfer operations required to fetch and execute this instruction in
the data path given in Figure 9.12. Notice that register ACC is a part of the ISA,
and therefore needs to retain the value that it had before interpreting the current
instruction. You are allowed to grow the stack in the direction of lower memory
addresses.

3. Explain how adding multiple CPU internal buses can help improve performance.

4. Explain with the help of diagram(s) how pipelining the processor data path helps
improve performance.

5. Consider a very small direct mapped cache with a total of 4 block frames and a block
size of 256 bytes. Assume that the cache is initially empty. The CPU accesses the fol-
lowing memory locations, in that order: c881H, 7742H, 79c3H, c003H, 7842H, c803H,
7181H, 7381H, 7703H, 7745H. All addresses are byte addresses. To get partial credit,
show clearly what happens on each access.

(a) For each memory reference, indicate the outcome of the reference, either \hit" or
\miss".

(b) What are the (cid:12)nal contents of the cache? That is, for each cache set and each
block frame within a set, indicate if the block frame is empty or occupied, and if
occupied, indicate the tag of the memory block that is currently present.

Chapter 8

Microarchitecture | Kernel Mode

A cheerful look brings joy to the heart, and good news gives health to the bones

Proverbs 15: 30

Look at Proverbs 23: 23

The previous chapter discussed aspects of the microarchitecture that deal with the imple-
mentation of the user mode ISA. The kernel mode ISA includes several additional features,
such as privileged registers, privileged memory address space, IO registers, and privileged
instructions. This chapter focuses on microarchitecture aspects that deal speci(cid:12)cally with
the implementation of these additional features present in the kernel mode ISA. The func-
tionality served by these features can be classi(cid:12)ed under 3 broad categories: processor
management, memory management (virtual memory system), and IO system. This chapter
is organized precisely along this classi(cid:12)cation.

The (cid:12)rst part of the chapter discusses microarchitectural aspects that are essential for
performing context switches at times of system calls, exceptions, interrupts, and return
from exception. The second part provides an in-depth treatment of virtual memory. After
presenting the ma jor concepts of virtual memory, we take a close look at the virtual memory
scheme in a MIPS-I system. Finally, the last part of the chapter address IO subsystem design
and other system architecture issues.

8.1 Processor Management

Processor management involves allocating the processor to a particular process. As we saw
in Chapter 4, the processor is generally time-multiplexed among the active processes in the

293

294

Chapter 8. Microarchitecture | Kernel Mode

system1 . We can break down the task of processor management into three aspects: recog-
nizing exceptions and interrupts; enabling and disabling of interrupts; and switching back
and forth between the user and kernel modes. We shall look at how the microarchitecture
recognizes exceptions, and performs switching between the user and kernel modes.

8.1.1

Interpreting a System Call Instruction

In Chapter 7, we brie(cid:13)y discussed the execution of syscall instructions. However, we did not
speci(cid:12)cally say what happens after the system is placed in the kernel mode. We shall see
here how the data path implements this part of the syscall instruction execution. Again, our
discussion is based on the MIPS-I ISA, although the underlying principles are applicable
to other ISAs as well. It is important to recall from Section 4.2.1 the ma jor functionality
speci(cid:12)ed by an instruction like syscall. We highlight these functions below:

(cid:15) Switch to kernel mode: In the MIPS-I ISA, this is done by modifying the state
register.

(cid:15) Disable interrupts:
register.

In the MIPS-I ISA, this is also done by modifying the state

(cid:15) Save return address: In the MIPS-I ISA, the return address of a syscall instruction is
saved in the privileged register called epc (exception program counter).

(cid:15) Record the cause for this exceptional event: In MIPS-I ISA, the same entry point
(0x80000080) is speci(cid:12)ed for all but two of the exceptional events. Therefore, to
identify the reason for transferring control to this memory location, the syscall code
is entered into the ExcCode (cid:12)eld of cause register.

(cid:15) Update PC to point to the entry point associated with syscall (0x80000080).

In order to perform these functions, the data paths presented in Chapter 7 need to be
enhanced to include the privileged registers state, cause, and epc. Figure 8.1 shows a
data path obtained by including these privileged registers in the dat path of Figure 7.7.

Table 8.1 presents a MAL routine for executing the MIPS-I syscall instruction in the
data path of Figure 8.1. The (cid:12)rst step of this routine updates the state register to place
the system in the Kernel mode and to disable device interrupts. The next step saves the
address of the syscall instruction in the EPC register. PC is then updated with the entry
point address 0x80000080. Because this entry point is common for many exceptional events,
the ExcCode (cid:12)eld of the cause register is set to 8 to indicate that the exceptional event in
this case is a syscall instruction.

1This is in contrast to the memory system and the IO system, which are generally space-multiplexed
among the active processes.

8.1. Processor Management

295

To Control Unit

Cause

EPC

SR

Shift

PRE_PC

PC

IR

Instr.
Decoder

32

Register Address

5

Register File
(RF)

s
u
B
 
r
o
s
s
e
c
o
r
P

Memory Interface

Kernel Mode
Registers

MAR

System Address Bus

Memory
Subsystem

System Data Bus

Register Data

MDR

ALU

Flags

Processor Data Path

Figure 8.1: A Data Path for Implementing the MIPS-0 Kernel Mode ISA

8.1.2 Recognizing Exceptions and Hardware Interrupts

A realistic processor must do more than fetch and execute instructions.
It must handle
exceptions and also respond to device interrupts, irrespective of whether it is in the user
mode or the kernel mode. The processor must explicitly check for exceptions whenever an
exception can occur. Similarly, at the end of executing an ML instruction, it must check if
there are any pending interrupts from IO devices. The actions taken in both these cases are
somewhat similar to those taken when executing a syscall instruction. After all, a syscall
instruction is a software interrupt.

Consider the add instruction of the MIPS-I ISA. This instruction is similar to the addu
instruction whose execution we saw in detail in Chapter 7.
It speci(cid:12)es the contents of
registers rs and rt to be added and the result to be placed in register rd. The only di(cid:11)erence

296

Step
No.

4

5
6
7

Chapter 8. Microarchitecture | Kernel Mode

Next Step No. MAL Instruction to be Generated
for Control Unit
for Data Path
Execute phase
SR<3:0> << 2 ! SR<5:0>

Comments

Set system in kernel mode
and disable interrupts
Pre PC ! EPC
Save instruction address
0x80000080 ! PC
Set PC to 0x80000080
8 ! cause<ExcCode> Update cause register

Goto step 0

Table 8.1: A MAL Routine for carrying out the Execute Phase of the MIPS-0 Instruction
syscall

between add and addu is that add speci(cid:12)es checking for arithmetic over(cid:13)ow (assuming the
integers to be in the 2’s complement number system).
In the event of an over(cid:13)ow, an
exception should be generated. Table 8.2 presents a MAL routine for interpreting the add
instruction.

Step
No.

Next Step No.
for Control Unit

4
5
6

7

8
9
10

if (cid:22)Ov goto step 0

Goto step 0

MAL Instruction to be Generated
for Data Path
Execute phase
R[rs] ! AIR
R[rt] + AIR ! AOR, Ov
if ( (cid:22)Ov && rd) AOR ! R[rd]
Take Over(cid:13)ow Exception
SR<3:0> << 2 ! SR<5:0>

Comments

Update Ov (cid:13)ag also

Set system in kernel mode
and disable interrupts
Save instruction address
Pre PC ! EPC
0x80000080 ! PC
Set PC to 0x80000080
12 ! cause<ExcCode> Update cause register

Table 8.2: A MAL Routine for the Execute Phase of the Interpretation of the MIPS-0
ISA Instruction Represented Symbolically as add rd, rs, rt. This MAL Routine is for
executing the ML instruction in the Data Path of Figure 8.18

The (cid:12)rst step of this MAL routine is same as before. In the ALU operation step, in
addition to writing the addition result in AOR, the Ov (over(cid:13)ow) (cid:13)ag is updated to indicate
the occurrence of over(cid:13)ow, if any. The next step explicitly checks for the value of Ov. If
this (cid:13)ag is not set, then the addition result is written to register rd and control goes back
to step 0. Otherwise, the microarchitecture proceeds to step 7 which begins the routine for
taking the exception. This part is the same as that of the syscall instruction except that
the code written in the ExcCode (cid:12)eld of the cause register is 12.

8.2. Memory Management: Implementing Virtual Memory

297

8.1.3

Interpreting an RFE Instruction

The kernel mode includes many privileged instructions, especially for manipulating di(cid:11)erent
privileged registers. One of the kernel mode instructions that warrants special treatment is
the rfe (restore from exception) instruction. This instruction tells the machine to restore
the system’s mode and interrupt status to what it was prior to taking this exception.
Ironically, it does not tell the machine to transfer control to the interrupted program; a
standard jr instruction is used to achieve this transfer of control, as we saw in Chapter 4.
Table 8.3 presents the MAL routine for the execute phase of the rfe instruction.

Step
No.

4

Next Step No. MAL Instruction to be Generated
for Data Path
for Control Unit
Execute phase
SR<5:0> >> 2 ! SR<3:0>

Goto step 0

Comments

Restore system mode
and interrupt status

Table 8.3: A MAL Routine for the Execute Phase of the Interpretation of the MIPS-0 kernel
mode Instruction rfe

8.2 Memory Management: Implementing Virtual Memory

8.2.1 Virtual Memory: Implementing a Large Address Space

Next, let us turn our attention to the implementation of the memory address space de(cid:12)ned in
the ISA. The most straightforward approach to is to implement the entire memory address
as system memory (i.e., physical memory). Modern ISAs, however, specify quite a large
address space, spanning 232 (i.e., 4 G) or even 264 locations. Implementing such a large
address space as physical memory is not a worthwhile proposition from the economic point
of view, despite today’s falling memory prices.

The above problem becomes more acute in a multitasked system. Recall from Chapter
5 that modern computer systems incorporate multitasking, permitting multiple processes
to be simultaneously active in the system. The processor is time-multiplexed among the
active processes by the operating system. At the time of a context switch, the current
state of the processor (including the register values) is saved in the kernel address space,
and the processor state is updated with the previously stored state of the process that is
scheduled to run next. It is important to note, however, that a process’ state is not limited
to its processor state; the state includes the memory values too. Therefore, besides saving
and restoring the register values, the memory values also need to be saved and restored.
However, time-multiplexing the system memory is quite impractical, as it requires writing
and reading huge amounts of data to a slow secondary storage device such as a hard disk.
Each context switch will then take seconds or even minutes! Again, a brute-force solution

298

Chapter 8. Microarchitecture | Kernel Mode

would be to provide a separate physical memory for each active process. But such a naive
approach stretches the physical memory requirement even further, especially considering
today’s high degrees of multitasking (64 processes and more). Imagine a computer system
with 64 (cid:2) 4 GB = 256 GB of physical memory!

The above discussion highlights the di(cid:14)culty of implementing the entire memory address
space of one or more processes by physical memory. Most of today’s computer systems
overcome this di(cid:14)culty by using a scheme called virtual memory. In this scheme, the ISA-
de(cid:12)ned memory address space is implemented at the microarchitectural level by a two-tier
memory system, consisting of a small amount of physical memory at the upper tier and
a large amount of swap space at the lower tier, managed by the operating system 2 . The
memory addresses generated by the processor, called virtual addresses, are translated into
physical addresses on the (cid:13)y by a Memory Management Unit (MMU). If a virtual address
refers to a part of the program or data space that is mapped to the physical memory, then
the contents of the appropriate location in the physical memory are accessed immediately.
On the other hand, if the referenced location is mapped to the swap space, then an exception
is generated to turn control over to the operating system, which gets the requested word
after a much longer time. Finally, if the referenced location is not mapped to either the
physical memory or the swap space, then special action needs to be taken by the operating
system. The low-level application programmer, as we saw in Chapter 3, is not aware of the
limitations imposed by the smaller amount of physical memory available.

While the physical memory can be accessed in a few processor cycles, accessing the swap
space takes millions of processor cycles. Because of this huge discrepancy, unless the number
of accesses to the swap space is limited to a very tiny fraction of the total references, the
performance of the system will be very poor. It is imperative that the operating system
does a good job here, and we will see how it does this in the next chapter.

Protection and Sharing: Another important issue in memory management in a multi-
tasking environment is one of protection. When the physical memory is partitioned among
multiple processes, the operating system has to protect itself and the others from accessing
each other’s memory.

Next, let us turn our attention to the implementing the virtual memory concept that
we have been discussing. Recall that the primary purpose of a virtual memory system is to
implement the large address space de(cid:12)ned in modern ISAs in a cost-e(cid:11)ective manner. Let
us (cid:12)rst highlight the main features of this memory system. In a virtual memory system,
the ISA-de(cid:12)ned memory address space is implemented at the microarchitectural level by a
two-tier memory system. This two-tier consists of a small amount of physical memory at
the upper tier and a large amount of swap space at the lower tier. The management of the
system is done by a combination of hardware (the memory management unit (MMU)) and
software (the memory management system of OS). The memory addresses generated by the

2The operating system generally stores the swap space in a hard disk.

8.2. Memory Management: Implementing Virtual Memory

299

Microarchitecture Level: Kernel Mode

ISA Level

Virtual Memory

Microarchitecture Level: User Mode
Updated only in Kernel Mode
Address
Translator

Physical Memory

CPU

Virtual
Address

Swap Space

Memory Mapping Exception

Addresses available in physical memory

Addresses available in swap space

Addresses available nowhere

Figure 8.2: Illustrating the Concept of Virtual Memory

processor, called virtual addresses, are translated into physical addresses on the (cid:13)y by the
MMU. If a virtual address refers to a part of the program or data space that is mapped to
the physical memory, then the contents of the appropriate location in the physical memory
are accessed immediately. On the other hand, if the referenced location is mapped to the
swap space, then the access time will be much longer. Finally, if the referenced location is
not mapped to either the primary memory or the swap space, then special action needs to
be taken by the system. Thus, the low-level application programmer sees a single memory
address space (virtual address space), and is not aware of the limitations imposed by the
smaller amount of physical memory available.

Today’s general-purpose computer systems have physical memory ranging from about 32
MB to about 512 MB, which is less than the memory address space de(cid:12)ned by the ISA for a
single process. The operating system partitions the available physical memory among itself
and the other active processes in the system, although not necessarily in equal portions.
Some portions are common to multiple processes so that they can share common data.
\What belongs to everybody belongs to nobody."
| Spanish Proverb

fbox

300

Chapter 8. Microarchitecture | Kernel Mode

Microarchitecture Level: Kernel Mode

ISA Level

Virtual Memory

Microarchitecture Level: User Mode
Updated only in Kernel Mode
Address
Translator

Physical Memory

CPU

Virtual
Address

Swap Space

Disk

Addresses available in physical memory

Addresses available in swap space

Addresses available nowhere

Figure 8.3: Illustrating the Implementation of Virtual Memory

Whereas the access time of the physical memory equals a few processor cycles, the access
time of the swap space equals millions of processor cycles. Because of this huge discrepancy,
unless the number of accesses to the swap space is a very tiny fraction of the total references,
the performance of the system will be really bad. In order to obtain good performance, the
mapping (from virtual addresses to physical addresses) is dynamically adapted in such a
manner that the locations that were frequently accessed in the recent past (and are therefore
expected to be accessed again in the near future) are mapped to the physical memory. The
infrequently accessed locations are mapped to the swap space.

With the above arrangement, whenever a memory access gets translated to the swap
space, the contents of that location are brought into a suitable location in the physical
memory prior to using them. At that time, if there is no empty space in the physical
memory, then the displaced contents are automatically swapped out to the swap space.
Thus, programs and their data are automatically moved between the physical memory and

8.2. Memory Management: Implementing Virtual Memory

301

secondary storage in such as way as to capture temporal locality among memory accesses.

8.2.2 Paging and Address Translation

To facilitate the implementation of virtual memory and to e(cid:11)ectively capture the spatial
locality present among the memory references (which is very important for obtaining good
performance), the translation from virtual addresses to physical addresses is done at a
granularity much larger than that of individual addresses. Two distinct schemes and their
combinations are widely used|paging and segmentation.

The paging scheme is similar to the cache memory scheme that we saw in the previous
chapter. In this scheme, the translation is done at the granularity of equal (cid:12)xed-size chunks
called pages. Each page consists of a block of words that occupy contiguous addresses in
the address space. It constitutes the basic unit of information that is transferred between
the physical memory and the swap space in the disk whenever the translation mechanism
determines that a transfer is required. (Pages are similar to blocks used in cache memory,
but are much bigger.) The virtual address space is partitioned into virtual pages, and the
physical memory is partitioned into page frames. Page sizes commonly range from 2 KB to
16 KB in size. Page size is an important parameter in the performance of a virtual memory
system.
If the pages are too small, then not much of spatial locality will be exploited,
resulting in too many page faults (i.e., accesses to the secondary storage). Given that the
access time of a magnetic disk is much longer (10 to 20 milliseconds) than the access time
of physical memory, it is very important to keep the number of page faults at an extremely
small value. On the other hand, if pages are too large, it is possible that a substantial
portion of a page may not be used, and temporal locality cannot be exploited to the desired
extent.

Information about the mapping from virtual page numbers to page frame numbers is kept
in tabular form in a structure called page table. Figure 8.4 shows the basic organization
of a page table, along with how it is used in address translation. The page table has
one entry for each virtual page. Each page table entry (PTE) has at least three (cid:12)elds|
Protection (cid:12)eld, Valid bit, and Page Frame Number (PFN). The Protection (P) (cid:12)eld
stores the access rights for the virtual page. The Valid (V) bit indicates if the virtual
page is currently mapped to physical memory, in which case page frame number (PFN) is
available the PFN (cid:12)eld.

The actions involved in translating a virtual address to a physical address are best
expressed using a (cid:13)ow chart. Figure 8.5 presents such a (cid:13)owchart. This (cid:13)owchart starts at
the top left corner. We can conceptually divide this (cid:13)owchart into two parts. The actions
on the left side of the (cid:12)gure are the frequently encountered ones and are best done by the
hardware (MMU). Those on the right are encountered infrequently and are done by the
software (MMS). In this (cid:13)owchart, the page table is assumed to be a hardware structure
that can be read in the Kernel mode as well as in the User mode; of course, writes to the page
table can only be done in the Kernel mode. Later, we will see more modern organizations in

302

Chapter 8. Microarchitecture | Kernel Mode

Virtual Address (Generated by Processor)

Virtual Page Number (VPN)

Offset within Page

0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

01

1

0

0 0 0

0

0

0

0 0 0

11

P

V

Page Frame Number (PFN)

Physical Memory

VP 5

Current Process’
Page Table

0 0

0 0 0 1

1

00

0 0

0
...

VPN 0
VPN 1
VPN 2
VPN 3
VPN 4
VPN 5
VPN 6

VPN 1M−1

Physical Address

0

0

0 0

0

0 0

0 0 0 1

0

0 0 0

0 0

0

0 0 0

11

Page Frame Number (PFN)

Offset within Page

PF 1, offset 3

..
.

...

...

...

location 0
location 1
location 2
location 3

location 4095
location 0
location 1
location 2
location 3

location 4095
location 0
location 1
location 2
location 3

location 4095

PF 0

PF 1

PF 2

location 0
location 1
location 2
location 3

location 4095

PF 2047

Figure 8.4: Virtual Address to Physical Address Translation Using a Page Table

which the page table is stored within the memory address space itself ! While going through
the sequence of actions of the (cid:13)owchart, it is useful to consult Figure 8.4 for a structural
understanding of what goes on.

Given a virtual address, the MMU splits it into two parts|a Virtual Page Number
(VPN) followed by an Offset within Page that speci(cid:12)es the location within the page.
For instance, in Figure 8.4, the VPN corresponding to virtual address 0x5003 is 5. The
MMU uses the VPN value as an index into the page table, and reads the corresponding
page table entry (PTE). If the Valid (V) bit of the PTE is set, then the virtual page is
currently present in the physical memory, and translation continues. This is the frequent
case. If the protection bits of the PTE indicate that the access is not permitted, then the
MMU generates a memory access violation exception to transfer control to the OS, which
generates an error message and terminates the o(cid:11)ending process. If the access is a permitted
one, the MMU calculates the physical address by concatenating PFN and Offset within
Page. The physical memory access is then performed using the physical address obtained.

8.2. Memory Management: Implementing Virtual Memory

303

Determine VPN & Offset

Lookup in PT with VPN

Is VP present
in PM?

Yes

Update PT Access Info

Is this type of 
access permitted?

Yes

Determine PA

Perform Memory Access

Hardware
(MMU)

Is VP present
in SS?

Yes

Copy Page from SS

Allocate new VP

Page Fault Exception

Update PT

Memory Access Violation Exception

Print Error Message

Terminate Process

Software
(MMS)

Figure 8.5: A Flowchart of the Steps Involved in Address Translation with a Page Table

In Figure 8.4, the PFN is 1, and the physical address is 0x1003. This location is shaded in
the (cid:12)gure.

If the Valid bit of the relevant PTE is not set, the requested VPN is not currently
mapped to physical memory, and the MMU generates a page fault exception. The genera-
tion of the exception automatically switches control to the OS, and eventually to the MMS
part of the OS. The MMS performs one of two things: (i) copy the required page from the
swap space to a page frame (if this involves replacing a dirty page, the old page is copied
to the swap space), or (ii) \create" a new page if the virtual page does not yet \exist" (for
example, when a process allocates a new stack frame in an unchartered territory). It then
updates the corresponding PTE to re(cid:13)ect the new translation, and transfers control back
to the interrupted process, which results in re-execution of the instruction that caused the

304

Chapter 8. Microarchitecture | Kernel Mode

page fault exception. Thus, page faults and page table updates are typically handled by
the OS software.

Example: In order to clarify these ideas further, let us go through an example based on
Figure 8.4. The example also gives an idea about the exorbitant space requirement of page
tables. Assume that the ISA (instruction set architecture) speci(cid:12)es a 32-bit address space
in which each address corresponds to a byte. The virtual memory system uses a page size
of 4 KB. The computer has 8 MB of physical memory.

1. How many bits of the 32-bit address are used to determine the Offset within Page?
Because the page size is 4 KB, the number of bits needed to determine the Offset
within Page is log2 4K = 12 bits.

2. How many virtual pages exist?
The virtual memory address space consists of 2 32 bytes. The number of virtual pages
is given by dividing the virtual address space size by the page size, and is therefore
equal to 232 bytes (cid:4) 4 KB = 220 = 1 M.

3. How many page frames exist?
The physical memory consists of 8 MB. The number of page frames is obtained by
dividing the physical memory size by the page size, and is therefore equal to 8 MB (cid:4)
4 KB = 2K.

4. If each PTE occupies 4 bytes, how many bytes will the page table occupy?
The page table has as many entries as the number of virtual pages, namely 1 M. If
each PTE occupies 4 bytes, the page table occupies 1 M (cid:2) 4 bytes = 4 MB.

5. What is the physical address corresponding to virtual address 0x00005003?
The least signi(cid:12)cant 12 bits of the virtual address (0x003) give the O(cid:11)set within Page,
and the remaining bits (0x00005) give the VPN. The MMU then identi(cid:12)es the PTE
corresponding to VPN 5. The V bit of this PTE is set, indicating that the PFN (cid:12)eld is
valid. Therefore, the PFN value is 0x001. On concatenating the PFN with the O(cid:11)set
within Page, we get the physical address of 0x001003.

8.2.3 Page Table Organization

An issue that we have skirted so far is the physical location of the page table. A few
decades ago, when address spaces were much smaller, the page table was small enough to
be stored in the MMU hardware structure, permitting fast address translations. As memory
address spaces grew, page tables grew along with them, eventually forcing system designers
to migrate them to the memory address space itself. Thus, the current practice is to store
the page tables of all active processes in the memory address space. The starting address
of each page table can be either calculated from the ASID (address space ID) or is recorded

8.2. Memory Management: Implementing Virtual Memory

305

in a separate table. But, this approach of storing the page tables in the memory address
space raises three important problems.

First, if the page table is allocated in the virtual address space, accessing the page
table requires an address translation, which in turn requires another translation, and so on,
resulting in an endless cycle of address translations. This problem is dealt with by specifying
a portion of the memory address space to be either untranslated (also called unmapped) or
direct mapped.
In the former case, if a virtual address is within an untranslated region,
then the physical address is the same as the virtual address. In the latter case, if a virtual
address is within a direct-mapped region, then the physical address is obtained by applying
some trivial hashing function to it. By placing the page table in an untranslated/direct-
mapped region, the page table’s physical location can be determined without performing a
page table-based address translation.

The second problem is the size of the page table. For a 32-bit byte-addressable user
address space partitioned into 4 KB pages, the page table has a million (2 32 B (cid:4) 4 KB =
220 ) entries, which requires at least 4 MB of storage. Considering the amount of physical
memory required to store the page tables of several active processes, storing the entire page
tables in the untranslated/direct-mapped address space seems to be a di(cid:14)cult proposition.
Ironically, large portions of the page tables may not be required by an application, and
only a small subset of the virtual pages may be currently mapped to physical pages. One
solution to deal with the size problem mentioned above is to store the user page tables in
the translated portion of the kernel address space, and to store only the kernel page table
(which is then required to access the user page tables) in the untranslated/direct-mapped
address space. Figure 8.6 shows such an organization.

Figure 8.6: Root Page Table

If the root page table is found to be too large, it can be organized in an hierarchical
manner, with only the topmost level of the hierarchy stored in the untranslated/direct-

306

Chapter 8. Microarchitecture | Kernel Mode

mapped address space. Figure 8.7 shows the organization of an hierarchical page table.
The page tables in all of the levels except the bottom-most one serve as directories for the
level immediately below it. The space occupied by each small page table (at each level) is
typically limited to one page. Many recent computer systems implement hierarchical page
tables. For instance, computer systems based on the DEC Alpha 21*64 processor, which
supports a 64-bit address space, typically use a 4-tiered hierarchical page table.

Figure 8.7: An Hierarchical Page Table

8.2.4 Translation Lookaside Bu(cid:11)er (TLB)

The third problem is that every memory reference speci(cid:12)ed in the program requires two
or more memory accesses, all of which except the last one are page table accesses done for
translating the address; the last access is to the translated memory location. The solution
commonly adopted to reduce the number of accesses for translation purposes is to cache the
active portion of the page table in a small, special hardware structure called Translation
Lookaside Bu(cid:11)er (TLB), which is kept within the MMU, as shown in Figure 8.8. Because
each TLB entry covers one page of memory address space, it is possible to get a very high
hit rate from a reasonably small TLB.

8.2. Memory Management: Implementing Virtual Memory

307

Kernel Address Space

e ’ ’

h

c

‘ ‘ C a

Page
Tables

TLB

MMU

Figure 8.8: Relationship between TLB and Page Tables

With the introduction of a TLB, the address translation procedure involves a small
change, as depicted in Figure 8.9. The main change is that the TLB has replaced the page
table as the primary structure used for obtaining address translation. The page table is
still used, but only if the TLB fails to provide a translation.

The TLB can be managed either by the hardware or by the software. We shall (cid:12)rst
discuss the operation when using a hardware-managed TLB. Figure 8.10 presents a (cid:13)owchart
of the actions in such a system. This (cid:13)owchart is a modi(cid:12)cation of the one shown earlier
without a TLB. The main di(cid:11)erence is that after determining the VPN, the MMU looks
in the TLB for the referenced VPN (instead of directly accessing the page table). If the
page table entry for this VPN is found in the TLB, the PFN and protection information are TLB hit
obtained immediately. If the protection bits indicate that the access is not permitted, then
scenario
a memory access violation exception is generated as before. If the access is a permitted one,
the physical address is determined by concatenating PFN and O(cid:11)set within Page, and the
main memory access is performed.

If there is no entry in the TLB for the speci(cid:12)ed VPN, a TLB miss is said to have TLB
occurred. The MMU handles the TLB miss by reading the required PTE from the page
miss sce-
table stored in the kernel address space. If the PTE is obtained, then the MMU updates
nario
the TLB with the new entry. If the page table does not contain a valid PTE for the virtual
page, then a page fault exception is generated as before. The MMS part of the OS handles Page
the page fault exception and then transfers control back to the program that caused the
fault sce-
nario
exception.

308

Chapter 8. Microarchitecture | Kernel Mode

CPU−generated Virtual Address

Virtual Page Number

Offset within Page

Physical Memory

0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

01

1

0

0 0 0

0

0

0

0 0 0

11

Virtual Page Number

VP

Page Frame Number

0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

01

1

1

00

0 0

0 0

0 0 0 1

0
...

VP 5

TLB Miss

Page
Table
Lookup

TLB

Physical Address

0

0

0 0

0

0 0

0 0 0 1

0

0 0 0

0 0

0

0 0 0

11

Page Frame Number

Offset within Page

PF 1, Offset 3

..
.

...

...

...

location 0
location 1
location 2
location 3

location 4095
location 0
location 1
location 2
location 3

location 4095
location 0
location 1
location 2
location 3

location 4095

PF 0

PF 1

PF 2

location 0
location 1
location 2
location 3

location 4095

PF 2047

Figure 8.9: Virtual Address to Physical Address Translation using a TLB

Among the actions given in this (cid:13)owchart, the actions in the leftmost column are per-
formed very frequently, and are critical for performance. These are always implemented in
hardware (MMU). The actions in the rightmost column are done very rarely, and so are
done by software (OS). Thus, actions that are generally implemented in hardware include
VPN determination, TLB hit determination, TLB LRU information update, memory access
restriction enforcement, physical address determination (concatenation of PFN and Offset
within Page, and physical memory access. In other words, the MMU hardware performs
the address translation, as long as the TLB lookups result in hits. Actions that are generally
implemented by the OS include page table creation and page fault handling functions such
as swap area (hard disk) access and page table update.

The actions in the middle column of the (cid:13)owchart deal with TLB miss handling and
TLB replacement, and are done on an occasional basis. With a hardware-managed TLB,
these actions are also done in hardware by the MMU. Some systems, however, implement
these steps in software, resulting in a software-managed TLB. We next take a more detailed

8.2. Memory Management: Implementing Virtual Memory

309

Determine VPN & Offset

Lookup in PT with VPN

Is VP present
in SS?

Yes

Lookup in TLB with VPN

TLB Hit?

TLB Miss

TLB Hit

Is VP present
in PM?

Yes

Page Fault
Exception

Copy Page from SS

Allocate new VP

Update PT

Update TLB with PTE

Memory Access Violation Exception

Update TLB LRU Info

Is this type of 
access permitted?

Yes

Determine PA

Perform Memory Access

Very Frequent

Occasional

Hardware
(MMU)

Print Error Message

Terminate Process

Very Infrequent

Software
(MMS)

Figure 8.10: A Flowchart of the Address Translation Steps with a Hardware-Managed TLB

look at software-managed TLBs.

8.2.5 Software-Managed TLB and the Role of the Operating System in
Virtual Memory

One aspect that must be clear by now is that unlike the implementation of cache memories,
the implementation of the virtual memory concept in modern computers is rarely done
entirely in hardware. Instead, a combination of hardware and software schemes is used,
with the software part being implemented as a collection of OS routines called the memory
management system (MMS). The page tables are usually placed in the kernel address space.

In a virtual memory system implementing a hardware-managed TLB, TLB misses are

310

Chapter 8. Microarchitecture | Kernel Mode

handled in hardware by the MMU. Examples of processors that include a hardware-managed
TLB are the IA-32 family and the PowerPC. The organization of such a system is illustrated
in Figure 8.11(i). In such a system, the TLB is not de(cid:12)ned in the (kernel mode) ISA, and
so the OS does not know about this hardware unit. The TLB manager hardware, which
handles TLB misses, is also unknown to the OS. The page table (PT) is de(cid:12)ned in the
kernel mode ISA so that when handling TLB misses, the TLB manager can perform a PT
lookup | hardware page table walking.

Operating System

Kernel Mode ISA

Hardware

PT Manager

Page Fault Exception

PT

PT

TLB Manager
TLB Miss

TLB

Operating System

PT Manager

Page Fault

PT

TLB Manager

TLB Miss Exception

TLB

Kernel Mode ISA

TLB

Hardware

(i) Hardware−Managed TLB

(ii) Software−Managed TLB

Figure 8.11: Conceptual Organization of a Computer System with (i) a Hardware-Managed
TLB; (ii) a Software-Managed TLB

8.2.5.1 Software-Managed TLB

In a system implementing a software-managed TLB, TLB misses are handled in software
by the MMS part of the OS. Examples of processors that require a software-managed TLB
are MIPS, Sparc, Alpha, and PA-RISC. The organization of such a system is illustrated
in Figure 8.11(ii). In such systems, the TLB speci(cid:12)cation is included in the kernel mode
ISA. A TLB miss in such a system would cause a TLBMiss Exception, which automatically
transfers control to the operating system. The operating system then locates and accesses
the appropriate PTE, updates the TLB with information from this PTE, and transfers

8.2. Memory Management: Implementing Virtual Memory

311

control back to the program that caused the TLB miss.

Figure 8.12 shows a (cid:13)owchart that re(cid:13)ects the working of a virtual memory system using
a software-managed TLB. On comparing this with the (cid:13)owchart of Figure 8.10, we can see
that there are no di(cid:11)erences in the steps themselves, but only in who performs some of the
steps. The steps in the middle column are performed here in software by the MMS part of
the OS, rather than by the MMU hardware.

Determine VPN & Offset

Lookup in PT with VPN

Is VP present
in SS?

Yes

Lookup in TLB with VPN

Is VP present
in PM?

Yes

Page Fault

Copy Page from SS

Allocate new VP

TLB Miss Exception

Update PT

Update TLB with PTE

Memory Access Violation Exception

TLB Hit?

TLB Hit

Update TLB LRU Info

Is this type of 
access permitted?

Yes

Determine PA

Perform Memory Access

Very Frequent

Occasional

Hardware
(MMU)

Software
(MMS)

Print Error Message

Terminate Process

Very Infrequent

Figure 8.12: A Flowchart of the Address Translation Steps with a Software-Managed TLB

The advantages of using a software-managed TLB are two-fold:

(cid:15) The OS can implement virtually any TLB replacement policy.

(cid:15) The algorithm, structure, and format of the page table are not (cid:12)xed, and the OS can
implement any page table organization.

312

Chapter 8. Microarchitecture | Kernel Mode

The disadvantage, of course, is that it takes more time to service a TLB miss. If the OS can
implement TLB replacement policies that reduce the number of TLB misses signi(cid:12)cantly,
then this strategy has an overall advantage.

8.2.6 Sharing in a Paging System

8.2.7 A Real-Life Example: a MIPS-I Virtual Memory System

Computer systems have implemented a wide variety of virtual memory organizations. They
di(cid:11)er primarily in terms of how the page table is organized, and who manages the TLB|
hardware or software. Instead of describing all possible organizations, we have attempted
to cover the fundamental principles behind them. For completeness, we will also look
at a concrete example|the memory management system in a MIPS-I based system|to
see how all of the di(cid:11)erent aspects work together 3 . The MIPS-I ISA supports one of the
simplest memory management organizations among recent microprocessors. It was one of
the earliest commercial ISAs to support a software-managed TLB. This feature implies
that the page table is not de(cid:12)ned in the kernel mode ISA, and that the OS is free to choose
a PT organization. Some support is provided, nevertheless, for implementing a simple
linear PT organization in the kernel’s virtual address space. Similarly, although the OS
is free to implement any replacement policy for the TLB, the ISA provides some support
for a random replacement policy. In the ensuing discussion, we (cid:12)rst describe the MIPS-I
kernel address space, and then the support provided in the kernel ISA to implement virtual
memory. Finally, we show how these ISA features can be used in a possible virtual memory
system organization for MIPS-I based systems.

8.2.7.1 Kernel Mode ISA Support for Virtual Memory

We shall start with the kernel address space. The MIPS-I ISA speci(cid:12)es a 32-bit kernel
address space. Part of the address space is unmapped so that accesses to this part can
proceed without doing a table lookup. The 4 GB address space is divided into 4 segments,
as illustrated in Figure 8.13. These segments have di(cid:11)erent mapping characteristics and
serve di(cid:11)erent functions:

(cid:15) kuseg: This 2 GB segment is cacheable and mapped to physical addresses. In the
kernel mode, accesses to this segment are treated just like user mode accesses. This
serves as a means for the kernel routines to access the code and data of the user
process on whose behalf they are running. Thus, translations for this segment are
available in the corresponding user page table (stored in kseg2).

3Our discussion of the MIPS-I virtual memory system is detailed enough to get a good appreciation of
the fundamental issues involved. However, it does not cover every nuance that a MIPS OS developer needs
to be aware of to develop the MMS part of the OS. Additional details can be obtained from sources such as
[?].

8.2. Memory Management: Implementing Virtual Memory

313

Kernel Address Space

Addresses
0x0000 0000

kuseg
2 GB

Mapped
Cacheable

kseg0
512 MB

kseg1
512 MB

Unmapped
Cached

Unmapped
Uncached

kseg2
1 GB

Mapped
Cacheable

0x8000 0000

0xa000 0000

0xc000 0000

0xffff ffff

Figure 8.13: Functionality-based Partitioning of the MIPS Kernel Address Space into Seg-
ments

(cid:15) kseg0: This 512 MB segment is cached and unmapped. Addresses within this segment
are direct-mapped onto the (cid:12)rst 512 MB of physical address space without using the
TLB. This segment is typically used for storing frequently executed parts of the OS
code|such as the exception handlers|and some kernel data such as the kernel page
table (which stores the mapping for the kseg2 segment). The TLB miss handler is
stored in this segment.

(cid:15) kseg1: This 512 MB segment is uncached and unmapped. It is also direct-mapped
to the (cid:12)rst 512 MB of physical address space without using the TLB. Unlike kseg0,
however, this segment is uncacheable. It is used for the boot-up code, the IO registers,
and disk bu(cid:11)ers, all of which must be unmapped and uncached.

(cid:15) kseg2: This 1 GB segment is cacheable and mapped to arbitrary physical addresses,
like kuseg. Translations for this segment are stored in the kernel’s page table (root
page table. This segment is typically used to store the kernel stack, \U-area", user
page tables, and some dynamically allocated data areas.

The MIPS-I kernel mode ISA supports a paged virtual memory with 4 KB sized pages.
A 32-bit address can thus be split into a 20-bit VPN part and a 12-bit page o(cid:11)set part. The

314

Chapter 8. Microarchitecture | Kernel Mode

ISA also speci(cid:12)es a uni(cid:12)ed 64-entry, fully-associative TLB. The OS loads page table entries
(PTEs) into the TLB, using either random replacement or speci(cid:12)ed placement. Among the
64 TLB entries, eight are wired entries; these do not get replaced when doing a random
replacement. This wiring feature allows the OS to store a few important PTEs, such as the
frequently required root PTEs. Figure 8.14 shows the format of a TLB entry. It has the
following (cid:12)elds/bits:

VPN

20

ASID

6

PFN

20

N D V G

1 1 1 1

VPN
ASID
PFN

: Virtual Page Number
: Address Space ID
: Physical Frame Number

N
D
V
G

: Noncacheable
: Dirty
: Valid
: Global

Figure 8.14: A MIPS-I TLB Entry

(cid:15) VPN (Virtual Page Number):

(cid:15) ASID (Address Space ID):

(cid:15) PFN (Physical Frame Number):

(cid:15) N (Noncacheable): This bit indicates if the mapped page is noncacheable. If it is set,
the processor sends the address to main memory, bypassing the cache.

(cid:15) D (Dirty): This bit indicates if the mapped page has been updated by the process.

(cid:15) V (Valid): This bit indicates if the entry contains a valid mapping.

(cid:15) G (Global): This bit indicates if the mapped page is shared. If it is set, the MMU does
not perform an ASID match while doing a TLB lookup.

TLB-Related Exceptions: TLB misses in a MIPS-I system are conveyed to the OS as
an exception. We can group the TLB-related exceptions into 3 categories:

(cid:15) UTLBMISS (User mode TLB Miss): This exception category includes cases where no
matching entry has been found in the TLB for an address in the kuseg portion of the
address space.
Such exceptions can occur for instruction fetch, data read, or data
write. This is the exception that is likely to occur the most frequently.

(cid:15) TLBMISS (Kernel mode TLB Miss): This category cases where either a TLB miss
has occurred to the kseg2 portion of the address space, or a matching TLB entry was
found with the Valid bit not set.

8.2. Memory Management: Implementing Virtual Memory

315

(cid:15) TLBMOD: This exception indicates that a TLB hit has occurred for a store instruction
to an entry whose Dirty bit is not set. The purpose is to let the OS set the Dirty bit
of the TLB entry.

Among these 3 categories, the (cid:12)rst one has its own exception vector (0x80000000) because
it is the one that occurs most frequently among the TLB-related exceptions. The last 2
categories are grouped into a single exception vector (0x80000080).

Privileged Registers: When a TLB miss exception occurs, information about the ex-
ception event (such as the faulting virtual address) needs to be conveyed to the OS. It is cus-
tomary to use a privileged register to store this information. Similarly, the OS might require
additional special registers for manipulating the TLB. The MIPS kernel mode ISA provides
6 privileged registers|EntryHi, EntryLo, Index, Random, BadVAddr, and Context|for
conveying additional information regarding TLB miss exceptions to the OS and for the OS
to modify the TLB. These additional registers are depicted in Figure 8.15, and are described
below:

(cid:15) EntryHi: This register is used to store a VPN and an ASID (Address Space IDenti(cid:12)er).

(cid:15) EntryLo: This register is used to store a PFN and status information. The information
in a TLB entry is equivalent to the concatenation of EntryHi and EntryLo values.

(cid:15) Index: This register is used by the OS to store the index value to be used for accessing
a speci(cid:12)c TLB entry. The 6-bit Index (cid:12)eld of this register can store values between
0-63.

(cid:15) Random: This register holds a pseudo-random value, which is used as the TLB index
when the OS performs random replacement of TLB entries. The MMU hardware
automatically increments it every clock cycle so that it serves as a pseudo-random
generator.

(cid:15) BadVAddr: The MMU hardware uses this register to save the bad virtual address that
caused the latest addressing exception.

(cid:15) Context: This register is used to store some information that facilitates the handling
of certain kinds of exceptions such as TLB misses. The PTEBase (cid:12)eld is set by the
OS, normally to the high-order address bits of the current user ASID’s page table,
located in the kseg2 segment of the kernel address space. The BadVPN (cid:12)eld is set by
the MMU hardware when the addressing exception occurs.

Privileged Instructions: The kernel mode ISA also provides the following privileged
instructions for the OS to re(cid:12)ll the TLB:

316

Chapter 8. Microarchitecture | Kernel Mode

P

1

VPN

20

PFN

20

0

17

0

18

ASID

6

0

6

N D V G

1 1 1 1

Index

6

Random

6

0

8

0

8

0

8

EntryHi

EntryLo

Index

Random

BadVAddr

BadVAddr

32

PTEBase

11

BadVPN

19

Context

0

2

Figure 8.15: Privileged Registers Present in the MIPS-I Kernel Mode ISA for Supporting
Virtual Memory

(cid:15) tlbwr: Write the PTE present in EntryHi-EntryLo pair onto the TLB at index
provided in register Random. This instruction is used by the OS to insert a PTE
randomly into the TLB.

(cid:15) tlbwi: Write the PTE present in EntryHi-EntryLo pair onto the TLB entry indexed
by register Index. This instruction is used by the OS to insert a PTE at a speci(cid:12)ed
index in the TLB.

(cid:15) tlbr: Read TLB entry indexed by register Index, and place the information into
EntryHi and EntryLo registers.

TLB Operation: Let us put in a nutshell the operation of the TLB hardware. If the
system is in the Kernel mode, all accesses to the kseg0 or kseg1 segments bypass the TLB,
and are direct-mapped as described earlier. If the access is to kuseg (in either mode) or
to kseg2, then the MMU determines the VPN and associatively compares it against the
VPN (cid:12)eld of all TLB entries. If there is a match, and if either the ASID (cid:12)eld of EntryHi
register matches that of the matching TLB entry or if the TLB entry’s Global bit is set,
then translation can continue. If no such entry is found in the TLB, a UTLBMISS exception

8.2. Memory Management: Implementing Virtual Memory

317

is generated if the address is to kuseg, and a TLBMISS exception is generated if the address
is to kseg2.
If a matching TLB entry was found, but its Valid bit was not set, then a
TLBMISS exception is generated, irrespective of whether the address is to kuseg or kseg2.
In all of these cases, the faulting VPN is placed in the Context register.

8.2.7.2 Operating System Usage of Machine Features

With the above background on the features provided in the MIPS-I kernel ISA to support
virtual memory, let us turn our attention to the barebones of a typical MMS used in MIPS-I
based systems. This MMS organizes each user page table as a simple linear array of PTE
entries. Each PTE is 4 bytes wide, and its format matches the bit(cid:12)elds of the EntryLo
register. Each user ASID’s PT requires 2 MB of space. All of the user ASID page tables
are placed in the kseg2 part of the kernel address space. The kernel PT stores the mapping
for the kseg2 segment only. The entire kernel PT (part of which contains the root PTEs) is
stored in kseg0. The placement of the page tables is pictorially shown in Figure 8.16. Each
kernel PTE stores the current mapping for 4 KB of kseg2. If the kernel PTE happens to
be a root PTE, then this 4 KB stores 1 K entries of a user page table, and that root PTE
e(cid:11)ectively covers 1 K user PTEs, or 1 K (cid:2) 4 KB = 4 MB of the user address space.

The OS makes sure that whenever a user process is being run, the ASID (cid:12)eld of the
EntryHi register re(cid:13)ects the user ASID and the PTEBase (cid:12)eld of the Context register re(cid:13)ects
the base virtual address of the user page table.

A UNIX-like OS typically uses the 8 wired entries of the TLB for storing kernel PTEs as
follows: one for a kernel PTE that maps 4 KB of the kernel stack, one for a kernel PTE that
maps 4 KB of the U-area, one each for a root PTE that covers 4 MB each of the currently
active user ftext, data, stackg, and up to 3 more for root PTEs that cover 4 MB each of
user data.

We shall focus our discussion on UTLBMISS, the most frequent TLB-related exception.
When a UTLBMISS exception occurs, the Context register gives the kseg2 virtual address of
the user PTE to be read to service the TLB miss. When the UTLBMISS handler attempts
to read the PTE present at this kseg2 virtual address, an address translation is required
because kseg2 is a mapped segment.
If a TLB entry is available for this kseg2 virtual
address (most likely, this should be one of the wired entries in the TLB), then the OS can
successfully access the required user PTE with just one memory access. In the worst case,
this TLB lookup for a kseg2 virtual address can cause a TLBMISS exception. The TLBMISS
handler will read the root PTE stored in the unmapped kseg0 segment, enter this root
PTE in the TLB, and return control back to the UTLBMISS handler.

After obtaining the required user PTE from kseg2, the UTLBMISS handler updates the
TLB with this PTE. Assembly language code for an UTLBMISS handler is given below.
Notice that the rfe instruction (at the end), which puts the system back in the user mode
is in the delay slot of the jr $k1 instruction, which speci(cid:12)es the transfer of control back to
the user program that caused the TLB miss.

318

Chapter 8. Microarchitecture | Kernel Mode

Addresses
0x8000 0000

0x8000 0080

kseg0

UTLBMISS Handler

Other
Exception
Handlers

0x9ff0 0000

0x9ff2 0000

0x9fff ffff

Root PTEs
Kernel PTEs
1 MB

512 MB
Unmapped, Cached

kseg2

PT for ASID 0
2 MB

PT for ASID 1
2 MB

Addresses
0xc000 0000

0xc020 0000

0xc040 0000

PT for ASID 63
2 MB

0xc7e0 0000

0xc800 0000

The PTE in kseg0 stores the mapping

for the virtual page in kseg2

Kernel Stack

1 GB
Mapped, Cacheable

0xffff ffff

Figure 8.16: Placement of Kernel Page Table and User Page Tables in the kseg0 and kseg2
Segments of the MIPS Kernel Address Space

0x80000000: mfc0

$k0, $context

mfc0

$k1, $epc

lw

$k0, 0($k0)

mtc0
tlbwr

jr
rfe

$k0, $EntryLo

$k1

# Copy context register contents (i.e., kseg2 virtual
# address of required user PTE) into GPR k0
# Copy epc contents (address of faulting instruction)
# into GPR k1
# Load user PTE from kseg2 addr to GPR k0
# This load can cause a TLBMISS exception!
# Copy the loaded PTE into EntryLo register
# Write the PTE in EntryLo register into TLB
# at slot number speci(cid:12)ed in Random register
# Jump to address of faulting instruction
# Switch to user mode

8.2. Memory Management: Implementing Virtual Memory

319

An Assembly Language UTLBMISS Handler for a MIPS-I System

User Mode

ML Program

Microroutine

Kernel Mode

ML Program
(UTLB Miss Handler)

lw

$2, 4($3)

Interpretation

Fetch instruction
Read rs and rt registers
Calculate address
Lookup in TLB
Access physical memory
End

M I S

B

L

T

U

0x8000 0000: mfc0
mfc0
lw
mtc0
tlbwr
rfe
jr

k0, context
k1, epc
k0, 0(k0)
k0, EntryLo

k1

Figure 8.17: A Program Execution View of a UTLBMISS Exception

8.2.8

Interpreting a MIPS-I Memory-Referencing Instruction

In the previous chapter, we had looked at the interpretation of a memory-referencing in-
struction, without considering the address translation process. The calculated (virtual)
address was directly copied to MAR, from where it was supplied to the memory subsystem.
In this section, we shall brie(cid:13)y revisit the interpretation of memory-referencing instructions,
by considering the address translation process also. Again, we tailor the discussion to the
MIPS-I ISA. For ease of understanding, we modify the familiar data path of Figure 8.1
to include a memory management unit. The modi(cid:12)ed data path is given in Figure 8.18.
It includes a microarchitectural register called VAR for holding the virtual address to be
translated. It also includes a software-managed TLB. The microarchitectural register MAR
now consists of 2 (cid:12)elds. The Offset (cid:12)eld is directly updated from the Offset (cid:12)eld of VAR.
The PFN (cid:12)eld can be updated either from the TLB or directly from the VPN (cid:12)eld of VAR.

Table 8.4 gives a modi(cid:12)ed MAL routine for interpreting a MIPS lw instruction in this
data path. In step 6, the calculated virtual address is copied from AOR to VAR (instead of
MAR). In step 7, the VPN (cid:12)eld of this address is supplied to the TLB. If it results in a TLB
miss, then a TLB miss exception is activated by transfering control to MAL step 10. On
the other hand, if there is a TLB hit|which will be the frequent case|the PFN (cid:12)eld of the
concerned TLB entry is copied to MAR’s PFN (cid:12)eld. The Offset (cid:12)eld of VAR is also copied
to the corresponding (cid:12)eld of MAR.

The TLB miss exception is initiated in steps 10-15. This part of the routine is similar to
the initiation of the over(cid:13)ow exception that we saw earlier. The main points to note are: (i)

320

Chapter 8. Microarchitecture | Kernel Mode

To Control Unit

Memory Interface

Cause

VAR

VPN

Offset

VPN

PFN

SR

EPC

Shift

PRE_PC

PC

IR

Instr.
Decoder

32

Register Address

5

Register File
(RF)

TLB

PFN
Offset
MAR

s
u
B
 
r
o
s
s
e
c
o
r
P

Register Data

MDR

ALU

Flags

Processor Data Path

Kernel Mode
Registers

System Address Bus

Memory
Subsystem

System Data Bus

Figure 8.18: A Data Path for Implementing the MIPS-0 Kernel Mode ISA Including Virtual
Memory

the BadVPN (cid:12)eld of Context register is updated with the virtual page number that missed
in the TLB; (ii) PC is updated with 0x80000000 or 0x80000080 depending on whether the
TLB miss occurred in the user mode or the kernel mode, respectively.

8.2.9 Combining Cache Memory and Virtual Memory

The discussion of virtual memory clearly parallels the cache memory concepts discussed in
the previous chapter. There are many similarities and some di(cid:11)erences. The cache memory
bridges the speed gap between the processor and the main memory and is implemented
entirely in hardware. The virtual memory mechanism bridges the size gap between the
main memory and the virtual memory and is usually implemented by a combination of

8.3.

IO System Organization

321

Step
No.

4
5
6
7

8
9

10

11
12
13
14

15

MAL Instruction
for Data Path
Execute phase
R[rs] ! AIR
SE(offset) + AIR ! AOR
AOR ! VAR
if (VAR.VPN misses in TLB)
goto step 10
else TLB[hit index].PFN ! MAR.PFN;
VAR.Offset ! MAR.Offset
M[MAR] ! MDR
if (rt) MDR ! R[rt]
Take TLB Miss Exception
SR<3:0> << 2 ! SR<5:0>

Comments

Goto step 0

Set system in kernel mode
and disable interrupts
Save instruction’s address
Prev PC ! EPC
VAR ! BadVAddr
Save bad virtual address
VAR.VPN ! Context.BadVPN Save bad VPN
Set PC to 0x80000000
0x80000000 | (SR<3> << 6) ! PC
or 0x80000080
Place TLBL code in cause
Goto step 0

2 ! Cause.ExcCode

Table 8.4: A MAL Routine for the Execute Phase of the Interpretation of the MIPS-0 ISA
Instruction Represented Symbolically as lw rt, offset(rs). This MAL Routine is for
executing the ML instruction in the Data Path of Figure 8.18

hardware and software techniques. Table 8.5 provides a succinct comparison of the two
concepts and their typical implementations.

The descriptions of cache memory and virtual memory were given in isolation.

8.3

IO System Organization

Another ma jor component of the kernel mode microarchitecture is the IO system, which
houses the IO devices, their interfaces, and their interconnections.
In terms of size, the
IO system is usually the largest part of any computer. This is because the IO devices are
electromechanical devices, which cannot be easily miniaturized. The IO interfaces also tend
to be bulky, as they are built out of several integrated circuits (IC chips). Moreover, most
of the IO interconenctions are made out of cables that are external to the IC chips.

The IO system performs the following functions:

322

Chapter 8. Microarchitecture | Kernel Mode

Cache Memory
No.
1. Cache hit time is small (1-2 cycles)

Virtual Memory
Physical memory hit time is large
(10-100 cycles)
2. Cache miss time is large (10-100 cycles) Page fault handling time is very large
3. Block size is small (4-32 words)
Page size is large (1K-1M words)
Usually fully-associative mapping or
4. Usually direct mapping or
high level of associativity
low level of associativity
On a page fault, the OS loads
5. On a cache miss, the cache hardware
loads the block from main memory
the page from disk to main memory
to cache memory
6. The cache memory is usually not
visible in the ISA

The virtual memory is
visible in the kernel mode ISA

Table 8.5: A Succinct Comparison of Cache Memory and Virtual Memory

(cid:15) Implement the IO address space de(cid:12)ned in the kernel mode ISA

(cid:15) Implement the IO interface protocols of the IO devices

(cid:15) Implement the IO devices themselves

The (cid:12)rst two functions are de(cid:12)nitely related, and we shall explain them in more detail.
Implementation of various IO devices is discussed in Appendix **.

8.3.1

Implementing the IO Address Space: IO Data Path

We shall begin our discussion of the IO system with the topic of implementing the IO address
space de(cid:12)ned in the kernel mode ISA, as we saw in Section 4.3. This address space can be
part of the memory address space (as in memory-mapped IO) or a separate address space
(as in independent IO). Irrespective of whether the IO register space is memory-mapped
or independently mapped, the hardware registers that implement this name space are built
separately from the memory system. When the processor executes an IO instruction|an
instruction that accesses the IO address space|it issues a signal indicating that the address
on the system bus is an IO address.

Unlike monolithic structures such as the register (cid:12)le used to implement the general-
purpose registers, the structures used to implement the IO address space are distributed.
That is, the set of hardware registers implementing the IO address space is distributed over
di(cid:11)erent hardware structures, depending on their function. For instance, the IO registers
that relate to the keyboard device will be physically located within the keyboard interface
module, and those that relate to the mouse will be physically located within the mouse

8.3.

IO System Organization

323

interface module. This type of distribution is required because the behavior of IO registers
vary, depending on the IO device they relate to, as discussed in Section 4.3 The number of
IO addresses allotted for an IO device is generally small, of the order of a few tens or less;
an exception is video memory.

Design of the IO data path is a very complex sub ject, perhaps even more complex than
that of the processor data path and the memory data path. Therefore, we shall present
this topic in a step by step manner, beginning with very simple data paths and steadily
increasing the complexity. The simplest IO data path we present consists of a single system
bus that connects several IO registers as shown in Figure ****.

GIVE MAL ROUTINE FOR an lw instruction that is an IO instruction Let this MAL
routine be related to the one given in virtual memory

GIVE MAL ROUTINE for a MIPS-like in instruction

When comparing the register (cid:12)le and the IO hardware registers, besides physical dis-
tribution, there is another ma jor di(cid:11)erence: a register in a register (cid:12)le has a (cid:12)xed address
whereas an IO hardware register may not have a (cid:12)xed address. Like a physical memory
location in a virtual memory system, an IO hardware register also can map to di(cid:11)erent
logical IO register addresses at di(cid:11)erent times.

In this section, we give an overview of selected aspects of computer IO and communica-
tion between the processor and IO devices. Because of the wide variety of IO devices and
the quest for faster handling of programs and data, IO is one of the most complex areas of
computer design. As a result, we present only selected pieces of the IO puzzle. We start
with a discussion of IO interface modules, which are pivotal in implementing the IO regis-
ters speci(cid:12)ed in the kernel mode ISA. These interface modules come in various kinds, and
provide di(cid:11)erent types of ports for connecting various IO devices. We consider the interface
module for connecting a keyboard as an illustration. We then introduce the system bus and
other types of buses to connect the IO interface modules to the processor-memory system.

8.3.1.1 System Bus

The IO hardware registers must be connected to the rest of the data path | the processor
and memory system | for them to be accessible by IO instructions. The most convenient
way to carry out this connection is to extend the processor-memory bus that we used earlier
to connect the processor and memory system. When a single bus is used in this manner
to connect all of the devices|the processor, the memory, and the IO registers|the bus is
usually called a system bus. Such a connection was illustrated in Figure 8.19.

8.3.2

Implementing the IO Interface Protocols: IO Controllers

Unlike the general-purpose registers and the memory locations, the IO registers are generally
implemented in a distributed manner, using special register elements inside the di(cid:11)erent IO

324

Chapter 8. Microarchitecture | Kernel Mode

interface modules. To perform the functionality speci(cid:12)ed in an IO port speci(cid:12)cation, an
IO interface module is used. The interface module that implements the port interacts with
the appropriate IO devices. From an IO device’s viewpoint, its interface module serves as
a conduit for connecting it to the rest of the computer system. An IO interface module
performs one or more of the following functions:

(cid:15) It interacts with IO devices to carry out the functionality speci(cid:12)ed for the IO port
that it implements. For example, a keyboard port may specify that whenever its
status bit is set, its data register will have a code value that corresponds to the last
key depressed or released in a keyboard.

(cid:15) It performs conversion of signal values and data formats between electromechanical
IO devices and the processor-memory system.

(cid:15) It performs necessary synchronization operations for correct transfer of data from slow
IO devices.

(cid:15) It performs necessary bu(cid:11)ering of data and commands for interfacing with slow IO
devices.

(cid:15) It performs direct transfer of data between Io devices and memory.

One side of an IO interface module connects to a system bus, and the other side connects
to one or more IO devices via tailored data links, as shown in Figure 8.19. The IO interface
module can be thought of as a small processor that has its own register set and logic
circuitry.

It is worthwhile to point out that most of the IO devices are electromechanical devices,
with some electronic circuitry associated with it. This circuitry carries out functions that
are very speci(cid:12)c to that device. For example, in a printer, this circuitry controls the motion
of the paper, the print timing, and the selection of the characters to be printed. The printer
interface module does not carry out these functions. Thus, an interface module directly
interacts only with the electronic circuitry, and not with the mechanical parts of the device.

When an IO write instruction writes a value to an interface module’s status/control
register, it is interpreted by the module as a command to a particular IO device attached
to it; and the module sends the appropriate command to the IO device.

8.3.3 Example IO Controllers

The IO controllers consist of the circuitry required to transfer data between the processor-
memory system and an IO device. Therefore, on the processor side of the module we have
the IO registers that are part of the kernel mode ISA, along with circuitry to interact
with the bus to which it is connected on the processor side. On the device side we have a
data path with its associated controls, which enables transfer of data between the module

8.3.

IO System Organization

System Bus

325

Address
Data
Control

CPU

Main
Memory

Status/Control Register

Status/Control Register

Data Register

Data Register

I/O Interface

I/O Interface (Disk Controller)

Links to Peripheral Devices

Figure 8.19: Use of IO Interface Modules to Implement IO Ports to Connect IO Devices

and an IO device. This side may be either designed or programmed at run-time to be
device-dependent.

8.3.4 Frame Bu(cid:11)er:

aka Display memory and Video memory.

A framebu(cid:11)er is a video output device that drives a video display from a memory bu(cid:11)er
containing a complete frame of data. The information in the bu(cid:11)er typically consists of
color values for every pixel (point that can be displayed) on the screen. Color values are
commonly stored in 1-bit monochrome, 4-bit palettized, 8-bit palettized, 16-bit highcolor
and 24-bit truecolor formats. An additional alpha channel is sometimes used to retain
information about pixel transparency. The total amount of the memory required to drive
the framebu(cid:11)er depends on the resolution of the output signal, and on the color depth and
palette size.

Before an image can be sent to a display monitor, it is (cid:12)rst represented as a bit map in
an area of video memory called the frame bu(cid:11)er. The amount of video memory, therefore,
dictates the maximum resolution and color depth available.

With a conventional video adapter, the bit map to be displayed is (cid:12)rst generated by
the computer’s microprocessor and then sent to the frame bu(cid:11)er. Most modern video
adapters, however, are actually graphics accelerators. This means that they have their own
microprocessor that is capable of manipulating bit maps and graphics ob jects. A small

326

Chapter 8. Microarchitecture | Kernel Mode

amount of memory is reserved for these operations as well.

Because of the demands of video systems, video memory needs to be faster than main
memory. For this reason, most video memory is dual-ported, which means that one set of
data can be transferred between video memory and the video processor at the same time
that another set of data is being transferred to the monitor. There are many di(cid:11)erent types
of video memory, including VRAM, WRAM, RDRAM, and SGRAM. The standard VGA
hardware contains up to 256K of onboard display memory. While it would seem logical that
this memory would be directly available to the processor, this is not the case. The host CPU
accesses the display memory through a window of up to 128K located in the high memory
area.
(Note that many SVGA chipsets provide an alternate method of accessing video
memory directly, called a Linear Frame Bu(cid:11)er.) Thus in order to be able to access display
memory you must deal with registers that control the mapping into host address space. To
further complicate things, the VGA hardware provides support for memory models similar
to that used by the monochrome, CGA, EGA, and MCGA adapters. In addition, due to
the way the VGA handles 16 color modes, additional hardware is included that can speed
access immensely. Also, hardware is present that allows the programer to rapidly copy data
from one area of display memory to another. While it is quite complicated to understand,
learning to utilize the VGA’s hardware at a low level can vastly improve performance.
Many game programmers utilize the BIOS mode 13h, simply because it o(cid:11)ers the simplest
memory model and doesn’t require having to deal with the VGA’s registers to draw pixels.
However, this same decision limits them from being able to use the infamous X modes, or
higher resolution modes.

Host Address to Display Address Translation The most complicated part of accessing
display memory involves the translation between a host address and a display memory
address. Internally, the VGA has a 64K 32-bit memory locations. These are divided into
four 64K bit planes. Because the VGA was designed for 8 and 16 bit bus systems, and due
to the way the Intel chips handle memory accesses, it is impossible for the host CPU to
access the bit planes directly, instead relying on I/O registers to make part of the memory
accessible. The most straightforward display translation is where a host access translates
directly to a display memory address. What part of the particular 32-bit memory location
is dependent on certain registers.

8.3.4.1 Universal Asynchronous Receiver/Transmitter (UART)

The transfer of data between two blocks may be performed in parallel or serial. In parallel
data transfer, each bit of the word has its own wire, and all the bits of an entire word are
transmitted at the same time. This means that an N -bit word is transmitted in parallel
through N separate conductor wires. In serial data transmission, bits in a word are sent in
sequence, one at a time. This type of transmission requires only one or two signal lines. The
key feature of a serial interface module is a circuit capable of communicating in bit-serial
fashion on the device side (providing a serial port) and in bit-parallel fashion on the bus side

8.3.

IO System Organization

327

(processor side). Transformation between parallel and serial formats is achieved with shift
registers that have parallel access capability. Parallel transmission is faster, but requires
many wires. It is used for short distances and when speed is important. Serial transmission
is slower, but less expensive, because it requires only one conductor wire.

The UART is a transceiver (transmistter/receiver) that translates data between par-
allel and serial interfaces.

The UART sends a start bit, (cid:12)ve to eight data bits with the least-signi(cid:12)cant-bit (cid:12)rst,
an optional parity bit, and then one, one and a half, or two stop bits. The start bit is the
opposite polarity of the data-line’s idle state. The stop bit is the data-line’s idle state, and
provides a delay before the next character can start. (This is called asynchronous start-stop
transmission).

8.3.4.2 DMA Controller

DMA transfers are performed by a special device controller called a DMA controller.
The DMA controller performs the functions that would normally be performed by the
processor when accessing the main memory. For each word transferred, it provides the
memory address and all the bus signals that control data transfer. The DMA controller
also increments the memory address after transferring each word, and keeps track of the
number of words transferred. A DMA controller may handle DMA operations for a number
of IO devices, or may be dedicated to a single IO device. The latter controller is called a
bus-mastering DMA controller.

Some computer systems permit DMA operations between two IO devices without in-
volving main memory. For example, a block transfer can be performed directly between
two hard disks, or from a video capture device to a display adapter. Such a DMA opera-
tion can improve system performance, especially if the system provides multiple buses for
simultaneous transfers to happen in di(cid:11)erent parts of the system.

8.3.4.3 Keyboard Controller

We shall look at a speci(cid:12)c example|an interface module for a keyboard|to get a better
appreciation of what IO interface modules are and how they work. An interface module
that connects a keyboard is one of the simplest and most commonly used interface modules
in a general-purpose computer. To study its design and working, we need to know a little
bit about how the keyboard device and the associated controller circuitry work.

Figure 8.20 shows a keyboard unit, along with its connections to the interface module.
Whenever a key is depressed or released, a corresponding K-scan code is generated and
sent to the encoder. The encoder converts the K-scan code to a more standard scan code,
and sends it to the interface module|housed in the system unit|usually through a serial
keyboard cable. Notice that if a key is depressed continuously, after a brief period the

328

Chapter 8. Microarchitecture | Kernel Mode

keyboard unit repeatedly sends the scan code for the key’s make code until the key is
released.

Data
Address
Control

IO Interface Module

Interrupt
Interrupt Acknowledge

Read/Write

D
e
c
o
d
e
r

Data Register

Status Register

Control
Logic

Serial-Parallel
Converter

Keyboard Cable

Keyboard Unit

Encoder

Figure 8.20: Role of IO Interface Module for Connecting a Keyboard Unit

With this background on the working of a keyboard unit, let us summarize the ma jor
functions the interface module needs to perform:

(cid:15) Incorporate the IO registers (a small portion of the IO address space) that are part
of the keyboard port.

(cid:15) Convert the scan code arriving serially from the keyboard unit into parallel form, and
place it in an IO register to be read by the processor.

(cid:15) Maintain the status of the keyboard circuitry|interface module and the keyboard
device|in an IO register to be read by the processor.

(cid:15) Generate an interrupt at the occurrence of keyboard-related events, such as updation
of the data register with a scan code.

(cid:15) Process processor commands, such as varying the keyboard’s typematic rate and delay.
This provides a means for the keyboard user to specify to the OS the desired repetition
rate.

8.3.

IO System Organization

329

Figure 8.20 also shows the internals of an interface module that can perform the above
functions4 . It incorporates a data register, a status register 5, an address decoder, a
serial-parallel converter, and control logic. The module works as follows. Upon receiving
a scan code from the keyboard unit, the serial-parallel converter converts it to parallel
form, and places the code in the data register. The control logic makes appropriate
modi(cid:12)cations to the status register, and raises an interrupt to inform the presence of a
valid value in the data register.

The interrupt causes the execution of the ISR part of the keyboard device driver, which
reads the input data register, and copies it to the corresponding process’ keystroke
bu(cid:11)er. The scan code is then converted into ASCII code or other meaningful data by the
systems software, which usually also displays the character on the monitor.

When the keyboard device driver wants to send a command, such as varying the repe-
tition rate, it executes IO write instructions to update the data register and the status
register. The control logic, upon sensing the update, conveys the information to the
serial-parallel converter, which converts it to serial form and sends it to the keyboard unit.

8.3.5

IO Con(cid:12)guration: Assigning IO Addresses to IO Controllers

Within a computer system, from the point of view of the processor (and the device drivers),
an IO controller is uniquely identi(cid:12)ed by the set of IO addresses assigned to its hardware reg-
isters. An IO instruction would specify an IO address, and the corresponding IO controller
would respond.

An issue that we have skirted so far is: how are IO addresses assigned to IO controllers?
And, how does an IO controller know the addresses assigned to its IO registers? A simple-
minded approach is for the kernel mode ISA to specify the addresses assigned to each
IO register. For example, kernel mode addresses 0x a0000000 - 0x a00000007 can be
earmarked for the mouse controller, addresses 0xa00000008 - 0xa000000ff for the keyboard,
and so on. The device drivers would then be written with these addresses hardcoded into
them. The designer of an IO controller can also hardcode addresses into the design so that
its address decoder recognizes only the IO addresses assigned to it. However, this approach
precludes standardization of IO controllers across di(cid:11)erent computer families, driving up
controller costs.

A slightly better option is not to hardcode the addresses in the IO controller, but provide
an ability to manually enter the addresses by setting switches or jumper connections on the
controller, before installing the controller in a computer system. Once the controller knows
the addresses assigned to it, its decoder can speci(cid:12)cally look for those addresses in the

4 In XT systems, for example, this function is implemented by an Intel 8255A-5 or compatible keyboard
interface chip.
5The number of registers incorporated in an IO interface module, in general, is higher than the number of
IO registers visible for the port at the kernel mode ISA level. This is similar to the processor microarchitecture
which incorporates a number of registers in addition to the ones de(cid:12)ned in the ISA.

330

Chapter 8. Microarchitecture | Kernel Mode

future. Although this solves the standardization problem, manual entering of addresses is,
nevertheless, clumsy.

8.3.5.1 Autocon(cid:12)guration

Autocon(cid:12)guration shifts the burden of deciding the IO addresses to the OS. In the basic
autocon(cid:12)guration scheme, called plug-and-play, the Plug and Play Manager part of the
OS enters the addresses into the controller at boot-up time. This requires some extra
instructions in the boot-up code, but removes the burden of manually entering the addresses.
The astute student might wonder how the Plug and Play Manager will communicate to an
IO controller that does not know the addresses it should respond to! What address will the
Plug and Play Manager use for getting the controller’s attention? Consider the following
classroom scenario.
In the (cid:12)rst day of class, the professor wants to assign his students
unique ID numbers that should be used throughout the semester. He wants to convey the
ID numbers to them without calling out their names. One way he could do this is by calling
out the individual seat numbers and telling the corresponding ID number. Once all students
know their ID numbers, individual students can be called out using their ID numbers, and
the students are free to sit wherever they like.

In a similar manner, before an IO controller in the system gets its addresses, we need a
di(cid:11)erent attribute to identify the controller. In the plug-and-play scheme, a unique num-
ber is associated with each IO connector position (analogous to class seat number). These
unique connector numbers help to create yet another address space, called the con(cid:12)guration
address space. Depending on the connector into which an interface module is plugged, the
module has a unique range of con(cid:12)guration addresses. Each module also has a con(cid:12)gura-
tion memory, which stores information about the type and characteristics of the module.
During the interface initialization part of boot-up, the Plug and Play Manager reads the
con(cid:12)guration memory of each IO controller that is hooked up to the system; it uses the
module’s con(cid:12)guration addresses to do this access. The con(cid:12)guration information uniquely
identi(cid:12)es the controller, and provides information concerning the device drivers and re-
sources (such as DMA channels) it requires. After obtaining the information present in the
con(cid:12)guration memory, the Plug and Play Manager assigns IO addresses to the IO registers
present in the controller. Thereafter, the OS uses these IO addresses to communicate with
the controller.

In the above scheme, address assignment is done only during boot-up time. This scheme
mandates that the computer be reset every time a new IO card is connected to the system.
While this scheme may be acceptable for IO controllers that connect to disk drives and CD-
ROM drives, which are rarely connected when the system is working (especially in a desktop
environment), it is somewhat annoying for IO controllers that connect to keyboards, mice,
speakers, etc, which may be connected or disconnected when the system is in operation.
Furthermore, in mission-critical computers, downtimes can be extremely costly. In order
to avoid resetting the computer every time a new IO controller is plugged in, modern

8.3.

IO System Organization

331

computer systems enhance the plug-and-play feature with a feature called hot plug or hot
swap. With this feature, if a new controller is connected when the computer system is
in operation, the system recognizes the change, and con(cid:12)gures the new controller with the
help of systems software. This requires more sophisticated software and hardware (such as
protection against electrostatic discharge) than does plug-and-play. Recent IO buses such
as USB and PCIe are particulary geared for the hot swap feature, as they incorporate a
tree topology. In older standards such as the PCI, the hot swap feature can be incorporated
by using a dedicated secondary PCI bus at each primary PCI slot, as illustrated in Figure
8.21. Because each secondary PCI bus has a single PCI slot, an IO controller can be added
or removed without a(cid:11)ecting other controllers.

Processor−Memory Bus

CPU

Cache
Memory

Cache
Interface &
Controller

Main
Memory

Memory
Interface &
Controller

PM−PCI Bridge

Primary PCI Bus

Primary PCI Bus

PCI−PCI Bridge

PCI−PCI Bridge

PCI−PCI Bridge

SCSI
Controller

Secondary PCI Bus

Secondary PCI Bus

Secondary PCI Bus

SCSI Bus (IO Bus)

Hot Swappable
Slots

USB Controller

Graphics
Interface &
Controller

Ethernet
Interface &
Controller

Disk
Interface &
Controller

CD−ROM
Interface &
Controller

Tape
Interface &
Controller

Input Output
Audio

Serial
Ports

Desktop Bus

Graphics

Ethernet

Disk Drive

CD−ROM Drive

Tape Drive

Figure 8.21: Hot Pluggable PCI Slots

332

Chapter 8. Microarchitecture | Kernel Mode

8.4 System Architecture

As with the processor system and the memory system, many of the characteristics of the
IO system are driven by technology. For example, the properties of disk drives a(cid:11)ect how
the disks are connected to the processor, as well as how the operating system interacts with
them. IO systems di(cid:11)er from the processor and memory systems in several important ways.
Although processor-memory designers often focus primarily on performance, designers of
IO systems must consider issues such as expandability and resilience in the face of failure
as much as they consider performance. Second, for an IO system, characterizing the per-
formance is a more complex characteristic than for a processor. For example, with some
devices we may care primarily about access latency, whereas with others bandwidth|the
number of bits that can be transferred per second|is more important. Furthermore, per-
formance depends on many aspects of the system: the device characteristics, the connection
between the device and the rest of the system, the memory hierarchy, and the operating
system.

Until recently, computer performance was almost exclusively judged by the performance
of the processor-memory subsystem. In the past few years, the situation has begun to change
with users giving more importance to overall system performance. This calls for balanced
systems with IO subsystems that perform as good as the processor-memory subsystem.

The IO controllers need to be connected to the processor-memory system in order for
them to serve as IO ports for hooking up IO devices. There are several ways in which
this connection can be made. The manner in which the controllers are connected a(cid:11)ect
the system performance in a ma jor way. The earliest computers connected each controller
directly to the processor using a direct connection. The resulting connection topology is a
star. as shown in Figure 8.22. In addition to the direct connections to the processor, DMA
controllers, if present, are connected to the main memory as well. The disadvantage of a star
topology is that it requires several cables, at least one for each controller. Moreover, each
controller has to be tailored for a particular processor, and cannot be used in computers
having a di(cid:11)erent processor.

8.4.1 Single System Bus

In order to reduce the number of cables required, the simplest approach that we can think
of is to hook up all of the controllers to the processor-memory bus, which connects the
processor and the main memory. When a single bus is used in this manner to connect
all of the ma jor blocks|the processor, the main memory, and the controllers|the bus is
usually called a system bus (as opposed to a processor-memory bus). Such a connection
was illustrated in Figure 8.19. The two ma jor advantages of hooking up the controllers to
a bus instead of using tailored connections are versatility and low cost due to fewer cables.
Adding new devices and their controllers to the system is very straightforward.

A bus has its own data transfer protocol, and so in a single-bus system, the data transfer

8.4. System Architecture

333

Processor-Memory Bus

CPU

Memory
Interface &
Controller

Memory

Keyboard
Interface &
Controller

Mouse
Interface &
Controller

Graphics
Interface &
Controller

Ethernet
Interface &
Controller

Disk
Interface &
Controller

CD-ROM
Interface &
Controller

Tape
Interface &
Controller

Graphics

Ethernet

CD-ROM Drive

Tape Drive

Figure 8.22: Connecting IO Controllers to the Processor via a Star Topology

protocol between any two devices in the system is identical. The operation of the system
bus is, in fact, quite straightforward. For each data transfer, the master device (usually
the processor or a DMA controller) places the slave address (a memory address or an IO
register address) on the address lines of the bus. The data lines are used to transfer the
data value to be written or the value that is read from a location. The control lines are
used to specify the nature of the transfer, and also to coordinate the actions of the sending
and receiving units.

A computer that used a single system bus, and became very popular during its time in
the 1970s was PDP-11, a minicomputer manufactured by Digital Equipment Corporation
(DEC). Its system bus was called Unibus. The earliest PCs also had only a single system
bus, as the (cid:12)rst and second generation CPUs ran at relatively low clock frequencies, which
permitted all system components to keep up with those speeds.

8.4.2 Hierarchical Bus Systems

A single bus con(cid:12)guration that interconnects the processor, the memory system, and the
IO controllers has to balance the demands of processor-memory communication with those
of IO interface-memory communication. Such a setup has several drawbacks:

(cid:15) The use of a single system bus permits only a single data transfer between the units
at any given instant, potentially becoming a communication bottleneck.

(cid:15) The use of a single system bus may impose a severe practical restriction on the number

334

Chapter 8. Microarchitecture | Kernel Mode

of controllers that can be connected to the bus without increasing the bus capacitance
and skew e(cid:11)ects. A single bus may have high electrical capacitance due to two reasons:
(i) the bus ends up being long as it has to connect all devices in the system, and (ii)
each device that is hooked up to the bus adds to the capacitance. To reduce the
capacitance, each device must have expensive low-impedance drivers to drive the bus.

(cid:15) Because of the nature of most of the IO devices, there is a signi(cid:12)cant mismatch in speed
between transfers involving their controllers and the high-speed transfers between the
processor and memory. If we use a single high-speed system bus, then we are forced to
use expensive, high clock speed, controllers 6 , but without signi(cid:12)cant improvements in
overall performance. By contrast, if we use a low-speed system bus, then the system
will have poor performance.

(cid:15) If we use a single system bus, many of its speci(cid:12)cations (such as word size) will be tied
to the ISA (instruction set architecture), and therefore tend to di(cid:11)er widely across
di(cid:11)erent computer families, and even between family members. This forces us to
design speci(cid:12)c controllers for each computer family, which increases the cost of IO
controllers.

These issues are a concern, except for small computers in which the processor, the main
memory, and the IO controllers are placed on a single printed-circuit board (PCB) or a single
chip (SoC), allowing the system bus to be fully contained within the board. Often, such
systems do not require high performance either. Larger computers such as main-frames and
modern desktops, on the other hand, incorporate a large number of IO controllers, making
it di(cid:14)cult to place all components on a single board. These IO devices tend to have widely
di(cid:11)erent speeds, and the system as a whole tends to require high IO throughputs. All of
these make it impossible to use a single system bus. Therefore, larger computer systems
use multiple buses to connect various devices. These buses are typically arranged in a
hierarchical manner, with the fastest bus at the top of the hierarchy and the slower buses
towards the bottom of the hierarchy7 . Devices are connected to the bus hierarchy at the
level that matches their speed and bandwidth requirements. Activity on one bus does not
necessarily obstruct activity on another.

Figure 8.23 illustrates this concept by depicting a 2-level hierarchical arrangement of
buses. The bus at the top of the hierarchy is typically used to connect the processor and
the memory, and is called a processor-memory bus as before. Apart from the processor-
memory bus, there is a (slower) IO bus that connects all of the controllers together, and
forms the backbone of the IO system. The IO system is connected to the processor-memory

6 If the IO controllers are made to operate at the same clock speeds as the processor, they will become very
expensive, without substantially enhancing the IO throughput, which depend primarily on the IO devices
themselves.
7A faster bus does not imply a higher speed of electrical transmission through the wires, but rather a
shorter time between meaningful bus events (the \bus clock period"). The controllers connected to a faster
bus have to react more quickly, which calls for more rigorous engineering.

8.4. System Architecture

335

system by means of an IO bus bridge, which acts as an interface between the IO bus and
the processor-memory bus. The interfacing function involves translating the signals and
protocols of one bus to those of the other, akin to the function served by the exit ramps
that are used to connect the high-speed highway road systems to slow-speed local road
systems.

Processor-Memory Subsystem

Processor-Memory Bus

CPU

Cache
Memory

Cache
Interface &
Controller

Main
Memory

Memory
Interface &
Controller

PM-IO Bridge

Graphics
Interface &
Controller

Keyboard
Interface & 
Controller

Mouse
Interface &
Controller

Graphics

IO Bus

Ethernet
Interface &
Controller

Ethernet

Disk
Interface &
Controller

CD-ROM
Interface &
Controller

Tape
Interface & 
Controller

IO Subsystem

Disk Drive

CD-ROM Drive

Tape Drive

Figure 8.23: Use of an IO Bus to Interconnect IO Interfaces in the IO System

Large computers and modern desktops extend the bus hierarchy further by using mul-
tiple buses within the IO subsystem, with a bus bridge serving as an interface whenever
two buses are connected together. These buses di(cid:11)er primarily in speed. Depending on the
speed requirements of an IO device, it is connected to the appropriate bus via a proper IO
controller. Based on speed, the di(cid:11)erent buses can be loosely classi(cid:12)ed into the following
categories:

(cid:15) processor-memory buses

(cid:15) high-speed IO bus or backplane buses

(cid:15) low-speed IO bus or peripheral buses.

Processor-memory buses are used within the processor-memory system, and are the ones
closest to the processor. They are generally short, high speed, and matched to the memory
system so as to maximize processor-memory bandwidth. We have already seen processor-
memory buses in detail in the previous chapter. We shall look into the other two types of
buses in this chapter.

336

Chapter 8. Microarchitecture | Kernel Mode

Despite the prevalence of standardization among computer buses, no standardization
exists for the terminology used to characterize computer buses! What some mean by a
\local bus" is entirely di(cid:11)erent from what others mean by it. The classi(cid:12)cation given above
is loosely related to other types of classi(cid:12)cation, such as:

(cid:15) Transparent versus non-transparent

(cid:15) Serial versus parallel

(cid:15) Local (internal) versus expansion (external)

8.4.2.1 High-Speed and Low-Speed IO Buses

The buses used within the IO system can be high-speed or low-speed. Unlike processor-
memory buses, these buses can be lengthy, and can be used to hook up controllers having
di(cid:11)erent data bandwidth requirements. Among the two categories of IO buses, the back-
plane buses are closer (physically as well as logically) to the processor, and serve as the
main expressway that handles a large volume of high-speed IO tra(cid:14)c. Aside from the
graphics interfaces of game PCs, the processor talks to most of the IO interfaces through
the backplane bus, albeit the transfer may eventually go through other buses also. High-
speed device interfaces such as video cards and network controllers are most often connected
directly to the backplane bus. Backplane buses are generally slower than processor-memory
buses, although the di(cid:11)erence in speed has been reducing lately. The name backplane bus
originated in the days when IO interface circuit boards used to be housed in a card cage
having a plane of connectors at the back. Corresponding pins of the backplane connectors
were wired together to form a bus, and each IO interface board was plugged into a connec-
tor. Examples of standard backplane buses are AGP bus, PCI bus, PCIe, and In(cid:12)niBand
bus.

Peripheral buses are usually slower than backplane buses, and are therefore not directly
connected to the processor-memory system. Instead, they are connected to a backplane bus,
and are therefore farther from the processor than the backplane bus. They are typically
used to connect controllers that connect to secondary storage devices such as disk drives,
CD-ROM drives, and tape drives.

8.4.2.2 Transparent and Non-transparent Buses

When a separate IO bus is used as in Figure 8.23, the new bus can be logically con(cid:12)gured
in one of two ways: (i) as a logical extension of the processor-memory bus (transparent
bus), or (ii) logically disjoint to the processor-memory bus (non-transparent). In the former
case, the IO controllers connected to the IO bus logically appear to the processor as if
they are connected directly to the processor-memory bus. The processor can read their
IO registers or write to them by executing IO read/write instructions. The bus bridge is

8.4. System Architecture

337

logically transparent to the processor; all it does is to translate the signals and protocols of
one bus to those of the other. Examples of expansion buses are ISA, PCI, AGP, and PCI
Express. Thus, the PCI bridge is designed in such a manner that IO interfaces connected
to the PCI bus logically appear to the processor as if they are connected directly to the
processor-memory bus.

In the latter case, called non-transparent bus, the processor cannot directly access the
registers in the IO controllers connected to the IO bus.
Instead, a bus adapter (also known
as bus control ler) acts as an IO controller whose registers can be directly accessed by the
processor. Logically speaking, the address space seen by the IO controllers connected to
such a bus is di(cid:11)erent from the IO address space seen by the processor (address domain
isolation). When the processor wants to communicate with an IO controller connected to
such a bus, it sends the appropriate command (along with proper IO controller identi(cid:12)ca-
tion) to the bus adapter, which then performs the required address domain translation and
interacts with the IO controller by accessing its registers. A non-transparent bus is rarely
used as a backplane bus.

Examples of standard non-transparent buses are SCSI, USB, and Firewire. For device
drivers to access these buses, they need to be connected to the computer through a bus
controller/adapter such as SCSI controller, USB host, and FireWire host. A PCI bus can
also be set up to operate as a non-transparent bus by connecting it to the backplane PCI
bus using a non-transparent PCI-to-PCI bridge. Non-transparent bus bridges (or embedded
bridges) are quite useful when an IO device requires a large address space. They are also
useful when con(cid:12)guring intelligent IO subsystems such as RAID controllers, because the
entire subsystem appears to the processor as a single virtual PCI device (only a single
device driver is needed for the subsystem). They also facilitate the interconnection of
multiple processors in communications and embedded PCI applications.

8.4.2.3 Serial and Parallel Buses

One of the important factors a(cid:11)ecting the cost of a bus, especially those that are not short,
is the number of wires in the bus cable. A serial bus transfers data one bit a time. Although
this impacts the bandwidth, it permits thin cables to be used. Serial buses were initially
used within the IO system for hooking up the controllers of low-bandwidth devices such
as keyboard, mouse, and audio. Those serial buses were somewhat slow, and served as
peripheral buses that connected to a backplane bus. A few standards were also developed
for such type of serial buses. Examples are Serial Peripheral Interface Bus (SPI) and I2C.

As bus speeds continued to increase, it became possible to design serial buses that o(cid:11)er
higher bandwidth. Serial buses such as USB, Firewire, and SATA have become very popular
now. Because a serial bus requires only a single signal wire to transmit the data, it is able to
save space in the connector as well as in the cable. This means that many more ports can be
physically located at the computer system unit’s periphery, and that topology can be more
sophisticated than the simple linear topology. USB was designed to allow peripherals to

338

Chapter 8. Microarchitecture | Kernel Mode

be connected without the need to plug expansion cards into the computer’s ISA, EISA, or
PCI bus, and to improve plug-and-play capabilities by allowing devices to be hot-swapped
(connected or disconnected without powering down or rebooting the computer). Moreover,
unlike parallel buses, serial buses are not con(cid:12)ned to a small physical area. Also, it is often
the case that serial buses can be clocked considerably faster than parallel buses, because
designers do not need to worry about clock skew as well as crosstalk between multiple wires,
and better isolation from surroundings acievable due to thinner cables.

Parallel buses provide multiple wires for transmitting addresses and data. Therefore,
bits of address or data move through the bus simultaneously rather than one at a time.
Expansion slots and ports of parallel buses have a large number of pins and are therefore
big. This means that only a small number of expansion slots and ports can be incorporated
on a motherboard or backplane.

8.4.2.4 Local and Expansion Buses (Internal and External Buses)

A bus is generally called a local bus (also called internal bus or host bus) if it is physically
\close" to the processor, and has no expansion slots. Its electrical and functional properties
are suited for relatively short wires and relatively simple device controllers with a fast
response time. As such, it is usually on the motherboard 8 , and is used for connecting the
processor to the memory, o(cid:11)-chip cache memory, or other high-speed structures. These
buses tend to be proprietary. Some standard buses such as VLB and PCI may also be used
as local buses, especially in small systems.

Larger computer systems often have the need to connect to more \distant" devices such
as peripheral devices and other computers. An expansion bus (or external bus) connects
the motherboard to such devices. To this end, it provides expansion slots, thereby allowing
users to \expand" the computer.
The expansion slots permit plug-in expansion cards
to be directly connected to the expansion bus. Standard buses that are commonly used as
expansion buses are ISA, PCI, AGP, USB, and PCIe. An expansion bus can be used as a
backplane bus or a peripheral bus. Historically, the ISA bus was the predominant expansion
bus. Then, the PCI bus became very popular, with the ISA bus being provided for legacy
interfaces.

8.4.2.5 Data Transfer Types

A bus may support multiple types of data tranfer.

Control Transfer: Control transfers are typically used by the processor for sending short,
simple commands to IO devices. They are also used by the processor for receiving status

8 In the embedded systems arena, an entire system is often integrated into a single chip (System on
Chip { SoC) or a single package (System in Package { SiP), in which case the local bus is contained
within a chip package.

8.4. System Architecture

responses from the devices.

339

Interrupt Transfer:
Interrupt transfers are used by devices that transfer very little data
and need guaranteed quick responses (bounded latency). Examples are keyboards and
pointing devices such as mice.

Bulk Transfer: Bulk transfers involve large sporadic transfers such as (cid:12)le transfers. De-
vices such as printers that receive data in one big packet use the bulk transfer mode. These
transfers generally do not have guarantees on bandwidth or latency.

Isochronous Transfer:
Isochronous transfers are guaranteed to occur at a predetermined
speed. This speed may be less than the required speed, in which case data loss can occur.
Streaming devices such as speakers that deal with realtime audio and video generally use
the isochronous transfer mode.

8.4.2.6 Bus Characteristics

We have seen some of the features of computer buses. Some other characteristics relevant
to buses are:

(cid:15) Clock speed

(cid:15) Bus width

(cid:15) Bandwidth

(cid:15) Number of slots

(cid:15) Address spaces supported (memory, IO, con(cid:12)guration, etc.)

(cid:15) Arbitration

(cid:15) Communication medium and distance (copper cable, (cid:12)ber cable, backplane, IC, wire-
less, etc.)

(cid:15) Physical topology (backbone, ring, tree, star, etc.)

(cid:15) Signalling method (single-ended, di(cid:11)erential, etc.)

(cid:15) Plug and Play (PnP) and hot plug features: Plug and play is the ability to automati-
cally discover the con(cid:12)guration of devices attached to a bus at system boot time, with-
out requiring recon(cid:12)guration or manual installation of device drivers. This automatic
assignment of I/O addresses and interrupts to prevent con(cid:13)icts and identi(cid:12)cation of
drivers makes it easy to add new peripheral devices, because the user does not need to

340

Chapter 8. Microarchitecture | Kernel Mode

\tell" the computer that a device has been added. The system automatically detects
the device at reboot time.

Hot plug or hot swap is the ability to remove and replace devices, while the system
is operating. Once the appropriate software is installed on the computer, a user can
plug and unplug the component without rebooting. A well-known example of this
functionality is the Universal Serial Bus (USB) that allows users to add or remove
peripheral components such as a mouse, keyboard, or printer.

8.4.2.7 Expansion Slots and Hardware Ports

An IO bus is a medium for devices to communicate. For an IO bus to be useful, it must
provide the ability to hook up IO controllers. These controllers are hooked up only at speci(cid:12)c
places in the bus where a hardware socket is located. We can classify these hardware sockets
into two kinds, depending on where they are physically located: internal (expansion slots)
and external (hardware ports).

Expansion slots are located inside the system unit, and serve as receptacles for directly
plugging in IO interface cards (or expansion cards). They are usually situated on the
motherboard, as illustrated in Figure 8.24. The (cid:12)gure also shows a sound card being inserted
into one of the expansion slots. Expansion slots are named after the bus to which they are
connected; their size and shape are also based on the bus. Some of the common expansion
slots are SATA slot, IDE or PATA slot, AGP (for graphics cards), PCI slot, and PCIe slot.

Hardware ports are specialized outlets situated on the front or back panel of the computer
system unit. These serve as receptacles for plugs or cables that connect to external devices.
The ports themselves are mounted on the motherboard or the expansion cards in such a way
they show through openings in the system unit panels. Looking at the sound card in Figure
8.24, we can see several hardware ports. When this card is inserted into an expansion slot,
these ports will become visible at the back side of the system unit. Hardware ports are
generally not made of male connectors, since the protruding pins of a male connector can
break easily. Table 8.6 lists some of the commonly found hardware ports, the typical cards
that house them, and their typical uses.

The connectors for the above ports cover a wide variety of shapes such as round (e.g.,
PS/2), rectangular (e.g., FireWire), square (e.g., RJ11), and trapezoidal (e.g., D-Sub).
They also cover a variety of colors such as orange, purple, or gray (e.g., PS/2 port for
keyboard), green (e.g., PS/2 port for mouse, TSR port for speaker), pink (e.g., TSR port
for microphone), amber (serial DB-25 or DB-9), and blue or magenta (e.g., parallel DB-25).
While many of the above ports have their own speci(cid:12)c connectors, others may use di(cid:11)erent
types of connectors. The RS232 port, for instance, may use a wide variety of connectors
such as DB-25, DE-9, RJ45, and RJ50.

8.4. System Architecture

341

Housing Controller

Typical Use
Port Name
Keyboard, Mouse
PS/2
Terminal, Printer
Serial (tty, COM1:, COM2:) RS-232
Printer
LPT1:, LPT2:
Parallel
VGA monitor
VGA
Video card
LCD monitor
Video card
DVI
LAN, ISDN
Network interface card
Ethernet
Flash drive, Mouse, Camera
USB controller
USB
computer network, Camcorder
FireWire
FireWire controller
Sound card
MIDI or \game"
Joystick, other game controllers
Motherboard, video card Uncompressed HDTV
HDMI
Speaker
Mini audio
RCA
Microphone, speaker, musical keyboard, video composite

Table 8.6: Commonly Found Hardware Ports

8.4.3 Standard Buses and Interconnects

We saw that computers use di(cid:11)erent types of buses, having di(cid:11)erent characteristics. An IO
interface that is tailored to connect to a particular bus cannot, in general, be connected to
a di(cid:11)erent bus, because of di(cid:11)erences in bus characteristics. Two important considerations
in designing an IO system, besides performance, are low cost and the ability to interchange
IO interfaces across di(cid:11)erent computer platforms. In order to satisfy these two criteria, it is
important to standardize the buses used in the IO systems of di(cid:11)erent computers, and make
them independent of the ISA (instruction set architecture) 9 . If the computer community
de(cid:12)nes a standard for IO buses, then di(cid:11)erent vendors can make IO interfaces suitable
for that standard bus, increasing the availability of IO interfaces and reducing their cost.
These motivations have propelled the development and use of standard buses in computers
designed in the last 2-3 decades, even if the computers implement di(cid:11)erent ISAs and are
manufactured by di(cid:11)erent vendors. This has led to an enormous degree of commonality
among the IO systems of di(cid:11)erent computers.

Although a uniform bus standard may be desirable, in practice, it is di(cid:14)cult to develop
a standard bus that covers all peripherals (because of the extremely wide range of transfer
speeds and other requirements). Therefore, over the years, several standard buses have
emerged. For each standard, several revisions|supporting higher data rates, lower supply
voltages, etc.|have also appeared. Some of the notable bus standards are ISA (Industry
Standard Architecture), IDE (Integrated Device Electronics), PCI (Peripheral Component

9The processor-memory bus tends to be proprietary.
It is di(cid:14)cult to de(cid:12)ne a standard for this bus,
because its signals and protocols are closely tied to the ISA implemented by the computer, and its speed to
the processor speed.

342

Chapter 8. Microarchitecture | Kernel Mode

Motherboard

Expansion Card

Hardware Ports

Midi

MIC

line in

line out

SPKR

Expansion Slots

Figure 8.24: Expansion Slots and Hardware Ports

Interconnect), SCSI (Small Computer System Interface), and USB (Universal Serial Bus).
Some of these were speci(cid:12)cally developed by the industry as standards whereas the others
became standards through their popularity. By the year 2000, two dominant standards
had emerged in the desktop personal computer market. These are the PCI and SCSI
standards. The IBM-compatible and Macintosh platforms being developed today have
invariably adopted PCI as the backplane bus and SCSI as the peripheral bus. A larger
fraction of workstation vendors are also adhering to these standards. Although systems
with older buses (ISA or IDE) continue to ship, such systems have rapidly been replaced on
all but the lowest-performance computers. We shall take a detailed look at the PCI, SCSI,
and USB standards, which are very popular today in the desktop computing world.

8.4.3.1 Peripheral Component Interconnect (PCI) Bus

This bus standard was developed by Intel in the early 1990s, and became a widely adopted
standard for PCs, thanks to Intel’s decision to put all of the related patents into the pub-
lic domain (which allows other companies to build PCI-based peripherals without paying
royalties). The PCI bus is typically used as a computer’s backplane bus for connecting
many high-bandwidth devices and other buses. Structurally, it is almost always set up as

8.4. System Architecture

343

Internal/
External
Internal
External
External

Clock
speed
8 MHz
66.6 MHz
5 MHz

Data
width
16 bits

8 bits

Data
Max Topology Application
Rate devices

Backplane

Specialized industrial use

133 MB/s
5 MB/s

7

50 pin bus Mass storage devices

External

10 MHz

16 bits

20 MB/s

15

68 pin bus Mass storage devices

External

16 bits

40 MB/s

External

160 MHz

16 bits

640 MB/s

15

15

Mass storage devices

Mass storage devices

Bus

ISA
ATA-7
SCSI-1
Fast
Wide
SCSI
Wide
Ultra
SCSI
Ultra640
SCSI
SATA/300
SAS-2
PCI 2.2
PCIe 2.0

AGP 8x
USB 2.0
Firewire 800

Internal
External
External

Internal

Internal
Internal

3 GHz
GHz
66 MHz

533 MHz

Serial
Serial
64 bits
Serial
32 lanes
32 bits
Serial
Serial

300 MB/s
6 Gb/s
533 MB/s
16 GB/s

2.1 GB/s
60 MB/s
800 Mb/s

4 pin cable Hard drives, CD/DVD drives
Tree
Hard drives, CD/DVD drives, scanners, printers
5 Backplane
1 Point-to
-point

Video card

1
127 Tree
63 Tree

Video card
Ubiquitous
Video devices

Table 8.7: Characteristics of Standard Computer Buses

an expansion bus; that is, expansion slots are attached to the bus. Logically, it is typically
set up as a transparent bus. That is, the PCI bridge is designed in such a manner that IO
interfaces connected to the PCI bus logically appear to the processor as if they are con-
nected directly to the processor-memory bus. The PCI bus also supports autocon(cid:12)guration;
thus, it supports three independent address spaces: memory, IO, and con(cid:12)guration. The
PCI bus provides good support for multimedia applications by providing high bandwidth,
long burst transfers, good support for multiple bus masters, and guaranteed low-latency
access. A 32-bit PCI bus operating at 33 MHz supports a peak transfer rate of 33 MHz (cid:2)
4 bytes = 132 MB per second. The PCI bus is multiplexed, which means that some signal
lines are used for multiple purposes, depending on the operation performed or the step of a
particular operation. Although this makes it slightly di(cid:14)cult to design PCI-compatible IO
interfaces, these interfaces require only fewer pins, which make them less costly.

The PCI bus is common in today’s PCs and other computer types. It is being succeeded
by PCI Express (PCI-E), which o(cid:11)ers much higher bandwidth. As of 2007 the PCI standard
is still used by many legacy and new devices that do not require the higher bandwidth of
PCI-E. PCI is expected to be the primary expansion bus in desktops for a few more years.

344

Chapter 8. Microarchitecture | Kernel Mode

8.4.3.2 Small Computer System Interface (SCSI) Bus

The original SCSI standard (now known as SCSI-1) was de(cid:12)ned by the American National
Standards Institute (ANSI), under the designation X3.131. The SCSI bus is usually used
as a peripheral bus for connecting storage devices such as disk drives, CD-ROM drives,
and tape drives.
It is also used to connect input-output devices such as scanners and
printers. As a peripheral bus, it is slower than the backplane bus, and is connected to the
computer through the backplane bus. Unlike the PCI bus, the SCSI bus is not usually set
up as a transparent bus. IO interfaces connected to the SCSI bus are therefore not directly
manipulated by the device driver’s IO instructions. Each SCSI-1 bus can support up to 8
controllers, allowing up to 8 peripherals to be connected.

IO controllers are connected to a SCSI bus via cables. The
Controllers and Devices:
SCSI bus itself is connected to the backplane bus via a SCSI control ler (also called a host
adapter), coordinates between all of the device interfaces on the SCSI bus and the computer.
The SCSI controller can be a card that is plugged into an available backplane bus slot or
it can be built into the motherboard. The controller itself is like a microcontroller in that
it has a small ROM or Flash memory chip that stores the SCSI BIOS | the software
needed to access and control the devices on the bus. It has the ability to perform DMA
operations. The controller can be directly accessed by the device driver running on the CPU.
On getting commands from the device driver, the SCSI controller accesses the controller of
the appropriate IO device, and performs the required IO operations. The controller can also
deliver electrical power to SCSI-enabled devices. The SCSI bus expects its device interfaces
to have functionality such as bu(cid:11)ering of data and ensuring of data integrity. Each device
connected to a SCSI bus, including the SCSI controller, must have a unique identi(cid:12)er (ID)
in order for it to work properly. This ID is speci(cid:12)ed through a hardware or software setting.
If the bus can support sixteen devices, for instance, the IDs range from zero to 15, with the
SCSI controller typically having the highest ID.

Cables and Connectors:
Internal devices are connected to a SCSI controller by a ribbon
cable. External devices are attached to the controller in a daisy chain using a thick, round
cable.
In a daisy chain, each device connects to the next one in line. For this reason,
external SCSI devices typically have two SCSI connectors | one to connect to the previous
device in the chain, and the other to connect to the next device. Di(cid:11)erent SCSI standards
use di(cid:11)erent connectors, which are often incompatible with one another. These connectors
usually use 50, 68 or 80 pins. Serial Attached SCSI (SAS) devices use (smaller) SATA
cables and SATA-compatible connectors.

SCSI Types: Since the introduction of the (cid:12)rst SCSI standard, many upgrades have been
introduced, resulting in a proliferation of SCSI standards. They di(cid:11)er in terms of speed, bus
width, connectors, and maximum number of devices that can be attached. Examples for

8.4. System Architecture

345

these improved SCSI standards are SCSI-2, Fast SCSI, Ultra SCSI, and Wide SCSI. All of
these SCSI buses are parallel. The newest type of SCSI, called Serial Attached SCSI (SAS),
however, uses SCSI commands but transmits data serially. SAS uses a point-to-point serial
connection to transfer data at 3 gigabits per second, and each SAS port can support up to
128 devices or expanders.

The SCSI bus has several bene(cid:12)ts. It is reasonably fast, with transfer rates up to 640
megabytes per second. It has been around for more than 20 years and has been thoroughly
tested; so it has a reputation for being reliable. However, the SCSI standards also have some
potential problems. They have limited system BIOS support, and have to be con(cid:12)gured
for each computer. There is also no common SCSI software interface. Finally, the di(cid:11)erent
SCSI types are incompatible, di(cid:11)ering in speed, bus width, and connector. The SCSI bus
is slowly being replaced by other buses such as serial-ATA (SATA).

8.4.3.3 Universal Serial Bus (USB)

Overview: The USB is a popular interconnect for connecting external devices to a com-
puter. Just about any computer that you buy today comes with one or more USB ports,
identi(cid:12)ed by the USB icon shown in Figure 8.25. These USB ports let you attach everything
from mice to printers to your computer quickly and easily. This standard was developed by
a collaborative e(cid:11)ort of several companies.

Figure 8.25: USB Icon

Universal Serial Bus (USB) provides a serial bus standard for connecting IO devices
to computers (although it is also becoming commonplace on video game consoles such as
Sony’s PlayStation 2, Microsoft’s Xbox 360, Nintendo’s Revolution, and PDAs, and even
devices like televisions and home stereo equipment). The USB’s low cost led to its adoption
as the standard for connecting most peripherals that do not require a high-speed bus.

Because the USB port is serial, it is small. The USB standard allows a thin (serial)
cable to connect an IO controller module to a USB port, and this makes it possible to move
the IO controller from the systems unit to the the IO device itself. In fact, the USB was
designed to allow peripherals to be connected without the need to plug expansion cards
into the computer’s ISA, EISA, or PCI bus, and to improve plug-and-play capabilities by
allowing devices to be hot swapped.

USB can connect peripherals such as mice, keyboards, gamepads and joysticks, scanners,
digital cameras, printers, external storage, networking components, etc. For many devices
such as scanners and digital cameras, USB has become the standard connection method.

346

Chapter 8. Microarchitecture | Kernel Mode

USB is also used extensively to connect non-networked printers, replacing the parallel ports
which were widely used; USB simpli(cid:12)es connecting several printers to one computer. As of
2005, the only large classes of peripherals that cannot use USB are displays and high-quality
digital video components (because they need a higher bandwidth than that supported by
USB).

Cables, Connectors, and Topology: The USB’s connectivity is di(cid:11)erent from that of
a regular bus. It uses a tree-type topology, with a hub at each node of the tree. Each hub
has a few connectors for connecting USB-compatible IO interfaces and other USB hubs,
sub ject to a limit of 5 levels of branching. The hub at the root of the tree is called the
root hub. It contains a USB host controller, whereas the remaining hubs contain a USB
repeater. The root hub is hooked to the backplane bus of the computer. Physically, it
may be implemented on a USB adapter card that is plugged onto an expansion slot, or it
may be directly implemented on the motherboard. Device drivers can directly access only
the host controller, which contains the hardware registers that map to the computer’s IO
address space. Not more than 127 devices, including the bus devices, may be connected to
a single host controller. Modern computers often have several host controllers, allowing a
very large number of USB devices to be connected. USB endpoints actually reside on the
connected device: the channels to the host is referred to as a pipe. The devices (and hubs)
have associated pipes (logical channels) which are connections from the host controller to
a logical entity on the device named an endpoint. Individual USB cables can run as long
as 5m, and have 4 wires: two are for the power lines (4.5 V and Ground) and the other
two form a twisted pair for data. USB cables do not need to be terminated. USB 2.0 uses
bursts unlike (cid:12)rewire.

Power supply: The USB also has wires for supplying electrical power from the computer
to the connected devices. Devices such as mice that require only a small amount of power
can thus obtain power from the bus itself, without requiring an external power source. The
USB connector provides a single nominally 5 volt wire from which connected USB devices
may power themselves. A given segment of the bus is speci(cid:12)ed to deliver up to 500 mA.
Devices that need more than 500 mA must provide their own power. Many hubs include
external power supplies which will power devices connected through them without taking
power from up-stream.

Enumeration: When the host controller powers up, it queries all of the devices connected
down-stream. For each device, it assigns a unique 7-bit address and loads the device driver
it needs. This process is called enumeration. Devices are also enumerated when they
are connected to the bus while the computer is working; recall that USB devices are hot-
swappable. At the time of enumeration, the host also (cid:12)nds out from each device the type of
data transfer it will perform in the future. The host also keeps track of the total bandwidth
requested by all of the isochronous and interrupt devices. They can consume up to 90

8.4. System Architecture

347

percent of the available bandwidth. After 90 percent is used up, the host denies access to
any other isochronous or interrupt devices. Control packets and bulk transfer packets use
any bandwidth left over (at least 10 percent).

IO Transaction: An IO request reaching the root hub from the backplane bus is prop-
agated to all nodes in the tree. All IO interfaces connected to the tree nodes will see the
request, just like in a regular bus, although the request may travel through one or more
hubs. However, a transaction from an IO interface to the processor-memory system is prop-
agated only towards the root hub, and is not seen by other IO interfaces connected to the
tree. This function is logically di(cid:11)erent from that of a bus.

USB 2.0 (High-speed USB): The standard for USB version 2.0 was released in April
2000 and serves as an upgrade for USB 1.1. USB 2.0 provides additional bandwidth for
multimedia and storage applications and has a data transmission speed 40 times faster than
USB 1.1. To allow a smooth transition for both consumers and manufacturers, USB 2.0 has
full forward and backward compatibility with original USB devices and can work with cables
and connectors made for the original USB. By supporting the following three speed transfer
speeds, USB 2.0 supports low-bandwidth devices such as keyboards and mice, as well as
high-bandwidth ones like high-resolution Webcams, scanners, printers and high-capacity
storage systems.

(cid:15) A Low Speed rate of 1.5 Mbit/s that is mostly used for Human Interface Devices
(HID) such as keyboards, mice and joysticks.

(cid:15) A Full Speed rate of 12 Mbit/s that was the fastest rate before the USB 2.0 speci(cid:12)-
cation. All USB Hubs support Full Speed.

(cid:15) A Hi-Speed rate of 480 Mbit/s.

Storage Devices: USB implements connections to storage devices using a set of stan-
dards called the USB mass-storage device class. This was initially intended for traditional
magnetic and optical drives, but has been extended to support a wide variety of devices.
As USB makes it possible to install and remove devices without opening the computer case,
it is especially useful for external drives. Today, a number of manufacturers o(cid:11)er exter-
nal, portable USB hard drives that o(cid:11)er performance comparable to internal drives. These
external drives usually contain a translating device that interfaces a drive of conventional
technology (IDE, ATA, SATA, ATAPI, or even SCSI) to a USB port. Functionally, the
drive appears to the user just like another internal drive.

348

Chapter 8. Microarchitecture | Kernel Mode

8.4.3.4 FireWire (IEEE 1394)

IEEE 1394 is a serial bus interface standard for high-speed communications and isochronous
real-time data transfer. It is more commonly known as FireWire, the name given by Apple
Inc., its creator (Sony Corp. calls it i.LINK).

While the USB was designed for low-to-medium speed peripherals, the FireWire was
designed for interfacing with high-speed devices such as digital camcorders and disk drives.
It has many similarities with the USB, such as plug-and-play, hot swap, provision of power
through the cable, and ability to connect many devices. Apart from speed di(cid:11)erences, the
big di(cid:11)erence between FireWire and USB 2.0 is that USB 2.0 is host-based, meaning that
devices must connect to a computer in order to communicate. FireWire, on the other hand,
is peer-to-peer, meaning that two FireWire devices can talk to each other without going
through the processor and memory subsystems. Implementing FireWire costs a little more
than USB.

The main features of FireWire are:

* Fast transfer of data * Ability to put lots of devices on the bus * Provision of power
through the cable * Plug-and-play: Unlike USB devices, each FireWire node participates
in the con(cid:12)guration process without intervention from the host system. * Hot-pluggable
ability * Low cabling cost

FireWire is a method of transferring information between digital devices, especially
audio and video equipment. FireWire is fast { the latest version achieves speeds up to 800
Mbps. At some time in the future, that number is expected to jump to 3.2 Gbps when
optical (cid:12)ber is introduced.

Up to 63 devices can be connected to a FireWire bus.

When the computer powers up, it queries all of the devices connected to the FireWire
bus and assigns each one an address, a process called enumeration. Each time a new device
is added to or removed from the bus, the FireWire bus is re-enumerated. FireWire is plug-
and-play as well as hot pluggable; so if you connect a new FireWire device to your computer,
the operating system auto-detects it and starts talking to it.

FireWire 800 is capable of transfer rates up to 800 Mbps, and permits the cable length
to be up to 100 meters. The faster 1394b standard is backward-compatible with 1394a.

Cables and Connectors: FireWire devices can be powered or unpowered. FireWire
allows devices to draw their power from their connection. Two power conductors in the
cable can supply power (8 to 30 volts, 1.5 amps maximum) from the computer to an
unpowered device. Two twisted pair sets carry the data in a FireWire 400 cable using
a 6-pin con(cid:12)guration. Some smaller FireWire-enabled devices use 4-pin connectors to save
space, omitting the two pins used to supply power. The maximum cable length is 4.5m.
The length between nodes can be increased by adding repeaters.

8.4. System Architecture

349

Sending Data via FireWire: FireWire uses 64-bit (cid:12)xed addressing, based on the IEEE
1212 standard. There are three parts to each packet of information sent by a device over
FireWire:

* A 10-bit bus ID that is used to determine which FireWire bus the data came from *
A 6-bit physical ID that identi(cid:12)es which device on the bus sent the data * A 48-bit storage
area that is capable of addressing 256 terabytes of information for each node

The bus ID and physical ID together comprise the 16-bit node ID, which allows for
64,000 nodes on a system. Data can be sent through up to 16 hops (device to device). Hops
occur when devices are daisy-chained together.

Digital Video: Now that we’ve seen how FireWire works, let’s take a closer look at one
of its most popular applications: streaming digital video. FireWire really shines when it
comes to digital video applications. Most digital video cameras or camcorders now have a
FireWire plug. When you attach a camcorder to a computer using FireWire, the connection
is amazing.

An important element of FireWire is the support of isochronous devices. In isochronous
mode, data is streamed between the device and the host in real-time with guaranteed
bandwidth and no error correction. Essentially, this means that a device like a digital
camcorder can request that the host computer allocate enough bandwidth for the camcorder
to send uncompressed video in real-time to the computer. When the computer-to-camera
FireWire connection enters isochronous mode, the camera can send the video in a steady
(cid:13)ow to the computer without anything disrupting the process.

8.4.3.5 Ethernet

Topology: At the most fundamental level, all Ethernet networks are laid out as a bus
topology, with the devices tapping into the bus.

Packet Assembly: To transmit a message across an Ethernet, a node constructs an
Ethernet frame, a package of data and control information that travels as a unit across the
network. Large messages are split across multiple frames.

The arbitration method followed for the Ethernet bus network is di(cid:11)erent from those
used for the buses inside the computer. Instead of using a centralized bus arbitration unit,
it takes a distributed approach. Before a device places a frame in the bus, it has to make
sure that the bus is not in use, i.e., no other frames are already present on the bus. The
device hardware follows the following protocol: it monitors the bus and waits until no frame
is present in the bus. Once it (cid:12)nds no frame to be present, it waits further for a short period
of time and checks again. If the bus is found to be idle, then the device begins transmitting
the frame.

350

Chapter 8. Microarchitecture | Kernel Mode

With the above arrangement, it is possible that two or more devices may simultaneously
(cid:12)nd the bus to be idle and start transmitting their frames at exactly the same time. In
Ethernet parlance, this is termed a col lision. When the Ethernet NIC detects a collision, it
waits for a random amount of time, re-checks to see if the bus is idle and then attempts a
re-transmission.

The original Ethernet described communication over a single cable shared by all devices
on the network. Once a device attached to this cable, it had the ability to communicate
with any other attached device. This allows the network to expand to accommodate new
devices without requiring any modi(cid:12)cation to those devices already on the network.

One interesting thing about Ethernet addressing is the implementation of a broadcast
address. A frame with a destination address equal to the broadcast address (simply called
a broadcast, for short) is intended for every node on the network, and every node will both
receive and process this type of frame.

8.4.3.6 Bluetooth

Bluetooth is an industrial speci(cid:12)cation for wireless personal area networks (PANs). The
standard also includes support for more powerful, longer-range devices suitable for con-
structing wireless LANs. Bluetooth provides a way to connect and exchange information
between devices like personal digital assistants (PDAs), mobile phones, laptops, PCs, print-
ers and digital cameras via a secure, low-cost, globally available short range radio fre-
quency. It is a radio standard primarily designed for low power consumption, with a short
range (power class dependent: 10 centimeters, 10 meters, 100 meters) and with a low-cost
transceiver microchip in each device.

Bluetooth lets these devices talk to each other when they come in range, even if they are
not in the same room, as long as they are within up to 100 meters of each other, depending
on the power class of the product.

Communication and Connection

A Bluetooth device playing the role of the master can communicate with up to 7 devices
playing the role of the slave. This network of up to 8 devices is called a piconet. At any
given time, data can be transferred between the master and one or more slaves; but the role
of the master switches rapidly among the devices in a round-robin fashion. (Simultaneous
transmission from the master to multiple slaves, although possible, is not very common).
Either device may switch the master/slave role at any time.

Bluetooth speci(cid:12)cation also allows connecting multiple piconets together to form a scat-
ternet, with some devices acting as a bridge by simultaneously playing the master role in
one piconet and the slave role in another piconet.

Any Bluetooth device will transmit the following sets of information on demand

* Device Name * Device Class * List of services * Technical information eg: device
features, manufacturer, Bluetooth speci(cid:12)cation, clock o(cid:11)set

8.4. System Architecture

351

Any device may perform an "inquiry" to (cid:12)nd other devices to which to connect, and
any device can be con(cid:12)gured to respond to such inquiries. However, if the device trying to
connect knows the address of the device it will always respond to direct connection requests
and will transmit the information shown in the list above if requested for it. Use of the
device’s services however may require pairing or its owner to accept but the connection
itself can be started by any device and be held until it goes out of range. Some devices
can only be connected to one device at a time and connecting to them will prevent them
from connecting to other devices and showing up in inquiries until they disconnect the other
device.

Every device has a unique 48-bit address. However these addresses are generally not
shown in inquiries and instead friendly "Bluetooth names" are used which can be set by
the user, and will appear when another user scans for devices and in lists of paired devices.
Most phones have the Bluetooth name set to the manufacturer and model of the phone by
default. Most phones and laptops will only show the Bluetooth names and special programs
are required to get additional information about remote devices. This can get confusing
with activities such as Bluejacking as there could be several phones in range named "T610"
for example. On Nokia phones the Bluetooth address may be found by entering "#2820#".
On computers running Linux the address and class of a USB Bluetooth dongle may be
found by entering "hcicon(cid:12)g hci0 class" as root ("hci0" may need to be replaced by another
device name).

Every device also has a 24-bit class identi(cid:12)er. This provides information on what kind of
a device it is (Phone, Smartphone, Computer, Headset, etc), which will also be transmitted
when other devices perform an inquiry. On some phones this information is translated into
a little icon displayed beside the device’s name.

Bluetooth devices will also transmit a list of services if requested by another device;
this also includes some extra information such as the name of the service and what channel
it is on. These channels are virtual and have nothing to do with the frequency of the
transmission, much like TCP ports. A device can therefore have multiple identical services.

Pairing

Pairs of devices may establish a trusted relationship by learning (by user input) a shared
secret known as a passkey. A device that wants to communicate only with a trusted device
can cryptographically authenticate the identity of the other device. Trusted devices may
also encrypt the data that they exchange over the air so that no one can listen in. The
encryption can however be turned o(cid:11) and passkeys are stored on the device’s (cid:12)le system
and not the Bluetooth chip itself. Since the Bluetooth address is permanent a pairing will
be preserved even if the Bluetooth name is changed. Pairs can be deleted at any time
by either device. Devices will generally require pairing or will prompt the owner before
it allows a remote device to use any or most of its services. Some devices such as Sony
Ericsson phones will usually accept OBEX business cards and notes without any pairing
or prompts. Certain printers and access points will allow any device to use its services by
default much like unsecured Wi-Fi networks.

352

Chapter 8. Microarchitecture | Kernel Mode

Air interface

The Bluetooth protocol operates in the license-free ISM band at 2.45 GHz. In order to
avoid interfering with other protocols that use this band, the Bluetooth protocol divides
the band into 79 channels (each 1 MHz wide) and changes channels up to 1600 times per
second. Implementations with versions 1.1 and 1.2 reach speeds of 723.1 kbit/s. Version
2.0 implementations feature Bluetooth Enhanced Data Rate (EDR), and thus reach 2.1
Mbit/s. Technically version 2.0 devices have a higher power consumption, but the three
times faster rate reduces the transmission times, e(cid:11)ectively reducing consumption to half
that of 1.x devices (assuming equal tra(cid:14)c load).

Bluetooth di(cid:11)ers from Wi-Fi in that the latter provides higher throughput and covers
greater distances but requires more expensive hardware and higher power consumption.
They use the same frequency range, but employ di(cid:11)erent multiplexing schemes. While
Bluetooth is a cable replacement for a variety of applications, Wi-Fi is a cable replacement
only for local area network access. A glib summary is that Bluetooth is wireless USB
whereas Wi-Fi is wireless Ethernet, both operating at much lower bandwidth than the
cable systems they are trying to replace.

Applications:

(cid:15) Wireless networking between desktops and laptops, or desktops in a con(cid:12)ned space
and where little bandwidth is required

(cid:15) Bluetooth peripherals such as printers, mice and keyboards

(cid:15) Bluetooth cell phones, which are able to connect to other cell phones, computers,
personal digital assistants (PDAs), and automobile handsfree systems.

(cid:15) Bluetooth mp3 players and digital cameras to transfer (cid:12)les to and from computers
Bluetooth headsets for mobile phones and smartphones * Medical applications Ad-
vanced Medical Electronics Corporation is working on several devices * For remote
controls where infrared was traditionally used. * Hearing aids Starkey Laboratories
have created a device to plug into some hearing aids [2] * Newer model Zoll De(cid:12)brila-
tors for the purpose of transmitting De(cid:12)brilation Data and Patient Monitoring/ECG
data between the unit and a reporting PC using Zoll Rescue Net software.

Speci(cid:12)cations and Features

The Bluetooth speci(cid:12)cation was (cid:12)rst developed by Ericsson

8.4.4 Expansion Bus and Expansion Slots

A bus is a set of electronic signal pathways that allows information and signals to travel
between components inside or outside of a computer.

8.4. System Architecture

353

An expansion slot (connector): Remember that the expansion bus, or external bus,
is made up of the electronic pathways that connect the di(cid:11)erent external devices to the rest
of your computer. These external devices (monitor, telephone line, printer, etc.) connect to
ports on the back of the computer. Those ports are actually part of a small circuit board
or ’card’ that (cid:12)ts into a connector on your motherboard inside the case. The connector is
called an expansion slot.

Note: Communication ports (com ports), printer ports, hard drive and (cid:13)oppy connec-
tors, etc., are all devices which used to be installed via adapter cards. These connectors
are now integrated onto the motherboard, but they are still accessed via the expansion
(external) bus and are allocated the same type of resources as required by expansion cards.
As a matter of fact (and unfortunately, in my opinion), other devices like modems, video
technology, network and sound cards are now being integrated, or embedded, right onto the
motherboard.

Expansion slots are easy to recognize on the motherboard. They make up a row of
long plastic connectors at the back of your computer with tiny copper ’(cid:12)nger slots’ in a
narrow channel that grab the metal (cid:12)ngers or connectors on the expansion cards. In other
words, the expansion cards plug into them. The slots attach to tiny copper pathways on the
motherboard (the expansion bus), which allows the device to communicate with the rest of
the computer. Each pathway has a speci(cid:12)c function. Some may provide voltages needed
by the new device (+5, +12 and ground), and some will transmit data. Other pathways
allow the device to be addressed through a set of I/O (input/output) addresses, so that the
rest of the computer knows where to send and retrieve information. Still more pathways
are needed to provide clock signals for synchronization and other functions like interrupt
requests, DMA channels and bus mastering capability.

As with any other part of the computer, technology has evolved in an e(cid:11)ort to increase
the speed, capability and performance of expansion slots. Now you’ll hear about more
busses - PCI bus, ISA bus, VESA bus, etc. Not to worry! These are all just types of
expansion (external) busses. They just describe the type of connector and the particular
technology or architecture being used. Thus, the adapter card being installed must match
the architecture or type of slot that it’s going into. An ISA card (cid:12)ts into an ISA slot, a PCI
adapter card must be installed into a PCI expansion slot, etc.

Traditionally, PCs have utilized an expansion bus called the ISA bus. In recent years,
however, the ISA bus has become a bottleneck, so nearly all new PCs have a PCI bus for
performance as well as an ISA bus for backward compatibility.

The current popular expansion bus is the PCI (Peripheral Component Interconnect) bus
for all cards except the graphics cards. For graphics cards, the bus of choice is AGP. Most
motherboards today have one AGP slot and several PCI slots. Your expansion cards will
plug into these card slots. Be sure you get cards that match the available type of slots on
your motherboard. Other popular computer buses include NuBus for the Apple Macintosh,
VESA local bus, and PCMCIA (PC Card).

354

Chapter 8. Microarchitecture | Kernel Mode

8.4.5

IO System in Modern Desktops

The IO system is one area where there is a signi(cid:12)cant di(cid:11)erence across di(cid:11)erent computers.
This is despite the use of standard buses and standard ports, which isolate any e(cid:11)ects of
the kernel mode ISA as well as processor characteristics. The signi(cid:12)cant di(cid:11)erences in IO
system organization arise from 3 factors:

(cid:15) Di(cid:11)erences in the number and types of buses used: First, systems often di(cid:11)er in the
number of backplane buses, serial buses, and peripheral buses used. Second, although
standard buses have been proposed, there are just far too many standards available
for each type of bus. For instance, for the backplane bus, the widely used standard
is the PCI. However, because the ISA bus was popular until recently as a backplane
bus, many systems still support an ISA bus that is connected to the PCI bus using a
ISA bus controller. On a di(cid:11)erent note, even within a standard, there are variations
such as SCSI, SCSI-2, and SCSI-3.

(cid:15) The manner in which the buses are interconencted: Even if two computer systems
used exactly the same buses for their IO system, they can di(cid:11)er in the manner in
which these buses are interconnected. Depending on the system requirements, system
architects connect the buses in di(cid:11)erent manners.

(cid:15) Di(cid:11)erences in the number and types of IO interfaces connected: Finally, even if two
computers have the same bus con(cid:12)guration, they may not have the same set of IO
devices hooked to them. Although they may have been shipped with identical system
con(cid:12)guration, the end user can add or remove IO devices later. Moreover, one user
may connect a particular IO interface, such as a keyboard interface, to one port while
another may connect it to a di(cid:11)erent port.

Figure 8.26 shows one possible organization of the IO system in a mid-range to high-
end desktop machine in 2002.
In this con(cid:12)guration, PCI is used as the backplane bus,
with slower devices sharing the lower-performance SCSI bus. The PCI backplane bus is
used to connect all interfaces to the processor-memory system. Several of the slow IO
devices (audio IO, serial ports, and the desktop bus) share a single port onto the PCI bus.
Serial ports provide for connections such as low-speed Appletalk network. The desktop bus
provides support for keyboards and mice. The second interface in the (cid:12)gure is used for
graphics output. The third interface is used for connecting to the ethernet network. The
fourth interface is the SCSI controller, which is used extensively for connecting bulk storage
devices such as disks, tapes, and CD-ROMs. In the (cid:12)gure, a disk drive, a tape drive, and
a CD-ROM reader are connected to the SCSI bus by means of their respective controllers
(interfaces).

8.4. System Architecture

355

CPU

Processor −Memory Bus

Memory

Memory Controller

PCI Bridge

PCI Bus (Backplane Bus)

I/O Controller

Graphics
Controller

Ethernet
Controller

SCSI
Controller

Input Output
Audio

Serial
Ports

Desktop Bus

Graphics

Ethernet

SCSI Bus (I/O Bus)

Disk
Controller

CD −ROM
Controller

Tape
Controller

CD −ROM Drive

Tape Drive

Figure 8.26: A Possible Organization of the IO System in a Modern Desktop Computer

8.4.6 Circa 2006

A circa 2006 PC incorporates many specialized buses of di(cid:11)erent protocols and bandwidth
capabilities, such as the Serial ATA, USB, and Firewire. The PCI bus is too slow to
be the "backbone" for hooking up all of these buses. A solution adopted since the late
1990s?? is to expand the functionality of the PMI-PCI bridge and connect some devices
directly to this bridge. Examples of such devices include the AGP. This central bridge
itself is usually organized as two parts|the northbridge and the southbridge or IO
bridge. The northbridge is used to connect the 3 fastest parts of the system|the CPU,
the memory, and the video card.
In a modern computer system, the video card’s GPU
(graphics processing unit) is functionally a second (or third) CPU.

The northbridge is connected to the southbridge, which routes the IO tra(cid:14)c between the
northbridge and the rest of the IO subsystem. The southbridge provides ports for di(cid:11)erent
types of buses, such as the PCI. With continued advances in bus standards and speeds,
the southbridge is steadily evolving to accommodate more complex bus controllers such as
Serial ATA and Firewire. So today’s southbridge is sort of the Swiss Army Knife of IO

356

Chapter 8. Microarchitecture | Kernel Mode

switches, and thanks to Moore’s Law it has been able to keep adding functionality in the
form of new interfaces that keep bandwidth-hungry devices from starving on the PCI bus.

In an ideal world, there would be one primary type of bus and one bus protocol that
connects the di(cid:11)erent IO devices
including the video card/GPU to the CPU and main
memory. Although we are unlikely to return to this utopian ideal, PCI Express (PCIe)
promises to bring some order to the current chaos. In any case, it is expected to dominate
the personal computer in the coming decade. With Intel’s recent launch of its 900-series
chipsets and NVIDIA and ATI’s announcements of PCI Express-compatible cards, PCIe
will shortly begin cropping up in consumer systems. In(cid:12)niband shows even more technical
promise towards returning to the ideal single bus.

8.4.7 RAID

Future Directions in IO Organization

What does the future hold for IO systems? The rapidly increasing performance of processors
strains IO systems, whose mechanical components cannot o(cid:11)er similar improvements in
performance. To reduce the growing gap between the speed of processors and the access
time to secondary storage (mainly disks), operating systems often cache active parts of the
(cid:12)le system in the physical memory. These caches are called (cid:12)le caches, and like the cache
memories we saw earlier, they attempt to make use of temporal and spatial localities in
access to secondary storage. The use of (cid:12)le caches allows many (cid:12)le accesses to be satis(cid:12)ed
by physical memory rather than by disk.

Magnetic disks are increasing in capacity quickly, but access time is improving only
slowly. In addition to increases in density, transfer rates have grown rapidly as disks in-
creased in rotational speed and disk controllers improved.
In addition, virtually every
high-performance disk manufactured today includes a track or sector bu(cid:11)er that caches
sectors as the read head passes over them.

Many companies are working feverishly to develop next-generation bus standards, as
standards such as PCI are fast approaching the upper limits of what they can do. All of
these new standards have one thing in common. They do away with the shared-bus topology
used in PCI and instead use point-to-point switching connections 10 . By providing multiple
direct links, such a bus can allow multiple data transfers to occur simultaneously.

HyperTransport, one of the recently proposed standards, is beginning to replace the
front side bus. For each session between nodes, it provides two point-to-point links. Each
link can be anywhere from 2 bits to 32 bits wide, supporting a maximum transfer rate of

10This trend is similar to the direct path based processor data paths we saw in Chapter 7. However, there
are some di(cid:11)erences between the direct paths used in modern processor data paths and the direct links used
in modern IO buses. In the case of IO buses, a direct connection between two nodes is established only
while they are communicating with each other. While these two nodes are communicating, no other node
can access that path.

8.5. Network Architecture

357

6.4 GB per second, operating at 2.6 GHz. By using a HyperTransport-PCI bridge, PCI
devices can be connected to the HyperTransport bus.

PCI-Express is another point-to-point system, allowing for better performance. It is
also scalable. A basic PCI-Express slot will be a 1x connection. This will provide enough
bandwidth for high-speed Internet connections and other peripherals. The 1x means that
there is one lane to carry data.
If a component requires more bandwidth, PCI-Express
2x, 4x, 8x, and 16x slots can be built into motherboards, adding more lanes and allowing
the system to carry more data through the connection. In fact, PCI-Express 16x slots are
already available in place of the AGP graphics card slot on some motherboards. As prices
come down and motherboards built to handle the newer cards become more common, AGP
could fade into history.

Motherboards can incorporate PCI-Express connectors that attach to special cables.
This could allow for completely modular computer system, much like home stereo systems.
The basic unit would be a small box with the motherboard having several PCI-Express
connection jacks. An external hard drive could be connected via USB 2.0 or PCI-Express.
Small modules containing sound cards, video cards, and modems could also be connected
as needed. Thus, instead of one large unit, the computer would be only as large as the
devices connected.

8.5 Network Architecture

Till now we were mostly discussing stand-alone computers, which are not connected to any
computer network. Computer networks are a natural extension of a computer’s IO orga-
nization, enabling it to communicate with other computers and remote peripherals such
as network printers. Unlike the IO buses which typically use electrical wires, the connec-
tions in a computer network could be via some forms of telecommunication media such
as telephone wires, ethernet cables, (cid:12)ber optics, or even microwaves (wireless). Based on
transmission technology, we can categorize computer networks into two: broadcast networks
and point-to-point networks.

Broadcast networks have a single communication channel shared by all the devices on the
network. Any of these devices can transmit short messages called packets on the network,
which are then received by all the other devices. An address (cid:12)eld within the packet speci(cid:12)es
the device for whom it is intended. Upon receiving a packet, a device checks this address
(cid:12)eld to see if it is intended for it.

By contrast, point-to-point networks consist of connections between individual pairs of
machines.

Physical distance is an important metric for classifying computer networks, as di(cid:11)erent
techniques are used at di(cid:11)erent size scales.

Most of today’s desktop computers are instead connected to a network, and therefore it

358

Chapter 8. Microarchitecture | Kernel Mode

is useful for us to have a brief introduction to this topic. A computer network is a collection
of computers and other devices that communicate to share data, hardware, and software.
Each device on a network is called a node. A network that is located within a relatively
limited area such as a building or campus is called a local area network or LAN, and a
network that covers a large geographical area is called a wide area network or WAN. The
former is typically found in medium-sized and large businesses, educational institutions,
and government o(cid:14)ces. Di(cid:11)erent types of networks provide di(cid:11)erent services, use di(cid:11)erent
technology, and require users to follow di(cid:11)erent procedures. Popular network types include
Ethernet, Token Ring, ARCnet, FDDI, and ATM.

Give a (cid:12)gure here

A computer connected to a network can still use all of its local resources, such as hard
drive, software, data (cid:12)les, and printer.
In addition, it has access to network resources,
which typically include network servers and network printers. Network servers can serve as
a (cid:12)le server, application server, or both. A (cid:12)le server serves as a common repository for
storing program (cid:12)les and data (cid:12)les that need to be accessible from multiple workstations|
client nodes|on the network. When an individual client node sends a request to the (cid:12)le
server, it supplies the stored information to the client node. Thus, when the user of a client
workstation attempts to execute a program, the client’s OS sends a request to the (cid:12)le server
to get a copy of the executable program. Once the server sends the program, it is copied
into the memory of the client workstation, and the program is executed in the client. The
(cid:12)le server can also supply data (cid:12)les to clients in a similar manner. An application server, on
the other hand, runs application software on request from other computers, and forwards
the results to the requesting client.

8.5.1 Network Interface Card (NIC)

In order to connect a computer to a network, a network interface card (NIC) is required.
This interface card sends data from the computer out over the network and collects incoming
data for the computer. The NIC for a desktop computer can be plugged into one of the
expansion slots in the motherboard. The NIC for a laptop computer is usually a PCMCIA
card. Di(cid:11)erent types of networks require di(cid:11)erent types of NICs.

The NIC performs the tasks that are at the lowest levels in the communication protocol.
The transmitter in the NIC places the actual bit stream on the communications channel.
Most often, it also performs the task of appending additional (cid:12)elds to the bit stream, such
as a preamble for timing purposes, or error check information. The receiver in the NIC
receives packets addressed to the computer, and most likely strips o(cid:11) preamble bits and
performs error checking.

Modems

8.6.

Interpreting an IO Instruction

359

8.5.2 Protocol Stacks

Computer networks may be implemented using a variety of protocol stack architectures,
computer buses or combinations of media and protocol layers, incorporating one or more of
........... such as ATM, Bluetooth, Ethernet, and FDDI.

Computer networks are also making great strides. Both 100 Mbit Ethernet and switched
Ethernet solutions are being used in new networks and in upgrading networks that cannot
handle the tremendous explosion in bandwidth created by the use of multimedia and the
growing importance of the World Wide Web. ATM represents another potential technology
for expanding even further. To support the growth in tra(cid:14)c, the Internet backbones are
being switched to optical (cid:12)ber, which allows a signi(cid:12)cant increase in bandwidth for long-haul
networks.

8.6

Interpreting an IO Instruction

We have discussed at length the IO system and di(cid:11)erent ways of organizing it. It behoves
us to take a look at how an IO instruction is interpreted for a microarchitecture. As we
recall from Chapters 5 and 6, in both the assembly-level architecture and the ISA, all of the
IO instructions in a systems program involve reading or writing IO registers. The rest of
the IO functions are performed by the IO interfaces in conjunction with the IO controllers
and, of course, the IO devices.

As far as the processor is concerned, the interpretation of an IO instruction is quite
similar to that of a load/store instruction. In fact, for memory-mapped IO addresses, there
is hardly any di(cid:11)erence, except that IO addresses are generally unmapped (no address
translation is required), and that the IO access step (the one analogous to the memory
access step) may take a much longer time than a memory access step.

Let us trace through an IO access microinstruction in a MIPS microarchitecture. This
microinstruction is executed after the ALU operation step in which the IO address is calcu-
lated. The IO request is initially transmitted over the processor-memory bus. The backplane
bus bridge, which is hooked to the processor-memory bus, recognizes the address as an IO
address. It performs the necessary signal conversions, and transmits the request over the
backplane bus. All of the IO interfaces hooked to the backplane bus see the address. The
interface that has the matching address responds to the request.

8.7 System-Level Design

The exact boundaries of chips and printed circuit boards (PCBs) keep changing. In the
early days, the processor was built out of multiple chips. With the current very large scale
integration (VLSI) technologies, the processor is invariably built as a single chip. The
main memory has traditionally been built out of multiple chips. However, that is changing

360

Chapter 8. Microarchitecture | Kernel Mode

now. Currently, parts of the cache memory hierarchy have migrated into the processor chip.
Research e(cid:11)orts are on to completely integrate the processor and memory systems into a
single chip.

In the domain of embedded systems, the situation can be di(cid:11)erent. An entire system is
often built on a single chip (system-on-a-chip).

8.8 Concluding Remarks

8.9 Exercises

1. Explain why interrupts are disabled when interpreting a syscall instruction.

2. What are the fundamental storage components in a paging-based virtual memory
system? Explain with (cid:12)gure(s) the basic working of such a virtual memory system.

3. A virtual memory system uses 15-bit virtual addresses. The physical memory consists
of 8 Kbytes. The page size is 2 Kbytes. The TLB can hold up to 3 entries. Both the
TLB and the page table are replaced using the LRU (least recently used) policy.

(a) Indicate using a diagram how the MMU would split a 15-bit address to get the
virtual page number (VPN).

(b) Consider the following sequence of memory address accesses: 0x6(cid:11)c, 0x7(cid:11)c, 0x6000,
0x4000, 0x3000, 0x2000, 0x7(cid:11)c, 0x2008, 0x74fc, 0x64fc. For each of these accesses, in-
dicate if it would be a TLB hit or miss. Also indicate if it would be a page fault or not.

(c) Draw the (cid:12)nal state of the TLB and the page table.

4. Explain the advantages of USB devices over PCI devices.

5. Explain what is meant by a hierarchical bus organization in a computer system.
Explain why hierarchical bus organizations are used in computer systems.

Chapter 9

Register Tranfer Level
Architecture

Listen to counsel and receive instruction, That you may be wise in your latter days.

Proverbs 19: 20

Our ob jective in this chapter is to study ways of implementing various microarchitecture
speci(cid:12)cations we saw in Chapters 7 and 8. Because of the complexity of hardware design, it
is impractical to directly design a gate-level circuitry that implements the microarchitecture.
Therefore, computer architects have taken a more structured approach by introducing one
or more abstraction levels in between. In this chapter, we study the RTL implementation
of the microarchitectures discussed in the last two chapters. An RTL view of a computer
is restricted to seeing the ma jor storage elements in the computer, and the transfer of
information between them.

A particular microarchitecture may be implemented in di(cid:11)erent ways, with di(cid:11)erent
RTL architectures. Like the design of the higher level architectures that we already saw,
RTL architecture design is also replete with trade-o(cid:11)s. The trade-o(cid:11)s at this level also
involve characteristics such as speed, cost, power consumption, die size, and reliability. For
general-purpose computers such as desktops, the most important factors are speed and
cost. For laptops and embedded systems, the important considerations are size and power
consumption. For space exploration and other critical applications, reliability is of primary
concern.

This chapter addresses some of the fundamental questions concerning RTL architectures,
such as:

(cid:15) What are the building blocks in an RTL architecture, and how are they connected
together?

361

362

Chapter 9. Register Tranfer Level Architecture

(cid:15) What steps should the RTL architecture perform to sequence through a machine
language program, and to execute (i.e., accomplish the work speci(cid:12)ed in) each machine
language instruction?

(cid:15) What are some simple organizational techniques to reduce the number of clock cycles
taken to execute each machine language instruction?

9.1 Overview of RTL Architecture

The RTL architecture implements the (more abstract) microarchitecture. The building
blocks at this level are at a somewhat lower level, such as multiplexers and ........

The RTL architecture follows the data path - control unit dichotomy.
RTL designers often use a language called register transfer language 1 to indicate
the control actions speci(cid:12)ed by the control unit. Each ma jor function speci(cid:12)ed at the
microarchitecture level | as a MAL instruction | is expressed as a sequence of RTL
instructions.

The sequence of operations performed in the data path is determined by commands
generated by the control unit. The control unit thus functions as the data path’s interpreter
of machine language programs.

At the RTL, the computer data path consists of several building blocks and subsys-
tems connected together based on the microarchitecture-level data path speci(cid:12)cation. The
ma jor building blocks of the microarchitectures we saw were registers, memory elements,
arithmetic-logic units, other functional units, and interconnections. Assumptions about the
behavior of these building blocks are used by the RTL designer.

9.1.1 Register File and Individual Registers

An RTL architecture incorporates all of the register (cid:12)les and individual registers speci(cid:12)ed
in the microarchitecture. Apart from these registers, the RTL architecture may incorporate
several additional registers and/or latches. These serve as temporary storage for values gen-
erated or used in the midst of instruction execution. One such register/latch, for instance, is
generally used to store a copy of the ML instruction that is currently being executed. This
register is generally called instruction register (IR). Similarly, the result of an arithmetic
operation may need to be stored in a temporary hardware register before it can be routed
to the appropriate destination register in the register (cid:12)le. The data path may also need to
keep track of special properties of arithmetic and logical operations such as condition codes;

1Modern designs are more frequently speci(cid:12)ed using a hardware description language (HDL) such as
Verilog or VHDL.

9.1. Overview of RTL Architecture

363

most data paths use a register called flags2 .

9.1.2 ALUs and Other Functional Units

We have just looked at RTL implementations of the storage elements present in a microar-
chitecture. Next, we shall consider the elements that serve to perform the operations and
functionalities speci(cid:12)ed in the ISA.

9.1.3 Register Transfer Language

By now, it must be amply clear that the data path does not do anything out of its own
volition; it merely carries out the elementary instruction that it receives from the control
unit. Thus, it is the control unit that decides what register value should be made available
in the processor bus in a particular clock cycle. Likewise, the function to be performed
by the ALU on the input values that are present at its inputs in a clock cycle is also
decided by the control unit. For ease of understanding, we shall use the register transfer
notation to express these elementary instructions sent by the control unit to the data path.
This language is called a register transfer language. Accordingly, an elementary instruction
represented in this language is called an RTL instruction. An RTL instruction may be
composed of one or more elementary operations called RTL operations, performed on data
stored in registers or in memory. An RTL operation can be as simple as copying data
from one physical register to another, or more complex, such as adding the contents of two
physical registers and storing the result in a third physical register. In RTL, data transfer
operations and arithmetic/logical operations are speci(cid:12)ed using a notation called register
transfer notation (RTN). In this notation, a data transfer is designated in symbolic form
by means of the replacement operator (!). An example RTL operation is
PC ! MAR

The semantics of this RTL instruction is quite straightforward: copy the contents of
register PC to register MAR. By de(cid:12)nition, the contents of the source register do not change
as a result of the transfer. The RTL operation does not specify how the microarchitecture
should do this copying; it merely speci(cid:12)es what action the microarchitecture needs to do.
We shall deal with the speci(cid:12)cs of the data transfer when we look at logic-level architectures
in Chapter 9.

2 In some architectures such as the IA-32, the flags register is visible at the ISA level (usually as a set
of condition codes). In MIPS-I, it is not visible at the ISA level.

364

Chapter 9. Register Tranfer Level Architecture

9.2 Example RTL Data Path for Executing MIPS-0 ML Pro-
grams

Designing a computer data path is better caught than taught. Accordingly, in this chapter
we will design several example data paths and illustrate the principles involved. For simplic-
ity and ease of understanding, we restrict ourselves to implementing the microarchitecture-
level data paths that we studied in Chapters 7 and 8 for the MIPS-0 ISA. We will start
with the simple data paths, and later move on to the more complex data paths. This simple
data path will be used in the discussions of the control unit as well.

In order to design an RTL data path, we consider each block in the microarchitecture,
and design it as a collection of combinational logic subcircuits and registers/latches. Thus,
we may add some more non-architected registers: IR to store a copy of the current ML
instruction (bit pattern) being executed, and flags to store important properties of the
result produced by the ALU. The output of the flags register is connected to the control
unit (not shown in (cid:12)gure), which can access its bits individually.

We shall augment the multi-function ALU (Arithmetic and Logic Unit) with an ALU
Input Register (AIR) and an ALU Output Register (AOR) to temporarily store one of the
input values and the output value, respectively.

Figure 9.1 pictorially shows one possible way of interconnecting the combinational sub-
circuits and the registers/latches so as to perform the required functions. This (cid:12)gure sup-
presses information on how the control unit controls the functioning of the subcircuits; these
details will be discussed later. In addition to the processor bus, some dedicated paths have
been provided to perform data transfers that are ine(cid:14)cient to be mapped to the bus. One
such path connects the offset (cid:12)eld of IR to the sign extend unit.

The RTL register IR and its outputs have special properties that warrant further dis-
cussion. The data path uses this register to store the bit pattern of the instruction being
interpreted and executed. Unlike other registers, the outputs from IR are organized as
(cid:12)elds, in line with the di(cid:11)erent (cid:12)elds speci(cid:12)ed in the MIPS instruction formats. These
outputs are depicted in Figure 9.2. The 6-bit opcode and func (cid:12)eld outputs are supplied
to the Opcode Decoder (OD), which produces a binary pattern that uniquely identi(cid:12)es the
opcode of the fetched instruction. This binary pattern is supplied to the processor control
unit (not shown in Figure 9.1). The 5-bit rs, rt, and rd outputs are used to select the
register that is to be read or written into. The register read addresses can be rs or rt,
whereas the register write address can be rt, rd, or 31. (Recall that when a subroutine call
instruction is executed, the return address is written into R31.) The 16-bit offset output
includes the least signi(cid:12)cant 16 bits of IR, and is supplied to a sign extend unit to convert
it into a 32-bit signed integer. This 32-bit output of the sign extend unit is connected to
the processor bus directly. This connection is provided for the purpose of executing the
memory-referencing instructions, the register-immediate ALU type instructions, and the
branch instructions, all of which specify a 16-bit immediate operand.

9.2. Example RTL Data Path for Executing MIPS-0 ML Programs

365

Memory Interface

PC

MAR

System Address Bus

Constants

1

2

4

opcode

rs

rt

rd

func

IR

offset

Opcode

To Control Unit

OD

31

Sign
Extend

32

Memory
Structure

Memory Subsystem

System Data Bus

MUX

5
Register Address

Register File
(RF)

Register Data

AIR

ALU

AOR

Flags

Z N C O

MUX

MUX

MDR

s
u
B
 
r
o
s
s
e
c
o
r
P

Processor Subsystem

Figure 9.1: An RTL Data Path for Implementing the MIPS-0 User Mode ISA

The ALU has two 32-bit data inputs, one of which is always taken from microarchitec-
tural register AIR via a direct path. The other input is taken from the processor bus. The
output of the ALU is fed to microarchitectural register AOR through a direct path. We need
register AOR because the ALU output cannot be routed to a storage location in the same
step via the processor bus (though which one of the ALU data inputs arrives in the same
step). Apart from the normal ALU output, other outputs such as zero, negative, and carry
are routed to a flags register via direct paths.

Finally, the data path includes the memory subsystem, which implements the memory
address space de(cid:12)ned in the user mode ISA. For interfacing the processor bus to the mem-
ory subsystem, this data path uses two special microarchitectural registers, called memory

366

Chapter 9. Register Tranfer Level Architecture

6

opcode

5

rs

5

rt

5

rd

target

IR

6

func

5

offset

Figure 9.2: Outputs of Microarchitectural Register IR, which holds a copy of the ML
Instruction

address register (MAR) and memory data register (MDR). The former is used to store the ad-
dress of the memory location to be read from or written to, and the latter is used to store
the data that is read or to be written. The address stored in MAR is made available to the
address inputs of the memory subsystem through the system address bus during a mem-
ory read/write operation. When a memory read operation is performed, MDR is updated
from the system data bus. Similarly, when a memory write operation is performed, the
contents of MDR are transmitted to the appropriate memory through the system data bus.

9.2.1 RTL Instruction Set

The process of executing a machine language (ML) instruction on the data path can be
broken down into a sequence of more elementary steps. We saw in Section 7.1 how we can
use a register transfer language to express these elementary steps. We can de(cid:12)ne a set of
RTL operations for the MIPS-0 data path de(cid:12)ned in Figure 9.1. An RTL operation can
specify the (cid:13)ow of data between two or more storage locations only if there is a connection
between them. Thus

MAR ! AOR

cannot be a valid RTL operation in this data path, because there is no direct connection
between MAR and AOR. Similarly,

MDR ! ALU

can not be a valid RTL operation, because ALU is combinational logic, and not a storage
device.

Normally, all bits of a register are involved in a transfer. However, if a subset of the bits
is to be transferred, then the speci(cid:12)c bits are identi(cid:12)ed by the use of pointed brackets. The
RTL operation

IR<15:0> ! MDR

speci(cid:12)es that bits 15 to 0 of IR are copied to MDR. Similarly, memory locations or general-
purpose registers (GPRs) are speci(cid:12)ed with square brackets. The RTL operation

9.2. Example RTL Data Path for Executing MIPS-0 ML Programs

367

R[rs] ! MDR

indicates that the contents of the GPR whose address is present in the rs (cid:12)eld (of IR) are
transferred to MDR. The rs (cid:12)eld speci(cid:12)es a particular GPR. Similarly, the RTL operation
MDR ! M[MAR]

indicates that the contents of MDR are transferred to the memory location whose address is
present in MAR.

If the interconnections of the data path are rich enough to allow multiple RTL opera-
tions in the same time period, these can be denoted by writing them in the same line, as
follows:

AIR + 4 ! AOR;

M[MAR] ! MDR

speci(cid:12)es that in the same step, the value in AIR is incremented by 4 and written to AOR,
and the contents of the memory location addressed by MAR are copied to MDR.

Finally, for RTL operations that are conditional in nature, an \if " construct patterned
after the C language’s \if " construct is de(cid:12)ned.
AOR ! PC
if (Z)

indicates that if the zero (cid:13)ag is equal to 1, the contents of AOR is copied to PC; otherwise no
action is taken.

9.2.2 RTL Operation Types

The actions performed by RTL operations can be classi(cid:12)ed into three di(cid:11)erent types:

1. Data transfer: These RTL operations copy the contents of one register/memory
location to another. An example data transfer RTL operation is PC ! MAR.

2. Arithmetic/Logic: These RTL operations perform arithmetic or logical operations
on data stored in registers. Data transfers can only occur along the interconnections
provided in the data path, from one register/memory location to another. An example
arithmetic RTL operation is AIR + PC ! AOR.

3. Conditional: These RTL operations perform a function such as a data tranfer if and
only if the speci(cid:12)ed condition is satis(cid:12)ed. An example conditional RTL operation is
if (Z) AOR ! PC.

Table 9.1 gives a list of useful RTL operations for the microarchitecture of Figure 9.1.
A given RTL operation may specify actions of more than one type. For example, the RTL
operation if (Z) AOR ! PC is a conditional as well as a data transfer RTL operation. The
RTL operations done (in parallel) in a single step form an RTL instruction.

368

No.

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

18
19
20
21

22

Chapter 9. Register Tranfer Level Architecture

RTL Operation
for Data Path

Comments

Data Transfer Type
PC ! MAR
PC ! MDR
PC ! AIR
PC ! R[31]
M[MAR] ! MDR
Read from memory
MDR ! M[MAR] Write to memory
MDR ! IR
MDR ! AIR
MDR ! R[rt]
MDR ! PC
R[rs] ! AIR
R[rs] ! PC
R[rt] ! AIR
R[rt] ! MDR
SE(offset) ! AIR
AOR ! R[rd]
AOR ! R[rt]
AOR ! MAR
Arithmetic/Logic Type
AIR op R[rt] ! AOR
AIR op const ! AOR
AIR op SE(offset) ! AOR
AIR op R[rt] ! Z

Copy sign-extended Offset (cid:12)eld of IR to AIR

Conditional Type
if (Z) AOR ! PC

Table 9.1: A List of Useful RTL Operations for the Data Path of Figure 9.1

9.2.3 An Example RTL Routine

Now that we are familiar with the syntax of RTL as well as the useful RTL operations for
the data path of Figure 9.1, we shall look at a simple sequence of RTL operations called an
RTL routine. Let us write an RTL routine that adds the contents of registers speci(cid:12)ed in
the rs and rt (cid:12)elds of IR, and writes the result into the register speci(cid:12)ed in the rd (cid:12)eld
of IR. The astute reader may have noticed that executing this RTL routine amounts to
executing the MIPS-I machine language instruction and rd, rs, rt in the data path of
Figure 9.1, provided the binary pattern of this instruction is languishing in IR.

In theory, we can write entire programs in RTL that can perform tasks such as ‘printing

9.3.

Interpreting ML Programs by RTL Routines

369

Step
0
1
2

Comments
RTL Instruction
Copy value of register rs to AIR
R[rs] ! AIR
AIR AND R[rt] ! AOR
AND this value with value in register rt
AOR ! R[rd] Write result into register rd

Table 9.2: An Example RTL Routine for the Data Path of Figure 9.1

\hello, world!" ’ and more; we will, of course, need a special storage for storing the RTL
programs and a mechanism for sequencing through the RTL program. Having seen the
di(cid:14)culty of writing programs in assembly language and machine language, one can easily
imagine the nightmare of writing entire programs in RTL! However, developing an RTL
program may not be as bad as it sounds, given that we can develop translator software such
as assemblers that take machine language programs and translate them to RTL programs.
The real di(cid:14)culty is that RTL programs will be substantially bigger than the corresponding
ML programs, thereby requiring very large amounts of storage. This is where interpretation
comes in. By generating the required RTL routines on-the-(cid:13)y at run-time, the storage
requirements are drastically reduced, as we will see next.

9.3

Interpreting ML Programs by RTL Routines

Having discussed the basics of de(cid:12)ning RTL routines, our next step is to investigate how
machine language programs can be interpreted by a sequence of RTL instructions that are
de(cid:12)ned for a particular data path. This section discusses how RTL instructions can be put
together as an RTL routine to carry out this interpretation for each instruction de(cid:12)ned
in the ISA. An RTL routine is allowed to freely use and modify any of the ISA-invisible
microarchitectural registers. The ISA-visible registers, however, can be modi(cid:12)ed only as
per the semantics of the ML instruction being interpreted.

9.3.1

Interpreting the Fetch and PC Update Commands for Each In-
struction

When implementing ISAs supporting (cid:12)xed length instructions, the actions required to per-
form the fetch command are the same for all instructions. The MIPS-0 ISA is no exception.
We shall consider this phase of instruction execution (cid:12)rst. In the data path of Figure 9.1,
register PC keeps the memory address of the next ML instruction to be fetched. To fetch
this instruction from memory, we have to utilize the memory interface provided in the data
path. This interface consists of two registers, MAR and MDR, that store the memory address
and data, respectively. Table 9.3 gives an RTL routine that implements the fetch phase of
executing the instruction. This routine also includes the RTL instructions for updating PC
after the fetch phase so as to point to the next ML instruction to be executed.

370

Chapter 9. Register Tranfer Level Architecture

Step RTL Instruction Comments
Fetch and Decode
PC ! MAR
M[MAR] ! MDR
MDR ! IR

Decode instr
PC increment
PC ! AIR
AIR + 4 ! AOR
AOR ! PC

Goto execute phase

0
1
2
3

4
5
6

Table 9.3: An RTL Routine for Fetching a MIPS-0 ML Instruction in the Data Path of
Figure 9.1

The (cid:12)rst step involves copying the contents of PC to MAR so that this address can be
supplied through the system address bus to the memory system. Thus, at the end of
step 0, MAR has the address of the memory location that contains the instruction bit pat-
tern. Step 1 speci(cid:12)es a memory read operation, during which the contents of the memory
location speci(cid:12)ed through the system address bus by MAR are read from the memory. The
instruction bit pattern so read is placed on the system data bus, from where it is loaded
into memory interface register MDR in the processor data path. Thus, at the end of step
1, the instruction bit pattern is present in MDR. In this RTL routine, we consider the time
taken to perform this step as one clock cycle, although the exact number of cycles taken
depends on the speci(cid:12)cs of the memory system used. In step 2, the instruction bit pattern
is copied from MDR into microarchitectural register IR.

Once the instruction bit pattern is copied into IR, the instruction decoding circuitry (not
shown in (cid:12)gure) decodes the bit pattern. The exact manner in which instruction decoding
is performed is not speci(cid:12)ed here; we have earmarked a separate step for carrying out the
decode operation. The opcode and funct (cid:12)elds of IR uniquely identify the instruction.
Steps 0-3 thus constitute the RTL routine for the fetch phase of instruction interpreta-
tion. Naturally, this portion is the same for every instruction in the MIPS ISA because
all instructions have the same size. Had the MIPS ISA used variable length instructions,
this fetch process would have to be repeated as many times as the number of words in the
instruction.

After executing an ML instruction, the system needs to go back to step 0 so as to execute
the next instruction in the ML program. However, prior to that, it has to increment PC to
point to the next instruction; otherwise the same ML instruction gets executed repeatedly.
In this RTL routine, this update of PC is done immediately after completing the instruction
fetch. Thus, in steps 4-6, PC is incremented by 4 to point to the next instruction in the
executed machine language program. After step 6, the system goes to the execute phase.

9.3.

Interpreting ML Programs by RTL Routines

371

This RTL routine takes 7 clock cycles to complete. We can, in fact, perform the PC
increment function in parallel to the instruction fetch function, and reduce the total number
of clock cycles required for the two activities. Table 9.4 provides the modi(cid:12)ed RTL routine,
which requires only 4 clock cycles. In this routine, in steps 0 and 1, the updated value of
PC is calculated in parallel with the transfer of the instruction bit pattern from the main
memory to MDR. It is important to note that multiple RTL operations can be done in parallel,
only if they do not share the same bus or destination register. In general, the more the
connectivity provided in a data path, the more the opportunities for performing multiple
RTL operations in parallel.

Step

0
1
2
3

RTL Instruction
Comments
Fetch, Decode, and PC increment
PC ! MAR;
PC ! AIR
M[MAR] ! MDR; AIR + 4 ! AOR Read instr
MDR ! IR
AOR ! PC

Decode instr
Goto execute phase

Table 9.4: An Optimized RTL Routine for Fetching a MIPS-0 ML Instruction in the Data
Path of Figure 9.1

9.3.2

Interpreting Arithmetic/Logical Instructions

We just saw an RTL routine for fetching an instruction. Next, let us consider the execute
phase of ML instructions. Unlike the fetch phase, this phase is di(cid:11)erent for di(cid:11)erent ML
instructions. Therefore, the RTL routines for the execute phase will be di(cid:11)erent for the
di(cid:11)erent instructions. We shall consider one instruction each from the four types of ML
instructions: (i) data transfer instruction, (ii) arithmetic/logical instruction, (iii) control
(cid:13)ow changing instruction, and (iv) syscall instruction.

Let us start by considering an arithmetic instruction. We shall put together a sequence
of RTL instructions to interpret an arithmetic/logical instruction.

Example: Consider the MIPS ADDU instruction whose symbolic representation is addu
rd, rs, rt. Its encoding is given below.

000000

rs

rt

rd

ADD

The (cid:12)elds rs, rt, and rd specify register numbers; the (cid:12)rst two of these contain the data
values to be added together as unsigned intergers. The rd (cid:12)eld indicates the destination
register, i.e., the register to which the result should be written to, as long as it is not

372

Chapter 9. Register Tranfer Level Architecture

register $0. In this data path, the addition of the two register values can be done in the
ALU. However, there is only a single bus to route the two register values as well as the
result of the addition operation. Therefore, we will have to do the routing of data values in
multiple steps and use the temporary registers AIR and AOR.

Table 9.5 speci(cid:12)es a sequence of RTL instructions for carrying out the execute phase of
this instruction in the data path of Figure 9.1. Let us go through the working of this RTL
routine. We name the (cid:12)rst RTL instruction of the routine as step 4, as the execute phase
is a continuation of the fetch phase, which ended at step 3.

Step

4
5
6

RTL Instruction
Comments
Execute phase
R[rs] ! AIR
Perform arithmetic operation
AIR + R[rt] ! AOR
if (rd) AOR ! R[rd] Write result

Table 9.5: An RTL Routine for the Execute Phase of the Interpretation of the MIPS-0
ISA Instruction Represented Symbolically as addu rd, rs, rt. This RTL Routine is for
executing the ML instruction in the Data Path of Figure 9.1

Step 4 begins the execution phase. First, the register operands must be fetched from
the register (cid:12)le. In step 4, the value in the rs (cid:12)eld of IR is used as an address to read the
general-purpose register numbered rs into microarchitectural register AIR. In step 5, the
contents of general-purpose register numbered rt are read and supplied to the ALU, which
adds it to the contents of AIR, and stores the result in the microarchitectural register AOR.
In step 6, the contents of AOR are transfered to the general-purpose register numbered rd,
if rd is non-zero. By performing this sequence of RTL instructions in the correct order, the
ML instruction addu rd, rs, rt is correctly interpreted.

9.3.3

Interpreting Memory-Referencing Instructions

Let us next put together a sequence of RTL instructions to fetch and execute a memory-
referencing machine language instruction. Because all MIPS instructions are of the same
length, the RTL routine for the fetch part of the instruction is the same as before; the dif-
ferences are only in the execution part. Consider the MIPS load instruction whose symbolic
representation is lw rt, offset(rs). The semantics of this instruction are to copy to
GPR rt the contents of memory location whose address is given by the sum of the contents
of GPR rs and sign-extended offset. We need to come up with a sequence of RTL in-
structions that e(cid:11)ectively fetch and execute this instruction in the data path of Figure 9.1.

100011

rs

rt

offset

9.3.

Interpreting ML Programs by RTL Routines

373

The interpretation of a memory-referencing instruction requires the computation of an
address. For the MIPS ISA, address calculation involves sign-extending the offset (cid:12)eld of
the instruction to form a 32-bit signed o(cid:11)set, and adding it to the contents of the register
speci(cid:12)ed in the rs (cid:12)eld of the instruction. In this data path, the address calculation is done
using the same ALU, as no separate adder has been provided. With this introduction, let
us look at the RTL routine given in Table 9.6 to interpret this lw instruction.

Step

4
5
6
7
8

Comments

RTL Instruction
Execute phase
R[rs] ! AIR
AIR + SE(offset) ! AOR
AOR ! MAR
Read from memory
M[MAR] ! MDR
if (rt) MDR ! R[rt] Write loaded value

Compute memory address

Table 9.6: An RTL Routine for the Execute Phase of the Interpretation of the MIPS-0 ISA
Instruction Represented Symbolically as lw rt, offset(rs). This RTL Routine is for
executing the ML instruction in the Data Path of Figure 9.1

In step 4, the contents of register speci(cid:12)ed in the rs (cid:12)eld is copied to AIR. In step 5,
the sign-extended offset value is added to this value to obtain the memory address. This
memory address is stored in AOR, and is copied to MAR in the next step. Recall that for a
memory transfer to take place, the address of the memory location must be present in MAR.
In step 7, the actual memory read is performed and the value obtained is stored in MDR;
this step is the same as step 2 of the fetch routine. Finally, in step 8, the loaded value is
copied to the register speci(cid:12)ed in the rt (cid:12)eld of IR.

9.3.4

Interpreting Control-Changing Instructions

The instructions that we interpreted so far | addu and lw | do not involve control (cid:13)ow
changes that cause deviations from straightline sequencing in the ML program. Next let us
see how we can interpret control-changing instructions, which involve modifying PC, usually
based on a condition.

Example: Consider the MIPS-0 conditional branch instruction whose symbolic representa-
tion is beq rs, rt, offset. The semantics of this instruction state that if the contents
of GPRs rs and rt are equal, then the value offset (cid:2) 4 + 4 should be added to PC so as
to cause a control (cid:13)ow change3 ; otherwise, PC is incremented by 4 as usual. The encoding
of this instruction is given below:

3The actual MIPS ISA uses a delayed branch scheme; i.e., the control (cid:13)ow change happens only after
executing the ML instruction that follows the branch instruction in the program. We avoid delayed branches
to keep the discussion simple.

374

Chapter 9. Register Tranfer Level Architecture

000100

rs

rt

offset

Step

4
5
6
7
8
9

RTL Instruction
Comments
Execute phase
PC ! AIR
SE(offset) << 2 ! AOR Multiply offset by 4
AIR + AOR ! AOR Calculate branch target address
R[rs] ! AIR
R[rt] == AIR ! Z
if (Z) AOR ! PC

Evaluate branch condition
Update PC if condition is satis(cid:12)ed

Table 9.7: An RTL Routine for the Execute Phase of the Interpretation of the MIPS-0 ISA
Instruction Represented Symbolically as beq rs, rt, offset. This RTL Routine is for
executing the ML instruction in the Data Path of Figure 9.1

Table 9.7 presents an RTL routine to execute this instruction in the data path given in
Figure 9.1. The execute routine has 6 steps numbered 4-9. The (cid:12)rst part of the routine
(steps 4-6) calculates the target address of the branch instruction. In step 4, the incremented
PC value is copied to AIR. In the next step, the sign-extended offset value is multiplied by
4 by shifting it left by 2 bit positions. In the next step, it is added to the copy of PC value
present in AIR to obtain the target address of the beq instruction. In step 8, the contents
of register rt are compared against those of AIR (which were copied from register rs), and
the result of the comparison is stored in (cid:13)ag Z. That is, Z is set to 1 if they are the same,
and reset to 0 otherwise. In the last step, PC is updated with the calculated target value
present in AOR, if (cid:13)ag Z is 1. Thus, if (cid:13)ag Z is not set, then PC retains the incremented value
it obtained in step 3, which is the address of the instruction following the branch in the
machine language program being interpreted.

9.3.5

Interpreting Trap Instructions

We have seen the execute phase of the interpretation for all of the instruction types except
the syscall instructions. Let us next look at how the data path performs this interpreta-
tion, which is somewhat di(cid:11)erent from the previously seen ones. Like the control-changing
instructions, syscall instructions also involve modifying PC. For the MIPS-I ISA, the pc is
updated to 0x80000080. In addition, the machine is placed in the kernel mode. We shall
take a detailed look at the corresponding RTL sequence in Section 8.1.1, along with other
kernel mode implementation issues.

9.4. RTL Control Unit: An Interpreter for ML Programs

375

9.4 RTL Control Unit: An Interpreter for ML Programs

We have seen how individual machine language instructions can be interpreted using a
sequence of RTL instructions for execution in a data path. Stated di(cid:11)erently, execution
of an RTL routine in the data path is tantamount to executing the corresponding ML
instruction. And, the execution of many such RTL routines is tantamount to executing an
entire ML program. In other words, when the data path executes an RTL program | a
sequence of RTL routines | it indirectly executes an ML program.

Who generates the RTL routine for each ML instruction? Where is the routine stored
after it is generated? These are questions we attempt to answer in this section. Generation
of the RTL instructions, in the proper sequence, is the function of the control unit. The
control unit thus serves as the interpreter of machine language programs. In other words,
it is the unit that converts ML programs to equivalent RTL programs. The algorithm for
doing this conversion is hardwired into the control unit hardware.

We can think of two di(cid:11)erent granularities at which the control unit may supply RTL
routines to the data path4 . The (cid:12)rst option is to generate the entire RTL routine for the
execution of an instruction (about 3-5 RTL instructions) in one shot, and give it to the data
path. The data path will then need a storage structure to store the entire routine. It will
also have to do its own sequencing through this RTL routine. Because of shifting some of
the sequencing burden to the data path, the control unit will be somewhat simpler.

In the alternate option | the more prevalent one | RTL instructions are supplied one
at a time to the data path. The control unit is then responsible for sequencing through the
RTL sequence. The data path just needs to carry out the most recent RTL instruction that
it has received from the control unit. We will be dealing exclusively with this option in this
chapter.

Figure 9.3 shows the general relationship between the overall data path and the proces-
sor control unit. The (cid:12)gure explicitly shows only the processor’s data path. The processor
control unit periodically receives the status of the data path, and speci(cid:12)es the next mi-
croinstruction to be performed by the data path. Thus, the processor control unit controls
the processing of data within the processor data path, as well as the (cid:13)ow of data within,
to, and from the processor data path. In this section we show how to design a processor
control unit that generates microinstructions in a timely fashion.

9.4.1 Developing an Algorithm for RTL Instruction Generation

Before the design of the control unit can begin, it is imperative to de(cid:12)ne the processor data
path (including the interface to the processor-memory bus) as well as the RTL routines

4An important point is in order here. RTL instructions are not supplied to the data path in the English-
like RTN format that we have been using in this chapter. Rather, they are supplied in an encoded manner
called microinstructions. In the ensuing discussion, we often use the terms \microinstruction" and \RTL
instruction" interchangeably.

376

Chapter 9. Register Tranfer Level Architecture

Processor

Status

Control
Unit

(ML Instruction, Flags)
Control

Data
Path

(Microinstruction)

Address

Data

Control

Figure 9.3: A Microarchitectural View of the Interaction between the Processor Control
Unit, Processor Data Path, and the Rest of the System

for interpreting each of the machine language instructions. The control unit designer then
translates all of the RTL routines into an encoded form called microroutines either by hand
or by a micro-assembler (which is a program similar to an assembler).
In implementing
the control unit, there are two distinct aspects to deal with: (i) proper sequencing through
the steps, and (ii) generating the appropriate microinstruction for each step. We can achieve
both of these tasks by (cid:12)rst combining the RTL routines for all of the machine language
instructions into a single microprogram. Such an algorithm will contain not only intra-
routine sequencing, but also inter-routine sequencing.

We shall use the now familiar data path of Figure 9.1 (on page 365) for designing the
sample control unit. For this data path, we had already seen the RTL routines to be
generated to carry out the fetch phase of the MIPS-0 ISA instructions, and the execute
phase of the following three instructions: (i) addu rd, rs, rt, (ii) lw rt, offset(rs),
and (iii) beq rs, rt, offset. These routines are available in Tables 9.4-9.7.

The next step in the design is to put all of these information together to develop a
(cid:13)owchart or algorithm for the functioning of the control unit. Table 9.8 combines the
generation of all of these RTL routines into a single algorithm starting at step 0. The
(cid:12)rst column, step, indicates the current step performed by the control unit, and the next
column, next step, indicates the (cid:13)ow of control through this algorithm. The third (cid:12)eld
indicates the RTL instruction to be generated for the current step, and the last (cid:12)eld indicates
comments, if any. The next step (cid:12)eld facilitates the processor control unit in its seqencing
function. As can be seen from the next step entries, control (cid:13)ow through the control unit
algorithm is mostly straightline in nature. i.e., the next step is generally the immediately
following step.

In this algorithm, the generation of the instruction fetch routine appears just once (as it
is the same for all instructions), and includes steps 0 through 3. The decode process takes
place in step 3, and enables the control circuitry to choose the appropriate RTL instructions
for the execution phase. The next step (cid:12)eld for step 3 has an entry of n. This indicates
that a multi-way branch is required in the control unit’s algorithm, based on the opcode of
the decoded ML instruction. Notice also that the next step (cid:12)eld is set to 0 at the end of
each execute routine to indicate that control has to go back to step 0 after the execution of
that RTL instruction.

9.4. RTL Control Unit: An Interpreter for ML Programs

377

Step

Next
Step

0
1
2
3

4
5
6

7
8
9
10
11

12
13
14
15
16
17

1
2
3
n

5
6
0

8
9
10
11
0

13
14
15
16
17
0

RTL Instruction Generated
for Data Path
Fetch phase of every instruction
PC ! MAR;
PC ! AIR
M[MAR] ! MDR; AIR + 4 ! AOR
MDR ! IR
AOR ! PC

Comments

Decode
Branch based on opcode

Execute phase of addu
R[rs] ! AIR
R[rt] + AIR ! AOR
if (rd) AOR ! R[rd] End of routine
Go to step 0

Execute phase of lw
R[rs] ! AIR
AIR + SE(offset) ! AOR
AOR ! MAR
M[MAR] ! MDR
if (rt) MDR ! R[rt] End of routine
Go to step 0

Execute phase of beq
PC ! AIR
SE(offset) << 2 ! AOR
AIR + AOR ! AOR
R[rs] ! AIR
R[rt] == AIR ! Z
if (Z) AOR ! PC

Execute phase of next ML instruction
...

End of routine
Go to step 0

Table 9.8: The Algorithm followed by the Control Unit for Generating RTL Instructions so
9.4.2 Designing the Control Unit as a Finite State Machine
as to Interpret MIPS-0 ML Programs for Execution in the Single-Bus Data Path of Figure
9.1
On inspecting the algorithm given in Table 9.8, we can see that the processor control unit
essentially goes through an in(cid:12)nite loop starting from step 0. This control unit can be
designed as a (cid:12)nite state machine (FSM). In order to do this, we can develop a state
transition diagram for this algorithm. This state diagram corresponds to a Moore-type
FSM, where the output values are strictly a function of the current state. Figure 9.4
presents a state transition diagram for this FSM. In this diagram, each step of Table 9.8 is
implemented by a state, which decides the microinstruction to be generated when in that
state. The (cid:12)gure also indicates the conditions that cause the control unit FSM to go from
one state to another. Most of the transitions are not marked by a condition, which means
that those transitions occur unconditionally. The (cid:12)rst 4 states, S0 - S3, correspond to the
instruction fetch routine; the microinstruction to be generated in each of these states s

378

Chapter 9. Register Tranfer Level Architecture

corresponds to what is given in Table 9.8 for step s. At the end of the microroutine for
instruction fetch, the fetched ML instruction would have been decoded, and a multi-way
branch occurs from state S3 to the appropriate microroutine for executing the instruction.
By specifying the states and their transitions, we specify the microinstruction the processor
control unit must generate in order for the data path to fetch, decode, and execute every
instruction in the ISA.

S0

PC

MAR, AIR

S1

M[MAR]

MDR; AIR+4

AOR

S2

MDR

IR

S3

AOR

PC

addu

lw

beq

S4

S5

S6

S7

S8

S9

S10

S11

S12

S13

S14

S15

S16

S17

Fetch & Decode
ML Instruction

Multi−way
Branch

Execute
ML Instruction

addu rd,rs,rt

lw rt,offset(rs)

beq rs,rt,offset

Figure 9.4: A State Transition Diagram for a Processor Control Unit for the Single-Bus
Data Path of Figure 9.1

Having developed a state transition diagram to interpret ML instructions, the next step
is to design a hardware unit that implements this state transition diagram. Figure 9.5 gives
a high-level block diagram of a generic processor control unit FSM. This FSM performs two
functions:

9.4. RTL Control Unit: An Interpreter for ML Programs

379

(cid:15) Sequencing through the control unit states

(cid:15) Generating the microinstruction for the current state

The left side of the (cid:12)gure handles the sequencing part, and the right side of the (cid:12)gure
handles the microinstruction generation part.

opcode

func

I
R

addu
lw
beq

Instruction
Decoder

Next State
Generator

done

Control Unit

Clock

S
t
a
t
e

MIR

Microinstruction
Generator

Sequencing through Microprogram

Generating the Microinstruction

Micro−operations for Data Path

Figure 9.5: A Block Diagram of a Processor Control Unit

The FSM’s state is stored in a state register. The microinstruction generator block
takes the contents of state as input and generates the corresponding microinstruction,
which can be optionally latched onto a MIR (microinstruction register). Please be careful
not to confuse state with PC; MIR with IR; and microinstructions with ML instructions.

The next state value is calculated by a combinational block called next state generator,
based on the current state and some inputs from the data path. Normally, the state value
is incremented at each clock pulse, causing successive microinstructions to be generated.
The following 3 events cause a deviation from this straightline sequencing.

(cid:15) When the current state corresponds to the end of the fetch microroutine (state 3 in our
example), a new ML instruction has just been decoded. The next state generator
has to deviate from straightline sequencing so as to start generating the execute-
microroutine corresponding to the newly decoded ML instruction. It determines the

380

Chapter 9. Register Tranfer Level Architecture

next state based on the output of the instruction decoder. Thus, the control unit is
able to generate the microinstructions for executing the newly decoded ML instruction.

(cid:15) Similarly, when the current state corresponds to the end of an execute-microroutine,
the next state generator block deviates from straightline sequencing, and initial-
izes state to zero so as to start fetching the next ML instruction.

(cid:15) Finally, some microinstructions require multiple clock cycles to complete execution.
For instance, a memory read microinstruction (written symbolically in RTL as M[MAR]
! MDR) may not be completed in a single clock cycle. Worse still, the completion time
of this microinstruction may not even be deterministic due to a variety of reasons,
as we will see later in this chapter. The easiest approach to handle state updation
when executing a multi-cycle microinstruction is to maintain the same state value
until the microinstruction is completed. For microinstructions with non-deterministic
execution latencies, information regarding their completion can be conveyed by the
data path to the control unit using a signal called done.

Thus, the updation of the state register in the control unit is performed by the next state
generator block, based on:

1. the contents of the state register

2. the outputs of the instruction decoder

3. the done signal from the data path.

9.4.3

Incorporating Sequencing Information in the Microinstruction

The next state generator block can be simpli(cid:12)ed by including in each microinstruction
the necessary sequencing information. That is, the goto operations of Table 9.8 can also be
included in the microinstruction (in an encoded form). With such an arrangement, when
a microinstruction is generated, if it contains a goto operation, this information can be
fed back to the next state generator block, as shown in Figure 9.6. For instance, if
the current microinstruction includes the encoded form of goto Sn, this information (along
with the instruction decoder output) will be used by next state generator to deviate from
straightline sequencing. Similarly, while sequencing through an execute-microroutine, if the
current microinstruction includes the binary equivalent of goto S0, next state generator
resets the state register to zero, so that in the next cycle the processor can begin fetching
the next ML instruction. Notice that the microinstruction generator block becomes
more complex.

At the digital logic level, the sequencing part of the control unit FSM can be implemented
using a sequencer plus a decoder or one (cid:13)ip-(cid:13)op per state. The sequencer can be built in
many ways. One possibility is to use a counter or shift register with synchronous reset

9.4. RTL Control Unit: An Interpreter for ML Programs

381

Clock

Control Unit

Next State
Generator
(Simpler)

S
t
a
t
e

Microinstruction
Generator
(More Complex)

opcode

I
R

func

addu
lw
beq

Instruction
Decoder

done

goto S*

MIR

Micro−operations for Data Path

Figure 9.6: A Block Diagram of a Processor Control Unit that encodes Sequencing Infor-
mation in the Microcode to Simplify the Next State Generator Block

and parallel loading facility. The microinstruction generator can also be built in more
than one way. Two common methods involve the use of either discrete logic or ROM. The
control units so designed are called hardwired control and microprogrammed control,
respectively. We will discuss both of these approaches in Chapter 10.

9.4.4 State Reduction

The state diagram of the control unit we just designed incorporates a separate microroutine
for the execution part of each ML instruction. For an ISA that speci(cid:12)es hundreds of instruc-
tions, this approach is likely to produce an FSM with thousands of states. Many of the states
in such an FSM can actually be combined by considering the fact that the execution micro-
routines of similar instructions have many equivalent states. For instance, the (cid:12)rst 3 states
in the execution microroutine of a memory-referencing instruction deal with computing the
e(cid:11)ective address, and will be the same for all memory-referencing instructions. Just like we
used a common microroutine for the fetch phase of all ML instructions, we can use a com-
mon address calculation microroutine for all memory-referencing instructions in the MIPS
ISA. If we take this approach, we can get a reduced FSM as shown in Figure 9.7. This FSM
has far fewer states than the one given earlier, allowing a much smaller Microinstruction
Generator to be used. Notice, however, that we will have more goto operations, which

18

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 46, NO. 1, JANUARY 2011

A Power-Efﬁcient 32 bit ARM Processor
Using Timing-Error Detection and Correction
for Transient-Error Tolerance and Adaptation
to PVT Variation
David Bull, Shidhartha Das, Member, IEEE, Karthik Shivashankar, Ganesh S. Dasika, Student Member, IEEE,
Krisztian Flautner, Member, IEEE, and David Blaauw, Senior Member, IEEE

Abstract—Razor is a hybrid technique for dynamic detection
and correction of timing errors. A combination of error detecting
circuits and micro-architectural recovery mechanisms creates
a system that is robust in the face of timing errors, and can be
tuned to an efﬁcient operating point by dynamically eliminating
unused timing margins. Savings from margin reclamation can
be realized as per device power-efﬁciency improvement, or para-
metric yield improvement for a batch of devices. In this paper, we
apply Razor to a 32 bit ARM processor with a micro-architecture
design that has balanced pipeline stages with critical memory
access and clock-gating enable paths. The design is fabricated on
a UMC 65 nm process, using industry standard EDA tools, with a
worst-case STA signoff of 724 MHz. Based on measurements on
87 samples from split-lots, we obtain 52% power reduction for
the overall distribution at 1 GHz operation. We present error rate
driven dynamic voltage and frequency scaling schemes where run-
time adaptation to PVT variations and tolerance of fast transients
is demonstrated. All Razor cells are augmented with a sticky error
history bit, allowing precise diagnosis of timing errors over the
execution of test vectors. We show potential for parametric yield
improvement through energy-efﬁcient operation using Razor.
Index Terms—Adaptive design, dynamic voltage and frequency
scaling, energy-efﬁcient circuits, parametric yield, variation
tolerance.

I. INTRODUCTION
I NTEGRATED circuits within microprocessors are oper-
ated with sufﬁcient margins to mitigate the impact of rising
variations at advanced process nodes. Margins are required to
cope with process variation, power delivery network limitations
[16]–[18],
temperature ﬂuctuations [17],
lifetime degrada-
tion [13], [14], signal integrity effects and clock uncertainty.
Inaccuracies in transistor models and EDA tools combined
with measurement tolerances on the tester also contribute to

Manuscript received May 13, 2010; revised July 21, 2010; accepted
September 12, 2010. Date of publication November 18, 2010; date of current
version December 27, 2010. This paper was approved by Guest Editor Kazu-
tami Arimoto.
D. Bull, S. Das, K. Shivashankar, and K. Flautner are with ARM Inc.,
Cambridge CB1 9NJ, U.K.
(e-mail: dbull@arm.com,
sdas@arm.com,
karthik.shivashankar@arm.com, krisztian.ﬂautner@arm.com).
G. S. Dasika and D. Blaauw are with the University of Michigan, Ann Arbor,
MI 48109 USA (e-mail: gdasika@eecs.umich.edu, blaauw@umich.edu).
Color versions of one or more of the ﬁgures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/JSSC.2010.2079410

the overall level of uncertainty, and consequently drive up
the margin requirements further still. While margins exist for
the entire duration of the processor lifetime, they are only
required for the worst-case combination of conditions that
occur extremely rarely, if at all, in practice. Excess margins
are essentially overheads that adversely impact both power and
performance. Reducing excess margins is clearly beneﬁcial,
but this is both expensive and difﬁcult without compromising
on design integrity.
Table I classiﬁes the various sources of variations according
to their spatial reach and temporal rate-of-change. Based on
their spatial reach, variations can be global or local in extent.
Global variations affect all transistors on die such as inter-die
process variations and ambient temperature ﬂuctuations. In
contrast, local variations affect transistors that are in the im-
mediate vicinity of one another. Examples of local variations
are intra-die process variations, local resistive (IR) drops in the
power-grid and localized temperature hot-spots.
Based on their rate-of-change with time, variations can be
classiﬁed as being static or dynamic. Static variations are essen-
tially ﬁxed after fabrication such as process variations, or man-
ifest extremely slowly over processor lifetime such as ageing
effects [13], [14]. Dynamic variations affect processor perfor-
mance at runtime. Slow-changing variations such as tempera-
ture hot-spots and board-parasitics induced regulator ripple have
kilo-hertz time constants. Fast-changing variations such as in-
ductive undershoots in the supply voltage can develop over a
few processor cycles [16], [18]. The rate and the duration of
these Ldi/dt droops is a function of package inductance and
the on-chip decoupling capacitance. Coupling noise and phase-
locked loop (PLL) jitter are examples of local and extremely fast
dynamic variations with duration less than a clock-cycle.
Traditional adaptive techniques [9]–[12], [16]–[24] based on
canary or tracking circuits can compensate for certain manifes-
tations of PVT variations that are global and slow-changing.
These circuits are used to tune the processor voltage and fre-
quency taking advantage of available slack. Tuning is limited to
the point where delay measurements through the tracking cir-
cuits predict imminent processor failure. These circuits are lim-
ited by measurement uncertainty, the degree to which current
and future events correlate and the latency of adaptation. Sub-
stantial margining for fast moving or localized events, such as
Ldi/dt, local IR drop, capacitive coupling, or PLL jitter must

0018-9200/$26.00 © 2010 IEEE

BULL et al.: POWER-EFFICIENT 32 bit ARM PROCESSOR USING TIMING-ERROR DETECTION AND CORRECTION

19

also be present to prevent potential critical path failures. These
types of events are often transient, and while the pathological
case of all occurring simultaneously is extremely unlikely in a
real system, it is impossible to rule this out. Tracking circuits
also incur signiﬁcant calibration overhead on the tester to en-
sure critical path coverage over a wide range of voltage and
temperature conditions. The delay impact of local variations and
fast-moving transients worsens at advanced process nodes due
to aggressive minimum feature lengths and high levels of inte-
gration. This undermines the efﬁcacy of tracking circuits.
Razor [1]–[4] is a hybrid technique that addresses the impact
of excess margins through dynamic detection and correction of
timing errors. Razor exploits the key observation that worst-case
variations occur extremely rarely in practice, by speculatively
operating the processor without the full timing margins. Timing
speculation incurs the risk of infrequent errors due to dynamic
variations. Such errors are detected using speciﬁc circuits that
explicitly check for late-arriving transitions at critical path end-
points, within a detection window around the rising clock-edge.
The detection window is deﬁned relative to the setup time, and
is sufﬁcient to detect transitions that occur either in or past the
setup window.
Error detection can be done either by comparing two dis-
crete samples [1], [2] or by using explicit Transition-Detector
circuits that monitor throughout the detection window [3]–[6].
Both techniques introduce a minimum-delay constraint required
to disambiguate between early transitions from the current cycle
and late-transitions from the previous. This constraint is met by
inserting delay-buffers on short-paths that intersect critical paths
being monitored for timing errors. Error correction is performed
by the system using either stall mechanisms with corrected data
substitution [1], [2], or by instruction/transaction-replay [3]–[6].
A combination of in situ error-detecting circuits and micro-ar-
chitectural recovery mechanisms creates a system that is robust
in the face of timing errors.
Timing-error tolerance enables a Razor system to survive
both local and fast-moving transient events, and adapt itself
to the prevailing conditions, allowing excess margins to be
reclaimed. Savings from margin reclamation can be realized
as a per device power-efﬁciency improvement, or parametric
yield improvement for a batch of devices. Improved power-ef-
ﬁciency results in a higher frequency of operation at the same
supply voltage, without incurring the power impact of voltage
overdrive. Alternatively, the same frequency of operation can
also be sustained at a lower voltage. This leads to quadratic
savings in dynamic power and exponential savings in leakage
due to reduced short-channel effects (SCE).
Measurements performed on a simpliﬁed Alpha pipeline [3],
[4] showed 33% energy savings by scaling the supply voltage
to the point of ﬁrst failure (PoFF) at extremely low error
rates. In [5], the authors evaluated error-detection circuits on a
3-stage pipeline imitating a microprocessor, using artiﬁcially
induced voltage droops and obtained 32% throughput gain at
same supply voltage (VDD), or 17% VDD reduction at equal
throughput. The authors extended this work to an open-RISC
microprocessor core in [6] where in situ error-detecting sequen-
tials (EDS) [5], [6] and Tunable Replica Circuits [7] are used
in conjunction with micro-architectural recovery support to

achieve 41% throughput gain at equal energy or a 22% energy
reduction at equal throughput.
In this paper, we apply Razor to an ARM-based processor
that has timing paths representative of an industrial design,
running at frequencies over 1 GHz, where fast-moving and
transient timing-related events are signiﬁcant. The processor
implements a subset of the ARM instruction set architecture
(ISA) and is fabricated on a UMC [15] 65 nm process, using
industry standard EDA tools, with a worst case static timing
analysis (STA) signoff of 724 MHz. Silicon measurements on
87 samples, including split lots, show a 52% power reduction of
the overall distribution for 1 GHz operation. Error-rate driven
dynamic voltage (DVS) and frequency scaling (DFS) schemes
have been evaluated.
This work extends our previous research presented in [1]–[4]
with the following innovations. (a) The micro-architecture is
designed with explicitly balanced pipeline stages resulting in
critical memory access and clock-gating enable paths, both of
which are monitored using explicit Transition-Detectors. The
micro-architecture responds to all timing errors by ﬂushing the
pipeline and re-executing from the next un-committed instruc-
tion. (b) A Transition-Detector design is presented with sig-
niﬁcantly reduced minimum-delay overhead. This design, de-
scribed in Section II, operates with traditional 50% duty-cycle
clocking and can be easily integrated in a traditional ASIC de-
sign ﬂow. (c) All Razor standard-cells are augmented with a
sticky error history bit that allows precise diagnosis of critical
path timing failures over the course of execution of test-pro-
grams. (d) Parametric yield improvement through energy-efﬁ-
cient operation using Razor is demonstrated based on measure-
ments from the test samples.
The remainder of the paper is organized as follows. Section II
describes the design of the transition-detector that ﬂags late-
transitions at critical path endpoints. The micro-architectural de-
sign of the processor is described in Section III. We provide the
chip implementation details in Section IV. Silicon results from
dynamic voltage and frequency-scaling experiments are pre-
sented in Section V. Section VI deals with the total energy sav-
ings using Razor. Section VII evaluates the potential for para-
metric yield improvement using Razor-based per chip tuning.
Finally, we summarize this paper in Section VIII and present
concluding remarks.

II. TRANSITION-DETECTOR CIRCUIT DESIGN
Fig. 1 shows the design of the Transition-Detector augmented
to a rising-edge triggered master-slave ﬂip-ﬂop. We use a similar
design of the Transition-Detector to monitor critical memory
access paths and clock-gating enables. The Transition-Detector
ﬂags late-arriving transitions at the monitored net by generating
a pulse in response to the transition and capturing it within a
clock-pulse, generated from the rising-edge of the clock (CK).
The Transition-Detector
incorporates
two conventional
pulse-generators for both rising and falling transitions on the
D input. The pulse-generators use skewed devices sized such
that the rising transition of the output pulse is favoured over
the falling, thereby generating a wide pulse at the output of the
pulse-generator. The width of the data-pulse is determined by
the sizing of the pMOS transistors in the p-skewed inverters

20

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 46, NO. 1, JANUARY 2011

TABLE I
CLASSIFICATION OF VARIATIONS

(with minimum-sized NMOS) and the nMOS transistors in
the n-skewed NAND gates (with minimum-sized PMOS). The
delay chain on the internal clock network deﬁnes an implicit
clock pulse that is active when transistors N1 (enabled by CK)
and N2 (enabled by nCK, the delayed and inverted version
of CK) are both ON. The data-pulse can be captured when
the clock-pulse is active by discharging the dynamic node,
DYN, thereby ﬂagging the ERROR signal. The ERN signal is
generated during pipeline recovery initiated in response to the
ERROR signal being ﬂagged. It resets the Transition-Detector
by precharging the dynamic node, DYN, and enabling it to
capture subsequent timing errors. Thus, DYN is conditionally
precharged only in the event of a timing error.
An additional RS-latch structure acts as a sticky error history
(EHIST) bit that is set whenever an error occurs. The EHIST
information is extremely useful for ofﬂine diagnostics since
reading out the EHIST information allows precise identiﬁcation
of each Transition-Detector that triggered over the course of a
test. The EHIST bit adds an additional 10% area and leakage
overhead to the Transition-Detector. However, the diagnostic
capability of the EHIST bit is required only during the initial
development phase of a design and can be excluded in a pro-
duction design.
Fig. 2 shows the conceptual timing diagrams that explain the
principle of operation of the Transition-Detector. The implicit
clock-pulse is active in the interval between the rising edge of
CK and the falling edge of nCK. As mentioned previously, the
and the width of the data-pulse
width of the clock-pulse
are determined by the internal clock-network delay and the
sizing of the pulse generators, respectively. Fig. 2(a) shows the
effective error-detection window. The error-detection window
begins (ends) when the trailing (leading) edge of the data-pulse
overlaps with the leading (trailing) edge of the clock-pulse for
duration greater than the minimum overlap (Tov) required for
evaluating the dynamic node, DYN. Thus, the total error-detec-
is the aggregate of
tion window width
the data-pulse and the clock-pulse widths after adjusting for the
minimum overlap required at the leading and the trailing edges.
The detection-window is ﬁxed after design and needs to be
adequate such that the delay-impact due to fast-moving phe-
nomena can be detected and recovered from. Typically, the de-
vice widths in the pulse-generators are sized so as to minimize
the total power overhead of detection while allowing sufﬁcient

detection-window width. From simulation results, on the pro-
cessor described in this paper, the generation of the error-de-
tection window resulted in the total power overhead due to the
Transition-Detectors to be 5.7% of the overall processor power
at the typical corner (TT/1.0V/85C).
In order that metastability in the main ﬂip-ﬂop is suitably de-
tected and ﬂagged, the error-detection window needs to cover
the setup window of the main ﬂip-ﬂop with sufﬁcient margin,
across PVT corners. Setup coverage is ensured by appropriately
sizing the pulse-generators for a sufﬁciently wide data-pulse.
Due to the added margin on the setup window, early transi-
tions on the D input are now ﬂagged as errors, even before
they cause actual setup violations and state-upsets in the main
ﬂip-ﬂop. This difference between the onset of the setup window
and error-detection window, shown in Fig. 2(b), is a measure
that is inherent in this design.
of the setup pessimism
of
was measured on silicon to be
This pessimism
the cycle time for 1 GHz operation, compared to the actual fre-
quency where incorrect state starts to be latched.
“Q” can become metastable when the input “D” transitions in
the setup window (the onset of which is marked by point B in
Fig. 2(b)). However, this is reliably detected and ﬂagged by the
Transition-Detector since the error-detection window subsumes
the setup window by design. The ERROR output of the Transi-
tion-Detector can become metastable due to a partial discharge
of the node, DYN, at the onset of the error-detection window
(marked by point A in Fig. 2(b)). However, since this occurs
before the main ﬂip-ﬂop setup window, the output “Q” is guar-
anteed to transition to its correct state without any impact on its
timing. Thus, metastability at the ERROR signal does not cause
state corruption within the pipeline.
Although extremely unlikely, it is possible that a metastable
ERROR output can potentially propagate to the pipeline re-
covery circuit. We address this in the conventional manner by
ensuring that the ERROR signals are eventually double-latched
within the pipeline before being processed by the recovery
circuit. This is subsequently discussed in greater detail in
Section III along with the micro-architectural description of the
design.
The Transition-Detector imposes a minimum-delay con-
straint to prevent early transitions from being ﬂagged as errors.
The portion of the error-detection window that exists after the
clock-edge determines the minimum-delay constraint. For this

BULL et al.: POWER-EFFICIENT 32 bit ARM PROCESSOR USING TIMING-ERROR DETECTION AND CORRECTION

21

Fig. 1. Transition-Detector circuit schematic.

Fig. 2. Conceptual timing diagrams illustrating Transition-Detector operation a) Error-detection window is a function of the data-pulse and clock-pulse widths b)
Flagging of early transition incurs a setup pessimism. c) Minimum-delay overhead is less than the clock-pulse width.

design of the Transition-Detector, the minimum-delay con-
straint is equivalent to the clock-pulse width after adjusting for
, as shown in Fig. 2(c).
the DYN evaluation delay, or
and
in
During design time, it is possible to adjust
order to trade-off performance penalty due to setup pessimism
) for reduced minimum-delay constraint
(determined by
). The minimum-delay constraint is met by
(determined by
the insertion of delay buffers on all short-paths that intersect
with a critical path being monitored. This constraint for the
Transition-Detector is expected to be signiﬁcantly less than the
high-phase of the clock used in previous designs [1]–[7]. For
our processor, the power overhead of the delay buffers required
to meet this constraint was 1.3% of the overall processor power
at the typical corner (from simulation).
Using the high-phase of the clock as the error-detection
window [1]–[7] requires a constant high-phase duration to be
maintained to prevent minimum-delay violations. This requires

the generation and distribution of an asymmetric duty-cycle
clock. Integrating the clock-pulse generator within the Tran-
sition-Detector precludes the need for phase truncation and
conventional 50% duty-cycle clocking can be used. This makes
the Transition-Detector easier to integrate in a conventional
ASIC ﬂow.

III. MICRO-ARCHITECTURE DESIGN

The core micro-architecture is shown in Fig. 3. It is a conven-
tional 6-stage in-order pipeline with fetch (FE), decode (DE),
issue (IS), execute (EX), memory (MEM) and write-back (WB)
stages. All the pipeline stages are explicitly balanced due to
a combination of up-front micro-architecture design and path-
equalization performed by back-end physical implementation
tools, such that all stages have critical path endpoints of sim-
ilar latency. The pipeline incorporates forwarding and interlock

22

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 46, NO. 1, JANUARY 2011

Fig. 3. Pipeline diagram of the ARM-based processor showing error-detecting Transition-Detectors and recovery control.

logic resulting in additional fanin to both dataplane and control
paths.
Tightly-Coupled instruction and data memories (IRAM and
DRAM), 2 KB each, hold 512 instruction and data words,
respectively. As in commercial ARM microprocessor designs,
the instruction and data-memory access paths are critical.
Transition-detectors monitor the RAM interfaces and ﬂag
timing violations at the address and the chip-select pins during
critical loads or instruction fetches. DRAM write accesses are
required to be non-critical and this is guaranteed by suitably
buffering store data, which eventually gets written into memory
after Razor validation. Pipeline registers are aggressively
clock-gated for low-power operation. Integrated Clock-gating
Cells (ICGs) with critical enables are also augmented with
Transition-Detectors.
The ERROR signals of individual stages are OR-ed together
and registered to generate the stage error signal. This is then
OR-ed with the ERROR signals from the subsequent stages
and so on. The composite pipeline ERROR signal (Fig. 3) is
double-latched to mitigate against potential metastability. Con-
sequently, all instruction commits have to be postponed by two
extra stabilization stages, S0 and S1, to budget for this synchro-
nization overhead. Forwarding paths from S0 and S1 prevent
pipeline interlocks and hence there is no Instruction Per Cycle
(IPC) degradation due to these extra stages. From simulations
performed under typical usage conditions, the power overhead
due to S0 and S1 was 2.4% of the total processor power.
When an error is detected, the entire pipeline is ﬂushed and
the next un-committed instruction is replayed. Replay occurs at
half-frequency such that a failing instruction does not incur re-
peated timing errors, thereby maintaining forward progress in
the pipeline. Micro-architectural replay is a conventional tech-
nique that often already exists in high-performance pipelines

to support speculative execution such as out-of-order execution
and branch-prediction. Therefore, it is possible to extend pre-ex-
isting recovery framework to support Razor timing speculation.

IV. CHIP IMPLEMENTATION DETAILS
Fig. 4 shows the die photograph of the processor. The pro-
cessor implementation details are provided in Table II. The de-
sign is fabricated in UMC65SP [15] high-performance process
with 1 V nominal supply voltage and 1.1 V as the overdrive
limit. The STA sign-off frequency was 724 MHz measured at
the worst-case corner (SS/0.9 V/125 C) where margins are bud-
geted for 10% voltage droop, slow silicon and temperature ef-
fects. We tested and measured 87 die from split-lots silicon with
30 samples from the fast (FF) lot, 37 from the typical (TT) and
20 samples from the slow (SS) lots, respectively. The proto-
type Razor processor is hosted on an ARM CPU sub-system
as a memory-mapped peripheral. The ARM CPU is used as a
test-harness for downloading code into the instruction memory
through an APB [26] bus interconnect. The execution output
from the general-purpose Register File (Fig. 3) and the Data
Memory (DRAM) is then read-out at the end of every test and
compared with a golden result set for correctness. The processor
implements error rate driven dynamic frequency and voltage
control (described in Sections V-A and V-B).
Out of a total of 2976 registers in the design, the top 503
most critical registers were augmented with a Transition-De-
tector for timing-error detection. This represents approximately
17% of the total ﬂip-ﬂops in the design. There are 149 ICG cells
in the design, of which 27 have Transition-Detector protection.
The Address and the Chip-Select pins of both the instruction
and data memories are monitored using Transition-Detectors.
In total, the design incorporates 550 Transition-Detectors. The
timing-critical endpoints are chosen after timing analysis on a

BULL et al.: POWER-EFFICIENT 32 bit ARM PROCESSOR USING TIMING-ERROR DETECTION AND CORRECTION

23

Fig. 4. Die photograph of the ARM-based Razor processor.

TABLE II
PROCESSOR IMPLEMENTATION DETAILS

placed-and-routed design at the slow corner. After identifying
the critical path endpoints, the netlist is again taken through the
implementation ﬂow. The ﬁnal design is then veriﬁed at mul-
tiple PVT corners to ensure that critical endpoints are always
protected by Transition-Detectors across all corners.
A critical concern during implementation is that the design
ﬂow does not result in additional critical endpoints. Otherwise,
the timing perturbation due to the incremental insertion of Tran-
sition-Detectors may lead to more timing endpoints to become
critical, thus impacting design closure. We avoid multiple im-
plementation iterations by imposing extra timing constraints on
the non-critical endpoints during logic optimization and place-
and-route. This ensures that the original list of critical paths is
preserved and design closure is achieved.
Table II shows the total power and area overhead of Razor
error detection and correction circuitry. From simulation results
at the typical corner (TT/1 V/85 C), the total overhead of the
550 Transition-Detectors from simulation results was 5.7% with
1.3% overhead due to the delay buffers required to meet the
minimum-delay constraint. The stabilization stages (S0 and S1)

Fig. 5. Throughput versus frequency characteristics for the Typical workload at
1 V VDD on device TT9. Number of failing Transition-Detectors is also plotted
against the secondary axis.

consume additional 2.4% power. Thus, the total power over-
head due to Razor was 9.4% of the baseline processor. The
combined Razor area overhead due to the Transition-Detectors,
minimum-delay buffers and the stabilization stages was 6.9% of
the total area, assuming 70% row utilization. Based on silicon
measurements, the setup pessimism (Section II) of the Tran-
sition-Detectors was measured to be 5% of the cycle time for
1 GHz operation at 1 V nominal supply voltage.

V. SILICON MEASUREMENT RESULTS
Fig. 5 shows the throughput versus frequency characteristics
for a test-program executed on device TT9 at 1 V nominal VDD.
This program (referred to as the Typical workload) computes
the prime-factor decomposition of an array of integers and rep-
resents typical usage conditions. The throughput measured at
each frequency point is normalized against the throughput at the
sign-off frequency of 724 MHz. When execution completes, the
EHIST information of individual Transition-Detectors is read
out. The number of Transition-Detectors incurring timing errors
is plotted as a function of frequency, against the secondary axis
on Fig. 5.
In the absence of timing errors, the throughput increases
linearly with frequency until the Point of First Failure (PoFF)
at 1.1 GHz, a 50% throughput increase compared to the design
point of 724 MHz. At the PoFF, there are four Transition-
Detectors that incur timing errors. Thereafter, multiple failing
Transition-Detectors contribute to a rapidly rising error rate
due to the balanced nature of the pipeline. A combination of
the rising error rate and the IPC overhead of recovery cause
exponential degradation in the throughput. Consequently, it is
desirable to limit operation to low-error rate regimes where the
maximum beneﬁts of energy-efﬁciency due to margin elimina-
tion can be claimed. Execution is correct until 1.6 GHz, after
which recovery fails. This enables a safety margin of 500 MHz
beyond the PoFF where the computation is still correct, albeit
at an exponential loss in efﬁciency.
Fig. 6 shows the portion of the layout screenshot of the
processor annotated with a map of failing Transition-Detectors
(represented by black rectangles) for test programs executed

24

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 46, NO. 1, JANUARY 2011

Fig. 6. Map of failing Transition-Detectors on chip TT9 at 1 V VDD: a) shows 4 failing Transition-Detectors for the Typical workload at 1.1 GHz. b) At 1.2 GHz,
122 Transition-Detectors incur timing failures indicating an increase in error rate. c) At 1.1 GHz, Power Virus workload has 249 failing endpoints compared to 4
for Typical.

on device TT9 at 1 V nominal VDD. Fig. 6(a) and (b) compares
the failure map for the Typical workload at the PoFF (1.1 GHz)
against that at 1.2 GHz. At 1.1 GHz, the 4 failing Transition-De-
tectors are in ID, EX and MEM stage buses, respectively. At
1.2 GHz, 122 Transition-Detectors fail timing. The failure map
is dominated by the Transition-Detectors in the Instruction De-
code bus located at the lower left-hand corner of the screenshot.
Fig. 6(a) and (c) compares the failure map for the Typical
workload against that for a synthetic Power Virus workload,
executed at the same operating point (1 V/1.1 GHz). The Power
Virus workload is a loop of compute-intensive instructions
that induces maximum on-chip activity leading to worst-case
voltage droops (both IR and Ldi/dt) in the power grid, while
exercising the worst-case STA critical path. A combination
of worst-case critical path sensitization and supply noise
conditions causes 249 Transition-Detectors to incur timing
errors compared to just 4 failures for the Typical workload.
Furthermore, the failure map for the Power Virus workload is
dominated by the EX stage bus located in the top right corner
of the screenshot. Thus, there exists a signiﬁcant variation in
timing characteristics across workloads due to different critical
paths being sensitized under varying voltage, temperature and
noise conditions.
The failure maps in Fig. 6 illustrate the observability of
critical path endpoints enabled by the EHIST information from
the Transition-Detectors. This signiﬁcantly enhances silicon
testability since failing test-vectors can be precisely diagnosed
by identifying critical path endpoints that
incurred timing
violations.

A. Razor-Based Dynamic Frequency Scaling
The adaptive frequency controller (AFC) (Fig. 4) exploits
dynamic workload variations by tuning operating frequency in
response to monitored error rates. In the adaptive mode, the pro-
cessor clock (FCLK) is sourced from a 31-tap ring oscillator in-
stead of the on-chip PLL. Coarse-grained frequency tuning is
achieved by changing the ring-oscillator tap setting. Vernier-
tuning is achieved in 24 MHz steps using a switched-capac-
itor ladder network. Hazard-free frequency adjustments occur
during dynamic processor operation by constraining all clock-
source and frequency selections to occur in the negative phase.

The pipeline error signal is double-latched to mitigate against
potential metastability (Fig. 3) and accumulated in a 10 bit error
register. During recovery, every alternate cycle is skipped such
that the operating frequency is effectively halved, thus ensuring
guaranteed forward progress within the pipeline. The frequency
control algorithm is implemented in hardware and is externally
programmable.
Fig. 7 shows the AFC response for a workload with three dis-
tinct phases consisting of loops of NOP, power virus and typical
workloads running on device TT9 at ﬁxed 1 V supply voltage.
The AFC is programmed to reduce the operating frequency by
24 MHz for every cycle where a timing error is detected. The fre-
quency is incremented by 24 MHz for every 1024 processor-cy-
cles without timing errors.
The highest frequency is measured in the NOP phase
(1.23 GHz). This is expected since the instruction mix is
heavily dominated by lightweight NOP instructions that gen-
erate minimal switching activity within the pipeline. The most
critical computations executed in the NOP phase are the address
calculations for the branch instructions at the loop boundaries.
When the workload transitions from the NOP to the Power
Virus phase, the processor is able to survive this abrupt sensi-
tization of worst-case critical paths, although the instantaneous
throughput is impacted due to extremely high error rates. The
AFC responds to the high error rate conditions by reducing
the frequency in 24 MHz steps until the error rate stabilizes at
approximately 1 GHz. Thus, the lowest frequency levels are
measured in the Power Virus phase.
In the Typical phase, the AFC output shows 4 distinct fre-
quencies between 1143 MHz and 1068 MHz, compared to just
one for both the NOP and the Power Virus phases. This is due
to paths of varying lengths being exercised during typical usage
compared to relatively ﬁxed-length paths for the synthetic NOP
and Power Virus loops. The processor is able to sustain a max-
imum of 14% throughput gain for the Typical workload com-
pared to the Power Virus loop.
The AFC response and the failure map experiments clearly
indicate that by reclaiming worst-case margins, the device TT9
is capable of sustaining frequencies in excess of 1 GHz for most
workloads, even though the actual design was signed off at 724
MHz. Hence, for the next Dynamic Voltage Control experiment,

BULL et al.: POWER-EFFICIENT 32 bit ARM PROCESSOR USING TIMING-ERROR DETECTION AND CORRECTION

25

Fig. 7. Dynamic Frequency Scaling: AFC response for a 3-phase workload consisting of the NOP, Power Virus and Typical workloads at 1 V VDD. Frequency is
increased or reduced in 24 MHz steps.

Fig. 8. Architecture of the closed-loop Razor voltage controller: The control algorithm is implemented in software running on an ARM1176. The supply voltage
is set by programming an external regulator using a DAC.

we keep the frequency ﬁxed at 1 GHz and vary the voltage as
dictated by the error rates.

B. Razor-Based Dynamic Voltage Scaling
Fig. 8 shows the architecture of the closed-loop controller im-
plemented for dynamic voltage management based on measured
error rates. The control algorithm is implemented in software on
the ARM CPU that hosts the Razor processor sub-system. The
voltage control decision is based upon the accumulated value of
100 samples of the on-chip error register, accessed through the
APB bus interface. The supply voltage is adjusted by program-
ming an external DC-DC regulator. The DC-DC regulator can
source 800 mA current that is sufﬁcient for the requirements of
the Razor processor with maximum current consumption less
than 150 mA. The response latency of the voltage control loop
is measured to be 55 us.
The voltage controller response on device TT9 is shown in
Fig. 9, for a three-phase program with loops of the NOP, Power
Virus and Typical workloads, running at ﬁxed 1 GHz frequency.
The error rate for device TT9 is plotted against the secondary
axis. The error rate is initially zero in the NOP phase since the
supply voltage is higher than the PoFF for the relatively light-
weight NOP instructions. The controller responds to the zero
error rate by reducing the supply voltage to 0.92 V for device
TT9, where infrequent timing errors occur. During the tran-
sition from the NOP to the Power Virus phase, the processor

experiences a surge in the error rate. The controller responds
to the high error rate by increasing the supply voltage in pro-
portional increments until the steady-state voltage is attained
at 1.07 V. Conversely, the error rate drops to zero during the
transition from the Power Virus to the Typical workload phase.
The steady-state voltage for the Typical workload is achieved at
0.96 V.
The controller response for devices SS6 and FF5 are also
plotted in Fig. 9. Device SS6 is amongst the slowest die out of
the 87 devices while FF5 is amongst the fastest with maximum
standby leakage. Thus, these devices represent the extremes of
the distribution of devices. The steady-state voltage measured
for the NOP, Power Virus and the Typical phases for each de-
vice in Fig. 9 is indicative of its native silicon-grade.
The dynamic voltage and frequency scaling experiments
in Sections V-A and V-B illustrate how Razor maximizes
the energy efﬁciency of the processor by tuning to the most
efﬁcient operating point depending upon speciﬁc workload
requirements. In situ error detection and recovery enables the
Razor processor to maintain correct operation in the presence
of fast-changing dynamic variations and worst-case critical
path sensitization. When dynamic variations persist, the Razor
voltage controller automatically adapts to higher voltage levels
so that low error rates are eventually achieved. In Section VI,
we quantify the energy savings obtainable with Razor-enabled
voltage tuning for 1 GHz operation.

26

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 46, NO. 1, JANUARY 2011

monitors to scale processor supply voltage according to pre-
vailing PVT conditions at runtime. Due to adaptation latency,
such techniques cannot respond in time to fast-changing voltage
droops that manifest during abrupt processor activity changes
(Fig. 9). At the minimum, margining is required to account for
this latency as well as for measurement uncertainties inherent
in the monitoring circuits. For our experiment, we assume
a dynamic adaptive loop where voltage scaling is limited to
the Power Virus voltage. Scaling voltage below this level can
potentially cause incorrect execution if the processor undergoes
a transition to the Power Virus workload. An additional 3%
margin is added to account for measurement uncertainty.
Fig. 10 shows the power consumed for the Typical workload
by the devices SS6, FF5 and TT9 using such a hypothetical
best-case adaptive technique. This is a best-case comparison of
the adaptive approach against Razor since only the minimum
required margins are accounted for. Typically, margins for local
temperature ﬂuctuations and ageing effects are also added which
will then lead to higher power consumption for the adaptive
approach. The device TT9 requires 1.1 V using the best-case
adaptive tuning and consumes 58.7 mW for the Typical work-
load. With Razor, TT9 consumes 40.5 mW that represents a 30%
power saving due to Razor compared to the adaptive approach.
The worst-case power, due to SS6, reduces by 25% using Razor.
Fig. 11 shows the power distribution for the 87 devices with
Razor versus operation at 1.2 V and the best-case adaptive tech-
nique. The power distribution at constant 1.2 V VDD is dom-
inated by the fast and leaky devices and therefore has large
spread (37 mW). In contrast, the power distribution with Razor
has a signiﬁcantly narrower spread (10 mW) due to the equal-
ization effect of a higher PoFF for the slower devices compen-
sating for the higher leakage on the faster devices. The mean of
the power distribution improves by 30 mW using Razor, a net
40% improvement over 1.2 V operation. Compared to best-case
adaptive tuning, the mean of the distribution shifts by 14 mW
(or 24%) when using Razor.
Sustained operation beyond the process overdrive limit of
1.1 V can have potential long-term gate-oxide reliability [14]
and accelerated wear-out implications [13]. In addition, exces-
sive overdrive exacerbates short-channel effects such as Drain
Induced Barrier Lowering (DIBL) [24] leading to exponential
increase in leakage, especially on the fast devices. From reli-
ability and leakage considerations, it is desirable to limit the
voltage overdrive to the process limit of 1.1 V.
SS6 requires at least 1.17 V when executing the worst-case
Power Virus workload at 1 GHz. Hence, limiting the long-term
overdrive operation to 1.1 V would necessarily require SS6 to
be discarded when operating without Razor at 1 GHz frequency.
Consequently, without Razor, operation at 1.1 V most certainly
incurs a parametric yield loss for a frequency target of 1 GHz
due to discarding the slow devices. In Section VII, we analyze
the impact on parametric yield at 1 GHz when the maximum
voltage for sustained, long-term operation is limited to 1.1 V.

VII. PARAMETRIC YIELD IMPROVEMENT USING RAZOR
Any yield improvement technique cannot be quantitatively
demonstrated with a small number of samples, however we can
still illustrate the principle of how Razor can be used to improve

Fig. 9.
Impact of process variations on Razor voltage controller response at 1
GHz frequency: Slowest device, SS6, requires the highest voltage and vice versa
for the fastest device, FF5. SS6 requires 1.17 V for the Power Virus phase.

VI. RAZOR ENERGY SAVINGS

From the Razor voltage controller response in Fig. 9, we
observe that the slowest chip, SS6, requires a minimum voltage
of 1.17 V in order to operate the Power Virus workload at
1 GHz frequency. For all our samples to operate correctly
without Razor, sufﬁcient margin is required to guarantee that
the slowest device (SS6) operates correctly in the worst-case.
Assuming Power Virus is the absolute worse-case code, then
at a bare minimum additional margin must be added for tem-
perature and safety. For 1 GHz operation, this translates to a
worst-case voltage of 1.2 V for 3% margin. Thus, for conven-
tional operation without Razor, the minimum required supply
voltage is 1.2 V such that all die operate correctly at 1 GHz.
Fig. 10 compares the power consumption for Razor-enabled
operation versus conventional operation at 1.2 V when exe-
cuting the Typical workload at 1 GHz frequency for each of
the three devices (FF5, SS6 and TT9). For the 1.2 V operation,
leakage power is a signiﬁcant contributor to the total power for
the fastest device, FF5 (approximately 50%). The slowest de-
vice SS6 consumes the least power at 1.2 V due to low leakage.
Even though the SS6 dynamic power is higher than that for
FF5, the higher contribution of leakage causes FF5 to be the
maximum power outlier for the entire distribution of devices.
With Razor-enabled voltage tuning, all devices operate at
the PoFF for the Typical workload. The lower PoFF for FF5
(0.92 V) compared to that for SS6 (1.07 V) compensates for its
higher leakage, leading to SS6 becoming the power outlier for
the distribution. The maximum power consumption for Typical
workload, considering all 87 devices, reduces from 100 mW
for the baseline 1.2 V operation to 48 mW for operation with
Razor. This represents a net 52% power saving at 1 GHz
operation. On a per chip basis, power consumption on TT9
reduces from 71 mW at 1.2 V to 40.5 mW using Razor, a net
43% power saving due to Razor.
Fig. 10 compares Razor with a hypothetical, best-in-class
adaptive technique. Adaptive techniques can be static where
the supply voltage or the body bias is calibrated, margined
and programmed on the tester [19]–[24]. Dynamic adaptive
techniques [9]–[12], [16]–[18] rely on process and temperature

BULL et al.: POWER-EFFICIENT 32 bit ARM PROCESSOR USING TIMING-ERROR DETECTION AND CORRECTION

27

Fig. 10. Power consumption on devices FF5, TT9 and SS6: Razor is compared against constant 1.2 V VDD and best-case Adaptive-tuning. Razor enables 52%
power saving overall compared to constant 1.2 V operation. Razor enables 30% power saving compared to best-case adaptive tuning for device TT9 and 43%
saving compared to1.2 V operation.

Fig. 11. Power distribution at the worst-case (WC) 1.2 V constant voltage operation versus Razor. Razor improves both the m and the s of the distribution.

the parametric yield for a distribution of devices. Functional
devices are required to meet a targeted frequency speciﬁcation
(Fmax) under a given power budget (Pmax), before they can be
shipped. In the following, we compare the parametric yield ob-
tained using Razor versus that with conventional overdrive op-
eration at constant 1.1 V VDD and an Adaptive Voltage Scaling
(AVS) approach based on an on-chip Ring Oscillator serving as
a process monitor. We have chosen the parametric yield targets
of 1 GHz frequency under 65 mW power consumption for typ-
ical usage conditions.

A. Parametric Yield With Constant 1.1 V Overdrive

The scatter plot in Fig. 12 shows the total power consumption
(dynamic and leakage) as a function of silicon-grade for all de-
vices when executing the Typical workload at the 1.1 V/1 GHz
operating point, without Razor. Operation without Razor re-
quires margins for the worst-case. Assuming Power Virus to be
the worst-case workload, we obtain the maximum frequency of
operation for each die by measuring the Point of First Failure
(PoFF) frequency (with 3% margin added for safety), when ex-
ecuting the Power Virus workload at 1.1 V VDD. Thus, the mea-

Fig. 12. Power at 1 GHz for Typical workload versus silicon-grade measured
by highest frequency for correct operation without Razor at 1.1 V constant VDD.
Measurements obtained on 87 die from split lots. Yield window is shown for
frequency target (Fmax) of 1 GHz and power target (Pmax) of 65 mW.

sured PoFF for the worst-case Power Virus workload represents
a margined frequency point under typical usage conditions.

28

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 46, NO. 1, JANUARY 2011

The device, FF5, sustains the highest frequency (1127 MHz)
for worst-case operation and consumes maximum power due to
high leakage. Devices SS6 and TT13 from the slow and the typ-
ical lots respectively, are the slowest devices from our test sam-
ples and operate the Power Virus workload at 890 MHz. Thus,
the devices follow an expected exponential trend with the fast
devices with high leakage dominating the total power consump-
tion compared to the slower devices.
Fig. 12 shows the parametric yield targets of frequency and
power, labeled as “Fmax” and “Pmax” respectively. Out of 87
devices, there are 7 devices that exceed the 65 mW power cri-
teria and 44 devices that fail the 1 GHz frequency criteria. Thus,
there are 36 yielding devices (or 41% yield) out of a total of 87.

B. Parametric Yield With Adaptive Voltage Scaling (AVS)

AVS techniques [9]–[12], [16]–[21] individually tune the
supply voltage of devices according to their native speed-grade,
based on delay measurements using on-chip process monitors.
Per-device tuning compensates for inter-die process variations.
However, extra margins are still required for fast-moving tran-
sients that are impossible to respond to in time. Such transients
can trigger during abrupt processor transition from low-activity
and non-critical operations to compute-intensive, heavyweight
instructions. Consequently, for safe operation, AVS is required
to be limited to a sufﬁciently margined point. We derive this
safe operating limit based on the failure point for the Power
Virus workload with added margin for safety (3%). In the
absence of dynamic detection and correction of errors, the AVS
technique cannot operate below this voltage due to potential
risk of incorrect execution.
Our AVS measurements use an on-chip Ring Oscillator for es-
timating the worst-case processor delay. We obtain a statistical
correlation function using a linear-ﬁt model that relates the mea-
sured Ring Oscillator frequency at 1 V VDD to the minimum
safe voltage requirement at 1 GHz. Due to the limited number
of test devices, we measure the correlation function using data
from every die. In the general case, a small number of samples
from different global corners of the process distribution could
be used as a training set to generate the correlation function for
the entire distribution of devices.
In our measurements, we add margins to the linear-ﬁt
model only to account for possible under-estimation of the
device voltage from the measured Ring Oscillator frequency.
Discounting margins for temperature and ageing allows an
optimistic comparison of AVS against Razor. Fig. 13 shows
the scatter plot of the PoFF voltage for Power Virus workload
versus the Ring-Oscillator frequency measured at 1 V for die
from the fast (FF), slow (SS) and typical (TT) lots, respectively.
It can be observed that the Ring Oscillator frequency is strongly
correlated with the minimum voltage requirement for each die.
The statistical correlation function for both data sets is com-
puted to be 95.3% for the entire training set of devices. When
measured across separate lots, this correlation is computed to
be 86.2% for the FF lot, 85.1% for the SS lot and 89.1% for
the TT lot, respectively. Due to the high correlation measured
across global process corners, the Ring Oscillator frequency
can be used to set the supply voltage for individual devices.

Fig. 13. Scatter-plot of the Power Virus PoFF (with 3% margin for safety) at
1 GHz versus measured Ring Oscillator frequency at 1.0 V: Ring-Oscillator fre-
quency shows signiﬁcant correlation (95%) with the minimum safe voltage, thus
showing accurate tracking across global process corners. Extra margin added for
voltage underestimation (36 mV).

Fig. 14. Comparison of Ring-Oscillator based AVS with constant 1.1 V oper-
ation: Scatter-plot of power at 1 GHz running typical code for both techniques
is shown as a function of silicon-grade. The maximum power reduces with AVS
due to voltage scaling on fast chips. However, power increases on slow chips
due to VDD exceeding 1.1 V limit leading to the U-shaped trend.

Fig. 13 shows the margining methodology for the Ring Os-
cillator based AVS. The device TT3 shows the maximum devi-
ation from the linear-ﬁt model, leading to a voltage underesti-
mation of 36 mV. Consequently, this voltage difference has to
be added as extra margin to the model to guarantee that the es-
timated voltage is always greater than the minimum voltage re-
quired for safe operation. This margin (36 mV) represents 3.2%
of the nominal voltage overdrive of 1.1 V.
The scatter plot of Fig. 14 shows the power consumption
of each die using the margined AVS model in Fig. 13 plotted
against
its native silicon-grade (maximum worst-case fre-
quency of operation with 3% margin). The U-shaped trend of
the scatter plot is a consequence of the power reduction on the
faster devices due to lower voltage operation and vice versa for
the slower devices. The maximum power consumption reduces
from 76 mW at constant 1.1 V operation to 68 mW using AVS,
a net 11% reduction in total power.
The voltage on the slow devices using AVS exceeds the 1.1 V
overdrive limit. In addition, the extra 36 mV margin for voltage
underestimation causes some of the typical devices to exceed

BULL et al.: POWER-EFFICIENT 32 bit ARM PROCESSOR USING TIMING-ERROR DETECTION AND CORRECTION

29

Fig. 15. Power versus silicon-grade scatter plot for AVS: Maximum supply
voltage is limited to 1.1 V VDD when the AVS voltage exceeds this limit on the
slow die. VDD tuning on the fast-die reduces the max power for the distribution
which improves the power yield. However, frequency yield is unchanged.

Fig. 16. Parametric yield with Razor compared against that with constant 1.1 V
VDD operation. Frequency uplift through margin reclamation on slow devices
and voltage scaling to PoFF for fast devices enables 100% yield through Razor.

the 1.1 V limit as well. Due to wearout and reliability concerns,
we limit the maximum voltage to the process overdrive limit
of 1.1 V for sustained, long-term operation. As a consequence,
devices incapable of sustaining correct operation at 1.1 V are
now discarded, leading to yield loss.
Fig. 15 shows the power versus silicon-grade scatter plot
where maximum VDD is limited to 1.1 V. AVS leads to lower
power consumption on the fast devices with the maximum
power outlier at 68 mW. Excluding the 2 devices violating the
maximum power constraint and the 44 devices fail the 1-GHz
frequency constraint, there are now 41 yielding devices out of
87, or 47% yield.

C. Parametric Yield With Razor

Fig. 16 shows the power versus silicon-grade scatter plot for
Razor-enabled operation on 87 devices executing the Typical
workload at 1 GHz frequency. The silicon-grade is again repre-
sented by the maximum frequency of operation, sustainable at
constant 1.1 V VDD. Due to the elimination of worst-case mar-
gins using Razor, each device operates at a higher frequency
when executing the Typical workload compared to the worst-
case Power Virus workload. Therefore, the entire scatter plot
shifts to higher frequency values. The slowest device, SS6, can
execute the Typical workload at near zero error rate conditions at
1015 MHz at 1.1 V VDD, thus exceeding the 1 GHz frequency
target. The highest PoFF for the Typical workload is measured
to be 1397 MHz on device, FF76.
The maximum power outlier when using Razor is measured
to be 48 mW which represents a 26% saving over the power
target of 65 mW and a net 37% saving over the worst-case power
(76 mW) at constant 1.1 V operation. Thus, all devices simul-
taneously meet both the power and frequency targets and 100%
yield is achieved. The yield obtained for the 1 GHz/65 mW para-
metric targets using constant 1.1 V operation, AVS and Razor
approaches is summarized in Table III.
A key observation here is that in case of Razor, the slowest
device SS6 executes most workloads below the process limit

TABLE III
SUMMARY OF YIELD OBTAINED USING THE 3 DIFFERENT TECHNIQUES:
CONSTANT 1.1 V OVERDRIVE, AVS AND RAZOR-TUNING FOR 87 TOTAL DIE

of 1.1 V. Thus, for long-term operation the supply voltage is
kept below 1.1 V for all devices, except for extremely rare
use cases equivalent to the pathological worst-case Power
Virus code. This is in contrast with the AVS approach where
operation beyond 1.1 V is sustained on a long-term basis for
the slower devices. Furthermore, safety margins and correlation
uncertainties cause more devices to require greater than 1.1 V
supply in the AVS approach compared to Razor.
For applications where the peak power consumption is a fun-
damental constraint, packaging and thermal limitations can im-
pose absolute restrictions on the supply voltage from exceeding
the 1.1 V VDD limit, even for the Power Virus workload. From
our measurements, there are 22 devices out of 87 that require
supply voltage in excess of 1.1 V for the Power Virus workload
with Razor-enabled operation. Discarding these devices leads
to 65 yielding devices (or 75% yield) when strict limits on the
maximum voltage of operation are applied.

VIII. SUMMARY AND CONCLUSION
In this paper, we presented the design of an ARM-based
microprocessor that uses Razor for energy-efﬁcient operation
through the elimination of timing margins. With Razor-based
voltage tuning, we achieved 52% energy savings at 1 GHz
operation on a distribution of 87 devices from split-lots. We
presented the design of a Transition-Detector with signiﬁcantly
reduced minimum-delay impact. The Transition-Detector relies
on locally generated clock and data-pulses and can operate
using conventional 50% duty-cycle clocking. Thus, it can be
easily integrated into a conventional ASIC design ﬂow.

30

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 46, NO. 1, JANUARY 2011

We demonstrated the operation of dynamic frequency and
voltage controllers that enable runtime adaptation to PVT varia-
tions and tolerance of fast transients through Razor error detec-
tion and recovery. The dynamic frequency controller was im-
plemented in hardware on-chip and relies on a Ring-Oscillator
clock-source to adjust frequency according to monitored error
rates. The voltage controller was implemented in software run-
ning on a separate ARM processor that samples the error reg-
ister through an APB bus interface and adjusts the voltage by
programming an external voltage regulator.
Finally, we demonstrated the potential for parametric yield
improvement using Razor. By trading margins for higher
frequency on the slow devices and lower power on the fast de-
vices, Razor-tuning enables more devices to meet the dual-sided
parametric yield constraints of frequency and power. Further
research is required to develop suitable manufacturing test
methodologies before Razor can be deployed in the ﬁeld. As
process technology scales to ultra-small geometries, Razor
mitigates the impact of rising variations by simultaneously
enabling higher performance at lower power consumption.

ACKNOWLEDGMENT

The authors would like to thank staff at United Microelec-
tronics Corporation (UMC) for providing, integrating, and fab-
ricating the silicon, as well as D. Flynn, S. Idgunji, and J. Biggs
at ARM for developing the “Ulterior” technology demonstrator
chip that hosts the Razor subsystem.

REFERENCES

[1] S. Das et al., “A self-tuning DVS processor using delay-error detection
and correction,” IEEE J. Solid-State Circuits, pp. 792–804, Apr. 2006.
[2] D. Ernst et al., “Razor: A low-power pipeline based on circuit-level
timing speculation,” in Proc. IEEE Micro Int. Symp. Microarchitecture,
Dec. 2003, pp. 7–18.
[3] D. Blaauw et al., “RazorII: in situ error detection and correction for
PVT and SER tolerance,” in IEEE ISSCC Dig. Tech. Papers, Feb. 2008,
pp. 400–401.
[4] S. Das et al., “Razor II: in situ error detection and correction for PVT
and SER tolerance,” IEEE J. Solid-State Circuits, pp. 32–48, Jan. 2009.
[5] K. Bowman et al., “Energy-Efﬁcient and metastability-immune timing-
error detection and instruction replay-based recovery circuits for dy-
namic variation tolerance,” IEEE J. Solid-State Circuits, pp. 49–63,
Jan. 2009.
[6] J. Tschanz et al., “A 45 nm resilient and adaptive microprocessor core
for dynamic variation tolerance,” in IEEE ISSCC Dig. Tech. Papers,
Feb. 2010, pp. 282–283.
[7] J. Tschanz et al., “Tunable replica circuits and adaptive voltage-fre-
quency techniques for dynamic voltage, temperature, and aging varia-
tion tolerance,” in IEEE Symp. VLSI Circuits Dig., Jun. 2009.
[8] D. Bull et al., “A power-efﬁcient ARM ISA processor using timing-
error detection and correction for timing-error tolerance and adaptation
to PVT variations,” in IEEE ISSCC Dig. Tech. Papers, Feb. 2010, pp.
284–285.
[9] A. Drake et al., “A distributed critical path timing monitor for a 65
nm high-performance microprocessor,” IEEE ISSCC Dig. Tech. Pa-
pers, 2007.
[10] J. Tschanz et al., “Adaptive frequency and biasing techniques for tol-
erance to dynamic temperature-voltage variations and aging,” in IEEE
ISSCC Dig. Tech. Papers, Feb. 2007, pp. 292–293.
[11] M. Nakai et al., “Dynamic voltage and frequency management for a
low power embedded microprocessor,” IEEE J. Solid-State Circuits,
pp. 28–35, Jan. 2005.

[12] K. Nowka, “A 32-bit PowerPC system-on-a-chip with support for dy-
namic voltage scaling and dynamic frequency scaling,” IEEE J. Solid-
State Circuits, Nov. 2002.
[13] S. Rangan, N. Mielke, and E. Yeh, “Universal recovery behavior of
negative bias temperature instability,” in IEEE Int. Electron Devices
Meeting Dig., Dec. 2003, pp. 341–341.
[14] A. M. Yassine et al., “Time dependent breakdown of ultrathin gate
oxide,” IEEE Trans. Electron Devices, pp. 1416–1420, Jul. 2000.
[15] UMC United Microelectronics Corporation [Online]. Available: http://
www.umc.com/
[16] T. Fischer et al., “A 90-nm variable frequency clock system for a
power-managed itanium architecture processor,” IEEE J. Solid-State
Circuits, pp. 218–228, Jan. 2006.
[17] R. McGowen et al., “Power and temperature control on a 90-nm ita-
nium family processor,” IEEE J. Solid-State Circuits, pp. 229–237, Jan.
2006.
[18] N. James et al., “Comparison of split- versus connected-core supplies
in the POWER6TM microprocessor,” in IEEE ISSCC Dig. Tech. Pa-
pers, Feb. 2007, pp. 297–298.
[19] J. Kao, M. Miyazaki, and A. R. Chandrakasan, “A 175-mV multiply-
accumulate unit using an adaptive supply voltage and body bias archi-
tecture,” IEEE J. Solid-State Circuits, pp. 1545–1554, Nov. 2002.
[20] J. Tschanz et al., “Adaptive body bias for reducing impacts of die-to-die
and within-die parameter variations on microprocessor frequency and
leakage,” IEEE J. Solid-State Circuits, pp. 1396–1402, Nov. 2002.
[21] J. Tschanz et al., “Effectiveness of adaptive supply voltage and body
bias for reducing impact of parameter variations in low power and
high performance microprocessors,” IEEE J. Solid-State Circuits, pp.
826–829, May 2003.
[22] G. Gammie et al., “A 45 nm 3.5 G baseband-and-Multimedia appli-
cation processor using adaptive body-bias and ultra-low-power tech-
niques,” in IEEE ISSCC Dig. Tech. Papers, Feb. 2008, pp. 258–259.
[23] C. Neau and K. Roy, “Optimal body bias selection for leakage
improvement and process compensation over different technology
generations,” in Proc. Int. Symp. on Low Power Electronic Devices
(ISLPED), Aug. 2003, pp. 116–121.
[24] A. Hokazono et al., “Forward body biasing as a bulk-si CMOS
technology scaling strategy,” IEEE Trans. Electron Devices, pp.
2657–2664, Oct. 2008.
[25] R. Kaur et al., “Uniﬁed subthreshold model for channel-engineered
sub-100-nm advanced MOSFET structures,” IEEE Trans. Electron De-
vices, pp. 2475–2486, Sep. 2007.
[26] AMBA APB Bus Speciﬁcation Reference Manual [Online]. Avail-
able:
http://arminfo/help/topic/com.arm.doc.ihi0024c/IHI0024C_
amba_apb_protocol_v2_0_spec.pdf

David Bull received the B.Sc. degree in computer
science from Royal Holloway College, University of
London, U.K., in 1991.
He is a consultant engineer at ARM Ltd., Cam-
bridge, U.K. He joined ARM in 1995, and spent nine
years working on various aspects of processor de-
velopment including micro-architecture and circuits.
He has worked on the ARM9 and ARM11 processor
families processor, and was the design lead for the
ARM926EJ-S. Since 2004 he has focused on re-
search into advanced circuit and micro-architectural
techniques, and has led the ARM RAZOR research project.

Shidhartha Das (S’03–M’08) received the B.Tech
degree in electrical engineering from the Indian In-
stitute of Technology, Bombay, India, in 2002 and
the M.S. and Ph.D. degrees in computer science and
engineering from the University of Michigan at Ann
Arbor in 2005 and 2009.
His research interests include micro-architectural
and circuit techniques for low-power and variability-
tolerant digital IC design. Currently, he is a Staff En-
gineer working for ARM Ltd., Cambridge, U.K., in
the Research and Development group.

BULL et al.: POWER-EFFICIENT 32 bit ARM PROCESSOR USING TIMING-ERROR DETECTION AND CORRECTION

31

Karthik Shivashankar received the B.E. degree in
electronics and communications from The National
Institute of Engineering, Mysore, India, in 2006 and
the M.Sc. degree in microelectronics from University
of Liverpool, U.K., in 2008.
His research interests include design methodolo-
gies for DVFS controller algorithms. Currently, he
is working as an Engineer at ARM Ltd., Cambridge,
U.K., in the Research and Development group.

Ganesh S. Dasika (S’01) received the B.S.E. degree
in computer engineering from the University of
Michigan at Ann Arbor, where he is now a Ph.D.
student in the Department of Electrical Engineering
and Computer Science.
His research interests mainly include designing
and compilation for power-efﬁcient, domain-speciﬁc
processors. He is a student member of the IEEE.

David Blaauw (M’94–SM’07) received the B.S.
degree in physics and computer science from
Duke University, Durham, NC, in 1986, the M.S.
degree in computer science from the University of
Illinois, Urbana, in 1988, and the Ph.D. degree in
computer science from the University of Chicago at
Urbana-Champaign in 1991.
Until 2001, he was with Motorola, Inc., Austin,
TX, where he was the Manager with the High Per-
formance Design Technology Group. Since 2001, he
has been on the faculty at the University of Michigan,
Ann Arbor, where he is currently a Professor. His work has focused on very large
scale integration design with particular emphasis on ultralow power and high
performance design. His current research interests include high-performance
and low-power VLSI circuits, particularly addressing nanometer design issues
pertaining to power, performance, and robustness.
Dr. Blaauw was the Technical Program Chair and General Chair for the In-
ternational Symposium on Low Power Electronic and Design. He was also the
Technical Program Co-Chair of the ACM/IEEE Design Automation Conference
and a Member of the International Solid-State Circuits Conference (ISSCC)
Technical Program Committee.

Krisztian Flautner (S’96–M’01) received the Ph.D.
degree in computer science and engineering from the
University of Michigan at Ann Arbor, where he is
currently appointed as a visiting scholar.
He is the Vice President of research and develop-
ment at ARM. ARM designs the technology that lies
at the heart of advanced digital products with more
than ﬁfteen billion processors deployed. He leads a
global team which is focused on the understanding
and development of technologies relevant to the pro-
liferation of the ARM architecture. The group’s activ-
ities cover a wide breadth of areas ranging from circuits, through processor and
system architectures to tools and software. Key activities are related to high-per-
formance computing in energy-constrained environments.
Dr. Flautner is a member of the ACM and the IEEE.

MIPS -X   INSTRUCTION   SET
and
PROGRAMMER’S   MANUAL

PAUL  CHOW

Technical  Report  No.  CSL-86-289

MAY 1988

The   M IPS -X   p ro jec t   ha s   been   suppo r ted   by   the   De fen se   Advanced   Re sea rch
P ro jec t s   Agency   unde r   con t rac t  MDAO03-83-C-0335.  Pau l   Chow   was   pa r t ia l ly
s u p p o r t e d   b y   a   P o s t d o c t o r a l   F e l l o w s h i p   f r o m   t h e   N a t u r a l   S c i e n c e s   a n d
Engineering  Research  Council  of  Canada.

MIPS-X   Instruction   Set
and  Programmer’s Manual

Paul  Chow

Technical Report No. 86-289
May 1986

Computer Systems Laboratory
Departments  of  Electrical  Engineering  and  Computer  Science
Stanford University
Stanford, California  94305

__

Abstract

MIPS-X  is  a  high  performance  second  generation  reduced
instruction set microprocessor. This document describes the visible
architecture  of  the  machine,  the  basic  timing  of  the  instructions,  and
the instruction set.
Keywords: MIPS-X  processsor,  RISC,  processor  architecture,
streamlined  instruction  set.

Copyright   0  1986   Stanford  University

e

Table   of Contents

1. Introduction
2. Architecture
2.1.   Memory  Organization
2.2.   General  Purpose  Registers
2.3.   Special  Registers
2.4.   The  Processor Status Word
2.4.1.   Trap   on Overflow
2.5.   Privilege  Violations
3. Instruction Timing
3.1.  The   Instruction  Pipeline
3.2.  Delays  and  Bypassing
3.3.  Memory   Instruction  Interlocks
3.4.  Branch   Delays
3.5.  Jump  Delays
3.6.  Detailed  Instruction  Timings
3.6.1.   Notation
3.6.2.   A  Normal  Instruction
3.6.3.   Memory  Instructions
3.6.4.   Branch   Instructions
3.6.5.   Compute  Instructions
3.6.5.1.   Special  Instructions
3.6.6.   Jump  Instructions
3.6.7.   Multiply Step   - ms?ep
3.6.8.   Divide  Step   -  dstep
4. Instruction Set
4.1.  Notation
4.2.  Memory   Instructions
4.2.1.   Id  -  Load
4.2.2.   st  - Store
4.2.3.   Idf  -  Load   Floating  Point
4.2.4.   stf  -  Store   Floating Point
4.2.5.   Idt  -  Load  Through
4.2.6.   stt  - Store  Through
4.2.7.  movfrc  -  Move   From  Coprocessor
4.2.8.  movtoc  - Move  To  Coprocessor
4.2.9.   aluc  - Coprocessor ALU
4.3.  Branch   Instructions
4.3.1.   beq  -  Branch   If  Equal
4.3.2.  bge  - Branch   If Greater  than  or Equal
4.3.3.   bhs  - Branch   If  Higher Or Same
4.3.4.   blo  - Branch   If Lower  Than
4.3.5.   blt  - Branch   If  Less   Than
4.3.6.   bne  - Branch   If Not Equal
4.4.  Compute  Instructions
4.4.1.   add  - Add
4.4.2.  dstep  - Divide Step
4.4.3.  mstart   - Multiply Startup
4.4.4.  mstep  -  Multiply Step
4.4.5.   sub  - Subtract

3

i

1
3
3
3
4
4
5
5
7
7
8
9
9
10
10
10
12
13
14
15
16
17
18
19
21
21
21
22
23
24
25
26
27
28
29
30
31
33
34
35
36
37
38
39
40
41
42
43
44

ii

4.4.6.   subnc  - Subtract with No Carry   In
4.4.7.   and  -‘Logical And
4.4.8.  bit  - Bit Clear
4.4.9.   not  -  Ones Complement
4.4.10.  or  - Logical Or
4.4.11.   xor  - Exclusive Or
4.4.12.  mov  - Move  Register  to Register
4.4.13.   asr  - Arithmetic Shift Right
4.4.14.   rotlb   -  Rotate  Left  by  Bytes
4.4.15.   rotlcb  -  Rotate   Left  Complemented  by Bytes
4.4.16.   sh  - Shift
4.4.17.   nop  -  No Operation
4.5.  Compute  Immediate  Instructions
4.5.1.   addi  -  Add  Immediate
4.5.2.   jpc  -  Jump PC
4.5.3.   jpcrs  -  Jump PC  and Restore State
4.5.4.   jspci  -  Jump  Indexed  and Store PC
4.5.5.  movfrs  - Move   from  Special Register
4.5.6.  movtos  - Move  to Special Register
4.5.7.   trap  - Trap Unconditionally
4.5.8.   hsc  - Halt  and  Spontaneously Combust
Appendix I. Some Programming Issues
Appendix II. Opcode Map
11.1.   OP   Field  Bit  Assignments
11.2.   Comp   Func  Field  Bit  Assignments
11.3.   Opcode Map  of All  Instructions
Appendix Ill. Floating Point Instructions
111.1.  Format
111.2.  Instruction  Timing
111.3.  Load   and  Store   Instructions
111.4.  Floating  Point   Compute  Instructions
111.5.  Opcode  Map  of  Floating  Point  Instructions
Appendix IV. Integer Multiplication and Division
IV.l.  Multiplication  and  Division Support
IV.2.   Multiplication
lV.3.  Division
Appendix V. Multiprecision Arithmetic
Appendix VI. Exception Handling
VI.l.   Interrupts
Vl.2.  Trap On Overflow
Vl.3.  Trap   Instructions
Appendix VII. Assembler Macros and Directives
VII.1.   Macros
VII.1   .I. Branches
VII.l.2. Shifts
Vll.l.3.   Procedure  Call  and Return
Vll.2.   Directives
Vll.3.  Example
Vll.4.   Grammar

45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
67
69
69
69
71
73
73
73
73
73
74
75
75
75
76
81
83
83
84
84
87
87
87
87
87
87
88
88

__

List   of Figures
FiIgure  2-l  :  Word  Numbering  in  MeGory
Fi ,gure 2-2: Bit  and Byte Numbering  in  a Word
Fi gure  2-3: The  Processor Status Word
Fi gure   3-l  :  Pipeline  Sequence
Fi,gure  Ill-l:  Floating  Point  Number  Format
Fi gure  IV-1 : Signed  Integer Multiplication
Fi gure   IV-2:   Signed  Integer  Division
FiIgure  VI-I  :  Interrupt  Sequence
Fiigure  VI-2:  Trap  Sequence

. . .
111

3
3
5
7
73
77
79
84
86

List  of Tables
Table 3-l : MIPS-X  Pipeline Stages
Table  3-2: Delay  Slots  for  MIPS-X  Instruction Pairs
Table  4-l  : Branch   Instructions
Table   IV-I  : Number  of Cycles Needed  to  do  a Multiplication
Table   IV-2: Number  of Cycles Needed  to do a Divide

V

7
9
32
78
78

1

1.  Introduction
This manual describes the visible architecture of the MIPS-X processor and
execute correct programs. MIPS-X  is a pipelined processor  that has no hardware
system  is  responsible  for  keeping  track  of  the  timing  of  the  instructions.

the  timing  information  required  to
interlocks.  Therefore,  the  software

The processor has a  load/store architecture and supports a very small number of  instructions. The  instruction set of
the processor will be described.

The  processor  supports  two  types  of  coprocessor  interfaces. One  interface  is  dedicated  to  the  floating  point  unit
(FPU) and the other will support up to 7 other coprocessors.  These instructions will also be described.

3

2.   Architecture

2.1.  Memory  Organization
The memory  is composed of 32-bit words and  it  is a uniform address space starting at 0 and ending at  232-1.  Each
memory  location  is a byte. Load/store addresses are manipulated as 32-bit byte addresses on-chip but only words can
be read from memory (ie., only the top 30 bits are sent to the memory system). The numbering of words in memory is
shown in Figure 2-l. Bytes (characters) are accessed by sequences of instructions that can do insertion or extraction of
characters  into  or  from  a word.  (See Appendix  I).  Instructions  that  affect  the  program  counter,  such  as  branches  and
jumps, generate word addresses. This means that the offsets used for calculating load/store addresses are byte offsets,
and displacements for branches and jumps are word displacements. The addressing is consistently Big Endian [I].

I

I

I

Word  0

Word  1

Word  2

1

. . . 11
word  23o-1

Figure  2-1: Word Numbering in Memory

Bytes are numbered starting with  the most significant byte at  the most significant bit end of  the word  The.bits   in a
word  are  numbered  0  to  31  starting  at  the most  significant  bit  (MSB)   and  going  to  the  least  significant  bit  (LSB).  Bit
and byte numbering are shown in Figure 2-2.

0

2

,

,

,

,

,

Byte  0  (MSB  end)

8

7

,

,

,

,

,

I

,

,

15 16

,

9

,

,

,

,

,

23 24

Byte 1

Byte  2

31

3

,

,

,

,

,

3

Byte 3 (LSB end)

Figure  2-2: Bit  and  Byte  Numbering  in  a  Word

The address space is divided into system and user space. An address with the high order bit (bit 0) set to one (1) will
access user space. If the high order bit is zero (0) then a system space address is accessed. Programs executing in user
space cannot access system space. Programs executing in system space can access both system and user space.

2.2.  General  Purpose  Registers
There are 32 general purpose registers (GPRs)  numbered 0 through 31. These are the registers named in the register
fields  of  the  instructions.  All  registers  are  32  bits.  Of  these  registers,  one  register  is  not  general  purpose.  Register  0
(r-0) contains the constant 0 and thus cannot be changed. The constant 0 is used very frequently so it is the value that is

4

stored in the constant register. A constant  register has one added advantage. One  register  is  needed  as  a  void
destination for instructions that do no writes or instructions that are being noped because they must be stopped for some
reason. This is implemented most easily by writing to a constant location.

2.3. Special Registers
There are several special registers that can be accessed with the Move SpeciaZ  instructions. They are:
The processor status word. This is described in more detail in Section 2.4.
PSW
Locations in the PC chain used for saving and restoring the state of the PC chain.
PC-4,  PC-1
The mul/cliv  register. This is a special register used during multiplication and division.
MD

2.4.   The  Processor  Status Word
The Processor Status Word (PSW)  holds some of the information pertaining to the current state of the machine. The
PSW  actually  contains  two  sets  of  bits  that  are  called  PSWcurrenf   and  PSWother.  The  current  state  of  the  machine  is
always  reflected  in  PSWcurrent. When  an  exception  or  trap  occurs,  the  contents  of  PSWcurrent  are  copied  into
PSWofher.   The  e bit  is  not  saved  PSWother  then  contains  the  processor  state  from  before  the  exception  or  trap  so  that
it  can  be  saved.  Interrupts  are  disabled,  PC  shifting  is  disabled,  overflows  are  masked  and  the  processor  is  put  into
system state. The  I bit  is cleared  if  the exception was an  interrupt. A  jump PC and restore state  instruction  (ipcrs)
causes  PSWother  to  be  copied  into  PSWcurrent. After the ALU cycle of the jpcrs instruction, the interrupts are enabled
and  the  processor  returns  to  user  state  with  its  state  restored.  Appendix  VI  describes  the  trap  and  interrupt  handling
mechanisms.

The  PSW  can  be  both  read  and written while  in  system  space,  but  a write  to  the  PSW while  in  user  space  has  no
effect. To change  the current state of  the machine via  the PSW, a move  to  special (movros)   instruction must be used  to
write the bits in PSWcurrent. Before restoring the state of the machine, a lllove  to special instruction must be used to
change  the bits in PSWother. All the bits are writable  except the  e bit and  the E-bit shift chain.

M m

u,  u

The  assignment  of  bits  is  shown  in  Figure  2-3. The  bits  corresponding  to  PSWcurrent are  shown  in  upper  case  and
those  in  lower  case  correspond  to  the  bits  in  PSWother. The  bits  are:
The  I bit should be checked by  the exception handler.
It  is  set  to 0 when  there  is  an  interrupt
I, i
request, otherwise it will be set to a 1. This bit never needs to be written but the value will be
retained until the next interrupt or exception. The i bit contains the previous value of the I bit but in
general has no meaning since only the I bit needs to be looked at when an exception occurs.
Interrupt mask. When set to 1, the processor will not recognize interrupts. Can only be changed by
a  system  process,  an  interrupt  or  a  trap  instruction.
When  set  to 1,  the processor  is executing  in user  state. Can only be changed by a  system process,
an  interrupt  or  a  trap  instruction.
Set to 1 when shifting of the PC chain is enabled.
Clear when doing an exception or trap return sequence. Used to determine whether state should be
saved  if  another  exception  occurs  during  the  return  sequence.  This  bit  only  changes  after  an
exception has occurred so the exception handler must be used to inspect this bit. See Appendix VI.
The E bits make up a shift  chain that is used to determine whether the e bit needs to be cleared when
an exception occurs. The E bits and the e bit are visible to the programmer but  cannot  be  written.

s  s
e

E

Processor Status Word

v9 v
09 0

The overflow mask bit. Traps on overflows are prevented when this bit is set. See Section 2.4.1.
This bit gets set or cleared on every exception. When a trap on overflow occurs, the 0 bit is set to 1
as seen by the exception handler. This bit never needs to be written. The o bit contains the previous
value of the 0 bit but in general has no meaning.

5

U 

u  010  

’ 

’ 

’ 

’ 

’ 

’ 

’ 

’ 

’ 

’ 

’ 

’ 

’ 

’ 

jEIEIEIE!eIvIVImIMji 

II 

Is 

ISI

Figure  2-3: The  Processor  Status  Word

2.4.1. Trap on Overflow
If the overflow mask bit in PSWcurrent  (V)  is cleared, then the processor will trap to location 0 (the start of all
exception and interrupt handling routines) when an overflow occurs during ALU or multiplication/division operations.
The  exception  handling  routine  should  begin  the  overflow   trap  handling  routine  if  the  ovefflow  bit  (0)  is  set  in
PSWcurrent.

The  V bit can only be changed while  in  system  space  so a  system call will have  to be provided  for user  space
programs  to  set  or  clear  this  bit.

2.5.  Privilege  Violations
__
User  programs  cannot  access  system  space. Any  attempt  to  access  system  space  will  result  in  the  address  being
mapped to user space. Bit 0 of the address will always be forced to 1 (a user space address) in user mode.

Attempting to write to the PSW while in user space will be the same as executing a rwp  instruction. The PSW is not
changed and no other action is taken.

There are no illegal instructions, just strange results.

Processor Status Word

Instruction  Timing

3.   Instruction   Timing
This chapter describes the MIPS-X instruction pipeline and the effects that pipelining has on the timing sequence for
various instructions. A section is also included that describes in detail the timing of the various types of instructions.

3.1.  The  Instruction  Pipeline
MIPS-X has a S-stage pipeline with one instruction in each stage of the pipe once it has been filled. The clock is a
two-phase  clock with  the phases  called  phase  I  (4~)  and  phase 2  (Q2).  The  names  of  the  pipe  stages  and  the  actions  that
take place in them are described in Table 3- 1. The pipeline sequence is shown in Figure 3-l.

Abbreviation

Name

Action

IF

RF

Instruction  Fetch

Fetch  the  next  instruction

Register Fetch

The instruction is decoded.
The register file is accessed during the second half
of  the  cycle  (Phase  2).

ALU

ALU Cycle

An  ALU  or  shift  operation  is  performed.
Addresses go to memory at the end of the cycle.

MEM

Memory Cycle

Waiting for the memory (external cache) to come back on read.
_-
Data output for memory write.

Write Back

The instruction result is written to the register
file during the first half of the cycle (Phase 1).

Table  3-1: MIPS-X  Pipeline  Stages

1.
2 .
3 .
4 .
5 .

IF

RF
IF

ALU 
RF
IF

MEM 
ALU 
RF
IF

WB
MEM  
ALU 
RF
IF

WB
MEM 
ALU 
RF

WB
MEM 
ALU 

WB
MJZM  

WB

Figure 3-1: Pipeline Sequence

Instruction  Timing

I

8

3.2.  Delays  and  Bypassing
A delay occurs because the result of a previous instruction is not available to be used by the current instruction. An
example is a compute instruction that uses the result of a  load instruction.
If in Figure 3-1, instruction 1 is a  load
instruction,  then  the  result  of  the  load  is  not  available  to  be  read  from  the  register  fde  until  the  second  half  of  WB  in
instruction 1. The first instruction that can access the value just loaded in the registers is instruction 4 because the
registers  are  read  on  phase  2  of  the  cycle. This means  that  there  is  a  delay of  two  instructions  from  a  load  instruction
until the result can be used as an operand by the ALU. An instruction &lay can also be called a d&y slot where an
instruction that does not depend on the previous instruction can be placed. This should be a nop if no useful instruction
can be found. Delays between instructions can sometimes be reduced or eliminated by using bypassing.

Bypassing allows an instruction to use the result of a previous instruction before it is written back to the register file.
This  means  that  some  of  the  delays  can  be  reduced. Table  3-2  shows  the  number  of  delay  slots  that  exist  for  various
pairs of instructions in MIPS-X. The table takes into account bypassing on both the results of a compute instruction and
a  load  instruction.  For  example,  consider  the  load-address  pair  of  instructions.  This  can  occur  if  the  result  of  the  first
load is used in the address calculation for the second load instruction. Without bypassing, there would be 2 delay slots.
Table 3-2 shows only 1 &lay slot because bypassing will take place.

The possible  implementations  for bypassing are bypassing only  to Source 1 or  to both Source 1 and Source 2. The
implementation  of  bypassing  in MIPS-X  uses  bypassing  to  both  sources. Bypassing  only  to  Source  1 means  that  the
benefits of bypassing can only be achieved if the second instruction is accessing the value from the previous instruction
via  the Source I register. If the second instruction can only use the value from the previous instruction as the Source 2
register, then 2 delay slots are required. Bypassing to both Sources eliminates this asymmetry. The asymmetry is most
noticeable  in  the  number  of &lay  slots  between  compute  or  load  instructions  and  a  following  instruction  that  tries  to
store the results of the compute or load instruction. Branches are also a problem because the comparison is done with a
subtraction of Source I - Source 2. Not all branch types have been implemented because it is assumed that the operands
can  be  reversed.  This means  that  it  will  not  always  be  possible  to  bypass  a  result  to  a  branch  instruction.  This
asymmetry  could  be  eliminated  by  taking  one  bit  from  the  displacement  field  and  using  it  to  decide  whether  a
subtraction  or  a  reverse  subtraction  should  be  used. The  tradeoff  between  the  two  types  of  bypassing  is  the  ability  to
generate  more  efficient  code  in  some  places  versus  the  hardware  needed  to  implement  more  comparators.  Table  3-2
shows  the delays  incurred  for both  implementions of bypassing.  It  is  felt  that bypassing  to both Sources  is preferable
and the necessary hardware has been implemented

Instructions  in the slot of bud  instructions  should not use  the same  register as  the one  that  is  the destination of  the
load  instruction. Bypassing  will  occur  and  the  instruction  in  the  load  slot will  get  the  address  being  used  for  the  load
instead of the value from the desired register.

One  other  effect  of  bypassing  should  be  described.  Consider  Figure  3-1.  If  instruction  1  is  a  load  to  rl   and
instruction 2 is a compute instruction that puts its result also in rl,  then there is an apparent conflict in instruction 3 if it
wants  to use  rl as its Source  I register. Both  the results from  instructions 1 and 2 will want  to bypass  to  instruction 3.
This  conflict  is  resolved  by  using  the  result  of  the  second  instruction. The  reasoning  is  that  this  is  how  sequential
instructions will behave. Therefore, in this example instruction 3 will use the result of the compute instruction.

Instruction  Timing

9

Instruction Pair
(Inst 1  -  Inst  2)

Delay Slots with
Bypassing  Only
to Source  1

Delay  Slots  with
Src llSrc2
Bypassing

Comment

Load  - Compute
Load - Address
Load - Data
Load - Branch
Compute - Compute
Compute - Address
Compute - Data
Compute - Branch

1
1
2
1
0
0
2
0

1
1
1
1
0
0
0
0

Loaded value used as address
Loaded value used for store data

Computed  value  used  as  address
Compute  result  used  for  store  data

Table  3-2: Delay Slots for MIPS-X  Instruction Pairs

3.3.  Memory  Instruction  Interlocks
There  are  several  instruction  interlocks  required  because  of  the  organization  of  the  memory  system.  The  external
cache is a write-back cache so it requires two memory cycles to do a store operation, one to check that the location is in
the cache and one to do the store. This means that a store instruction must be followed by a non-memory instruction so
that there can be two memory cycles available. For example, a store followed by a compute instruction is okay because
the  compute  instruction  does  not  use  its MEM  cycle.  The  software  should  try  to  schedule  non-memory  instructions
after all stores. If this is not possible, the processor will stall until the store can complete. Scheduling a nop instruction
is not sufficient because an instruction cache miss will also generate a load cycle. This cannot be predicted so the
hardware  must  be  able  to  stall  the  processor.

There are no restrictions for instructions after a load instruction. There is a restriction that a load instruction cannot
have  as  its  destination  the  register  being  used  to  compute  the  .address   of  the  load.  The  reason  is  that  if  the  load
instruction misses  in  the external cache,  it will still overwrite  its destination  register. This occurs because a  late miss
detect scheme is used in the external cache. The load instruction must be restartable.

3.4.  Branch  Delays
Besides  the  delays  that  can  occur  because  one  instruction must wait  for  the  results  of  a  previous  instruction  to  be
stored  in a  register or be bypassed,  there are also delays because  it  takes  time  for a branch  instruction  to compute  the
destination  for  a  taken  branch.  These are  called  branch delays  or  branch slots.  MIPS-X  has  two  branch  slots  after
If  instruction 1  is a branch  instruction,  then  it  is not until
-every  branch  instruction. Again, consider Figure 3-l.
instruction 4 when the processor can decide that the branch is to be taken or not to be taken.

Instruction  Timing

10

The  branch  slots  can  be  filled with  two  types  of  instructions. They  can  either  be  ones  that  are  always  executed  or
ones  that must be  squashed if  the branch does not go  in  the predicted direction. Squashing means  that  the  instructions
are  converted  into  nops by  preventing  their write  backs  from  occurring. This  is  used  if  the  branch  goes  in  a  direction
different from the one that was predicted This mechanism is described in more detail in Section 4.3.

3.5. Jump Delays
The computation of a  jump destination address means  that  there are  two delay slots after a  jump  instruction before
the program can begin executing at  the new address. The computation uses  the ALU  to compute  the  jump address so
the  result  is  not  available  to  the  PC  until  the  end  of  the ALU  cycle. Unlike  branches  however,  the  instructions  in  the
delay slots are always executed and never squashed.

3.6.  Detailed  Instruction  Timings
This section describes the timing of the instructions as they flow through the data path. It does not describe the
controls of  the  datapath   and  the  timing  required  to  set  them  up. These  timing  descriptions  are  intended  to make more
clear the programmer’s view of how each instruction is executed. The description of each instruction given in the later
sections is generally insufficient when it is necessary to know the possible interactions of various instructions.

The  timing  for  what  happens  during  an  exception  is  not  described  here. Appendix  VI  discusses  the  handling  of
exceptions.

The notation that will be used to describe the instruction timings will be shown first and then the execution of a
normal  instruction will be given. The  timing  for. each  type of  instruction  is  then described  in more detail. Finally,  the
timing for mstep and dstep are  treated separately. These are  the multiply and divide step  instructions. They do not fit  in
with the other types of compute instructions because they use the MD register.

IF-1
$1
$2
rSrc1, rSrc2

3.6.1.  Notation
The description of each  type of  instruction will show what parts of  the  datapath  are active and what  they are doing
for the instruction during each phase of execution. The notation that is used is:
IF,RF,ALX,MEM,WB
These  are  the  names  of  the  pipestages  as  described  in  Table  3-  1.
This is the clock cycle before the IF cycle of the instruction being considered.
Phase 1 of the clock cycle.
Phase 2 of the clock cycle.
Register values on  the Srcl and Src2 buses, corresponding  to  the Source 1 and Source 2 addresses
specified in the instruction.
Value to be written into the destination register specified by the Destination field of the instruction.
The Srcl bus is used.
aluSrc1, aluSrc2 ALU latches corresponding to the values on the Srcl and Src2 buses, respectively.
IR
The “instruction register.
Memory data register for values coming onto the chip.
MDRil-i
Memory data register for values going off chip.
MDRout

rDes t

Instruction  Timing

11

rResult
PC sow

The  result  register.
The PC  source to be used for this instruction. It will be one of: the displacement adder, the trap
vector, the incrementer,  the ALU or from the PC chain.
The value from the PC incrementer.
FCinc
The last value in the PC chain.
PC-4
Regcn>, Regcn..m>
Bit n or Bits n to m of register Reg.
Reg  is  shifted  left  n.bits.
Either rResult  or MDRin
The onchip  instruction cache.
Reserved  for  Stanford.

Reg<<  n
Bypass  source
Icache
RFS

P

Instruction  Timing

12

3.6.2.  A  Normal  Instruction
This section will show what each part  of the datapath  is doing during each phase of  the execution of an  instruction
The description of specific instruction types in the following sections will only describe the action of the relevant parts
of the datapath  pertaining to the instruction in question.

3

3  

$1
+2

RFS
PC  bus  c=   PC,,,
Precharge tag comparators, valid bit store

Do tag compare
Valid  bit  store  access
Icache address decoder e= FCc26..31>
Detect Icache hit
Precharge Icache
Do incrementer (calculate next sequential instruction address)
Do Icache access
IR e= Icache

RF 

91
+2

Do  bypass  comparisons
aIuSrc1  c=  r&cl
or aluSrc1~  Bypass source
aluSrc2 c== rSrc2
or aluSrc2 C=  Bypass source
or aluSrc2 e= Offset value
Displacement adder latch = Displacement value
MDRout c= rSrc2
or MDRout - Bypass source

ALU  

$1

$2

Do ALU, do displacement adder (for branch and jump targets)
Frecharge Result bus
Result bus c=  ALU
rResult c= Result bus
Memory address pads c= Result bus (There may be a latch here)

MEM 

+1  
02

RFS
MDRin = rResult
or MDRin e= Memory data pads
or Memory data pads e= MDRout

WB  

4~
$2

rDest e= MDRin
RFS

Instruction  Timing

13

3.6.3. Memory Instructions
These  instructions do accesses  to memory  in  the  form of  loads  and  stores. The  coprocessor  and  floating  point
instructions have exactly the same timings. The only difference is that the processor may not always source an operand
or  use  an  operand  during  a  coprocessor  instruction.

The MDRout   register  is  implemented  as  a  series  of  registers  to  correctly  time  the  output of  data  onto  the memory
data pads. These registers are labelled MDRout.RF@,, MDRoutALU$,,  MDRout.ALU$,  and MDRout.MEM$,  ~

IF-1  

IF

RF 

$1
$2

$1

$2

$1
$2

ALU  0,

$2

MEM 

$1

RFS
PC  bus  ti  PC,,,
Precharge tag comparators, valid bit store

Do  tag  compare
Valid  bit  store  access
Icache address decoder c= PC<26..3  l>
Detect Icache hit
Precharge Icache
Do incrementer (calculate next sequential instruction address)
Do Icache access
IR   e=  Icache

Do  bypass  comparisons
aluSrc1 =  rSrc1
or aluSrc1 e= Bypass source
aluSrc2 (= Offset value
MDRou~.RF~~   e=  rSrc2  (For  stores)
or MDRou~.RF$~  = Bypass source (For stores)

Do  ALU(add)
Precharge  Result  bus
MDRoutALU$,  e= MDRout.RF$,  (For stores)
Result bus e ALU
rResult e Result bus
Memory address pads e Result bus
MDRoutALU~, = MDRout.ALU~,  (For stores)

MDRoutMEM~,  c= MDRout.ALU+2  (For stores)
MDRin e Memory data pads (For loads)
or Memory data pads  c= MDRou~.MEM~~  (For  stores)

WB  

+1
02

rDest  e= MDRin (For loads)
RFS

. .

Instruction  Timing

14

3.6.4. Branch Instructions
These  instructions do a compare  in  the ALU. The PC value  is  taken  from  the displacement ad&r when a branch  is
taken and from the incrementer when a branch is not taken.

I

IF-1  

IF

RF 

$1
$2

$1

$2

+1
4 2

$2

RFS
PC  bus  e=   PC,,,
Precharge tag comparators, valid bit store

Do tag compare
Valid  bit  store  access
Icache address decoder = PC<26.‘.3 l>
Detect Icache hit
Precharge Icache
Do incrementer (calculate next sequential instruction address)
Do Icache access
IR e Icache

Do  bypass  comparisons
aluSrc  1 e= rSrc  1
or aluSrc1 e= Bypass source
aluSrc2 e= rSrc2
or aluSrc2  t=  Bypass source
Displacement adder = Displacement value

Do ALU(Src1  - Src2),  do displacement adder (for branch target)
Precharge Result bus
Evaluate condition at the end of $r  before the rising edge of e2
PC  bus e= Displacement adder (Branch taken)
or PC  bus e= Incrementer (Branch not taken)
Tag compare latch e= PC bus
rResult e= Result bus

MEM 

WB  

+1 
+2

$1
$2

RFS
MDRin   c=  rResult

RFS
RFS

Instruction  Timing

3.65 Compute  Instructions
These instructions are mostly 3-operand instructions that use the ALU to do an operation. Some of them do traps or
jumps. These are treated separately in Section 3.6.6. The timing for instructions that access the speciaZ  registers is
described  in  Section  3.6.5.1.

3

15

IF-1  

IF 

RF 

$1
$2

@l

$2

@l
$2

RFS
PC  bus  e=  PC,,,
Precharge  tag  comparators,  valid  bit  store

Do tag compare
Valid  bit  store  access
Icache address decoder e= FC<26..31>
Detect Icache hit
Precharge Icache
Do incrementer (calculate next sequential instruction address)
Do Icache access
IR  e=  Icache

Do  bypass  comparisons
aluSrc1 e= rSrc  1
or aluSrc1 = Bypass source
aluSrc2 e rSrc2
or aluSrc2 = Bypass source
or aluSrc2 c= Immediate value (for Compute Immediate Instructions)

Do  ALU
Precharge  Result  bus
Result bus = ALU
rResult c= Result bus

MEM 

wB  

4~  
+2

$1
$2

RFS
MDRin + rResult

rDest e= MDRin
RFS

Instruction  Timing

i

16

3.651. Special Instructions
These  instructions  (mvtos  and mvfrs)  access the  special registers described  in Section 2.3.

IF-1  

IF 

RF 

ALJJ  

%
+2

$1

$2

$1
42

$1

+2

RFS
PC  bus  e=   PC,,,
Precharge tag comparators, valid bit store

Do tag compare
Valid  bit  store  access
Icache address decoder e= PC<26..31>
Detect Icache hit
Recharge   Icache
Do incrementer (calculate next sequential instruction address)
Do Icache access
IR e Icache

Do  bypass  comparisons
aluSrc  1 e= rSrc  1 (For movtos)
or aluSrc1 t=  Bypass source (For mvt~s)

Do ALU(pass  Srcl)
Recharge Result bus
Result bus C=  alu Srcl (For movtos)
or Result bus e= Special Register (For movfrs)
Special Register c= Result bus (For movtos)
rResult C=  Result bus

MEM 

WB 

+1 
$2

91
+2

RFS
MDRin  =  rResu.lt

rDest e= MDRin (For movfkr)
RFS

Instruction  Timing

17

3.6.6. Jump Instructions

IF-1  

IF 

RF 

ALU 

+1
$2

$1

$2

$1
$2

$1

$2

RFS
PC  bus  e=  PC,,,
Precharge  tag  comparators,  valid  bit  store

Do tag compare
Valid  bit  store  access
Icache address decoder (=  P&26.-3  l>
Detect Icache hit
Precharge Icache
Do incrementer (calculate next sequential instruction address)
Do Icache access
IR (z Icache

b

Do  bypass  comparisons
aluSrc1 * rSrc  1
or aluSrc1 = Bypass source
aIuSrc2 * Immediate value (For jspci)

Do ALU(add)
Precharge  Result  bus
Result bus  c= PCinc  (Forjspci)
PC bus C=  ALU (For jspci)
or PC bus e= PC-4,  shift PC chain (For@  andjpcrs)
or PC bus e Trap vector (For  trap)
PSWcurrent  (= PSWother  (Forjpcrs)
rResult c= Result bus

MEM 

+I 
Q2

WB 

$1

RFS
MDRin  =  rResult

rDes  t c MDRin (For jspci)

Instruction  Timing

18

3.6.7. Multiply Step - mstep
The  MD   register  is  implemented  as  a  series  of  q2-e1   registers. They  are  called  MDresult.+2,   MDresult.$l,
MDmdrin.@2,  and MDwb.Q,. The names reflect the names of the bypass registers used when bypassing to the register
file. The  special register  that  is visible for reading and writing  is MDresult.+2. This chain of registers is necessary for
restarting  the  sequence  after  an  exception. MDwb.el contains the true value of MD. When an interrupt occurs, the
write-back  into this register is stopped just like write-backs to a register in the register file. The value in this register is
needed to restart the  sequence. One  cycle  after  an  interrupt  is  taken,  the  contents  of MDwb.$r   are  available  in
h4Dresult.Q2.  This value has to be saved if the interrupt routine does any multiplication or division.

The mstart  instruction has similar timing with a different ALU operation.

There must be one instruction between the instruction that loads the h4D  register and the first instruction that uses the
MD register. This occurs when starting a multiplication or division routine and when restarting after an interrupt.

--

IF-1  

IF

RF 

$1
$2

41

$2

$1

ALU 

4+

$2

4~

$2

WB 

RFS
PC bus = F’C,,,
Precharge tag comparators, valid bit store

Do tag compare
Valid  bit  store  access
Icache address decoder C=  PC<26..3  l>
Detect Icache hit
Precharge Icache
Do incrementer (calculate next sequential instruction address)
Do Icache access
IR e=  Icache

Do  bypass  comparisons
aluSrc1 = rSrcl<< 1
or  aluSrc1  e= Bypass  source<< 1
aluSrc2 e=  rSrc2

Do ALU(add)
Latch aluSrc  1
Precharge  Result  bus
Result bus (= ALU (MSB (MDresult.@r)  is 1)
or Result bus e= aluSrc1  (MSB (MDresult~$+)  is 0)
rResult = Result bus
MDresult.$2  c== MDres~lt.~~<c  1

MDresult.$r  = MDresult.$2
MDRin (= rResult
MDmdrin.$2  c= MDresult.+,

rDest = MDRin
MDwb.Ql  = MDmdrin.$
RFS

Instruction  Timing

3.6.8. Divide Step  - dstep
The  MD  register  is  also  used  for  this  instruction. See Section 3.6.7 for a description of its implementation and the
notation  used

19

IF-1  

IF

RF 

$1
$2

01

$2

$1
+2

ALU 

$1

RFS
PC bus e= PC,,,
Precharge  tag  comparators,  valid  bit  store

Do tag compare
Valid  bit  store  access
Icache address decoder * PC<26..31>
Detect Icache hit
Recharge  Icache
Do incrementer (calculate next sequential instruction address)
Do Icache access
IR   e=  Icache

Do  bypass  comparisons
ah&cl (= rSrcl<< 1 + MSB(MDresult.~l)
or aluSrc1 e= Bypass  source<<  1 + MSB(MDresult.$,)
aluSrc2 e= rSrc2

Do ALU(sub)
Precharge  Result  bus
Result bus = ALU (MSB (ALU result) is 0)
or Result bus c= aluSrc1 (MSB (ALU result) is 1)
rResult e Result bus
MDresult.$,  (= MDres~lt.~~<c  1 + Complement of MSB(ALU result)

_ _

MEM  @,
$2

WB 

4+

$2

MDresult.+l   c= MDres~lt$~
MDRin  =  rResult
MDmdrin.$  * MDresult.$

rDest = MDRin
MDwb.+l  e= MDmdrin.@,
RFS

Instruction  Timing

Instruction  Timing

4.  Instruction  Set
There  are  four  different  types  of  instructions. They  are memory  instructions,  branch  instructions,  compute
instructions, and compute immediate instructions. Coprocessor instructions are part of the memory instructions.

21

4.1. Notation
This  section  explains  the  notation  used  in  the  descriptions  of  the  instructions.
The  most  significant  bit  of  x.
MSB(x)
x  is  shifted  left  by  y  bits.
x<< y
x  is  shifted  right  by  y  bits.
x>> y
x  is  a  number  represented  in  base  y
X#Y
x is concatenated with y.
x  II Y
Address of the instruction being fetched during the ALU cycle of an instruction
PCcurrent
Address  of  the  next  instruction  to  be  fetched.
PCnext
The contents of CPU register n.
WsO-0
The contents of register n in the floating point unit (FPU).
FM&Q
Reg<n>,  Reg<n..m>
Bit n or Bits n to M of register Reg.
The contents of memory at the location addr.  The value accessed is always a word of 32 bits.
The value of n sign extended to 32 bits. The size of n is specified by the field being sign extended.
The register number used as the Source 1 operand
The register number used as the Source 2 operand
The register number used as the Destination location.
The register number used as the Source 1 floating point operand.
The  register  number  used  as  the  Source  2  floating  point  operand.
The  register  number  used  as  the  Destination  floating  point  register.
Coprocessor  instruction.
The memory address  register. The  contents  of  this  register  are  placed  on  the  address  pins  of  the
processor.
The memory  data  register. The  address  pads  of  the  processor  always  reflect  the  contents  of  this
register.

Memory[addr]
S i g nE x t e n d
rSrc  1
rSrc2
rDes t
fSrc  1
fSrc2
fDest
cop1
MAR

MDR

4.2.  Memory  Instructions
The memory  instructions  are  the  ones  that  do  an  external memory  cycle. The most commonly used memory
instructions  are  load  and  store. The  other  instructions  that  are  part  of  the  memory  instructions  are  the  coprocessor
instructions. They do not always generate a memory cycle that is recognized by memory. Instead the coprocessor uses
the cycle. This is explained in more &tail in the individual instruction descriptions.

22

4.2.1. Id - Load

3

Dest
Srcl
TY 
OP
l(),()(-)O, ““I ““I D 9,

9

9

9

Offset(17)
,s 9

9

9

9 ,,,,

Assembler
Id Offset[rSrcl],rDest
Operation
Reg@est)   c= Memory[SignExtend(Offset)   -t Reg(Srcl)J
Description
The  offset  field  is  sign  extended  and  added  to  the  contents  of  the  register  specified  by  the  Srcl  field  to  compute  a
memory address. The contents of that memory location is put into Reg@est).

Note: An  instruction  in  the slot of a  loud  instruction  that uses  the same  register as  the  load instruction  is  loading  is
not guaranteed to get the correct result. Do not try to use the loud slots in this manner.

Id

Load

Id

4.2.2. st - Store

i

23

TY 
OP
1 o,* 10,

Srcl
9 “9, 9

Src2
Offset(17)
9
9 ‘, ” ,9 ,,s ,I,) 9 ,,,s

Assembler
st  Offset[rSrcl]JSrc2
Operation
Memory[SignExtend(Offset) + Reg(Src  l)] e= Reg(Src2)
Description
The  offset  field  is  sign  extended  and  added  to  the  contents  of  the  register  specified  by  the  Srcl  field  to  compute  a
memory address. The contents of Reg(Src2) are stored at that memory location.

This  instruction  requires 2 memory cycles, one  to  read  the cache and  then one  to do  the store. To obtain maximum
performance,  instructions  that do not  require a memory cycle should be scheduled after a store  instruction  if possible.
Otherwise, the processor may stall for one cycle.

-_

st

Store

st

24

4.2.3. Idf - Load Floating Point

Srcl
TY 
OP
1(),1()(-J,‘,’ ,, P

Dest
9 ,, 9
9

9

8

9

9

9

Offset(  17)
9
9
9

9

P

9

9

9

,9

Assembler
ldf Offset [rSrc  l] ,fDes t
Operation
FReg@est)   c= Memory[SignExtend(Offset)  + Reg(Srcl)J
Description
The  offset  field  is  sign  extended  and  added  to  the  contents  of  the  register  specified  by  the  Srcl  field  to  compute  a
memory address. The contents of  that memory  location  is put  into  the  register specified by Dest  in  the  floating point
unit (FReg(Dest)). The CPU  ignores the data returned in the memory cycle.

Note: An  instruction  in  the slot of a  load  instruction  that uses  the same  register as  the  load  instruction  is  loading  is
not guaranteed to get the correct result. Do not try to use the loud slots in this manner.

Note:  If a processor configuration does not have an FPU  then different code must be generated  to-emulate  the
floating point instructions. Any code that tries to use FPU instructions when there is no FPU will not execute correctly.

Idf

Load Floating Point

ldf

4.2.4. stf - Store Floating Point

25

Src2
TY 
OP
Srcl
I(),1 I*,,,,,,” 9

9,

9

9

9

9

9

9

Offset(  17)
,9 9
9
9

9

9

9

9

9

Assembler
stf  Offset[rSrcl],fSrc2
Operation
Memory[SignExtend(Offset) + Reg(Srcl)]  (=  FReg(Src2)
Description
The  offset  field  is  sign  extended  and  added  to  the  contents  of  the  register  specified  by  the  Srcl  field  to  compute  a
memory address. The contents of the floating point register specified by Src2  are stored at that memory location. The
CPU does not put out any data during this write memory cycle.

Note:  If a processor configuration does not have an FPU  then different code must be generated  to emulate  the
floating point instructions. Any code that tries to use FPU instructions when there is no FPU wiIl not execute correctly.

stf

Store Floating Point

stf

26

4.25. ldt - Load Through

3

OP
TY 
Dest
Srcl
Offset(  17)
10*()11,999,9999,9999999999999999

Assembler
Idt Offset[rSrc  I],rDest
Operation
Reg(Dest) e= Memory[SignExtend(Offset)  + Reg(Srcl)]
Description
This  instruction  is  the same as  Id  except  that  it  is guaranteed  to bypass  the cache. There is no  check  to  see whether
the location being accessed currently exists in the cache.

The  offset  field  is  sign  extended  and  added  to  the  contents  of  the  register  specified  by  the  Srcl  field  to  compute  a
memory address. The contents of that memory location is put into Reg@est).

Note: An  instruction  in  the  slot  of  a  load  instruction  that  uses  the  same  register  as  the  load  instruction  is  loading  is
_.”
not guaranteed to get the correct result. Do not try to use the load slots in this manner.

idt

Load  Through

ldt

4.2.6. stt - Store Through

27

TY 
Jl
0 10

OP
1

11

9

Srcl
’

9

9

I

9

Src2
,
9

,

,

,

9

,

9

9

Offset(  17)
,
9
9

,

9

,

,

9

9

9

Assembler
stt  Offset[rSrcl],rSrc2
Operation
Memory[SignExtend(Offset) + Reg(Src l)] e= Reg(Src2)
Description
This  instruction  is  the same as  st  except  that  it  is guaranteed  to bypass  the cache. There  is no check  to see whether
the location being accessed currently exists in the cache.

The  offset  field  is  sign  extended  and  added  to  the  contents  of  the  register  specified  by  the  Srcl  field  to  compute  a
memory address. The contents of Reg(Src2) are stored at that  memory location

--

stt

Store Through

stt

28

4.2.7. movfrc - Move From Coprocessor

I

CS2lCD
CSl
Func
COP#
Dest
Srcl(r0)
OP
TY 
10101()o()()(),9999,99,99999,999,999

I

cop1

I

Assembler
movfrc  CopI,rDest
Operation
MAR = SignExtend(Cop1)  + Reg(Src 1)
Reg(Dest) * MDR
Description
This  instruction  is  used  to  do  a  Coprocessor  register  to  CPU  register  move.

The Cop1  field  is sign extended and added  to  the contents of  the  register specified by  the Srcl field. The Srcl field
should be Register 0 if the Cop1 field is to be unmodified (hackers take note). The Cop1 field will appear on the address
lines of the processor where it can be read by the coprocessor. The coprocessor will place a value on the data bus that
*-
will be stored in Reg@est)  of the CPU. The memory system will ignore this memory cycle.

The Cop1 field is decoded by the coprocessor-s to fmd the coprocessor being addressed (COP#)  and the function to be
performed. A  possible  format  is  shown  above. The  fields  C’S1 and  CS2ICD   show  possible  coprocessor  register  fields.
The format is flexible except that all coprocessors  should fmd  the COP#  in the same place.

No&:  An instruction in the slot of a movfic  instruction that uses the same register that the mo#rc  instruction is
loading is not guaranteed to get the correct result. Do not try to use the slots in this manner.

movfrc

Move  From  Coprocessor

movfrc

4.2.8. movtoc - Move To Coprocessor

29

TY 
0 1 1

Jl

OP
1

Srcl(r0)
110 0
0
0 01

Src2
’
’

’

9

COP#
,
9

Func
9

9

,

9

I

I

9

cop1

CSl
9

9

9

CS2lCD
,
,
9

I

Assembler
movtoc  CopI,rSrc2
Operation
MAR * SignExtend(CopI) + Reg(Src1)
MDR = Reg(Src2)
Description
This  instruction  is  used  to  do  a  CPU  register  to  Coprocessor  register  move.

The Cop1  field  is sign extended and added  to  the contents of  the  register specified by  the Srcl  field. The Srcl  field
should be Register 0 if the Cop1  field is to be unmodified (hackers take note). The  Cop1  field  will  appear  on  the  address
lines of the processor where it can be read by the coprocessor. The contents of register Src2 are placed on the data lines
so that the coprocessor can access the value. The memory system will ignore this memory cycle.
__

The Cop1 field is decoded by the coprocessors to find the coprocessor being addressed (COP#) and the function to be
performed. A  possible  format  is  shown  above. The  fields  CSI and  CS2ICD   show  possible  coprocessor  register  fields.
The format is flexible except that all coprocessors should find  the COP# in the same place.

movtoc

Move  To  Coprocessor

movtoc

30

4.2.9.  aluc  - Coprocessor  ALU

COP#
Srcl (rQ)
TY 
OP
J1011011000001000001 9
9

1

9

Func
9

9

9

CSl
9

9

9

I

CS2KD
9
9
9

[

9
1
cop1

Assembler
aluc  Cop1
Operation
MAR = SignExtend(Cop1)  + Reg(Src1)
Description
This  instruction  is used  to execute a coprocessor  instruction  that does not require  the  transfer of data  to or from  the
CPU.

This instruction is actually implemented as:
movfrc  CopI,rO   1

The Cop1  field  is sign extended and added  to  the contents of  the  register specified by  the Srcl  field. The Srcl  field
should be Register 0 if the Cop1 field is to be unmodified (hackers take note). The Cop1 field will appear on the address
lines of the processor where it can be read by the coprocessor. The memory system will ignore this memory cycle.

The  Cop1  field  is  decoded  by  the  coprocessor’s  to find   the  coprocessor  being  addressed  (COP#)  and  the  function  to  be
performed. A  possible  format  is  shown  above. The fields CSI and CS2ICD  show possible coprocessor  register  fields.
The  format  is  flexible  except  that  all  coprocessor-s  should  find   the  COP#  in  the  same  place.

Note  that  this  instruction  is needed  to perform floating point ALU operations. Only floating point loads and stores
have  special  FPU  instructions.

aluc

Coprocessor  ALU

aluc

31 

.

4.3.  Branch  Instructions
As described previously  in Section 3.4, all branch  instructions have  two delay slots. The  instructions placed  in  the
slots can be either ones  that must always execute or ones  that should be executed  if  the branch  is taken. There are  two
flavours of branch instructions that must be used depending on the type of instructions placed in the slots. They are:
The instructions in the slots are always executed. They are never squashed (turned into nops).
No  squash:
All  branches  are  statically  predicted  to  go  (be  taken). This means  that  the  instructions  in  the
Squash  if  don’t  go:
branch slots should be instructions from the  target instruction stream. If the branch is not
taken, then the instructions in the slots am squashed.
The instructions in the slots must be both of the same type. That is, they should both always execute or both be from
the target instruction stream. If squashing takes place, both instructions in the slots are treated equally.

Note  that  for best performance, it  is  best  to  try  to  find  instructions  that  can  always  execute  and  use  the  no  squash
branch  types.

Branch instructions can be put in the slot of branches that can be squashed

The branch conditions are established by testing the result of
Reg(Src 1)  -  Reg(Src2)
where  Srcl and  Src2  are  specified  in  the  branch  instruction. The  condition  to  be  tested  is  specified  in  the COND  field
of the branch instruction, The expressions used to derive the conditions use the following notation:
Bit 0 of the result is a 1. The result is negative.
N
The  result  is  0.
Z
32-bit 2’s-complement overflow has occurred in the result.
V
A  carry  bit  was  generated  from  bit  0  of  the  result  in  the  ALU.
C
Exclusive-Or
$
Some branch conditions that are usually found on other machines do not exist on MIPS-X. They can be synthesized
by  reversing  the  order  of  the  operands  or  comparing with  kg(O)  in Source 2  (Src2=0).   These  branches  are  shown  in
Table  4-1  along  with  the  existing  branches.

--

32

Branch

Description

Expression

beq
ke
w
bhi
bhs
ble
blo
blos
blt
bne
bP1
bmi
bra

Branch if equal
Branch if greater than or equal
Branch  if  greater  than
Branch if higher
Branch if higher or same
Branch if less than or equal
Branch if lower than
Branch if lower or same
Branch  if  less  than
Branch  if  not  equal
Branch  if  plus
Branch  if  minus
Branch  always

z
N@V
(N@V)+Z
c+z
C
(N@V)+Z
c
c+z
N@V
z
3
N

Table  4-1:  Branch  Instructions

Branch  To  Use
If Synthesized

blt  (rev  ops)
blo (rev ops)

bge  (rev  ops)

bhs  (rev  ops)

bge (cmp to Src2=0)
blt (cmp to Src2=0)
beq fl,fi

I

33

4.3.1. beq - Branch If Equal

SQ
Src2 
Srcl
TY
Cond
oo,ooI,,,,,,“9 91s,
s = 1 a Squash  if don’t go
s = 0 3  No  squashing

9

9

9

9

9

9

Disp(16)
9
9
9

9

9

9

9

9 9,

;  No  squashing
;  Squash  if  don’t  go

Assembler
h rSrc  1 ,rSrc2,Label
beqsq rSrc  1 ,rSrc2,Label
Operation
If  [Reg(Src  1) - Reg(Src2)]  3  Z
then
PCnext  = PCcuxrent  + SignExtend@isp)
Description
If  Reg(Src1)   equals  Reg(Src2)  then  execution  continues  at  Label  and  the  two  delay  slot  instructions  are  executed.
The value of Label is computed by adding PCcutrent + the signed displacement.

If Reg(Src1)  does not equal Reg(Src2),  then the delay slot instructions are executed for beq and squashed  for beqsq.

beq

Branch  If Equal

34

4.3.2. bge - Branch If Greater than or Equal

3

Srcl
9   9,
9  

TY
Cond
(-)(),I 11,  
s = 1 3  Squash ifdon’tgo
s = 0 * No  squashing

9

Src2 
9
9

9

9

SQ
Is,

9

9

9

9

9

9

Disp(l6)
9
9
9

9

9

9

9

9

9

; No  squashing
;  Squash  if  don’t  go

Assembler
rSrc  1 ,rSrc2,Label
bge
bgesq rSrc  1 ,rSrc2,Label
Operation
If [Reg(Srcl)  - Reg(Src2)]  * N $  V
then
PCnext  (z  PCcurrent + SignExtend(Disp)
Description
This  is  a  signed  compare.

If  Reg(Src1)   is  greater  than  or  equal  to  Reg(Src2)  then  execution  continues  at  Label  and   the  two  delay   slot
instructions are executed. The value of Label is computed by adding PCcurrent + the signed displacement.

If Reg(Src1)  is less than Reg(Src2),  then the delay slot instructions are executed for bge and squashed for bgesq.

Branch  If  Greater  Than  Or  Equal

3

35

4.3.3.  bhs - Branch If Higher Or Same

Disp(16)
SQ
Src2  
Srcl
TY
Cond
()(),01(),   9999(9999,s(999999999999799

s = 1 3  Squash  if don’t go
s = 0 + No squashing

; No  squashing
;  Squash  if  don’t  go

Assembler
rSrc  1 ,rSrc2,Label
bhs
bhssq rSrc  l,rSrc2,Label
Operation
If PReg(Srcl)  - Reg(Src2)]  3  C
then
PCnext  e= PCcurrent + SignExtend@isp)
Description
This is an unsigned compare.

If  Reg(Src1)   is  higher  than  or  equal  to  Reg(Src2)  then  execution  continues  at  Label   and  the  two  delay  slot
instructions are executed. The value of Label is computed by adding PCcurrent + the signed displacement.

If Reg(Src 1) is lower than Reg(Src2),  then the delay slot instructions are executed for bhs  and squashed for bhssq.

bhs

Branch  If Higher Or Same

bhs

36

4.3.4. blo - Branch If Lower Than

Disp(l6)
SQ
Src2 
Srcl
Cond
TY
oo,110,9999,9999,s,999999999999999

s = 1  =B  Squash  if don’t go
s = 0 * No squashing

; No  squashing
;  Squash  if  don’t  go

Assembler
rSrc  1 ,rSrc2,Label
blo
blosq rSrc  1 ,rSrc2,Label
Operation
If  [Reg(Srcl)  - Reg(Src2)]  3  C
then
PCnext  e PCcurrent + SignExtend@isp)
Description
This  is  an  unsigned  compare.

If  Reg(Src1)   is  lower  than Reg(Src2)  then  execution  continues  at  Label   and  the  two  delay  slot  instructions  are
_.
executed. The value of Label  is computed by adding PCcurrent + the signed displacement.

If Reg(Src1)  is higher than or equal to Reg(Src2) or if there was a carry generated, then the delay slot instructions are
executed for blo  and squashed for blosq.

blo

Branch  If  Lower  Than

blo

4.35 blt - Branch If Less Than

37

TY
Cond
(),() 11,
()

Srcl
, 
9

, 

,

,

,

Src2 
,
,

9

SQ
Is,

9

9

9

Disp(l6)
,
9 9,) 3

,

,

,

,

,

,

s = 1 3  Squash  if don’t go
s = 0  3  No squashing

;  No  squashing
;  Squash  if  don’t  go

Assembler
rSrc  1 ,rSrc2,Label
blt
bltsq  rSrc   l,rSrc2,Label
Operation
If [Reg(Src  1) - Reg(Src2)]  * N $  V
then
PCnext  (= PCcurrent + SignExtend@isp)
Description
This  is  a  signed  compare.

If  Reg(Src1)   is  less  than  Reg(Src2)  then  execution  continues  at  Label   and  the  two  delay  slot  instructions  are
__
executed. The value of L-ubeZ  is computed by adding P&u-rent  + the signed displacement.

If Reg(Src1)   is greater  than or equal  to Reg(Src2),   then  the delay slot  instructions are executed for  bit  and squashed
for  bltsq.

blt

Branch If Less Than

blt

38

4.3.6. bne - Branch If Not Equal

TY
0 Oil

Cond
0

l,

Srcl
,

,

9 9,

,

Src2 
, 9,

SQ
Is,

,

9

9

9

9,

Disp(l6)
9
9
9

9 9,) 9

9

s = 1 + Squash  if don’t go
s = 0 * No squashing

;  No  squashing
;  Squash  if  don’t  go

Assembler
rSrc  l,rSrc2,Label
bne
bnesq rSrc  1 ,rSrc2,Label
Operation
If  [Reg(Srcl)  - Reg(Src2)]  * z
then
PCnext  = PCcurrent + SignExtend@isp)
Description
If Reg(Src1)  does not equal Reg(Src2) then execution continues at Label and the two delay slot instructions are
executed. The value of Label  is computed by adding PCcurrent + the signed displacement.

If Reg(Src1)  equals Reg(Src2),  then the delay slot instructions are executed for bne and squashed for bnesq.

bne

Branch If Not Equal

bne

4.4.  Compute  Instructions
Most of the compute instructions are 3-operand  instructions that use the ALU or the shifter to perform an operation
on  the  contents  of  2  registers  and  store  the  result  in  a  third  register.

i

39

40

4.4.1. add - Add

TY 
0

1

OP
1

0

0

Comp Func(  12)
Dest
Src2
Srcl
  ““I  99s’~““looooooollool~

Assembler
add rSrc  1 ,rSrc2,rDest
Operation
Reg(Dest) e= Reg(Src1)  + Reg(Src2)
Description
The sum of the contents of the two source registers is stored in the destination register.

add

Add

add

I

41

4.4.2, dstep  - Divide Step

Camp  Func(  12)
Dest
Src2
Srcl
TY 
OP
01~0~~J”“~““1”“1000101100110[

Assembler
dstep  rSrcl,rSrc2,rDest
Operation
Srcl should be the same as Dest.

ALUsrcl  c= Reg(Srcl)cc  1 + MSB(Reg(MD))
ALUsrc2 c= Reg(Src2)
ALUoutput   e= ALSJsrcl  - ALUsrc2

If MSB(ALUoutput)  is 1
then
Reg@est)   e= ALUsrcl
Reg(MD)  e= Reg(MD)c< 1
else
Reg@est)   e= ALUoutput
Reg(MD)  (z Reg(MD)<c 1 + 1
Description
This is one step of a l-bit restoring division algorithm. The division scheme is described in Appendix IV.

. .

dstep

Divide Step

dstep

42

4.4.3. mstart - Multiply Startup
e

i

TY 
OP
Srcl
Comp Func(l2)
Dest
Src2
Jolroooloooool”“i99’91000011100110~

Assembler
ms tart rSrc2,rDest
Operation
If MSB(Multiplier loaded in Reg(MD))  is 1
then
Reg(Dest) (= 0 - Reg(Src2)
Reg(MD)  e= Reg(MD)ce  1
else
Reg(Dest) e= 0
Reg(MD)  e= Reg(MD)cc  1
Description
This  is  the  first  step  of  a  l-bit  shift  and  add multiplication  algorithm  used when  doing  signed multiplication.  If  the
most  significant  bit  of  the  multiplier  is  1,  then  the  multiplicand  is  subtracted  from  0’  and  the  result  is  stored  in
Reg(Dest). The multiplication scheme is described in Appendix IV.

mstart

Multiply  Startup

mstart

4.4.4. mstep - Multiply Step

3

43

TY 
OP
Src2
Srcl
Dest
Comp Func(  12)
01l0001   ““1”“1”9  ‘1 0 0 0 0 1 0 0 1 1 0 0 1~

Assembler
mstep rSrc  1 ,rSrc2,rDes  t
Operation
Srcl should be the same as Dest.

If  MSB(Reg(MD))  is  1
then
Reg@est)  = Reg(Srcl)c<  1 + Reg(Src2)
Reg(MD)  e= Reg(MD)c< 1
else
Reg@est)  = Reg(Srcl)<<  1
Reg(MD)  = Reg(MD)c< 1
Description
This  is  one  step  of  a  l-bit  shift  and  add  multiplication  algorithm. The multiplication scheme  is described  in
Appendix  IV.

m s t ep

Multiply  Step

mstep

44

4.4.5.  sub  - Subtract

Comp Func(  12)
Dest
Src2
Srcl
TY 
OP
01~100   ““1”“I’9’91000001100110[

Assembler
sub rSrc  1 ,rSrc2,rDest
Operation
Reg(Dest) * Reg(Src1)  - Reg(Src2)
Description
The Source 2 register is subtracted from the Source 1 register and the difference is stored in the Destination register.

sub

Subtract

sub

4.4.6. subnc - Subtract with No Carry In

45

TY 
OP
0111OOl   “

Srcl
“

I

src2
  “

“

Dest
Camp  Func(  12)
I  ““1000000100110[

Assembler
subnc rSrc  1 ‘rSrc2,rDest
Operation
Reg(Dest) e Reg(Src1)  + Reg(Src2)
Description
The  l’s  complement  of  the  Source  2  register  is  added  to  the  Source  1  register  and  the  result  is  stored  in  the
Destination register. This instruction is used when doing multiprecision subtraction.

The following  is an example of double precision subtraction. The operation required  is C  = A - B,  where A, B and
C are double word values.
subnc
rAhi,rBhi,rChi
rAlo,rBlo,ll
bhssq

addi
nw
sub

rChi,#l,rChi

rAlo,rBlo,Clo

11:

;subtract high words
;check if subtract of low
-words
generates  a  carry
I
;branch if carry set
;add  1  to  high  word  if  carry

;subtract low words

subnc

Subtract with No Carry  In

subnc

46

4.4.7. and - Logical And

Src2
Srcl
TY 
OP
OllOOl   “ “ I  ‘)  ‘)  I  9  “‘1

0

Dest
0
0

0

0

0

1

Comp Func(  12)
0
0
0
1

1~

Assembler
and rSrc  1 ,rSrc2,rDest
Operation
Reg(Dest) = Reg(Src1)   bitwise  and Reg(Src2)
Description
This is a bitwise  logical and of the bits in Source 1 and Source 2. The result is placed in Destination.

and

Logical  And

and

4.4.8.  bit   -  Bit  Clear

47

Comp Func(  12)
Dest
src2
Srcl
TY 
OP
o111oo1”“1”“l”“loooooooo1o11[

Assembler
bit   rSrcl,rSrc2,rDest
Operation
Reg@est)  c=  Reg(Srcl)  bitwise  and Reg(Src2)
Description
Each bit that is set in Source 1 is cleared in Source 2. The result is placed in Destination.

bit

Bit Clear  .

bit

I

49

4.4.10.  or  -  Logical  Or

TY 
OP
Func(  12)
01]100~““1”“1”“1000000111011[

Assembler
or rSrc  1 ,rSrc2,rDest
Operation
Reg(Dest) e= Reg(Src1)  bitwise  or Reg(Src2)
Description
bitwise  logical or of the bits in Source 1 and Source 2. The result is placed in Destination.

or

Logical  Or

This is a 
or

50

4.4.11.  xor  -  Exclusive  Or

OP
TY 
JO 1 1 10

01

9

Srcl
9
9

Src2
9
9

Dest
9
9

9

I

9

9

I

9

9

IO 0

0

Comp Func(  12)
0 0 0
0 1 10

1

l[

Assembler
xor rSrc  1 ,rSrc2,rDes  t
Operation
Reg(Dest) e Reg(Src1)  bitwise  exclusive-or Reg(Src2)
Description
This  is  a  bitwise   exclusive-or  of  the  bits  in  Source  1  and  Source  2.  The  result  is  placed  in  Destination.

xor

Exclusive  Or

xor

4.4.12.  mov  -  Move  Register  to  Register

51

Comp Func(  12)
Dest
Srcl
TY 
OP
Jol~loo~““~oooool”“loooooco11ool~

Assembler
mov rSrc  1 ,rDes t
Operation
Reg(Dest) (= Reg( Srcl)
Description
This is a register to register move. It is implemented as
add  rSrc  l,rO,rDest .
This mnemonic is provided for convenience and clarity.

mov

Move  Register  to  Register

mov

52

4.4.13.  asr  -  Arithmetic  Shift  Right

TY 
OP
j0 110 0

11

’

Srcl
’

’

’

IO 0

0

0 01

’

Dest
’
’

’

IO 0

0

Comp Func(  12)
1 Olb b
b

d d d d[

Assembler
asr  rSrcl,rDest,#shift  amount
Operation
Reg(Dest)  e  Reg(Srcl)>>   shift amount  (See  below  for  explanation  of  shifr   amount)
The high order bits are sign extended.
Description
The  contents  of  Source  1  are  arithmetically  shifted  right  by  shift  amount. The  sign of  the  result  is  the  same as  the
sign of Source 1 s The result is stored in Destination. The range of shifts is from 1 to 32.

To determine  the encoding  for  the shift amount,  first  subtract  the  shift amount  from  32. The  result  can  be  encoded  as
5 bits. Assume the 5-bit encoding is bbbef, where bbb is used in the final encoding. The bottom two bits (&I  are fully
decoded to yield &i&f  in the following way:
ef
00
01
10
1 1

dddd
0001
0010
0100
1000

__

For example, to determine the bits required to specify the shift amount for the shift instruction
asr r4s3,#5
first  do  (32-5)  to  get  27  and  then  encode  27  according  to  the  above  to  get  1101000.

asr

Arithmetic  Shift  Right

asr

4.4.14.  rotlb  -  Rotate  Left  by  Bytes

i

53

Comp Func( 12)
Dest
Src2
TY 
Srcl
OP
011001I”“I”“I”“1000011000000[

Assembler
rotlb  rSrcl,rSrc2,rDest
Operation
Reg@est)  = Reg(Src1)  rotated left by Reg(Src2)<30..31>  bytes
Description
This  instruction  rotates  left  the  contents  of  Source  1  by  the  number  of  bytes  specified  in  bit  30  and  bit  31  of  Source  2.
For  example,
Reg(Src1)  = ABOlCD23#16
Reg(Src2) = 51#16

rotlb rSrc  1 ,rSr&Dest

Reg(Dest) = OlCD23AB#16

rotlb

Rotate  Left  by  Bytes

rotlb

54

4.4.15.  rotlcb  -  Rotate  Left  Complemented  by  Bytes

3

TY 
0 

OP
1100 

Srcl
11  ” 

Src2
I 

Dest
”  ” 

Comp Func(  12)
I  ”  ” 
10000 

’ 

* 

10000000~

Assembler
rotlcb rSrc  1 ,rSrc2,rDest
Operation
Reg(Dest) c==  Reg(Src1)  rotated left by BitComplement[Reg(Src2)<30..31>]  bytes
Description
This instruction rotates left the contents of Source 1 by the number of bytes specified by using the bit complement of
bits 30 and 31 in Source 2. For example,
Reg(Src1)  = ABOlCD23#16
Reg(Src2) = 51#16

rotlcb rSrc  1 ,rSrc2,rDest

Rotate amount is Bit-Complement of 01#2  = 10#2  = 2.
Reg(Dest) = CD23AB01#16

rotlcb

Rotate  Left  Complemented  by  Bytes

rotlcb

56

4.4.17. nop  - No Operation

TY 
JO 111

OP
9 0 10

0

00

0 10

0

0

0 0 10

0

0

0 010 0

0

0

Camp Func(  12)
1
0
0
0

10

0

noP
Operation
t= Reg(0) + Reg(0)
Description
This instruction does do not much except take time and space. It is implemented as
add rO,rO,rO

noP

4.5. Compute  Immediate  Instructions
The compute  immediate  instructions have one source and one destination  register. They provide a means  to  load a
17-bit  constant that is stored as part of the instruction. Some of the instructions are used to access the special registers
described in Section 2.3. In general, instructions that do not fit in with any of the other groups are placed here.

e

57

4.51.  addi  -  Add  Immediate

I

Assembler 
_
addi Src 1 ,#Immed,Dest
Operation
Reg@est)  C= SignExtend(Immed) + Reg(Src1)
Description
The value of the signed immediate constant is added to Source 1 and the result is stored in Destination.

addi

Add  Immediate

addi

i

59

4.5.2.   jpc   -  Jump PC

TY 
Jl
111

OP
0 110 0

0

0 010 0

0

0 0 10

0

0

0 010 0 0

Camp  Func(  12)
0 0
0
0
0

0

0

1

l(

Assembler
jpc
Operation
PCnext e= PC-4
Description
The PC chain should have been loaded with the 3 return addresses. PCnext is loaded with the contents of PC-4
which should contain a return address used for returning from an exception to user space.

This instruction should be the second and third of 3 jumps using the addresses in the PC chain. The first jump in the
sequence should be jpcrs  which also causes some state bits to change.

jpc

Jump PC

.iPc

60

4.53.  jpcrs   -  Jump PC  and   Restore  State

i

Jl  111   1  110  6  0  0  010  0  0  0  010   0  0  0  010  0  0  0  0  0  0  0  0  0  1  11

Assembler
jpcrs
Operation
PC shifting enabled
PSWcurrent = PSWother
PCnext = PC-4
Description
The PC chain should have been loaded with the 3 return addresses. PCnext  is loaded with the contents of PC-4
which should contain the frost  return address when returning from an exception to user space.

This instruction should be the fast  of 3 jumps using the addresses in the PC chain. The next two instructions should
be jpcs to jump to the 2 other instructions needed to restart the machine.

The machine  changes  from  system  to  user  state  at  the  end  of  the ALU  cycle  of  the  jpcrs  instruction.  -The  PSW  is
changed at this time as well.

When this instruction is executed in user state, the PSW is not changed The effective result is a jump using the
contents  of  PC-4  as  the  destination  address.

jpcrs

Jump PC and Restore State

jpcrs

4.54.   jspci   -  Jump  Indexed   and  Store  PC

61

Immed(  17)
TY 
Dest
Srcl
OP
~,loo()l,,,,I,“‘I  v,,,,*,,,,,,,,,,

Assembler
jspci  rSrcl,#Imme&rDest
Operation
PC c= Reg(Src1)  + SignExtend(Immed)
Reg(Dest) e= PCcment + 1
Description
This  instruction  has  two  delay  slots. The  address  of  the  instruction  after  the  two  &lay  slots  is  stored  in  the
Destination register. This is the return location. The immediate value is sign extended and added to the contents of
Source  1.  This  is  the  jump  destination  so  it  is  jammed  into  the  PC. The displacement is a 17-bit signed word
displacement.

This instruction provides a fast linking mechanism to subroutines that are called via a trap vector.

jspci

Jump Indexed and Store PC

jspci

62

4.5.5. movfrs   - Move  from   Special  Register

TY 
11
110

OP
1

110 0

0

0 01

9

Dest
’
’

’

10

0

0

0 010 0

0 0

Camp Func(  12)
0 0 0
0 01

s

s

[

Assembler
movfrs  SpecialReg,rDest
Operation
Reg@est)   c= Reg(Spec)
Description
This instruction is used to copy the special registers described in Section 2.3 into a general register. The contents of
the special register are put in the destination register. The value used in the Spec  field for each of the special registers is
shown in the table below along with the assembler mnemonic.

SpecialReg

Psw
md
pcm4

spec

001
010
100

__

The PW  (psw)  can be read in both system and user state.

A move from pcm4 causes the PC chain to shift after the move.

movfrs

Move  from  Special  Register

movfrs

4.5.6.  movtos   - Move  to   Special  Register

63

TY 
OP
110 101 ’
11

Srcl
’

’

’

IO 0

0

0 0 10

0

0

0 010 0

0 0

Camp Func(  12)
0
0 0
0 01

[

’
9
spec

Assembler
movtos rSrc  1 ,SpecialReg
Operation
Reg( Spec)  G=  Reg( Src 1)
Description
This instruction is used to load the special registers described in Section 2.3. The contents of the Source 1 register is
put in the special register. The value used in the Spec  field for each of the special registers is shown in the table below
along with the assembler mnemonic.

SpecialReg

Psw
md
pcml

spec

001
010
100

._

Accessing the PSW  (pw)  requires the processor to be in system state. Otherwise the instruction is a nop in user
state.

A move topcml causes the PC chain to shift after the move.

After a move to md,  one cycle may be needed before an mstart  or mstep  instruction to settle some control lines to the
ALU.

movtos

Move  to Special Register

movtos

64

4.5.7.  trap   - Trap Unconditionally

TY 
111

Jl

OP
1 0 10

0

0

0 010 0 0

0 010 0 0

0 0 01

9

9

Vector(8)
9
9
9

’

9

IO 1

11

Assembler
trap  Vector
Operation
Stop  PC  shifting
PC * Vector << 3
PSWother = PSWcurrent
Description
The shifting of  the PC chain  is stopped and  the PC  is  loaded with  the contents of  the Vector  field shifted  left by 3
bits. The PSW of the user space is saved.

This  is an unconditional  trap. The  instruction  is used  to go  to a  system  space  routine  from user  space. The  state of
.-
the machine changes from  user to system after the ALU cycle of the trap instruction.

The trap instruction cannot be placed in the first delay slot of a branch, jspci, jpc, or jpcrs  instruction. See  Appendix
VI  for  more  details.

The  assembler  should  convert  Vector  to  its  one’s  complement  form  before  generating  the machine  instruction.  ie.,
the machine instruction contains the one’s complement of the vector.

trap

Trap  Unconditionally

trap

3

65

4.58.  hsc   -  Halt   and  Spontaneously  Combust

TY 
OP
110 0
11

111

1

1

1

1 10

0

0

0 010 0

0

0

0

0

0 0 0

0

0

0

0

0 0 0 01

Assembler
hsc
Operation
Reg(31) e= PC
The  processor  stops  fetching  instructions  and  self  destructs.
Note  that  the  contents  of  Reg(31)  are  actually  lost.
Description
This is executed by the processor when a protection violation is detected. It is a privileged instruction available only
on the -NSA  versions of the processor.

hsc

Halt  and  Spontaneously Combust

hsc

Halt  and  Spontaneously Combust

hsc

I

67

Appendix   I
Some  Programming   Issues
This appendix contains some programming  issues  that must be stated but have not been  included elsewhere  in  this
document.
1. Address 0 in both system and user space should have a nop instruction. When an exception occurs during
a  squashed  branch,  the  PCs  for  the  instructions  that  have  been  squashed  are  set  to  0  so  that when  these
instructions are restarted  they will not affect any state. The  nop at address 0  is also convenient for some
sequences when it is necessary to load a null instruction into the PC chain.
2. The instruction cache contains valid bits for each of the 32 buffers. There is also a bit to indicate whether
the buffer contains  system or user  space  instructions. When  it  is  necessary  to  invalidate  the  instruction
cache entries for a context switch between user processes, a system space routine is executed that jumps to
32 strategic locations to force all of the system bits to be set in the tags. Thus when the new user process
begins, the cache is flushed of the previous user process. An example code sequence is shown at the end
of  this  appendix.
3.  After  an  interrupt  occurs,  no  registers  should  be  accessed  for  two  instructions  so  that  the  tags  in  the  bypass
registers can be flushed. If a register access is done, then it is possible that the instruction will get values
out of the bypass registers written by the previous context instead of the register file. This should not be a
problem because  the PCs must be saved first anyways. Since this happens in system space, the interrupt
handler  can  just  be  written  so  that  the  improper  bypassing  does  not  occur.
4. There is no instruction that can be used to implement synchronization primitives such as test-and-set. The
proposed method  is  to use Dekker’s algorithm or some other software scheme [3]  but if this proves to be
insufficient  then  a  load-locked  instruction  can be  implemented  as  a  coprocessor  instruction  for  the cache
controller.  This  instruction  will  lock  the  bus  until  another  coprocessor  instruction  is  used  to  unlock  id
This can be used to implement a read-modify-write cycle.
5. A long constant can be loaded with the following sequence:
.data
labell:
. word
.text
Id

labell[rO],r5

OxABCD1234

r5 now contains ABCD1234816
6. If a privileged instruction is executed in user space none of the state bits can be changed This means that
writing the PSW becomes a nop. Reading  the  PSW  returns  the  correct  value. Trying  to  execute  a  jpcrs
only does a  jump  to  the address  in PC-4 and does not change  the PSW. There  is no  trap  taken  for a
privilege violation.
7. Characters can be inserted and extracted with the following sequences:
For each of these examples, assume
r2  initially  contains  stuv
r3  initially  contains  wxyz
where  s,  t,  u,  v,  w,  x, y and z are byte values.

I
; Byte  insertion - byte u gets replaced by w
;

addi
rotlb
sh
rotlcb

rO,#2,rl
r2,rl,r2
r3,r2,r2,#24
r2,rl,r2

; r2  <--  uvst
;  r2  <--  vstw
; r2 <-- stwv

;
i Extract byte  - extract byte u from r2 and place it in r3
I

addi
rotlb
sh

r0,#2,rl
r2,rl,r3
r3,rO,r3,#24

; r3  <--  uvst
; r3 <-- u

Programming  Issues

2&,&q  2!J@b
:_   :
.
. 
. 
.‘.‘. 

jspcl  

ro,xoKleaqrt-l

10x187@ -  1’
* 
)spcl 
lo,KMwJo

10x1880-  -  l5
* 
jspcl 
lo,#oxl94Qla

IoIl8do:

slakenop IS
Jspcl 
lo,#oKl88q!0

3

69

Appendix  II
Opcode   Map
This  is a summary of how  the bits  in  the  instruction opcodes have been assigned. The first sections will show howI
the  bits  in  the  OP  and  Comp  Func   fields  are  assigned.  Then  the  opcode  map  of  the  complete  instruction  set  will  be
given.

11.1. OP Field Bit Assignments
The OP  bits  are  bits  24   in  all  instructions.  For  memory  type  instructions  the  bits  have  no  particular  meaning  by
themselves. For branch type instructions the bits in the OP field (also known as the Corui field) are assigned as follows:
Set to 0 if branch on condition true, set to 1 if branch on condition false
Bit 2
Condition upon which the branch decision is made. 00 = unused, 01 = Z, 10 = C, 11 = N @  V
Bits  3-4
For  compute  type  instructions  the  bits  are  assigned  as  follows:
Set  to  1  if  the ALU  always  drives  the  result  bus  for  the  instruction
Bit  2
Set  to  0
Bit  3
Set  to  1  if  the  shifter  always  drives  the  result  bus  for  the  instruction
Bit  4
For  compute  immediate  type  instructions  the  bits  are  assigned  as  follows:
Set  to  1  if  the ALU  always  drives  the  result  bus  for  the  instruction
Bit  2
These bits have no particular meaning by themselves
Bits  3-4

11.2.   Comp  Func  Field  Bit  Assignments
The  Camp   Func   bits  are  bits  20  through  31  in  the compute  and  compufe   immediate  type  instructions.  The  bits  are
_-
assigned according to whether they are being used by the ALU or the shifter. The bits for the ALU are assigned in the
following way:
Bits  20-22
Bit 23
Bit 24
Bit 25
Bits  26-29

Unused
Set  to  1  for  dstep,  0  otherwise
Set to 1 for multiply instructions (mstart,  mstep), 0 otherwise
CarryintotheALU
Input to the P function block.

3its  30-31

Srcl * Src2
Bit  26
Srcl  .  Src2
Bit  27
Srcl  l Src2
Bit  2%
--
Srcl 0 Src2
Bit  29
Input  to  the  G  function  block.
0  for  ALU  add  operation,  1  otherwise
Bit  30
0  for  ALU  subtract  operation,  1  otherwise
Bit 31
The  bits  for  the  shifter  are  assigned  as  follows:
Unused
Bits  20-21
Set  to  1  for  funnel  shift  operation  (sh  instruction)
Bit  22
Set  to  1  for  arithmetic  shift  operation  (asr  instruction)
Bit  23
Set  to  1  for  byte  rotate  instructions  (rotlb,  rotlcb)
Bit  24

Opcode  Map

70

Bit  25
Bits 2531

For byte rotate instructions, set to 1 if rotlb,  0 if rotlcb
Shift amount  for  funnel and arithmetic shift operations  (sh and asr  instructions). The  range  is 0  to
31  bits.  Although  this  can  be  encoded  in  five  bits,  the  two  low-order  bits  are  fully  decoded;
therefore, the field is seven bits. The two low-order bits are decoded as follows: 0 = bit 31, 1 = bit
30, 2  =  bit  29,  3  =  bit  28.  For  example,  a  shift  amount  of  30 would  become  1110100  in  this
seven-bit encoding scheme.

Opcode Map

Comments
*
*
*

OF
000
010
100
110
001
011
101
111
101

11.3. Opcode Map of All Instructions
Memory Instructions
TY
Instruction
Id
10
st
10
ldf
10
stf
10
ldt
10
stt
10
movfrc
10
movtoc
10
aluc
10
Branch  Instructions
TY
Instruction
00
beq
he
00
bhs
00
blo
00
00
blt
bne
00

COND
001
111
010
110
011
101

Srcl=O, *
Srcl=O
Srcl=O, Dest=O, *

Comments

Srcl=O

Src2=0
Src2=0

OP
100
000
000
000
100
100
100
100
100
100
100
100
001
001
001
001
100

Comp Func
000000011001
000101100110
000011100110
000010011001
000001100110
000000100110
000000100011
000000001011
000000001111
000000111011
000000011011
000000011001
OOOlObbbdddd
000011000000
000010000000
OOlOObbbdddd
000000011001

Compute  Instructions
TY
Instruction
add
01
dstep
01
mstart
01
mstep
01
sub
01
subnc
01
a nd
01
bit
01
not
01
or
01
xor
01
mov
01
asr
01
rotlb
01
rotlcb
01
sh
01
nap
01
Compute Immediate Instructions
OP
TY
Instruction
100
11
addi
000
11
jspci
101
11
jpc
111
11
jpcrs
11
011
movfrs
010
11
movtos
110
11
trap
unused
001
11
A star (*)  indicates an  instruction  that has  its Desf  field  in  the position where  the  Src2  field normally sits. This can
also be determined by decoding the MSB of the type field and the middle bit of the OP field.

Comp Func
Immed
Immed
000000000011
000000000011
OOOOOOOOOrrr
OOOOOOOOOrrr
0vvvvvvvv011

Comments
* (Immed is a 17-bit
*
signed  constant)
*

rrr =  special  register
rrr = special register
Srcl-0, vvvvvvvv=vector

Src2=0
Src2=0, bbbdddd=rotate

amount

bbbdddd=rotate  amount
Srcl=O,  Src2=0,  Dest=O

Opcode   Map

Opcode Map

i

7 3

Appendix   Ill
Floating  Point  Instructions
This  describes  the  floating  point  opcodes  and  formats  of  the  instructions  implemented  in  the MIPS-X  Instruction
Level Simulator (milsx).

111.1.   Format
All  floating  point  numbers  are  represented  in  one  32-bit word  as  shown  in  Fig.  III-l.  The  fields  represent  the
following floating point number:
(-1)”  x 2exp-  12’ x (1 + fraction) q
This  is  an  approximate  IEEE  floating  point  format.

exp (8 bits)
,
,
9

,

,

,

,

S

I 

I

,

,

,

,

,

,

,

,

fraction  (23  bits)
,
,
,
,

9

,

,

,

,

,

1

,

9

,

Figure  III-l: Floating  Point  Number  Format

111.2. Instruction Timing
All floating point instructions are assumed to take one cycle to execute. More  realistic  timing- numbers  can be
derived by multiplying the number output by mifs  by an appropriate constant

111.3.  Load  and  Store  Instructions
There are 16 floating point registers. They are loaded and stored using the Zq and stf  instructions defined in the
instruction set. Moves between  the  floating point  registers and  the main processor are done using  the movif and movfi
instructions. These use the movtoc and movfrc formats defined in the instruction set. Note that only 4 of the 5 bits that
specify a floating point register in the ldf, stf, movif and movfi  instructions are used

111.4. Floating Point Compute Instructions
The  format  of  the  floating  point  compute  instructions  is  the  one  shown  in  the  description  of  the  aZuc   coprocessor
instruction. The coprocessor number  (COP#)   is 0  for  the  floating point coprocessor. The  Func   field specifies  the
floating  point  operation  to  be  performed.

Floating  Point

111.5. Opcode Map of Floating Point Instructions
In  the  following  table:
rl,r2  are cpu registers from rO..r31
fl,f2  are floating point registers from  fO..flS
n is an integer expression
Operation
Func
TY  OP
Instruction
f2 = fl + f2
000000
10 101
fl,f2
fadd
f2 e= fl - f2
000001
10 101
fl,f2
fsub
f2 * fl x f2
000010
10 101
fl,f2
fmul
f2 * fl / f2
000011
10 101
fl,f2
fdiv
f2 e= float(f1)
000100
10 101
fl,f2
cvtif
10  101  000101  f2  e=  int(f1)
fl,f2
cvtfi
000110 f2 = fl x f2
10  101
fl,f2
imul
10 101 000111 f2 e= fl / f2
fl,f2
idiv
10 101 001000 f2  e=  fl  mod  f2
fl,f2
mod
fl c= rl
001001
10 111
rl,fl
movif
rl * fl
10 101
001010
fl,rl
movfi
10 100
ldf  n[rl],fl
10 110
n[rl],fl
stf

Comments
Srcl=O,  Dest=O
Srcl=O, Dest=O
Srcl-0, Dest-0
Srcl-0,   Dest-0
Srcl-0,  Dest=O
Convert  int  to  float
Srcl-0, Dest-0
Convert  float  to  int
Srcl=O, Dest-0
Integer  multiplication
Srcl-0,  Dest=O
Integer  division
Srcl=O, Dest=O
Integer  mod
Srcl-0,  CSl-0
Srcl-0, CS2=0
See  instruction  page
See  instruction  page

Floating  Point

7 5

Appendix   IV
Integer  Multiplication   and  Division
This appendix describes the multiplication and division support on MIPS-X. The philosophy behind why the current
implementation was  chosen  is  described  first  and  then  the  instructions  for  doing multiplication  and  division  are
described.

WI.  Multiplication  and  Division  Support
The goal of the multiplication and division support in MIPS-X is to provide a reasonable amount of support with the
smallest  amount  of  hardware  possible.  Speed  ups  can  be  obtained  by  realizing  that  most  integer  multiplications  are
used to obtain a 32-bit  result, not a 64-bit  result. The result is usually the input to another operation, or it is the address
of  an  array  index.  In  either  case  a  number  larger  than  32  bits would  not make  sense.  Since  the  result  is  less  than  32
bits, one of  the operands  is most  likely  to be  less  than 16 bits or  there will be an overflow.  In general  this means  that
only about 16 l-bit multiplication or division steps are required to generate the final answer. For very small constants,
instructions can be generated  inline   instead of using a general multiplication or division routine. Therefore,  it was felt
that  there was no great advantage  to  implement a  scheme  that could do more  than 1 bit at a  time  such as Booth
mu1  tiplication.

The other advantage of only generating a 32-bit  result is that it is possible to do multiplication starting at the MSB of
the multiplier meaning that the same hardware can be used for multiplication and division. The required hardware is a
single register, the MD register, that can shift left by one bit each cycle, and an additional multiplexer at the source 1
input  of  the  ALU,  that  selects  the  input  or  two  times  the  input  for  the  source  1  operand.

IV.2. Multiplication
Multiplication  is done with  the simple  l-bit shift and add algorithm except  that  the computation  is started  from  the
most significant bit instead of the least significant bit of the multiplier. The instruction that implements one step of the
algorithm  is called  rnstep.   For
mstep rSrc  1 ,rSrc2,rDest
the  operation  is:
If the MSB of the MD  register is 1
then
rDest   e=  2 x rSrc1 + rSrc2
else
rDest  = 2 x rSrc1

Shift  left  MD

For signed multiplication, the first step is different from the rest. If the MSB of the multiplier is 1, the multiplicand
should be subtracted from 0. The instruction called mturt is provided for this purpose. For
mstart  rSrc2,rDes  t
the  operation  is

*  Multiplication  and  Division

76

If  the MSB  of  the MD  register  is  1
then
rDest  e= 0 - rSrc2
else
rDest  C= 0

Shift  left  MD

To show the simplest implementation of a multiplication routine assume that the following registers have been
assigned  and  loaded
rMer  is the multiplier,
rMund  is the multiplicand,
rDest  is the result register
rLink  is the jump linkage register.
Then,
m o v t o s
nw
mstart
mstep
jspci

;Move  the multiplier
;Needed  for hardware
;Do   the  first  mstep.
;Repeat  31 times
;Return

into  MD
timing  reason s--see  movtos
Result  goes into rDest

rMer,rMD

rMand,rDest
rDest,rMand,rDest
rLink,#O,rO

It is possible to speed up the routine by using the assumption described previously that the numbers will not both be
a full 32 bits long. The simplest scheme is to check to see if the multiplier is less than 8 bits long. Some statistics
indicate  that  this  occurs  frequently.

The routine shown in Figure IV-1 implements multiplication with less than 32 msteps  on average. It will actually do
a full 32 msteps  if it is necessary. In this case it is most likely that overflow will occur and this can be detected if the V
bit in the PSW is clear so that a trap on overflow will occur. Assume that the registers rMer,  rMand  anii  rDest  have
been assigned and loaded as in the previous example. Two temporary registers, rTemp1  and rTemp2 are also required

The number of cycles  required, not  including  the  instructions needed  for  the call sequence  is shown  in Table  IV-l.
Compare  this with  the simple  routine using  just 32 steps which  requires 35  instructions  to do  the multiplication and a
Booth  2-bit  algorithm  that will need about 19  instructions.  It can be observed  that  if most multiplications  require 8 or
less rnsteps,  then this routine will be faster than just doing 32 msteps all the time.

IV.3.  Division
For division, the same set of hardware is used, except the ALU is controlled differently. The algorithm is a restoring
division algorithm. Both of the operands must be positive numbers. Signed division is not supported as it is too hard to
do for the hardware required [2].

The dividend  is  loaded  in  the MD  register and  the register  that will contain  the remainder  (&em)   is  initialized  to 0.
The divisor is loaded into another register called (rDor).  The result of the division (quotient) will be in MD. For
ds  tep  rRem,rDor,rRem
the  operation  is:

Multiplication and  DiGsion

I

7 7

;i
;
;
;
;
;
;
r’
i

MUL

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .
i;;;ii;;i;;;;i;iirrr,~~~**,,,*,,,,*,,,,,~,,.*,,,,,,**,~*,,,,*,,,*,*,.**
.,
i
;
;
;
i
:
:
;
i

fast, unchecked, signed  multiply
rLink = link
rMand  =  src2
rDest   =  rMer   =  srcl/dest
rTemp1  =  temp
rTemp2  =  temp

Note:

This code has been reorganized

MUL:

asr
bne
sh
movtos
mstart
mstep
lmul8bit:
mstep
mstep
mstep
mstep
jspci
mstep
mstep

lnot8:

rMer,rTemp2,#7
rTemp2, rO,lnot8
rO,rMer,rTempl,#24
rTempl,md
rMand,rDest
rDest,rMand,rDest

rDest   ,rMand,rDest
rDest,rMand,rDest
rDest,rMand,rDest
rDest,rMand,rDest
rLink,#O,rO
rDest   ,rMand,rDest
rDest,rMand, rDest

rTemp2,#l,rTemp2
rTemp2,rO,lmul8bit
rMand, rDest
rDest,rMand,rDest
rDest,md
rMand,rDest
rDest,rMand,rDest
rDest,rMand,rDest
rDest,rMand,rDest
rDest,rMand,rDest

; Test  for  positive  8-bit  number

;  assume  8  bit

; may  need  nop  before  this

; 8  bit  negative

;  do  full  32  bits
; may  need  nop  before  this

addi
beqsq
mstart
mstep
movtos
mstart
mstep
mstep
mstep
mstep

24  msteps

mstep
jspci
mstep
mstep

rDest,rMand,rDest
rLink,#O,rO
rDest,rMand,rDest
rDest,rMand,rDest

Figure  IV-l: Signed  Integer  Multiplication

Multiplication  and  Division

78

Number of msteps needed

Number of cycles with positive multiplier
Number of cycles with negative multiplier

8

13 
15 

32

42
42

Table  IV-l: Number of Cycles Needed to do a Multiplication
Set ALUsrcl input to 2 x rRem + MSB(rMD)
Set ALUsrc2 input to rDor
ALUoutput  * ALUsrcl   - ALUsrc2

If  MSB(ALUoutput)  is  1
then
rRem = ALUsrcl
rMDr-2xrMD
else
rRem  (= ALUoutput
rMD~2xrMD+l

At the end of 32 dsteps  the quotient will be in the MD register, and the remainder is in rRem.

A  routine  for  doing  division  is  shown  in Figure  IV-2. The  dividend  is  passed  in  rDend   and  the divisor  in  rDor.  At
the end, the quotient is in MD and rQuot  and the remainder is in rRem. Note  that  rDend   and  rRem   can  be  the  same
register, and rDor  and rQuot  can be the same register. The  dividend  and  divisor  are  checked  to  make  sure  they  are
positive. This routine does a 32-bit by 32-bit division so no overflow can occur.

The number of cycles needed, not including the calling sequence and assuming the operands are positive, is shown in
Table  IV-2.

Number of dsteps  needed

Number of cycles needed

8

34 

32

60

Table IV-2: Number of Cycles Needed to do a Divide

Multiplication  and  Division

.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .
,~,,,,*,*,**,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,*,,*,,,,,,,,,,,,,,,,,,,*,

79

0

fast, unchecked, signed  divide  (should  check  for  zero  divide)
rLink  =  link
rDend,rRem = srcl
rDor  =  rQuot   =  src2/dest
(trashed)
rTemp1  =  temp
(trashed)
rTemp2  = temp

;
;
; DIV
i
;
i
:
i
(dividend)
;
;
( d i v i s o r / q u o t i e n t )   ;
;
;
i
;
;
;
;
This code has been reorganized
Note:
;
;
;
;
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .
;;;;;;*l*,,**,*,,,,*,,,*,,*,,,,,,,,,*,,***~~*,,**,,~,~~~~~*,,**,,~~~,~,*,
DIV:

rDend,rTemp2
rDend,rO,lcinitl

; dividend  >  0  ?

lcinitl:

mov
he
nap
nw
sub

M-q
addi
nw
sub
sub
addi

lcinit2:

bltsq
movtos
mdv
sh
movtos
beq
mov
addi
ldivfull:
addi
ldivloop:
dstep
dstep
ldivloopr:
dstep
dstep
dstep
dstep
dstep
addi
dstep
bnesq
dstep
dstep
movfrs
Jw=
nap
nap
sub

lcinit3:

jspci
nw
nap

rO,rDend,rDend

rDor,rO,lcinit2
rO,#Oxff,rTempl

rO,rTemp2,rTemp2
rO,rDor,rDor
rO,#Oxff,rTempl

rTempl,rDend,ldivfull
rDend,md
r0 ,rRem
rO,rDend,rDend,#8
rDend,md
rO,rO,ldivloop
rO,rRem
rO,#8,rTempl

'rO,#32,rTempl

rRem,rDor,rRem
rRem,rDor,rRem

rRem,rDor,rRem
rRem,rDor,rRem
rRem,rDor,rRem
rRem,rDor,rRem
rRem,rDor,rRem
rTempl,#-8,rTempl
rRem,rDor,rRem
rTempl,rO,ldivloopr
rRem,rDor,rRem
rRem,rDor,rRem
md,rQuot
rTemp2,rO,lcinit3

; make dividend > 0

;  divisor  >  0  ?
;  check  for  8-bit  dividend

; rTemp2 > 0 if positive result
; make divisor  >  0

;  do  8-bit  check
;  start  32-bit  divide

; shift  up  divisor  to  do  8  bits
;  start  8-bit  divide

; loop  counter

; do  full  32  dsteps

; decrement  loop  counter

: get  result
; check  if  need  to  adjust  sign  of  result

rO,rQuot,rQuot

rLink,#O,rLink

; adjust sign  of result
; return

Figure W-2: Signed  Integer  Division

Multiplication  and  Division

Multiplication  and  Division

i

81

Appendix  V
Multiprecision   Arithmetic
Multiprecision  arithmetic  is  not  a  high  priority  but  it  is  desirable  to  make  it  possible  to  do.  The  minimal  support
necessary will be provided. The most straightforward way to do this would seem to be the addition of a carry bit to the
PSW. However, this turns out to be extremely difficult.

The  following  program  segments  are  examples  of  doing  double  precision  addition  and  subtraction.  The  only
addition required to the instruction set is the Subtract with No Carry (subnc) instruction. This is only an addition to the
assembly language and not to the hardware.

Assume that there are 2 double precision operands (A and B) and a double precision result to be computed (C).
Assume that the necessary registers have been loaded.
;Double  precision  addition

add
sub
bhssq

addi
nw
add

rAhi,rBhi,rChi
rO,rBlo,rClo
rAlo,rClo,ll

rChi,#l,rChi

rAlo,rBlo,rClo

11:

*Double   precision  subtraction
I

subnc
bhssq

rAhi,rBhi,rChi
rAlo,rBlo,ll

addi
nap
sub

rChi,#l,rChi

rAlo,rBlo,Clo

11:

;add  high  words
;get -rBlo;   branch  does  subtract
;check  to see if carry generated
;branch if carry set
;add  1  to  high  word  if  carry

;add  low  words

;subtract high words
;check if subtract of low
*words  generates a carry
I
;branch  if  carry  set
;add  1  to  high  word  if  carry

;subtract low words

Multiprecision  Arithmetic

82

Multiprecision  Arithmetic

83

Appendix  VI
Exception   Handling
An exception  is defined as either an event  that causes an  interrupt or a  trap  instruction  that can be  thought of as a
software interrupt. The two sequences cause similar actions in the processor hardware. Because there is a branch delay
of 2, three PCs from the PC chain must be saved and restarted on an interrupt. Three PCs are needed in the event that a
branch has occurred and fallen off the end of the chain. The two branch slot instructions and the branch destination are
saved for restarting. Restarting a trap is slightly different and is explained later. See Section 2.4 for a description of the
PSW  during  interrupts,  exceptions,  and  traps.

VIA.  Interrupts
Interrupts are asynchronous events  that  the programmer  has no control over. Because  there are several  instructions
executing at the same time, it is necessary to save the PCs of all the instructions currently executing so that the machine
can be properly restarted after an interrupt. The PCs are held in the PC chain. When an interrupt occurs, the PC chain
is frozen (stops shifting in new values) to allow the interrupt routine to save the PCs of the three instructions that need
to be restarted These are the PCs of the instructions that are in the RF, ALU and MEM cycles of execution. This
means  that  no  further  exceptions  can  occur  while  the  PCs  are  being  saved. When  the  interrupt  sequence  begins,  the
interrupts  are  disabled,  PSWcwrent   is  copied  into  PSWother  and  the  machine  begins  execution  in  system  state.  The
contents of PSWother  should be saved if interrupts are to be enabled before the return from the interrupt. The contents
of the MD register must also be saved and restored if any multiplication or division is done.
If the interrupt routine is
very short and interrupts can be left off, it is possible to just leave the PC chain frozen, otherwise the three PCs must be
saved. To save the PCs use movfrs  with PC-4 as the source. The PC chain shifts after each read of PC-4.
_-

The interrupt routine will start execution at location 0.
It must  look at a  register  in  the  interrupt controller  to
determine how to handle the interrupt. This sequence is yet to be specified.

To  return  from  an  interrupt,  interrupts must  first  be  disabled  to  allow  the  state  of  the machine  to  be  restored. The
PSW must be  restored and  the PC chain  loaded with  the  return addresses. The PC chain  is  loaded by writing  to PC-l
and  it  shifts  after  each  write  to  PC-l.  The  instructions  are  restarted  by  doing  three  jumps  to  the  address  in  PC-4   and
having  shifting  of  the  PC  chain  enabled  This means  that  the  addresses will  come  out  of  the  end  of  the  chain  and  be
reloaded at the front in the desired order.

It will cause PSWother to be copied to PSWcurrent with
The first of the three jumps should be a jpcrs  instruction.
the interrupts turned on and the state returned to user space. The machine state changes after the ALU cycle of the first
jump. The last two instructions of the return jump sequence should bejpc instructions.

A  problem  arises  because  an  exception  could  occur  while  restarting  these  instructions.  The  PC  chain  is  now  in  a
state  that  it  is not possible  to  restart  the sequence again using  the standard sequence of  first saving  the PC chain. The
start of an exception sequence should fust  check the e bit in the PSW to see whether it is cleared. The e bit will be set
only when the PC chain is back in a normal state. If it is clear, then the state of the machine should not be resaved. The
state  to use  for  restart  should  still be available  in  the process descriptor  for  the process being  restarted when  the

Exception  Handling

84

lret:

inst
;:
inst
inst
C
--- interrupt ---
d
inst
e
inst

;Instructions  a, b and c are restarted

save:

inthlr:  bra  to  save  if  e  bit  set
Do necessary fixes
bra  nosave
Save  PSWother
Save  MD
movfrs  pcm4,rA
pcm4,rB
movfrs
movfrs  pcm4,rC
nosave: Enable interrupts
.
Process interrupts
.
Disable interrupts
Restore  MD
Restore PSWother
rA,pcml
movtos
rB,pcml
movtos
rC,pcml
movtos
jpcrs
jpc
jpc
execution begins at label lret

;Start  of interrupt handler
;e  bit  clear  so  don't  save  PC  chain

;do  save  if  interrupts  to  be  enabled
*if  necessary
isave  PCs if necessary

;if  necessary  and  above  saving  done

*if  necessary
I
-if  necessary
I
;restore  PCs

-This
changes  the  PSW  as  well
I
;Doesn't  touch  PSW

Figure  VI-l:  Interrupt  Sequence
exception occurred. The sequence for interrupt handling is shown in Figure VI-l.

Vl.2.  Trap On Overflow
A trap on overflow (See Section 2.4.1) behaves exactly like an interrupt except that it is generated on-chip instead of
externally. This interrupt can be masked by setting the V bit in the PSW.

When a  trap on overflow occurs,  the 0 bit  is set  in  the PSW. The exception handling  routine must check  this bit  to
see if an overflow is the cause of the exception.

V1.3.   Trap  Instructions
Besides the Trap on Overflow,  there is only one other type of trap available. It is an unconditional vectored trap to a
system space routine in low order memory. After the ALU cycle of the trap instruction the processor goes into system
state  with  the  PC  chain  frozen.  The  instruction  before  the  trap  instruction  will  complete  its WB  cycle.  The  PSW  is
saved by copying PSWcurrent to PSWother as described in Section 2.4. PSWcurrent  is  loaded as  if  this were an
interrupt.

Exception  Handling

8 5

Before interrupts can be turned on again, some processor state must be saved. The return PCs are currently in the PC
chain. Three PCs must be read from the PC chain and the third one saved in the process descriptor. It is the instruction
that is in the RF cycle. The instruction corresponding to the PC in MEM completes so it need not be restarted. The PC
in the ALU cycle should not be restarted because it is the trap instruction. PSWother must be saved so that the state of
the prior process is preserved. If PSWother is not saved before interrupts are enabled, then another interrupt will smash
the PSW of the process that executed the trap before it can be saved

All trap instructions have an g-bit  vector number attached to them. This provides 256 legal trap addresses in system
space.  These  addresses  are  8  locations  apart  to  provide  enough  space  to  store  some  jump  instructions  to  the  correct
handler.  If  this  is not enough vectors, one of  the  traps can  take a  register as an argument  to determine  the action
required.

The return sequence must disable interrupts, restore the contents of PSWother and MD if they were saved and then
disable PC shifting so that the return address can be shifted into the PC chain. Two more addresses must be shifted in
as well so that the restart will look the same as an interrupt. This can be done by loading the addresses of two nop
instructions into the PC chain ahead of the return address. Three jumps to the addresses in the PC chain are then
executed using jpcrs and twojpcs. The first jump will copy the contents of PSWother into PSWcurrent and turn on PC
shifting. The processor state changes after the ALU cycle of the jpcrs.  The change of state also enables interrupts and
puts  the  processor  in  user  space.

If an interrupt occurs during the return sequence then
determine whether the state should be saved.

the  interrupt handler will look at the  e  bit  in  the PSW  to

The flow of code for taking a trap and returning is shown in Figure VI-2.

Exception  Handling

86

.

trap

vecnum

lret:

pcm4,rO
vecnum:  movfrs
movfrs  pcm4,rO
movfrs  pcm4,r31
Save PSWother
Save  MD
Enable interrupts

iinstruction  before trap
-trap  instruction
#
;save  this one to restart
;if  necessary
*if  necessary
I
*if necessary and above saving done
,

Process  requested  trap
.
Disable interrupts
Restore MD
Restore  PSWother
rO,pcml
movtos
rO,pcml
m o v t o s
r31,pcml
m o v t o s
jpcrs
jpc
jpc
execution begins at label lret

;movtos  x,pswc   where  x  has  M  bit  set
-if  necessary
,
*if  necessary
,
;assume a nop at 0

*instruction
I

after  trap

Figure VI-2: Trap Sequence

Exception  Handling

3

87

Appendix  VII
Assembler   Macros  and  Directives
This appendix’  describes the macros and directives used by the MIPS-X assembler. Also provided is a full grammar
of the assembler for those that need more detail.

*

VII.1. Macros
Several  macros  are  provided  to  ease  the  process  of  writing  assembly  code. These allow low level details to be
hidden, and ease the generation of code for both compilers and assembly language programmers.

VII.1  .I. Branches
The assembler synthesizes these instructions by reversing the operands and using a bit  or a bge
bgt,  ble
instruction.

VII.1.2.  Shifts
lsr, Is1

These instructions are synthesized from the sh instruction. For example:
lsr rl,r2,#4
shifts rl four bits right and puts the result in r2.

VII.1.3.  Procedure  Call and Return
pjsr subroutine,#expl,reg2 A simple procedure call. The stack pointer is decremented by expl.  The return address is
stored on the stack. On return, the stack pointer is restored. Reg2 is used as a temporary.
No  registers  are  saved.

ipjsr reg 1 ,#exp 1 ,reg2
ipj sr exp2,reg  1 ,#expl ,reg2 A call to a subroutine determined at run time. The particular subroutine address must be
in a register (regl) or be addressable off a register (exp2  + regl). The stack pointer and
the return address handling is identical to pjsr.  Reg2 is used as a temporary.
Jump  to  the  return  address  stored  by  a  pjsr   or  ipjsr  macro.

ret

.data

.end
.eop

Vll.2.  Directives
Signals  the beginning or  resumption of  the  text segment. This allows code  to be grouped  into one
.text
area. Labels in the text segment have word values.
Signals the beginning or resumption of the data segment. Labels in the data segment have byte
values. Ordering within the data segment is not changed.
Signals the end of the module.
Signals the end of a procedure. No branches are allowed to cross procedure boundaries. This
directive was  added  to  reduce  the memory  requirements  of  the  assembler. Reorganization  can  be
done by procedure instead of by module.
Allows  a  string  literal  to  be  put  in  the  data  segment..
Initializes a word of memory.

.ascii  “xxx”
.word exp

‘Provided  by  Scott McFarling

Assembler Macros  and  Directives

88

.float number
id = exp

.def id = exp
.noreorg
xeorgon
.comnmi~n
*glob1  id

Jit rl,r2,...
.lif r5,rlO,...

Initializes  a  floating  point  literal.
Sets  an  assembly-time  constant. This allows a code generator  to emit co& before  the value of
certain offsets  and  literals  are  known. The  assembler will  resolve  expressions  using  this  identifier
for aliasing calculations etc.
Sets a link-time constant The identifier will be global.
Allows reorganization to be turned off in local areas.
Turns reorganization back on.
Defines a labeled common area of n words. Common area names are always global.
Makes an  identifier global or accessible outside  the module. The  .globl  statement must  appear
before  the  id  is  otherwise  used.  All  procedure  entry  points  should  be  made  global,  otherwise  the
code may be removed as dead.

Give a list of registers that are live for the following branches. lit is for registers live if the branch
is taken and  .lif is for registers live if the branch is not taken. Liveness information is used for
interblock reorganization and branch scheduling.

Vll.3. Example
;program   1+1  =  2?
.data
labell:
. word 1
.text
.globl  -main
main:
-

error:

.end

trap
ret

1

Id
addi
addi
bne
ret

labell[rO],rl
rl,#l,rl
r0,#2,r2
rl,r2,error

{ comment = ;.*  1

statement

Vll.4. Grammar
.
file
i file line
: 
\n
line
1 COMMENT \n
I statement COMMENT \n
1  statement  \n
:  label
1  binALUState
1  monALUState
I  specstate
I  nopstate
I  addistate
I  jspcistate
I  shiftstate
I  loadstate
I  storestate
I  branchstate
I  copstate
I  miscstate
I  directstate

Assembler Macros  and  Directives

-_

i

89

label
binALUState
binALUOp

monALUState
monOp
specstate
specialReg

nopstate
addistate
jspcistate
shiftstate

loadstate

storestate

branchstate

branchOp

branchSqOp

copstate

I  macrostate
..
{ ID must be in column 1  )
ID  :
..
binALUOp  reg,reg,reg
.
ADD
i
SUB
I
OR
I
XOR
I
ROTLB
I
ROTLCB
I
MSTEP
I
DSTEP
I
SUBNC
I
BIC
I.
monOp reg,reg
i.
MSTART reg,reg
NOT
i.
MOV
MOVTOS reg,specialReg
i.
MOVFRS   specialReg,reg
MD
i
PSW
PCM4
I
PCMl
I..
NOP
..
ADDI   reg,#exp,reg
..
JSPCI  reg,#exp,reg
.
ASR  reg,reg,#exp
i
SH  reg,reg,reg,#exp
LSR  reg,reg,#exp
I
LSL  reg,reg,#exp
I.
LD  exp[reg],reg
i
LD  #exp,reg
{ adds constant to literal pool and loads it  1
I LDT exp[reg],reg
I LDF exp[reg],freg
.
i ST  exp[reg],reg
STT exp[reg],reg
I STF exp[reg],freg
.
i branchop  reg,reg,ID
branchSqOp reg,reg,ID
I BRA  ID
.
i BEQ
BNE
I BGE
I BGT
I BHI
I BHS
I BLE
I BLO
I BLS
I BLT
.
i BEQSQ
BNESQ
I BGESQ
I BGTSQ
I BHISQ
I BHSSQ
I BLESQ
I BLOSQ
I BLSSQ
I BLTSQ
.. MOVTOC exp,reg

Assembler Macros  and Directives

.-

( string:  ,re*m  )

I MOVFRC exp,reg
I ALUC exp
I  floatBinOp   freg,freg
I  floatMonOp   freg,freg
I  MOVIF  reg,freg
MOVFI freg,reg
FADD
FSUB
FMUL
FDIV
IMUL
IDIV
MOD
CVTIF
CVTFI
TRAP  exp
JPC
JPCRS
TEXT
DATA
END
EOP
ASCII STRING
WORD exp
FLOAT FLOATCONSTANT
ID = exp
=  exp
DEF  ID
REORGON
NOREORG
COMM ID,INT
GLOBL  ID
LIT liveList
LIF liveList
r-3
liveList,reg
PJSR ID,#exp,reg
li
IPJSR  reg,#exp,reg
IPJSR  exp,reg,#exp,reg
I
RET
I.
exp addOp term
i
- factor
term
I.
+
i.
term multOp factor
i
factor
*
.
:  ( exp )
I  ID
1 INT
I  HEXINT
: REG
:  FREG

90

floatBinOp

floatMonOp
miscstate

directstate

liveList

macrostate

ew

addOp
term
multOp
factor

reg
freg

notes:

( like C:  Oxl2fc   )
{  rO..r31  )
{  fO..f15  1

1) only labels and directives may start in column 1
2) Keywords are shown in upper case just to make them
In reality, they  MUST  be  lower  case.
stand out.
3) directives begin with a  ' Of

Assembler Macros  and  Directives

91

References

PI

PI

131

Cohen, Danny.
On Holy Wars and a Plea for Peace.
IEEE Computer  14(  10):48-54,  October, 1981.
Gill, J., Gross, T., Hennessy, J., Jouppi, N., Pxzybylski, S. and Rowen,  C.
Summary   of  MIPS  Instructions.
Technical  Note  83-237,  Stanford  University,  November,  1983.
Lamport,  Leslie.
A  Fast  Mutual  Exclusion  Algorithm.
Technical  Report  7,  DEC  Systems  Research  Center,  November,  1985.

See	discussions,	stats,	and	author	profiles	for	this	publication	at:	http://www.researchgate.net/publication/2954925

Should	computer	scientists	experiment	more?
Computer,	31(5),	32-40

ARTICLE		in		COMPUTER	·	JUNE	1998

Impact	Factor:	1.44	·	DOI:	10.1109/2.675631	·	Source:	IEEE	Xplore

DOWNLOADS
55

VIEWS
109

CITATIONS
319

1	AUTHOR:

Walter	Tichy
Karlsruhe	Institute	of	Technology

235	PUBLICATIONS			4,333	CITATIONS			

SEE	PROFILE

Available	from:	Walter	Tichy
Retrieved	on:	18	September	2015

Should Computer Scientists Experiment More?
 Excuses to Avoid Experimentation
Walter F. Tichy
University of Karlsruhe, Germany
Nov. 		
Abstract
puters and programs are human creations, so
we could conclude that computer science is not
Computer scientists and practitioners defend
a natural science in the traditional sense.
the lack of experimentation with a wide range
I think that the engineering view of com-
of arguments. Some arguments suggest that
puter science is too narrow, too computer-
experimentation may be inappropriate, too
myopic. First of all, the primary sub jects of
di(cid:14)cult, useless, and even harmful. This ar-
inquiry in computer science are not merely
ticle discusses several such arguments to illus-
computers, but information and information
trate the importance of experimentation for
processes[]. Computers play a dominant role
computer science.
because they make information processes eas-
This is a preprint of an article with the same
ier to model and observe. However, by no
title that appeared in IEEE Computer, (),
means are computers the only place where in-
May 		, { .
formation processes occur. In fact, computer
Keywords: Empiricism, experiments, labo-
models compare poorly with information pro-
ratory, scienti(cid:12)c method.
cesses found in nature, say in nervous systems,
in immune systems, in genetic processes, or,
if you will, in the brains of programmers and

Is computer science an
computer users. The phenomena studied in
computer science are much broader than those
experimental science?
arising around computers.
Do computer scientists need to experiment at
Regarding \syntheticness", I prefer to think
all? Only if we answer \yes" does it make
about computers and programs as models.
sense to ask whether there is enough of it.
Modeling is in the best tradition of science,
In his Allen Newell Award Lecture, Fred
because it helps us study phenomena closely.
Brooks suggests that computer science is \not
For example, for studying lasing, one needs
a science, but a synthetic, an engineering
to build a laser. Regardless of whether lasers
discipline"[].
In an engineering (cid:12)eld, test-
occur in nature, building a laser does not
ing theories by experiments would be mis-
make the phenomenon of massive stimulated
placed. Brooks and others seem troubled by
emission arti(cid:12)cial. Superheavy elements must
the fact that the phenomena studied by com-
be synthesized in the lab for study, because
puter scientists appear manufactured | com-
they are unstable and do not occur naturally,



yet nobody assumes that particle physics is
synthetic. Similarly, computers and software
don't occur naturally, but they help us model
and study information processes closely. Us-
ing these devices does not render information
processes arti(cid:12)cial.
A ma jor di(cid:11)erence to traditional sciences is
that information is neither energy nor matter.
Could this di(cid:11)erence be the reason we see lit-
tle experimentation in computer science? To
answer this questions, let's look at the purpose
of experiments.
 Why should we experi-
ment?
When I discuss the purpose of experiments
with mathematicians, they often exclaim that
experiments don't prove a thing.
It is true
that no amount of experimentation provides
proof with absolute certainty. What then are
experiments good for? We use experiments for
theory testing and for exploration.
Experimentalists test
theoretical predic-
tions against reality. A community gradually
accepts a theory if all known facts within its
domain can be deduced from the theory, if
it has withstood numerous experimental tests
and if it correctly predicts new phenomena.
Nevertheless, there is always an element of
suspense: To paraphrase Dijkstra, an experi-
ment can only show the presence of bugs in a
theory, not their absence. Scientists are keenly
aware of this uncertainty and are therefore
ready to shoot down a theory if contradicting
evidence comes to light.
A good example of theory falsi(cid:12)cation in
computer science is the famous Knight-and-
Leveson experiment[]. The experiment was
concerned with the failure probabilities of
multi-version programs. Conventional the-
ory predicted that the failure probability of



a multi-version program was the product of
the failure probabilities of the individual ver-
sions. However, Knight and Leveson observed
in an experiment that the failure probabili-
ties of real multi-version programs were sig-
ni(cid:12)cantly higher. In essence, the experiment
falsi(cid:12)ed the basic assumption of conventional
theory, namely that faults in program versions
are statistically independent.
Experiments are also used for exploring ar-
eas where theory and deductive analysis do not
reach. Experiments probe the in(cid:13)uence of as-
sumptions, eliminate alternative explanations
of phenomena, and unearth new phenomena
in need of explanation. In this mode, exper-
iments help with induction: deriving theories
from observation.
Arti(cid:12)cial neural networks are a good exam-
ple of this process. After having been dis-
carded on theoretical grounds, experiments
demonstrated properties better
than pre-
dicted. Researchers have now developed bet-
ter theories to account for these properties.
. Traditional scienti(cid:12)c method
isn't applicable
The fact that | in the (cid:12)eld of computer sci-
ence | the sub ject of inquiry is information
rather than matter or energy makes no no dif-
ference to the applicability of the traditional
scienti(cid:12)c method. In order to understand the
nature of information processes, computer sci-
entists must observe phenomena,
formulate
explanations and theories, and test them.
There are plenty of computer science the-
ories that haven't been tested. For instance,
functional programming, ob ject-oriented pro-
gramming, and formal methods are all thought
to improve programmer productivity, program
quality, or both.
It is surprising that none
of these obviously important claims have ever
been tested in a systematic way, even though
they are all   years old and a lot of e(cid:11)ort

has been invested in developing programming
languages and formal techniques.
Traditional sciences use theory test and ex-
ploration iteratively because observations help
formulate new theories that can be tested
later. An important requirement for any ex-
periment, however, is repeatability. Repeata-
bility makes sure that results can be checked
independently and thus raises con(cid:12)dence in
the results and helps eliminate errors, hoaxes,
and frauds.
. The current level of experi-
mentation is good enough
Suggesting that the current level of experi-
mentation doesn't need to change is based on
the assumption that computer scientists, as a
group, know what they are doing. This argu-
ment maintains that if we need more experi-
ments, we'll simply do them.
But this argument is tenuous; let's look at
the data.
In [],    papers were classi-
(cid:12)ed. Only those papers were considered fur-
ther whose claims required empirical evalua-
tion. For example, papers that proved the-
orems were excluded, because mathematical
theory needs no experiment. In a random sam-
ple of all papers ACM published in 		, the
study found that of the papers with claims
that would need empirical backup,  % had
none at all.
In journals related to software,
this fraction was  %. The same study also
analyzed a non-CS journal, Optical Engineer-
ing, and found that in this journal, the fraction
of papers lacking quantitative evaluation was
merely %.
The study by Zelkowitz and Wallace[]
found similar results. When applying con-
sistent classi(cid:12)cation schemes, both studies re-
port between  % and  % unvalidated pa-
pers in software engineering. Zelkowitz and
Wallace also surveyed journals in physics, psy-
chology, and anthropology and again found



much smaller percentages of unvalidated pa-
pers there than in computer science.
Relative to other sciences, the data shows
that computer scientists validate a smaller
percentage of their claims. One could argue
that computer science at age   is still young
and hence a comparison with other sciences is
of limited value. I disagree, because   years
seems plenty of time for two to three genera-
tions of scientists to establish solid principles.
But even on an absolute scale, I think that
it is scary when half of the non-mathematical
papers make unvalidated claims. Assume that
each idea published without validation would
have to be followed up by at least two valida-
tion studies (that's a very mild requirement).
It follows trivially that no more than one third
of papers published could contain unvalidated
claims. The data suggests that computer sci-
entists publish a lot of untested ideas or the
ideas published are not worth testing.
I'm not advocating replacing theory and en-
gineering by experiment, but I am advocating
a better balance. I advocate balance not be-
cause it would be desirable for computer sci-
ence to appear more scienti(cid:12)c, but because of
the following principal bene(cid:12)ts:
(cid:15) Experiment can help build up a reliable
base of knowledge and thus reduce un-
certainty about which theories, methods,
and tools are adequate.
(cid:15) Observation and experiment can lead to
new, useful, and unexpected insights and
open up whole new areas of investigation.
Experimentation can push into unknown
areas where engineering alone progresses
only slowly, if at all.
(cid:15) Experimentation can accelerate progress
by quickly eliminating
fruitless
ap-
proaches, erroneous assumptions, and
fads. It also helps orient engineering and
theory into promising directions.

Conversely, when we ignore experimenta-
tion and avoid contact with reality, we hamper
progress.
. Experiments cost too much
The (cid:12)rst line of defense against experimenta-
tion goes typically like the following: \Doing
an experiment would be incredibly expensive"
or \For doing this right, I would need hundreds
of sub jects, I would be busy for years without
being able to publish, and the cost would be
enormous."
To this, a hard-nosed scientist might say:
\So what?" Instead of being paralyzed by cost
considerations, he or she would (cid:12)rst probe the
importance of the research question. When
convinced that a fundamental problem is be-
ing addressed, an experienced experimentalist
would then go about planning an appropriate
research program, actively look for a(cid:11)ordable
experimental techniques, and suggest interme-
diate steps with partial results along the way.
For a scientist, funding potential should not
be the only or primary criterion for decid-
ing what questions to ask. In the traditional
sciences, there is a complex social process at
work in which important questions crystallize.
These become the foci of research, the break-
through goals that open up new areas, and
scientists actively search for economic ways
to conduct the necessary experiments. For
instance, the (cid:12)rst experimental validation of
General Relativity was tremendously expen-
sive and barely showed the e(cid:11)ect. The ex-
periment was performed by Sir Issac Edding-
ton in 		. Eddington used a total solar
eclipse to check Einstein's theory that grav-
ity bends light when it passes near a massive
star. At the time, this was a truly expensive
experiment since it involved an expedition to
Principe Island (West Africa) and the tech-
nology of photographic emulsions had to be
pushed to its limits. However, it was impor-



tant to test whether Einstein was correct or
not.Not many investigations are of a scope com-
parable to General Relativity, but there are
many smaller, but still important questions to
answer. How can such work be done econom-
ically? Since cost seems to be uppermost in
everybody's mind, I will spend more space on
this issue than on the others. My goal is to
help the cost-conscious scientist or engineer
overcome the cost barrier.
Experiments can indeed be expensive. But
are all of them prohibitively expensive?
I
think not. There are meaningful experiments
that (cid:12)t the budget of small laboratories. There
are also expensive experiments that are worth
much more than their cost. And there is a
wide spectrum in between.
Benchmarking. Though often criticized,
benchmarks are an e(cid:11)ective and a(cid:11)ordable
way of conducting experiments. Essentially, a
benchmark is a sample of a task domain; this
sample is executed by a computer or by human
and computer. During execution, well-de(cid:12)ned
performance measurements are taken. Bench-
marks have been used successfully in widely
di(cid:11)ering areas, including speech understand-
ing, information retrieval, pattern recognition,
software reuse, computer architecture, perfor-
mance evaluation, applied numerical analysis,
algorithms, data compression, logic synthesis,
and robotics. A benchmark provides a level
playing (cid:12)eld for competing ideas, and assum-
ing the benchmark is su(cid:14)ciently representa-
tive, it allows repeatable and ob jective com-
parisons. At the very least, a benchmark can
quickly eliminate unpromising approaches and
exaggerated claims.
Constructing a benchmark is usually in-
tensive work, but the burden can be shared
among several laboratories. Once a bench-
mark is de(cid:12)ned, it can be executed repeatedly

at moderate cost. In practice, it is necessary
to evolve benchmarks to prevent over-(cid:12)tting.
Regarding benchmark tests in speech recog-
nition, Ra j Reddy writes: \Using common
databases, competing models are evaluated
within operational systems. The successful
ideas then seem to appear magically in other
systems within a few months, leading to a vali-
dation or refutation of speci(cid:12)c mechanisms for
modeling speech."[]
In many of the examples cited above, bench-
marks have caused a sudden blossoming of the
area, because they made it easy to identify
promising approaches and discard poor ones.
I agree with Reddy that \all of experimental
computer science could bene(cid:12)t from such dis-
ciplined experiments."
Costly experiments. When human sub-
jects are involved in an experiment, the cost
often goes up dramatically, while signi(cid:12)cance
goes down. When are expensive experiments
justi(cid:12)ed? When the implications of the in-
sights gained outweigh the cost. Let us take
an example. A signi(cid:12)cant segment of the soft-
ware industry has converted from C to C++
at a substantial cost in retraining. One might
well ask how solidly grounded the decision to
switch to C++ was. Other than case stud-
ies (which are questionable because they don't
generalize easily and may be under pressure to
demonstrate desired outcomes), I'm not aware
of any solid evidence showing that C++ is su-
perior to C with respect to programmer pro-
ductivity or software quality. Nor am I aware
of any independent con(cid:12)rmation of such ev-
idence. However, while training students in
improving their personal software processes,
my research group has recently observed that
C++ programmers may make many more mis-
takes and take much longer than C program-
mers of comparable training { both during ini-
tial development and maintenance. Suppose



this observation is not a (cid:13)uke. Then running
experiments to test the fundamental tenets of
ob ject-oriented programming would be truly
valuable. These experiments might save re-
sources far in excess of their cost. The ex-
periments might also have a lasting and pos-
itive e(cid:11)ect on the direction of programming
language research. They may not only save
industry money, but also save research e(cid:11)ort.
It is useful to check what scientists in other
disciplines spend on experimentation. Every-
one realizes that drug testing in medicine is
extremely expensive, but only desperate pa-
tients accept poorly tested drugs and thera-
pies. In aeronautics, we demand that airfoils
be tested; expensive wind tunnels have been
built for just this purpose. Numerical simula-
tion has reduced the number of such tests, but
not eliminated them. In many sciences, simu-
lation has become an important form of exper-
imentation, and computer science might also
bene(cid:12)t from good simulation techniques.
In
biology, Wilson names the Forest Fragmenta-
tion Pro ject in Brasilia as the most expensive
biological experiment ever[]. While clearing
a large tract of the Amazon jungle, isolated
patches of various sizes ( to     hectares)
were left standing. The purpose was to test
hypotheses regarding the relationship between
habitat size and number of species remain-
ing. And the list of experiments continues {
in physics, chemistry, ecology, geology, clima-
tology, and on and on. Any reader of Scien-
ti(cid:12)c American can (cid:12)nd experiments in every
issue. Computer scientists need not be afraid
or ashamed of conducting large experiments
when exploring important questions.
 Just as this article went to press, we learned that
a paper by Les Hatton, \Does OO Really Match the
Way We Think?" will appear in the May issue of IEEE
Software, reporting strong evidence of the negative ef-
fects of C++.

. Demonstrations will su(cid:14)ce
In his 		 Turing Award lecture, Juris
Hartmanis argues that computer science dif-
fers su(cid:14)ciently from other sciences to per-
mit di(cid:11)erent standards in experimentation,
and that demonstrations can take the place of
experiments[]. I couldn't disagree more. De-
mos can provide proof-of-concepts in the en-
gineering sense, or provide incentives to study
a question further. Too often, however, these
demos merely illustrate a potential. Demon-
strations depend critically on the observers'
imagination and their willingness to extrap-
olate; they do not normally produce solid evi-
dence. To obtain such evidence, a careful anal-
ysis is necessary, involving experiments, data,
and replication.
What
would
be interesting questions amenable to experi-
mentation in the traditional sense? Here are
a few examples. The programming process is
poorly understood; computer scientists could
therefore introduce di(cid:11)erent theories of how
requirements are re(cid:12)ned into programs and
test them experimentally. Similarly, a deeper
understanding of intelligence might be discov-
ered and tested. The same applies to research
in perception, questions about the quality of
man-machine interfaces, or human-computer
interaction in general. Also, the behavior of
algorithms on typical problems or on comput-
ers with storage hierarchies cannot be pre-
dicted accurately. Better algorithm theories
are needed and should be tested in the labora-
tory. Research in parallel systems is currently
generating a number of machine models; their
relative merits can only be explored experi-
mentally. This list is certainly not exhaustive,
but the examples all involve experiments in
the tradition of science: They require a clear
question, an experimental apparatus to test
the question, data collection, interpretation,
and sharing of the results.



. There is too much noise in
the way
The second line of defense against experimen-
tation goes like this: \There are too many
variables to control, and the results would be
meaningless, because the e(cid:11)ects I'm looking
for are swamped by noise."
True, experimentation is di(cid:14)cult { for re-
searchers in all disciplines, not just computer
science. I think researchers who are invoking
this excuse are looking for an easy way out.
An e(cid:11)ective simpli(cid:12)cation for repeated ex-
periments is benchmarking.
Fortunately,
benchmarking can be used for many ques-
tions in computer science. The sub jective and
therefore weakest part in a benchmark test is
the composition of the benchmark; everything
else, if properly documented, can be checked
by the skeptic. Hence, the composition of the
benchmark is always hotly debated (is it rep-
resentative enough?), and benchmarks must
evolve over time to get them closer to what
one wants to test.
Experiments with human sub jects involve
many additional challenges.
Several (cid:12)elds
have found techniques for dealing with hu-
man variability, notably medicine and psychol-
ogy. We've all heard about control groups,
random assignments, placebos, pre- and post-
testing, balancing, blocking, blind and double-
blind studies, and the battery of statistical
tests. The fact that a drug in(cid:13)uences di(cid:11)erent
people in di(cid:11)erent ways doesn't stop medical
researchers from testing. And when control
is impossible, then case studies, observational
studies and an assortment of other investiga-
tive techniques are used. Indeed, medicine of-
fers many important lessons on experimental
design, on how to control variables and how
to minimize errors. Eschewing experimenta-
tion because of di(cid:14)culties is not acceptable.

. Progress will slow
The argument here is that if everything must
be backed up by experiment before publica-
tion, then the number of ideas that can be
generated and discussed in the scienti(cid:12)c com-
munity will be throttled and progress will slow.
This is not an argument to be taken lightly.
In a fast-paced (cid:12)eld such as computer science,
the number of ideas being discussed is obvi-
ously important. However, experimentation
need not have an adverse e(cid:11)ect; quite the con-
trary.First,
increasing the ratio of papers with
meaningful validation has a good chance of
actually accelerating progress: Questionable
ideas will be weeded out more quickly and sci-
entists will concentrate their energies on more
promising approaches.
Second, I'm con(cid:12)dent that good conceptual
papers and papers formulating new hypothe-
ses will continue to be valued by readers and
will therefore get published. It should be un-
derstood that experimental testing of these hy-
potheses will come later.
So it is a matter of balance once more.
Presently, non-theory research rarely moves
beyond the assertive state, a state character-
ized by such weak justi(cid:12)cation as \it seems
intuitively obvious", or \it looks like a good
idea", or \I tried it on a small example and
it worked." Reaching a ground (cid:12)rmer than
assertion is desirable.
. Technology changes too fast
This concern comes up frequently in computer
architecture. Trevor Mudge summarizes it:
\...the rate of change in computing is so great
that by the time results are con(cid:12)rmed they
may no longer be of any relevance."[	] The
same can be said about software. What good
is an experiment when the duration of the ex-
periment exceeds the useful life of the exper-



imental sub ject, i.e., of a software product or
tool?If a question becomes irrelevant quickly, it
is perhaps too narrow and not worth spending
a lot of e(cid:11)ort on it. But behind many ques-
tions with a short lifetime lurks a fundamental
problem with a long lifetime. My (cid:12)rst advice
to scientists here is to probe the fundamental
and not the ephemeral, and to learn to tell
the di(cid:11)erence. My second advice hinges on
the observation that technological change of-
ten shifts or eliminates assumptions that were
taken for granted. Therefore, scientists should
anticipate changes in assumptions and pro-
actively employ experiments to explore the
consequences of such changes. Note that this
type of work is much more demanding, and
can have much higher long-term value, than
merely comparing software products.
. You'll never get it published
This is actually partly true. Some established
computer science journals have di(cid:14)culty (cid:12)nd-
ing editors and reviewers capable of evaluat-
ing empirical work. Promotion committees
may be dominated by theoreticians. The ex-
perimenter is often confronted with review-
ers who expect perfection and absolute cer-
tainty. However, experiments are conducted in
the real world and are therefore always (cid:13)awed
somehow. Reviewers may also build up im-
possibly high barriers. I've seen demands for
experiments to be conducted with hundreds of
sub jects over a span of many years involving
several industrial pro jects before publication.
That smaller steps are still worth publishing
because they improve our understanding and
raise new questions is a thinking that some are
not familiar with.
However, this situation is changing. In my
experience, publication of experimental results
is not a problem of one chooses the right out-
let. I'm on the editorial board of three jour-

nals; I review for quite a number of additional
journals and have served on numerous confer-
ence committees. All non-theory journals and
conferences that I've seen would greatly wel-
come papers reporting on solid experiments.
The occasional rejection of high-quality papers
not withstanding, I'm convinced that the low
number of good experimental papers is a sup-
ply problem.
The funding situation for experimentation is
more di(cid:14)cult, especially in industry/academia
collaborations. However, it helps to note that
experimentation may give industry a three to
(cid:12)ve year lead over the competition. For ex-
ample, suppose an experiment discovered an
e(cid:11)ective way to reduce maintenance costs by
using software design patterns. The industrial
partner of such an experiment could exploit
this result immediately, especially since the ex-
periment prepared the groundwork for adopt-
ing the technology. Given a two-year publica-
tion time lag and various other delays (such as
the results being noticed by others, let alone
adopted), the industrial partner in such an
experiment can exploit at least a three-year
lead. Lucent Technologies estimates that it is
presently bene(cid:12)ting from a (cid:12)ve-year lead in
software inspection methods based on a series
of in-house experiments, apparently despite
(or because of ) vigorous publication of the re-
sults.On the negative side I fear that the \sys-
tems researcher" of old will face di(cid:14)culties.
Just building systems is not enough unless the
system demonstrates some kind of a \(cid:12)rst,"
a breakthrough. Computer science continues
to be favored with such breakthroughs and we
should continue to strive for them. The ma-
jority of systems researchers, however, works
on incremental improvements of existing ideas.
These researchers should try to become re-
 Larry Votta, private communication, Lucent
Technolgies.



spectable experimentalists. They must artic-
ulate how their systems contributes to our
knowledge. Systems come and go;
insights
about the concepts and phenomena underly-
ing systems are what is needed. I have great
expectations for systems researchers who use
their skills in setting up interesting experi-
ments.
 Why substitutes won't
work
Can we get by with forms of validation that
are weaker than experiments? It depends on
what question we're asking, but here are some
excuses that I (cid:12)nd less than satisfactory.
. Feature comparison is good
enough
A frequently found model of a scienti(cid:12)c paper
is the following. The work describes a new
idea, prototyped perhaps in a small system.
The claim to \scienti(cid:12)cness" is then made by
feature comparison. The reports sets out a list
of features and qualitatively compares older
approaches with the new one, feature by fea-
ture.I (cid:12)nd this method satisfactory when a rad-
ically new idea or a signi(cid:12)cant breakthrough
is presented, such as the (cid:12)rst compiler for a
block-structured language, the (cid:12)rst timeshar-
ing system, the (cid:12)rst ob ject-oriented language,
the (cid:12)rst web browser. Unfortunately, the ma-
jority of papers published take much smaller
steps forward. As computer science becomes a
harder science, mere discussions of advantages
and disadvantages or long feature comparisons
will no longer be su(cid:14)cient. Any PC magazine
can provide those. A science, on the other
hand, cannot live o(cid:11) such weak phenomeno-
logical inferences in the long run.
Instead,

scientists should create models, formulate hy-
potheses, and test them using experiments.
. Trust your intuition
In his March 		 column, Al Davis, the
editor of IEEE Software suggests that gut
feeling is enough when adopting new soft-
ware technology; experimentation and data
are super(cid:13)uous[]. He even suggests ignoring
evidence that contradicts one's intuition.
However, instinct and personal experience
sometimes lead down the wrong path and com-
puter science is no exception. Here are some
examples. For about twenty years,
it was
thought that meetings were essential for soft-
ware reviews. However, recently Porter and
Johnson found that reviews without meetings
are neither substantially more nor less e(cid:11)ec-
tive than those with meetings[]. Meeting-
less reviews also cost less and cause fewer de-
lays, which can lead to a more e(cid:11)ective inspec-
tion process overall. Another example where
observation contradicts conventional wisdom
is that small software components are propor-
tionally less reliable than larger ones. This ob-
servation was (cid:12)rst reported by Basili [] and
has been con(cid:12)rmed by a number of disparate
sources; see Hatton [] for summaries and an
explanatory theory. As mentioned, the failure
probabilities of multi-version programs were
incorrectly believed to be the product of the
failure probabilities of the component versions.
Another example is type checking in program-
ming languages. Type checking is thought to
reveal programming errors, but there are con-
texts when it does not help []. P(cid:13)eeger et al.
[ ] provides further discussion of the pitfalls
of intuition.
What we can learn from these examples is
that intuition may provide a starting point,
but must be backed up by empirical evidence.
Without grounding, intuition is highly ques-
tionable. What one thinks obvious may turn

	

out to be dead wrong sometimes.
. Trust the experts
During a recent talk at a top US university,
I was about to present my data, when a col-
league interrupted and suggested that I skip
that part and go on to the conclusions. \We
trust you." was the explanation. Flattering
as that was, it shows a disturbing misunder-
standing of the scienti(cid:12)c process (or someone
in a hurry). Any scienti(cid:12)c claim is initially sus-
pect and must be examined closely. Imagine
what would have happened if physicists hadn't
been skeptical about the claims by Ponds and
Fleischman regarding cold fusion.
Frankly,
I'm continually surprised how
much the computer industry and sometimes
even university teaching relies on so-called
\experts" of all kinds, who fail to back up their
assertions with evidence. Science, on the other
hand, is built on healthy skepticism.
It is a
good system to carefully check results and to
accept them only provisionally until they have
been con(cid:12)rmed independently.
 Problems do exist
Here are some excuses that are in(cid:13)uenced by
the quality of experiments in computer sci-
ence.. Flawed experiments
\Experiments make unrealistic assumptions",
or \The data was manipulated", or \It is im-
possible to quantify the variable of interest,"
are some of the criticisms. There are many
more potential (cid:13)aws: Experimenters may pick
irrelevant questions, may neglect to provide
enough detail for repeating experiments, may
be nonchalant about control, may not validate

observations, forget to bound errors, use inap-
intelligence. The weak reasoning methods of
propriate measurements, over-interpret their
the (cid:12)rst theory have gradually given way, or
results, produce results that do not general-
have been coupled with, knowledge bases [].
ize, etc.Good examples of solid experimentation in
Other examples include symbolic vs. subsym-
bolic processing, RISC vs. CISC, the various
models for predicting the performance of (par-
computer science are rare. And there will al-
allel) computers, and the competition among
ways be questionable, even bad experiments.
programming language families (logic, func-
However, the conclusion from this observation
tional, imperative, ob ject-oriented, rule-based,
is not to discard the concept of experimenta-
constraint-based). Another important exam-
tion. We should keep in mind that other sci-
enti(cid:12)c (cid:12)elds have been faced with bad experi-
ple is algorithms theory. The present theory
has many drawbacks;
in particular,
it does
ments, even frauds. But the scienti(cid:12)c process
not account for the behavior of algorithms on
on the whole has been self-correcting. Bad
ideas, errors, and downright hoaxes have been
\typical" problems[]. A more accurate the-
ory that applies to modern computers would
weeded out, sometimes promptly (see cold fu-
be valuable.
sion) sometimes belatedly (see the Piltdown
man).We can be sure of one thing, though: If sci-
A prerequisite for competition among the-
ories is falsi(cid:12)ability. Unfortunately, computer
entists overlook experimentation or neglect re-
science theorists rarely produce falsi(cid:12)able the-
examining others' claims, an important source
ories; they tend to pursue mathematical theo-
of self-correction will be cut o(cid:11) and the (cid:12)eld
ries that are disconnected from the real world.
may drift into the wrong direction.
Thus, it has largely fallen to experimentalists
and engineers to formulate falsi(cid:12)able theories.
. Competing theories
While computer science is perhaps too
A science is most exciting when there are two
young to have brought forth grand theories,
or more strong, competing theories. When
my greatest fear is that the lack of such theo-
a new, ma jor theory replaces an older one,
ries might be caused by a lack of experimen-
one speaks of a paradigm shift, while the sta-
tation.
If scientists neglect experiment and
ble periods in between are called \normal sci-
observation, they'll have di(cid:14)culties discover-
ence". Physics provides interesting examples
ing new and interesting phenomena worthy of
of paradigm shifts.
better theories.
There are a few competing theories in com-
puter science, none of them earth-shaking.
The physical symbol system theory vs.
the
 In Ch. 	 of The Quark and the Jaguar, W.H. Free-
knowledge processing theory in AI is one of
man (		), Gell-Mann provides a lucid discussion of
the relationship between mathematics and science. If
them. These two theories attempt to explain
science is concerned with describing nature and its
 Piltdown man are fossil remains found in England
laws, then mathematics is not a science, because it
is not concerned with nature; it is concerned with the
in 	. The fossils were thought to be a species of pre-
logical consequences of certain assumptions. On the
historic man and generated scholarly controversy that
other hand, mathematics can also be viewed as the rig-
lasted about   years. In 	, intense re-examination
orous study of what might have been, i.e., the study
showed the remains to be fraudulent. The fossils con-
of hypothetical worlds, including the real one. In that
sisted of skillfully disguised fragments of a quite mod-
case, mathematics is the most fundamental science of
ern human cranium ( ,    years old), the jaw and
all.
teeth of an orangutan, and the tooth of a chimpanzee.
 

. Soft science
\Soft science" means that experimental results
cannot be reproduced. Experiments with hu-
man sub jects are not necessarily soft. There
are stacks of books on how to conduct exper-
iments with humans. Experimental computer
scientists can learn the relevant techniques or
ask for help. The side-bar provides some start-
ing points.
. Misuse
The argument goes along the following lines:
\Give the managers or funding agencies a sin-
gle (cid:12)gure of merit and they will use it blindly
to promote or eliminate the wrong research."
I think this is a red herring. Good managers,
good scientists, and good engineers all know
better than to rely on a single (cid:12)gure of merit.
Second, there is a much greater danger in re-
lying on intuition and expert assertion alone.
Keeping decision makers in the dark has an
overwhelmingly higher damage potential than
informing them to the best of ones abilities.
 Conclusion
Experimentation is central to the scienti(cid:12)c
process. Only experiments test theories. Only
experiments can explore critical factors and
bring new phenomena to light so theories can
be formulated in the (cid:12)rst place. Without ex-
periments in the tradition of science, computer
science is in danger of drying up and becoming
an auxiliary discipline. The current pressure
to concentrate on applications is the writing
on the wall.
I have no doubt that computer science is
a fundamental science of great intellectual
depth and importance. Much has already been
achieved. Computer technology has changed
society, and computer science is in the pro-
cess of deeply a(cid:11)ecting the weltanschauung of


the general public. There is also much evi-
dence suggesting that the scienti(cid:12)c method ap-
plies. As computer science leaves adolescence
behind, I hope to see the experimental branch
of this discipline (cid:13)ourish.
Acknow ledgments This essay has bene(cid:12)ted
tremendously from numerous discussions with
colleagues. I'm especially grateful for thought-
provoking comments by Les Hatton, Ernst
Heinz, James Hunt, Paul Lukowicz, Anneliese
v. Mayrhauser, David Notkin, Shari Lawrence
P(cid:13)eeger, Adam Porter, Lutz Prechelt, and
Larry Votta.
References
[] Victor R. Basili and B.T. Perricone. Soft-
ware errors and complexity: An empiri-
cal investigation. Communications of the
ACM, ():{, January 	.
[] Frederick P. Brooks. Toolsmith II. Com-
munications of the ACM, 	():{,
March 		.
[] Al Davis. From the editor. IEEE Soft-
ware, ():{, March 		.
[] Edward A. Feigenbaum. How the What
becomes the How. Communications of the
ACM, 	():	{ , May 		.
[] Juris Hartmanis. Turing award lecture:
On computational complexity and the na-
ture of computer science. Communica-
tions of the ACM, ( ):{, October
		.
[] Les Hatton.
Reexamining
the
fault density{component size connection.
IEEE Software, ():	{	, 		.
[] John N. Hooker. Needed: An empiri-
cal science of algorithms. Operations Re-
search, (): {, March 		.

[] Edward O. Wilson. The Diversity of Life.
Harvard University Press, 		.
[] Marvin V. Zelkowitz and Dolores Wal-
lace. Experimental models for validating
computer technology.
IEEE Computer,
(), May 		.

[] John C. Knight and Nancy G. Leveson.
An experimental evaluation of the as-
sumption of independence in multiver-
sion programming. IEEE Transactions on
Software Engineering, SE-():	{ 	,
January 	.
[	] Trevor Mudge. Report on the panel: How
can computer architecture researchers
avoid becoming the society for irrepro-
ducible results? Computer Architecture
News, ():{, March 		.
[ ] Shari Lawrence P(cid:13)eeger, Victor Basili,
Lionel Briand, and Khaled El-Emam. Re-
buttal to March 	 editorial. IEEE Soft-
ware, (), July 		.
[] Adam A. Porter and P.M. Johnson. As-
sessing software review meetings: Re-
sults of a comparative analysis of two ex-
perimental studies.
IEEE Transactions
on Software Engineering, ():	{,
March 		.
[] Lutz Prechelt and Walter F. Tichy. An
experiment to assess the bene(cid:12)ts of inter-
module type checking.
In Proc. Third
Intl. Software Metrics Symposium, pages
{	, Berlin, March 		. IEEE Com-
puter Society Press.
[] Anthony Ralston and Edwin D. Reilly.
Encyclopedia of Computer Science, Third
Edition. Van Nostrand Reinhold, 		.
[] Ra j Reddy. To dream the possible dream.
Communications of the ACM, 	(): {
, May 		.
[] Walter F. Tichy, Paul Lukowicz, Lutz
Prechelt, and Ernst A. Heinz. Experi-
mental evaluation in computer science: A
quantitative study. The Journal of Sys-
tems and Software, ():{, January
		.


International Journal of Scientific and Research Publications, Volume 3, Issue 4, April 2013  
ISSN 2250-3153  
 

 

 

 

 

 

1 

A 16-bit MIPS Based Instruction Set Architecture for 
RISC Processor  

Sagar Bhavsar *, Akhil Rao *, Abhishek Sen *, Rohan Joshi * 

 
* B.Tech (Electronics), Veermata Jijabai Technological Institute 
Mumbai, India 

    
    Abstract- Microcontrollers  and microprocessors  are  finding 
their  way 
into  almost  every  field 
in 
today‟s  world, 
incorporating  an  element  of  „smartness‟  into  conventional 
devices.  Energy  efficient,  space  efficient  and  optimized 
microcontrollers are the need of the day. Our paper proposes a 
new Instruction Set that is a subset of the MIPS architecture. It 
derives  the  advantages  of  MIPS  like  simplicity  and  speed. 
Besides, since  it  is a  smartly optimized subset of MIPS,  it  is a 
smaller  version  consisting  of  the  most  commonly  required 
instructions. 
 
    Index Terms- ISA, MIPS, Processor design, RISC.  
 

I.  INTRODUCTION 

M 
IPS  is  a  reduced  instructions  set  computer  (RISC) 
architecture.  It  is  one  of  the  first  RISC  Instruction  set 
architectures.  MIPS  is  an  acronym  for  “Microprocessor 
without  interlocked  pipeline  stages”.  It  was  developed  by  a 
team  led  by  John  Hennessey  at  Stanford  University.  MIPS 
implementations are primarily used in embedded systems such 
as  Windows  CE  devices,  routers,  residential  gateways,  and 
video  game  consoles  such  as  the  Sony  PlayStation  2  and 
PlayStation  Portable.  Until  late  2006,  they  were  also  used  in 
many  of  SGI's  computer  products.  MIPS  implementations 
were  also  used  by  Digital  Equipment  Corporation,  NEC, 
Pyramid  Technology,  Siemens  Nixdorf,  Tandem  Computers 
and  others  during  the  late  1980s  and  1990s.  Since  MIPS  is  a 
RISC  computer  it  employs  less  number  of  transistors  and 
hence decreases the transistor count. Pipelining is thus heavily 
employed  to  make  use  of  extra  available  space  on  the  chip  to 
improve code execution performance. MIPS was defined to be 
a  32  bit  architecture  called  MIPS32.  Later  Revisions  of  this 
architecture is 64 bit in size and hence called MIPS64.  [1] 

II.  MIPS 16 INSTRUCTION SET DESCRIPTION 

A. Motivation 

    Small  scale  applications  do  not  require  that  much  of 
computing  power.  This  paper  proposes  a  reduced  version  of 
MIPS  instruction  set  for  such  small  scale  applications.  This 
ISA  will  be  called  MIPS  16.  The  main  aim  of  this  ISA  is  to 
reduce  the  transistor  count  of  a  MIPS  processing  unit  by 
scaling down the bus and register width and providing less but 
enough  number  of  instructions  for  small  scale  applications. 
The  implementation  of  such  an  instruction  set  would  take  up 

less  real  estate  on  the  chip  (or  FPGA)  and  will  allow  more 
peripherals  to  be  fabricated  on  a  single  chip  making  it  ideal 
for  a  System-On-Chip 
(SOC) 
implementation  of  an 
application.  It  will  also  be  beneficial  in  embedded  system 
design  where  a  custom  processor  core  implementation  is 
required  with  tight  instruction  requirements  so  that  it  takes 
less space on a FPGA. 

B. Instruction Set Specification [3] 

    MIPS  instructions  have  fixed  width.  The  original  MIPS 
32  ISA  has  32  bits  wide  instructions.  Each  instruction  in 
MIPS16  is  16  bits  wide.  Further,  MIPS16  has  8  internal 
registers  as  opposed  to  the  32  registers  of  MIPS32.  As  the 
name  suggests,  data  bus  is  16  bits  wide  and  address  bus  is 
preferably  16  bits  wide  too.  I/O  support  is  memory  mapped. 
Memory  is  accessed  by  LOAD  and  STORE  instructions.  The 
instructions  follow  an  <operand  register,  register,  register> 
format. 
The Instructions can be divided into 4 groups:  
a.  Arithmetic:  Basic  computational  instructions 
add and subtract. 
b.  Logical: Operations like AND, OR, EXOR  
c.  Data Transfer: Load and Store operations 
d.  Branch and control: Jump, Call, Return, etc. 
The ISA supports direct and immediate addressing modes.  

C.  Instruction Word Format  

A  MIPS16  instruction  is  16  bits  wide.  Since  MIPS  uses  a 
Register-Register  type  of  instruction  a  general  instruction 
specifies  two  source  registers  and  a  destination  registers.  The 
format of such an instruction will be 
ADD Rs1, Rs2, Rd 
 
Rs1 = First source operand register 
Rs2 = Second Source operand register  
Rd = Destination register 
 
The  instruction  word  has  a  5  bit  op-code  specifying  the 
operation  to  be  performed.  Number  of  operands  may  be 
variable  e.g.  ADD  requires  three  operands  while  NOT 
requires only  two. Format  of  a  three operand  instruction word 
is shown in Table I 

TABLE I 
THREE OPERAND INSTRUCTION 

www.ijsrp.org 

 

 

 

 

 

2 

Jump  instructions  have  two  modes  viz..PC  relative  and 
absolute  modes.  In  PC  relative  mode.  The  lower  5  bits  of  the 
instruction are used to specify a 5 bit signed value as shown in 
Table  IV.  This  value  is  added/subtracted  to  the  PC  to  get  the 
jump  address.  The  PC  relative  mode  is  used  for  conditional 
jump 
instructions.  The  absolute  mode 
is  used 
for 
unconditional  jumps  and  jump-&-link  instructions.  In  these 
instructions  the  all  bits  other  than  the  opcode  are   used  to 
specify a 11 bit signed PC offset value.  

The  instruction  set  is  so  designed  so  as  to  simplify  the 
instruction decoding logic and the control logic.  

Rs1 
3 bits 

Rs2 
3 bits 

Rd 
3 bits 

Reserved 
2 bits 

International Journal of Scientific and Research Publications, Volume 3, Issue 4, April 2013  
ISSN 2250-3153  
 
Op-code 
5 bits 
 
In  case  of  ALU  instructions,  the  2  reserved  bits  act  as 
function  bits  where  they  are  used  to  distinguish  between 
versions  of  a  common 
instruction.  For 
instance, 
the 
instructions  ADD  and  ADC  have  the  same  opcode  but 
different  function  bits.  This  results  in  a  simpler  control  logic 
as  the  reserved  bits  are  decoded  directly  by  the  ALU  control 
logic. 
In  case  of  lesser  operands  appropriate  operand  is  given  a 
constant value. E.g. NOT  instruction  requires only one  source 
and  one  destination  operand.  Therefore,  Rs2  field  will  be 
made  “000” as shown  in Table  II. Likewise a POP  instruction 
will  require  only  destination  and  hence  both  the  source 
operands  will  be  constant  and  only  destination  needs  to  be 
provided as shown in Table III. 
NOT Rs, Rd 

 

TABLE VI 
 JUMP (ABSOLUTE MODE) INSTRUCTION 

Immediate 

11 

it signed PC Relative offset  

Op-code 

5 bits 

TABLE II 
TWO OPERAND INSTRUCTION 

Rs 
3 bits 

Constant  Rd 
000 
3 bits 

Reserved 
2 bits 

Op-code 
5 bits 
 

POP Rd 

TABLE III 
ONE OPERAND INSTRUCTION 

Reserved 
2 bits 

Constant  Constant  Rd 
000 
000 
3 bits 

Op-code 
5 bits 
 
The unused  fields  in an  instruction are also used  to provide 
immediate  input.  The  size  of  the  immediate  field  depends  on 
the number of operands instruction uses.  
ADDI Rs, Rd, #10 
Rs = Source Register 
Rd = Destination register 
#10 = Immediate value (0-31 in decimal)  
 

TABLE IV 
 IMMEDIATE INSTRUCTION 

Op-code 
5 bits 

Rs 
3 bits 

Rd 
3 bits 

Immediate 
5 bits 

Instructions  like  the  move  immediate(MVIH  / MVIL)  require 
a  8-bit  value  to  be  specified  within  the  instruction.  In  such  a 
case,  the  8  bt  value  is  split  into  2  parts.  The  higher  3  bits  are 
specified  in place of  the Rs1 operand and  the next  lower 5 bits 
are specified in the lower 5 bits of the instruction.  

TABLE V 
 MOVE IMMEDIATE INSTRUCTION 

Op-code 
5 bits 

Rs 
7:5 bits of imm value 

Rd 
3 bits 

Immediate 
4:0 bits of imm value 

 

 

D.  Comparison between MIPS-16 and MIPS-32 [7] 

 
MIPS-16  can  be  considered  to  be  a  derivative  of MIPS-32 
instruction  set.  But  the  philosophies  behind  their  design  are 
different.  MIPS-16  provides  more  flexibility  in  terms  of 
optimizing 
the  design  by  keeping  only 
the 
required 
instructions. MIPS-16  is  designed  for  small  scale  applications 
while  MIPS-32  is  a  high  performance  32-bit  architecture 
which  can  handle  large  data  and  perform  fast  calculations  by 
employing multiple pipelines and multiple registers at the  cost 
of larger chip area and complicated logic design.  
Some key differences have been highlighted in TABLE VII 

Serial 
No. 
1 

2 

3 
4 

5 

6 

TABLE VII 
COMPARISON OF MIPS-16 AND MIPS-32 ISA 

Instruction 

MIPS-16 
set 
Instruction  word  length 
is 16-bit 
Supports  only  8  general 
purpose registers 
Register is 16 bits wide 
Program  counter  should 
be 
incremented  by  2 
after  every  instruction 
(for 
non-branching 
instructions) 
ALU  is  simpler.  It  does 
not  support  operations 
with  bulky  logic  like 
Multiplication 
and 
Division 
Floating 
point 
instructions 
are 
not 
included in MIPS-16 

MIPS-32  Instruction  
set 
Instruction  word  length  is 
32-bit 
Supports  up  to  32  general 
purpose registers 
Register is 32 bits wide 
Program  counter  should 
be  incremented  by  4  after 
every 
(for 
instruction 
non-branching 
instructions) 
ALU  is  complicated.  It 
supports 
complicated 
like 
operations 
Multiplication 
and 
Division. 
Floating 
Point 
instructions  are  included 
and  are  called  SIMD 
instructions 

www.ijsrp.org 

International Journal of Scientific and Research Publications, Volume 3, Issue 4, April 2013  
ISSN 2250-3153  
 

 

 

 

 

 

3 

7 

8 

9 

Pipelining 
not 
is 
essential  and  depends 
on the application. 
Transistor  count  and 
chip area is less 
Suitable  for  small  scale 
applications 
and 
applications  with 
low 
computing requirement 

Pipelining  is a key  feature 
of  a  MIPS-32  based 
processor. 
Transistor  count  and  chip 
area is more 
Suitable 
high 
performance 
high 
throughput applications. 

for 

E.  List of Instructions 

 

As  the  op-code  has  a  5  bit  length  there  are  32  possible 
distinct  instructions.  If  the  reserved  bits  at  the  end  of  the 
instruction  are  utilized  for  grouping  2  or  more  similar 
instructions  more  op-codes  can  be  incorporated  in  the 
instruction  set e.g. ADD and ADC can be grouped  together as 
they  perform  similar  function  with  the  difference  being 
inclusion  of  carry  into  the  sum.    A  complete  list  of  37 
instructions  has  been  provided  in  Table  VIII  with  a  short 
description of each instruction. 

 
 
 
 
 

 

TABLE VIII  
MIPS-16 INSTRUCTION SET 

Mnemonic 

Sr. 
No. 
ADD 
1. 
ADC 
2. 
SUB 
3. 
SBB 
4. 
AND 
5. 
OR 
6. 
XOR 
7. 
NOT 
8. 
SHIFTL 
9. 
10. 
SHIFTR 
11.  ADDI 
12. 
SUBI 
13.  MOV 
14.  MVIH 
15.  MVIL 
LDIDR 
16. 
STIDR 
17. 
18. 
LDIDX 
STIDX 
19. 
JMP 
20. 
JMPI 
21. 
JGEO 
22. 
23. 
JLEO 
JCO 
24. 
JEO 
25.   
PUSH 
26. 
27. 
POP 
CALL 
28. 
29. 
JAL 

30.  MOVSP 
31. 
RET 
32. 
STC 
33.  NOP 
34.   HLT 
RST 
35. 
36. 
IE 
ID 
37. 

 

Description 

Instruction 
Format 
Adds Rs1 and Rs2 and stores the sum in Rd ignoring carry.  
ADD Rs1, Rs2 ,Rd 
Adds Rs1 and Rs2 and stores the sum in Rd with previous carry.  
ADC Rs1, Rs2 ,Rd 
Subtracts Rs2 from Rs1 and stores the difference in Rd ignoring the previous borrow. 
SUB  Rs1, Rs2 ,Rd 
Subtracts Rs2 from Rs1and stores the difference in Rd with the previous borrow.  
SUB  Rs1, Rs2 ,Rd 
Performs Bitwise AND of Rs1 and Rs2 and stores the result in Rd 
AND Rs1, Rs2 ,Rd 
Performs Bitwise OR of Rs1 and Rs2 and stores the result in Rd 
OR Rs1, Rs2 ,Rd 
Performs Bitwise XOR of Rs1 and Rs2 and stores the result in Rd 
XOR Rs1, Rs2 ,Rd 
Performs Complement of Rs1 and stores the result in Rd 
NOT Rs1 ,Rd 
Shifts Rs1 by one place to the left and store it in Rd 
SHIFTL Rs1 ,Rd 
Shifts Rs1 by one place to the right and store it in Rd 
SHIFTR Rs1 ,Rd 
Adds a 5-bit unsigned value to Rs1 and stores the sum in Rd 
ADDI Rs1 ,Rd,#5-bit 
Subtracts a 5-bit unsigned value from Rs1 and stores the difference in Rd 
SUBI Rs1 ,Rd,#5-bit 
Copies Rs1 to Rd 
MOV Rs1 ,Rd 
Copies immediate value into higher byte of Rd 
MVIH Rd,#8-bit 
MVIL Rd,#8-bit 
Copies immediate value into lower byte of Rd 
LDIDR Rs1 ,Rd,#5-bit  Loads Rd with a nibble at address given by [Rs1 +5 bit  immediate value] 
Stores Rd with a nibble at address given by [Rs1 +5 bit  immediate value] 
STIDR Rs1 ,Rd,#5-bit 
LDIDX Rs1, Rs2 ,Rd 
Loads Rd with a nibble at address given by [Rs1 + Rs2] 
Stores Rd with a nibble at address given by [Rs1 + Rs2] 
STIDX Rs1, Rs2 ,Rd 
Unconditional jump to address offset by 11 bit signed value from current PC value  
JMP #11-bit 
Unconditional jump to address offset by 5 bit signed value added to R d 
JMPI Rd,#15 
Conditional Jump to PC + 5 bit signed offset if R s1 is greater than or equal to Rs2 
JGEO Rs1 ,Rs2,#5-bit 
JLEO Rs1 ,Rs2,#5-bit 
Conditional Jump to PC + 5 bit signed offset if R s1 is less than or equal to Rs2 
Conditional Jump to PC + 5 bit signed offset if carry is set  
JCO #5-bit 
Conditional Jump to PC + 5 bit signed offset if R s1 equals to Rs2 
JEO Rs1 ,Rs2,#5-bit 
Push Rs1 to the stack top and update stack top 
PUSH Rs1 
POP Rd 
Pop from the stack top and store the value to Rd and update stack top 
Calls a subroutine located at [Rs1]. Return address is pushed onto stack 
CALL Rs1 
Calls a subroutine located at [PC + 11 bit signed offset].Return address is pushed onto 
JAL #11-bit  
stack. 
Copies value at Rs1 to stack pointer SP 
Return from a function. Return address is popped from the stack  
Set the carry flag 
No operation. Idle machine cycle should be executed  
Halts the processor. 
Resets the processor. 
Enables the interrupt. 
Disables the interrupt. 

MOVSP Rs1 
RET 
STC 
NOP 
HLT 
RST 
IE 
ID 

www.ijsrp.org 

International Journal of Scientific and Research Publications, Volume 3, Issue 4, April 2013  
ISSN 2250-3153  
 

 

 

 

 

 

4 

F.  Implementation Strategies 

The  implementation  strategies  that  can  be  employed  will 
depend  on 
the  application. 
  Some  of 
the  design 
considerations are listed below: 
a.  Single-Cycle or Multi-cycle implementation: [3], [4], 
[5]  
Implementation can use a single cycle or a multi 
cycle control system for its datapath.  
A single cycle control system performs all the 
elementary datapath operations in a single cycle. This 
generally requires dedicated hardware for every 
phase of instruction fetching, decoding and 
execution. It is faster at the cost of larger chip area.  
       A multi cycle implementation divides the 
execution of an instruction into well-defined time 
states. The execution happens in a timely order and 
might require different number of time states for 
different instruction. The main advantage is the 
hardware can be shared for similar elementary 
functions in different time states. Multi cycle should 
be preferred for applications with smaller chip area 
requirements. 
b.  Pipelining Requirement:[6]  
Pipelining provides performance enhancement by 
concurrent execution of more than one pair able 
instructions. The design involves use of multiple 
datapaths and a logic to check for pairability and 
hazard removal that occurs due to concurrent 
execution. This significantly complicates the design 
and takes a larger chip area. But the performance 
improvement would be tremendous. 

 
 

III.  CONCLUSION 

MIPS-16  is  thus  a  low-cost,  compact  and  hence  in  effect  a 
low  power  RISC  instruction  set  architecture  as  compared  to 
the  MIPS-32  architecture.  Its  compact  size  and  flexibility 
makes  it  ideal  for  an  optimized  implementation  of  an 
embedded  system.  It  provides  all  the  basic  instruction  and 
functionality  for a  small  scale embedded system not  involving 
heavy  arithmetic  calculations.  MIPS-16  can  be  implemented 

 
 

on  FPGA  by  an  appropriate  strategy  as  per  the  application's 
requirement.  Single  cycle  design  should  be  used  for  better 
performance  while  multi  cycle  design  should  be  preferred  for 
compactness.  Pipelining  can  further  improve  the  system 
performance. 

ACKNOWLEDGMENT 

It gives us immense pleasure  to thank Prof Dr. R  D Daruwala, 
our Project Guide who always motivated us. He was kind   
enough  to  help  us  with  our  doubts  and  was  a  source  of 
inspiration for this project. 

REFERENCES 

[1]  http://en.wikipedia.org/wiki/MIPS_architecture. 
[2]  http://logos.cs.uic.edu/366/notes/mips%20quick%20tutorial.htm 
[3]  David A. Patterson and John L. Hennessy, Computer Organization and 
Design, 3rd ed. 
[4]  Lecture  notes  by  Howard  Huang,  University  of  Illinois  at  Urbana-
Champaign. [Online]. Available: 
http://www.howardhuang.us/teaching/cs232/11-Single-cycle-MIPS-
processor.pdf 
[5]  Lecture  notes  by  Howard  Huang,  University  of  Illinois  at  Urbana-
Champaign. [Online]. Available: 
http://www.howardhuang.us/teaching/cs232/12-Multicycle-
datapath.pdf 
[6]  Lecture  notes  by  Howard  Huang,  University  of  Illinois  at  Urbana-
Champaign. [Online]. Available: 
http://www.howardhuang.us/teaching/cs232/15-Pipelining.pdf 
[7]  MIPS Official Website. Available: 
http://www.mips.com/products/architectures/mips32/ 
 

AUTHORS 

First Author – Sagar Bhavsar, B.Tech (Electronics), VJTI, 
Mumbai, sagar.bhavsar1410@gmail.com 
Second Author – Akhil Rao, B.Tech (Electronics), VJTI, 
Mumbai, akhil.rao410@gmail.com 
Third Author – Abhishek Sen, B.Tech (Electronics), VJTI, 
Mumbai, abhisen1007 @gmail.com 
Fourth Author – Rohan Joshi, B.Tech (Electronics), VJTI, 
Mumbai, vakerorohan@gmail.com 
 
Correspondence Author – Sagar Bhavsar, 
sagar.bhavsar1410@gmail.com, catchsag@gmail.com, 
+919833393603

www.ijsrp.org 

A Detailed Analysis of Contemporary ARM and x86 Architectures
Emily Blem, Jaikrishnan Menon, and Karthikeyan Sankaralingam
University of Wisconsin - Madison
{blem,menon,karu}@cs.wisc.edu

Abstract
RISC vs. CISC wars raged in the 1980s when chip area and
processor design complexity were the primary constraints and
desktops and servers exclusively dominated the computing land-
scape. Today, energy and power are the primary design con-
straints and the computing landscape is signiﬁcantly different:
growth in tablets and smartphones running ARM (a RISC ISA)
is surpassing that of desktops and laptops running x86 (a CISC
ISA). Further, the traditionally low-power ARM ISA is enter-
ing the high-performance server market, while the traditionally
high-performance x86 ISA is entering the mobile low-power de-
vice market. Thus, the question of whether ISA plays an intrinsic
role in performance or energy efﬁciency is becoming important,
and we seek to answer this question through a detailed mea-
surement based study on real hardware running real applica-
tions. We analyze measurements on the ARM Cortex-A8 and
Cortex-A9 and Intel Atom and Sandybridge i7 microprocessors
over workloads spanning mobile, desktop, and server comput-
ing. Our methodical investigation demonstrates the role of ISA
in modern microprocessors’ performance and energy efﬁciency.
We ﬁnd that ARM and x86 processors are simply engineering
design points optimized for different levels of performance, and
there is nothing fundamentally more energy efﬁcient in one ISA
class or the other. The ISA being RISC or CISC seems irrelevant.

1. Introduction
The question of ISA design and speciﬁcally RISC vs. CISC
ISA was an important concern in the 1980s and 1990s when
chip area and processor design complexity were the primary
constraints [24, 12, 17, 7]. It is questionable if the debate was
settled in terms of technical issues. Regardless, both ﬂourished
commercially through the 1980s and 1990s. In the past decade,
the ARM ISA (a RISC ISA) has dominated mobile and low-
power embedded computing domains and the x86 ISA (a CISC
ISA) has dominated desktops and servers.
Recent trends raise the question of the role of the ISA and
make a case for revisiting the RISC vs. CISC question. First, the
computing landscape has quite radically changed from when the
previous studies were done. Rather than being exclusively desk-
tops and servers, today’s computing landscape is signiﬁcantly
shaped by smartphones and tablets. Second, while area and chip
design complexity were previously the primary constraints, en-
ergy and power constraints now dominate. Third, from a com-

mercial standpoint, both ISAs are appearing in new markets:
ARM-based servers for energy efﬁciency and x86-based mo-
bile and low power devices for higher performance. Thus, the
question of whether ISA plays a role in performance, power, or
energy efﬁciency is once again important.
Related Work:
Early ISA studies are instructive, but miss
key changes in today’s microprocessors and design constraints
that have shifted the ISA’s effect. We review previous com-
parisons in chronological order, and observe that all prior com-
prehensive ISA studies considering commercially implemented
processors focused exclusively on performance.
Bhandarkar and Clark compared the MIPS and VAX ISA by
comparing the M/2000 to the Digital VAX 8700 implementa-
tions [7] and concluded: “RISC as exempliﬁed by MIPS pro-
vides a signiﬁcant processor performance advantage.” In an-
other study in 1995, Bhandarkar compared the Pentium-Pro to
the Alpha 21164 [6], again focused exclusively on performance
and concluded: “...the Pentium Pro processor achieves 80% to
90% of the performance of the Alpha 21164... It uses an aggres-
sive out-of-order design to overcome the instruction set level
limitations of a CISC architecture. On ﬂoating-point intensive
benchmarks, the Alpha 21164 does achieve over twice the per-
formance of the Pentium Pro processor.” Consensus had grown
that RISC and CISC ISAs had fundamental differences that led
to performance gaps that required aggressive microarchitecture
optimization for CISC which only partially bridged the gap.
Isen et al. [22] compared the performance of Power5+ to Intel
Woodcrest considering SPEC benchmarks and concluded x86
matches the POWER ISA. The consensus was that “with ag-
gressive microarchitectural techniques for ILP, CISC and RISC
ISAs can be implemented to yield very similar performance.”
Many informal studies in recent years claim the x86’s
“crufty” CISC ISA incurs many power overheads and attribute
the ARM processor’s power efﬁciency to the ISA [1, 2]. These
studies suggest that the microarchitecture optimizations from the
past decades have led to RISC and CISC cores with similar per-
formance, but the power overheads of CISC are intractable.
In light of the prior ISA studies from decades past, the signif-
icantly modiﬁed computing landscape, and the seemingly vastly
different power consumption of ARM implementations (1-2 W)
to x86 implementations (5 - 36 W), we feel there is need to
revisit this debate with a rigorous methodology. Speciﬁcally,
considering the dominance of ARM and x86 and the multi-
pronged importance of the metrics of power, energy, and perfor-

A version appears in the 19th IEEE Intl. Symposium on High Performance Computer Architecture (HPCA 2013)

2

mance, we need to compare ARM to x86 on those three metrics.
Macro-op cracking and decades of research in high-performance
microarchitecture techniques and compiler optimizations seem-
ingly help overcome x86’s performance and code-effectiveness
bottlenecks, but these approaches are not free. The crux of our
analysis is the following: After decades of research to mitigate
CISC performance overheads, do the new approaches introduce
fundamental energy inefﬁciencies?
Challenges: Any ISA study faces challenges in separating
out the multiple implementation factors that are orthogonal to
the ISA from the factors that are inﬂuenced or driven by the
ISA. ISA-independent factors include chip process technology
node, device optimization (high-performance, low-power, or
low-standby power transistors), memory bandwidth, I/O device
effects, operating system, compiler, and workloads executed.
These issues are exacerbated when considering energy measure-
ments/analysis, since chips implementing an ISA sit on boards
and separating out chip energy from board energy presents addi-
tional challenges. Further, some microarchitecture features may
be required by the ISA, while others may be dictated by perfor-
mance and application domain targets that are ISA-independent.
To separate out the implementation and ISA effects, we con-
sider multiple chips for each ISA with similar microarchitec-
tures, use established technology models to separate out the
technology impact, use the same operating system and com-
piler front-end on all chips, and construct workloads that do not
rely signiﬁcantly on the operating system. Figure 1 presents an
overview of our approach:
the four platforms, 26 workloads,
and set of measures collected for each workload on each plat-
form. We use multiple implementations of the ISAs and speciﬁ-
cally consider the ARM and x86 ISAs representing RISC against
CISC. We present an exhaustive and rigorous analysis using
workloads that span smartphone, desktop, and server applica-
tions. In our study, we are primarily interested in whether and,
if so, how the ISA impacts performance and power. We also
discuss infrastructure and system challenges, missteps, and soft-
ware/hardware bugs we encountered. Limitations are addressed
in Section 3. Since there are many ways to analyze the raw
data, this paper is accompanied by a public release of all data
at www.cs.wisc.edu/vertical/isa-power-struggles.
Key Findings: The main ﬁndings from our study are:
◦ Large performance gaps exist across the implementations, al-
though average cycle count gaps are ≤ 2.5×.

Figure 1. Summary of Approach.
◦ Instruction count and mix are ISA-independent to ﬁrst order.
◦ Performance differences are generated by ISA-independent
microarchitecture differences.
◦ The energy consumption is again ISA-independent.
◦ ISA differences have implementation implications, but mod-
ern microarchitecture techniques render them moot; one
ISA is not fundamentally more efﬁcient.
◦ ARM and x86 implementations are simply design points op-
timized for different performance levels.
Implications: Our ﬁndings conﬁrm known conventional (or
suspected) wisdom, and add value by quantiﬁcation. Our results
imply that microarchitectural effects dominate performance,
power, and energy impacts. The overall implication of this work
is that the ISA being RISC or CISC is largely irrelevant for to-
day’s mature microprocessor design world.
Paper organization: Section 2 describes a framework we de-
velop to understand the ISA’s impacts on performance, power,
and energy. Section 3 describes our overall infrastructure and
rationale for the platforms for this study and our limitations,
Section 4 discusses our methodology, and Section 5 presents the
analysis of our data. Section 7 concludes.
2. Framing Key Impacts of the ISA
In this section, we present an intellectual framework in
which to examine the impact of the ISA—assuming a von Neu-
mann model—on performance, power, and energy. We con-
sider the three key textbook ISA features that are central to the
RISC/CISC debate: format, operations, and operands. We do
not consider other textbook features, data types and control, as
they are orthogonal to RISC/CISC design issues and RISC/CISC
approaches are similar. Table 1 presents the three key ISA fea-
tures in three columns and their general RISC and CISC char-
acteristics in the ﬁrst two rows. We then discuss contrasts for
each feature and how the choice of RISC or CISC potentially
and historically introduced signiﬁcant trade-offs in performance
and power. In the fourth row, we discuss how modern reﬁne-
ments have led to similarities, marginalizing the choice of RISC
or CISC on performance and power. Finally, the last row raises
empirical questions focused on each feature to quantify or val-
idate this convergence. Overall, our approach is to understand
all performance and power differences by using measured met-
rics to quantify the root cause of differences and whether or not

SPEC CPU200610 INT10 FPDesktopCoreMark2 WebKitMobileLighttpdCLuceneDatabase kernelsServerCortex A8Beagle BoardAtom N450Atom Dev BoardCortex A9Panda Boardi7-Core2700SandyBridge26 WorkloadsFour PlatformsOver 200 MeasuresWattsUpPower MeasuresPerfinterface to Hwperformance countersPerformancePowerRISC v CISC appears irrelevantSimulated ARM instruction mixBinary Instrumentation for x86 instruction infoOver 20,000 Data Points + Careful AnalysisA version appears in the 19th IEEE Intl. Symposium on High Performance Computer Architecture (HPCA 2013)
Table 1. Summary of RISC and CISC Trends.
Operations
◦ Simple, single function operations
◦ Single cycle

3

◦ Complex, multi-cycle instructions
◦ Transcendentals
◦ Encryption
◦ String manipulation
◦ Even w/ µ code, pipelining hard
◦ CISC latency may be longer than
compiler’s RISC equivalent

Operands
◦ Operands: registers, immediates
◦ Few addressing modes
◦ ARM: 16 general purpose registers
◦ Operands: memory, registers, immediates
◦ Many addressing modes
◦ x86: 8 32b & 6 16b registers

◦ CISC decoder complexity higher
◦ CISC has more per inst work, longer cycles
◦ Static code size: RISC > CISC

/
C
6
8
S
x
I
C

Format
M ◦ Fixed length instructions
/
◦ Relatively simple encoding
C
R
S
◦ ARM: 4B, THUMB(2B, optional)
I
A
R
◦ Variable length instructions
◦ Common insts shorter/simpler
◦ Special insts longer/complex
◦ x86: from 1B to 16B long
s ◦ CISC decode latency prevents pipelining
l
a
◦ CISC decoders slower/more area
t
s
c
a
i
r
◦ Code density: RISC < CISC
r
o
t
n
t
s
o
i
H
C
s ◦ µ -op cache minimizes decoding overheads
e
c
◦ x86 decode optimized for common insts
n
d
e
n
◦ I-cache minimizes code density impact
g
e
r
r
e
T
v
n
o
C

◦ CISC insts split into RISC-like micro-ops;
optimizations eliminated inefﬁciencies
◦ Modern compilers pick mostly RISC insts;
µ -op counts similar for ARM and x86

◦ x86 decode optimized for common insts
◦ CISC insts split into RISC-like micro-ops;
x86 and ARM µ -op latencies similar
◦ Number of data cache accesses similar

l
s
a
n
c
o
i
i
r
t
i
s
p
e
m
u
Q
E

◦ How much variance in x86 inst length?
Low variance ⇒ common insts optimized
◦ Are ARM and x86 code densities similar?
Similar density ⇒ No ISA effect
◦ What are instruction cache miss rates?
Low ⇒ caches hide low code densities

◦ Are macro-op counts similar?
Similar ⇒ RISC-like on both
◦ Are complex instructions used by x86 ISA?
Few complex ⇒ Compiler picks RISC-like
◦ Are µ -op counts similar?
Similar ⇒ CISC split into RISC-like µ -ops

◦ Number of data accesses similar?
Similar ⇒ no data access inefﬁciencies

ISA differences contribute. The remainder of this paper is cen-
tered around these empirical questions framed by the intuition
presented as the convergence trends.
Although whether an ISA is RISC or CISC seems irrelevant,
ISAs are evolving; expressing more semantic information has
led to improved performance (x86 SSE, larger address space),
better security (ARM Trustzone), better virtualization, etc. Ex-
amples in current research include extensions to allow the hard-
ware to balance accuracy with energy efﬁciency [15, 13] and ex-
tensions to use specialized hardware for energy efﬁciency [18].
We revisit this issue in our conclusions.
3. Infrastructure
We now describe our infrastructure and tools. The key take-
away is that we pick four platforms, doing our best to keep them
on equal footing, pick representative workloads, and use rigor-
ous methodology and tools for measurement. Readers can skip
ahead to Section 4 if uninterested in the details.
3.1. Implementation Rationale and Challenges
Choosing implementations presents multiple challenges due
to differences in technology (technology node, frequency, high
performance/low power transistors, etc.); ISA-independent mi-
croarchitecture (L2-cache, memory controller, memory size,
etc.); and system effects (operating system, compiler, etc.). Fi-
nally, platforms must be commercially relevant and it is unfair
to compare platforms from vastly different time-frames.
We investigated a wide spectrum of platforms spanning In-
tel Nehalem, Sandybridge, AMD Bobcat, NVIDIA Tegra-2,
NVIDIA Tegra-3, and Qualcomm Snapdragon. However, we
did not ﬁnd implementations that met all of our criteria: same
technology node across the different ISAs, identical or similar

microarchitecture, development board that supported necessary
measurements, a well-supported operating system, and similar
I/O and memory subsystems. We ultimately picked the Beagle-
board (Cortex-A8), Pandaboard (Cortex-A9), and Atom board,
as they include processors with similar microarchitectural fea-
tures like issue-width, caches, and main-memory and are from
similar technology nodes, as described in Tables 2 and 7. They
are all relevant commercially as shown by the last row in Ta-
ble 2. For a high performance x86 processor, we use an Intel i7
Sandybridge processor; it is signiﬁcantly more power-efﬁcient
than any 45nm offering, including Nehalem. Importantly, these
choices provided usable software platforms in terms of operat-
ing system, cross-compilation, and driver support. Overall, our
choice of platforms provides a reasonably equal footing, and we
perform detailed analysis to isolate out microarchitecture and
technology effects. We present system details of our platforms
for context, although the focus of our work is the processor core.
A key challenge in running real workloads was the rela-
tively small memory (512MB) on the Cortex-A8 Beagleboard.
While representative of the typical target (e.g., iPhone 4 has
512MB RAM), it presents a challenge for workloads like SPEC-
CPU2006; execution times are dominated by swapping and OS
overheads, making the core irrelevant. Section 3.3 describes
how we handled this. In the remainder of this section, we discuss
the platforms, applications, and tools for this study in detail.

3.2. Implementation Platforms

Hardware platform: We consider two chip implementations
each for the ARM and x86 ISAs as described in Table 2.
Intent: Keep non-processor features as similar as possible.

A version appears in the 19th IEEE Intl. Symposium on High Performance Computer Architecture (HPCA 2013)
Table 2. Platform Summary.
Table 3. Benchmark Summary.
ARMv7 ISA
32/64b x86 ISA
Notes
Benchmarks
Set to 4000 iterations
CoreMark
Similar to BBench
WebKit
SPECCPU2006
10 INT, 10 FP, test inputs
Represents web-serving
lighttpd
Represents web-indexing
CLucene
Represents data-streaming and
Database kernels
data-analytics

Domain
Mobile
client
Desktop
Server

4

Cortex-A8
Cortex-A9
Architecture Sandybridge Atom
OMAP3530
OMAP4430
N450
Core 2700
Processor
1
2
Cores
4
1
0.6 GHz
1 GHz
1.66 GHz
3.4 GHz
Frequency
2-way
2-way
2-way
4-way
Width
In Order
OoO
In Order
Issue
OoO
16 KB
32 KB
24 KB
32 KB
L1 Data
16 KB
32 KB
32 KB
L1 Inst
32 KB
256 KB
1 MB/chip
L2
256 KB/core 512 KB
—
—
—
8 MB/chip
L3
256 MB
1 GB
1 GB
16 GB
Memory
NEON
NEON
SSE
SIMD
AVX
60 mm2
70 mm2
216 mm2
66 mm2
Area
65 nm
45 nm
Tech Node
32 nm
45 nm
Beagleboard
Pandaboard
Desktop Dev Board
Platform
iPhone 4, 3GS
Galaxy S-III
Desktop Netbook
Products
Motorola Droid
Galaxy S-II
Lava Xolo
Data from TI OMAP3530, TI OMAP4430, Intel Atom N450, and Intel
i7-2700 datasheets, www.beagleboard.org & www.pandaboard.org
Operating system: Across all platforms, we run the same
stable Linux 2.6 LTS kernel with some minor board-speciﬁc
patches to obtain accurate results when using the performance
counter subsystem. We use perf’s1 program sampling to ﬁnd
the fraction of time spent in the kernel while executing the SPEC
benchmarks on all four boards; overheads were less than 5% for
all but GemsFDTD and perlbench (both less than 10%) and the
fraction of time spent in the operating system was virtually iden-
tical across platforms spanning ISAs.
Intent: Keep OS effects as similar as possible across platforms.
Compiler: Our toolchain is based on a validated gcc 4.4 based
cross-compiler conﬁguration. We intentionally chose gcc so
that we can use the same front-end to generate all binaries. All
target independent optimizations are enabled (O3); machine-
speciﬁc tuning is disabled so there is a single set of ARM bi-
naries and a single set of x86 binaries. For x86 we target 32-bit
since 64-bit ARM platforms are still under development. For
ARM, we disable THUMB instructions for a more RISC-like
ISA. We ran experiments to determine the impact of machine-
speciﬁc optimizations and found that these impacts were less
than 5% for over half of the SPEC suite, and caused performance
variations of ±20% on the remaining with speed-ups and slow-
downs equally likely. None of the benchmarks include SIMD
code, and although we allow auto-vectorization, very few SIMD
instructions are generated for either architecture. Floating point
is done natively on the SSE (x86) and NEON (ARM) units. Ven-
dor compilers may produce better code for a platform, but we
use gcc to eliminate compiler inﬂuence. As seen in Table 12 in
Appendix I, static code size is within 8% and average instruction
lengths are within 4% using gcc and icc for SPEC INT, so we
expect that compiler does not make a signiﬁcant difference.
Intent: Hold compiler effects constant across platforms.
3.3. Applications
Since both ISAs are touted as candidates for mobile clients,
desktops, and servers, we consider a suite of workloads that span

1 perf is a Linux utility to access performance counters.

these. We use prior workload studies to guide our choice, and
where appropriate we pick equivalent workloads that can run on
our evaluation platforms. A detailed description follows and is
summarized in Table 3. All workloads are single-threaded to
ensure our single-core focus.
Mobile client: This category presented challenges as mobile
client chipsets typically include several accelerators and careful
analysis is required to determine the typical workload executed
on the programmable general-purpose core. We used CoreMark
(www.coremark.org), widely used in industry white-papers,
and two WebKit regression tests informed by the BBench
study [19]. BBench, a recently proposed smartphone bench-
mark suite, is a “a web-page rendering benchmark comprising
11 of the most popular sites on the internet today” [19]. To avoid
web-browser differences across the platforms, we use the cross-
platform WebKit with two of its built-in tests that mimic real-
world HTML layout and performance scenarios for our study2 .
Desktop: We use the SPECCPU2006 suite (www.spec.org)
as representative of desktop workloads. SPECCPU2006 is a
well understood standard desktop benchmark, providing insights
into core behavior. Due to the large memory footprint of the
train and reference inputs, we found that for many benchmarks
the memory constrained Cortex-A8, in particular, ran of mem-
ory and execution was dominated by system effects. Instead, we
report results using the test inputs, which ﬁt in the Cortex-A8’s
memory footprint for 10 of 12 INT and 10 of 17 FP benchmarks.
Server: We chose server workloads informed by the Cloud-
Suite workloads recently proposed by Ferdman et al. [16]. Their
study characterizes server/cloud workloads into data analytics,
data streaming, media streaming, software testing, web search,
and web serving. The actual software implementations they
provide are targeted for large memory-footprint machines and
their intent is to benchmark the entire system and server clus-
ter. This is unsuitable for our study since we want to iso-
late processor effects. Hence, we pick implementations with
small memory footprints and single-node behavior. To represent
data-streaming and data-analytics, we use three database ker-
nels commonly used in database evaluation work [26, 23] that
capture the core computation in Bayes classiﬁcation and data-
store3 . To represent web search, we use CLucene (clucene.

2 Speciﬁcally coreLayout and DOMPerformance.
3CloudSuite uses Hadoop+Mahout plus additional software infrastructure,
ultimately running Bayes classiﬁcation and data-store; we feel this kernel ap-
proach is better suited for our study while capturing the domain’s essence.

A version appears in the 19th IEEE Intl. Symposium on High Performance Computer Architecture (HPCA 2013)
Table 4. Infrastructure Limitations.
Implications
2nd order for core design
Best effort
Best effort
µ arch effect, not ISA
Out of scope
Out of scope
See server benchmarks
Tracks emerging uses
CloudSuite more relevant
gcc optimizations uniform
<10%
Results show 2nd order
4-17%
Validated use (Table 5)
Validated use (Table 5)
Second-order
Second-order
Best effort; extant nodes

Limitation
Multicore effects: coherence, locking...
No platform uniformity across ISAs
No platform diversity within ISAs
Design teams are different
“Pure” RISC, CISC implementations
n Ultra low power microcontrollers
Server style platforms
i
a
m
Why SPEC on mobile platforms?
o
D
Why not SPEC JBB or TPC-C?
Proprietary compilers are optimized
Arch. speciﬁc compiler tuning
No direct decoder power measure
Power includes non-core factors
Performance counters may have errors
Simulations have errors
g Memory rate effects cycles nonlinearly
n
Vmin limit effects frequency scaling
i
l
a
c
ITRS scaling numbers are not exact
S

s
e
r
o
C

s
l
o
o
T

5

allows us to eliminate the main memory and I/O power and ex-
amine only processor power. We validated our strategy for the
i7 system using the exposed energy counters (the only platform
we consider that includes isolated power measures). Across all
three benchmark suites, our WattsUp methodology compared to
the processor energy counter reports ranged from 4% to 17%
less, averaging 12%. Our approach tends to under-estimate core
power, so our results for power and energy are optimistic. We
saw average power of 800mW, 1.2W, 5.5W, and 24W for A8,
A9, Atom, and i7 (respectively) and these fall within the typical
vendor-reported power numbers.
Technology scaling and projections: Since the i7 processor
is 32nm and the Cortex-A8 is 65nm, we use technology node
characteristics from the 2007 ITRS tables to normalize to the
45nm technology node in two results where we factor out tech-
nology; we do not account for device type (LOP, HP, LSTP).
For our 45nm projections, the A8’s power is scaled by 0.8× and
the i7’s power by 1.3×.
In some results, we scale frequency
to 1 GHz, accounting for DVFS impact on voltage using the
mappings disclosed for Intel SCC [5]. When frequency scal-
ing, we assume that 20% of the i7’s power is static and does
not scale with frequency; all other cores are assumed to have
negligible static power. When frequency scaling, A8’s power is
scaled by 1.2×, Atom’s power by 0.8×, and i7’s power by 0.6×.
We acknowledge that this scaling introduces some error to our
technology-scaled power comparison, but feel it is a reasonable
strategy and doesn’t affect our primary ﬁndings (see Table 4).
Emulated instruction mix measurement: For the x86 ISA,
we use DynamoRIO [11] to measure instruction mix. For the
ARM ISA, we leverage the gem5 [8] simulator’s functional em-
ulator to derive instruction mixes (no ARM binary emulation
available). Our server and mobile-client benchmarks use many
system calls that do not work in the gem5 functional mode.
We do not present detailed instruction-mix analysis for these,
but instead present high-level mix determined from performance
counters. We use the MICA tool to ﬁnd the available ILP [20].
3.5. Limitations or Concerns
Our study’s limitations are classiﬁed into core diversity, do-
main, tool, and scaling effects. The full list appears in Table 4,
and details are discussed below. Throughout our work, we fo-
cus on what we believe to be the ﬁrst order effects for perfor-
mance, power, and energy and feel our analysis and methodol-
ogy is rigorous. Other more detailed methods may exist, and we
have made the data publicly available at www.cs.wisc.edu/
vertical/isa-power-struggles to allow interested readers
to pursue their own detailed analysis.
Cores: We considered four platforms, two from each ISA. A
perfect study would include platforms at several performance
levels with matched frequency, branch predictors, other microar-
chitectural features, and memory systems. Further, a pure RISC
versus CISC study would use true RISC and CISC cores, while
ARM and x86’s ISA tweaks represent the current state-of-the-
art. Our selections reﬂect the available, well-supported imple-

sourceforge.net), an efﬁcient, cross-platform indexing im-
plementation similar to CloudSuite’s Nutch. To represent web-
serving (CloudSuite uses Apache), we use the lighttpd server
(www.lighttpd.net) which is designed for “security, speed,
compliance, and ﬂexibility”4 . We do not evaluate the media-
streaming CloudSuite benchmark as it primarily stresses the I/O
subsystem. CloudSuite’s Software Testing benchmark is a batch
coarse-grained parallel symbolic execution application; for our
purposes, the SPEC suite’s Perl parser, combinational optimiza-
tion, and linear programming benchmarks are similar.
3.4. Tools
The four main tools we use in our work are described below
and Table 5 in Section 4 describes how we use them.
Native execution time and microarchitectural events: We
use wall-clock time and performance-counter-based clock-cycle
measurements to determine execution time of programs. We
also use performance counters to understand microarchitecture
inﬂuences on the execution time. Each of the processors has
different counters available, and we examined them to ﬁnd com-
parable measures. Ultimately three counters explain much of
the program behavior: branch mis-prediction rate, Level-1 data-
cache miss rate, and Level-1 instruction-cache miss rate (all
measured as misses per kilo-instructions). We use the perf tool
for performance counter measurement.
Power: For power measurements, we connect a Wattsup
(www.wattsupmeters.com) meter to the board (or desktop)
power supply. This gives us system power. We run the bench-
mark repeatedly to ﬁnd consistent average power as explained in
Table 5. We use a control run to determine the board power alone
when the processor is halted and subtract away this board power
to determine chip power. Some recent power studies [14, 21, 9]
accurately isolate the processor power alone by measuring the
current supply line of the processor. This is not possible for
the SoC-based ARM development boards, and hence we deter-
mine and then subtract out the board-power. This methodology

4Real users of lighttpd include YouTube.

A version appears in the 19th IEEE Intl. Symposium on High Performance Computer Architecture (HPCA 2013)
Table 5. Methodology Summary.
(a) Native Execution on Real Hardware

6

Measures
Execution time,
Cycle counts

Inst. count (ARM)

Inst. count (x86)

Inst. mix (Coarse)

Inst. length (x86)

Microarch events

Full system power

Board power

Processor power

Methodology
◦ Approach: Use perf tool to sample cycle performance counters; sampling avoids potential counter overﬂow.
◦ Analysis: 5 - 20 trials (dependent on variance and benchmark runtime); report minimum from trials that complete normally.
◦ Validation: Compare against wall clock time.
◦ Approach: Use perf tool to collect macro-ops from performance counters
◦ Analysis: At least 3 trials; report minimum from trials that complete normally.
◦ Validation: Performance counters within 10% of gem5 ARM simulation. Table 9 elaborates on challenges.
◦ Approach: Use perf to collect macro-ops and micro-ops from performance counters.
◦ Analysis: At least 3 trials; report minimum from trials that complete normally.
◦ Validation: Counters within 2% of DynamoRIO trace count (macro-ops only). Table 9 elaborates on challenges.
◦ Approach: SIMD + FP + load/store performance counters.
◦ Approach: Wrote Pin tool to ﬁnd length of each instruction and keep running average.
◦ Approach: Branch mispredictions, cache misses, and other uarch events measured using perf performance counters.
◦ Analysis: At least 3 trials; additional if a particular counter varies by > 5%. Report minimum from normal trials.
◦ Set-up: Use Wattsup meter connected to board or desktop
(no network connection, peripherals on separate supply, kernel DVFS disabled, cores at peak frequency, single-user mode).
◦ Approach: Run benchmarks in loop to guarantee 3 minutes of samples (180 samples at maximum sampling rate).
◦ Analysis: If outliers occur, rerun experiment; present average power across run without outliers.
◦ Set-up: Use Wattsup meter connected to board or desktop
(no network connection, peripherals on separate supply, kernel DVFS disabled, cores at peak frequency, single-user mode).
◦ Approach: Run with kernel power saving enabled; force to lowest frequency. Issue halt; report power when it stabilizes.
◦ Analysis: Report minimum observed power.
◦ Approach: Subtracting above two gives processor power.
◦ Validation: compare core power against energy performance counters and/or reported TDP and power draw.

(b) Emulated Execution

Measures
Inst. mix (Detailed)

ILP

Methodology
◦ Approach (ARM): Use gem5 instruction trace and analyze using python script.
◦ Approach (x86): Use DynamoRIO instruction trace and analyze using python script.
◦ Validation: Compare against coarse mix from SIMD + FP + load/store performance counters.
◦ Approach: Pin based MICA tool which reports ILP with window size 32, 64, 128, 256.

mentations available to our group. The impact of higher per-
formance emerging cores is included in our synthetic processor
study.
Domain: We picked a representative set of workloads we feel
captures a signiﬁcant subset of modern workloads. We do not
make broad domain-speciﬁc arguments, since that requires truly
representative inputs and IO subsystem control for the mobile
and server domains. Our study focused on single-core, and thus
intentionally avoids multi-core system issues (e.g., consistency
models, coherence, virtualization, etc.).
Measurement and tool errors: Our measurements are pri-
marily on real hardware, and therefore include real world errors.
We execute multiple runs and take a rigorous approach as de-
tailed in Table 5. Eliminating all errors is impractical, and our
ﬁnal result trends are consistent and intuitive.
Analysis: We have presented our analysis of this rich data set,
and will release the data and our analysis scripts to allow inter-
ested readers to pursue their own detailed analysis.
4. Methodology
In this section, we describe how we use our tools and the
overall ﬂow of our analysis. Section 5 presents our data and

analysis. Table 5 describes how we employ the aforementioned
tools and obtain the measures we are interested in, namely, ex-
ecution time, execution cycles, instruction-mix, microarchitec-
ture events, power, and energy.
Our overall approach is to understand all performance and
power differences and use the measured metrics to quantify the
root cause of differences and whether or not ISA differences
contribute, answering empirical questions from Section 2. Un-
less otherwise explicitly stated, all data is measured on real hard-
ware. The ﬂow of the next section is outlined below.
4.1. Performance Analysis Flow
Step 1: Present execution time for each benchmark.
Step 2: Normalize frequency’s impact using cycle counts.
Step 3: To understand differences in cycle count and the inﬂu-
ence of the ISA, present the dynamic instruction count measures,
measured in both macro-ops and micro-ops.
Step 4: Use instruction mix, code binary size, and average dy-
namic instruction length to understand ISA’s inﬂuence.
Step 5: To understand performance differences not attributable
to ISA, look at detailed microarchitecture events.
Step 6: Attribute performance gaps to frequency, ISA, or ISA-

A version appears in the 19th IEEE Intl. Symposium on High Performance Computer Architecture (HPCA 2013)

7

independent microarchitecture features. Qualitatively reason
about whether the ISA forces microarchitecture features.
4.2. Power and Energy Analysis Flow
Step 1: Present per benchmark raw power measurements.
Step 2: To factor out
the impact of technology, present
technology-independent power by scaling all processors to
45nm and normalizing the frequency to 1 GHz.
Step 3: To understand the interplay between power and perfor-
mance, examine raw energy.
Step 4: Qualitatively reason about the ISA inﬂuence on microar-
chitecture in terms of energy.
4.3. Trade-off Analysis Flow
Step 1: Combining the performance and power measures, com-
pare the processor implementations using Pareto-frontiers.
Step 2: Compare measured and synthetic processor implemen-
tations using Energy-Performance Pareto-frontiers.
5. Measured Data Analysis and Findings
We now present our measurements and analysis of perfor-
mance, power, energy, and the trade-offs between them. We
conclude the section with sensitivity studies projecting perfor-
mance of additional implementations of the ARM and x86 ISA
using a simple performance and power model.
We present our data for all four platforms, often comparing
A8 to Atom (both dual-issue in-order) and A9 to i7 (both OOO)
since their implementations are pair-wise similar. For each step,
we present the average measured data, average in-order and OoO
ratios if applicable, and then our main ﬁndings. When our analy-
sis suggests that some benchmarks are outliers, we give averages
with the outliers included in parentheses.
5.1. Performance Analysis
Step 1: Execution Time Comparison
Data: Figure 2 shows execution time normalized to i7; av-
erages including outliers are given using parentheses. Average
ratios are in table below. Per benchmark data is in Figure 16 of
Appendix I.

Finding P1: Large performance gaps are platform and bench-
mark dependent: A9 to i7 performance gaps range from 5× to
102× and A8 to Atom gaps range from 2× to 997×.
Key Finding 1: Large performance gaps exist across the four
platforms studied, as expected, since frequency ranges from 600
MHz to 3.4 GHz and microarchitectures are very different.
Step 2: Cycle-Count Comparison

Data: Figure 3 shows cycle counts normalized to i7. Per
benchmark data is in Figure 7.

Figure 3. Cycle Count Normalized to i7.

Server
SPEC INT SPEC FP
Ratio
Mobile
1.3 (23)
1.5 (2.7)
1.2
A8 to Atom 1.2 (12)
2.2
2.5
2.1 (7.0)
A9 to i7
1.7
Finding P2: Per suite cycle count gaps between out-of-order
implementations A9 and i7 are less than 2.5× (no outliers).
Finding P3: Per suite cycle count gaps between in-order im-
plementations A8 and Atom are less than 1.5× (no outliers).
Key Finding 2: Performance gaps, when normalized to cycle
counts, are less than 2.5× when comparing in-order cores to
each other and out-of-order cores to each other.
Step 3: Instruction Count Comparison

Data: Figure 4a shows dynamic instruction (macro) counts on
A8 and Atom normalized to Atom x86 macro-instructions. Per
benchmark data is in Figure 17a of Appendix I. Per benchmark
data for CPIs is in Table 11 in Appendix I.
Data: Figure 4b shows dynamic micro-op counts for Atom
and i7 normalized to Atom macro-instructions5 . Per benchmark
data is in Figure 17b. of Appendix I

Figure 2. Execution Time Normalized to i7.
Server
SPEC INT SPEC FP
Ratio
Mobile
3.7 (103)
4.2 (7.4)
3.5
A8 to Atom 3.4 (34)
A9 to i7
7.4
5.8
8.4
7.2 (23)
Outliers: A8 performs particularly poorly on WebKit tests
and lighttpd, skewing A8/Atom differences in the mobile and
server data, respectively; see details in Step 2. Five SPEC FP
benchmarks are also considered outliers; see Table 8. Where
outliers are listed, they are in this set.

(b) Micro-Ops
(a) Macro-Ops
Figure 4. Instructions Normalized to i7 macro-ops.
5 For i7, we use issued micro-ops instead of retired micro-ops; we found that
on average, this does not impact the micro-op/macro-op ratio.

MobileSPEC INTSPEC FPServer051015202530Normalized Time(130)(72)(24)(344)A8AtomA9I7MobileSPEC INTSPEC FPServer0246810Normalized Cycles(23)(13)(7)(61)A8AtomA9I7MobileSPEC INTSPEC FPServer0.00.51.01.52.0Normalized Macro-Ops(3.2)ARMx86MobileSPEC INTSPEC FPServer0.00.51.01.52.0Normalized Micro-Ops(1.5)Atomi7A version appears in the 19th IEEE Intl. Symposium on High Performance Computer Architecture (HPCA 2013)

8

Outliers: For wkperf and lighttpd, A8 executes more than
twice as many instructions as A96 . We report A9 instruction
counts for these two benchmarks. For CLucene, x86 machines
execute 1.7× more instructions than ARM machines; this ap-
pears to be a pathological case of x86 code generation inefﬁ-
ciencies. For cactusADM, Atom executes 2.7× more micro-ops
than macro-ops; this extreme is not seen for other benchmarks.
Finding P4: Instruction count similar across ISAs. Implies
gcc picks the RISC-like instructions from the x86 ISA.
Finding P5: All ARM outliers in SPEC FP due to transcen-
dental FP operations supported only by x86.
Finding P6: x86 micro-op to macro-op ratio is often less than
1.3×, again suggesting gcc picks the RISC-like instructions.
Key Finding 3: Instruction and cycle counts imply CPI is less
on x86 implementations: geometric mean CPI is 3.4 for A8, 2.2
for A9, 2.1 for Atom, and 0.7 for i7 across all suites. x86 ISA
overheads, if any, are overcome by microarchitecture.
Step 4: Instruction Format and Mix
Data: Table 6a shows average ARM and x86 static binary
sizes, measuring only the binary’s instruction segment. Per
benchmark data is in Table 12a in Appendix I.
Data: Table 6b shows average dynamic ARM and x86 in-
struction lengths. Per benchmark data is in Table 12b in Ap-
pendix I.

Table 6. Instruction Size Summary.
(b) Instruction Length (B)
(a) Binary Size (MB)
ARM
x86
ARM
x86

e Minimum
l
i
Average
b
o
M
Maximum
T Minimum
p
o
t
Average
N
k
s
I
e
Maximum
D

p
o
t
P
k
F
s
e
D

Minimum
Average
Maximum

0.02
0.95
1.30

0.53
1.47
3.88

0.66
1.70
4.75

0.02
0.87
1.42

0.65
1.46
4.05

0.74
1.73
5.24

4.0
4.0
4.0

4.0
4.0
4.0

4.0
4.0
4.0

2.4
3.3
3.7

2.7
3.1
3.5

2.6
3.4
6.4

2.5
4.0
0.18
0.12
r Minimum
e
3.2
4.0
0.59
0.39
Average
v
r
e
3.7
4.0
1.00
0.47
Maximum
S
Outliers: CLucene binary (from server suite) is almost 2×
larger for x86 than ARM; the server suite thus has the largest
span in binary sizes. ARM executes correspondingly few in-
structions; see outliers discussion in Step 3.
Finding P7: Average ARM and x86 binary sizes are simi-
lar for SPEC INT, SPEC FP, and Mobile workloads, suggesting
similar code densities.
Finding P8: Executed x86 instructions are on average up to
25% shorter than ARM instructions: short, simple x86 instruc-
tions are typical.
Finding P9: x86 FP benchmarks, which tend to have more
complex instructions, have instructions with longer encodings
(e.g., cactusADM with 6.4 Bytes/inst on average).

6A8 spins for IO, event-loops, and timeouts.

Data: Figure 5 shows average coarse-grained ARM and x86
instruction mixes for each benchmark suite7 .

Figure 5. Instruction Mix (Performance Counters).

Data: Figure 6 shows ﬁne-grained ARM and x86 instruction
mixes normalized to x86 for a subset of SPEC benchmarks7 .

Figure 6. Selected Instruction Counts (Emulated).

Finding P10: Fraction of loads and stores similar across ISA
for all suites, suggesting that the ISA does not lead to signiﬁcant
differences in data accesses.
Finding P11: Large instruction counts for ARM are due
to absence of FP instructions like fsincon, fyl2xpl, (e.g.,
tonto in Figure 6’s many special x86 instructions correspond
to ALU/logical/multiply ARM instructions).
Key Finding 4: Combining the instruction-count and mix-
ﬁndings, we conclude that ISA effects are indistinguishable be-
tween x86 and ARM implementations.
Step 5: Microarchitecture

Data: Figure 7 shows the per-benchmark cycle counts for
more detailed analysis where performance gaps are large. The
raw data for this ﬁgure is in the Cycles worksheet of our pub-
licly released spreadsheet [10].
Data: Table 7 compares the A8 microarchitecture to Atom,
and A9 to i7, focusing on the primary structures. These details
are from ﬁve Microprocessor Report articles8 and the A9 num-
bers are estimates derived from publicly disclosed information
on A15 and A9/A15 comparisons.
7 x86 instructions with memory operands are cracked into a memory opera-
tion and the original operation.
8 “Cortex-A8 High speed, low power” (Nov 2005), “More applications for
OMAP4” (Nov 2009), “ Sandybridge spans generations” (Sept 2010), “Intel’s
Tiny Atom” (April 2008), “Cortex A-15 Eagle Flies the Coop” (Nov 2010).
9 60 for A15.

ARMx86ARMx86ARMx86ARMx8620%40%60%80%100%Percent of psuedo-µopsLoadStoreBranchOtherMobileSPEC INTSPEC FPServerARMx86ARMx86ARMx86ARMx860.00.51.01.52.02.53.03.54.0Fraction of x86 pseudo-µopsgccomnetppsoplextontoLoadStoreBranchMoveALULogicalMulDivSpecialOtherA version appears in the 19th IEEE Intl. Symposium on High Performance Computer Architecture (HPCA 2013)

9

(a) In-Order

Figure 7. Cycle Counts Normalized to i7.

(b) Out-of-Order

(a) In-Order
(b) Out-of-Order
Figure 8. Branch Misses per 1000 ARM Instructions.

A8
Atom

Table 7. Processor Microarchitecture Features.
(a) In-Order Cores
ALU/FP
Pipeline
Issue
Threads
Depth Width
Units
2/2 + NEON
1
2
13
2
2
16+2
2/2 + IMul
(b) Out-of-Order Cores
Issue Threads ROB
Entries for
Size LD/ST Rename Scheduler
width
- 9
64/36

4
4(6)

20
54

-/4
160

56
168

A9
i7

1
2

Br. Pred.
BTB Entries
512
128

BTB

512
8K - 16K

Finding P14: Observe large microarchitectural event count
differences (e.g., A9 branch misses are more common than i7
branch misses). These differences are not because of the ISA,
but rather due to microarchitectural design choices (e.g., A9’s
BTB has 512 entries versus i7’s 16K entries).
Finding P15: Per benchmark, we can attribute the largest
gaps in i7 to A9 performance (and in Atom to A8 performance)
to speciﬁc microachitectural events.
In the interest of space,
we present example analyses for those benchmarks with gaps
greater than 3.0× in Table 8; bwaves details are in Appendix II.
Key Finding 5: The microarchitecture has signiﬁcant impact on
performance. The ARM and x86 architectures have similar in-
struction counts. The highly accurate branch predictor and large
caches, in particular, effectively allow x86 architectures to sus-
tain high performance. x86 performance inefﬁciencies, if any,
are not observed. The microarchitecture, not the ISA, is respon-
sible for performance differences.
Step 6: ISA inﬂuence on microarchitecture
Key Finding 6: As shown in Table 7, there are signiﬁcant dif-
ferences in microarchitectures. Drawing upon instruction mix
and instruction count analysis, we feel that the only case where
the ISA forces larger structures is on the ROB size, physical
rename ﬁle size, and scheduler size since there are almost the
same number of x86 micro-ops in ﬂight compared to ARM in-
structions. The difference is small enough that we argue it is not
necessary to quantify further. Beyond the translation to micro-
ops, pipelined implementation of an x86 ISA introduces no addi-
tional overheads over an ARM ISA for these performance levels.

Finding P12: A9 and i7’s different issue widths (2 versus
4, respectively)10 explain performance differences up to 2×, as-
suming sufﬁcient ILP, a sufﬁcient instruction window and a well
balanced processor pipeline. We use MICA to conﬁrm that our
benchmarks all have limit ILP greater than 4 [20].
Finding P13: Even with different ISAs and signiﬁcant differ-
ences in microarchitecture, for 12 benchmarks, the A9 is within
2× the cycle count of i7 and can be explained by the difference
in issue width.
Data: Figures 8, 9, and 10 show branch mispredictions & L1
data and instruction cache misses per 1000 ARM instructions.
The raw data for these ﬁgures is in the Branch Misses, L1
Data Misses, and L1 Inst Misses worksheets, respectively,
of our publicly released spreadsheet [10].

10We assume the conventional wisdom that A9 is dual issue, although its
pipeline diagrams indicate it is quad-issue.

coremarkwk_layoutwk_perfmeanastarlibquantumhmmerh264gobmkbzip2sjenggccperlbenchomnetppmeansoplexGemsFDTDcalculixpovraytontonamdleslie3DmilccactusADMbwavesmeanlucenedb_kernelslighttpdmean02468101214Normalized Cycles152538642317661A8Atomwk_perfwk_layoutcoremarkmeanastarhmmerlibquantumgobmksjenggccperlbenchh264bzip2omnetppmeansoplexGemsFDTDcalculixnamdpovraytontocactusADMmilcleslie3Dbwavesmeanlucenelighttpddb_kernelsmean02468101214Normalized Cycles30A9i7coremarkwk_layoutwk_perfmeanastarlibquantumhmmerh264gobmkbzip2sjenggccperlbenchomnetppmeansoplexGemsFDTDcalculixpovraytontonamdleslie3DmilccactusADMbwavesmeanlucenedb_kernelslighttpdmean0102030405060 Branch MPKI71373A8Atomwk_perfwk_layoutcoremarkmeanastarhmmerlibquantumgobmksjenggccperlbenchh264bzip2omnetppmeansoplexGemsFDTDcalculixnamdpovraytontocactusADMmilcleslie3Dbwavesmeanlucenelighttpddb_kernelsmean0102030405060Branch MPKIA9i7A version appears in the 19th IEEE Intl. Symposium on High Performance Computer Architecture (HPCA 2013)

10

(a) In-Order
(b) Out-of-Order
Figure 9. Data L1 Misses per 1000 ARM Instructions.

(a) In-Order
(b) Out-of-Order
Figure 10. Instruction Misses per 1000 ARM Instructions.

5.2. Power and Energy Analysis
In this section, we normalize to A8 as it uses the least power.
Step 1: Average Power
Data: Figure 11 shows average power normalized to the A8.
Per benchmark data is in Figure 18 of Appendix I.

Figure 11. Raw Average Power Normalized to A8.

Server
SPEC INT SPEC FP
Mobile
Ratio
3.0
3.1
3.1
3.0
Atom to A8
21
17
20
i7 to A9
20
Key Finding 7: Overall x86 implementations consume signiﬁ-
cantly more power than ARM implementations.
Step 2: Average Technology Independent Power
Data: Figure 12 shows technology-independent average
power–cores are scaled to 1 GHz at 45nm (normalized to A8).
Per benchmark data is in Figure 19 of Appendix I.
SPEC INT SPEC FP
Mobile
Ratio
Server
Atom to A8
0.6
0.6
0.6
0.6
i7 to A9
7.0
6.1
7.4
7.6
Finding E1: With frequency and technology scaling, ISA ap-
pears irrelevant for power optimized cores: A8, A9, and Atom
are all within 0.6× of each other (A8 consumes 29% more power
than A9). Atom is actually lower power than A8 or A9.

Figure 12. Tech. Independent Avg. Power Normalized to A8.

Finding E2: i7 is performance, not power, optimized. Per
suite power costs are 6.1× to 7.6× higher for i7 than A9 with
1.7× to 7.0× higher frequency-independent performance (Fig-
ure 3 cycle count performance).
Key Finding 8: The choice of power or performance optimized
core designs impacts core power use more than ISA.
Step 3: Average Energy
Data: Figure 13 shows energy (product of power and time).
Per benchmark data is in Figure 20 of Appendix I.

Figure 13. Raw Average Energy Normalized to A8.

coremarkwk_layoutwk_perfmeanastarlibquantumhmmerh264gobmkbzip2sjenggccperlbenchomnetppmeansoplexGemsFDTDcalculixpovraytontonamdleslie3DmilccactusADMbwavesmeanlucenedb_kernelslighttpdmean0102030405060L1 Data MPKI498A8Atomwk_perfwk_layoutcoremarkmeanastarhmmerlibquantumgobmksjenggccperlbenchh264bzip2omnetppmeansoplexGemsFDTDcalculixnamdpovraytontocactusADMmilcleslie3Dbwavesmeanlucenelighttpddb_kernelsmean0102030405060L1 Data MPKIA9i7coremarkwk_layoutwk_perfmeanastarlibquantumhmmerh264gobmkbzip2sjenggccperlbenchomnetppmeansoplexGemsFDTDcalculixpovraytontonamdleslie3DmilccactusADMbwavesmeanlucenedb_kernelslighttpdmean0102030405060Inst Cache MPKI269A8Atomwk_perfwk_layoutcoremarkmeanastarhmmerlibquantumgobmksjenggccperlbenchh264bzip2omnetppmeansoplexGemsFDTDcalculixnamdpovraytontocactusADMmilcleslie3Dbwavesmeanlucenelighttpddb_kernelsmean0102030405060Inst Cache MPKIA9i7MobileSPEC INTSPEC FPServer0510152025303540Normalized PowerA8AtomA9I7MobileSPEC INTSPEC FPServer012345678Normalized TI PowerA8AtomA9I7MobileSPEC INTSPEC FPServer0.00.20.40.60.81.01.21.41.6Normalized EnergyA8AtomA9I7A version appears in the 19th IEEE Intl. Symposium on High Performance Computer Architecture (HPCA 2013)
Table 8. Detailed Analysis for Benchmarks with A9 to i7 Gap Greater Than 3×.
Analysis

Benchmark

Gap

11

omnetpp
db kernels
tonto
cactusADM
milc
leslie3D

bwaves

3.4
3.8
6.2
6.6
8.0
8.4

30

Branch MPKI: 59 for A9 versus only 2.0 for i7; I-Cache MPKI: 33 for A9 versus only 2.2 for i7.
1.6× more instructions, 5× more branch MPKI for A9 than i7.
Instructions: 4× more for ARM than x86.
Instructions: 2.8× more for ARM than x86.
A9 and i7 both experience more than 50 data cache MPKI. i7’s microarchitecture hides these misses more effectively.
4× as many L2 cache misses using the A8 than using the Atom explains the 2× A8 to Atom gap. On the A9, the data cache
MPKI is 55, compared to only 30 for the i7.
324× more branch MPKI, 17.5× more instructions, 4.6× more instruction MPKI, and 6× more L2 cache misses on A8 than
Atom. A9 has similar trends, including 1000× more branch MPKI than the i7.

Mobile
Ratio
A8 to Atom 0.8(0.1)
i7 to A9
3.3

SPEC INT SPEC FP
0.8 (0.6)
0.9
1.7
1.7 (1.0)

Server
0.8(0.2)
1.8

Finding E3: Despite power differences, Atom consumes less
energy than A8 and i7 uses only slightly more energy than A9
due primarily to faster execution times, not ISA.
Finding E4: For “hard” benchmarks with high cache miss
rates that leave the core poorly utilized (e.g., many in SPEC
FP), ﬁxed energy costs from structures provided for high-
performance make i7’s energy 2× to 3× worse than A9.
Key Finding 9: Since power and performance are both primar-
ily design choices, energy use is also primarily impacted by de-
sign choice. ISA’s impact on energy is insigniﬁcant.

Step 4: ISA impact on microarchitecture.
Data: Table 7 outlined microarchitecture features.
Finding E5: The energy impact of the ISA is that it requires
micro-ops translation and an additional micro-ops cache. Fur-
ther, since the number of micro-ops is not signiﬁcantly higher,
the energy impact of x86 support is small.
Finding E6: Other power-hungry structures like a large L2-
cache, highly associative TLB, aggressive prefetcher, and large
branch predictor seem dictated primarily by the performance
level and application domain targeted by the Atom and i7 pro-
cessors and are not necessitated by x86 ISA features.
5.3. Trade-off Analysis
Step 1: Power- Performance Trade-offs
Data:
Figure 14 shows the geometric mean power-
performance trade-off for all benchmarks using technology node
scaled power. We generate a cubic curve for the power-
performance trade-off curve. Given our small sample set, a
core’s location on the frontier does not imply that it is optimal.

Figure 14. Power Performance Trade-offs.
Finding T1: A9 provides 3.5× better performance using 1.8×
the power of A8.

Finding T2: i7 provides 6.2× better performance using 10.9×
the power of Atom.

Finding T3: i7’s microarchitecture has high energy cost when
performance is low: benchmarks with the smallest performance
gap between i7 and A9 (star in Figure 14) 11 have only 6× better
performance than A9 but use more than 10× more power.
Key Finding 10: Regardless of ISA or energy-efﬁciency,
high-performance processors require more power than lower-
performance processors. They follow well established cubic
power/performance trade-offs.
Step 2: Energy-Performance Trade-offs

Data:
Figure 15 shows the geometric mean energy-
performance trade-off using technology node scaled energy. We
generate a quadratic energy-performance trade-off curve. Again,
a core’s location on the frontier does not imply optimality. Syn-
thetic processor points beyond the four processors studied are
shown using hollow points; we consider a performance targeted
ARM core (A15) and frequency scaled A9, Atom, and i7 cores.
A15 BIPS are from reported CoreMark scores; details on syn-
thetic points are in Appendix III.

Figure 15. Energy Performance Trade-offs.

Finding T4: Regardless of ISA, power-only or performance-
only optimized cores have high energy overheads (e.g., A8 and
i7).

Finding T5: Balancing power and performance leads to
energy-efﬁcient cores, regardless of the ISA: A9 and Atom pro-
cessor energy requirements are within 24% of each other and
use up to 50% less energy than other cores.
11 Seven SPEC, all mobile, and the non-database server benchmarks.

0123456Performance (BIPS)05101520253035Power (W)A8A9Atomi7i7 - low perf0123456Performance (BIPS)051015202530Energy (J)Synthetic Points Are HollowA9 2 GhzA15 2 GhzAtom 1 Ghzi7 2GhzA8A9Atomi7A version appears in the 19th IEEE Intl. Symposium on High Performance Computer Architecture (HPCA 2013)
Table 9. Summary of Challenges.
Description

Challenge

12

Board Cooling (ARM)

Networking (ARM)

Networking (Atom)

Perf Counters (ARM)

Compilation (ARM)

Tracing (ARM)

No active cooling leading to failures
Fix: use a fan-based laptop cooling pad
ssh connection used up to 20% of CPU
Fix: use a serial terminal
USB networking not supported
Fix: use as standalone terminal
PMU poorly supported on selected boards
Fix: backport over 150 TI patches
Failures due to dependences on > 100 packages
Fix 1: pick portable equivalent (lighttpd)
Fix 2: work through errors (CLucene & WebKit)
No dynamic binary emulation
Fix: Use gem5 to generate instruction traces

# Finding

Table 10. Summary of Findings.
Support

e
c
n
a
m
r
o
f
r
e
P

Fig-2
Fig-3

1 Large performance gaps exist
2 Cycle-count gaps are less than 2.5×
(A8 to Atom, A9 to i7)
3 x86 CPI < ARM CPI:
Fig-3 & 4
x86 ISA overheads hidden by µ arch
4 ISA performance effects indistinguishable Table-6
Fig-5 & 6
between x86 and ARM
5 µ architecture, not the ISA, responsible
Table-8
for performance differences
6 Beyond micro-op translation, x86 ISA
introduces no overheads over ARM ISA
1 x86 implementations draw more power
than ARM implementations
2 Choice of power or perf. optimization
impacts power use more than ISA
3 Energy use primarily a design choice;
ISA’s impact insigniﬁcant
s 1 High-perf processors require more power Fig-14
than lower-performance processors
f
f
o
-
e
2 It is the µ -architecture and design
d
a
r
methodology that really matters
T

Table-7

Fig-15

Fig-11

Fig-12

Fig-13

r
e
w
o
P

Representative
Data: A8/Atom
2× to 997×
≤ 2.5×
A8: 3.4
Atom: 2.2
inst. mix same
short x86 insts
324× Br MPKI
4× L2-misses

Atom/A8 raw
power: 3×
Atom/A8 power
@1 GHz: 0.6×
Atom/A8 raw
energy: 0.8×
A8/A9: 1.8×
i7/Atom: 10.9×
ED: i7@2GHz<A9
A15 best for ED
i7 best for E D1.4

7. Conclusions
In this work, we revisit the RISC vs. CISC debate consid-
ering contemporary ARM and x86 processors running modern
workloads to understand the role of ISA on performance, power,
and energy.Our study suggests that whether the ISA is RISC or
CISC is irrelevant, as summarized in Table 10, which includes
a key representative quantitative measure for each analysis step.
We reﬂect on whether there are certain metrics for which RISC
or CISC matters, and place our ﬁndings in the context of past
ISA evolution and future ISA and microarchitecture evolution.
Considering area normalized to the 45nm technology node,
we observe that A8’s area is 4.3mm2 , AMD’s Bobcat’s area
is 5.8mm2 , A9’s area is 8.5 mm2 , and Intel’s Atom is 9.7
mm2 [4, 25, 27]. The smallest, the A8, is smaller than Bob-
cat by 25%. We feel much of this is explained by simpler core
design (in-order vs OOO), and smaller caches, predictors, and
TLBs. We also observe that the A9’s area is in-between Bobcat

Finding T6: DVFS and microarchitectural techniques can
provide high energy-efﬁciency to performance-optimized cores,
regardless of the ISA: i7 at 2 GHz provides 6× performance at
the same energy level as an A9.
Finding T7: We consider the energy-delay metric (ED) to
capture both performance and power. Cores designed balancing
power and performance constraints show the best energy-delay
product: A15 is 46% lower than any other design we considered.
Finding T8: When weighting the importance of performance
only slightly more than power, high-performance cores seem
best suited. Considering E D1.4 , i7–a performance optimized
core–is best (lowest product, and 6× higher performance). Con-
sidering E D2 , i7 is more than 2× better than the next best design.
See Appendix IV for more details.
Key Finding 11: It is the microarchitecture and design method-
ologies that really matter.

6. Challenges
During this study, we encountered infrastructure and system
challenges, missteps, and software/hardware bugs. Table 9 out-
lines these issues as a potentially useful guide for similar studies,
and we describe them in more detail below.
Board cooling: The A8 and A9 boards lack active cooling,
and repeatedly rebooted due to over-heating while under test. A
fan-based laptop cooling pad ﬁxed the problem.
Network over USB: The ssh connection to the A8 and A9
boards used up to 20% of the CPU, which was unsuitable for
performance benchmarking. We instead used a serial terminal
to access these boards. The Atom board does not support USB
networking; we used the Atom as a stand-alone terminal.
Microprocessor PMU infrastructure: The performance
counters on the ARM processor are poorly supported on
community-supported boards. We backported over 150 TI
patches to the Linux kernel to support performance counters and
PMU interrupts.
Compilation: gcc works remarkably well as a cross-platform
compiler for simple benchmarks like SPEC which relies on
libc. However, for the ARM environment, the compiler often
fails when compiling complex code bases that have not been rig-
orously tested on Linux, due to dependences on over 100 pack-
ages. Overcoming these linking errors is a tremendously tedious
process. We either carefully choose equivalent highly portable
workloads (e.g., lighttpd) or worked through the errors (e.g.,
CLucene and WebKit).
Tracing and debugging: ARM open-source tracing infras-
tructure is limited, and lacks dynamic binary translation tools
like Pin or DynamoRIO. ptrace based approaches were too
slow; QEMU correctly emulated, but its JIT obfuscated the in-
struction stream. We used gem5 for ARM traces; gem5 does not
support all benchmarks (e.g., lighttpd).

A version appears in the 19th IEEE Intl. Symposium on High Performance Computer Architecture (HPCA 2013)

13

and Atom and is close to Atom’s. Further detailed analysis is
required to determine how much the ISA and the microarchitec-
ture structures for performance contribute to these differences.
A related issue is the performance level for which our re-
sults hold. Considering very low performance processors, like
the RISC ATmega324PA microcontroller with operating fre-
quencies from 1 to 20 MHz and power consumption between
2 and 50mW [3], the overheads of a CISC ISA (speciﬁcally the
complete x86 ISA) are clearly untenable. In similar domains,
even ARM’s full ISA is too rich; the Cortex-M0, meant for low
power embedded markets, includes only a 56 instruction subset
of Thumb-2. Our study suggests that at performance levels in
the range of A8 and higher, RISC/CISC is irrelevant for perfor-
mance, power, and energy. Determining the lowest performance
level at which the RISC/CISC ISA effects are irrelevant for all
metrics is interesting future work.
While our study shows that RISC and CISC ISA traits are
irrelevant to power and performance characteristics of mod-
ern cores, ISAs continue to evolve to better support exposing
workload-speciﬁc semantic information to the execution sub-
strate. On x86, such changes include the transition to Intel64
(larger word sizes, optimized calling conventions and shared
code support), wider vector extensions like AVX, integer crypto
and security extensions (NX), hardware virtualization exten-
sions and, more recently, architectural support for transactions
in the form of HLE. Similarly, the ARM ISA has introduced
shorter ﬁxed length instructions for low power targets (Thumb),
vector extensions (NEON), DSP and bytecode execution exten-
sions (Jazelle DBX), Trustzone security, and hardware virtual-
ization support. Thus, while ISA evolution has been continuous,
it has focused on enabling specialization and has been largely
agnostic of RISC or CISC. Other examples from recent research
include extensions to allow the hardware to balance accuracy
and reliability with energy efﬁciency [15, 13] and extensions to
use specialized hardware for energy efﬁciency [18].
It appears decades of hardware and compiler research has
enabled efﬁcient handling of both RISC and CISC ISAs and
both are equally positioned for the coming years of energy-
constrained innovation.
Acknowledgments
We thank the anonymous reviewers, the Vertical group, and
the PARSA group for comments. Thanks to Doug Burger, Mark
Hill, Guri Sohi, David Wood, Mike Swift, Greg Wright, Jichuan
Chang, and Brad Beckmann for comments on the paper and
thought-provoking discussions on ISA impact. Thanks for vari-
ous comments on the paper and valuable input on ISA evolution
and area/cost overheads of implementing CISC ISAs provided
by David Patterson. Support for this research was provided by
NSF grants CCF-0845751, CCF-0917238, and CNS-0917213,
and the Cisco Systems Distinguished Graduate Fellowship.
References

[1] ARM on Ubuntu 12.04 LTS battling Intel x86?

http:

//www.phoronix.com/scan.php?page=article&item=
ubuntu_1204_armfeb&num=1.
[2] The ARM vs x86 wars have begun:
In-depth power analysis
of Atom, Krait & Cortex A15 http://www.anandtech.com/
show/6536/arm- vs- x86- the- real- showdown/.
[3] Atmel
Datasheet,
http://www.atmel.com/Images/
doc2503.pdf.
[4] chip-architect,
http://www.chip- architect.com/news/
AMD_Ontario_Bobcat_vs_Intel_Pineview_Atom.jpg.
[5] M. Baron. The single-chip cloud computer. Microprocessor Re-
port, April 2010.
[6] D. Bhandarkar. RISC versus CISC: a tale of two chips. SIGARCH
Comp. Arch. News, 25(1):1–12, Mar. 1997.
[7] D. Bhandarkar and D. W. Clark. Performance from architecture:
comparing a RISC and a CISC with similar hardware organiza-
tion. In ASPLOS ’91.
[8] N. Binkert, B. Beckmann, G. Black, S. Reinhardt, A. Saidi,
A. Basu, J. Hestness, D. Hower, T. Krishna, S. Sardashti, R. Sen,
K. Sewell, M. Shoaib, N. Vaish, M. Hill, and D. Wood. The gem5
simulator. SIGARCH Comp. Arch. News, 39(2), Aug. 2011.
[9] W. L. Bircher and L. K. John. Analysis of dynamic power man-
agement on multi-core processors. In ICS ’08.
[10] E. Blem, J. Menon, and K. Sankaralingam. Data to accompany
a detailed analysis of contemporary arm and x86 architectures,
www.cs.wisc.edu/vertical/isa- power- struggles, 2013.
[11] D. Bruening, T. Garnett, and S. Amarasinghe. An infrastructure
for adaptive dynamic optimization. In CGO ’03.
[12] R. Colwell, C. Y. Hitchcock, III, E. Jensen, H. Brinkley Sprunt,
and C. Kollar. Instruction sets and beyond: Computers, complex-
ity, and controversy. Computer, 18(9):8–19, Sept. 1985.
[13] M. de Kruijf, S. Nomura, and K. Sankaralingam. Relax: An ar-
chitectural framework for software recovery of hardware faults.
In ISCA ’10.
[14] H. Esmaeilzadeh, T. Cao, Y. Xi, S. Blackburn, and K. McKinley.
Looking back on the language and hardware revolutions: mea-
sured power, performance, and scaling. In ASPLOS ’11.
[15] H. Esmaeilzadeh, A. Sampson, L. Ceze, and D. Burger. Archi-
tecture support for disciplined approximate programming. In AS-
PLOS ’12.
[16] M. Ferdman, A. Adileh, O. Kocberber, S. Volos, M. Alisafaee,
D. Jevdjic, C. Kaynak, A. D. Popescu, A. Ailamaki, and B. Fal-
saﬁ. Clearing the clouds: a study of emerging scale-out work-
loads on modern hardware. In ASPLOS ’12.
[17] M. J. Flynn, C. L. Mitchell, and J. M. Mulder. And now a case
for more complex instruction sets. Computer, 20(9), 1987.
[18] V. Govindaraju, C.-H. Ho, and K. Sankaralingam. Dynamically
specialized datapaths for energy efﬁcient computing. In HPCA
’11.
[19] A. Gutierrez, R. G. Dreslinski, T. F. Wenisch, T. Mudge, A. Saidi,
C. Emmons, and N. Paver. Full-system analysis and characteri-
zation of interactive smartphone applications. In IISWC ’11.
[20] K. Hoste and L. Eeckhout. Microarchitecture-independent work-
load characterization. Micro, IEEE, 27(3):63 –72, 2007.
[21] C. Isci and M. Martonosi. Runtime power monitoring in high-end
processors: Methodology and empirical data. In MICRO ’03.
[22] C. Isen, L. John, and E. John. A tale of two processors: Revisiting
the RISC-CISC debate. In 2009 SPEC Benchmark Workshop.
[23] C. Kim, T. Kaldewey, V. W. Lee, E. Sedlar, A. D. Nguyen,
N. Satish, J. Chhugani, A. Di Blas, and P. Dubey. Sort vs. Hash
revisited: fast join implementation on modern multi-core CPUs.
VLDB ’09.
[24] D. A. Patterson and D. R. Ditzel. The case for the reduced in-
struction set computer. SIGARCH Comp. Arch. News, 8(6), 1980.
[25] G. Quirk.
Improved ARM core, other changes in TI mobile
app processor, http://www.cs.virginia.edu/~skadron/
cs8535_s11/arm_cortex.pdf.
[26] J. Rao and K. A. Ross. Making B+- trees cache conscious in main
memory. In SIGMOD ’00.
[27] W. Wang and T. Dey.
http://www.cs.virginia.edu/
~skadron/cs8535_s11/ARM_Cortex.pdf.

A version appears in the 19th IEEE Intl. Symposium on High Performance Computer Architecture (HPCA 2013)
Appendices
All raw data mentioned below can be found in the
Appendix I: Detailed Counts

attached to this document (right click on link to save).

14

Data: Figure 16 shows execution time normalized to i7. The raw data for this ﬁgure is in the Time worksheet of our publicly
released spreadsheet [10].

(a) In-Order

Figure 16. Execution Time Normalized to i7.

(b) Out-of-Order

Data: Figure 17a shows dynamic instruction (macro) counts on A8 and Atom normalized to Atom x86 macro-instructions. The
raw data for this ﬁgure is in the Macro-ops worksheet of our publicly released spreadsheet [10].
Data: Figure 17b shows dynamic micro-op counts for Atom and i7 normalized to Atom macro-instructions12 . The raw data for
this ﬁgure is in the Micro-ops worksheet of our publicly released spreadsheet [10]. Note: for icc results, preprocessor directives
changed so that gcc and icc use the same alignment and compiler hints.

(a) In-Order

(b) Out-of-Order
Figure 17. Instruction Counts Normalized to i7 macro-ops.

Data: Table 11 shows CPIs. This data is also in the CPI worksheet of our publicly released spreadsheet [10].
Table 11. Cycles per Instruction (CPI) Per Benchmark.
SPEC INT
SPEC FP

Mobile
t
u
o
y
a
l

f
r
e
p

k
r
a
m
e
r
o
c

Server

l
e
n
r
e
k

b
d

d
p
t
t
h
g
i
l

e
n
e
c
u
l

0.4 0.47 0.1
1.0 0.6 0.2

m
u
t
n
a
u
q
b
i
l

r
e
m
m
h

k
m
b
o
g

4
6
2
h

2
p
i
z
b

g
n
e
j
s

c
c
g

h
c
n
e
b
l
r
e
p

p
p
t
e
n
m
o

D
T
D
F
s
m
e
G

x
e
l
p
o
s

x
i
l
u
c
l
a
c

y
a
r
v
o
p

o
t
n
o
t

d
m
a
n

D
3
e
i
l
s
e
l

c
l
i
m

M
D
A
s
u
t
c
a
c

s
e
v
a
w
b

-

-

k
w

k
w

r
a
t
s
a

ARM 0.02 1.3 1.3
x86 0.02 1.4 1.4

0.6 0.6 0.9 1.7 2.1 0.5 0.6 3.9 1.9 2.0
0.7 0.7 0.9 1.5 2.1 0.7 0.7 4.1 1.7 1.7

1.4 1.3 2.7 2.0 4.8 0.9 0.9 0.7 1.7 0.8
1.5 1.3 2.6 1.8 5.2 0.9 0.9 0.7 1.5 0.8

12 For i7, we use issued micro-ops instead of retired micro-ops; we found that on average, this does not impact the micro-op/macro-op ratio.

coremarkwk_layoutwk_perfmeanastarlibquantumhmmerh264gobmkbzip2sjenggccperlbenchomnetppmeansoplexGemsFDTDcalculixpovraytontonamdleslie3DmilccactusADMbwavesmeanlucenedb_kernelslighttpdmean05101520253035Time405454758614221372363130997344A8Atomwk_perfwk_layoutcoremarkmeanastarhmmerlibquantumgobmksjenggccperlbenchh264bzip2omnetppmeansoplexGemsFDTDcalculixnamdpovraytontocactusADMmilcleslie3Dbwavesmeanlucenelighttpddb_kernelsmean05101520253035Time102A9i7wk_perfwk_layoutcoremarkmeanastarhmmerlibquantumgobmksjenggccperlbenchh264bzip2omnetppmeansoplexGemsFDTDcalculixnamdpovraytontocactusADMmilcleslie3Dbwavesmeanlucenelighttpddb_kernelsmean0.00.51.01.52.02.53.03.5Macro-Ops4.717ARMx86wk_perfwk_layoutcoremarkmeanastarhmmerlibquantumgobmksjenggccperlbenchh264bzip2omnetppmeansoplexGemsFDTDcalculixnamdpovraytontocactusADMmilcleslie3Dbwavesmeanlucenelighttpddb_kernelsmean0.00.51.01.52.02.53.03.5Micro-OpsAtomi7A version appears in the 19th IEEE Intl. Symposium on High Performance Computer Architecture (HPCA 2013)

15

Data: Table 12a shows average ARM and x86 static binary sizes, measuring only the binary’s instruction segment. Note that
bwaves fails to run due incorrect code produced by ifort. This data is also in the Code Size worksheet of our publicly released
spreadsheet [10].
Data: Table 12b shows average dynamic ARM and x86 instruction lengths. This data is also in the Inst Length worksheet of our
publicly released spreadsheet [10].
Table 12. Instruction Size Details: (a) Static Binary (MB), (b) Average Dynamic Instruction (B).
Mobile
SPEC INT
SPEC FP
t
u
o
y
a
l

Server

k
r
a
m
e
r
o
c

f
r
e
p
-

k
w

-

k
w

m
u
t
n
a
u
q
b
i
l

r
a
t
s
a

r
e
m
m
h

k
m
b
o
g

4
6
2
h

2
p
i
z
b

g
n
e
j
s

c
c
g

h
c
n
e
b
l
r
e
p

p
p
t
e
n
m
o

D
T
D
F
s
m
e
G

x
e
l
p
o
s

x
i
l
u
c
l
a
c

y
a
r
v
o
p

o
t
n
o
t

d
m
a
n

D
3
e
i
l
s
e
l

c
l
i
m

M
D
A
s
u
t
c
a
c

s
e
v
a
w
b

l
e
n
r
e
k

b
d

d
p
t
t
h
g
i
l

e
n
e
c
u
l

n ARM 0.02 1.3 1.3
i
x86 - gcc 0.02 1.4 1.4
B
x86 - icc 0.6 1.5 1.5
t ARM 4.0 4.0 4.0
s
x86 - gcc 2.4 3.7 3.7
n
I
x86 - icc 2.5 3.2 3.2

0.6 0.6 0.9 1.7 2.1 0.5 0.6 3.9 1.9 2.0
0.7 0.7 0.9 1.5 2.1 0.7 0.7 4.1 1.7 1.7
0.7 0.7 1.0 1.3 2.2 0.7 0.8 4.3 1.9 2.2
4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0
2.9 3.0 3.0 3.5 3.1 3.6 3.5 2.8 2.9 2.7
3.2 2.9 3.6 3.3 3.3 3.4 3.6 2.9 3.2 2.8

1.4 1.3 2.7 2.0 4.8 0.9 0.9 0.7 1.7 0.8
1.5 1.3 2.6 1.8 5.2 0.9 0.9 0.7 1.5 0.8
1.5 1.7 3.1 2.2 6.8 1.0 1.4 0.8 2.0 —
4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0
2.7 3.4 2.9 2.6 3.4 3.3 4.1 2.6 6.4 3.0
3.1 3.6 3.3 3.5 4.2 4.9 5.0 4.1 6.1 —

0.4 0.47 0.1
1.0 0.6 0.2
1.4 1.8 0.2
4.0 4.0 4.0
3.7 2.6 3.7
2.7 2.7 2.8

Data: Figure 18 shows average power normalized to the A8. The raw data for this ﬁgure is in the Avg Power worksheet of our
publicly released spreadsheet [10].

(a) In-Order

(b) Our-of-Order
Figure 18. Average Power Normalized to A8.

Data: Figure 19 shows technology-independent average power–cores are scaled to 1 GHz at 45 nm (normalized to A8). The raw
data for this ﬁgure is in the Avg TI Power worksheet of our publicly released spreadsheet [10].

(a) In-Order
(b) Out-of-Order
Figure 19. Technology Independent Average Power Normalized to A8.

coremarkwk_layoutwk_perfmeanastarlibquantumhmmerh264gobmkbzip2sjenggccperlbenchomnetppmeansoplexGemsFDTDcalculixpovraytontonamdleslie3DmilccactusADMbwavesmeanlucenedb_kernelslighttpdmean0510152025303540Average PowerA8Atomwk_perfwk_layoutcoremarkmeanastarhmmerlibquantumgobmksjenggccperlbenchh264bzip2omnetppmeansoplexGemsFDTDcalculixnamdpovraytontocactusADMmilcleslie3Dbwavesmeanlucenelighttpddb_kernelsmean0510152025303540Average PowerA9i7coremarkwk_layoutwk_perfmeanastarlibquantumhmmerh264gobmkbzip2sjenggccperlbenchomnetppmeansoplexGemsFDTDcalculixpovraytontonamdleslie3DmilccactusADMbwavesmeanlucenedb_kernelslighttpdmean0246810Average PowerA8Atomwk_perfwk_layoutcoremarkmeanastarhmmerlibquantumgobmksjenggccperlbenchh264bzip2omnetppmeansoplexGemsFDTDcalculixnamdpovraytontocactusADMmilcleslie3Dbwavesmeanlucenelighttpddb_kernelsmean0246810Average PowerA9i7A version appears in the 19th IEEE Intl. Symposium on High Performance Computer Architecture (HPCA 2013)

16

Data: Figure 20 shows raw energy (power and time). The raw data for this ﬁgure is in the Energy worksheet of our publicly
released spreadsheet [10].

(a) In-Order

(b) Out-of-Order

Figure 20. Energy Normalized to A8.
Appendix II: Detailed bwaves Analysis
As noted in Section 5, the bwaves benchmark performed signiﬁcantly worse (up to 30× more cycles) on ARM cores than on x86
cores. Contributing to this gap, we found that ARM cores executed 17.5× more instructions than x86 cores. We believe that most
of the ARM to x86 gap for bwaves can be explained by this large differences in the number of instructions required to complete the
same amount of work.
We performed detailed analysis to ﬁnd the source of the instruction count discrepancies. To begin, we found from the execution
proﬁle that complex double ﬂoating point operations which the compiler translates to longer instruction sequences for ARM than for
x86 are a signiﬁcant source of additional instructions: 37% of all cycles for ARM cores are spent in
aeabi dadd and 29% of all
cycles are spent in aeabi dmul, while neither of these routines appear in the x86 summary).
We use ﬂags to force gcc to compile ﬂoating point instructions to SSE 2 (x86) and NEON (ARM) instructions. This decision is the
most fair in general, since ARM’s VFP unit is known to be signiﬁcantly slower than the NEON unit for single precision ﬂoating point
operations. However, unlike the VFP unit, the NEON unit is not IEEE754 compliant, and double precision operations are mapped to
library calls. The result is that for ARM architectures, gcc—in the absence of FP relaxation—compiles double-precision ﬂoating point
arithmetic to library calls which add signiﬁcant overhead compared to short instruction sequences on x86. One solution to bwave’s
outlier status would be to use different compiler ﬂags for benchmarks with signiﬁcant amounts of double precision arithmetic.

coremarkwk_layoutwk_perfmeanastarlibquantumhmmerh264gobmkbzip2sjenggccperlbenchomnetppmeansoplexGemsFDTDcalculixpovraytontonamdleslie3DmilccactusADMbwavesmeanlucenedb_kernelslighttpdmean0.00.51.01.52.02.53.03.54.0Energy (J)A8Atomwk_perfwk_layoutcoremarkmeanastarhmmerlibquantumgobmksjenggccperlbenchh264bzip2omnetppmeansoplexGemsFDTDcalculixnamdpovraytontocactusADMmilcleslie3Dbwavesmeanlucenelighttpddb_kernelsmean0.00.51.01.52.02.53.03.54.0Energy (J)A9i7A version appears in the 19th IEEE Intl. Symposium on High Performance Computer Architecture (HPCA 2013)
Appendix III: Synthetic Points
A15: Using reported Coremark scores (Coremarks/MHz) and mW/MHz from Microprocessor Reports and ARM documentation, we
assume a 2GHz operating frequency and compute the Coremark score and energy. We then scale A9 BIPS results by the ratio of the
A15 Coremark score to the A9 Coremark score to get an A15 performance projection.
Frequency scaled cores: For frequency scaled cores, we project performance by assuming a linear relationship between performance
and frequency. We scale energy projections as detailed in the Technology scaling and projections paragraph of Section 3.4.
Appendix IV: E Dx Analysis

17

Data: Figure 21 shows the impact of the exponent x on the product E Dx . Note that a lower product is better. When x is greater than
1.4, i7 outperforms all other cores.

Figure 21. Impact of exponent, x, on product E Dx .

2-1202122Exponent (x)100101102103104105106107108109101010111012Product (EDx)A8A9Atomi7A15i7 @ 2GHzCSE 3322 Computer Architecture I 

Research Paper on  
Comparisons of PowerPC and Pentium II  
Memory Hierarchies 

 By: 
 
 
 
November 8, 1999 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
P E R S P E C T I V E S

Kevin Skadron
University of Virginia
Margaret Martonosi
David I. August
Princeton University
Mark D. Hill
University of Wisconsin−Madison
David J. Lilja
University of Minnesota
Vijay S. Pai
Rice University

A report to the US
National Science
Foundation argues 
that simulation and
benchmarking
technology will require 
a leap in capability
within the next few
years to maintain
ongoing innovation 
in computer systems.

Challenges 
in Computer
Architecture
Evaluation
Q uantitative evaluation has become the mainstay of computer
architecture research. However, the tremendous complexity
of computer systems is making them both difﬁcult to reason
about and expensive to develop. Detailed software simula-
tions have therefore become essential for evaluating ideas in
the  computer  architecture  field.  Industry  uses  simulation
extensively during processor and system design because it is the easiest and
least expensive way to explore design options. Simulation is even more
important in research to evaluate radical new ideas and characterize the
nature of the design space.
Table 1 shows a dramatic shift toward simulation-based research reﬂected
in papers presented at the International Symposium on Computer Architecture.
Papers on simulations now constitute 80 to 90 percent of the total at this pre-
mier conference—up from only 28 percent in 1985. By comparison, papers
based on direct measurements of real systems or on mathematical models have
fallen to less than 10 percent from almost 35 percent in 1985.
Unfortunately, it is becoming harder and more time-consuming to construct
accurate simulation models of modern computer systems. Further, the sub-
stantial effort required to develop high-ﬁdelity simulation tools typically yields
few academic rewards. Finally, as the range of important applications becomes
more diverse, creating a suitable set of publicly available benchmarks and met-
rics becomes more challenging. Together, these difﬁculties are likely to encour-
age  researchers  to  focus  on  problems  suited  to  the  current  evaluation
infrastructure—a type of research that only “looks where the light is good.” 
Research on multiprocessor systems, in particular, has already hit a wall in
performance evaluation. As Figure 1 shows, the proportion of multiprocessor
papers submitted to ISCA dropped from a peak of 50 percent in 1985 to less
than 10 percent in 2001. This trend is echoed in other forums and seems 
due in part to the difﬁculty of performing in-depth research evaluations related
to these systems. 
This research downturn comes at a crucial time. Over the next 10 years,
even single-chip computers will likely be multiprocessor systems. Even in
uniprocessor systems, increased on-chip heterogeneity and specialization as
well as new metrics such as power and temperature will make simulation
and modeling difﬁcult, if not impossible, using current tools and method-
ologies.

30

Computer

P u b l i s h e d   b y   t h e   I E E E   C o m p u t e r   S o c i e t y

0018-9162/03/$17.00 © 2003 IEEE

In December 2001, a number of researchers met
under the aegis of the US National Science Foun-
dation’s Computer Systems Architecture program
to discuss the experimental and evaluation prob-
lems that processor architecture research faces. 
The 18 workshop panelists agreed that current
limitations and trends in evaluation techniques are
troublesome and could noticeably slow the rate of
computer systems innovation. They recommended
new research to help make quantitative evaluations
of  computer  systems  manageable  again  (www.
ee.princeton.edu/~mrm/CPUperf.html). 
Speciﬁcally, the panel’s report advocated support
for research in the areas of simulation frameworks,
benchmarking methodologies, analytic methods,
and validation techniques.

SIMULATION FRAMEWORKS
Quantitative evaluation of computer architec-
tures  relies heavily on  simulators and  simulator
infrastructure, yet today’s robust and publicly avail-
able simulation tools are by no means capable of
supporting the full range of studies that the archi-
tecture community must pursue.

s
r
e
p
a
p
 
f
o
 
e
g
a
t
n
e
c
r
e
P

Problems
Current simulation  infrastructures are written
in ways that pose several problems. First, sequen-
tial C or C++ simulator code does not resemble
the systems under study. This discrepancy forces
simulator writers to perform a complex mapping
from the concurrent, structural nature of the sys-
tems to the sequential, procedural nature of the
simulator’s underlying programming  language.
The mapping process is typically ad hoc and error-
prone. As a result, simulator code obscures the
machine actually being modeled, which can com-
promise the validity of conclusions drawn from
the simulation results. 
Further, the mapping schemes used between sim-
ulators—and even within a single simulator—are
typically distinct. These differences limit the inter-
operability of simulator code, which in turn limits
code reuse and the collaboration potential of cur-
rent simulation methodologies.
Given the difﬁculty of reusing the code in current
tools, computer architecture research tends to focus
on questions relating to machines for which simu-
lation models exist. This restricted focus means that
parts of the design space as well as potentially inter-
esting new ideas are inadequately explored because
the available tools do not support them.
The challenge is even greater for multiprocessor
simulators  than  for  uniprocessor  simulators.

Table 1. Performance evaluation methodologies used in a sampling of 
papers from the Proceedings of the International Symposium on Computer
Architecture. 
Total
Mathematical 
papers*  Simulation  Measurement  modeling 
25 
22 
2 
0 
0 
6 
24 
30 
6 
9 
23 
32 
14 
1 
12 
43 
28 
2 
0 
5 

Other 
2  
0  
1  
16  
21 

Year 
2001 
1997 
1993 
1985 
1973 

*Note: Total papers are not necessarily the sum of papers across the columns because some
papers used more than one evaluation technique.

100

Other (including microarchitecture)
Fault tolerance
Data flow
Interconnection networks
Multiprocessors

7
7
9
1

2
8
9
1

7
8
9
1

2
9
9
1

7
9
9
1

80

60

40

20

0

5
7
9
1
 
n
i
 
t
e
e
m
 
t
o
n
 
d
i
d
 
A
C
S
I

Figure 1. Relative percentage of papers by topic category submitted to the 
International Symposium on Computer Architecture from 1973 to 2001.
Multiprocessor papers have declined especially rapidly since 1993. Source: 
M.D. Hill and R. Rajwar, “The Rise and Fall of Multiprocessor Papers in the 
International Symposium on Computer Architecture (ISCA),” 2001; www.cs.
wisc.edu/~markhill/mp2001.html. Reprinted with permission.

Multiple processors and the system structures that
connect them already stress the capabilities of cur-
rent modeling tools. As system complexity contin-
ues to skyrocket, these tool-related problems will
only worsen. 

Simulation recommendations
All workshop participants agreed that simulation
frameworks and simulator construction frameworks
were superior to monolithic simulators or simula-
tor code libraries written in sequential languages. 
Simulation frameworks provide an infrastruc-
ture for pluggable components and hierarchical

August 2003

31

Signiﬁcant research
is needed to develop
reliable abstractions
for multiprocessor
simulators and to
improve their speed,
accuracy, and 
modularity.

abstraction by deﬁning interfaces that resem-
ble concurrent communication in real sys-
tems—not the function-call interface found
in sequential programming languages such
as C. Simulation frameworks encourage shar-
ing  of  component  models  across  research
groups by nullifying conventions speciﬁc to
any  given  monolithic  simulator  or  code
library.
Simulation construction frameworks enable
rapid  exploration  of  design  alternatives  by
automatically weaving architectural compo-
nent models together as implied by a machine
description. Abstracting the machine descrip-
tion from the simulator code gives researchers a
clear  picture  of  the  machine  actually  being 
modeled.
Modularity and portability. Framework modularity
lets researchers consider different evaluation levels.
For  instance,  they  can  use  abstract  evaluations
with analytic models to study ideas in the earliest
stages of development and increase the detail in
evaluations as the work progresses. Frameworks
also encourage code reuse and expose a machine’s
structure through its description, thereby speeding
validation  of  machine  models.  Examples  of
recently developed frameworks include Asim1 and
the Liberty Simulation Environment.2
Support for constructing simulator frameworks
will have important long-term beneﬁts by making
it easier to extend, reuse, and validate simulator
components.  Relatively  unrestricted  machine
description languages encourage consideration of
unconventional machines, and multilevel simula-
tions let researchers substitute analytic or real-time-
logic models for speciﬁc components.
The NSF workshop panelists accordingly rec-
ommended that the computer architecture com-
munity  encourage  research  to develop modular
and portable frameworks. Government and indus-
try sources should fund efforts in two phases: 

• develop new frameworks to foster maximal
innovation, and 
• advance the most successful frameworks for
wide distribution in the R&D community. 

Model accuracy. The community must also arrive
at some consensus on what constitutes appropri-
ate validation for various types of research stud-
ies. Models of even an existing architecture are
unlikely to capture every nuance of actual hard-
ware behavior, and for many studies such detail
simply slows research unnecessarily. 

32

Computer

A key question  is whether absolute or relative
accuracy is required—that is, are exact performance
projections necessary or is it sufﬁcient to produce
only a reliable projection of how different parts of
the design space perform relative to each other? The
answer may dictate different validation strategies.3
Multiprocessor infrastructure is a particular con-
cern. Signiﬁcant research is needed to develop reli-
able abstractions for multiprocessor simulators and
to improve their speed, accuracy, and modularity.
Both shared-memory and message-passing systems
need models.
Other challenges lie in simulating the heterogene-
ity inherent in many upcoming embedded systems
while preserving portability of common components
between  general-purpose  and  embedded-system
processor simulations.
Funding and disseminating research. Funding agen-
cies can play an important role in advancing simu-
lation  technology  by  aggressively  supporting
research programs to develop new frameworks and
components. The academic community can in turn
support this important research area by creating a
forum for disseminating these contributions and
recognizing them in peer-reviewed publications.

BENCHMARKING
While benchmarking  issues are diverse, a  few
problems seem particularly acute for architecture
research.

Problems
First, current benchmarks  focus almost exclu-
sively on high-performance/desktop workloads, rep-
resented  by  the  SPECcpu  benchmarks,  and  on
scientiﬁc workloads, such as the SPLASH bench-
marks. No benchmark suite can be a one-size-ﬁts-
all solution. While the SPEC and SPLASH appli-
cation classes remain important, other classes are
growing too rapidly to ignore. Examples include
embedded  systems  (for  which  the  University  of
Michigan  recently  released  the MiBench  suite4),
mobile  computing  (for  which  the  University  of
California, Los Angeles, has released the Media-
Bench suite5), real-time computing, server work-
loads, and networking workloads. Behavior types
such as dependability, power, and pointer usage also
need benchmarks. 
Second, all benchmark suites, including SPEC
and  SPLASH,  require  better  characterization  to
show what portion of the total “behavior space”
they really represent. Unfortunately, it is unclear
whether the current SPECcpu suite adequately rep-
resents today’s high-performance/desktop applica-

tions, let alone other application classes such as
mobile computing. Similarly, nonscientiﬁc work-
loads have become at least as important as scientiﬁc
workloads in multiprocessor systems, especially in
the  small-scale  systems  used  by  servers. Yet  the
R&D community does not have a system for iden-
tifying important benchmark characteristics and
how benchmark applications embody them. Such
a system would also help in identifying character-
istics  that  a  suite  does  not  represent  well  and
thereby guide the development of new benchmarks. 
Third, full-size (“macro”) benchmarks can com-
bine many behaviors in ways that can be hard to
conceptualize. Microbenchmarks offer a way to
isolate individual program behaviors or individual
aspects of a processor’s performance. In addition,
researchers can combine them to construct more
thorough  application  and  system  performance
models and to determine the importance of vari-
ous system behaviors.6,7 Yet microbenchmarks are
almost completely lacking at this time. 
Fourth, the batch-processing mode of running
benchmarks—as if each one had exclusive access to
the CPU for the duration—is unrealistic. Many sys-
tems run multiple processes simultaneously, often
sharing the hardware at a ﬁne granularity. The com-
munity needs a sound methodology for modeling
workloads in addition to individual benchmarks.
The methodology must include operating system
effects, especially in multiprocessor applications.
Moreover, we need more work to understand com-
plex workloads, such as database management sys-
tems, whose exact execution path is a function of
many system parameters.8

Benchmarking recommendations
The  NSF  workshop  panelists  recommended
aggressive support for programs to develop and
characterize robust, portable benchmarks for appli-
cation domains outside the traditional high-per-
formance/desktop domain. In addition, research is
needed to develop techniques for characterizing
and  abstracting  benchmarks  and  for  providing
parameterized, synthetic workloads.
Reward structure. Developing benchmarks, bench-
marking methodologies, and tools is hard work.
While developers of widely accepted benchmark
suites and methodologies see signiﬁcant payoff for
their work in citations and other recognition, the
community’s tendency to support only a small set
of benchmark suites leads to a winner-takes-all sit-
uation. Consequently, the current reward structure
makes research in this area too risky for most aca-
demics. 

Nonscientiﬁc 
workloads have
become at least 
as important as 
scientiﬁc workloads
in multiprocessor
systems.

The  academic  architecture  community
must provide a forum for evaluating and dis-
seminating benchmarks. Such a forum, per-
haps  organized  as  a  standing  committee,
could  encourage  benchmark  creation  in
areas of increasing importance as well as vet
benchmark  quality  and  characterization.
Hopefully,  both  committee  service  and
benchmark submission will, in time, be seen
as prestigious. The committee might take on
some characteristics of a conference program
committee,  and  a  benchmark’s  approval
might include an accompanying publication that
will have the stature of a more traditional peer-
reviewed paper.
Open source. The panelists envision a benchmark
suite for research purposes and so recommend that
all benchmarks be publicly available with source
code. Users should be permitted to improve the
algorithms in benchmarks with the proviso that
they make such changes publicly available as well.
Some means for incorporating improvements back
into the standardized form of the benchmarks is
also important.
Synthetic benchmarks. Finally, the panelists recom-
mend synthetic benchmarks that are coded to run
on a real computer but parameterized to provide a
range of different behaviors. Synthetic benchmarks
would let researchers explore a wider range of the
application behavior space, even when no publicly
available  benchmark  exists.  Parameterization
would allow a single benchmark to produce a vari-
ety of behaviors, covering a larger portion of the
behavior space. With a suitable choice of parame-
ters, the synthetic benchmarks should be able to
demonstrate  behaviors  similar  to  those  demon-
strated by “real” benchmarks. This concept could
be  extended  to  create  parameterized  workloads
with a variable mix of program behaviors and rates.

ABSTRACTIONS AND METHODOLOGY
Computer architecture’s heavy emphasis on sim-
ulation effectively discourages the research com-
munity from exploring other useful and possibly
more informative modeling techniques. The few
published  papers  using  and  proposing  analytic
models have not stimulated signiﬁcant follow-up
efforts. Stories abound of such papers receiving
knee-jerk negative rejections from program com-
mittees  and  other  researchers.  Among  the  NSF
workshop  participants,  however,  even  skeptics
admit that analytic models have a place and that
some aspects of the research community’s hostility
are cause for concern. 

August 2003

33

Researchers cannot
pursue futuristic
investigations when
they are limited to
systems for which no
benchmark programs
are available.

Analytic  models  and  simulation  are  not
mutually exclusive. Analytic models can help
to understand a system in ways that simulation
does not. They can also be used to validate 
a  simulation-based  model.  Some  panelists
described experiences in which such validation
exercises proved extremely valuable.

Problems
The research community’s preference for
papers that emphasize measurements with
detailed simulation has generated valuable
results, but it has also undermined work on more
far-reaching approaches that cannot yet be ade-
quately simulated. In general, we should be care-
ful 
to  value  quantitative  results 
for 
the
understanding  they  provide,  applying  Richard
Hamming’s dictate to computer systems evaluation:
“The purpose of computing is insight, not num-
bers.”
Further, some aspects of very detailed evaluation
can be unrealistic and wasteful when exploring a
technology that is sufﬁciently speculative to lack
detailed behavior and timing data. More impor-
tantly, researchers cannot pursue futuristic investi-
gations when they are limited only to systems that
can already be simulated and for which no bench-
mark programs are available. For instance, analyt-
ical performance-evaluation tools can model the
expected behavior of future systems; they can also
model the expected impact of future compiler and
hardware modiﬁcations, thereby avoiding unnec-
essary costs associated with more detailed simula-
tion models or actual implementations.
In domains currently without good benchmarks,
new abstract workloads could help if we can justify
the abstractions and characterize their representa-
tiveness. Further, as  system and workload  com-
plexity increases, detailed simulation studies will
take far too much time. This constraint already lim-
its multiprocessor simulations. Increasing hetero-
geneity in uniprocessor systems-on-a-chip foretells
a similar problem. We need scientiﬁc methods for
abstracting  evaluations  to  explore  the  desired
design space accurately but efﬁciently. 

Recommendations for analytic models
Workshop panelists agreed that developing sci-
entific  methods  for  abstracting  evaluations  to
explore large design spaces is imperative. Analytic
models, both as modules within an overall evalua-
tion framework and as a way of validating simula-
tor behavior, are important tools that the research
community is mostly lacking today.

34

Computer

Statistical or other techniques that clearly demon-
strate  how  analytic  models  capture  important
behaviors would  substantially  improve  the way
most practitioners view such models, especially if
the models are easy to use and understand. Some
techniques,  such  as  performance  counters  and
related tools, are driven by direct measurements yet
offer many of the same beneﬁts as analytic models.

METRICS, EVALUATION ACCURACY, 
AND VALIDATION
Quantitative  evaluations  must  give  accurate
insights about trends and behavior. Model inac-
curacies  can  lead  to  incorrect  predictions  and 
even spurious research threads that take years to
resolve. 
Yet many studies need not predict exact values
for performance, power, and other metrics. Rather,
they need only provide a reliable projection of how
different parts of the design space perform relative
to one another. Such results are especially impor-
tant for exploring hypothetical architectures and
targeted future technologies in which the lack of
detailed design information makes absolute accu-
racy impossible.

Problems
Some  modeling  assumptions  are  essential  for
achieving relative accuracy, while others add need-
less complexity. The current understanding of cor-
rect abstraction levels and other important aspects
of accurate models is poor. This leads to wasted
effort  on  models  and  simulations  that  contain
unnecessary detail while simultaneously lacking
certain essential information. For hypothetical sys-
tems, high precision—no matter how detailed the
model—can  be  wasted  if  the  assumptions  that
underlie the detail are inappropriate or change over
time. Early-stage studies should focus on charac-
terizations of broad parameter spaces.
Another  problem  with  simulation  evaluation
accuracy and validation is that most architecture
research uses the same basic, aggregated statistics:
average instructions per second (IPC), cache miss
rate,  branch  misprediction  rate,  and  so  on.
However, average values conceal bursty behavior
and  can  therefore  be misleading  by  aggregating
underestimates  and  overestimates  over  time.
Unfortunately, simple standard deviations are not
helpful because the events being measured seldom
ﬁt a Gaussian distribution. We need a wider range
of methods and metrics for analyzing processor per-
formance as well as a better understanding of how
to use them appropriately. 

In fact, the computer architecture community
continually searches for new metrics to provide bet-
ter insight and simplify evaluation. Thermal pack-
aging is a recent case in point: How do we abstract
away speciﬁc thermal packaging details to obtain
a generalized metric for heat regulation, similar to
the  way  IPC  abstracts  away  circuit-design  and
clock-cycle-time details? The difﬁculty of evaluat-
ing  and  generalizing  new  ideas  in  temperature-
aware computing is a serious obstacle to effective
research in this area, and similar difﬁculties appear
in many areas of architecture research. 
More generally, how do we effectively capture
tradeoffs between the continuing need for perfor-
mance and other design needs? How do we reﬂect
real-time and reliability requirements? How do we
reﬂect soft real-time application requirements, with
a range of acceptable quality-of-service levels, so
that architects can understand how to trade qual-
ity of service against other design goals?

Recommendations for validation techniques
The community must improve its understanding
of an accurate model’s essential components. This
understanding underlies the development of tech-
niques for deﬁning less-detailed simulations that
still provide relative accuracy. It also supports the
development of methods to verify that accuracy. 
Computer architects need better metrics as well
as statistical techniques and tools that are accessi-
ble and easy to use.9 They also need metrics for new
areas, including power, temperature, reliability, and
quality of service. Even existing metrics, such as the
energy-delay product now widely used for power-
aware computing, need expansion to encompass
real-time computing and other design goals.
The ﬁeld is rife with different simulation techniques.
There is little agreement on when to use certain bench-
marks or inputs or, despite recent work,10-12 on what
conﬁgurations to model for various types of experi-
ments and what areas require the greatest investment
in modeling detail. Sound and veriﬁable modeling
methodologies require further research.
Unlike most other scientiﬁc research disciplines,
published computer architecture results are rarely
independently verified. This lack of independent
experiment replication appears to result primarily
from the lack of an appropriate reward structure.
Workshops  such  as  the  successful  2002  and 
2003 Workshops on Duplicating, Deconstructing,
and Debunking (www.ece.wisc.edu/~wddd/) are a
step in the right direction, but a publication with
the imprimatur of a refereed journal is needed to
completely legitimize the replication and veriﬁca-

tion of results. Existing journals might offer a dis-
tinct track for publishing results of this nature.
W ithout major advances in the areas dis-
cussed  here,  limitations  in  the  current
evaluation  infrastructure  will  likely
restrict computer architecture research to a narrow,
incremental, and ultimately irrelevant enterprise.
The  research  community  must  play  its  part  by
embracing  high-quality  work  in  these  areas.
However, because the work involves a large invest-
ment of time and effort, the NSF workshop panel
argued that a substantial investment of government
and industry research funds is required to jump-
start it. 
New venues for publishing and disseminating
work  are  also  necessary.  Without  funding  and
promising  prospects  for  academic  recognition,
research and development in these areas is likely to
languish at its current slow pace. The current short-
comings in computer systems evaluation could ulti-
mately even obstruct the innovation that is driving
the information-technology revolution. I

References
1. J. Emer et al., “Asim: A Performance Model Frame-
work,” Computer, Feb. 2002, pp. 68-76.
2. M. Vachharajani et al., “Microarchitectural Explo-
ration with Liberty,” Proc. 35th Ann. IEEE/ACM
Int’l Symp. Microarchitecture, IEEE CS Press, 2002,
pp. 271-282.
3. R. Desikan, D.C. Burger, and S.W. Keckler, “Mea-
suring Experimental Error in Microprocessor Simu-
lation,”  Proc.  28th  Ann.  Int’l  Symp.  Computer
Architecture, IEEE CS Press, 2000, pp. 266-277.
4. M.R. Guthaus et al., “MiBench: A Free, Commer-
cially Representative Embedded Benchmark Suite,”
Proc.  4th  Ann.  IEEE  Int’l  Workshop  Workload
Characterization, IEEE CS Press, 2001, pp. 3-14.
5. C. Lee et al., “MediaBench: A Tool for Evaluating
and Synthesizing Multimedia and Communications
Systems,” Proc. 30th Ann. Int’l Symp. Microarchi-
tecture, IEEE CS Press, 1997, pp. 330-335.
6. R.H. Saavedra and A.J. Smith, “Analysis of Bench-
mark Characteristics and Benchmark Performance
Prediction,” Trans. Computer Systems, Nov. 1996,
pp. 344-384.
7. M. Seltzer et al., “The Case for Application-Speciﬁc
Benchmarking,” Proc. 7th Workshop Hot Topics in
Operating Systems, IEEE CS Press, 1999, pp. 102-
107.

August 2003

35

8. A. Alameldeen et al., “Simulating a $2M Commercial
Server on a $2K PC,” Computer, Feb. 2003, pp. 50-
57.
9. J.J. Yi, D.J. Lilja, and D.M. Hawkins, “A Statistically
Rigorous  Approach  for  Improving  Simulation
Methodology,” Proc. 9th Int’l Symp. High-Perfor-
mance Computer Architecture, IEEE CS Press, 2003,
pp. 281-291.
10. T. Sherwood et al., “Automatically Characterizing
Large  Scale  Program  Behavior,”  Proc.  10th  Int’l
Conf. Architectural Support for Programming Lan-
guages and Operating Systems, ACM Press, 2002,
pp. 45-57.
11. J.W. Haskins Jr. and K. Skadron, “Memory Refer-
ence Reuse Latency: Accelerating Sampled Microar-
chitecture Simulation,” Proc. 2003 IEEE Int’l Symp.
Performance Analysis of Software and Systems, IEEE
Press, 2003, pp.  195-203.
12. T. Conte, M.A. Hirsch, and K.N. Menezes, “Reduc-
ing State Loss for Effective Trace Sampling of Super-
scalar Processors,” Proc. 1996 Int’l Conf. Computer
Design, IEEE CS Press, 1996, pp. 468-477.

Kevin  Skadron  is  an  assistant  professor  in  the
Department of Computer Science at the University
of Virginia. His research interests are in computer
architecture and performance analysis, especially
temperature- and power-aware computing, appli-
cations of feedback control, and branch prediction.
Skadron received a PhD in computer science from
Princeton University. He is a member of the IEEE
and  the  ACM.  Contact  him  at  skadron@cs.
virginia.edu.
Margaret Martonosi is an associate professor in
the Department of Electrical Engineering at Prince-
Wants YouComputer 
Computer 
Wants You
Computeris always looking for interesting editorial
content. In addition to our theme articles, we have
other feature sections such as Perspectives, 
Computing Practices, and Research Features as 
well as numerous columns to which you can 
contribute. Check out our author guidelines at 
http://computer.org/computer/author.htm 
for more information about how to contribute to 
your magazine. 

ton University. Her research interests are in com-
puter architecture and the hardware/software inter-
face, particularly power-efﬁcient microarchitectures
and  power-adaptive mobile  systems. Martonosi
received a PhD in electrical engineering from Stan-
ford University. She is a senior member of the IEEE
and  a  member  of  the  ACM.  Contact  her  at
martonosi@princeton.edu.

David  I. August  is  an  assistant  professor  in  the
Department of Computer Science at Princeton Uni-
versity, where he also directs the Liberty Research
Group to develop open source tools for systematic
processor  design-space  exploration  (http://lib-
erty.princeton.edu). His research interests are in
computer architecture and back-end compilation.
August received a PhD in electrical and computer
engineering  from  the  University  of  Illinois  at
Urbana-Champaign. He is a member of the IEEE
and the ACM. Contact him at august@cs.princeton.
edu.

Mark D. Hill is professor and Romnes Fellow in
both the Computer Sciences Department and the
Electrical and Computer Engineering Department
at  the  University  of  Wisconsin–Madison.  With
David Wood, he also codirects the Wisconsin Mul-
tifacet project to improve commercial servers. His
research interests include cache design, cache simu-
lation,  translation  buffers,  memory  consistency 
models, parallel simulation, and parallel computer
design. Hill received a PhD in computer science from
the University of California, Berkeley. He is an IEEE
Fellow.  Contact  him  at  markhill@cs.wisc.edu  or
www.cs.wisc.edu/~markhill.

David J. Lilja is a professor of electrical and com-
puter engineering at the University of Minnesota
in Minneapolis. His research interests are in com-
puter architecture, parallel processing, nanocom-
puting,  and  computer  systems  performance
analysis. Lilja received a PhD  in electrical engi-
neering from the University of Illinois at Urbana-
Champaign. He is a senior member of the IEEE
and  a  member  of  the  ACM.  Contact  him  at
lilja@umn.edu.

Vijay S. Pai is an assistant professor in the Depart-
ment of Electrical and Computer Engineering at
Rice University. His research interests include com-
puter architecture, high-performance networking,
and performance evaluation. Pai received a PhD in
electrical and computer engineering from Rice Uni-
versity. Contact him at vijaypai@rice.edu.

36

Computer

  	 
    
 !  

"#%$&'	(*)( +-,/.102 #%.1#% (3! )4 3&5	 (6578):9 ;<# += 2&(': >?( &3@7A5	 B?)
9C=0D#% BE$!;% )2)B?FGBE 5;H#% $ $5	B

JIK! MLN LCLO! P!QRII MSTGUHVMINVMW4 LVMINLYXZV\[AI]\G]^X_IK!  
 M`EO!  /QR /EIaLbXc]\CIK!  DS /d\ / D efV\LI /C]^Xg
[A /H[A 
8! d\! / /!d

Ph

aV/Ri aV/!j O!QG VMCk VMd\W

	]\QR QRII  / C 8[aKHVMd\ El
i]^Xc ML L]\ V\S /mCVMd\K! /nMV\S  /Ko% 	 KHVM
i]^Xc ML L]\HV\S4pO!aS! VMK! 
i]^Xc ML L]\ jW kO!II

q/r\r\s

q/r\r\ s
 4Ji	 4  p efk 
 
 
R
 	 	k

[
 
K!  IK! MLN L]^X

aV/Ri aV/!j O!QGVMkVMd\W4NLC VMU!U! ]ME MS% l

	]\QRQRII / 	KHVM

 !E /aLIhD ]^Xg VMW

Xc]\!NVoH !  

q/r\r\ s



"#+ &;% )43 &5	(

]! o
 /E /hE ]\!  



 

	

%
    "!# !$ !#!$!# !$!# !$!#!$ !#!# !$!#!$ !#!$ !# !$!# !$!# !# !$!# !$!# !$ !#!$ !#!$ !# !
 &'()  *!# !#!$!# !$!# !$!# !$!# !$!#!# !$!# !$!#!$ !#!$!#!$!#!# !$!#!$!# !$!#!$!#!$ !#!+% 
,.- /1032)  4 56 0 +!$!# !$!# !$!# !$!#!# !$!# !$!#!$ !#!$!#!$!#!# !$!#!$!# !$!#!$!#!$ !#!%
,.( '-78!$!# !$!# !#!$!# !$!# !$!# !$!# !$!#!# !$!# !$!#!$ !#!$!#!$!#!# !$!#!$!# !$!#!$!#!$ !#!
:9
;$<'=1 ?>
@A014-7 0*!$!#!$!#!# !$!#!$ !#!$ !#!$!# !$!# !#!$!# !$!# !$ !#!$ !#!$ !# !

EO!UH /aL[/VMWN VM a[aK! I M [AIO! B!$!# !#!$ !#!$!# !$!# !$!#!$ !#!#!$ !#!$ !#!$ !# !$!# !$!# !

[aK! MSO!W !dT ! ID!# !$!#!$!#!$ !#!#!$ !#!$ !#!$!# !$!# !$!#!# !$!#!$!#!$ !#!$!#!$!# !

[A]\UH D]^ X_K! ML NLF!# !$!#!$!#!$ !#!#!$ !#!$ !#!$!# !$!# !$!#!# !$!#!$!#!$ !#!$!#!$!# !
&$<DJ1=K A-3' )' LM:' )J1 0') N#1 -3  AA ,. -< : -7 
;$<'=1 IH
a[aK!I  M[A IO!aVMW dE VM! nMVMI]\B!# !#!$ !#!$!# !$!# !$!#!$ !#!#!$ !#!$ !#!$ !# !$!# !$!# !
iUH /W ! D
EI OH[AIO! 
!#!#!$ !#!$!#!$!# !$!#!$!#!# !$!#!$ !#!$ !#!$!# !$!# !
HLI OH[AI ]\8  !I#!#!$!# !#!$!# !$!# !$!#!$ !#!$ !#!#!$ !#!$ !# !$!# !$!# !$ !#!
HLI OH[AI ]\8 k M[A]S  /#!# !#!$!# !$!# !$!#!$ !#!$ !#!#!$ !#!$ !# !$!# !$!# !$ !#!
C /d\NLI  /4W  $!#!$!#!$!# !#!$!# !$!# !$!#!$ !#!$ !#!#!$ !#!$ !# !$!# !$!# !$ !#!
SR M[AO!I]\8  ! I!#!$!# !#!$!# !$!# !$!#!$ !#!$ !#!#!$ !#!$ !# !$!# !$!# !$ !#!
J1-< 4) 0UM0:WVXJ1 UWY*!# !$!# !$!#!$ !#!$ !#!#!$ !#!$ !# !$!# !$!# !$ !#!
;$<'=1 +T
C /]\aS /mO[Z
 /?!# !$!#!$!# !$!#!#!$ !#!$!#!$!# !$!# !$!#!# !$!#!$!#!$ !#!$!# !$!# !
UH / aVMI]\\ !$!#!$!#!$!# !#!$!# !$!# !$!#!$ !#!$ !#!#!$ !#!$ !# !$!# !$!# !$ !#!
a[aK!I  M[A IO! W!#!$!#!$!# !#!$!# !$!# !$!#!$ !#!$ !#!#!$ !#!$ !# !$!# !$!# !$ !#!
VMWN S!VM I]\8 ! I!#!$!# !$!#!$!#!# !$!#!$ !#!$ !#!$!# !$!# !#!$ !# !$!# !$ !#!$ !#!$ !# !
HLI OH[AI]\ M^
HS]_8!#!$!# !$!#!#!$!# !$!#!$ !#!$ !#!$!# !#!$ !#!$!# !$!# !$!#!$ !#!

Ea[
	]\I]\W!# !$!#!$!#!# !$!#!$!#!$ !#!$!# !$!# !#!$!# !$!# !$!#!$!# !$!#!

Ea[ACR 	]\I]\ W!#!$!# !$!#!# !$!# !$!#!$ !#!$ !#!$!# !#!$ !#!$!# !$!#!$!#!$ !#!

Ea[

Ea[AC
kVMIaVB!$!#!#!$!# !$!#!$ !#!$!#!$!# !#!$ !#!$!# !$!#!$!#!$ !#!
q$`
HLI OH[AI ]\f 
[aK! MS O!W /Aa
 bc!# !$!# !$!#!$ !#!$ !#!#!$ !#!$ !# !$!# !$!# !$ !#!
mhUHV\L L!dd !$!#!$ !# !$!#!$!# !$!# !#!$!# !$!#!$!#!$ !#!$!# !#!$ !#!$!#!$ !#!$!#!$!# !
%]EV\Sfmh UHV\LL !dB!#!$!# !#!$!# !$!# !$!#!$ !#!$ !#!#!$ !#!$ !# !$!# !$!# !$ !#!
maVMH[aK8mhUH V\LL! dD!$!#!#!$ !#!$!#!$!# !$!#!$!#!# !$!#!$ !#!$!#!$!#!$ !#!
^@ I D mCV\[ajG	]\I ]\W.!# !$!#!$!#!# !$!#!$ !#!$ !#!$!# !$!# !#!$ !# !$!# !$ !#!$ !#!$ !# !
^@ I DmCV\[ajR i4]\WN[Ah. !$!#!#!$!# !$!#!$ !#!$ !#!$!# !#!$ !#!$!# !$!#!$!#!$ !#!

>>
q/s
C7]
C7C
C7E
EG
E7Q
G3E
G3E
G7G
G3e
G3e



 
>
q
 
q
C
q
 
C
E
q
 
E
G
O
C
 
q
s
C
 
q
 
q
P
C
 
q
 
C
Q
C
 
q
 
E
Q
C
 
q
 
G
Q
C
 
q
 
s
Q
E
 
q
q
G
E
 
q
 
q
E
 
q
 
C
q
P
E
 
C
E
 
E
E
 
E
 
q
q
E
 
E
 
C
E
 
E
 
E
E
s
E
 
E
 
G
E
 
G
E
 
G
 
q
E
 
G
 
C
E
 
s
E
 
s
 
q
;$<' =1 

J
@$@A 56=)  56  0'  08!#!$!#!$ !#!$!#!$!# !#!$!#!$!# !$!#!$!#!$ !#!
	OHLI]\Q*k  ML d\I!# !$!#!$!#!$ !#!#!$ !#!$ !#!$!# !$!# !$!#!# !$!#!$!#!$ !#!$!#!$!# !
]E]\WNL !#!$!# !$!#!$!#!$!# !#!$!# !$!# !$!#!$ !#!$ !#!#!$ !#!$ !# !$!# !$!# !$ !#!
!#!$!# !$!#!$!# !$!#!$!#!# !$!#!$!#!$ !#!$!#!$!# !
V/hE]\O!Ie8  /I K!] S]\W]\d\h
mCV\LN[D 	 /W WNL !$!# !$!#!$!#!$ !#!$!# !#!$ !#!$!# !$!# !$!#!$ !#!#!$ !#!$ !#!$ !# !$!# !$!# !
C /]\aS /mO[Z
 /+!#!$!# !#!$!# !$!# !$!#!$ !#!$ !#!#!$ !#!$ !#!$!# !$!#!$!#!
HLI OH[AI]\ M^
HS ]_
!#!#!$ !#!$!#!$ !#!$!#!$!# !#!$!#!$!# !$!#!$!#!$ !#!
HLI OH[AI]\ f
[aK! MSO! W /W!#!$ !#!$!#!$!# !$!#!$!#!# !$!#!$ !#!$!#!$!#!$ !#!
QR !dD!#!# !$!# !$!#!$!#!$ !#!$!# !#!$ !#!$!# !$!# !$!#!$ !#!#!$ !#!$ !#!$ !# !$!# !$!# !
C /]\aS /mO[Z
 /+!#!$!# !#!$!# !$!# !$!#!$ !#!$ !#!#!$ !#!$ !#!$!# !$!#!$!#!
!#!#!$ !#!$!#!$ !#!$!#!$!# !#!$!#!$!# !$!#!$!#!$ !#!
HLI OH[AI]\ M^
HS ]_
;$<'=1 IO
J15)'  06'04 . A):
!$!#!$ !#!$!#!$ !#!#!$!#!$ !#!$!#!$!# !$!#!

EQT O!WNVMI ]\
!$!# !$!# !$!#!$ !#!$!#!#!$ !#!$!#!$!# !$!#!$!#!# !$!#!$ !#!$ !#!$!#!$ !#!

EQT O!WNVMI ]\RI]E]\WNL#!#!$ !#!#!$!#!$ !#!$ !#!$!# !$!#!#!$!# !$!#!$ !#!$ !#!$ !# !
m /H[aK!QGVMj!L!#!$!#!$!# !#!$!# !$!# !$!#!$ !#!$ !#!#!$ !#!$ !# !$!# !$!# !$ !#!

EQT O!WNVMI ]\Gi] [A MLL
!#!#!$ !#!$!#!$ !#!$!#!$!# !#!$!#!$!# !$!#!$!#!$ !#!
C MLO!WIaL$ !#!# !$!#!$ !# !$!#!$!# !$!# !#!$!# !$!#!$!#!$ !#!$!# !#!$ !#!$!#!$ !#!$!#!$!# !

EUH / MSO!U!# !$!# !$!#!$ !#!#!$!#!$ !#!$ !#!$!# !$!#!#!$!# !$!#!$ !#!$ !#!$ !# !

 

EIaVMW WNL!#!$!#!$!# !$!# !#!$!# !$!# !$!#!$ !#!$ !#!#!$ !#!$!#!$!# !$!#!$!#!
maVMH[aK8i MSN[AI]\.!$!# !#!$!# !$!# !$!#!$ !#!$ !#!#!$ !#!$ !# !$!# !$!# !$ !#!
^@ I 
!#!$!# !#!$!#!$ !#!$!#!$!# !$!#!#!$!# !$!#!$!#!$ !#!$!# !
]_
E  /
;$<'=1 
	
;$0-3)A08 !#!$!#!$!# !#!$!# !$!# !$!#!$ !#!$ !#!#!$ !#!$ !# !$!# !$!# !$ !#!
!O!IO! Dk   M[AI]\ 
!$!#!$ !#!$!#!#!$ !#!$!#!$!# !$!#!$ !#!# !$!#!$ !#!$ !#!$!# !$!# !
!#!# !$!# !$!# !$!#!$ !#!$ !#!#!$ !#!$ !#!$!# !$!# !$!#!# !$!#!$!#!$ !#!$ !# !$!# !
()'=<
!#!$!# !$!#!$!# !$!#!$!#!# !$!#!$!#!$ !#!$!#!$!# !
LM3')  4.  A):
,.= =K 04: 9

 
G3Q
sq

s\r
e7C
e7E
eG
e7e

PC
PE

Pe
P7P
PQ


 

 

G
 
q
G
 
q
 
q
G
r
G
 
q
 
C
G
 
C
s
C
G
 
C
 
q
s
C
G
 
C
 
C
G
 
C
 
E
G
 
E
G
 
E
 
q
G
 
E
 
C

>
s
 
q
P
q
s
 
q
 
q
P
q
s
 
q
 
C
s
 
q
 
E
s
 
C
P
s
s
 
C
 
q
P
s
s
 
C
 
C
s
 
C
 
E
s
 
C
 
G
e
 
q
P
r


>
,

 

	W




	

8	

q\q


k
i
!$!#!$ !#!$!#!$ !#!#!$!#!$ !#!$!#!$!# !$!#!
a[aK!I  M[A IO!aVMW dE VM! nMVMI]\
iUH /W ! D
EI OH[A IO! $ !$!#!$!# !$!# !#!$!# !$!#!$!#!$ !#!$!# !#!$ !#!$!#!$ !#!$!#!$!# !
mW][ajR kNVMd\ aVM Q6]^X_IK! 

[aK! MS O!W! d
 ! I+!#!$!# !$!#!#!$!# !$!#!$ !#!$ !#!$!# !
4W]E]\ iWN VM8]^XbIK! DC /]\aS / mO[ Z
 /I!$!#!$ !#!$!#!$!# !#!$!# !$!# !$!#!$ !#!$ !#!
k

 /WN S
!#!$ !#!$!#!$!#!$ !#!#!$!#!$ !#!$!# !$!# !$!#!# !$!# !$!#!$ !#!$ !#!$!# !
VMWNS!VMI ]\8  !I!# !$!#!$!# !$!# !#!$!# !$!#!$!#!$ !#!$!#!#!$ !#!$!#!$ !#!$!#!$!# !
HLI OH[AI]\ M^
HS]_8!#!$!# !$!#!#!$!# !$!#!$ !#!$ !#!$!# !#!$ !#!$!# !$!# !$!#!$ !#!

Ea[
	]\I]\WO!! IB!$!#!$!# !$!#!# !$!# !$!#!$ !#!$ !#!$!# !#!$ !#!$!# !$!#!$!#!$ !#!
VMQ6P!W ][a j
!$!# !$!# !$!#!$!# !$!#!#!$!# !$!#!$ !#!$ !#!$!# !#!$ !#!$!# !$!# !$!#!$ !#!
C MV\ShRP!W][aj8!#!$ !# !$!#!$!# !$!# !#!$!# !$!#!$!#!$ !#!$!# !#!$ !#!$!#!$ !#!$!#!$!# !
C MV\ShRP!W][ajGQR !d
kNVMd\aVMQ
!$!#!$ !#!$!#!$ !#!$!#!#!$ !#!$!#!$!# !$!#!$!#!
i MSN[A I]\GP!W ][a jD!#!$!# !$!#!$!#!# !$!#!$ !#!$ !#!$!# !$!# !#!$ !# !$!# !$ !#!$ !#!$ !# !
L LO! MSGP!W][ajF!# !$!#!$!# !$!#!$!#!# !$!#!$ !#!$ !#!$!# !$!# !#!$ !# !$!# !$ !#!$ !#!$ !# !
L LO! MSGP!W][ajR E  / IaL]\fV\[aK! De8 NLL" !#!$ !#!$!# !$!# !#!$ !# !$!# !$ !#!$ !#!$ !# !

Ea[ACR	]\I ]\W O!!IB!$!# !$!#!$!#!# !$!#!$ !#!$ !#!$!# !$!# !# !$!# !$!# !$ !#!$ !#!$ !# !
GJ
EI]\ 
\VMW NS!VMI ]\
!$!#!$ !#!$!#!# !$!#!$!#!$ !#!$!#!$!# !#!$!#!$!# !$!#!$ !#!$ !#!
HLI OH[AI ]\f 
[aK! MS O!W  / !$!#!$!#!# !$!#!$ !#!$ !#!$!# !$!# !#!$ !# !$!# !$ !#!$ !#!$ !# !
q/s
HLI OH[AI ]\f 
[aK! MS O!W  /C i4]\ I LLd\!QR  /I	]\U!I]\H L!# !#!$ !# !$!# !$ !#!$ !#!$ !# !

[aK! MSO!W !dT ! ICV/hE]\O!Ic!# !$!# !#!$!# !$!#!$!#!$ !#!$!# !#!$!#!$!#!$ !#!$!#!$!# !
XcI \
E I]\aVMd\ T	 /WW\!#!$!# !#!$!#!$!# !$!#!$ !#!$ !#!#!$ !#!$ !#!$ !# !$!# !$ !#!
mJ
EK!
kVMIaVG	 / WW!$!# !$!#!$!#!$ !#!$!# !#!$ !#!$!# !$!# !$!#!$ !#!#!$ !#!$ !#!$ !# !$!# !$!# !
e
	 /WW
!$!# !$!# !$!#!$ !#!$!#!#!$ !#!$!#!$!# !$!#!$!#!# !$!#!$ !#!$ !#!$!#!$ !#!
%]E]\jO!U
	 /WW%G IK! 
IK8C]_
!#!$!# !$!#!$!# !$!#!$!#!# !$!#!$!#!$ !#!$!#!$!# !
kE / 	 /W WNL!# !$!#!$!#!$ !#!$!# !#!$ !#!$!# !$!# !$!#!$ !#!#!$ !#!$ !#!$ !# !$!# !$!# !

EK!
XcI\ 
EI ]\aVMd\ T	 /WW
!#!$!# !#!$!#!$ !#!$!#!$!# !$!#!#!$!# !$!#!$!#!$ !#!$!# !
kVMIaVG	 / WWV/hE]\ O!IF!$!#!$ !#!$!# !#!$ !#!$!# !$!# !$!#!$ !#!#!$ !#!$ !#!$ !# !$!# !$!# !
aVMHLUHVM /IVM Ia[aK@
 [aK! /QGVMIN[
!$!#!$ !#!$!#!$!# !$!#!#!$!# !$!#!$ !#!$ !#!$!# !
aVMHLUHVM /I VMIa [aKfV/hE]\O! I!# !#!$!# !$!#!$!#!$ !#!$!# !#!$ !#!$!#!$ !#!$!#!$!# !
%]E]\jO!U
	 /WWV/hE]\O!I$a

e3b !#!#!$!# !$!#!$!# !$!#!$!#!# !$!# !$!#!$ !#!$!#!$!# !
C /]\aS /mO[Z
 / QR!d
kNVMd\aVMQ
HL M[b!#!$!# !$!#!#!$ !#!$!#!$!# !$!# !$!#!
HLI OH[AI]\ M^
HS]_QR!dDk NVMd\aVMQ
HL M[b!$!#!# !$!#!$!#!$ !#!$!#!$!# !
!$!#!# !$!#!$!# !$!#!$!#!$ !#!#!$!#!$ !#!$!#!$!# !$!#!
GJ	 IN [/VMWiVMI K

 
Xc]\ IK! 
HLI OH[AI ]\HLCi4 / 	h![AW 
ai b#!#!$!# !$!#!$!#!$ !#!$!# !#!$ !#!$!# !$!# !$!#!$ !#!
^@ I 
E / 
]_
!#!$!# !$!#!$!#!# !$!#!$!#!$ !#!$!# !$!# !#!$!# !$!# !$!#!$ !#!$ !#!

q\q

C7C
C7E
CG
C7e
C3P
E7]
EG
E7e
E7Q

s\s

s\r
e7]

e7C
e7Q

PQ



C
 
q
e
C
 
C
P
E
 
q
q
C
E
 
C
q
G
E
 
E
q
Q
E
 
G
C
q
E
 
s
E
 
e
q
E
 
P
E
 
Q
C
s
E
 
r
E
 
q
]
E
 
C
r
E
 
q
C
E
 
q
E
E
 
q
E
 
E
 
q
e
E
r
G
 
q
s
]
G
 
C
s
E
G
 
E
s
G
G
 
G
G
 
s
s
e
G
 
e
s
P
G
 
P

^
G
 
Q
G
 
r
e
q
G
 
q
]
e
q
G
 
q
G
 
q
C
a
e
s
G
 
q
E
a
G
 
q
e
r
s
 
q
P
s
s
 
C
 

	W



 

8	

C /]\aS /mO[Z
 /4  /WNS! L.!$!#!$!#!# !$!#!$ !#!$ !#!$!# !$!# !#!$!# !$!# !$!#!$ !#!$!#!

[aK! MSO!W !dT ! ICQR!dI!$!# !#!$ !#!$!# !$!# !$!#!$ !#!#!$ !#!$ !#!$ !# !$!# !$!# !
C /]\aS /mO[Z
QR!d+!$!#!$ !#!$!#!$!# !$!#!#!$!# !$!#!$!#!$ !#!$!# !
 / iKHV\L 
HS]_ iKHV\L D4VMP!W ?!$!# !$!#!$ !#!$ !#!#!$ !#!$ !#!$!# !$!# !$ !#!
HLI OH[AI ]\M ^
HLI OH[AI ]\f 
[aK! MS O!W  /CQR! d?!$ !#!$!# !$!# !$!#!$ !#!#!$ !#!$ !#!$ !# !$!# !$!# !
%NLI ]^X_m /H[aK! QGVM jTi]\d\aVMQG L!# !$!#!$ !#!$ !#!$!# !$!# !#!$ !# !$!# !$ !#!$ !#!$ !# !

 

EIaVMW WNLc!# !$ !#!$!#!$ !#!$!#!$!# !#!$!#!$!# !$!#!$ !#!$ !#!#!$ !#!$!#!$!#!$ !#!$!#!
maVMH[aKf
EIaVMI NLI N[/ L!$!#!$!# !$!# !#!$!# !$!#!$!#!$ !#!$!# !#!$ !#!$!#!$ !#!$!#!$!# !

eG
eG
e7e
e7Q
PC
Pe
P7P



G
 
q
s
C
G
 
C
G
 
E
G
 
G
G
 
s
s
 
q
s
 
C
s
 
E
d\






	

 /QThTI KHVM !j!LCI]Gi]^Xc MLL]\ V\S / mCVMd\K! /nMV\S /K oHQThRV\ S NL]\ /o
]
QThTVMP!W IhRVM HSGK! NLC[A]\ I O!]\OHLd\O!NS! VMH[A  IK!]\O!d\K! ]\O!IIK!   ML MVMa[aK

Xc]\K! NLgXZVMIKG

HV\S pO!aS!VMK! VMH S

i]^Xc MLL]\

IKHVM!jfQTh8I K! M LN L[A ]\QR QRII A  QR /QTPH /aLAo%i ]^Xc MLL]\
_]\j
Xc]\ IK! /  MV\ S !dRVMH SG /\VMWOHVMI]\G ]^X_QT h
jW%kO!I I/o
 ML MVMa[aKTd\ ]\O!U
Xc]\IK!  /K!  /WURVMHSRLO!U!UH]\I/o
PH /aL]^X
4IKHVM!jRVMWWHIK!  QR / Q
IK! 
 k
i
IK!NLIK! MLNLc_ ]\O!WNSfKH V/E  DPH / /@QR UH]E LLP!W 
_CIK! ]\O!I _CK!]\Q
VMWWN V\[A Eo
 ML UH M[AN VMWW hf
EI /E /W^
IK! GWNV/hE ]\O! I
Xc]\T K!NL!INVMWK!  /WU6_CIK
IKHVM!j
EK! /!d
VMWNL]@IKHVM!j?efVMj
 MSI]\
OHVM!d
i4]\ IOHL
Xc]\K!NLU!]E]^X
	
 MV\S !dGVMHSR  \VMWOHVMP! W 
Xc / MSPH V\[ajT]\8 IK!  IK! MLNL
V\LIDP!O!ID !]\I W M V\L I/oCI KHVM!j?
E IaVMH S! VMaS
QR  _C IK
 ARP!W  c_]\jG K!]\ O!a LVMH SGVMWW]_C! d

e8N[A]ELh!LI /QGL 	]\UH ]\aVMI]\
QR I]TOH L IK!  /U! W]\ II /gXZV\[AWIh

Xc]\U! ]MNS!d



 


Z
 
 
 

 
 

 
	W



 \

 \8	

"#%$&'	(*)(+-,/.102#% .1#% (3!)4 3 &5	(6 578 ):9 ; <# +=2&(':>?(&3@7A5	 B?)
9C=0D#% BE$!;% )2)B?FGBE 5;H#% $ $5	B
Ph
aV/Ri aV/!j O!QG VMCk VMd\W
efV\LI /C]^Xg
[A /H [A 8 !d\! / /!d
 ! E /aLIh
Xc]\!NVoH !  Eo
VMW
]^X
q/r\r\ s
i]^Xc MLL]\ V\S  /mCVMd\K! /nMV\S  /Ko%	 KHVM

NLIKHVMIDK!d\K!  / UH /
K! RQGVR Q6]^X
PH 
Xc]\QGVMH [A T[/VM
VMa[aK!I M[AIO!  
IK! RL O!UH  /aL[/VMWNVM
K! NL NLC  MVMWn/ MSTGKHVMaS
V\[aK! / E MSTPhR AR M[A O!I !dDQTO!W I U!W H LIOH [AI]\H LCLQTO!WIaVM!  /]\OHLWh
_CVM  fIK!]\O!d\K
HLIOH[AI]\ HLT HLNS 8IK! 
LI]\!d V_CHS]_*]^X
 ! IT Ph

[aK! MSO!W !d
IK! 
U!][A MLL ]\/oVM HS
 ML ]\W !d8 IK! 8S /UH /HS  /H [A MLSh HVMQR N[/VMWWh
 	
	   

 	  ! #"$ %&R UH]\ WN [AhGN L
IK! R
 [aK!  MS O!W!dR  !I/o% S MLd\! MS
K!  TS /IaVMWNLC ]^X
Xc]\WW]_ MS
V\LTUHVMIG ]^X
IK! @
EO!UH /aL[/ VMWN VMG kd\IaVMWD
E d\HVMWi] [A MLL]\a
 k
 i baoCVM f U! ML / I MS?IK!NL
:K!d\K?LUH  / MSf]^X
IK! MLN LVMHS
n
]7]fe
VM GS NL[AOHLL MS
IK! T\ VM ]\OHL KHVM aS  _CVM RIaV\S  /]
Xc]\ Q*V
VMHS8V
E /hG[A ]\QRUHV\[AIWN V/hE]\O!I#a
CMQRQ-PhMQ
G\QR QWb4 MLO!WI MS
XcO!WW
	[AOH LI]\Q6S  MLd\G
XcO!H [AI]\G]^X_IK! DWNV/hE]\O!I/o
XchTIK!  
WNV/hE /CQR  /Ia VMW4e@
R I M[aK! !]\W]\d\h
bcE
]('6aSaVA_C
]R E /
I ML IE M[AI]\ aL
Xc ]\Q
IK! TL QT O!WN VMI]\8]^X
 MVMWPH /H[aK! QGVMj!Lc_  / 
OHL MS
K!  R[AO!  /IS MLd\
GT]\ 8IK!  DV/E /aVMd\ 
ai bC[A]\O! IC ]^X
QGVM IaVMHLVMf HL I OH[A I]\ Gi4 / 	 h![AW 

 





 

	
 
	
 
 
Z
L
 
q
C
 
 
q
 
	
 
 
C
 
 

R
  	 






QRU!]ME 
[A]\HLIaVM IGLIaVMd\ MLT]^X
IK!]\O! d\K
KHV/E G PH / /d\ ]\!d
e8N [A]\U!][A MLL ]\G S ML d\H L
]^X
QR / I
IK! QRN[A]
LUH / MS
H[A MV\L!dIK!  [AW][aj

I M[aK!!]\W]\ d\ ML4K! /WUH MS
QRN [A]\
K! LO!P
IaLKHV/E TPH / /
U!][A MLL ]\aLAoS_CK!W  TU! UH  /W  ! !dRQRU! ]ME MSGIK!  T IK!]\O!d\K! U!O! I	 P!O! IDIK!  /PH  /!  
K!NL
Xc]\QR ]\ 
HLVM INVMP!W 
 MV\L Wh8]\ O!I
U!] [A MLL!dGUH ]_ /
VMHSfQR]\ 
IK!aLI
UHV\[A MSfPh8I K! 
KHV\LCW  MSTI]T IK!   M L MVMa[a KRGN S / I
Xch!dDS 
 / /ICQR  MVMHL	 ]^X_V\[aK!  / ! dDIK! DS  ML MSTUH /
Xc]\
QGVM H[A T QRU! ]ME  /QT  AI
IK! RQR MVMH LNLI]8 OHL RIK! GHLIOH [AI]\?% /E  /Wi VMaVMWW /WNLQ
! T ]^X
aib !K! / / I fI K! T VMU!U!W N[/ VMI]\@U! ]\d\aVMQfoVMHSf AR M[AO!I DQTO! WIU!W L[/VMWNVMHLIOH [AI]\HL
PH 8 MV\LWh?U!WNV\[A MS
K! f V\S!S I ]\H VMW KHVMaS  _CVM f[/VM
UHVMa VMW W  /W
IK! f[aK!U?PH M[/VMOHL f]^ X
]\
IK!  H[A MV\L !dT[A a[AO!IS /HL I ML
K!  / VM I
_]
SNLIH [AICVMa[aK!I M[AIO!  MLIKH VMI AR U!W]\I	IK!NL
Xc]\H[A M V\L  MSRUH /
Xc MVMIO! D ]^X
	 /hG%]\!dGHLIOH [AI]\W^@]\aSbCVMHS
^+a
Xc]\QGVMH [A El	 
i
Qo

EO!UH /a L[/VMWNVM 
q\q
VMU!U!]EV\[aKo\V\LIK!  H VMQR 	 LO! d\d\ MLIaLAo\NL% PHV\L MSO!UH ]\TVMD I /WWd\ / IH[A]\QRU!W /
K!  
IKHVMIR ARIaV\[A IaLTH S / UH /HS / IT]\UH  /aVMI]\H LGVMIG[A]\QR U!W G IQR GVMHS
 /H[A]S MLTIK! /Q
I]
]\! 
Xc]\QJIK!  	HLIOH[AI]\
K! ML  [A ]\H [AO!  / I]\ UH /aVMI]\HLVM 	IK!  / AR IaV\[AI MS
HL I OH[AI]\ #_]\ aS
PhIK! @KHVMaS _CVM EoVMH S
 AR M[AO!I MS
UHVMaVMWW /W
K!  fP!d\d\ MLI8V\S\ VM IaVMd\ 
]^X
IK! f
UHVMaVMWW /W!S! VMIaVMUHVMIKHL
 MS KHVMaS _C VM Eo3 _CK!N[aKDQGVM! Wh [A]\H LNLIaL ]^X
U!][A MLL ]\	NL4IK!  L QRU!W 
K! 
VMa[a K! I M[AIO! D  /W  MLK! M V/ WhG ]\
IK! TVMP!WIh8]^XgIK!  R[A]\QRU! W /
K!  
 /H[A]S! dfVMWNL]G MLO!WIaL





	



	
 
	
	

	
 
Z
	
 
 
 
 
^
 
 
^

 
 
q
G[A ]S  ARUHVMHL]\ o! V\L	IK!  LW]\IaL VM 
 ARIaV\ [AI MS

WW MS

_CIKGi	LK _CK! /GLO

[A /I UHVMaVMWW /WNLQ:NL	!]\I




	   	   	

K! G 
EO!UH  /a L[/VMWNVM 
VMa[aK! I M[AIO!  
S
 /aL
Xc]\ Q
IK! 
 
@IKH VMID IK!  TL[aK! MS O!W!dG]^ X
"$#	IK!  KHVMaS  _CVM  Ph ML]\W !dIK! CS! VMIaVS  /UH /H S /H[A ML
! 
IK! CHLI OH[AI]\HLN L[/VM   MS ]\O!I
ShHVMQRN[/ VMW W hMaShHVMQRN[ L[a K! MSO!W! d b
K! CP!d\ d\ M LIK! HS aVM H[A  CN L4IK!  [A]\QRU! W ARIh ]^X
IK! CKH VMaS _C VM IKHVMIKHV\LI]PH  S MLd\! MS
K!  G / I THLIOH [AI]\HLKH V/E TI]fPH  8 LI]\ MS@
I]@ [/VM h@]\O!I
IK! GShHVMQR N[
L[aK!  MS O!W! d
- )$
"%-)$+*
IK! @\VM]\OH L8[A]\ I]\W P!IaLD a&% ('
)$+*,%
IK! @KHVMaS _CVM ?VMW ]\! d
_CIK
V\S!SI ]\o4V8[A ]\QRU!W  ARGNLLO!  TQR M[aKH VM!NLQ-!  / MS!L I]8PH  T QR U! W /QT  /I /SG IKH VMIW]E]\j!L
 /Ia[

VMI8VMW WIK! @ V/\ VM WN VMP! W f M L]\ O!a [A MLRVMHS
IK! fUH ]E]\W ]^X
HLIOH[AI]\HLT /E  /h? [Ah! [AW E oCVMH S
NLLO! ML
IK! /Q
V\[/ [A]\aS!VMH[A W_CIK?I K! 8NLLO!  8VMWd\]\IK! Q
WIK!]\ O!d\K 
VMa[aK!I M[AIO!  MLKHV/E 
d\W]\PHVMWU! N[AIO!  ]^Xb IK!  D[A]S 
IK! V\ S\ VM IaVMd\ D]^XbW]E ]\j ! dTVMIV
Xc]\L[aK! MSO!W! d!oEIK! NLKH V\LPH / /
 ARU!W]\  MS
Xc]\CIK! 

EO!UH /aL[/VMWN VMVMa[aK! I M[AIO! VMWNL]
oHVMHSGKH V\L LK! ]_C8 I]R[/VMOHL  !]\IN[A MVMP!W 
QRU! ]ME /QT /EI

Xc]\!NVo  !  TKHV\ L /E]\WE  MS
VMW
 ! E  /aLIhG]^X
IK! 
Xc]\ Q
K! R ]\!d\]\!df M L M VMa[aK
VMI
UHVMaVMWW /W4U! ][A MLL!dR  /]\
OHL MSGfV
U!][A MLL]\ I]TPH  
	VRL QRU!W   
!
h! 
!
q
QR / I/o!I ]GIK! 
HVMWWh8 I]RIK! T[AO!  / I ML MVMa[aKf]\
U!][A MLL]\/oVMHS
	VT 
 i		
IK! 

k
i
a
EO!UH /aL [/VM WNVMk d\ IaVMW
Ed\HVMW4 i ][A MLL]\b

C

 
 
Z
^
 
 
o
 
b
 
 
^

E

 
	

e

^

 
 

	 

	 


NL!]\ IP! H S!dGI]G]\ ! RLUH M[A
d\O
[A]\
E /fIK! G
EO!UH /aL [/VM WNVMDVMa[aK!I M[AIO!  DIaL /W
Xc]\ AR![A /U!I]\

E]\ QR D]^XgIK! TQR  /IK!] S!L
4I KHV\LD L /E /aVM WUH]EL L P!W TQRU!W /QT A IaVMI]\H L
aVMI ]\
&*
'! 
%%&*
CVMH S
 M[A ]ME  / hRVM El #   % &
 
$"$
!
 
,%
K! ]\O!I
]^X
	
]\aS /	NL LO! CQR M [aKH VM! NLQJ[/VMDPH  CQR U! W /QT  /I /SIK!]\O!d\K
 /IK! /4IK!  
%&

%
&&%
&& %
%& ]\V
K!  
%& %$
!  & % &#"
%&%$
! & % &&"
"
" 
 / /I_C V/h! L
IaL /W
Xb[/VM8PH  QRU!W / QT /I /S
RQG VMhR S
IK! D
 k
i4 oIK! 
 [aK! MS O! W!dD  !Ica
 # bS_CK! N[aKTIK! [AO! /I
K!  QGVM R[A]\QRUH ]\! /I	]^X
V?C /]\aS /GmO[ Z
IK! MLN LTNLR VMWW VMPH]\O! I/o O!IW  n/ ML
 /GVMH S
QR] S
 MS?QRU! W /QT /EIaVMI]\?]^X
IK! 
  NL4I]D LI]\ CIK!  H LIOH[AI]\HL4]^X
HS]_
[A /Ia VMW n/ M S HL I OH[AI]\ 
]^X
K!  gXcO!H [AI]\
IK! U! ]\d\aVMQ6PH / !d
 AR M[AO!I M S
HLNS  IK!  HLIOH[AI]\.^
HS ]_o
Xc]\ Q
_CK! N[aKRHS  /UH  /HS /I
VM 8 NLLO! MS	 
	I]@IK!  8!O! H[AI]\HVMW  !IaL
HL I OH[AI]\ HL
VM 
 A_1H LIOH[AI]\HL
VMWW][/VMI MSR  /I  ML	R IK! D 
^o _CK! W ]\ WNSR H LIOH[AI]\HL	d\ /ILK! 
XcI MST]\O! I/o! MLL / INVMWWhTQR]M!d
! # "$% &@NLDKHVMHSW MS
IK! f[A]S  
IK! M_CHS]_6VMW ]\!d@ IK!  GW /!d\IK]^X
K!  
 	
	  
Ph
! INVMW Wh
LI]\ !d@IK! G  M LO!WIaL
?IK!  fC /]\aS /TmO[ Z
 /TVMH S
[A]\QRQRII! dGIK!  /Q
I]@IK! 
%
%&)'QR  M[aKHVM!NLQ1I]
C /]\aS /m O[Z
K! 
C /d\NLI / 4W  % &	   
 / /QR U!W]Mh! L!(' 	&
 ML ]\W E IK! L I]\aVMd\ C[A]\
N[AIa L
SRI /H LE  LQTO! WNVMI]\H L_ / [/VM MS ]\O!I4 P hD
E I /E / ^
VMWWNV\[A 
V\LUHVM ID]^X
K!NL IK! MLNL
I]fVMHVMWh n/ TIK!  RQR UHV\[AI]^X
[aKHVM!d\! dGIK!  R\VM]\OHLUH VMaVMQR /I  /aL
q/s
V\LL ][A NVMI MSD_CI K@IK! R
 	gXc  /Ia[aK8PH VMHS  _C NSIK
 O!QTPH /]^ X
4
  :S /U!IK
4P! aVMH[aK@U!  MSN[AI]\
O!! IaLHHLIOH [AI]\8VMHSfS!VMIaVT[/V\[aK! 
\
EI]\ 
HVMHS8QG VR Q
 LAoHe8O!WI U!W / aLVMH Sf%]EV\S
O!Q
NLLO! 
K! M L 
PHV\[aj8UH]\Ia L
A_CI  
IaVMjE  /fI]G[A]\H LNS  /aVMI]\_C K!W T S MLd\!!d
 MLO!WIaL_ / 
VMHS?QR U!W /QT / I 
!dGIK! f
 

E]\QR RQR]\  
! G IO! !!dE L
KHV/E G PH / /?QGV\ S R /dE VMaS!d@IK! 
]^XgIK!  TKHVMaS _CVM 
PHV\[aj8UH]\WN[AhE o% PH M[/VMOHL 
P!aVMH[aK@U!  MS N[AI]\ oHNL LO! TQR  M[aKHVM! NLQ6VMHSD_CI 

E
X

[

	
 

Q

 
	

 
Z
 
V

V
^
 
 
 
 

 


	
 
 

	
[A]\O! aL D]^X

IK! 

S MLd\

 ARU!WNVM! !d

Ia V\S / ]
L E]\WE MS
GI K! D[aKHVMU!I  /a LI KHVM I

K! ML  DKHV/E  PH / /fS NL[AOHLL MSG8IK!  
Xc]\W W]_
KHV/E f  M[A /IWh?O!IWn/ MS?WQRI MS
E O!UH /aL[/VMWNVM
! A_*U!][A MLL]\ fVMa[aK! I M[AIO!  ML
efVMh
QRU! W /QT AI MSfIK!  8C  ML /\VMI]\
e8]ELIT ]^X
IK! /Q

EIaVMI]\
VMU!U!]E V\[aK
VMHS
SNL
U!H[A U!W ML
[/VMa S MS8 IK! R	 / I aVMW n/ MSf
[aK! MSO!W! dR  ! IVMU!U!]E V\[aK@PH M[/VMOHL T]^XgVG[A]\QRU! W ARRNLLO!  
QR M[aK
VM!NLQ
VMHSfQGVMhG]\ UH / aVM HS@P! OHL MLAo IK! 
 AR![A /U! I]\8 PH /! dGIK!  Te8 /IaV
]_
VMa[aK!I M[AIO!  
K! NLC IK!  MLNLAo! IK! ]\O!d\ KfIK! QRU!W /QR A IaVMI]\T]^X
_CK!N[aKGN LVMPH]\O!II ]TK!ICIK! DQG VMjE /I
VT[A /
Ia VMWn/ MSG 
 Do!VMWN L]
K!d\K!W d\KIaLIK! LQRU!W
 MS
NLLO!  W]\d\N[VMHSRQG VRQTO! QO!IWnMVMI]\R]^XbIK! 
_ / /8 !O!H [AI]\H VMW  !IaL
]\UH /aVMHSGP!OHL MLCPhGL KHVM  !dTIK! /Q1PH /I

 


	 

	 

_CV\LGS MLd\ ! MS
 ! I. _CK!N[aK
K!NLTIK! ML NLTU!  ML  /IaLT IK!  
IK! 
V\LR UHVMIG]^X

[aK!  MSO! W! d

k
i
VMIRIK!  8 ! E  /aLIh@]^X
PH /!d?S  /E  /W]\UH MS
a
EO!UH /aL [/VM WNVMRk d\ Ia VMW
Ed\HVMW i] [A MLL]\b
Xc]\ !NVoH ! 
VMW

Xc]\WW]_C! dG[aKH VMU! I /d\E ML
K!  
G8[aKHVMU!I /aL
O!U@ ]^X
IK! MLN LNLQGV\S 
K! T PH]S h@]^XgIK!  
S ML[AU!I]\
]^X
IK! R\VM]\OHL
K! NLD H [AWOH S MLD V8P! 
VMa[aK! I M[AIO!  
V/E]\D]^X
IK! f
k 
i
K!  T
[aK! MSO! W! dT ! IVMH S8IaLCQRU! W /QT  /EIaVMI]\GVM 
[A]\QRUH]\! /IaLCOHL MSG 8IK! DU! ][A MLL]\
Xc]\O! IKT[aKHVMU!I /	 SNL[AOHLL ML4 IK! C WNV/hE]\O! I]^X
HS NSOHVMW
SNL[AOHL L MSD TS /Ia VM WR	KHVMU!I /E
K!  
4H VMWWhEo	IK! 
[A /WWNL
IK! @
 [aK!  MS O!W! d@ ! I
IQR! dfVMH VMWh!LNLT]^X
S /IaVM W MS
VMHSU!]MNS ML
LQTO!WNVMI ]\TU!] [A M LLVMHSR IK!   MLO!WIaLC VM IaVMP!O! WNVMI MSG VMH SG AR U!WNVM!  MST8	KH VMU! I /
!O!IO! 
QRU! ]ME /QT /EIaL	VMHSG ]\I K! / CQR MVMHL	 ]^X_ UH /
Xc]\QGVMH [A  /! KHVMH[A /QR /I VM DVMWNL]RSNL[AOHLL MS

G
Z
 
 
 
	
	


q
C

 
	

 

 
 
V

 
X
 
 
V
 
s
 
 
  	 


 \

	











*8		D 



	

VRP! 
K! T ]ME /aVMW W	
k 
iJVM a[a K!I M[AIO!  DNL U! ML /I MS8fIK!NL [aKHVMU! I //o4 VMW]\! dM _CIK
Xc]\O!
K! NL NL	 OHL MSG V\LCIK! 
S ML[A U!I]\ R]^XbIK! \VM ]\OHLC[A]\QRUH]\! /IaLca

 # b
]\IK!  /CIKHVMGIK! 
Xc /
S!VMI ]\
Xc]\CIK! 
S /IaVM W MSR VMHVMW h!LNLC]^XbIK! 

[aK! MS O!W! dT ! IIKH VMI
Xc]\WW]_L
iW MV\L D 
q/s

k
i? HLIOH[A I ]\8L / IVMH SGQR]\ S  /IaVMWNL ]\8IK! U! ]\UH]E L MS8 VMa[aK! I M[AIO!  
Xc]\IK! 



	



	

 

IK! @
 k
i
]^X
K! f VMa[aK!I  M[AIO!aVMWC]\ dEVM!nMVMI]\
NLTS NS  MS
I]@IK!  / GPH V\LN[GO!!IaL
K! THLI OH[A I]\ f ! I[A]\HLNLIaL]^X_ IK! 
i]\d\ aVMQ
	]\O! I /VMHS8IK! D P!aVMH[aK
a4d\O! 
U! MSN [AI]\VMHSf NL M LUH]\H LP!W  
K!  R
[aK! MSO! W! dR !IVMW]\!d
[A]\ I]\W
]_
]ME /aVMWW
Xc]\IK!  
_CIKD IK! CC /d\NL I /4 W 	VMHSDIK! CH LIOH[AI]\
k M[A] S /
Xc]\QGLIK!  QGVMPH ]ShD ]^X
IK! U! ][A ML
W W\IK! !O! H[A I ]\HVMW ! IaLHOHL MSIK! 	 VMa[aK! I M[AIO! 	VM 	 d\]\O! UH MSDO!H S /IK! S R M[AO!I]\
L]\
 !I
K! 
k
ifOHL M L4I K!  / CVMIK!QR  /IN[ W]\d\N[	 O!!IaLAoEV
P!I	 Ld\!  MSDQT O!WIU! W A/o/V[A]\I]\W
 /CO!! I
LI]\ D P!O[Z
W]EV\SR O!!I/oHVMHS8V
Ia VMHLY Xc /CO! ! I/oHV

 
 



	












 


 



X
 
	
 


 
 


C
 
q
b
 

 
	
 
 
q
e
	
 
s
t
i
n
U
 
n
o
i
t
c
u
r
t
s
n
I

t
i
n
U
 
n
o
i
t
u
c
e
x
E

Instruction Cache

Program
Counter

Branch
Predictor

Instruction 1

Instruction 2

Instruction 3

Instruction 4

Instruction Decoders

Scheduling Unit

8-Port
Register
File

Reorder
Buffer

Instruction
Window

ALU1

ALU2

ALU3

Load
Unit

Store
Buffer

CTU

16-bit
Multiplier

4d\O! # C

lC
 k
i

Data Cache
a[aK! I M[AIO!aVMW dEVM!nMVMI]\ 

e
 
q
 


	 
    

K! U!UH /W ! L IaVMd\ MLgXc ]\IK!  


k 
i

VM IK! 

Xc]\WW]_C! d!l

*HLIOH[AI ]\8! /Ia [aKR P!W][aj

Xc]\Q-H LIOH[AI]\8 [/V\[aK!  





HLI OH[AI]\ 8k M [A] S DVM HS8L[aK! MS O! W ]\ I]

XcO!H [AI]\HVMWO! !IaL

!  M[AO!I  

"$#d^@ I mCV\[ajR  M LO!WIaLI]
%'&6C MLO! WI	]\QR QRI MLO!W IaL

 /

 /]\ aS  /CP! O[Z
Xc]\ Q6 /]\aS /CP! O[Z

 /CI]T  /d\NLI /

W 

K! D LI OH [AI O! ]^ XbI K! U! UH /W!  NLCLK! ]_CG 84 d\O! # C
[Ah![AW  W]\ !d

	V\ [aKR U!UH /W!  LIaVMd\ N L]\! 

Time

IF

ID

IF

EX

ID

IF

WB

EX

ID

IF

RC

WB

EX

ID

IF

RC

WB

EX

ID

RC

WB

EX

4d\ O! #C

Cl iUH /W!  D
EIOH[AIO!  

RC

WB

RC

P
 
 
 
 

 
 
C
 
 
 
 









]^XbIK!  U!]
K! D HLI OH[AI ]\8  ! IN L MLUH]\H LP!W 
Xc]\QG VMIaVM!! dIK!  D[A]\ I]\W

EUH  M[AO!WNVMIE 
[A ML L]\_CI K8IK! 
	]\O! I /VMHS8IK! D P!aVMH [aKfU!  MS N[AI]\
K! /WU8 ]^X_IK!  Ti]\d\ aVMQ
U! MSN[AI]\ 
 AR M[AO!I ]\?NLTL O!U! UH]\ I  MS


k
i
IK!]\O! d\KJV
LO!UH /aL[/VMWNVMTP!aVMH [aK
ITNL
 /	
 MSTP!aVMH [aKGIaVMd\ /IP! O[Z
QR] S
Xc]\ QR!dDQT O!WI U! W CU!  MSN[AI]\H L OHL! dTV
[/VMUHVMP!W  ]^X_UH  /

]_

 





	


 



K! 8U!QG VM h
I]?S M[A] S WG@HLIOH [AI]\HL
IK! fHLIOH[AI]\k M[A] S /TNL
XcO!H[AI ]\?]^X
HLIOH[AI]\ ._CIKR IK!  \VM]\OHL
LO!PHLI IO!I IK! ]\U%[A]S  
I	KHV\LI]
 /E /hT [Ah![AW 
Xc]\C IK!   
_ / /@VG [A]\ HS I]\HVMW
 / /INVMI DPH /I
[A]\I ]\WP! IaL M `EO!  MSG PhfI K! T   
IVMWNL]GKHV\LI]8S
VMHS8VM8O!H[A]\HSI ]\HVMWP!aVMH[aKGHLIOH[AI]\8V\LC M`E O! MSTPhRIK!  

[aK! MS O!W! d
 ! I

 

$







K! D /d\NL I /
W  NLVMP!W I]RKH VMHSW DV
_]\ aLI
Xc ]\Q- MV\[a KR]^X4Xc]\O!CHLI OH[AI]\H LXb	VMH S8V\[/[A /U!I
_]
P!O[Z
 /
	]\HL M`EO! /IWhE oCVM
UH]\I8 MV\S
VMHS
V\[/[A]\QRU! WNL KRIK!N LIaV\L j

	[/V\ L T S M[A] S D]^X_ /d\KIL]\O!a[A D ]\UH /aVMHS!L
Xc]\Q-IK!   /]\aS /
Xc]\O! C MLO!WIC[A]\QRQRIaL
UH]\IW_CI @e
[A /WW[/VM
PH fOH L MS
I]

 









IK! /  I /d\ /	IK!QR /IN[	VMHSR% ]\d\N[  !IaLa #bao
K!  AR M[AO!I ]\TO!!I[A]\HLNLIaL	 ]^X
W]EV\SfO!! I/oH]\! TLI]\ 
P!I I /d\  /CQT O!W IU! W  //oE ]\! 
O!! I/o VMH S8]\!  T[A]\I]\WIaVMHLYXc  /

]\! 

Q
 
 


	
 
P

 

r

 
 
 
 
Z
 

a
I
 
Q
	
G
	
 

q
e
	
XcO!H[AI ]\HVMWO! ! ICIa VMjE  MLCV\LC !U! O! ICIK!   M`E O! MST]\UH  /aVMH S! LAo% [A]\I]\W4Ld\HVMWNLAoHVMHS
O!!I
	V\[aK
S MLI HVMI ]\G  /d\N LI  /CIaVM d
XcO!H [AI]\ HVMWO! !ICQGV/h
]\ QGV/hT !]\IC  /IO! 8V
 MLO!WIC\ VMWO! 

,. :<563  - -MUM0:

]\UH /aVMHS!L VMH S8 / IO!HL
K! T VMI K!QR /IN[ VMHS8W]\ d\N[O! !I M`EO!  MLCI
P!IL]\O! a[A 
_].E7C
IUH /
Xc]\QGLV\S!SI]\o
I]D[A]\ QRU! W /I K _CIK! T]\!  [Ah! [AW 
]\! E7C
P!I	 MLO! WI
INLd\OHVMaVM I / MS
	]\QRUHVMNL]\H LQGV/h8PH 
LO!P!Ia V\[A I]\o[A ]\QRUHVMN L]\oL QRU! W DW]\d\N[
XcO!H [AI]\HLAo4VMHS
LK!
XcI!d
XcI !d NL V\[/[A]\ QRU! WNLK!  MSOHL!dDV PHVM /WHLK! 
Ld\!  MSD]\	O!H L d\! MS
K!  LK!
XcI /_CK! N[aKT[/VMTLK!
XcI
VMhGVMQR ]\O! ICGPH ]\I KfS  M[A I]\HL

 B):=)  

_ / /@VR d\ /! /aVMWU!O! UH]EL RQRN[A]\U!] [A MLL]\VMH S
K! TU!QG VMh8SNL IH[AI ]\fPH /I
VGSd
QTO! WIU!W A/o	LH [A G QTO! WIU!WN[AVMI]\HL
IK! 8 H[AWOH L]\?]^X
IaVMWL d\HVMWU!] [A M LL ]\G NL
VM
I /d\ /
K!  G
k
 i
Xc]\O!HSf @Sd\ IaVMW	Ld\H VMW	U! ][A MLL!d8 VMU!U! WN[/VMI]\HL
VM R[A ]\QRQR]\!Wh
 /
QTO! WIU!W
 M`EO!  ML	I
_]
P!ILd\ ! M SGL]\O! a[A D ]\UH  /aVMHS!L WC 
L[A]\QRU! W /QT /Ib Xc]\QGVMI
IC /IO!HLC]\! 
E7C
Xc]\IK!  D
k
 i?KHV\LV
	O! / IWhEoE IK!  QTO! WIU!W /4 QRU! W /QT / I MS
P! IL d\! MST  ML O!W I\VMWO! 
CT[Ah![A W WNVMI /H[Ah

'4

UM0:

Xc]\ Q*IK!  RS!VMIaVG[/V\[aK! 
Xc]\KH VMH SW!dG MV\S8]\UH  /aVMI]\HL
K! TW]EV\S8 O!!I NL  M LUH ]\HLP! W 
K!  W]E V\SRO! !IV\ [/[A /U! IaL V
VMHSRIK! D S!VMIaVDQR / QR]\h
P!IL]\ O!a[A ]\UH /aVMH SM_C K!N[aKG [A]\IaVMHL
E7C
IK! V\ S!S  M LL	]^X
IK!  S!VM IaV
I_CWWH /IO!RV$E7 C
P!IS! VMIaVD MLO!WI	 IK!  ! ARI [Ah![AW CO!UH ]\8VDS!VMIaV
IK! 
ZX
 /
VRQGVMIa[aK!! dGV\S! S MLL@IK!  TLI]\ TP! O[Z
Xc]\ Q
HS!LIK! R S!VMIaV
I
K!I ]\
[/V\[a K! 

r
 
 

 
	
	
 
 
 
 
 
	
 
q
e
	
 
	
 
 
 
	
 
	
X

 
Xc]\ GIK! R MLO!WIT[/VM?PH  G /IO!! MS
S!VMIaVfN L!]\IT 
 ARI /HVMWWhfPH  
IK! 8[/V\ [aK!  EoID NLDW]E V\S MS
 /H[A  EoW]EV\SR]\UH /a VMI ]\HLVM  !]\I d\OHVMaVMI / MSRI]R[A]\QRU! W /I c _CIK! R ]\!  
[Ah![AW 



 UM 0: 

P!IS! VMIaV
E7C
_]D]\UH /aVMH S!LAl V
 / M[A / E ML4 I
K!  LI ]\  P!O[Z
V\S! S MLLC VMHSG V$E7C
P!IS!VMIaV
K! C\VMW O! CVMHSTV\S!S  ML L	VM  LI]\ MSD I]V P! O[Z
I]QR /QR]\hc_CK! /
\VMWO! 
 /	VMH S
VM 
OHLK! MS
IK! 
IS]E  ML !]\ I /IO! fVT MLO!WI
W]EV\ S8]\UH  /aVMI]\
S!VMIaVT P!OHLNLC!]\ I f[A]\I /I]\W_CIKf V
IVMWW ]_LW]EV\S8 Ph UHV\LL !dR]^XgLI]\ MLLH[A D IK! 
V\S! S MLL MLVM 
LI]\  MS8@VRLUH  M[ANVMW [A]\I /I
QGV/hPH C NLLO! MSD DIK!  gXc]\WW]_C! d[Ah![AW 
?! A_ LI]\ CHLIOH[AI]\
V\S!S MLL VMP! W CQR /QR]\ h[A /W W
X_I K! 
LI ]\  
P!O[Z
 / NLC! ]\I
XcO!W W
K!  
O! QTPH  /]^X
 /I MLC 8IK!  DP!O[ Z
 /NLC QRU!W /QT / IaVMI]\
Xc]\O!NL M[A]\QRQR  /HS  MS
S /UH /HS /I

;$0)&'0 

 .UM 0: 

VMHSS]E  ML
 M[A /E ML]\! fL]\O!a[A G]\ UH /aVMH S
K! f[A]\I]\WCIaVMHLYXc  /TO!! I
!]\ IT /IO!
 /U! WNV\ [A MLIK! G[AO! /I
i:V\ S!S  MLL# _CK!N[aK
NLDV8! A_FE7C
K! GL]\O!a[A R ]\UH  /a VMH S
P!I
	]\O! I /

 ML O!W I
i ]\d\aVMQ

q
]
 
 
J


 
	
	
 

 
 
 

 
 
V
 
	
 
  	 


 \

Xc]\Q1VDL[/VMWNVM
 / /INVMI ML VLO! UH /aL[/VMWNVM U!] [A MLL]\
K!  
[aK! MSO!W !d  ! I	N LS_C KHVMIC S 
NLLO! 
W /E  /W UHVMaVMWW /WNLQ:VMH S
IaLbXcO! H[A I]\T NL4I]
U!][A MLL ]\
Sh HVMQRN[/VMWWh S /I M[AIH LIOH [AI]\
V\L
QGVMhfHL IOH[AI]\ HL
V\LDUH]ELLP!W 8S /UH /HS !dfO! UH]\ IK! 8 V/\VMWNVMP!W R  ML]\O!a[A ML
VMHS
NLLO! 
XZV\[A D]^X
 /WNVMIE DU! WNV\[A /QR /ICVMH Sf I /
d\E ML VTd\ / ! /aVMWNS MVT]^X
IK! 
IK! 
4d\O! E
UH]\IaL
_]
QGVY]\ R[A]\QRUH]\! /IaL
]^X
IK!  

[aK!  MSO! W! d
 ! I/l@HLIOH[AI]\ 
HS ]_*VMHS
C /]\aS /
 /
mO[Z

]GS /I /QR ! IK! DPH ML IUH]E LLP!W T[A]\
LQTO!WNVMI ]\HL_ / T[/ VM  M SG]\O!I
K!  
KHV\SGIK! 
Xc]\W W]_C!d
UHVMaVMQR / I /aLAl

d\O! aVMI]\f]^X
IK! T
 [aK!  MS O!W!dT ! I/o! AR I /HLE 
Xc]\IK! T 
k
i
QR] S /W4IKHVMI_CV\LU!]\UH]EL MS
PHV\L 

k M[A]S DL n/ ]^X

bC
[aK! MSO!W!d
QTS / / U
 /IhW a
E7C
!O!WW%PhUHV\LL !dT ]^Xb  MLO!WIaLI]R!O! H[AI]\HVMW4  !IaL

 ! I

NL LO! Eo GT ML O!W I_C I UH]\IaL
GG  LAo
4 k:  !I/o
L d\! MSRI  /d\  /C e8O!W IU! W /ca


 
U!UH  /W! MSb

 ! I/o

	]\ I]\W	 aVMHLYXc /  !IVMHS

qRq

P!I

q\q

 


	












Z
 
 
 
q
I
^
 


q
e

 

G
 

 

 

Q
 

q
q
q
e
	
 
Instruction

Src1

Src2

Dest

Opcode

Tag Generator

Reorder Buffer

Instruction Window

Operand/Tag

 Tag

R
E
A
D
Y

Data

Dest

Dest

 Src1

 Tag

V
V
A
A
L
L
I
I
D
D

S
R
C
1

R
E
A
D
Y

B
R
A
N
C
H

I
S
S
U
E
D
1

Src1

Data

O
P
C
O
D
E

S
C
H
E
D
U
L
E
R

D
E
S
T

T
A
G

Src2

Data

I
S
S
U
E
D
2

S
R
C
2

R
E
A
D
Y

 Src2

 Tag

    Register File
         Data

    Register File
 Address Decoder

    To/From
Execution Unit

4d\O! $ E

l	mW][ajRkNVMd\aVMQ6]^X_ IK!  


[aK!  MSO! W! d

 ! I

q
C
 
q
K!  / [aKHVM!d\ M L	KHV/E PH /  /GQG V\S C I]
]\O!I IaLQRU!W  /QT /EIaVM I ]\

IK! U! ]\UH]E L MSRQR] S /WH]^X

IK! D
  *_CK!W [/VMh!d

O!Q

K! 
 KHV\L	]\! Wh$G NLL O! CUH ]\IaL ! ]_HLI MV\S
P! O!I I	 [/VMRLIWWKHV/E  VQGVR Q
]^X
PH M[/VMOHL D]^XbUH ]\ILKH VM!d  b
NL LO! ]^X
HL IOH[AI ]\HLUH /[Ah![AW $a
G!o! ]\! WhMET  LCVM OH L MSGGIK! DS  MLd\
K! D L M[A]\HSGN LI KHVM IC HL I MV\SG]^X
[/VMI ]\NL% IKH VMI4 IK!  e8O!WIU!W /H NL! ]CW]\! d\ /4U! UH /W!  MSV\L ]\! WhVC!]\
K! 	IK! aSQR] S
U!UH /W ! MSRE / aL  ]\8N L[A O! / IWhRV/\VMWNVMP!W 
ZX_8IK!  
XcO!IO! Eo% V
U!UH  /W! MSGe8O!WIU!W /

 
IK! 
VMhG[aKHVM!d\ MLCI]
NLCS ML d\! M S%o!IC [/VM 8PH  OHL MSM_CIK!]\O! IC QGVMj !d

K! T 
[aK! MSO!W !dR  ! IL I]\  MLIK! 
S M[A]S MSGHLIOH[AI]\HL# a
E7CTQGVR QT O!QMb4 H LNS IK! 
E /hT [AW][ajR[Ah![AW EoEIK!  DH LIOH [AI]\8 
[aK!  MSO!W /a
 b	L MVMa[aK! ML
HL IOH[AI]\ M^
HS]_8a
IK! ]\O!d\K?I K! M L R H LI OH[AI ]\HL I]
HS@H S /UH /HS / IDH LIOH[AI]\H LDVMH S@NLLO!  MLIK!  /Q
I]8IK! 
PH 
?IK!  8C /]\aS /
VM 8LI]\ MS
K! G  MLO!WIaL
!O!H[AI]\ HVMW ! Ia L
Xc]\ 8[A]\QRQRII!d
 /
mO[ Z
IK! /Q
I]?I K! 
C /d\NLI / G4 W  
]
V/E]\ NS
LI]\aVMd\ ?[A]\
N[AIaLT MLO!WI! d
Xc]\Q
 	
	
!  #"$ %&o!  /d\NLI /C /H VMQR! dQR M[aKHVM! NLQ:NL	QR U!W /QT /I MS
K! 
 	 
	
W]E]\jE M S8O!UH ]\
 /I 

 :[/VMfPH 
V\L VRL! d\W T D o_CIKf! A_H LIOH [AI]\H LS M[A]S MSfVMHS
K! 
Xc ]\Q*IK! D I]\Uo VMH Sf[A]\QRU!W /I MSTHLIOH [AI]\HL[A]\QRQR II MS
 /I /  MS
Xc]\Q6IK!  DPH]\II]\Q
U!]\d\aVMQ1]\aS /NLQGVM Ia VM! MSTPH]\IKGIK!  C /]\aS / mO[Z
 /VMH STIK! 
^o3_C K!N[aKT LQRU!W
 ML
IK! 
K!   MLO!WIaL d\ /I[A]\QRQR II /S I]IK! C  /d\NLI /
aLI NL LO! VMP! Ia VMI]\TU!] [A MLL
]\WNS ML I
4W ]\!Wh. _CK! /8 IK!  /hR  MV\[a KGIK! PH ]\II]\Q6]^X_ IK! D C /]\aS  / m O[Z
 /! IK OHLU!  ML /!d
IK! %&
m
	]\QRQR]\GI]T PH]\IKfIK! 
LIaVMI D ]^X_I K! DU! ][A  ML L]\
NLCIK! D VMWNS!VMI]\
VMHS8IK! D
 !I/o1_CK!N [aKfK!]\WNS!L IK!  ,%
"% TP!I
Xc]\ MV\[aK@ /Ihf VMHS
[A]\ I]\WNLIK! T \VMWNS!VMI]\ fU! ][A MLL
]^X_QRNL U!  MSN[AI  MST P!aVMH[aK
f[/V\L 
 MV\[aKf[A]\QRUH]\! /IC NL AR U!WNVM!  MSR8IK! 
XcO!H [AI]\ 8]^X
K!  
Xc]\WW]_C !dG
E M[AI ]\ HLA oHVM HS8I K! 
VMa[aK!I M[AIO! aVMW4S  /IaVMW]^X_IK! DWNV/hE]\O! I [A /WWNLNLCS NL[AOH LL MSG 8IK! 
! ARI	KHVMU!I /

q
E
 

Q
s
 

 


	
 
 
^
b
 

 
 


Q

 
 

 

 
 
^
 
 


	 



	

U!WNVM8]^Xb IK!  DC /]\aS /Cm O[Z
LK!]_ LCIK!  
4d\O! # E
 /CVMW]\!d_C IKRIaLQG VY]\LO!P
]E]\
E7CC]_L ]^X
I4[A]\HLN LIaL]^X
[A]\QRUH]\! /IaL
W]\d\N[ IKH VMI LI]\ 	IK!  	\VM]\OH L IaVMdE LAoES!VMIaVVMH SD[A]\I ]\W
P!IaLV\LL] [AN VMI M SW_C IK8 IK! 
S MLI H VMI]\8 /d\NLI /]^X_  MV\ [aK8 HLIOH[AI]\
K!  
S!VMIaVT / I ML
IK!  mVM  O!U% S!VM I M S_C IKR IK!  MLO!WIaL_Xc]\Q1IK!  !O! H[AI]\HVMW ! IaL /E /hT [Ah! [AW EoE VMH SR MV\S
Xc]\CIK!  ! A_JH LIOH[AI]\HL
]\UH /aVMI ]\HLVM D UH /
Xc]\ QR MSTI]
]\P!IaVM8IK! D L]\ O!a[A ]\UH  /aVMH S!L

Src1, Src2 Operands/Tags

Destination Identifiers

Result Tags

Results from FUs

Src1 Identifiers

Src2 Identifiers

Tag Generator

5

CAM
cells

32

32

DATA
 cells

R
E
A
D
Y

B
I
T

W
R
I
T
E

D
R
I
V
E
R

5

CAM
cells

R
E
A
D

D
R
I
V
E
R

4

L
O
O
K
U
P

A
R
R
A
Y
S

5

CAM
cells

R
E
A
D

D
R
I
V
E
R

4

L
O
O
K
U
P

A
R
R
A
Y
S

I/O Bus

Shift Bus

Write Commit to Register File

 /

Cl	 4W]E]\iWNVM8]^Xb IK!  DC /]\aS  /m O[Z
4d\O! $ E
:d\]\O!U ]^X
aV/h
VM R NS /IN[/VMWo AR![A /U! I
WW	IK! ME7C8] _L
[A /WWNL
Xc]\DIK! G %]E]\j O!U
XcIaLAoH MV\[aKGP!W] [aj8LK!
Xc]\O! /I  MLNL [AWN V\L L
 MS8V\LVTP!W] [aj
K! /8IK! D /]\aS / P!O[ Z
 /LK! 
XcIaL
 M[AI E /WhEo\ M V\[a KR / IhRLK! 
S]_CG PhR]\! a
LCVD  MLO! WI/o IK!  P!W] [ajGVMICIK! I]\U
Xc]\O! b
XcIaL Ph
V\[/[A /U!IaL! A_
S MLI HVMI ]\R NS /I
 /aLAo7_CK! W  IK!  P!W] [ajTVMIIK!  PH]\II]\Q-[A]\QRQR IaL IaL MLO!WIaL
W 
I]TI K!  /d\ NLI /

q
G
 
 
	
 
C

	
	
 
 
 
 
 

 
^
 
Z
 


 


	!

q/s

K! T S M[A]S MS
! A_HLIOH[AI]\HLVM TL /I
]^X
IK! 
D VMHS
%&
(' 	

I]f IK! GC / ]\a S / DmO[ Z
VMHS
I]fIK!  GC /d\NLI /D4W TI]f ]\P!IaVM?IK!  GL]\O!a[A R ]\UH /aVMHS! L

 /
V\LL ][A NVMI E 8L MVMa[a K
NL
[/VM  MS@]\ O!I

IK! 8m
I]@S /I /QR !  
IK! G /I MLVM GU!  ML /I
[A]\HSI]\HL _CIKG  MV\[aK8L]\O! a[A ]\UH /aVMHS8 M`EO! MLI/l
K! / D[/VMGPH  DIK! /  UH]EL LP!W  

S MLIHVMI]\8]^X
ZXbIK! D  /d\NLI  /8`EO! MLI ]\8NLC IK! 
 M[A / IWhT AR M[AO! I MSRHLIOH[AI]\o
Xc]\ Td\ /II!df[A]\QRQRII /SG I]8IK! RC  /d\NLI /D4W 
Ia L ML O!W IN L_CVM I !d8@IK!  R mPH  
?L OH[aK?[/V\L EoIK! GS!VMIaV8NL MV\S
IK! / 
ZX
IK! Gm:VMHS?LO!U! U!W MSfI]fIK!  G
Xc]\ Q
VM  QTO!WI U!W HLIa VMH [A M L]^XbIK!  D LVMQR   /d\NLI / U! ML /I GIK!  mo IK!  /G IK!  %]E]\jO!U
XchTIK!  QR]ELIC[AO! /IC HLIaVMH[A 
NS /I 
 aV/hTN LCOHL M SRI]

ZX
IK!   /d\NLI  /NL	IK! S MLIH VMI]\R /d\NLI /	 ]^X_V U! / ]\OHL	 H LIOH[AI]\ oEP! O!ICIaL	  MLO!WI
bao!IK! /fIK!  %
HL IOH[AI]\ 8KHV\ L! ]\I
NLC!]\I  MV\ Sha
Xc]\IKHVMI
!NLK! MSG AR  M[AO!I]\
IK! 
S MLI HVMI]\  / d\NL I / 
NLD  MV\S
Xc]\ Q
IK! 8m-VMHSL /I
I]@IK!  f
K!  8 MV\Sh
P!I
 MV\SG 8IK!N LC[/V\ L $ _]\O! WNSGPH 
XcOHL]\W_CIK8]\IK! / MV\ ShGP! IaLCOHL MS
I]GV/E ]\NSG[A]\
XZVMWNL . a
&% ('CP! Ib
IK!]\O!d\K!]\O!IIK! D
^oIK!NLP! I_CWW% K!  /H[A PH   
Xc / MSTI]RV\LC IK!  
 /d\NLI / KHV\LC VMW MV\ShTPH  / /G[A]\QRQRII /SDI]DIK! 
IK!  M`E O! MLI MS
IK!  WNV\LIHLIa VMH [A  ]^X
ZX
C /d\NLI  /4W  a
I S]E ML! ]\I ARNLIVM hQR]\ Df IK! T mcbaoH IK! /fIK!  
]\UH  /aVMHS@NL MV\S
S  M[AIWh
Xc ]\Q6I K! D ? VMH SGL /IC I]T IK! D 

&% ('	P! IVMW]\!d$_C IKTIK! ]\UH  /aVMH STNL IKHVMI IK! 
K!   MV\L ]\
Xc]\ MV\S !dD IK!  

YOHLIV
Xc]\ QGVMI ]\G I]RS /I  /QR! c_C K! /IK!  /IC NLCd\ /II! dTVT\VMWNSME7C
OHL MLCIK!NLC
P!IS! VMIaV
]\
P!I	
 

 


 

X
 
q
 
V
 
^
 
 
C
 

'
^
 
 
E
 
^
 
^
	
s
	
 
]\UH  /aVMHS! LNLC NS  / IN[/VMW
Xc]\VMWW4IK! 
Xc]\IK! 
WIK!]\O!d\KfIK! TV\ LL][AN VMI E TL MVMa[aK
L]\O! a[A 
K!  [aKHVM!d\ NL% QGV\S  	I]V/E]\NSDVM
Xc]\O! 4 /I ML
 / /H[A D IK! 	 I]\U
]_ LAo\IK! /  	N LVL W d\KI4S
IK! HLIOH [AI]\TRIK!  S M[A]S MS
]\!  ]^X
_ / /TIK! L]\O! a[A ]\UH /aVMHST]^X
 / ]\!  /]\ OHL	QGVMIa [aK
PH /I
#"

HLIOH[AI]\ffIK!  TLVMQR D P!W][aj
P!W][aj%o
_CIKfI K! TS MLI HVM I]\ fNS /I
 /C]^X
 
IK! T!  A_
HLIOH[AI]\ HL VM R LK!
XcI MS
K!NL[A]\ O!WN S@KHVM U!UH  /?V\ LI K! RS MLIHVMI]\@NS / I
 /aL]^X
)
]T O!HS  /aLIaVMHSGIK!NLAo
IK!  QGVMIa[aK!!d]^X_IK! /CL]\O! a[A  ]\UH /aVMHS!LC ][/[AO! 
IK!  m
I]
[A]\HLN S /VM8  AR! VMQR U!W ]^Xb IK!  HLIOH[AI]\8L M`EO!  /H[A  8VTS M[A] S MSRP!W] [ajGLK!]_C8PH  /W]_l




 
	



 



#

! "

!   

 %




$
'&(

 /aLa
! INVMW WhEoMIK! 
Xc]\O!CS MLI HVMI]\TNS  /I
[E3 b	VM  VMWW][/VMI MS
 /I ML4
H[AWOH S!d% )
IK! L]\O! a[A C]\ UH /aVMHSD  M`EO! MLIaL
IK! m@PhDL K!
 /WNS! L
I ]IK! k 

XcI !dI K! /Q
]_
_CK!  /
Xc]\CIK!   / d\K IC]\UH /aVMHS!LVM  D[/ VM MSR]\O! I/o! IK!  La[
]\UH /aVMHSG]^XbIK! 
LIHLIOH[AI]\ a')
[E3b
QGVM Ia[a K! M L_CI KfIK! TS ML I HVMI]\fNS / I
L
VMf]\P]\OHL /]\
IK! CMHSfHLIOH [AI]\
 /]^X
IK! UH]\ IV\LL d\! MS
Xc]\ L]\O!a[A  ]\UH /aVMH S8 M`EO! MLIaLCVM 
R MS%o IK! NLCU!]\P!W /Q-NL MV\LWhGL]\WE MS
K OHLAoIK! 
Ph8UH /QGVM ! / I Wh8 SN LVMP!W !dG IK! 
[/V\L ML
LUH M[A
QGVMIa[aKf /\VMWOH VMI]\@U!] [A MLL 
LI HLI OH[AI]\fNL
#
*
#
)DQGVMIa[aK! MSM_CIKfIK!  TS  MLIH VMI]\@ NS /I
La [
]\UH /aVMHSf]^XgIK! 
 /
]^Xb IK! ]\IK!  /c E
Xc]\CIK! 

EQRWNVM	  MV\L]\! !dRVMWNL]TK! ]\WNS!L
HL IOH[AI]\ HL	G IK! D S M[A] S MSR P! W][aj
]\UH /aVMHS!L]^XbIK!  ]\IK! /C HLI OH[AI]\HLC RIK!  DLVMQR P! W][aj

HL IOH[AI]\ HLVM GS M[A] S MS%oIK!  /DS  MLIH VMI]\@ /d\NLI /aLVM RV\ LLd\! MS
K! /@! A_
%
Xc ]\Q-I K!  4VMd
 / ! /aVMI]\V\ LUH VMIC]^X
IK!  /d\ NLI / /HVMQR!dQR  M[aKH VM!NLQ
K! 
O!!N`EO! 
 /E /hIQT 
P!I[A]\O!I /CH[A /QR /I MS
 / ! / aVMI]\[/ VM8 PH D QRU!W / QT AI MS
4VM d
LQRU! W #E
V\LV
 DHLI MV\S
IK! 
C /]\aS /mO[Z
 / LK!
XcIa L
8E
P!I[A]\O! I / NLCOHL MSGI]Rd\  /!  /aVMI DIK! 
P!I	


q
e
Z
 

V
 
#

 



 

#

 
q
q
#

 


[
 
q
q

 
 
^
V
'
 
	
 
	
s
	
_CV/h!L S M[A] S MSTR VP!W] [aj
P!I[A]\O!I /	PH M[/VMOHL IK! HLIOH [AI]\H L	VM VMW
]^X
K! 
]^X%Xc]\O! 
  8IK! DP! W][ajR  /QGVMG[A]\H LIaVMI
Xc]\VMWWP!W] [aj! L_CK! W ]\ !WhRIK! 
_]R4
mCLC]^X_I K! 
Xc]\O!	 

ETe@
mCL[a KHVM !d\  
IK!  M`EO! MLI  MS  / d\N LI /NL%!]\I  MV\S hEoMIK!  	O!! N`EO! ! %
ZX
'V\LL][ANVMI MS$ _CIKD INL4 LO! U!U!W MS
%
NLVMWNL]8LI]\  MS@fIK! 
]8QR ! QT nA  P! I
I]G IK!  R
	LUH V\[A TOHLVMd\ Eo4 !INVMWWhGIK! RLVMQR 
IK! f MLO!WITNLT!]\I8V/\VMWNVMP!W E oCIK!  %
 /WNS
P!IaLR ]^X
W]_ /
IK! @kVMIaV
K OHLAoc_CK!  /
'8NL
P!I	S! VMIaV
IK! E7 C
Xc ]\QI K! CmfDU!WNV\[A 	]^X
VMO!I]\QGVM IN[/VMW Wh MV\S
 MS
K! NL  MLO!WIaL DVLQRU!W
[A /W WNL	IKHVMI LI]\ DIK! ( %
S MLd\
Xc]\ IK! 
e
'\oH V\LC IK!  /hGS ]
!]\ I!  / MSGVM hR MV\ SRUH]\ IaL
K! D _]\a LI8[/V\L @C /]\aS /Gm O[Z
VMWW IK!  f /d\K IT]\UH  /aVMH S!L8VM 
 /R MV\S] [/[AO!aL._CK!  /
 
Q MV\S!L_ Xc]\Q:IK!  LVMQR C /Ih
Xc /!dI]DIK!  LVMQR C / d\N LI //o [/VMOHL!d
K!  / NL! ]$_]\aLI
V8L /UH VMaVMI R /Ihf@IK! 
 /d\NLI /HLIaVMH [A R NLVMWW] [/VMI MS
[/V\L  ._C I RV\ L MV\[aK
S MLI HVMI ]\
moVMHS?I
]\!Wh
d\ /IaL
_CII  /?I]@]\ H[A G? IaLDW
Xc /IQR 
K! /IK! GHLIOH [AI]\H L
!NLK
 TVMW]\!dW_C IK@IK! T MLO!WIaL
 AR M[AO!I ]\o%I K! R !O!H [AI ]\HVMW	 !IaL  /IO! fIK!  
K!  ML 




VM  IK! /ROHL M STPh
IK!  C /]\a S / mO[ Z
 /I]
L MVMa[aK
Xc]\QG VMIa[aK! !d /I MLAo VMH STO!U% S!VMI IK! /Q
GI K! #_CI  U!KHV\L 

 





L#J1&S )4

[A /WWNLDIKH VMIRVM fOHL MS?I]?LI]\ 8IK! 
 /WNS?N LT[A]\QRUH ]EL MS?]^X
 e
IK! 
K! fk

E3b
K!  8S M[A] S /RL /UH VMaVMI ML
]\O!I
 MV\[aK?H LIOH[AI]\*a4 d\ O! WE
 /
Xc]\
S MLI HVMI]\ ?NS /I
XcI!U!O! IaL]^XgIK! 
P!I HLIOH[AI]\ HLVMHS@U! WNV\[A MLIK!  /Q*]\@IK!  TLK! 
Xc ]\Q*IK! . E7C
IK! ML 
P!IaL
e-[A / WWNL
K! /hDd\ /I	LK!
XcI  MS I] IK!  CI]\U
Xc]\O! LW]\IaL _CK!  /TVMWWIK!  S  MLIHVMI]\DNS  /I
 /aL
IK! V\S!S I]\HVMW
Xc]\V _CIaVMP!W e6[A /WW! VMHS
Xc]\O!/oEI KOHL	V/E]\NS !dIK! C!  / MS
XcI	S] _CRPh
LK!

q
P
V
s
	
 
I
 
^
 
'
s
	

 
	
 

 
 
 
^

 
 
 
 
 

s

 
 
s
	
 

 /aL
[A /WWE]\ TIK! CLNS [A]\I]\WNL4 IK!  LK! 
W]\d\N[IKHVMId\]E MLS_CIK
IK! NS / I
XcI!d ]^X
K!  i	
I
"$# 
I
IK!  /\VMWOHVMI]\R U!KHV\L 
MSNL [aKHVMd\ ML	IK! QGVMIa[aKTW! ML a
VMHSG[aKHVMd\  ML
XbSO! ! d

]^XgIK! ,%
VMWNL]GQGVMjE MLCOHL  
VMWNS! VMI]\@ ! I/o VMHS@ SNLVMWW]_ LVM hf MV\S
"% TLd\H VMWb Xc]\Q
Xc]\Q-\VMWN ST / I  M L

IK! 

PRE

CAM

CAM

CAM

CAM

CAM

4d\O! $E

El	 k
 

 /WNS

m]\IK
IK! Rm
_]8NS  / IN[/VMWk 

LLK!]_C
 /WNS! L
Co4IK! / RVM TI
4d\O! .E

_CK!W 
QGVMIa[aKRVMdEVMH LIIK!  
 /WNS!LL I]\  IK! L VMQR  \VM WO!  Eo P!O! IC]\!  NLOH L MSTI]
%&
Xc / MS]ME /	VL!d\W 
IK! C]\IK! /bXc]\	I K! 
UH]\IC[A /WWNL1 _ / U!  
_ ]DL /UHVMaVMI cG


%& 
UH]\ Ie*[A /WW
WIK!]\O!d\KRI K!  LI]\aVMd\  W]\d\ N[C NLS O! U!WN[/VMI MSD RIK!  I
_]D [A /WW! E /aL]\oEIK! 
QGVY]\ [A]\I P!O!I]\I]D IK!  L n/ ]^XbIK! [A /WWHNL IK!  ]\O!I!d
[A]\! d\ MLI]\8VMHST! ]\IIK!  LI]\aVMd\ 
 /W /QR /I
K! RVM MV
Xc]\D PH]\IK@E /aL]\HL [/VMQR D]\O! IDVMWQR ]ELI IK! RLVMQR E o% P!O! I IK! T IK! T K! /d\KI
KHV\ LTPH  / /S MLd\! MS VM]\O!HS
K!  8 / I GC /]\aS /RmO[ Z
PH M[/VMQR  RIK! f S M [AN S!d
 /
XZV\[AI ]\
IK! fS!VMIa V
[A /W W
 L !d@I K! DQ
UH]\If e
[A /WW_]\O!WNS?KH V/E 8 [/VMOHL MS?O! ! /E /K! /d\KIaL
VMHS
_CV\LIaVMd\ fSO! GI ]fIK! 8V\S!SI ]\HVMWC]\O!I! d@ M`EO! MS@I]@VMWd\?IK!  8[A /WWNLAoS_C K!W R IK!  MG
UH]\I
I]fIK! 
[A /WWNL
?UH /
Xc M[A IWh
P ]\OHLWhE oIK! 8V\ S!S I]\H VMWLI]\aVMd\ @VMH SLK! 
XcI
W]\d\N[G V\S! S!L
UH]_ / C  M`E O!  /QR / I/ o\P! O!ICIK!  H[A /QR / INLCQR!]\

q
Q
 

 



 
 
 
 

 



 
	
Q
	
 
 
 
 
	
	

I
 
 
q/r

1/1=

,.'



 /WNSRVM #GR%]E ]\j O!Uf aV/h! LAo!]\!  
Xc]\C  MV\[aK8L]\O! a[A ]\UH /aVMHS
_CIKG M V\[a K8k

W]\!d
]^X
IK!  
IK! 8LVMQR R  /d\NLI /D
HLIaVMH[A MLD ]^X
[/V\ L G ]^X
QTO! WIU!W 
Xc]\O!T ! A _1 HLI OH[A I ]\HL
IK! 
 //o!IK! D ! A _JH LIOH[AI]\8 LK!]\O! WNS8]\! WhRd\ /IIK!  DQR]E LI[AO! / IS!VMIaV
C /]\aS /mO[Z
K! 
XcO!H[AI]\ G]^XbIK!  D%]E ]\j O!Uf a V/hT NL I]RS hH VMQRN[/VMWWh
S /I /QR! CIK!  QR]ELIC[AO! /I HLIaVMH[A 
VMHS
VMWW ]_:]\!W h@I KHVM I
S!VMIaVGI ]fPH T MV\S
Xc]\Q
IK! GC /]\aS  /Dm O[Z
 /
WNL]!o4
XgIK! R  /d\NLI /
Xc]\ QIK!  % ]E]\jO!U
HL Ia VMH[A N L! ]\I4U!  ML  / I DIK! CC /]\aS / mO[ Z
 //oMIK!  /DIK! C [A]\ I]\WLd\H VMWNL
 aV/hM_CW W4PH 
OHL MS8I ]GIO!f ]\fIK! 
QTO! WIU!W AR  AaL_Xc]\Q*IK! TC  /d\NLI /4 W 
K!  T%]E]\jO!U
Xc]\QGVMI ]\GNL
o_C K!W DIK!  DIQR! dD
 aV/h!LVM  
SNL [AOHLL MSG 8QR]\  DS /IaVMW@
E M[AI]\W G
V/\VM WNVMP!W  G 4 d\O!  # G

&, 

S )4

Xc]\DV. E7CR /Ihf mcbCV\ LLd\! MS
P!IaL
P!IO! !N`E O! 
 /WNSf LI]\ ML IK!  
K! T4 


 VM D [A]\QRUHVM MS
 MV\[aKR /d\N LI / HL IaVM H[A   RI K! 
I]
_CIKRIK!  
K!  ML 
[A /WWNL
 e




!O!H[AI]\ HVMW4 ! IaLAoHVMH S8IK! D [A]\ MLUH]\HS! dRQGVMIa[aKRW! ML 8IK! T e
 / IO! ! M S
Xc ]\Q6IK! 
[A /WWNL
VM GIK! /?SE /@]\ I]fIK!  
IK! 
W!  ML
K!  /h
bVM 8L /I O!U





 /HVMP!W IK! UH ]\IaL VMHSM_CI 
_]\a SGW ! ML	]^ X_I K! DS! VMIa VT[A  /W WNLXbIK! ]\O! d\K8IK!  #_CI DS E  /aLAo I]
K!  CNS /IN[/VMW![A /WWNL4 OHL MSD TIK!  k 

C3b
 /WN S.a4d\O!  G
IK! C MLO!WI	 I]IK!  kVM IaV
 /WNS
QGVM jE  O!U8IK! 
4
 /WNS
K! D]\! WhGS
 / /H[A NLCIKH VMIIK!  
i	 
[A /WWS ]E ML!]\ IQGVMjE OHL 
,%
IK! QGVMIa[aKRU! KHV\L 
Xc]\ QGVMI]\ 8SO! !d
"% P! IC 
]^X_I K! 

 
 
 
 
 
C
 
q
 
q
C
 

s
	
a
s
	
s
 
 
a
 



 
 







a

 
q
 


 
Z
 
LM' '

S )4

C7]

E7E


$"
IK! HLIOH [AI]\RVMHSRVMWNL] IK!  "$# 
K!  kVMIa V
 /WN SDK!]\WNS!L	IK! E7C
P!I MLO! WI	 ]^X
LVMQR  O!!N`E O! (%
P!I/oHV
I]\IaVMW4]^X
'IKHVMId\ /IaLCLK!
! INVMWWhEo VT[A]\UhR]^X_IK! 
P!IaL
XcI MS
 /WN S
I]GIK! G4
IK! R MLO!WI
 /WNSf NLU! WNV\[A MS@@IK!  RW]_  /
P!IaL]^X
IK! GS! VMIaV
 /WNS
K! /
IK! @!O!H[AI ]\HVMW  !I/oC IK! %

$"
IK! "$ # 
VM E ML
Xc ]\Q
_CII /VMHS
'f[A]\U hNLT]ME  /
IK! 
"$ # 

$"
P!IRd\ /IaLGL  /I
K! @ LK!
XcIG !U!O!IaLT]^X
P!IaLT]^X
IK! 
Xc]\O! GI]\ U!QR ]ELIR /I ML
VMWW	 H LIOH [AI]\H LKHV/E  TIK!  /"$ # 

$"
P!ID ML /IDVMI
VM G[A ]\!! M [AI MS8I]8d\ ]\O!H S%o	L]8IKHVMI
"$# 
IK! @k 

IK! @L IaVMI
kO! !d
IK! f M V\SU! KHV\ L E oCIK! 
W!  ML
Xc]\Q
 /WNSVM 

IK! _C I  U!KH V\L EoIK! 
 /WNS% o _CK! W S O! !d
W!  ML	 ]^Xb IK!  kVMIaV
IK! 
SE / T]\I]


W ! ML_Xc]\Q6IK! D4
 /WNSRVM 
SE /R]\I]
IK! 
W!  ML








 



 




K! D VMW NS!VMI ]\G ! ICN LIK!  ]\! WhG[A]\QRQR ]\RW]\d\N[PH  /I
 /VMHS
_ / /R IK!  DC /]\aS /mO[ Z
I LI]\ MLIK! ,%
Xc]\ MV\[aK@ /Ih8fIK! G 
 :HSN[/VMI!d
IK! R HL IOH[AI ]\D^
HS]_
"%T P!I
%]E ]\j O!U8 aV/hEoELQRWNVM4I]IK! ]\!  
IK! /\VMW NS Ih
 //o
IK! C  /]\ aS  /mO[Z
\VMW NSIh
/!]\
NLOHL MSR K! /  I]T \VMWN S!VMI IK!  / I ML_ Xc]\WW]_C !dT V
PHV\SRP!aVMH[aK
]^XbIK! HLIOH [AI]\RHLNS VT S M[A] S MSR P!W] [aj%oEIK!  D[A]\ 
]TV\[/[A]\O!IgXc]\CIK! \ VMW NSIh
LUH]\HS!d ,%
"% P! I
Xc]\C MV\[aKT HLIOH[AI]\RNL LK! 
XcI MSTI]IK! 	 
mC
[A /WWNL a4 d\O!  E
G b
VM@ \VMWNS@Ld\HVMW	NL
HS ]_1VMHS
NL S /I M[AI  MS%&%IK!  RH LIOH [AI]\ ^
K! TPHV\S@P!aVMH[aK
UHV\LL MS
]\I]8I K! R VMW NS!VMI ]\? !I
K!  R	
 C
[A /WW	OH L MLIK!NL
Xc]\QGVMI]\
VMHS
IK! CV\ LL /I]\
K! NL4  MLO!WIaL 
MSNL [aKHVMd\ CQR ]S MLDIK! % ]E ]\jO! URaV/h
IK!  [aKHVMd\ 
L / IaL4O!U
,%
]^X
IK!  
"% L d\HVMWHT VMW W! IK! C  / I MLVMPH ]ME C Ica
HLIOH[AI]\ HL
Xc]\WW]_C! dIK!  P! aVMH[aK

Ld\ HVMW%NL	L / II]DIK!  !  /Ia[aKR !I
IK! /Q-VM  \VMWNS! VMI MS% oVMH SRV
W W!]^X
]_b
IK!  U!]\d\aVMQ


	

)
#
 


 

s
	

 
^

)
#
 
 

)
#
 

)
#
 
 
 





 








 




 







 
 



 
	
 

 
	
 
 
 
 

 
REORDER

BUFFER

LOOKUP (i)

VAL_BIT

VAL_CNTRL

INSTRUCTION

WINDOW

ROUTING CHANNEL

G!l	 VMWNS!VMI]\ G ! I
4d\O!  $E
NL M[A /E MS
QR]\ HLIOH [AI]\HL O! IW!IK!  [A]\ M[AI i
VMh
Xc]\Q:IK! 
I]DLI]\UR IK!  
Xc /Ia [aK!!d]^ X
XcI/o  _CK!N[aK._CWW

 J[/VM8LIWW LK! 
K! DS M[A] S ! dTVMWNL]R LI]\UH LP!O! IC IK! 
	]\I ]\W4aVMHLYXc  / !I
K!  S M[A] S  /_CWW

 
[/VMOHL  IK! QR NLU! M SN [AI MS
 /I / IK! 
HL IOH[AI]\ HL	 GIK!  S M[A] S /CI]
LOH[aKD IKHVMI	O! HS  /IK!  ML [Aa[AO! QGLIaVMH[A MLAo/IK! C LK!
KHV/E I]PH S  ML d\! MS
XcI\VMWNSD ! U!O! IaL
Xc]\IK! 
L /Ig XZVMWNL D
HL IOH[AI]\ HL VM  
QRNLU! MSN[A I MS
XcI MSR8V\L \VMWNSRHLIOH [AI]\HL
LK!
IK! /h8VM 
K! D i?NLQRU! W /QT / I  /S
PhGLK!
XcI!d
I8LQRU! WhTV\L VM8\ VMWNSTH LIOH[AI]\

 / /H[A  RK!  /d\K I]^X_IK! [A /WWNL	RIK!  D C /]\aS /C mO[ Z
m M[/VMOHL ]^XbIK!  DS
IK! 
I]
 /C]_
[A]\  MLUH]\HS !d@[A /WWN L 
HS ]_:]_o	 V8S M[AI]\! RI]8]\!  G[A]\! ! M[AI]\
IK! G HLIOH[AI]\^
_ / /TIK! , %
 /	NL!]\IUH ]ELLP!W 
mC[A /WW!I]DIK! C  /]\ aS  /Cm O[Z
Xc ]\Q:IK! 	 
PH /I
"% L d\HVMW
]@V/E]\NS@ I MS ]\OHLDKHVMHS
]\O!I !d! o	IK! GVMO!I]
]\O! I /RV/\VMWNVMP! W ._CIK?ef  
[/VM?PH 
_ / /GIK!  /Q
OHL MSR I]Td\ /! /aVM I  IK!  ]\O!I !dR[aKH VM!!  /W%PH  /I

C
q
 
 
 
 
 
 
 
 
Z
 
	

q
E

 
 



 







C7C

W W
HS]_
IK! HLIOH [AI]\
NLLI]\ MSD
HLIOH[AI]\ 
Xc]\ QGVMI ]\D UH /Ia VM !!d I]DVM
H[A WOHS!d IaL]\ U%[A ]S  EoH L]\ O!a [A ]\UH  /aVMH S!LAo! S  MLIH VMI]\RIaVMd! o MV\ShTP! I/oE NLLO!  MST P!I/o VMH STIK!  
K OHLAoO! !WjE  IK! T S 
[A]\HSI ]\HVMWP!a VMH [aKfH LIOH [AI]\
iJ @[/V\L 
Xc / MSfL[aK!  MSO!W!d
]^X
NL4 M`E O! MSI]DV\[AIOHVMWWhD MV\SDIK! 
QR / IK!]SD  /QRU!W]M hE M S 
IK! k 

o7_CK! /  VM! ]\IK! /	 MV\S
]\UH /aVMHS!L
XcI  /DVM
NL LO! Eo4K! /  
IK! R]\U%[A]S  GVMH S@]\UH  /aVMH S!LD]^X
VM
HLIOH[AI]\
VM TN LLO! MS
QGVY ]\
E 
O!U@]^X
HS ]_:NLQGV\S 
K!  RHLIOH[AI]\D^
S M[AIWhG I]8IK! T! O!H[AI ]\HVMW  !I
 /WNS!LCV\ LLK!]_C8 84 d\O!  # E

SRC1

CONTROL

SRC1 DATA

INSTRUCTION

SRC2 DATA

SRC2

SCHEDULER

CONTROL

4350
lambda

10,945  lambda
l	HLIOH [AI]\M^
4d\O! $E

HS ]_

HS ]_:VMWNL]f[A]\H LNLIaL]^X
E7CG]_L
 //o IK!  RH LIOH[AI]\^
\OHLIDW  jE D IK! RC /]\aS / mO[ Z
]^X
[A]\ MLUH ]\H S /H[A 
]\!  
I]
!  
LI]\!d? ]\! f H LIOH[AI]\
[/VMUHVMP!W  f]^X
]_
W]\d\N[\oC MV\[aK
HS ]_1VMH S@IK! RC /]\aS /DmO[ Z
_ / /fIK! R HLIOH [AI]\ ^
NLQGVM IaVM! MS8 PH / I
 /DL]8IKH VMIDIK! 
U!]\d\aVMQ6]\aS /NLQGVM Ia VM ! MS
RIK!  D
  VMH SRIK!   \VMWNS! VMI]\R U!] [A MLLC ]\8VD PHV\SR P! aVMH[aK
NLCLQRU!W
 MS

 
 


^
a
b
 

s

V
 


 
s
 
 
s
 
	
	

 








C7E

IK! W]\ d\N[	E ]\WE MS$_C I! dIK!  La[
K! 
Ea[
	]\I]\WO!!I /H [A]\QR UHV\LL MLQR ]E LI4]^X
]\UH /aVMHS_CI K@IK!  T ML O!W IVM !d
IK! T! O!H [AI]\HVMW  !IaLAo4VMHSfd\ /!  /aVMI!d8[A]\I]\W
Xc]\ Q
Ld\H VMWNLCI ]
PH DOHL MSR PhRIK! D HL IOH[AI]\f
 [aK! MS O!W /
VM]\OHLC[A]\I]\WP!IaLCV\LL][ANVMI MSM_CIK
 /WNS% o!VMHS8VM O! U%S! VMI MSG /E  /hR[Ah![AW 
 MV\[aKG HLI OH[A I ]\8 VM  DL I]\  M SRGIK!NL

CAM BLOCK

RDY BLOCK

PRED BLOCK ISS1 BLOCK

BYPASS

WRITE

4
entries

28
entries

4d\O!  $E

el 
Ea[

	]\I]\WO! !I

LO!P
	SNS MS@I]GP!W][aj!L
XcO!IK!  /
NL
W]\d\ N[
]f ARU!WNVM @IK! R]\UH /a VMI ]\o  MV\[aK
]_:]^X
d\KIIK! /h
IK! / gXcO! H[AI]\ 8V\LLK! ]_C8 84 d\O! $E
_CIKG HVMQR  MLH SN [/VMI ! d
XcICI]
!]\Q-W 
P!W][aj% oEi  MSN[AI]\DP! W] [aj%oELLO! MSD P!W] [aj%oEm h UHV\LL P!W] [aj
VM El	VM Q
P!W][aj%oEC MV\Sh
VMHS
^@I 
QGV\ S 
P!W][aj
LO!P
HVMQR!d8[A]\ E  /I]\fVMHS
IK! RLVMQR  
Xc]\WW]_
I]
VMII  /QRU!IKHV\LPH  / / 

SNL]\R GI K!  WNV/ hE]\ O!I/ o!I ]TVMWW ]_
PH /II /CO! HS /aLIaVMH S!dT ]^X_IK!  DS  MLd\



 


q
q
 

 
 
q
 
e
 
 
	
 
CG

;$'5

()1-/

PEo!IK! QGVY]\UH ]\I]\ 8]^Xb IK!  D[/VMQ1P!W] [ajRNL[A]\QR UH]EL MSR]^XbI K! 
LCL / /R 84d\ O!  #E
%
[A /WW!d\ /!  /aVMI MLIK! LK!
I]D LI]\ IK! 
e6[A / WWNL4OHL MS
K!  i	 
XcI[A]\I]\W% Ld\HVMWNL

 %
Ao%V\L_ /W WV\L [aKH VMd\ MLCO!UG IK!  QGVMIa[aKRW!  MLS O! ! d
 /\VMWOH VMI]\
Xc]\IK! 

DUP_RDY

PRE

CAM

CAM

CAM

CAM

CAM

4d\ O! $E

PEl VMQ6P! W][aj

 /d\NLI /NS / I
Xc]\
 /
.a
! A_1HLI OH[AI ]\HLDVM GS M[A]S MS% oIK!  
K! /
%&

IK!  C /]\aS /mO[ Z
]\UH /aVMHSbNLCL / II]
ZXbIK! ]\ UH /aVMHS
 /CVMH SRIK!  DC /d\NLI /C4 W 
IK! L a[
(%
%
IK! 
NLV/\VM WNVMP!W G  / IK! /	]\! ]^XbIK! /QfoE INL	 MV\SGVMHSGLK!
 /WNSTRIK! 
XcI MSTI]
HL IOH[AI]\ M^
HS]_
ZXbIK!  ]\UH  /aVMH SGNL! ]\IhE  /IC MV\ShE o IK!  O! !N`E O!  
V\LL][ANVMI MSM_CIK



$"
"$# 
INL MV\S
K! NL IaVMd8NLIK! /
P!I
 //o4VMW]\! dM_CIK@IK!  
IK! TC /]\aS /D mO[Z
Xc ]\Q
%
LK!
XcI MSf ?V\LDI K! 
'R]^X
IK! RHLIOH [AI]\@
IK! RHLIOH[AI]\
^
HS ]_
K! /
IK! 

!NL K8IK! /C AR M[AO!I]\ R8IK!  D!O! H [AI]\HVMW4   !IaLAo!IK!  /S MLIHVMI]\G IaVMdELVM  
HL I OH[AI]\ HL
QGVM Ia[a K! M S8VMdEVMHLI IK! ML   %
(%
%
I]RO!U% S!VMI R IK! T [A]\  MLUH ]\H S!d
 /WNS! LfIK! 

HS]_
HL IOH[AI]\ M^
P!IaLD]^ X
 /TLKH VM MLD IK!  RW]_ /
mO[ Z
Xc]\  Eo	I K! 
IK! 8C /]\aS  /

SNL [AOHLL MS@PH 
L


Xc]\Q
1U!]\P! W /Q
IKHVMI
 MLO!WI MS
KHV/!dfVfL /UH VMaVMI RNS / IIh
]^X
IK! GS!VMIa V
 /WNSf HLI  MV\ S
_CV\L! ]\IV/\VMWNVMP!W 
IK!NLK_CV\LI KHVM IC G[/V\L  IK!  ]\UH /aVMH S._CV\L  MV\S hEoIK!  D[A]\ MLUH ]\HS !d


%
I]@ IK!  fHLI OH[A I ]\
L]\QR 
HS]_6VMHS?IK! 
 /WNS
 M[A /E MS
YO!! j?\VMWO!  
K!NL

HS ]_F_CV\LDPH /!dfO! U%S!VMI MS
QGVMIa [aK!  ML_C K!W RIK! 8HLIOH [AI]\
^
 ML O!W I MS@ 
O!!! M[A MLLVM h

 
s
'
 
'
 
 
^


q
 

 

)
#
 
 
 
^

'

 
s

 
^
'

 
QGVMIa[aKG /\VMWOH VMI]\f
!  TL]\WO!I]\D_CV\ LI]RIO! f]
VMHS@[A]\QRU!W N[/VMI  MST ]\IK! /NLLO! ML
IK! 
&% ('P! I IaL /W
&%(' P!I# _CV\LD L /IP!O! IDV\LD IK! 
XgNLL /ID V\L
VG MLO!WI]^X
[/V\L  RIK! 


I]fV8UH]\I /INVMW[A]\QTP!H VMI]\HVMW	W]E ]\U
IK! RQG VMIa[aKo4IK!NLD [A]\ O!WN S
LUH M[A
WNL]!oIK!NL
IO!
HVMWLIaVMd\ ML]^X_IK! 
U!]\P!W  /Q
S MLd\ oH VMH S8I_CV\LCQR UH]ELLP!W  VMh _CV/h
_CV\L MVM Wn/  MST 8I K! 
 &%('L d\H VMW
WNVMI /b4I]IK!  i	?[A /WW
P!W] [ajMaSNL[AOH LL MS
I]D ]\O!I IK! 
Xc]\Q1IK!  C MV\S h
,%
"#%
kC [A /WWH NL OHL MS
]
d\ /IVM]\O!HSGIK!NLA oI K! D k i
K! /GIK! 
'DNL MV\S
Xc]\Q1IK! 

$"
IK!  "$# 
mo\]\! [A ]\U h
]^X
P!I	NL U!WNV\[A MS

IK!  kC
mC[A /WWTIK! C  MV\ ShTmW][aj
kC 1[A /WW
VMHS
kC 1P!INL
IK! Rk  i
ZX
I NLU! WNV\[A MS8@IK! Tk  i
VGSO!U!W N[/VMI T[A ]\U h8]^X
o4IK! T i	 J[A /WW	SN L[a KHVM d\ ML IK!  Ge
W]\d\ N[
VMHS
SNLVMWW]_ LVM hfQGVMIa[aKo _CK!W 
INL
o IVMW W]_LCIK!  Te
]^X
[A /WW4VMWNL]RQGVMjE MLOHL 
K!  Ti	 
QGVMIa[aKGU!] [A MLL I]G[A]\ I O! 
IK! , %
"%  
Xc]\ QGVMI ]\T V/\VM WNVMP!W 
Xc]\Q:IK!  VMWNS! VMI]\R  !I	 d\K I	 ! AR I	 I]DIVMH STIO!HL	]
IK!  / \VMWOHVMI ]\G 8[/ V\L  DI K!  HLIOH [AI]\R NL \VMWNS

. '4

()1-/

src1 lookahead ready

L
A
T
C
H

L
A
T
C
H

L
A
T
C
H

L
A
T
C
H

LAT_

CNTRL

mi

L
A
T
C
H

L
A
T
C
H

L
A
T
C
H

L
A
T
C
H

RDY_BIT

WR_RDY

mf

4d\O! #E

Ql C MV\S hRP!W][aj

NLNLLO! MS%o4 IK!  RO!! N`E O! 
IaLDS  MLIHVMI]\
V\LL][ANVMI MS
_C IK
K! /?VM? HL IOH[AI ]\ 


efVMIa[aK!!dRNL [/VM MS8]\O!I
	]\I]\W	O! !I
P!W][aj8]^Xg IK!  R
Ea[
 / d\NLI / NL L /II ]GIK! RVMQ
]\UH /aVMHS!LM _CWW PH @O! U%S!VMI MSJVMIGIK!  @ /H S

IK! 
]^X
]\O!IM_CK! N[aK
HS
aLI8 U!KHV\ L @I]
IK! 
K! D ML O!W I]^X_IK! DQGVM Ia[aKGNLVMWNL]ROH L MSG I]RL /IIK!  (&% ('C P!I]^X
[Ah![AW  
IK! DQGVMIa[aK!!d

C
s
 
Z
 

[

 
 
^

)
#
 
 
 
q

X
 
]
 
Z
 

 
^
q
 


 
C7e
IK! 


Xc]\VMRNLLO!  Ph
IKHVMIIK!  HLIOH[AI]\HL	 d\ /IC[A]\HLNS  / MS
HL I OH[AI]\ HLVMK! MV\ST ]^XbI QR Eo\L]
Xc]\QGVMI]\
 ARUH M[AI !dIK!   MLO!WI	I]D PH P hUHV\LL MS b

. a
K!  LVMQR  QGVMIa[aK

NLVMWNL]DOHL MS
(%
%
 /WNS! L_CIKTIK!   MLO!WIaL
IK!  ! ARI	[Ah![AW I] O!U%S! VMI IK!  
IK!  L M[A]\HS
Xc]\Q
U!KHV\L ]^ X
KOHLAo IK!  / DNLVM@ ]ME /WNVMU8PH  /I
IK! T!O!H[AI ]\HVM W	  ! IaL
_ / /fIK!  
]\WNS@VMHSf!  A_HLIaVMH[A ML
Q3b	I]
L /IaL]^XbWNVMIa[aK!  MLc a4d\O! $E
_]
]^Xb IK! QGVM Ia[aKT 
Xc]\ QGVMI]\ 
K!NL M`E O! MLIK!  OHL ]^XbI
_]THLIa VMH[A ML
LI ]\  IK!  I
b	Ld\H VMWNL[A]\QR! d
K! 
VMQ*P!W][ajGVM DWNVMIa[aK! MSGPh
Xc]\ Q6IK!  
! INVMWQG VMIa[aK
1a
Xc]\O!( 
aLIU! KHV\L Eo4VMHSfIK!  /@WNVMIa[aK!  MSGP h8IK!  T L M[A]\HSfL /I]^ X
aLID L /I ]^XgWN VMIa[aK! MLSO!  !dGIK!  
IK! 
 M`EO!\ VMW /ICI]RVTLK!
WNVMIa[a K! MLCSO! !dTIK! DI K! aSR U!KHV\L .a
K!  ]\O!IU!O!I
XcI /d\ NLI /C]\UH /aVMI]\ 
]^Xb IK! ML WN VMIa[a K! M L
HVMWQGVMIa[aK
b4UH V\LLCIK!]\O! d\Kf VMWW%IK! ]\IK! /C P!W][aj! L	I]T  MV\[aKRIK!  #_CI 
WNVMIa[aK!  ML d\ /! /aVMI MLIK! 
_]fL /IaLD]^X
IK! RI
_ / /
C
[A /WW	PH  /I
SE / P!W][aj
K! G	
%#&%
	&%( '
WNVMIa[a KRL d\HVMWNL_Xc ]\PH]\ IK
IVMWNL ]Dd\ /! /aVMI ML	 IK!  
Xc]\QGVMI]\
IK!]\O!d\K

"

NLDIK!  GIQR ! dfSNVMd\aVMQ
VM
4d\O! WE
IK! G!U! O!IaL
L d\H VMWNLTV\L
1dEVMI E o_C IK? IK!  
 / U! M L /IaVMI ]\G ]^XbI K! \VM  ]\OHLC /E /IaL	][/[AO!! dTR IK!  DC MV\S hRP!W][aj

PHASE  1

PHASE  2

PHASE  3

PHASE  4

SRC1 TAG

MI

SRC1 LOOKAHEAD
                     READY

SRC1 READY

shifted

written

MF

mf = old mi

mf = mi

4d\O! $ E

lC  MV\ShRP! W] [ajR QR !d

kNVMd\aVMQ

&% ('	[A]\I]\W!P! I VMH S
IK! S!VMIaV
NLV\ VMNVMI]\T]^X
mC?[A /W WK!]\WN S!L IK!  
K! kC

 MSVMaVM!d\ /QR /I
[A /WWoAP!O!I1_C IK!]C MV\SUH]\ Ia LKa V\LIK!  P!I4 S]E ML% !]\I4 d\ /INLLO!  MS b%VMHSV QR]S 

 

 
 
 
 


b
 



a

 
 
 
 
 
r
 
 
r

C3P
kC -[A /WW	[A]\I]\WNL
Xc]\DQR]\ 
XcI !dGW ]\d\ N[Wa
]^X
K!  .^
]\O! I!dfLUHV\[A 7b
I]8VM WW] _
IK!  RLK!
( &%('CP! I
ICL /IaLCIK! P! ICG IK!  L M[A]\H SGU! KH V\L 
IK! #_C I!d
]^XbIK! 
S /UH /HS !d
O!UH]\
	&%( 'CLd\ HVMW
%#&%
IK! L IaVMI  ]^XbIK!   "
 C@[A /WW
Xc]\Q1IK!  	 


][/[AO!aL_CK! /fVM fHL I OH[A I]\G d\ /IaLCNLLO! MS8 V\LLO! QR!dT IKHVMI]\ ! D]^X_IaLC]\UH  /aVMH SW_CWW% M[A /E 
IK! 
PhUHV\LL MSf \VM WO! 
Xc ]\Q
VM@ MVMW /4 k
]\UH /aVMI]\ o4P! O!IIK! 
Ph UHV\ LL!dG!  /E /IaVMjE ML
U!WNV\ [A 8 PH M[/ VMOHL 8]^X
IK! NLT [/V\ L EoIK! 
VMHSIK! 8NLLO!  8KH V\LTI]@PH  f!  /dEVMI MS
[/V\[aK! GQR NLL
	&%( '_CIKTIK! 
%#&%
&% ('	P! I
IK! 
kCJ[A /W W!  ML /IaL	IK! 
K!  D
]^X


"
&%('CP!ICNLL /ICI]
IK! DHLIOH [AI]\f
[aK!  MSO! W /V\LC I!  / MS!L IK!NL 
Xc]\QGVMI]\RPH 
Xc]\ 
 
IK! DL IaVMI]^XbIK!  DL  M[A ]\H SGU!KHV\L  .a_CK! /GIK!  D L[aK!  MSO!W!dDPH  /d\ HLXb

N# 4-7  0

()1-/

PRED_BIT

PRED_

PRED_CONTROL

PRED_

PRED_I

COMPARE

RESULT

4d\O!  #E

]l i MS N[AI]\GP!W] [aj

?O!!N`EO! gXc MVMIO! ]^X
IK! 
[aK! MSO! W!d  !I4 NL IK!  _CV/hIKH VMHS W ML4[A]\HSI]\HVMW! P!aVMH[aK
]\QGVMWW hf ?VMa[aK! I M[AIO!  ML QRU! W /QT / I
HL I OH[AI ]\HL
!dG P!aVMH[aK?U!  MSN[AI]\ oS_CK!  /? IK! 
V\[AI OHVMW  ML O!WI4]^X
IK! P!aVMH[aKDN L M[A /E MS%o/IK!  HLIOH [AI]\DNL NLLO! MSI]IK!  	 ]\I]\WH aVMHLY Xc /
_ / /fIK!  TU! MSN[AI MS
PH /I
1[A]\QRUHVMN L]\@NLIK!  /
aC # b
 !I
[/VM MSf]\O! I@ IK! G C a
UHVMIKJVMHS
IK! @V\[AI OHVM WUHVMIK
bTI]?S M[ANS  W_CK! /IK!  /RIK!  @[A]\ I]\W
]_*]^X
IK! fU! ] [A MLL]\GNL
K! R 
[aK! MS O!W!dR ! Id\]E  ML]\!  TLI /U@VMK!  MV\S@ VMHS@[/VM ML]\O!IIK!NL
I]G PH T VMWI /  MSG]\ !]\ I
[A]\QRUHVMNL ]\
%&% IK! 
Ea[
	]\I]\W!O! !I
	V\L	L]E]\RV\L4 IK!  MLO!WI4 ]^X
IK! CP! aVMH[aK
[A]\HSI]\
]\UHL
U
YOHLIVRL /I ]^X
I]
K! NL  MSOH [A MLCIK! T C 
NL /I O! ! M SGPhG IK! T !O!H[AI ]\HVMW  !I

 

 
 



%
"

V
 
^

 
 
 
q
 
 

 
q
 

	

C7Q
VMHS8L]\QR W]\d\N[dE VMI MLC IKH VMI 
! / MS MSRI]G LI]\  DIK! D[A ]\  M[AIi
O!! II]TOHL 
i
Xc]\Q-IK!  
	]\ I]\WHO! !I_ Xc]\	IK! NLU!O! UH]EL NL
K!  V\S!SI ]\HVMWHW]\d\N[C M`EO! MST IK!  
E a[
IK! !  A_
i
GP! I
#"
Xc]\C IK!  [A]\ HS I]\HVMW
]\ I EoEIK!  S M[A] S /CU!WNV\[A ML	IK! 
LK!]_CG G4 d\O! # E
P!aVMH[aKT HLI OH[A I ]\
IK!  La[
P!I	VMHS
IK! cE7 ]
P!I	 QRNLU! MS N[AI MS i
TIK!  La[
P!IaL



XcI MSTI]
 %%&DP! ICIKH VMId\ /IaLLK! 
[A /W W[A ]\IaVMH LCVT[A]\UhR ]^XbIK!  
mC
K! D i		k
XcIMS!VMIaVD [A /WW LQRWNVM4I]
K!  [A /WWNLV LK! 
IK! CP!aVMH[aK
IK! [A /WW _CK! /
NLS  M[A]S  MS
HL IOH[AI]\
Xc]\	IK!  MV\ShDP! I4  MVMW //oMP! O! I	VMdE VM_C IK
IK! C]\! COHL MS
L]\QR  QR! ]\ S
 / /H[A ML%  /dEVMaS!d
[A /WW% K!]\WNS! LCIK! P! I _CK!N[aKGHSN[/VMI ML
IK! DL ]\O!a[A D]^ X_I K! DL K!
i	 	k
K!  
XcI[A]\I ]\W4 Ld\H VMW
IKHVMI	IK! [AO!   /I HLI OH [AI ]\T NL	 V[A]\HSI]\HVMWH P!aVMH [aKTH LIOH [AI]\M a
IK! S  M[A]S  /K _CWW!KHV/E 
[A]\HS I]\H VMWCP!aVMH [aKHLIOH[AI]\
 / /I NVMI G PH / I
I]
VMHSV
HLIOH[AI]\ 
YO!QRU
_ / /V
S
K! D4
m
]^XbIK!   ML O!WI]^X_I K! P! aVMH[aK8[A]\H SI]\GNL  MV\SR PhTIK!  Di		k
Cef i% [A /WW
 
K!  ]\O!IU! O! I
  %% &P!IIK! ]\O! d\KfVM
VMHSRN LC[A]\QRUH VM  MS_CIKRI K! 
Xc]\Q1IK! 
dEVMI 
 dEVMI TN LOHL M SfI]8 S M[ANS 
_CK! /IK!  / IK!  
U! MSN[AI]\D_CV\LD[A]\ M[AI ]\! ]\I
ZXg[A]\ M[AIWh
&%('R Ld\H VMWNL
U! MSN [AI MS%o	I K! 
IK! fHLIOH[AI]\
I]
UHV\LLRIK! ]\ O!d\K
I]
RVMWW]_ MS

IK!  
$_CW WHL IW WHIK!! jDIK!  HLIOH[AI]\TNL!]\I  MV\S hTVMH S_CWWH !]\I[A]\HLN S /

[aK! MS O!W  //o\
#"
 
P!IaS NL[AOH LL MS
P!W][ajb
I_Xc]\VMR NLLO! 
CIIK! LVMQR CIQR EoMIK! 
WNVMI /	 T IK!  LLO! MS
d\ /Ia LDL /ID / E / fIK!]\O!d\K?IK! T HLIOH [AI]\@ NL! ]\IDNLLO! MS
NL[A]\ M[AIWh
XgIK! TP! aVMH[aK
K OHLAo
U! MSN [AI MS
ICS]E ML! ]\ICd\]TI K! ]\O! d\KGIK!  L[aK!  MSO! W!dD U! ][A MLLCVMH SRW MV/E ML IK!   MV\SR UH ]\IgXc  / 
Xc]\IK!  NLLO! ]^ X_VM !]\ IK!  / [A]\I]\WIaVMH LYXc /C HLIOH[AI]\

,%
ZX
"% Ld\H VMWNL	 d\ /! /aVMI MSRVMHS8 L /II]
IK! P!a VMH[aKM_CV\LQR NLU! MSN[AI MS%oMIK!  /RIK! %&
IK!  VMW NS!VMI ]\R  ! II ]D \VMW NS!VMI VMWWHIK!    /I MLRIK! D
 [aK!  MS O!W!dD  !I_Xc]\WW]_C ! dDIKHVMI
"P! I	NL	VMWNL]OHL MS
TIK!NLU!][A MLLI]DQGVMjE  CLO!  IKH VMI	]\!WhTV
P!aVMH[aKT HLI OH[A I ]\
K!  *
"P!aVMH[aKD HLIOH [AI ]\d\  /!  /aVMI ML IK! 

"CLd\HVMW
&% (' 
]_oMIK!  
Xc]\QGVMI]\


[aK!  MSO! W! dD ! I/o VMHSR I_C WW%NLLO! IK!  H LIOH [AI]\R
UHV\LLIK!]\O!d\K8I ]DIK! 
NLVMWW ]_ MST I]

 
q
 
q
]
 
 
)





q

]

	
q

E
q
 
 
 
C

 
 
Z
 
Z
b
 
 
 


 
 
 
 
 
 
q
 
 
 



*



*



 
IK! !  AR I[Ah![AW 
NL	[/VM  MSD ]\O! I 
I]8IK! Ri		k
LK!
XcIVMHS8 [/VM GPH 

P!aVMH[aK8PhUHV\LL! dTNLCS NL[AOH LL MSR 8QR ]\ S  /IaVMW% @
E  M[AI]\W E
IK!  i		k
C C 8[A /WW
K!   MLO!WI	]^X
IK! U!  MSN[AI]\

 1[A /WW	L]fIKH VMIDIK!  R
d\ /IDW]ELI
S]E  ML!]\I
Xc]\QGVMI]\
LO!PHL M`EO! /I WhTOH L MS

G b
WW%IK!NL
d\ /IaLK_CII /
Xc]\WW]_C!dfV

@AAA  4

()1-/

IK! 
[aK! MSO! W!d  !I4 KHV\ST V[A]\QRQR]\ P! I

 /Ih
Xc]\  MV\ [aK
]^X
! INVMWE E / aL]\
Xc]\CIK!   ML /II! dD]^Xb IK! ( 
VMICI K! K! MVMIC]^XbIK! D H LI OH[AI ]\8
 [aK! MS O! W /
mO! II]TVMWW]_
VMHS
P!I

Ea[AC
IK!  f
Ea[
VMHS?U!WNV\[A MS
_]
I]fI
_CV\ LTLU!WI
 aoI
[/V\L G ]^X

oHIK! D 
! 

mC
[A /WW%LI]\ MLCIK!  
P! IVMHS
LCLK!]_C884 d\O! $E
	]\I ]\W4O!!Ia L
q\q
%
(%
NLVMdEVMf V
 /WNSTd\ /I
K! QG VMIa[aKT W] [/VMWW!  MLd\]\!dTIK!]\O! d\K8IK!  
XcIM S!VM IaVT[A /W W
LK!
V\LL  /I MS
T IK! gXc]\O!IKT U!KHV\L  
IK! HLIOH [AI]\
d\ /IaLNLLO! MS
K!  
!

	 C
[A /WWIaVMjE ML	V
W]E]\jRVMICIK!  ML  W ! MLR IK!  NL LO! U! KH V\L  VMHSGS  /I /QR!  ML1_CK!  /IK! /IK! H LIOH [AI]\TNL	PH /!d
NLLO! MSf]\ !]\I
@[/ V\L  T]^X
VM@NLLO!  Eo4IK! 

Xc]\QGVMI]\fd\ /! /aVMI MSfPhfIK!  R[A /WW4NL OHL MS@Ph
P! I
[A /W W%I]TL /ICIK!  

!

IK! # ^

ISS1_BIT

WR_ISS1

SORRY

ISS1_GATE ISS1_VAL

4d\O! $E

q\q

lLLO!  MSRP! W][aj

LCW ]EV\ SGPhUHV\ LL  !dTN LC QRU!W /QT / I /S%o VM8H LIOH [AI]\f [/VM8 PH  DNLLO! MSR  AR UH  M[AI! dTIKHVMI
mO! ID?IK!  R /E /ID]^ X
V8U! / ]\OHLW ]EV\ S?HLIOH[AI]\
_C WWd\ /I
IK! G  MLO!WI
Xc ]\Q
Ph UHV\ LL MS
%( ,%$"
%()"$
V8[/ V\[a K! 
QRN LLAoIK! T  ML O!W I_CW W	! ]\IPH  GV/\VMWNVMP! W 
@IKH VMID[/V\L E o4IK! 
"
&

C
r
a
 
 
 
 
 
V


%
"


q
 
 
q
 

X
 
 

q
 
 
 
 

PHASE  1

PHASE  2

PHASE  3

PHASE  4

E7]

SRC1 TAG

tag match

SRC1 READY

MATCH LOCAL

Result Not Available

ISSUED1 BIT

INVALID ISSUE

issue

ClLLO!  MSRP! W] [ajG E  /IaL	]\ @V\[aK!  De8NLL
4d\O! $ E
Xc ]\QI K! C% ]EV\ST  !I _CWWE PH COH L MSP hIK!  
! 4 [A /WWE I] d\ /! /aVMI 

Xc]\QGVMI ]\ d\ /! /aVM I M S
IK! ' Ld\HVMW	I]8L I]\U@I K! TU! ] [A MLL ]^X
L /II! dRIK!  
DP! I VMH S
Xc]\a[AP!WhG ML /IIK! 

%( ,% $"
%()"$DLd\ HVMW	] [/[AO!aLLWd\K IWh8WNVMI //o4 VMHSfIK! 
P!I
K!NLN L M `EO!  MS8 V\LI K! 
 "
&
'
Ld\HVMWNLDVMWNL]8OHL MS@PhfIK! 
K!  
LIaVMI 
VMW  MV\ShfKHV/E R[aKH VM!d\ MS
P!ID [A]\O!WNS
 

!

	8[A  /W WHI]D HS N[/VMI  I]
IK! ! O!H [AI]\H VMW  !IaL IKH VMIC IK!  NLLO!  ]\RIKHVMI LUH M[A
[CUH]\I
 /]\!  /]\ OHLS! VMIaV
NL\VMW NS%oL]GI KHVM I!]G ]\UH /aVMI ]\H LDVM T[/VM MS8 ]\O!I ]\@IK! 
4d\O! E
 / U! M L /IaLIK! C\VM  ]\OHL / E /IaL][/[AO! ! d
IK! CP! W] [aj]\R V[/V\[aK!  QRNLL
e8]\ S NL[AOHLL]\HL
'DLd\H VMWNLIK! 
K!  
W]EV\S@P hUH V\LL! d@VM R 
E M[AI]\
E
]\
[A]\HLD]^ X
VMHS
IK! RU!]EL
&% ('	P! I/o  /H LO! ! d
LVMQR  V\LIKHVMICOHL M STPhTIK! ^
kC J[A /WWH I]
 ML /I IK!  
IKHVMIIK! 
NL M[A /E  MS
HL I OH[AI]\ 8S]E MLC!]\ Id\ /IL [aK! MSO!W MSTO!  IW%IK!  D[A]\ M[AIS! VMIaV

IK! C]\O! I! dDLUH V\[A Eo\IK! ,%
"%C Ld\ HVMW
]QGVM jE 	IK! CQR]ELI4]^X
Xc]\Q
IK!  VMWNS!VMI]\T !I
NLdEVMI MSD_CI KfIK! 
 DP!I fIK! T 
!

	 CJ[A /WWVMH SfUHV\LL MS@]\@I]GIK!  T HLIOH[AI]\

[aK! MSO! W /

 
q
 


 
q

 
 
q
C
 
 
G
 

 
 
='A()1-/

e8P hUHV\ LLD[A]\ I]\WCLd\HVMWNLDI]
Xc]\Dd\  /!  /aVMI!dfIK!  
-LQR U!W NLI N[ I M[aK!!N`EO! TNLOHL MS
QRU! W /QT  / IgXc O!W WKGfP hDG8PhUHV\LL!d
IK! 8VMQ
P!W] [aj% o4HS N[/VMI!d
Xc]\ Q
K!  RQGVMIa[aKfW!  ML
! ARI [Ah! [AW E o%VM TVMW MV\Shf NLP! W I]GI K! 
 ML O!W I_C WWVME 
IK! 
UH]\I ]\ _CK!N [aKf IK! 
fIK!  
%
mhUHV\L L	P!W ][a j
"\W! MLAo\HS N[/VMI!dIK!  UH]\IC IK!  HLIOH[AI]\TNL NLLO! MS
Xc]\O!
K! 
 "
]\o4VM  TL / HL  MS8 Ph8IK! TmhUHV\LLP!W] [ajfVMHSfOH L MS8 I]8S NL[aKHVMd\  TVMh8]\! 
]^XgIK!  
eRPhUHV\LL
SE /f]\ I]8IK!  TPh UHV\ LL QTO! WIU!W AR  /LC W][/VMI MS
K! M L RL d\HVMWNLDVM T IK! /
[A]\I ]\WL d\HVMWNL
(%
%
VMI
IK! GPH]\II]\Q
]^X
IK! 
 /WNS
IK!]\O! d\K?IK! GP hUH V\LL
SE /aLD VMH S?IK!  R! M[A MLLVMh

PhUHV\LL !dT IaVMjE MLU!WN V\[A 

 8:W()1-/

Xc]\RIK! @ C /]\aS  /G mO[Z
K! D^@I  8P!W ][ajN LRLQRWNVM
IK! 
 /_CIK
IK! f]\! fOH L MS
I]
[/V\L TIK!  THLIOH[AI]\fNL
PH /!dGI KHVM IIK! _CI TSE /aL VM T IO!!  MSf]
]\!Wh@S
 / /H[A 

%GK!]\ WNS! L
(%
K!  
V@[A]\HS I]\HVMWP!a VMH [aK? HLI OH[AI]\
IK! 8QRNLU! MS N[AI MS@i: I]@PH 

OHL MSf 
[/V\L T ]^X
VRPHV\S@P!aVM H[a Ko4VMH S
LK!]\O!WNS@!]\ IPH  R]ME  /
_CII /W_C IK@IK! T MLO!WI ]^XgIK! 
V\S!SI ]\HVMWS E /NLU!  ML / II]8 SE  
WN L]!o4VM
[A]\HSI ]\
$	Ld\ HVMWK_CK!N[aKfNL
IK! $
$	
( (%
%P!IaL
 M`EO!  MSTPhR IK!  

&=

1

0  

XcO!H [AI]\o
	]\ I]\W!  !I4[/VM ML%]\ O!I4 NS / IN[/VMW\W]\d\N[/VMW
D IK! C
E a[
W IK!]\O!d\KD  MV\ [aK ] _
e3b
Xc]\Q-IK!   MLIC]^XbIK! d\]\ O!U
a4d\O! # E
Xc]\O!  /I  ML	S 
IK! I ]\U
K!  Te
 / LW d\KIWh
mC[A /WW4fIK! 
P!W] [aj% oV\L_ /WW V\L IK! T kC
kC1[A /WW4 f IK! R VMQ
[A /WWNLVMHSfIK! Tk  i
VM R !] _1Ia VMHLUHVM / I
C MV\ Sh@P!W ][aj
WNVMIa[aK!  ML H LI MV\S
]^X
 MSd\ 
Id\d\  / MS
U
]\UH L
WNL]!o

E
q

q
 
 

%
q
 

 
Z
Z
 
 
 
q
Z
 
 
	

	

 
E7C
_] U!KHV\ L ML
IK! C]_L PhDI
IK! /hD WNVMdPH /K! HSDIK! C  MLI	]^X
IK! cC7Q
IK! ML  P!IaL4 
XcI!d ]^X
	IK!  LK! 
o3_C K!W IK! C[A]\ MLUH]\ HS ! dWNVMIa[aK!  MLDIK! 
U!KH V\L 
]_ L4NL[/VM  MS ]\O!I	VMI IK!  CN L!d MS d\ C]^X
_CIa[aKTI]DIK!  [A]\ M[AI	\VMWO!  L]\QR  A_CK! / 
Xc]\O!CVM IO!! MST]\GS O! !dDU! KHV\L # EDVMH S_C WWHL
I]\U
TIK! CQRNS!SW ]^X
IKHVMI	U!KHV\ L 
aS  /UH  /H S!dO! UH]\ RIK! S  /WNV/hD ]^X
IK!  MV\S
]\UH  /aVMI]\
Xc]\Q:IK! 
WNVMIa[aK! MLVM  !]
 / /H[A NLIKHVMIIK!  (
C /]\aS /CmO[Z
 /b
K! L M[A ]\HSRS
W]\! d\ /!  M[A MLLVMh
V/\VMWNVMP!W ]\! WhGGU! KH V\L 
Eo
C MV\ShRP!W][aj%oHV\LCIK!  
GI K! 
HVMW4QGVMIa[aKR
Xc]\QGVMI]\._C WWPH  
_CK! /f IK! DIaVM dEL d\ /IIK! 
[A]\  M[AI\VMWO!  
K!  TLV/! dT@LUH V\[A 
[/VMOHL MS8PhGIK! /VMPH L /H[A DNL
OHL MSR O!U8P hRIK!  A RI aVD W]\ d\N [  M`EO! MSTGIK! D i MS N[AI]\G P!W][aj
K! (  &%('CP! I
Xc]\O! ! A_JHLIOH[AI]\HL_C WW]\! WhGd\ /IIK!  
Xc]\ IK! 
[A]\ M[AIC\VMWO! 
?IK!  8L M[A]\H S?U! KH V\L E o /H S  /! d8IK! ML 
?IK!  GIK!aS
U!KHV\L EoP! O!I
IK! 8L[aK!  MS O!W!dfLIaVMIaL
K! NLC  MLO! WIaL8VTS  M[A MV\L MSf
  JO! IWnMV
HL I OH[AI]\ HLCOHL /W  ML L
IKHVMI[Ah! [AW 
Xc]\ L[aK! MSO! W!d
I]\V\L
Xc]\O!
 /I  MLDVM  GU!aV\[AIN[/VMWWhD_C V\LI MS% oVMH S?VMH[A MV\L R 
  -LIaVMWWNL
]
V/E]\NS
&% ('CP! IaL ]^X_IK!  I]\U
Xc]\O!  / I ML	 NLP!W I]
Xc]\ QGVMH[A EoIK!  
IK!NLCS /d\aV\S!VMI ]\8G UH /

IK! 
HLI OH[AI]\ @
[a K! M SO! W /VM  
VMW
_CV/h!L L /If[/V\ L D ]^X
VT! ]\QGVMW4 LK! 
XcI[Ah![AW 
K! NLC QR MVMHL
NLLOHVMP!W 8HLIOH[AI]\HL
IK! 8 HLIOH[AI ]\
IKHVMIT 
IK! 
Xc]\Q
HS

[aK! MSO!W /DNLD!]\ IRVMP!W G I]
PH]\II]\Q
C7QD / I  MLAo\ I_C WWHNLL O!  H LIOH[AI]\HLbXc]\Q1IK! I]\U
Xc]\O!  /I ML	 S /UH /HS ! dDO!UH]\
IK! R!O!H[AI]\H VMW	  !IaL_CIK!]\O! ID[aK!  M[aj! d._CK!  /IK! /IK! 
IK! RV/ \VM WNVM P! WI hG]^X
]\UH /aVMHS!L
VM 
]\UH /aVMHS
ZXbLOH[aKGVM GHL IOH[AI]\ TNLNLLO!  MS%o VMHSR 
 MV\ ShT]\ C!]\I
INL_Xc]\O! HSRIKHVMIIK! La[
,%
	 ?[A /WWbfIK! RLLO!  MSfP!W][aj
_CV\L!]\I  MV\Shf hE /I/o%IK! RLVMQR  
a
! 

 Ld\H VMWNL
"%
VM T  ML /I HSN[/ VMI !d8VM@ \VMWN SfNLLO! T ]\@IKH VMIDUH ]\I

[/V\L T]^X
VGLK!
XcIDLIaVMWW [Ah! [AW Eo%IK! 
( &%('CP! I /QGVMHL	IK!  DLVMQR V\LCGIK!  MLIC]^X_IK!    / I ML
KHVMHSW! dT]^XbIK!  
 /I MLAo
IK! 
fIK!  D MLI]^X
 / /H[A NL8IK! T i MSN[AI]\8P! W][aj
!]\I K! /QGVY ]\ S
IK! DP! aVMH[aK
IK! D U!  MSN[AI MSRUH VMIK@NL[/VM MSG]\O! IVMdE VMHLI IK! D  MLO! WI]^ X
IK! 
[A]\QRUHVM NL]\f ]^X
[A]\HSI ]\T HLI OH[A I ]\
 /I O! ! MS
Xc]\ Q:IK!  !O! H[AI]\ HVMW% ! I
mO! I_Xc]\	 IK!  CI]\U
Xc]\O! 	 /I MLAo
IK! 8P! aVMH[aK
VMHS
[A]\HS I]\ H LIOH[AI]\
IK! 8P! aVMH[aK
ZX
]\! fQR]\ 8 UH]EL LP! W Ih
IK! / 8 NL

q
 
Z

 
 
 
	
 
 
X

 
X
q
q
 
 
Z
 
 
 
E7E
_CVM 8 
[aK! MSO!W!dGNLQRU! W /QT /EI MS
IK!NLD[A]\O! WNS@KH VMU!UH  /?
VMUHVMI.a
HL I OH[AI]\ ?VM  

E]^XcI
XZVM
[A]\H SI]\fH LIOH [AI]\8 KHV\L VMW MV\Sh
IKHVMI IK! 
baoHIK! /8 IN LCUH]EL L P!W 
!NLK! MSG AR  M[A O!I]\
IK! R[A]\H SI]\ 
_CWW !]_:PH R@IK! 
PhfIK! R IQR D IK! TP!a VMH[aK
K!  R MLO!WI]^X
NLD S M[A]S  MS
#"
8P! IC KHV\LI]TPH 
C /]\aS /mO[Z
 /
R IK!NLC[/V\L Eo!IK! D[A]\QRUH VMNL]\R]^X_IK! DS  M[A] S  MS
[/VM  MS ]\O! I	VMdEVMH LI	 IK!  La[
]\UH /aVMHSD MV\ S
Xc]\ Q
IK! C m

IK!  /E /I ]^X
VMG
 LIaVMWW!VMHS
IK!   MLO!WI	]^XbIK!  [A]\ HS I ]\R VM! d
Xc]\ Q1IK!  !O! H[AI]\ HVMW ! I/oEIK!   /d\O!WNVM [A]\QR UHVMNL]\
_CWW%PH 
LI WW[/VM  MST ]\O! I GIK! ML  /I ML
 /aL
Xc]\O! IK? / Ih@S
 /I MLVM R!]\ID AR!V\[AIWhf NS  / IN[/VMW V\LIK! 
Xc]\O!
IK! TI ]\U
E /
K! NL4 NL! / MS  MS I][A]\!! M[AI4IK!   E /IN[/VMW
Xc]\QIK! ]\IK! /I K!  / K_C IKD Ia L4V\S!S I]\H VMW! ]\ O!I!d
Ld\H VMWNLd\ ]\ !d8IK!]\ O!d\ K
IK! 
\VM]\ OHLD [A /WWNLIKHVMI#_ / 
QRNLVMWd\! MSGPH  M[/VMOH L T]^XgIK! T ]\P]\OHL
 / /H[A ML	 GI K! # _CNSIKHLC]^ XbI K! \ VM]\OH LCP!W] [aj! L
S

X

E


 
 
 
)





q
 
 
Z
 
Z
 
EG



 





K! f
Ea[AC@	]\ I]\WCO!! I/o	V\ L
Eo	 NL
LK!]_C?4d\O!  ME
VMWQR]E LI
V8QR]\ QGVMd\ R]^X
I [/VM ML	]\O!ILQRWNVMW]\d\N[/VMW% ]\UH /aVMI]\@ VMHSG  /OHL MLQR ]E LIC ]^Xb IK! 
IK! 

Ea[
	]\I]\WO!!I
WNV/hE]\O!I [A /WWN L_Xc ]\Q*
Ea[
	]\I]\W

WRITE

BYPASS

ISS2 BLOCK

READY BLOCK

CAM BLOCK

4
entries

28
entries

K! D S

 / /H[A MLPH /I

4d\ O! $E
_ / /RIK!  I

ElC 
Ea[ACT	 ]\ I]\WO!! I
_]T[A]\I]\WO! !IaLCVM DWNLI MSTPH  /W]_ l

]\UH  /aVMHSD_CK! W 
W]EV\Sf
IK! 
	]\I]\WaVM HLY Xc / HLI OH [AI]\f M`EO! ML]\! Wh8IK!  TLa[
HS ]_?KH V\L4PH  / /TS  MLd\! MS
LI OH[A I]\
 M`EO!  MLI K! CLa[AC ]\UH /aVMH S
K!  HLIOH[AI]\ 
NLLO!  UH]\ I	 PH /I
KHV\L MLO! WI MSD TLKH VM! d]^X
_CIK
_ / /DIK! 
VM
QR HSDVMHS
IK!NLS /IaVM W!
K! NL NL IK!   MV\ L]\RPH /K! HS
W]EV\SR VMHSTIK! [A ]\ I]\WHIaVMHLYXc /H LIOH [AI]\
IK! VMPHL /H[A 
WNL]!oIK!  _CI GSE /aLVM 
P!W][ajf
]^X
IK! 8
E a[ACf	]\I]\WCO!! I
i MSN[AI ]\
VMh
!]@W]\ !d\ /T IO!! MS
Xc]\ TV@[A]\HS I]\HVMW P!aVMH [aK?H LIOH [AI]\*aV\L
NLTS]\ ! 8

Ea[
	]\I]\Wb

 

 


 
q
q
 
q
 
 
q
Z

q
	
 
^
 
 
]
Z
q
 
QGVMIa[aK!!dT NL! ]GW]\! d\ /IO!!  MS8]
X_IK! 
 / /IfIKHVMIIK! 
K! R VMQ
P!W][ajG NLS 
K!NLNL	[/VMOH L MSTP hDIK!  HVMP!WIhD]^X
IK! 
E a[ACD	]\ I]\W%O!!I	I]
HL IOH[AI]\ TNL \ VMW N S
IaVMjE Vd\W QRUHL CVMII K!  ,%
QGVMIa[aK! ML_CIKTIK! 
K! NL	  MLO! WIaL T O!!!  M[A MLLVMh
"% Ld\HVMW
( %
&% (' P!I
( %
%
'\o_C I M LI]
IK! 
 /WNSRVMHS8L /II! dD]^X_IK! 
mO!IIK! 
&% ('C
Xc]\QGVMI]\._C WW4 LIWW% W]E]\j8VMI
HLI OH[AI]\ f
[a K! M SO! W /I KH VMIW]E ]\j!L VMICIK!  

,%
Xc]\  QGVMj ! dDVM hRS M[ANL]\
IK! 
"% L d\HVMW%PH 
K OHLAo! ]
H[A]\ M[AI	NLLO! _C WW% MLO!WI
]_J]^XbH LIOH [AI]\G AR M[AO!I]\
VMHS8LO!PHL M `EO! / IW hT!]T  
 M[AI]\8IK! 
P! I	 NL	 !]
W]\! d\ /dEVMI MS_C IKRIK! 
	C?[A /W Wo\IK!  ( 
R IK! 
!
 C
I]T IK! DHLIOH[AI]\ f
[aK! MSO! W /
NLUHV\LL M S8S  M[A IW h

"% P!IP!O!I

,%

Xc]\O!C /I ML]^XbIK! D
E a[
	]\ I]\WO! !I/o _CK!  / IK!  LUHV\[A D[A MVMI MS
 !W jE CR IK! I ]\U
_CV\ LIaVMjE  /T O!UTPhDIK! 
PhDIK!  VMPHL /H[A ]^ X
WNVMIa[aK!  ML4 T IK!  C MV\S hDP!W] [aj
aLIL /I	]^X
IK! 
 ARIaVCW]\ d\N [ DI K!  i MSN[AI]\D P!W][aj% oMK! / I	 [A MVMI MLQR ]ELIWh_C K!I  LUH V\[A 
WIK!]\O!d\K
IK!NLLUHV\[A E oIK! W]\d\N[
LK!]_ LIK!  LLO! MSC P!W] [ajT][/[AO!Uh !dDQR ]ELI ]^X
IK! 4 d\O!  #E
NLCVMWQR ]ELICIK! DLVMQR  L n/ DV\L GIK!  D]\IK! /C  /I MLAo _C IK8V
Xc A_
 ARIaVDdE VMI MLgXc]\Q-IK! 
]\O! I!dT d\]\! dRV\[A]ELL
i MSN[AI ]\RP!W] [ajRIKHVMIc_  / ! M[A MLLVMhGVMH SGQR  /IaVMW





 

! 


[aK!  MSO! W /CNL NS MVMW	V\L IKHV\LI]R d\ /! /aVMI 
K! T [A /IaVM W4W ][/ VMI  ]\@]^ X
IK! THLIOH [AI]\
[A]\QRQR ]\8 MV\S
L d\HVMWNLac Xc]\
VM
HLIOH[AI]\ @NLLO! 7bI]8 PH]\ IK
IK! GL]\O! a[A T]\UH /aVMHS!LD ]\
IK! 
IK! M_CI G SE  /aL
aLI
IaV/E /W	PH  /hE ]\H S
L IK! ML G  MV\S?Ld\H VMWNL$_CWW!]\I
IK! / R UH]\ IaL
IK!  L]\O!a[A [A ]\ I]\WHO!!IaLA oI K!  ]\U% [A]S  DVMH STIK!  S MLIHVMI]\TIaVMdMa_C K!N[aK
KHV/E  I]DPH  NLLO! MS
K!  ]\U%[A]S NL
VMW]\!d$ _C IKTI K! C]\UH /aVMHS!LXbVM QR ]ME MSHLNS IK!  
E a[
 /WNS! L

Ea[ACS!VMIaV
qc`
%
H[A WOHS MSR 8IK! T
Ea[
S!VMIaV
 /WNSGI]_C VMaS!LC IK!  
HLIOH[AI]\@
[aK! MSO! W //o _CK!W  IK! 

 /WNS
S!VMIaV

Ea[AC
NLH[AW OHS M ST GIK! 

E
s

Z
Z

 
 

 
 
Z

 

 

q

 
 
q
E
q
 


 

	

 

 
q

'

 
E7e
XcI /PH  /!dRNLLO!  MS% oVMHS8NL]\!Wh
K! T LI]\  
HL IOH[AI]\ fNLLI]\ MS8f VGLI]\ T`E O! /O!  
[A]\QRQR II M SCI ]IK! QR  /QR]\ h_CK!  /DIK! CLI]\  HLIOH [AI]\Dd\  /IaL4LK!
XcI MS]\O!I
Xc]\QIK!  PH]\II]\Q
XcI /VMWW H LIOH[AI]\H LCPH  
K!N LCQR  MVMHLI KHVM IVRLI]\  D NL[A]\QRQRII MS
]^X_I K! T
 
Xc]\ DI
]\!Wh8V
KHV/E 
!NL K! M S@VMHS@IK! / TN L!]8UH]ELLP!WIh8]^X
VMhf]\! R]^X
IK! /Q
I]f[/VMOHL GVM
 AR![A /U!I]\
K!NL	U!  M[/VMO!I]\ TKHV\ LI ]DPH  IaVMjE /TG [/V\L  ]^XbVD LI]\  V\L	I	NLVMR /E  /aLP! W  U!][A MLL]\H[A 
_C II /G I]
IK!  QR /QR]\ hEo\ I[/VM !! ]\IPH  O!H S ]\! 

VALID

INSTRUCTION WINDOW

1
1
0
0
0
0
1
1

load   addr

store  addr 

data

ltag

stag

mispredicted branch

btag

load forwarding

valid

STORE BUFFER

1

stag

addr

data

4d\ O! $E

G!lC 
EI]\ D\ VMWNS!VMI]\

R IK! [/V\L  ]^XbVPHV\SR P!a VMH [aK oEIK!   LI]\ T IK! D
E I]\  mO[ Z
 /_Xc]\Q1IK!  QRNLU!  MSN[AI MS
!INVMWWhE o/IK!NL1_CV\L4 QRU! W /QT /I /S PhDLQRU! Whc_CVMI!d O! IWEI K! 
KHV\L4I]PH C\VMW NS!VMI MS
UHVMIK
"P!I4PH  
II!d
Xc]\ [A]\QRQR
IK! 
  Do\VMHST [aK!  M[ajIaL
d\ /IaLLK!
LI ]\  C HL IOH[AI ]\
XcI  MSD]\O!I	]^X
"W_CV\L
ZX
IK! 
 ML  /I/o	IK!  8LI]\ 8P! O[Z
 /T / Ih_]\ O!WNSVMWNL]@PH 8 \VMWNS!VMI MS
!  
VA_
ILK!]_ LVTLI]\ 
_CIK8I K!NLQRU!W  /QR  AIaVMI ]\TN LC AR U!WNVM!  MS._C IKG IK! DK!  /WUG]^X
4d\ O! 
HL I OH[AI]\ 
Xc ]\Q1I K!  QRNL U!  MSN[AI MSD UHVMIK8VMW MV\ShT HLNS  IK!  D
EI]\ Dm O[Z
 /
ICVMWNL]TLK!]_L
XcI /IK! D \VMWNS! VMI]\
VT! A_JP!W] [ajR ]^X
! 
[A]\ M[AIC UHVMIK@V
Xc]\ Q6IK!  
Xc /Ia[aK! MS
HL IOH[AI ]\

$"( "!)8 V\L
]^X
IK!  G! A_:HLI OH[A I]\?NLD V8 W]E V\S?HLIOH [AI]\?V\ [/[A MLL! dfIK! GLVMQR  
IK! R MVMW /
IK! RW]EV\ S@d\ /IaLNLLO! MS%o4I$ _CWW d\ /IDIK! 
_CVMaS!dfNL [/VM MSf]\O!I/oS_C K! /
Xc]\
L W]EV\S
LI ]\  

V
 

 
 
 
q
 
*



 
*



 

E
 
q
G
 
 
 
 
E3P
 / ]\!  /]\ OHL "
]@V/E ]\NS@IK! NLAo4 IK! G LI]\ G
IK! f
EI]\  G mO[Z
 /
IK! 8
E I]\ GmO[Z
 /
Xc ]\Q


K! NL	 [/VMTPH  V\[aK! /E MSDPh
Xc]\WW]_C! dDVQRNLU!  MSN[AI]\
KHV\L	I ]PH \VMW NS!VMI  MSD QRQT MSN VMI /W
 //o4VMHS@V
bCI]8IK! R
E I]\  G mO[Z
L / HS !dG IK! RS MLI HVMI ]\@Ia VMd8]^Xg IK! T PHV\Sf P!aVMH [aK
XcI /


o4IK! G LI]\ 8[/VM
[A]\QRUHVM NL]\
_C IK
PH R  \VMWNS! VMI MS
IK! 
K! NL# _CWW	 !]\I
PH 8 V8LQRU!W 


 TVM  
\LAoVMHSf IK! G 
EI]\ TmO[Z
[A]\QRUHVM NL]\@V\L IK! 
]\ WW MS8]ME  /V
XcI / MV\ [aK! !d8VMWW
 /


KOHLAo
_CWWCKHV/E GI]?S  M[A NS M_CK!  /I K! /
Xc]\ 8IK!  GQRNLU! MS N[AI]\
PH 
IK!NLTLI]\  fNL
]\G V
XcI /
VMWIK! ]\O!d\ KfIK! D[A ]\ I ]\W Ia VMHLYXc / HLIOH [AI]\8S]E  ML! ]\I!]\QG VMWWhTQGVMjE  OH L D]^X_IaL
o!ICNL


]\UH /aVMHSGNL	NLLO!  MS._C IKGV
 M`EO!  MS
[A]\I]\W
L ]\!WhTIK!  La[
Xc]\CIK! D VMPH]ME   MV\L]\ 
K! / 
Ia VMHLY Xc / HLIOH [AI ]\o!IK! DO!OHL MS8P!IaLC]^X_IK! D ]\U%[A] S  T8IK!NL[/V\L 
VM 
OHL MSGI]GLI]\ DIK! 
SO!U!WN [/VMI [A]\ UhR]^ X_I K! 



VG4 k:HLIOH [AI]\ o%]\! WhfIK!  TLa[ACR]\UH  /aVMH S_CWWPH 

NLLO! MS%o1 _CK!N[aKfNL
]^X
[/V\L 
 /WNS! L	VM O! U%S! VMI MS _CIKTIK! C  MLO!WIaL
! V\LIS]E  ML	!]\I	! /  MSTVMhD ]\U% [A] S 
m]\IKT IK!  S!VMIaV
Xc]\Q-IK! D!O!H[AI ]\H VMW4  !Ia LCVMIC IK!   /HSR]^X_IK! D[Ah![AW 



 
h
 
a
 
 
 
 
q
 
 
q
 

 

 
E7Q







 
 



IK! THLIOH [AI]\W^
HS ]_
K! THLI OH[A I]\ @
[a K! M SO! W /
Xc]\QGLCIK! DPH V\[ajPH ]\! 
]^X
L
LK!]_C8 G4 d\O! $ E
o!ICNL	IK! QGVMG [A]\ I]\WW! d
IK! QR]E LIC[A]\QRU!W AR
L M[AI]\GVMH SGNLC VMWNL]
q/s
]^X
VMWW

R
E
A
D

D
R
I
V
E
R
S

CNTRL

ALU

LOAD/STORE ALU  LOGIC MUL

LOGIC

LOOKUP

LOGIC

LOGIC

AND

ARRAYS

AND

LOOKUP

ARRAY

SPECIAL

LOOKUP

ARRAY

R
E
A
D

D
R
I
V
E
R
S

4d\O! $E

q/s

lHLIOH [AI]\f
 [aK!  MS O!W /

4E [A]\I]\W%P! IaL	H SN [/VMI!d IK! ($' #	 VM U!  ML / IT IK!  
Ho]\! 
Xc]\  MV\[aKRHLIOH[
K! /GV!  A_HLIOH [AI]\RNL	S 
I]\RIhUH $ a DoE ef o 4 kDo!
  ?VMH S8 C b
[A]S MS%o\IK! C[A ]\  ML UH]\ HS! d$'  #%P! I4DIK! 
 d\ /IaLL /I/o_CK!W IK!  MLI	VM  ML /I4 PhU! WNV\[A!d
&%('*
VRn/  / ]R]\@IK! / L K!
XcI !U!O!IaL
K!  ML 
P!IaLVM 
IK! /fO!IWn/ MS8VMW]\!dM_C IK@IK! 
&%(' *
,%
MSNL[aKHVMd\!d8]^X
VMHS
TLd\HVMWNLI]8[A]\I]\WIK! T [aKHVMd\!d

 
 
"% 
&% DW!  MLIKHVMI
CI IK!  T /H S8]^XgIK!  RL[aK!  MSO! W!d!o% IK! 
!U!O!IaLI]GIK! R %]E ]\j O!U? aV/h!L
IK! 
VM DL /IVM  DS E /R]\I ]TIK! %
VMHSf
E a[AC
"W! ML	]^XbIK! 

Ea[
S!VMIaV
 /WNS! LIK!]\O!d\K

IK!   MV\SGS E / aL

"

 


 
 
 
 
 
 
	
 
^
	
 
*

 

%
q

 
NcA 

LM 4-3'  46

 M 0  ')N# =K A

4

4 X 4

Multiplexers

4

FUs

ALU1

ALU3/STORE

ALU2/MUL

LOAD/CNTRL

CENTRALIZED
DEDICATED
elHLIOH [AI]\f
 [aK! MS O!W / i4]\I LLd\!QR  /I	 ]\U!I]\HL
4d\O! $ E
L /W M[AI! dRK!]_:IK! T
Xc]\
_]G]\U!I]\H L#_ / RV/\VMWNVMP! W 
eo I
4d\O! .E
LLK!]_C?
Xc ]\Q-IK!  D
LI OH[A I]\HL_ /  N LLO! MS

V/\VMWNVMP! W fUH]\Ia
aLIM _CV\LRI ]?NLL O! @VMh
G bTVMHS
K! 
HLIOH[AI]\
]\JVMh
]\O! IG]^X
HLIOH[AI]\ fI]RIK!  T[A]\ MLUH ]\HS!d
IK! /fOHL !d8V
XcO!W WGM RWGGQTO!WIU! W R  //o! S E  D IK! 
K! TQTO! WIU! W R  /C [A]\ I]\ W	Ld\HVMWNLVMH SfIK!  
!O!H[AI ]\HVMW	 ! I
NLLO! 
]^X
HLIOH[AI]\HL
_]\O!WN SRd\ /I [A]\I]\ WW M SRPhRVT [A / IaVMWn/ MST[A]\I]\WW /
!O! H[AI]\ HVMW	 !IaL
K! RL M[A]\HSf]\U!I ]\_CV\L I]8S  MSN[/VMI TLUH  M[A
[DUH ]\IaL I]8LUH M[A
K!NL!]\I]\!Whf V/E ]\N S M SRIK! 
P!O! WjhRQTO! WIU!W AR  AaLAoP! O!IVMWNL]RQG V\S  IK! 
[A]\ I]\W4QR]\ 
	V\ [aKD UH]\IK _]\O! WNS!]_?KHV/E 	IaL]_C TS MS N[/VMI MSD[A]\I]\Wo
L QRU!W
 MSVMHS
SNL I P!O!I MS
	[A / I aVMW n/ MS
VMHSR IaLS 
HVMIO! QGV\S  IUH ]ELLP! W I]
 MVMWn/ IIK! ]\O!d\K
XcO!WW
	[AOHLI]\Q
K! DL M[A]\HSG ]\U! I ]\G KHV\LCPH / /8 QRU! W /QT AI MSD GIK! NLCS  MLd\

E
r
 
 
q
 
q
	
^
 
q
 

 
 
C
 


[
 

 
 
 
G3]

NcA. ,.AA 056 0

!]\QG VRQT O!Q: 
[A  /H[AhEoV\LQGVMhTUH ]\IaL V\LIK!   O! QTPH /C]^X
  LCGIK!  DU! ][A MLL]\
mO! IIK!NLW MV\ S!L I]RVMGO!!  MV\L]\HVMP! WhGWNVMd\ DLn/ ]^Xb IK! D[A /WWo
VM   M`EO!  MST GI K!  S!VMIaVT[A  /WW
QTO!WIU!W h
!dTIK! T Ln/  
]^XgIK! TS MLd\ 
8G
UH]\ITS!VMIaVG[A /WW_C V\L
HVMWWhfS  M[ANS MS8O!UH ]\
VMHS
kO!  GI]f IK!  RWQRI MS8 O!QTPH /]^X
Xc]\  GOH L! dfID 
]\U!IQR n/ MS8 A RI  /HLE  /W h8PH 
IK! 8S  MLd\
UH]\IaLAo%V
[A]\QRU!]\ QRNL  KHV\SGI]
PH DQGV\ S  PhGV\LLd\! !dR LUH M[A
[  LI]RLUH  M[A
[UH ]\IaL

l	S MSN[/VMI MSR LUH M[A

[/VMWWhTI]R  

 
 
 
 
 

Rl	LKHVM MSGPH  /I

l	LKHVM MSGPH /I

%Tl	S MSN[/VMI  MS
)l	S MSN[/ VMI  MS

VMHSGIK! Def  C i	 	 

O!! I

_ / /G C
_ / /G  E
Xc]\C C @C 
 	

VMHSGIK! 


 O! !I

HLIOH[AI]\HL

Xc]\4 kJHLIOH[AI]\HL

 !oEIK! L IOH[AI O!  C]^X
!]\
I]V\[/[A]\O! I	I]DV\ [aK! /E 
IK! HLIOH [AI]\TL /IK _CV\L IaVMjE /
% H LIOH[AI]\8]\! Wh8!  / MS!L ]\! D ]\UH /aVMHS6a	7b_CK! W DIK! 
QGV R QTO!Q:O! I W nMVMI ]\
K! 
"
&% &
HLI OH[A I]\
K! NL QR MVMHLAoPH]\IK?H LIOH[AI]\H LD[/VM?PH 
! / MS!L]\!Wh
& 

LQTO!WIa VM!  /]\ OHL Wh
NL LO! MSR]\8IK!  DLVMQR UH ]\I

@AAA W,.) : <5

VMWW[/VMI 
Xc]\Q
HLIOH[AI]\HL
IK! TNLLO!  T]^X
NLOHL MSf 
aLI 8VMWd\]\IK!Q
]\WNS ML I
K! 
QGVMjE MLID MV\ Lh@ I]8QRU!W /QT / I IK!NLDV\L IK! R]\WNS MLI
IK! G
K! G :HVMIO! R]^ X
d\]\ ML
I]fIK! 8PH ]\II]\Q
HL I OH[AI]\ 6_]\ O!WN S
PH f[AW ]EL /
]^X
IK! GHLIOH [AI]\6_CH S]_
K!  fS /IaVMWNL
SNL[AOHLL MSRPH  /W]_l
 MV\[aK8[/ VMI  /d\]\hGVM  
 / WNVM I MSRI]

 
 
 
	

 


 



q


 


 
"


 
 
 

	
 
^
 

EO!U!N L!d\WhE oI K! 8S  M[ANL]\I]@KHV/E  8]\! Wh
,.U
aVMdEVMHLI
IK!  fS MLd\ 
E@  L
IK! TV\ S!S I]\fI]GLn/ D]\]\O! I! d!o%P!O!I
IK! DU! ]\UH]E L MSDG bao
_CV\L!]\I QGV\S  PH M[/VMOHL 
]^X
_ / /U?IK!]\O!d\KIK! 
aLITQG VMjE T ]\!  fL
K! fL[aK! MS O!W /DKHV\ L
OHL MS
IK! 8VMW d\]\ IK!Q
I]
oEQG VMjE  VM! ]\IK! /CL
V/\VM WNVMP!W  
HL IOH[AI]\HL	 I]
HST IK!  ]\WNS  MLIC  
HLI
_ / /UR]^X
IK!   /QGVM ! !dI]
IK! 
HST IK!  L M[A]\HST ]\WNS MLIC  
HLI
oVMHS
HVMWWhT QGVMjE  VIK!aS
_ / /UHLVM 
	[/VMWW MS@L
WIK!]\O!d\K@ IK!  ML TL]
HSfIK! T IK!aS8]\WNS MLID  :H LI
_ / /UfI]
V\[AIOHVMW Wh QRU!W / QR AI  MS PhDOHL! dS h HVMQR N[% ]E ]\jO! URaV/h!L _CK! N[aK
!NLK
IK!NL4IaV\LjD
VMWd\ ]\IK! Q6V\S!S! LIK!NLO!U8 I]RVR [A]\H LNS /aVMP!W 
SNL [A /I  IQR  EoE IK! DL  / NVMW HVMIO!  D]^X_IK!  
VMQR]\O!I
E /RI]D KHVM HS W  IK!  ML #ED  LAoIK!  L[aK!  MSO!W! dKH V\SRI]D PH   AR I /H S  MS
]ME /
I]D I
_]U!KHV\L ML	HL I M V\ST ]^X
IK! 
Ph
IKHVMIGD  L[/VMT PH KH VMHS W MS
K!  / NL! ]$_CV/h
[AO! / I S ML d\
K! 
]\!WhW_CV/h8I]GV\[aK! /E  DIK! NLNL P h8 /IK! / LW]_C !dRIK! T[AW][aj8]\
OHL !d VM!]\IK! /	VMWd\]\ IK!QIKH VMI	[/VMT[/VMh]\O!IIK! NL]\UH  /aVMI]\ o\]^X
L /W M[AI! dGNLLOHVMP!W 
 JH LIOH[AI ]\HL
Xc ]\Q6VDUH ]E]\ W4]^X_ HLIOH[AI]\ HLAoGUH VMaVMWW /W
Cd\]\]\OHL LQTO! WNVMI]\HL
_CWWPH RVM
IaL]^Xg IK! NLAo4V\LIK! / 
I]GPH R [/VM M S8]\O!II]G]\PH L /E  TIK! 
_CWWKHV/E 
PH /! 
H[A  MV\L G ?IK! GO!QTPH /]^X
LIaVMWWNLD
IK! 
]\WNS  MLI
aLI@VMWd\ ]\IK!Q
NL
!]\ ITLIN[AIWh
Xc]\W W]_ MS
K!  
 @HLIOH[AI]\HLVM NLLO! MSD 
]\aS  /	I]IK!  LI]\ CO!!I
 $,.L J1&  
4 k
VU!  /]\OHL
NL! ]\I	NLLO!  MS
HLIOH[AI]\
IK! CU!]\UH /CLIaVMI 
I] QGVMIaVM 

 fHLI OH[A I]\
NLUH /HS! d
IK! DIK!  LI]\ C H LIOH[AI]\D PH]\O! H S!VM MLAo4k
NL LO! MSf]\O!I
HL IOH[AI]\ HL VM 
WNL]!o4S O!!d8VMh@[AW] [ajf[Ah! [AW EoH ]\!Whf ]\! T]^X
]\aS  /
]^X
	
IK! GI
_]
[/VMPH GN LLO! MS
]@NLLO!  GPH]\IK
VMITIK!  8LVMQR GIQR _]\ O!WNS? M`E O! R ARIaV
[A]\QRUHVMN L]\HLI]
_]!o\VMHS
_ / /D IK!  CI
IK! /  NLVM! ]\IK! /
 fHLIOH[AI]\DPH  /I
HSD
VMWNL ]$_CK! /IK! /	I K! / h
VM  
Xc /! dI]D IK!  LVMQR  V\S!S  MLL	 ]\!]\I
K! NLS _CV\LS
[AO!WII]
QR U!W /QT  / I TIK! [A O!  /I S  MLd\Rd\E  /
IK! IQR!d[A]\H LIaVMIaL	]\G L[aK! MS O!W!dVMHS
IK!  LIOH[AIO!  ]^X
IK! D S M Ld\ 
IKRIK!  VMPH ]ME  WQRIaVMI]\ o\I_ ]\O!WNSGL / /Q:PH  /II /CI]

G
q

 


 

 

L

 

 
q
 
 
 

X
 

 



	

Q

 


X
 
^
	
 
 

X
 
 
 
^
G3C
LI]\ TI]8]\!  TUH]\I/o4P! O!IDUH VM! dGIK!  TW]EV\SD_CIK@IK!  R[A]\I]\W
V\LL d\
VMHS
W]EV\S
PH]\IK
IaVMHLY Xc /  MLO!WI  MSf @VGUH ]ELLP! W TNLLO! T]^X
HLIOH[AI]\ HL 
VG[AW] [aj@[Ah! [AW 
VMHS_CV\L
Xc /  MS
U! 

IK! 
HLIOH[AI]\HLTQG VMjE MLTOH L f]^X
IK! fQTO! WIU!Wh
]^X
L[aK! MSO! W! d
K! 
&#@A N# 
 BUM
&%('L d\HVM W1a
HSN [/VMI! dCIKH VMIIK!   ML]\O!a[A  NL MV\Sh b
K! NL  /H LO!  ML4 IKHVMI4 DIK! 
"
XcO!I O!  C
VPH /I I /S M Ld\ 
]^X
IK! QT O!WIU!W
 /O!! I MLO!WIaL4R VL! d\W [Ah![AW  ]\UH /aVMI]\o
]\D
 /CNLU! UH  /W! MS%o! ]fV\S! S I]\H VMW[aKH VM!d\ ML_C WW	PH  T M`E O!  MS8 @IK! 
IK! RQT O!WI U!W 
HLI OH[AI]\ f
[a K! M SO! W /

K!   MLI]^XbIK!  HLI OH[A I]\R IhUH  MLVM  NLLO!  MSR]\O! I
aLITVM Wd\]\ IK!Q

]^X
	

]\aS  /OH L! d

IK! LVMQR  

]\WNS MLI

s
 


 
X
X
 
	
 

 
 





G3E

q/s

LK!]_JIKH VMICP hUH V\LL! dTNLCVM
K! D L QTO!WNVMI ]\T M LO!WIaL]^XbIK!  
VMa[aK!I M[AIO!  

k 
i
IK!  VMPHL /H [A 	 ]^X
Ph UHV\LL!d!o
Xc MVMI O! _Xc ]\4IK!  LO!UH  /aL[/VMWNVMVMa[aK!I M[AIO!  
 ARI / QR /WhC[AI N [/VM W
	IK! G H LIOH[AI]\?NLLO! MS@
W MV\ LI Cf[AW][aj
 /E /hf HLIOH [AI ]\6_CW WKHV/E GVMI
[Ah![AW TWNVMI /H[Ah
]\! 
CMHSf[AW] [ajG[Ah! [AW Eo! VMH S8IK! 
[Ah![AW  Eo!I K!   MLO! WIaLC  M[A /E  MST 8IK!  
S /UH /HS  / ICHLIOH[AI]\
Xc]\4N LLO! 	 IK!  EMaSD[AW][aj[Ah![AW 
L[a K! MSO!W  MS
K!  HLIOH[AI]\T
[aK!  MSO! W /%KH V\LPH / /DS  MLd\! MS
_CK!  /GINLLO!  MLCVMGHLIOH [AI]\o
I/o 
LOH[a KRIKHVMIIK! PhUHV\ LL !d
NLC[A]\QRU!W /I /WhD K!NS! S  /RI]
aLXb ]\	IK! C ]\UH  /aVMHS
_CK! / IK! /IK! CHLIOH[AI]\ TVMW MV\S h
I	S]E ML!]\ I	j!]_
aLXb
KHV\LIaL4]\UH /aVMHS
_CWW!PH Ph UHV\ LL  MS
TIK! !INVMW!LIaVMd\ ML	]^X
K!NLS_CV\L! M[A MLLVMh
IK! S  MLd\RV\L	IK!  S M[ANL]\
I]R QRU!W  /QT  /IPh UHV\ LL !d._CV\LLIWW4! ]\IQG V\S 
IKHV\ LVMWNL]R MLO!WI MS8fVTE  /h8LQRU!WNLIN[
R.GTQTO! WIU!W AR AaL
XcO!WW1G
ShHVMQRN [CW ]\d\ N[IKH VMI d\ /! /aVMI  MLIK! D[A]\I]\W4Ld\H VMWNLg Xc]\CIK! 
IK! HLIOH [AI]\RPH /!d
mhUHV\L L!dD NL[/VM   MST ]\O!IPhRL /HS! dDIK!  S MLIHVMI]\RIaVMd
]^X
NLLO! MS%oHI]RIK! (  (%
%
VMHS
 /WNS! LC]^X
IK! 
HLIOH[AI]\W^
HS ]_
efVMIa[aK!!d_CWW

IK! C MLUH  M[AIE  QGVMIa[aKW! ML
PH [/VM   MSD ]\O!I	 d\KIVA _CV/ h
IK! V\ LL /I]\T]^X
_CWW MLO!WI 
VMHS
K!NLc_C WWV\S! SfIK! 
HL IOH[AI ]\HLIKHVMI# _ / $ _CVMI!dG ]\@VR  MLO! WI/o%I]RIK! 
UH]E ]\W ]^X
NLLOHVMP!W 
SNL[AOH LL MS
_]R LUH M[ANVMW4[/V\L ML]^X_ Ph UHV\LL!dTVM 
HL I OH[AI]\ HL





 	



@ I / M LI !d
e8]ELID]^X
NLIK! TW]EV\S@Ph UHV\ LL!d
Xc MVMI O! TQRU!W /QT /E I MSR@IK!  R
IK!  IQR  EoM IK! W ]EV\ SR ML O!W IK_CW WHPH  V/\VMWNVMP! W T ]\! D [AW] [ajT[Ah![AW CPH M[/VMOHL ]^X_[/V\[aK!  K!I/oVMHS
QRNLLAo%P hUH V\LL!d
[/V\[aK! 
YOHLID W jE DIK! R  1 MLO!WIaL
ID[/VM
[/V\L R]^X
mO! I
PH TPhUHV\LL  MS
VR[aKHVMf]^X
fQR ]ELI [/V\ L MLAo4VTW]EV\ SfHLIOH[AI]\8NL VMIIK!  
[/VM!!]\I PH T[/ VM  M SG]\O!I
]E]\I]^X
S /UH /HS /H[A MLVMHS@ INL V\S\VM IaVMd\ /]\OHL
I]GOHL RI
L MLO! WIDV\LDL]E]\V\L UH]ELLP! W 
!]\IK!NL

 

 
 
 
 
 
 
 
 
'
'

 
 
 
 





^
 
 
	
 

 
G7G
Xc]\
aLITV\LLO! QR! d@V@[/V\[aK! 
[/VM MS@]\O!IT Ph
 MV\ L]\ oCVfLUH M[A NVMW[/ V\L  G]^X
K!I
PhUHV\LL! d@NL
Xc]\
 MLO!WITVM 8IK! /L /IO!U
S /UH /HS !d@]\ IaL
K! 8 H LIOH [AI]\H L
 /E /h@W ]EV\S?]\UH /aVMI ]\
ZX_IK! HLIOH [AI]\GNL
PHV\L MSGO!UH]\fIK!   ARUH  M[AI MSR MLO!WIg Xc]\Q-W]EV\SG PhUH V\LL! d
L[a K! MSO!W !d
QRN LL][/ [AO! aLAoIK! 8
NLLO! MS
VMHS?Vf[/V\[aK! 
 ML /IaLIK! R MV\Sh
VMHS
NLLO! MS@P!IaLIKHVMI
_ / 
XZVMWNL  /WhRL / IVMHS8VM WNL]T 
Xc]\ QGL	IK!  /W /\VM IC! O!H [AI]\H VMW4  !IaL ]^X_VM8\VMWNST NLLO!  

Xc]\
VMd\OHVMP!W @ UH]\ IRNLT IKH VMIGP hS ]\!d? IK!NLR LUH M[AO!WNVMIE 

_CVMaS !d!o IK!  @VMW MV\Sh
_CV\LI MSfV\ L]\IK! / MV\ShGH LIOH [AI]\H L[A]\O! WNSGKH V/E DPH  / /f NLLO! MSG
WQRI  MSTNL LO! DUH]\ IaL VM  
U!WNV\ [A ]^X
IK! ML 	SO!P!]\OHLHLI OH[AI]\H L
mO! I%IK!NL%NL%d\ ]\! dI]C KHVMU!UH  /D
Xc M`EO!  / IWhaV\LLO!QR!d
W]EV\S8H LIOH[AI]\D _]\O!WNS
W]EV\ SfPh UHV\LL!d! oIK!  
WNL]!o
VRd\ ]E]S
_CIK!]\ O!IIK! 
S ML d\
[/V\[aK! 
UH]\I /IN VMWW h@IO! I]@ VC
	[Ah![AW RWNVMI /H [Ah
HLIOH[AI]\
K!  8QGVM
 MV\ L]\
Xc]\G
  6LIaVMWWNL
S /UH /HS /ITH LIOH [AI]\H LAo VMH SIK!  8 MVMW /
U!WNV\[A M S?S!VM IaV
NLT[A W]EL / Wh
IK! /h?VM 8NLLO! MS%oIK! 
PH /II / IK! DUH  /
Xc]\ QGVMH[A 
KOHLAoH IK!  D]ME /aVMWW% UH /
Xc]\QGVMH [A  QRU! ]ME /QT /I [/VMOH L MSGP hGW]EV\S
PhUHV\LL !dT]ME /aL KHV\ S] _LI K! ][/[/V\L]\H VMW4NLLO! UH ]\Ic_C V\LIaVMd\ 



'	



Xc]\QGVMH [A  NL P!aVMH [aKR PhUH V\LL! d
!]\I K! / !!]M \VM IE VMU!U!]EV\[aKGI]D QRU! ]ME  UH /
K! 
XZV\[AI]\aL	
INLQR! QRnA  MS PHV\STP!aVMH [aKRUH  /HVMWIhEoE]\!  ]^X
PH /! 
IK! QGVMTS /IQR /IaVMW
QGVM 
K! /fIK! D MLO! WI]^XgVTP! aVMH[aK@[A]\H SI]\8H LIOH [AI]\6a
LO!UH /aL[/ VMWNVM VMa [aK! I M [AI O! ML	
]\
_CK!N[aKfI K! TS  M[A NL ]\fI]GP! aVM H[a K@NL QGV\S  7bCNL  ARUH  M[AI MS8I]GPH  
 /IO!! MSfVMI IK!  T /HS8]^XgIK! 
V\LLO!QRU!I]\
Xc]\DNLLO!  
HL IOH[AI]\ @NLDVMO! I]\QGVMIN[/VMWWhf [A]\H LNS / MS
[Ah![AW  EoI K! R P!aVMH[a K

'[A]\ M[AIi
NLCQGV\S DI KHVM II K! 
U! MSN [AI]\M_C V\Lc_C ]\ !d8VMHS8 IK! (     
NLL / II]RIK! 
"
iJO!! IVMI IK! 
 /HS8]^X
IK! T[A h![AW 

EQTO! WIaVM! /]\OH LWhE o!IK!  
 MLO!WI]^X
IK! 
P!aVMH[aK@[A]\ HSI]\
VM E MLVMH S8NL[A]\ QRUHVM MSGVMdEVM HLI IK! DU!  MS N[AI MSR  MLO! WI H LNS  DIK!  
IKHVMI
[/V\L 
8IK!  
IK! ]\d\ HVMWHU!  MS N[AI ]\_CV\ L	[A]\ M[AI/oEIK!  i?O!! I	NLI]\WNS
I]Dd\! ]\ IK! QRNLU! MS N[AI MSDi


	
 
 
	
^
 
 
b
 
 
 
 

 


 

Q

 
^
 

 

^
 
VGQRNLU! MS N[AI]\ oH IK! R [A]\  M[AIDi JNL VMW MV\Sh
VMHS
VMI
]\o_CK!W  T @IK! R[/V\L T]^X
[A]\I O! 
R MLL /H [A E oEIK! PH V\SRP! aVMH[aK
IK! S ]E]\ LI / UG]^XbIK!  iO! ! ICVMHS8 [/VMR PH OH L MSR d\K ICVA_CV/h
[AW ][ajR [Ah![AW PhG V\S]\U! I!dT IK! D VMPH]ME  DU!] [A MS O!  
UH /HVMWIhR NL MSOH[A MST PhR]\ ! 

!]\ IRPH  / /?QRU!W 
WIK!]\O!d\KIK! 8V\ S\VMIaVM d\ MLRVM 8]\P]\OHLAoP! aVMH [aKP h UH V\LL!d@ KH V\L
U!]MNS!dTIK!  
8IK!  DU!] [A MLL ]^X
[AO!  /IS ML d\
QR / I  MSTfI K! 
V\S!SI]\ HVMWbXc MVMIO! MLI]
IK! TI ]\U
Xc]\O!D HLI OH[AI ]\
LW ]\IaLAo	VGW]\I]^X
[AOHLI]\Q
]\O! I!dfVMHS
VMWd\! QR /I_C V\L M`EO! MS
PhUH V\LL! dD_]\O!WNS@QR MVM@QR] S
QRU!W  /QT /IaVM I]\8 ]^X
IK! R ARNLI!dGW]\d\N[
[/VMI]\@]^X
P!aVMH[aK
VMHS A RI  /HL]\D ]^X
IK! 	\VM]\ OHL4P!W][aj!L
K!  IQR  bXZV\[AI]\ 4 M`E O! MSI] /H LO! IK! [A]\ M[AI! MLL
]^X
WNL]!o4d\]\]\ OHLD I MLI!d
IK!NLIK! MLNL
IK!  RQR U!W  /QT A IaVM I]\ W_ /IPH  /hE ]\H S@IK!  GL[A]\UH R]^X
_]\O!WN SRKHV/E  I]TPH D[/ VM  M SR]\O!IVMI MV\[aKGLI /U8]^X_ IK! D [aKH VM! d\ I]TQGVMjE LO!  D! ]\IK!!dT  /WNL NL
	[AOH LI]\Q-S  MLd\
S MLI]MhE MS%o]\! ]^ X_I K! QG VM GSNLV\S \VMIaVMd\ MLC]^XXcO!WW
baoM /E /_CK!  /IK!  	 MLO!WI%]^X
IK! 	P! aVMH[aK
 IK! [AO!  /I%QRU! W /QT A IaVMI]\$ a
E M[AI]\# E
IK! [Ah![AW EoMIK!  P!aVMH[aKT HLIOH[
[A]\HSI ]\T HLI OH[A I ]\T NL  ARUH M[AI MSDI]D VME  VMI	 IK!   /HS
]^X
I]\NLD !]\IG [A]\ HL NS /  M S
Xc]\R VMNLLO! 
K!  f[A]\QRUHVMNL]\? ]^X
IK! 8U!  MSN[AI]\?VMdEVMHLIRIK! 
NLD[A]\  M[AIWhfU! MS N[AI MS%o4IK! 
IK! RP! aVMH[aK
HLNS  RIK! G
V\[AI OHVMW ML O!W IDN L
ZX
[/VM  MSf]\O!I
HL I OH[AI]\ 8NLLI WW%!]\I[A ]\HLN S / MS
Xc]\VM8NLLO! D P!O! IIK!  
[A]\ MLUH ]\H S!d
P! Id\ /IaL

[A]\  M[AIW h
KOHLA o!V
L / I
K! NL	 W MV/E  MLIK! 
P!aVMH[aKRH LIOH[AI]\R! /E /	d\ /IaL NLLO! MS
U! MSN [AI MS
Xc  / 
Xc]\CIK! NLL O!  ]^X
]\IK! /C[A]\ I]\W% IaVMH LYXc / HLIOH [AI]\HLcaV\[AIOH VMWWhTVM8V\S \VMIaVMd\ 
 
VfQRNLU! MS N[AI]\o4IK!  8H LIOH [AI]\? d\ /IaLT[A]\HLNS
IK!  8[/V\ L G ]^X
I]@ P!aVM H[a KPh UHV\LL !d b
 /  MS
Xc]\CNLL O! R IK!  ! A RI [Ah![AW 
	]\ QRUHVM MSRI]
IK! P! aVMH[aKGPh UHV\ LL!dRVMU!U! ]E V\[aKo! IK!NLNL
Xc]\ IK! TPH V\SfP! aVMH[aK@UH /HVMWIh
VM
K!  TPHV\ S@P!aVMH [aK@UH  /HVMWIhf[Ah! [AW MLd\ /I
V\S!SI ]\HVMWC[Ah! [AW  
[/VMH[A /W W MST ]\O! IC 8[/V\ L D ]^X

 LIaVMWWNL

G
s
 
 
	
 
 

 
 
 
 
E
 
q
	
 
^
 
 
 

%
 
	
 
 
 
 

	





G3e

Xc]\VTP!OHL
[AW] [ajG[Ah![AW WNVMI /H [AhE o!IK! / NLC !]R MV\L]\
I KfVMWW!O!H[A I]\ HVM W4 ! IaLC]^X
HLIOH[AI]\ HL NLLO!  MS
VW_CI  Eo4 /E  /@I K!]\ O!d\K?IK!  / G[/VM
Xc]\
PH G QGVR Q
O!Q
VMITV
N[AI
[A]\
"#&%&
XZV\[AI	IKHVMI	IK!  
K! NL NL	 SO!  I]DIK!  
IQR CVMH ST]\!Wh
G#_C I UH]\Ia LV/ \VMWNVMP!W 
& 

VMHS8IK! ( HL I OH[A I]\ HL S]R!]\I_CI 
VMhG MLO!WII]RIK! 
]\ IK!  
mo% VMH S8f[/V\L 
]^X
NL LO! R[Ah![AW  Eo VMI W M V\L ID]\! TH LIOH [AI]\@NL d\OHVMaVMI / MS@I]GPH  R /IK! /]^XgIK! /Q
K! 
IK! DLVMQR  UH]\ I V\LIK!  /h_  /  NLLO! MST]\
]\O! I MST]\ I]
 ML O!W IaL_Xc]\Q1IK! D  L VM  !]\QGVMWWh
QTO! WIU!W /4S  MLd\!  MS
XZV\L I MLIQR U!W /QT  A IaVMI]\D ]^X
]_ /E // oMI K! 
Xc]\
eP! I	 Ld\! MS
IK! 
Xc]\O!HS@I]8IaVMjE  RVMIDW MV\LI$C8[AW][ajf [Ah![AW ML
Y M[A I#_CV\ L
IK! G
k 
iJU! ]
K! NL [A]\ O!WNS@ MLO!WI
Xc]\	NLLO!  CP hDIK!  e8O! WIU!W /4VMHS
N[A I	V\LIK!  CMHS
UH]\I /IN VMW! MLO!W IK_C  I  [A]\ 
UH]\ INLLKH VM MS
' HLIOH [AI]\Gd\  /IaLCNLLO!  MSR]\8IK!  

V/hGV
IK! D  C
CMHSGUH ]\I
GIK!  D!  ARI[AW][aj
"

 #"
m]\ IKf IK! D  MLO! WIaL_C WW PH D MV\Sh
CMHS8UH ]\I
HL IOH[AI]\ 8d\ /IaLCNLLO! MS8]\8IK!  
[Ah![AW  Eo!VM 
"
Xc]\V$_C I 
PHV\[aj
R IK! IK! aSR[AW] [ajT[Ah! [AW 
]
KHVMHS W IK! NL LIOHVMI]\ oH V
LQGVMWW_C I PHV\[aj
[A]\I ]\W W  /! / MS!LI]TPH DS  ML d\! MS













K!   MLO!WI]^XbIK! QTO! WIU!Wh$ _CWW%!]
PHV\[aj_CWW% PH  QGVMH VMd\ MS
IK! # _C I 
 / N LK!]_
HLIOH[AI]\ 
UH]\I
VM E T]\?IK!  MCMH S
ZX
V
NLNLLO! MS%o IK!  /

IK! R! ARI
W]\!d\ / 
"

 #"
IK! / NL VM8NLLO!  ]^X_VM
%
IK! DL[aK!  MS O!W /	I]T L / 
[AW][ajR[Ah![AW  EoE IK!  [A]\I ]\W W  /_CWW% [aK! M[aj
"
HL I OH[AI]\ GI]R   EM a
]\8IK! DIK!aSRUH ]\Ib
ZX_!]\ I/o! IK! /8 !]T MLO!WI _CWW PH 
VM !d
Xc]\Q
Xc]\Q*IK!  DQTO! WIU! W /C[/VMfPH 
IK!aSf[Ah! [AW EoH VMHSf IK!  D MLO!WI
IK! T  ET]\ fIK!NLUH]\ I8 IK! 
LV
Xc /Whf]\O!I M S@I]GIK!NL UH]\I
K!  GS MLI IaVMd8]^X
IK! 
HLIOH[AI]\ _CWW KHV/E TI]8PH 
"

 #"
]\O!I MSR I]TIK!  IaVMdTQGVM Ia[aK!!dD W]\d\N[ d\K ICVA_C V/hGL]T IKHVMIU! ]\UH /Ph UHV\LL!dTNLC[/VM MST]\O!I

 






^
q

s
 



^
V
s
 
 
q
 
V

 

 
%
 
	
 
 




	
 
 
'
X
 
 

'
 
G P
IK! 
Xc]\a[A MSR]\8 I]
ZXbIK! /  NLCVMGNL LO! D]\ GIK! $EMaSGUH ]\I/oH IK! /G IK!  QTO!WIU!WhD  MLO! WIC NL
UH]\I/o
I]G /IO! @IK!  RS!VMIaVG]\@IK! 
UH]\I
ZX
VRW ]EV\S@HLI OH[A I]\@NL VMWNL]f VMPH]\O! I
Xc]\O! IK
G\IK
IK! /?V8 [A]\ I ]\WL  d\HVMWNLDL  /ID]\O! ID I]8IK! G4 k:O!! IDI]fV\ Lj@ID I]8K! ]\WNS@IK! T MLO!WI
Xc]\
IK! T! ARI [Ah![AW E oH M LL  /INVM WWh8P!W ][aj! dRIK! T W]EV\Sf MLO! WI
K!  / 
NL!]G  MV\L]\
Xc]\IK! TW]EV\S
_CWW!d\  /I	 NLLO!  MS
 ML O!W II ]Dd\ /IUH /QGVM ! / I WhK! /WN S
O!U oV\ L	VQTO! WIU!Wh HLIOH[AI]\
]\H[A  /E /h
CT[AW][ajG[Ah![AW  ML	PH M [/VM OHL  D]^X_IK! $CT [Ah![AW WNVMI /H[AhE oHVMH SM_C WW%!  /E  /C P!W][ajRIK! W]EV\ S8 MLO!WIgXc]\
QR]\ IKHVM
[Ah![AW   8LOH[/[A MLL ]\

IK! fQTO! WIU!WhfI]@IK!  DEMaSUH]\IRH LI MV\S]^ X
IK! 8 MLO!WIT]^X
Xc]\T ]\O!I !d
K! 8 MV\L]\
!]\ IDKH V/ ! dWE8 : NLLO!  ML
IK! .CMHS@ UH]\ID NL IKHVMIDI K! /  RVM T QR]\ T[aKHVMH[A ML ]^X
V8[AW][aj
[Ah![AW  Eo%VM HS@L ]RQR]\ 
[aKHVMH[A MLIKH VMIIK!  EMaSfUH]\I#_CWW4PH 
Xc / 
aVMIK! /IKHVM@ IK!  CMH SfUH]\I
[A]\QRUH VM MSTI]TVDW]E V\S%o! IK! ] [/[AO! /H[A 
WIK!]\O!d\K8VD K! d\K! /CU! ]\I hTN L	d\E  /RI]R VDQT O!WIU! Wh
]^X
IK! NLDN LE / h@aVM 8 V\LDI K! 8[A]\
N[AI$_C WW !]\I
PH G IK!  / R[/V\L R]^X
CfVMWO?NLLO!  MLAo]\
S!VMIaVT[/V\[aK! 
!]\ICPH  NLLO! MSGVMIC IK!  LVMQR  IQR C ]\CIK! /  NLV
LI ]\  NL	NL LO! MS%o]\VDW ]EV\SRQG V/h
QRNL L
K!  4d\O! 
LK!]_ LIK!  UH /a[A /IaVMd\ ][/[AO! /H[A ]^X_ V$_C I  ]ME /
]_
Xc]\C\VM]\OHL

]\ I E oV
_CI 8]ME  /
PH /H[aK!QGVM j!LTL  QTO!WN VMI MS
LIaVMWW
VMJ
  
Xc]\Q
 / /I
]_6NLTS 
]ME /
]_
][/[AO!aL_CK! /
 ML O!W IaLCVM  /IO!!  MSRP hRIK! D !O!H [AI]\H VMW4  !IaL P! O!IC IK!  D m
KHV\L
LIaVMWW% ][/[AO! aL_CK!  /RIK! D mNL	 !]\I VMP!W  I]T[A]\QRQRI4IK! 
 ML O!W I_C I UH]\Ia LAo_CK!W V
]\!WhG
 ML O!W IaL_Xc]\Q:IK! PH]\ II ]\Q
QR]EL I	P! W] [ajT I]DIK! C  /d\NLI /C4 W #aV\LIK!  /hTVM  !]\I  MV\S h b
K! 
[aK! 
X_ MV\L]\
Xc]\ P!W][aj! dTIK! TW]EV\ SfVMHSf! ]\IDVMh8]\IK! /D  
NLIKHVMIIK!  
Ph UHV\ LL!dRW]\d\N[
HLIOH[AI]\HLCNLLO! MSG ]\@VTPh UHV\L L MS
Xc]\IK! D W]EV\S8N LCI K! D ]\!WhG]\ ! 
IK! 
IKH VMINLC ML /IIaVMP! W  a
QRNLLXbVMHSfKH V\LDVMW MV\S h8PH / /
W]EV\Sf MLO!W ID[/VMfPH T  
NL LO! MSf]\
VG[/V\[aK! 
S MLd\! MS
WNL]!o
IK!NLc_CV\LIK! TL QR U!W  MLI_CV/h8 I]Gd\OHVMaVMI / T IKHVMI!]G  MLO!WId\ /IaLP!W][ajE MS
!I 
Xc]\VM@HS  
VMQR ]\O!I]^X_I QR  

 
 
q
 
 


X
V
 
s
 
C
 
Z
 
s
	
 
	
 

 
  	 








  





	 

]7]8e

EI !d\ /IVM M VG M`EO!  / QR /EIVMHS
VM
VMQTP!I]\OHLCIQR! dRLUH M[A
[/VMI]\@]^X
n
XcO!WW[AOHLI]\ Q6 MVMWnMVMI]\8]^ X
b /]\!QR  / I[AW MVMWhRUH]\I MS8I]RV
WNVMQT P%S!V
fV.]
]
QMO6a

[aK! MSO!W !d
IK! 
 ! I

$%
Xc]\ TVMPHVMH S]\ !!dGI
PH 
]\U!I]\
I MSG]\ O!I V\LVT UH]ELLP! W 
K! T LhIK! MLN LCUHVMI K
VGLQGVMWW / VMH S@U!QR I\ E  /aL]\f]^X
K! T HLI OH[AI ]\?
 [aK!  MS O!W / L M[AI]\f]^X
[A]\QRU!W  /I /W h
IK! HLIOH[AI ]\
HS]_6_CV\LLh IK! MLn/ MSOH L!d IK!  LIaVMH S! VMaST [A /WWE WP!aVMh
V/\VMWNVMP! W _CIK
[AII]E]\WNL 
Eh!LI  /Q
IK! m / jE / W /h
K!   /I L[aK!  MSO!W!d
VMWd\ ]\IK! QF_CV\L	 UHVMH LIaVMj!d\Wh
 MSD DI K!  )
'
LUH M[A
&WN VM!d\OHVMd\ VMHSTLh IK! MLn/ MSDVMHS
]\U! IQRn/ MSOH L! dIK!  C 4

 ML O!W I! dRS MLd\ D_CV\L [A]\ E  /I MSR I]Gef  WNV/hE ]\O!I/oHP!O! I_CV\ L
E /aL ]\
Xc]\O!HS
K! 
I]PH   ARI  /QR  /W hWN VM d\ 
CII /QRU!IaLI]WQRI IaLK!  /d\K I 
]\aS  /	I]
VMWd\
IK_CIK
IK!  MLI	]^X
QGV\S IC /E /TWNVM d\ /CM_CNS IKo!VMH SGIC VMWNL]T  M`E O!  MSR VDW]\IC ]^X_V\S! SI]\HVMW ]\O!I!d
IK! D
!]\I K! /  AR!VMQR U!W  _CV\LI K! i?O!! I
Xc]\ IK!  D
k
 i6_CK! N[aK_CV\ LCVMWNL]TLh IK!  MLn/ MS
OHL!d
IK!  VMPH]M E  U!][A MSO!  Eo P!O!I[A]\O!WNS
!]\ IQR / /I4 IK! LUH / MST  M`E O!  /QR  /IaL
WNL]!oE QR]\ CIKHVM
*
XcIMLI]\aVMd\ T[A /WWNLOHL MSRI]R LI]\ IK! 
UH /a[A /I]^XbIK! D[AO!  / ICS ML d\GNLQGV\S   O!UG]^X_ LK! 
 CVM HS
Xc]\	  MV\ [aKDHLIOH [AI]\

IK! 
 [aK!  MS O!W!d  !I
H S
HVMWWhEo
 %&
% &
(' 	 

G3Q

 

 















q
 
 
s
O
 

G

 
^

C

 

s
 
q
 
 
^
 

P

 
s
]
%
 

VMHSIK! fV/\VMWNVMP! WIh@]^X
IK! 8S  MLd\
IK! fS hHVMQRN[
VMIRIK!  8 /d\O!WNVMRL IOH[AIO! G]^X
W]E]\j !d
[A]\O!aL Eo IK!  GIQR 
XcO!WWC[AOHLI]\ Qd_CV\L
V\S]\U! I MS
aV/h!LAoIK! 8S M[ANL ]\
I]fOHL 
%]E]\jO!U
IaVMjE /R I]
XZV\[AI]\g Xc]\
IK! DL!d\W QR ]ELIWQRI!d
Y M[AIPH MVMaLVDI MLIQR]\hTV\L I]
!NL KRIK! U! ]
XcO!WW[AOHLI]\ Q*S ML d\H L

NLIK! DWN V/hE]\O!IC]^XbIK! D /I D 
[aK!  MSO!W! d

 ! I [/VMU!IO! MS

Xc]\Q-IK!  DefVMd\N[

4d\O! # G
WNV/hE]\O!I_C HS]_





OHL!d@IK! 8efVMd\N[
K! 8  /I 8 
[aK! MSO!W  !df ! IDKHV\LTPH / /S  MLd\!  MS
		  /aL]\
W /E  /WH  MS I]\/o I KHV\LC IO!!  MSR]\O! ICI]
WIK!]\O!d\K8VDE  /hTW]_
WNV/hE]\O!I MS I]\ 
PH D VMG ARI /QR /Wh
E /aLVM IW  CVMHS
OHL /_Xc  /HSWh
k
I]E]\ W
m]\ IKT IK!  
k % MS IVMH S
IK! 	 ]\QRUH V\LL	V/hE]\O!IXC
IK! /TV\S!S I]\HVMWg Xc MVMIO!  MLAoS _ / 
L U!I R]^X
_ / RI  MS%o	VMH S
 MSI]\
Xc]\O! HS?I]fPH 8[AO!Q
PH /aL]\QR  
K! @ [aK! 
 MV\L ]\
Xc]\GL /W M[AI!d
efVMd\N[fV\LRIK!  @S MLd\
 /Ih?I]E]\ W#_CV\ LRIKHVMI
Xc]\O!HShfL O!U! UH]\ Ia L I/o%VMH S8IK!  
e@
 

VMIV
XZVMP! N[/VMI MS8IK! / 
 MLO!WIaVM IS MLd\@ [A]\ O!WNSGPH 
[A]\QRUHVMa VMI E /Wh
W] _ /C[A]ELI
[A MVM I MS_CIK? IK! R K!U]7Q
K! 8 S MLd\
_CV\L
VM 
UHVMaVMQR /I /aL# _CK!N[aK
U!Q
 M[AIE  IaVMHLNLI]\D[aKH VM!!  /W4W /! d\IK8 NL]
QRN [A]\fL[A QR]ELU! ][A  ML L
K!  
QRN[A]\
VMHSGIK!  WNVMQT P%S!V NL M`E O! \VMW / II].]

Xc]\
i
IK! 
QTQRN[A]\o

G
r
 

X

 
 
q
 

 
e
 
E
 
 
	
 
X
 
 
]
 
Q

q
 
]
	
 
 
Z
 
 
s
 
4d\O!  #G

lC
 [aK!  MS O!W!d

 ! IC V/hE]\ O!I

s
]
 
q






sq

K!  d\ /! /aVMWQR / IK! ]S ]\W ]\d\h

Xc]\WW]_ MSRGIK!  WNV/hE ]\O!IU! ][A MLLCNL I /QR n/ MSD PH /W]_l

K!  UH]_ / VMHSGd\ ]\O! HSGW! MLCVM ]\O! I MSGGQR /IaVMW
WNV/hE  / d\]\ ! dGV\[A]ELLCIK! W /!d\IK
]^X
QR /IaVMW:CIKHVMI
Xc M`EO!  / IWhDI /QRHVMI MSD._CNS  IO! !j!L	 ]^X
K!  Ia V\[A  ML VM 
IK! [A  /WWN L
O!
E /IN [/VMW WhIK!]\O!d\ KTIK! 
 [aK! MS O! W! d  ! I
K!  C]\ !Wh AR! [A /U!I]\DNL IK!  CS!SIaV\[A 
R IK! L a[
HS ]_o  _CK!N[aKTO! H LE /IN[/VMWWh
]^XbIK!  HLIOH[AI]\.^
 /WNS
VMHSGLa [ACDS!VMIaV
G QR /IaVMW: CH LI  MV\SR]^X_K!]\n/]\IaVMWWhTG QR /IaVMW
K!  DO!U! UH /UH]\I]\@ [A]\H LNLIaL]^X
_]TUH VMIaL
L !d\W D [A / WW%NLC!]\ QGVMW WhGS NS MSTI]TI
IK! RW ]\d\N[T QRU!W / QT /I /S8IK!]\O!d\K? IaVMH LNLI]\aL
WQR I MSG ]\?IK! RI]\UVMHS
PH]\II]\Q
Ph
UH]_ / VMHSfd\]\ O!HSf IaV\[A ML
K!]\ n/]\IaVMW
W]_  /UH ]\I]\ @QR]ELIWh8[A]\HLNLIaL]^X
K!  
IK! 
]\O!I !dWa
b4]^X_IaV\ [A MLd\ ]\! dTIK! ]\ O!d\K8IK!  D[A /WW
G QR /IaVMW
OHL!dGIK! TQR /IaVMW:C
WWE /IN [/VMW[A ]\ I ]\W	IaV\[A ML@ IK! G 
[aK! MS O!W! dG  ! IDVM RS aVA_C
XcILd\HVMWNL
E /IN[/VMW4LK! 
WNV/ hE / NL! ]\QG VMWWhROHL MS8 I]G]\O!I 
QR /IaVM W:E
K! 
WNV/hE /
Xc]\
IK! 
IK!  LK!
XcI4Ph
Xc]\O!W]\d\ N[\oEVMHS
VMWNL]
Xc]\	V [A]\ O!U! W C ]^X
P!I4W! ML
IK!  e-[A /WWNL
VM /Wh

hWQRI MSbHIK!  CQR /IaVMW:E WNV/hE /4NLOHL MS
U!WNV\[A M LS_CK! / CI K!  ]\ O!I! dDLUH V\[A NL  ARI /QR /W
I]R [A]\!!  M[A IK!]\ n/]\IaVMW4L M[AI]\HL

W WHIK! W ]\!d\ /CW /!d\IKR IaV\[A MLCVM ]^XbQR!Q

O!QB_C NS IKGI]

 MS OH[A IK! /C [/VMUH V\[AIaVMH[A 


 






 





q
 
 
q

q
 


 
 
q
 

 
 
a
 

 
 
	
&+

(-,
+4

	9
8;

&&+ 


$

&65

4VMP!W #G

   



	

"%

$#

'


	


$! " 



! "

*)
('

.*
 
/0

$""


3 )
 	21
('

"

"
7

 %
'
'
"6)
('

"

:
*


'
%" 
"

('
>/
=

 %


<%


"

"6)
 /4 /WNS! L
l	C  /]\aS  / mO[Z

*

6)

S /IaVM W MSGS ML [A U!I ]\8]^X_ IK!  D\VM]\OHL [A /WWNL OHL MSG 8IK! 
S MLd\ f]^X_ IK!  T
[aK!  MSO!W!d
 /CNLC VT /d\O!WNVMWhGLIOH[AIO!  MSGS MLd\
 !ICN LH[AWOHS MSR GIK!NL 
E M[AI ]\
K!  DC /]\aS /mO[ Z
[AOHLI]\ Q
I	 VM SNL[AOH LL MS
]o
dEVMI ML
TIK! C ]\IK! /	KHVMH S%o!V W]\I	]^X

VMHSTVMW WH[A /WWNL4OHL  MS
VM  OHL MST TIK!  HL IOH [AI ]\.^
HS ]_
VMHSRVMGVMII /QRU! I	KHV\LPH / /TQGV\S I]
[A]ME  /V\ L	QGVMh
K! D WNV/ hE]\ O!IU! W]\IaLC]^X_L]\QR  ]^XbIK!  D[A /WWNLVM DVMWNL]TU! ]MNS  MS
]^X_I K! /Q-V\ LCUH]ELL P!W 

 

$



@?



K! 
 /WN S!L ]^X
\VM ]\OHL
]^X
6	]\I / I
[A /WWc a e
VMHS8S ML[A U!I]\ R]^X_ MV\ [aK

 /VMW]\ !dM_CIK8IK!  /H VMQR E oH `EOHVM IIhE oHIhUH 
IK!  
C /]\aS /mO[ Z
S!S  MLLVMP!W Re8 /QR ]\hEo4 e
-VMHS]\Q
[/[A MLL e8 /QR]\hbao
 /WNSR VM D d\E /R84VMP!W #G

s
C








&

&






&

	




8










8







	



4




 
q






 

q
q
P

 
 
 






 
q
 
J1<:

.;$  ))

(cid:100)(cid:97)(cid:116)(cid:97)

(cid:115)(cid:104)(cid:105)(cid:102)(cid:116)

(cid:115)(cid:104)(cid:105)(cid:102)(cid:116)

(cid:115)(cid:104)(cid:105)(cid:102)(cid:116)

(cid:100)(cid:97)(cid:116)(cid:97)

(cid:115)(cid:104)(cid:105)(cid:102)(cid:116)
(cid:105)(cid:110)(cid:112)(cid:117)(cid:116)

(cid:115)(cid:104)(cid:105)(cid:102)(cid:116)
4d\O! #G

(cid:115)(cid:104)(cid:105)(cid:102)(cid:116)
Cl	 mJ
E K!

XcI\
EI]\aVMd\ T	  /WW


EH[A   MV\[aKG / I hR GIK!  /I C  /]\aS  /m O[Z
 /LK! 
XcIaL Ph
Xc]\O!  /E  /hR[Ah![AW 
 AR![A /U!I
Xc]\R
 -LIaVMW WNLXba o /E / hfI hUH R]^ X
XcIDI
VMHS?LK!
Xc]\QGVMI]\
[A /WW	QTOH LI PH 8VMP!W RI]@LI]\ GIaL
S]_CI ]fIK! 
Xc]\O!I K? /Ih@PH /W]_1I
C]\O!I!dfIK! 8S! VMIaVfIK! ]\O!d\KIK!  / G[A /WWNLI]@ MV\[aK
Xc]\O!I K8[A /WWHNLV\ [/[A]\QRU! WNLK! MSDPhT OHL! dDIK!  IK!aSTW /E  /W!QR /IaVMW_C  ML
XcI!U!O!I]^X
IK! L K!
IK! 

EK!
XcI !d
VMHS8LI ]\a VMd\ DVM   PH]\IK8KHVMH SW MS _CIK8V
L! d\W QR] S
 MSRk
U
]\U oHV\ LCLK!]_CG
aLIIaVMH LQRNLL]\fdEVMI 

[A]\I]\W	 Ld\H VMW NL W]_o VMHSf IK! 
]\ QGVMW WhEo!IK!  
4d\O! 
L / IaLO!U@IK! D !U!O!I
Xc]\D LK!
XcI !d
K! /

4d\]E  ML K!d\K o%IK!NLdE VMI TNLIO!!  MS8]
VMHS8IK! 
%R]\O! IU! O!I
(%

 /IO!HLDW]_:?IK! G! ARI
! ARIDdEVMI 8 ]\UH / HL
XcI /(
IK! 
VMHS?O!U%S!VM I ML
Xc / M SPHV\[a jGIaVMHL QRNLL]\8dEVMI T]\UH  /HL I]RQG VMIaVMGIaLLIaVMI E oLH[A 
U!KHV\L Eo%IK! 
IK! D /]\aS /

W]_b_ Xc]\VM8HS /I /QRHVMI  VMQR]\O!IC ]^X_IQR 
PhTjE / /U! !d(
 / [A]\O!WNSGLIa VMWWKa
P!O[Z

s
E
 
a
 
 


	

G
 
C
 

 
^
Z
 

 
LM' '
;$  ))

_CIKR[A]\HLNS /aVMI]\TI] IK!  S! VMIaVD [A /WWo\PH]\IKT
 /	NL	S  MLd\!  MS
K!   /I  C /]\aS /P! O[Z
[A /WWo7_CK! N[aKGVMWW]_L	PH ]\IKG MV\S !d
VMHS._CI!d
EDNL	IK!  S!VMIaV
Ln/  VMHSR RI QR !d
4d\O! G
 /d\ KIDS 
 / /IUH ]\IaL
	V\ [aK@ UH]\IDKH V\L IaL]_C
IaVMHLNLI]\
I]G IaLL I]\aVMd\ G [A / WWIK!]\O!d\K
%\o1 _CK!W 
%
!]\O! UH]\IaLDV\[/[A MLL( (%
Xc]\O! UH]\ IaLDV\[/[A MLL
]\IK!  /
IK! 
8 [A]\I]\W	Ld\ HVM W
VMHS
	]\HL  M`EO! /IW hEo / d\K I]\UH /aVMHS!L QGV/h8 PH T MV\S
acXc]\ O!
Xc]\Q
VMHS
Xc]\O! 
Xc]\ Q
bao


CVMHS8[A]\ MLUH]\ HS ! d
 MV\[aKG MLO!WIC]\
Xc]\O!C ML O!W IaLQGV/hTPH $_C II /Da
VMHS

(cid:83)(cid:104)(cid:105)(cid:102)(cid:116)(cid:32)(cid:73)(cid:110)(cid:112)(cid:117)(cid:116)

(cid:83)(cid:104)(cid:105)(cid:102)(cid:116)(cid:32)(cid:69)(cid:110)(cid:97)(cid:98)(cid:108)(cid:101)

(cid:51)
(cid:32)
(cid:116)
(cid:105)
(cid:66)

(cid:50)
(cid:32)
(cid:116)
(cid:105)
(cid:66)

(cid:49)
(cid:32)
(cid:116)
(cid:105)
(cid:66)

(cid:48)
(cid:32)
(cid:116)
(cid:105)
(cid:66)

(cid:83)(cid:104)(cid:105)(cid:102)(cid:116)(cid:47)(cid:77)(cid:101)(cid:109)(cid:111)(cid:114)(cid:121)
(cid:67)(cid:101)(cid:108)(cid:108)

(cid:48)
(cid:32)
(cid:116)
(cid:105)
(cid:66)

(cid:49)
(cid:32)
(cid:116)
(cid:105)
(cid:66)

(cid:50)
(cid:32)
(cid:116)
(cid:105)
(cid:66)

(cid:51)
(cid:32)
(cid:116)
(cid:105)
(cid:66)

(cid:100)(cid:97)(cid:116)(cid:97)

(cid:100)(cid:97)(cid:116)(cid:97)

(cid:77)(cid:97)(cid:116)(cid:99)(cid:104)(cid:32)(cid:48)
(cid:77)(cid:97)(cid:116)(cid:99)(cid:104)(cid:32)(cid:49)
(cid:77)(cid:97)(cid:116)(cid:99)(cid:104)(cid:32)(cid:50)
(cid:77)(cid:97)(cid:116)(cid:99)(cid:104)(cid:32)(cid:51)
(cid:77)(cid:97)(cid:116)(cid:99)(cid:104)(cid:32)(cid:52)
(cid:77)(cid:97)(cid:116)(cid:99)(cid:104)(cid:32)(cid:53)
(cid:77)(cid:97)(cid:116)(cid:99)(cid:104)(cid:32)(cid:54)
(cid:77)(cid:97)(cid:116)(cid:99)(cid:104)(cid:32)(cid:55)

(cid:83)(cid:104)(cid:105)(cid:102)(cid:116)(cid:32)(cid:79)(cid:117)(cid:116)(cid:112)(cid:117)(cid:116)
El kVMIaVR	  /WW
4d\ O! G

s
G
 
 
Z
 

	
 
"


 
 

 


 

 


b
 
 
s\s

;$'5

;$ ) )

Xc]\O!
UH]\Ie*[A / WWoELK! ]_C RG 4d\O!  cG
G!o NL	OHL MS
Xc]\C MV\S ! dT]\ _CI!d
I]\G /I  ML	]\ IaVM dT / I  ML	QGVMIa[aK
(cid:83)(cid:104)(cid:105)(cid:102)(cid:116)(cid:32)(cid:73)(cid:110)(cid:112)(cid:117)(cid:116)

(cid:83)(cid:104)(cid:105)(cid:102)(cid:116)(cid:32)(cid:69)(cid:110)(cid:97)(cid:98)(cid:108)(cid:101)

I]TS /I /QR!  _CK! N[aKRS  MLIHV

(cid:51)
(cid:32)
(cid:116)
(cid:105)
(cid:66)

(cid:50)
(cid:32)
(cid:116)
(cid:105)
(cid:66)

(cid:49)
(cid:32)
(cid:116)
(cid:105)
(cid:66)

(cid:48)
(cid:32)
(cid:116)
(cid:105)
(cid:66)

(cid:83)(cid:104)(cid:105)(cid:102)(cid:116)(cid:47)(cid:77)(cid:101)(cid:109)(cid:111)(cid:114)(cid:121)
(cid:67)(cid:101)(cid:108)(cid:108)

(cid:48)
(cid:32)
(cid:116)
(cid:105)
(cid:66)

(cid:49)
(cid:32)
(cid:116)
(cid:105)
(cid:66)

(cid:50)
(cid:32)
(cid:116)
(cid:105)
(cid:66)

(cid:51)
(cid:32)
(cid:116)
(cid:105)
(cid:66)

(cid:100)(cid:97)(cid:116)(cid:97)

(cid:100)(cid:97)(cid:116)(cid:97)

(cid:77)(cid:97)(cid:116)(cid:99)(cid:104)(cid:32)(cid:48)
(cid:77)(cid:97)(cid:116)(cid:99)(cid:104)(cid:32)(cid:49)
(cid:77)(cid:97)(cid:116)(cid:99)(cid:104)(cid:32)(cid:50)
(cid:77)(cid:97)(cid:116)(cid:99)(cid:104)(cid:32)(cid:51)

	 /WW

(cid:83)(cid:104)(cid:105)(cid:102)(cid:116)(cid:32)(cid:79)(cid:117)(cid:116)(cid:112)(cid:117)(cid:116)
G!l e
4d\ O! #G
Xc]\R[A]\Q
]\TIaVMd@OHL MS
W ! GNLD !INVMWWh@S NL[aKHVMd\ MS? VMHS? IK! G  /d\NLI /
	V\[aK?QGVMIa[aK
Ia LD[A]\  MLUH]\ HS !d-)$CVMH S
]f /\VMWOHVMI E o MV\[aK@QGVMIa[aK@W! 
W! ML
UHVMNL ]\?NL U!O!I
]\
IaVMHL NLI]\ RIKHVMIG VMII /QRU!IaLDI]?[aKHVMd\ 8I
ZX
VMWW P!IaL
QGVMIa[aKo IK!  /
VMWW
OHL MLT VM
V\[AI E GU
QGVMIa[aKGW!  $_CWW[aKHVMd\  
IaVMHL NLI ]\a L fIK! T e
IK! 

[A /WWNL_CWWPH T ]\UH /@VMH SfIK! 
IK!  ]\IK! /CKHVMHS%o!
]\! ]\CQR]\ P! IaLS ]T!]\IC QGVMIa[aKoE IK! /R IK! 
IaVMHLNLI]\aL_C WWHU! O!WW%IK! 
QGVM Ia[a KDW ! C[A W]EL CI]d\ ]\O! HST LH[A C IK! CIaVMHLNLI]\	 dE VM
IaVMHLNLI]\aL
	S]_C
IK! CU! O!WW
XZV\[AI]\ ]^X
NLL
RGI QR MLIKHVMI]^ X
IK! 
U!O!WW
O!U8 IaVMH LNLI]\
 L!dRIK!NLCQR /IK!]S% oH]\ !Wh8 / I MLCIKHVMI
QR /IK! ]SGNLOHL MS%o
XgVTU!  M[aKHVMd\ 
WI /HVMIE  /WhEo 
QGVMIa[aKGW! ML
QGVM Ia[a KW_C WW[aKHVMd\  DO!U@I K! 


	
 
	
 
 
	
 

 
	
 

	
 
X
	
	

q
Q

 
 
[A /WWoVMHS
Xc]\R MV\[aK
IK! /
V\S!SI ]\HVMWU!  M[a KHVM d\ f[A]\I]\WIaVMHLNLI]\aL _]\O! WNSPH  f M`E O! MS
_ / /TIK! 
QR]\   QR UH]\ IaVMIW hEo VM8 V\S!S I ]\HVMWLh H[aK!]\!nMVMI]\G LI /U._ ]\O! WNSR PH  M`E O! MS
PH /I
S MLI HVMI]\ @e
Xc]\QR /C QR /IK!]S
Xc]\ Eo _ D[aK!]E L D IK!  
K!  / 
[A /W WNLCVMHSGIK! DW]E]\jO! U@[A /WWNL
VMII K! D[A]EL IC]^X_UH] _ / [A]\HLO!QRU!I]\

1/1=

;$ ) )

V?L]\O! a[A f /d\NLI /RNS /I
 /
e8]\ f IKHVM
S MLI HVMI]\ Je
]\! 
[A /WW[/VM
QGVMIa[aK
IK! DQR]E LI[AO!  / IS  MLIHVMI]\
K! D\VMW O! D ]\Ia VMdGS  ML   M S8[A]\QR ML_Xc]\Q*IK!  
[A /WW%QGVMIa[aK!! d
NS /I 
WWH]\IK! / QGVMIa[aK!!d  /I ML
 /
IK!  /]\ aS  /	P!O[ Z
 //o7_CK!N[aKTN L[A W]EL  ML II ]DIK! I]\UG]^X
Xc]\ Eo V8[Aa[AO! IKH V\LPH / /
PH /W]_
ID QTOHLI PH RSNL [aKH VM d\ M S
K!  / 
S MLd\! MS@IKHVMID UH /
Xc]\QGL
IK!  O!QTPH  /	 ]^X_  /I MLAo$ 4 oHVMH SG M`E O! ML	Wd
IK!NLIaV\L jR8 [A]\ HL IaVMII QR c _C IKG MLUH  M[AIC I]
LUHV\[A 

(cid:108)(cid:111)(cid:111)(cid:107)(cid:117)(cid:112)(cid:105)(cid:45)(cid:49)(cid:44)(cid:52)

(cid:108)(cid:111)(cid:111)(cid:107)(cid:117)(cid:112)(cid:105)(cid:45)(cid:49)(cid:44)(cid:51)

(cid:108)(cid:111)(cid:111)(cid:107)(cid:117)(cid:112)(cid:105)(cid:45)(cid:49)(cid:44)(cid:50)

(cid:108)(cid:111)(cid:111)(cid:107)(cid:117)(cid:112)(cid:105)(cid:45)(cid:49)(cid:44)(cid:49)

(cid:108)(cid:111)(cid:111)(cid:107)(cid:117)(cid:112)(cid:105)(cid:45)(cid:49)(cid:44)(cid:48)

(cid:112)(cid:105)(cid:44)(cid:52)

(cid:112)(cid:105)(cid:44)(cid:51)

(cid:112)(cid:105)(cid:44)(cid:50)

(cid:112)(cid:105)(cid:44)(cid:49)

(cid:112)(cid:105)(cid:44)(cid:48)

(cid:112)(cid:105)(cid:44)(cid:52)

(cid:100)(cid:105)(cid:44)(cid:52)

(cid:100)(cid:105)(cid:44)(cid:52)

(cid:112)(cid:105)(cid:44)(cid:51)

(cid:100)(cid:105)(cid:44)(cid:51)

(cid:100)(cid:105)(cid:44)(cid:51)

(cid:112)(cid:105)(cid:44)(cid:50)

(cid:100)(cid:105)(cid:44)(cid:50)

(cid:100)(cid:105)(cid:44)(cid:50)

(cid:112)(cid:105)(cid:44)(cid:49)

(cid:100)(cid:105)(cid:44)(cid:49)

(cid:100)(cid:105)(cid:44)(cid:49)

(cid:112)(cid:114)(cid:101)

(cid:109)(cid:97)(cid:116)(cid:99)(cid:104)

(cid:112)(cid:105)(cid:44)(cid:48)

(cid:100)(cid:105)(cid:44)(cid:48)

(cid:100)(cid:105)(cid:44)(cid:48)

(cid:108)(cid:111)(cid:111)(cid:107)(cid:117)(cid:112)(cid:105)(cid:44)(cid:52)

(cid:108)(cid:111)(cid:111)(cid:107)(cid:117)(cid:112)(cid:105)(cid:44)(cid:51)
4d\O! #G

(cid:108)(cid:111)(cid:111)(cid:107)(cid:117)(cid:112)(cid:105)(cid:44)(cid:49)
(cid:108)(cid:111)(cid:111)(cid:107)(cid:117)(cid:112)(cid:105)(cid:44)(cid:50)
l	% ]E]\ jO! U
	 /WW%GIK!  

(cid:108)(cid:111)(cid:111)(cid:107)(cid:117)(cid:112)(cid:105)(cid:44)(cid:48)
IK8C]_

:Wd4o
% /I
IK! TL [aK! /QGVMIN[
Xc]\IK! 

oVMHS
] !
IKf /IhaLIaVMI! d

] !
Xc]\Q*IK!  

I]\U

ZX

E7C o	4 d\O! G
LK!]_L
fIK!  TL[aK!  /QGVMIN[\oIK! / TVM 

s
e
 
 

 

 
 
 
 
s
 


!
!

 
q

!
!
 
 
q
 


 
s

b
 
IK! \ VMNVMP! W   /\VMWOHVMI MLI]TVD\VMWO!  ]^X
]o! IK! /GIK! 
ZX
[Aa [AW MLS_C IK8 V
\VMN VMP!W  HLN S  IK! /Q
_CNL Eo INLC VM8]\UH /@ [Aa[AO! I/o!VMHSGIK!  D[A]\!! M[AI!d
IK! /
[Aa [AW   /U! ML  / IaLCVTLK!]\I[A a[AO!I
b4VMHS "$aS NL[aKHVMd\ 
K!  ML C\VMNVMP! W MLAo
Ia VMHL NLI]\ ! / MS !]\ IPH U!  ML /I
U! M[aKH VMd\  W]E ]\j O!U
Xc]\W W]_C!d
W]E]\jO!U
baoH[A]\
Xc]\ Q6I]
IK! 
 M`EOHVMI]\ HLAl

VMHS

LM :%

 .;$  ) )

 

 




QR] S

%QR ]SC

	

4d\O! 
eRL K!] _LI K! 
 MV\S@VMHSW_CI TSE /aLCOH L MS8f IK! TS  MLd\
Xc]\DS  !dTIK! 
%
%
"AL d\HVMW
K! 
"/Ld\H VMWE  /U! ML /IaL%V[A]\I]\WELd\H VMWIKH VMI]\UH  /HL O!UIK! 
"
 
"

A_CI ]\UH /aVMI]\
Xc]\	  MV\S
[]_
[A]\  MLUH]\HS!dDUH]\ IaL]\GVM WW!IK! S! VMIaVD[A /WWNL TIKH VMILUH  M[A

match_dest

read

match_local

match_tag

write

write_b

read_b

READ DRIVER

WRITE DRIVER

el kE  / 	 /WWNL
4d\ O! #G
 /IK! /CIK! 
XcO!H[AI ]\fV\LVTL! d\W 
SNLIP!O! I MSRQTO!WIU!W ARE  //oS ! d
_]RS E / aL
K! DI
%
%
'RLd\HVMWcaSO! ! dW_CI TU!KHV\L 7b
L d\HVM WaSO! !dG  MV\S@U! KHV\L 7b]\ IK! 


%



s
P
 
 
 
 
a
 


C
"



q

!
G
 

%
 

%

 
 
"

IK! 

%
%
"ES NL[aKHVMd\ MSTSO! !dD ]\IK!  /U!KHV\L ML
"EL d\HVM Wo!VMH STjE  / /U
]\I]D IK! 


"

$"R VM D! / MS  MSRI]RV\ LLO!  $_C K! /8]\! 
#VMHS
IaVMHL NLI]\ aL[A]\ I ]\ WW MSRPh
K! D ARIaVD 
O!U %
o\IK!  	]\IK! /S ]E ML4! ]\I VMII /QRU!I% I]U! O! WWI4 S ]_C
SE / %N LI h !dCI]U! O!W W
K! 
"

OHL !d IK! CSE /aLH DIK! NL [A]\
QGVY]\ 4V\S\VMIaVM d\ ]^X
d\O! aVMI]\
NLIKHVMI4]\! WhIK!  
IaVMHLNLI]\aL
8I K! 
HVMW	L IaVMd\ T VM  D fL / M L
f MLL /H [A Eo! IK! 
XcO!H [AI]\ 8]^X
INLIaVMI!dR IK!  DU
IaVMHLNLI]\
?IK!  
I]@IK!  GHVMHS?dE VMI EoC V/E ]\NS!df IK! 8 O!!!  M[A MLLVMWh@KO!d\ 
IaVMHLY Xc / MS
HVMWL IaVMd\ 8N L
_]8L / ML U
Ln/  T M `EO!  MS
Xc]\D IK! RI
IaVMHLNLI]\aLD
]\aS  /DI]8QGVM IaVMfIK!  GLVMQR R S!d
NS /IN[/VMW Ln/ ]^X_ IK!  HS 
K!NLVMWN L]T  /H LO!  MLCIK!  /WNVMIE  /Wh
[/VMUHVMP! WI h
OHL]\f /d\]\8VMHS
IK! U%S
OHL ]\8  /d\ ]\8  MLO!WI !dD8V
E /hR[A]\QRUHV\[AICS MLd\ 
Xc]\IK!  KO! d\ DS E  /aL

s
Q

%

%
 
	
 
)


)
#
	

%
"
 

	

 
	

	
 
Z
Z
 
s\r

 







LM' '-3 ))

4d\O! # G
LK!]_LCI K! D L[aK! /QGVMIN[C]^X_ IK! D LK!
XcIMLI]\aVMd\ T[A /WW%OH L MSRGIK! D HLIOH[AI]\
IK! D IaVMHLQR NLL]\8dEVMI MLOH L MSGfIK!  
HS]_
mo
XcIMLI]\aVMd\ R[A /WW 8IK!  
LK!
HLI  MV\S8]^X
L /  MLIaVMHLN LI]\aL VM  OHL  MSG I]
U!]MNS PH  /II /L
_CIa[aK! ! d

write_it

shift

shift

shift input

data

shift

shift

shift

shift

data

PEl 


EK! 

4d\O! #G

XcI\ 
EI]\aVMd\ T	 /WW
IK! "
]\QGVMWW hTVTS! VMIa VT[A /W W%N L_C II /GP hR U!WNV\[A!d

TVMH S
VMHS

T]\8IK! 
IK! D8UH V\LLIaVMHLNLI]\aL
W! ML]^X_IK! 
[A /W WVM HS8 / HVM P!W  !d
RNL_CII /8 PhGS NL[aKH VMd\!d
IK! DLI]\ MS
NL_CII /R PhGS NL[aKH VMd\! d
IK! DL I]\ MSR\ VMW O!  IK! ]\ O!d\ K8IK! 
	W! c_C K!W DV
\VMWO!  IK!]\O!d\K8 IK!  
W! 

K! 

IK! GI QR 

VM _C II /@I]G]\! Whf]\H[A T@IK!  /W
 /WN S!L]^XgIK! R
VMHS 
Xc]\TV8!  A_:H LIOH [AI]\ oI]f IK! R IQR 
VM G[A  MVMI MS
IK! /h
IK! /h@d\ /I

Xc 

IQR 
LK!
XcI MS

Xc]\Q



 
 



 
 
P
^
 
 
 
^


"


 

 


 

 
]
 

 
q

 

 

^
	

e7]
]\O!I
Xc ]\Q
SNL [/VMaS MS
VMHS
Xc MVMIO!  R KHV\L PH / /@OH L MS@I]f VM?V\S\VM IaVMd\ M_CK!W 
IK! G
K! NL
S MLd\ !!dTI K!N LS!VMIa VR[A /W W
! A_ / IhRNL[A MVMI MS% oH V
RNLCLK!
XcI MSG8I]TIK! 
E /h IQR  V
P!Ia LC]^X_IK! 
S!VMIaV
 /WNS
_CWWKHV/E DV
T!  / MS!LI]TPH  
] _J]\! WhGV
$	& o%V\LC IK!  D[A /WW

K!NLQR MVMH LIKH VMIIK!  
\VMWO! ]^X
PhD S 
XZVMO!W I
%UH ]\IaL	VM ! ]W]\! d\ / M`E O! MS
]VMWW]_
[/V\L . _C I T]^ X
IK! ._]\aLI
8I]
Xc]\O! 
 /I MLPh@IK! G LVMQR  T MLO!WI/o IK! GL / ML
$	
$
S! S?SO! !dfIK!  ._CI G[Ah![AW 
IK! RUH VMIK
Xc]\ Q
]\I!d
I]fI O!?]
Ia VMHLN LI]\TN LOHL MS
IKHVMIIK! $_CNSIK8]^ X_I K! 
_]
NLIK!  
[AIN[/VMW SQR /HL]\GGIK!  
S MLd\oH I
WNV/hE  /aL]^X_fVMHS
Q3b I]T [A MVMI D VD /WNVMIE /Wh
UGI aVMHL NLI ]\aLVM   OHL MSDa 4 d\O! #G
IK!8[A /WW
 
4datam.cif scale: 0.100000    (2540X)     Size: 30 x 50 microns      
 

   


					 			 			 			 			 			 			 			







		
 
 



   
					 			 			 			 			 			 			 			







		
 









 





						 			 			 			
		
		 			 			 		 		




             
 
                 




 

						 			 			 			
		
		 			 			 		 		




             
 
                 
     




 





             
 
                 
     








 




		 	




		
		 	
		
 
 
 
 
			
		 			 			 		 	
             
 
     
                 








 




		 	




		
		 	
		
 
 
 
 
		 			 			 		 	
			
             
 
                 
     





		 	




		
		 	
		
             
 
                 
     






		 	







		
		 	
		
             
		
 
                 
     



		 		




  
 
  

		 	







		
		 	
		
             
		 	
		
 
                 
     


		 		




  
 
  

		 	




		
		 	
             
		 	
		
 
                 
     







  
 
  






		
		 	
             

		 	
		
		
		 	
 
                 
     




 
 

		 			




 



		
		 	
             




		 	
		
		
		 	
 
 
		 	
                 
     



 
 

		 			




 



		
		 	
             

		 	
		
		
		 	
 
 
		 	
                 
     


 
		






 



		
		 	

             
		 	
		
		
		 	

		
   








 



		
		 	
		
		 	
		 			 		
             
		
		
		 	
		 	
  


   
   










 

 



		
		 	
		
		 	
		 			 		
		
		
		 	
		 	
  
   
   
  









 

 



		
		 	
		
		
		 	
		 	
  
   
   
  









 



		 	


 



		
		 	
  
 
		
		
		 	
		 	
		
  
   
   
  









 



		 	


 



		
		 	
  
 
		
		
		 	
		 	
		
  
   
   
  






		 	


 


		
		 	
  
   
   





		 	


 


		
		 	
  





   
   




		 			 			 		 	
		 	

  
 
 

 



		
			 	
		 	
		
  

   




   
   




		 			 			 		 	
		 	

  



 

 

 



		
			 	
		 	
		
  

 
   
   
   




		 			 			 		 	
		 	
		 	

 



  

 
   
   


   


						 			
		 			 			 		 	
		 	
		 	
		 			 	

 



  
   
   


		 	
						 			
		 			 			 		 	
		 	
		 			 	


 


   
   





 

						 			 			 			 			 			 			 	
   
   




 


 
		 	
 

						 			 			 			 			 			 			 	
   
   



 



 
		 	

   
   



 




  
  







   
   




 


						 	
		 			 			 	
		 			

  
  







   
   





 

						 	
		 			 			 	
		 			

  
  
   
   





 


   
   
 
		 	
						 			 			 			 			 			 			
		




 



  

   
   
 
		 	
						 			 			 			 			 			 			
		




 


   
   




 

  

			 			 			 			 			 			 			 			 	
		
 
   
   



 



			

  

			 			 			 			 			 			 			 			 	
		
 
   
   




 


			


 
   
   



 





  
  
 
   
   
		 			
		 			 			 	

 
		 			 	


 





  



  



   
   
 
		 			
		 			 			 	

 
		 			 	


 





  
  
   
   
 


 






  

			
							 			 			 		 		 		 	 		 	 		 	
 
   
   



 



  


							 			 			 		 		 		 	 		 	 		 	
			
 
   
   



 
 


 



 


Ql kVMIaVR	  /WW V/hE ]\O! I
4d\ O! #G

^
 
 
 
q

 
 
]
$
 
q
 
 

 
V
 
]
$
U
	
Z
 

^
 
 

































































































	
	
	
	
	
	
	
	
	
	
	
	
	
	
 
' -<

P!W] [ajTVM QGV\S O! UT]^Xb VDLQR U!W 
K!  WNVM Ia[a K! MLOH L M S
R IK! C MV\S h
VMHS
V\L	LK!]_CTD IK! 4d\ O!  ML G
IK! 
VMHS
IK! 
m]\IK

 M[AIaLIK! D[aKHVM!d\!dT!U! O!I _CK!  /GIK!  
K! ]\O!IU!O!I]^X_IK! D [A / W WH  




]IaVMHLNLI]\W]\d\N[
Ld\ HVMWNL	VM C!  / MS MS
NLK!d\ K

latch

latch

latch input

latched data

latch

latch

l	 aVMHLUHVM /I VMIa[aK@
[aK! /QGVMIN[
4d\O! # G

irdy_lat1.cif scale: 0.100000    (2540X)     Size: 23 x 32 microns      
		 			 			
		 	
		 			 			 		


		 	
		 			 			
		 			 			 		

        
       

		 	
		
        
       

  
		 			 			 		
		 			 		
		 	
		
              
        



       

  
		 			 			 		
		 			 		
		 	
		
              
        
       


		 	
		
              
        
       



  
		
		 	
		

              
        
       



  
		
		 	
		


              
        
		 	
       




  
  
		
  
		 	
		


              
        
		 	
       




  
  
		
  

		 	
		


              
        
		 	
       




  
  
		
  
		 	
		


              
        
		 	
       




  
  
		


              
        
		 	
       


		



		 			
              
        
		 	
       


		



 
		 			
              
        

		 	

		

 


		 	


		


 
   


		 	

		 			 		
  


		


 
   


		 	

		 			 		
  

		

 
   


		 	
  

 
  
		

 
		 	
		
   


 
  

		 	
  

 


  
		

 
		 	
		
   


 
  

		 	
  

  
		

 
		 	
		
   


  


 
		 	
		
   

  

 
 
 
		 	
		 			 			 		
		
		 			 		
   

  



 
 
 
		 			 			 		
		 	
		
		 			 		

  

 

  
		 			 			 		
		 			 			 	

 


		 	

		 			 			 		
		 			 			 	

 


		 	

 



 
		 			 			 		 			
		 			 	
		



		 			 	
		 			 			 		 			
 
		
]l aVMH LUH VM / IVMIa[aK8 V/hE ]\O! I
4d\O! #G

e
q
q
 
r
G
 
q
]
 








 





 
 
r
 
 
 
 
 
 
 
 






































	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
 
q
e7C

 





 


1/1=

,.'



LCLK!]_C8RI K!  P!W ][a jRSNVMd\aVMQ
Xc]\C IK!  H LIOH[AI]\f
 [aK! MS O! W /a4d\O!  #E
bao!IK! 
q/s
E7CC]_LAoM MV\[aK IK!  LVMQR 4 K! /d\KI4 V\L% IK! 	 [A]\I]\WVMH SS!VMIaVCUH ]\I]\HL
S MLd\N L[A ]\QRU! NL  MSC]^X
]^X
IK!  fHLI OH[A I]\ 
HS]_
L
IK! f% ]E]\j O! UJ aV/h? W]\ d\N[8VMWW]_L
WQR I MS@VMQR]\ O!IT]^ X
]\O!I !dTLUHV\[A  K!]\ n/ ]\ IaVM WWhEoVMW WIK! % ]E]\j O! Uf aV/h! LCVM  P!O!H [aK!  MST I]\ d\ /IK!  / VMICIK!  [A /I /
]^X
IK!  8

K! / h
VM 8 LQR WNVM I]fIK! R]\ ! MLDOHL MS
?IK!  GC /]\aS /TmO[ Z
 //oP! O!I
U!K h!LN[/VMWWh
HS8 IK!  ]\WNS MLIH LIOH[AI]\ 
E / I MSRV\L!] _JIK! /gXcO!H[AI ]\8 NLCI]
\
 
4 k
K!  
LUH M[AN VMW4%]E]\jO!U@ aV/h!LC[/VMhR]\O!ICIK!   E  /aL 
XcO!H [AI]\8V\LC IK! /hG V\LL /IVMWW Ld\H VMWNLVMPH]ME 
IK! R]\WN S MLID HLI OH[A I ]\
aV/h!LNL
IK! G% ]E]\ jO! U
]\!  G]^X
eMIK[A /WW	
IK! 
K! G WNV/hE]\ O!ID]^X
LK!]_C8 84 d\O! # G
q\q
sct16.cif scale: 0.100000    (2540X)     Size: 32 x 35 microns      
                        
     
     
                        




     
     
 
 
 
 
								 			
		 			 			 			 			 			
                        






		 	

 




     
     
 
 
 
 


								 			
		 			 			 			 			 			
                        






		 	

 




     
   
     
     
 
 
 
 
								 			
		 			 			 			 			 			
                        



		 	
     





     
   
     
     

 
 
 
 

                        



		 	
     

		 	




     
   
     
     

 
 
 
 
 

		 	
		 	
		 	
		 	
                        


 

 





					
		 	
		 	




     
   
     
     

 




 
 
 
 

		 	
		 	
		 	
		 	
                        


 
 





		 	

					
		 	




     
   
     
     

 
 
 
 

		 	
		 	
		 	
		 	
                        


		 	








     
   
     
     


		 	
		 	
		 	
		 	
                        


		

		 	
				








   
     
     
     



		 	
		 	
		 	
		 	
                        


		

		 	
				




   
     




		





   
     





		







   
     


		
		 			


		

		






   
     



		
		 			


		

		




   
     
		


		






   
     
		



		 		
		






   
     
		



		 		
		





		







		
				







		 			 			 			 			 			 			




		




				







		 			 			 			 			 			 			




		
				











		
				
		 	
		 	
		 	
		 	
           











		
				
		 	
		 	
		 	
		 	
           















		
				
		 	
		 	
		 	
		 	
           








 
 


		
				
		 	
		 	
		 	
		 	
           













 
 


		
				
		 	
		 	
		 	
		 	
           








 
 


		
           



 

 

 

 

 

 
 


		
           




 
 
 


		
					 			 		 			 			 			 			 			 			
           










 
 
 


		
					 			 		 			 			 			 			 			 			
           



           



           
e3b

	 /WWV/hE]\O!I$a
l% ]E]\ jO! U
4d\O! #G
q\q



 


 
^
 
 

 
 
q
 
 










































	
	
 
q
e7E

'D;$ )) 

e8]S
 MSfE /a L]\@]^X
IK! RS!VMIaV8[A /WWNL OHL MSf@IK! RHLIOH [AI]\?
[aK!  MSO! W /I]fLI]\ 
HLIOH[AI]\
IK!  \VM]\OHLHL I OH[A I]\ 
IhUH  [A /WW!KHV\L
Xc]\ IK!   
[A /WW
K!  S!VMIaV
IhUH  P!IaL
IK!  / R  MV\S@UH]\IaLTV\ LDI K! R HLIOH[AI]\?[A]\O!WNS@ PH RNLLO! MS@]\VMh@]^X
IK! 
aLI
IK! / TUH]\IaLAo
O!U? ]\!Wh
K!  GUH]\ IaL
_CK!W R IK! R]\IK! /T S!VM IaV@ [A /WWN LKHV/E 
VM G]\UH /! MS
UH]\I
]\!  G MV\S
YOHLI
SO!!dT IK! #G\IK8U! KHV\ L E o%V\ LCIK! D I /H VMW%QG VMIa[aKR W! ML _CWW%PH  
[aKHVM!d\! dTIK!  / LIaVMI 
SO!!d
IK! DL [aK! MSO!W !dD U!KHV\ L ML

N# -<'  ;$0 )c;$  )) 

[A /WWHNL	IK! [A]\ MLUH ]\HS !dRi  M[aKH VMd\ D 	]\ I]\W [A /WWH IKHVMI[A]\
W]\!d$_CIKG  MV\[aKRS!VMIa V
MSNL [aKHVMd\!d]^X
I ]\WNL4IK! C[a KHVMd\ !d
IK! ! U!O!IaLI]IK! C% ]E ]\jO! URaV/h!L
WWVM E /hLQRU!W 
 :i M[aKHVM d\ R	]\ I]\Wo1 _CK! N[aK8!  / MS!L [A]\QR U! W ARR W]\d\N[D I]R OH L TIK!  
Xc]\ IK! 
 AR![A /U!I
 MLO!WI
]^X_]\ ! DL
_ / /UG]^X_L[aK! MSO!W !d
I]T[A]\ I]\WIK!  !  AR I

   



K! 8 
[aK! MSO!W  !df  ! IO!IWn/ ML VWG
]7]f e
U!KH V\L 8[AW] [aj@ O!! ! !df VMI
WW	IK! G[A]\I]\WCLd\HVMWNLD VM Gd\ /! /aVMI MS@ ]
U!KHV\L G]^X
HLDSO!aVMI ]\
QGVY]\  /E /IaL][/[AO! !dG @I K! G C /]\aS /D mO[ Z
HLIOH[AI]\^
 /DVMH S
U!KHV\L 
VM  WN LI MST 84VMP!W # G
HS ]_*VM fO! U%S!VMI MS
_CIKIK! 
IK! fHLIOH[AI]\
IK! fC /]\aS /GmO[Z
m]\IK
 /RVMH S
_CK! W RIK! GWNVMII /D ?U!KHV\L 
IK! 8!O!H[AI ]\HVMWC !IaLAo IK! 
 ML O!W IaL
Xc ]\Q
Xc]\QR / ?U! KHV\ L 
Xc]\IK! $_CI 
W! ML
U!KH V\L DI]RL /IO!U8IK! 
HS]_J! / MS!L VM8 AR IaV
K! D HLIOH[AI ]\W^

nE o_CIK? MV\[aK
IK!NLD[AW] [aj
K! 
HS ]_1SO! ! d8 MV\[aK

J

	
 
	

 
 
	
 
 



	
q
C
 
s
 
Z
 
 
C
 
^
q
C
 
 





 
 %
"
6 =
.
	
=




""



eG

"*

$ %

""



.%* 

6=
"
"
6=
=

%


.%

.
	

(

 -1
=

 

 . 

'6

ClC 
[aK! MSO! W! d
4VMP!W #G
 /KH V\LC IK!  WO R O!hT ]^XbPH ]\IK
K! DC / ]\a S /mO[Z
VMHS
]\UH /aVMI ]\
	UH ]\IaL VMH S8S]E  MLC!]\I
Xc]\O! IKRU!KHV\L Eo
K!  H LIOH[AI]\H L	VM NLLO! MS

IK! 
Xc]\Q:IK!  
 ARIaV L /I O!U
 M`EO!  VMh
VMHSRIK!  ]\UH /aVMHS!L_CW WHKH V/E I]DPH WNVMIa[aK! MS
RPhTIK!   L VMIC IK!  NL! dD MS d\ ]^X
U!KH V\L 

 ! IC QR!d

 

$



@?



4VMP!W 
4d\O! 
U!KHV\L 
H[A WOHS!dDIK!  #_]\ aL I

 /SO! !dR MV\[aK
ERL K!] _LI K! D ]\UH /aVMI]\@]^X
IK! TC  /]\aS  /m O[Z
 MV\[aK8UHVMI]^X
CRN LVT IQR!dTS NVMd\ aVMQ*]^Xg IK! D QGVY]\Ld\HVMWNL 8IK!  TC  /]\aS  /mO[Z
 //o
	[/V\L  
S /WNV/ hR]^X_ L]\QR ]^X_IK! /Q



 
	
&+
(-,
+4
&&+ 

8;

	
&+
(-,
	9
	
&+
(-,
	9
8;
&&+ 
+4

4VMP!W #G

.
	

6)



"=
""
6)

 
"





:
%" 
"("
="


="
(6)
#


3""

"%
)
#
"%
""

!
<%

*





=
*:

6=
 %


%
6)

=
 
:

="


3)
@ "


#
)

  ""



"%
)
6=
!
<%
""

!
%

 
- 1
=
:

'63)



"

El C /]\aS /mO[ Z
 /iKH V\L DQR!d


%
6)
*)
6)





 
=

&
&





$
 


4
/




&



 

4
/





/
&


 


 
 
 

 

 
^
q
 



G
 
 
G
 
q
 
 
=






&
&

'



	


	









$
&






1
/
/


/

	


1
/
/






/


&







	




	

/
/

1


 
Xc]\  /\VMWOHVMI]\ o VMHSD IK!  S  MLIHVMI]\
K! gXc]\ O! IKTU!KHV\L CU!  M[aKHVMd\ ML4IK!  C W]E]\j O!UTW! ML
HLIOH[AI]\HL
K!  
aLIU! KHV\ L $ _CI ML
 /WNS!LCL K!
XcI/oHI K! /  /PhR[A  MVM I !dT ! A_J / I ML_Xc]\ !  A_
[A /WWNLTVMHS /\VMWOHVMI MLTIK!  %
 ML O!W IT\VMW O! M LT I ]@IK! @S!VMIaV

Xc]\R MV\S !d
W!  ML

IK! GI K! aS
U!KHV\L 
?I K! 8 L M[A]\HS
U!KH V\L E o
\-P! OHL ML
]\IK!  GS!VMIaV@[A /WWNLDVM GU!  M[aKHVMd\ MS
mO[ Z
Xc]\D  MV\ S !d!oVM HS
IK! T  MLID]^X
IK!  RC /]\aS /
 /DLK! 
XcIaL
kO! ! d8IK!  TIK! aS@U! KHV\L Eo4IK! 

EH [A  /\VMWOHVMI]\f]^X
 /
C /]\aS /m O[Z
/IaVMdEL VMHSGIK! D  M V\S hGP!IVM D  MV\S
\VMWO! M L
Xc]\Q6IK!  
%
	QR QT MSNVMI  /W
hRU! M [A MS ML MV\S!d! oIK! R S MLIHVMI]\@ /d\NLI /
 /WNS@[/VM! !]\IDLK!
XcI

_]TU! KHV\L MLC MVMW /
XcIaL I
Xc]\ Eo I LK! 
K!  / 
 /WNS! L
VMII K! DLVMQR  I QR V\ LCI K! ]\ IK! /

PHASE  1

PHASE  2

PHASE  3

PHASE  4

DATA_BIT

write data

1.7

read data

DATA

0.8

0.6

1.1

data written

entry - 4

0.6

0.5

MATCH_LOCAL

write

read

MATCH_TAG

1.0

initial match

MATCH_DEST

final match

LOOKUP

lookup results

1.6

match

0.4

1.0

1.4

4d\O! # G

ClC /]\aS /mO[ Z

 /QR!dDk NVMd\aVMQ

HL M[b

K!   MV\S
U!KHV\L NLIK! [A IN[/VMW U!KH V\L LH [A CI	 IaVMjE ML
PH LI]DSNL[aKH VMd\ 
Xc]\O! L]\O!a[A 
]\UH /aVMHS!L_Xc]\Q:IK! LVMQR C / Ih.a_]\aLI[/V\L 7b
 /aL4 VM OHL MST [AO!  /IWhEo
L /H L  VMQRU! W
L4 !]
IK!NLI QR [A]\O!WN SRPH D QRU! ]ME MSD 
Xb!  M[A MLLVMh

e
s

 

 
 
 


 
 
 
q
a
q
 
 

 
e7e









J1->D'04*J1- H ;$0)

 /WNS!L
XcI MSGI]RIK!  T	]\ I]\W
IK! TS  M[A]S  MS8HLIOH [AI]\HL VM TLK!
K! T [A]\I]\WP!Ia LC]^X
&% ('P!IaL
%
VMHS
SO!!dG IK! 
VMHS@IK! 
Xc]\IK! T ]\UH /aVMH S!L
K! 
aLID U!KHV\L  
C /]\aS /mO[Z
VM D MV\S
Xc ]\Q6IK! 
 /VMH S8WNVMIa[aK!  MSR8S O!! dTIK!  D IK!aSGU! KH V\L 
K!  DQGVMIa[aK
IK!  e6W]\ d\N[CKH V\L	 PH / /RS MLd\! MS
VMdEVMHL IIK! ML  Ia VMdE L	N L	[/VM   MS
]\O!I T IK!  LVMQR  U!KHV\L 
I]@ VMW W]_:IK! GIa VMdf P!IaLI ]@[aKHVM!d\ 8 SO!! dfIK!  RQG VMIa[aK
VMHS?LIWWU!]S OH[A GIK! G[A]\ M[AID 
%
Ld\HVMWNL _CK!N[aK8IO! f]
IK! 
QGVMIa [aKGU!][A ML LNLVMWNL]G[A]\I]\WW MSGPh8 IK! 
LO!WI b
K! 
HS NSOHVMWHUH]\ IaLG[/V\ L  ]^X
HV\[AIIhD]\G IK!  /Q-SO! ! dDIKHVMI [Ah![AW 
K!   MLO!WI! dDQGVMIa[aK
XcO!IO! TOHL 
Xc]\QGVMI ]\f NLL I]\  MS@ @IaVMHLUHVM / IWNVMIa[aK! MLI]fLV/E 

Xc]\O!IK
@IK!  
Xc]\
I
&% ('P! IaLAoENL	OH L MS
U!KHV\L EoIK! QG VMIa[aK

Xc]\ QGVMI ]\RVMW]\! d$_CIKRIK!  
I]
! /dEVMI IK!  NLLO! ]^ X
GTWNLIaLIK! \ VM]\OH LC /E / IaL	][/[AO!!d
XZVMWNL NLLO!  
IK!  HLIOH [AI ]\G G[/V\L  D]^X_V

4VMP!W #G
IK!  HLIOH[AI ]\^
HS]_
SO!! dDIK! 
Xc]\O! CU!KH V\L ML
K!  QGVMIa[aK!! d][/[AO! aL	 RIK!  I]\U
Xc]\O!
 /I  MLS
LIaVMWWo! I NL IK!  LVMQR V\L
EP! O!IC 8[/V\L ]^X_V
XcIC[Ah![AW 
!]\QGVMW LK! 
 / /I WhTSO! !dTV
GI K!   ML IC]^X_IK!  / I  M L



$ 

!": '
=

 "

6

:

=
=

'
%
%""

" "
!": 
=
:
	

 "
G!l	HLIOH [AI]\M^

4VMP!W #G

=
=
6
 "

:


'
%

%""
!": '
=
" "
"
=
" 

:
	

 "
HS ]_iKH V\L D4 VMP!W 


 
 



 


 
 
)

q
'

 
a
	
 
'


Z
 
 
 
 
 
Z
 
 
=

	
&
&
4
/

4



&
 


4
/

$







4






 
&
 



4
/





&





&





4
 
4
 
 
e3P
XcI!dD NLC [/VM MS
HS ]_o! IK!  LK!
H[A  IK!  HLI OH [AI ]\HLCVM H LNS   IK!  HLIOH[AI]\.^
%
]_JIK!  (
Xc]\O! HLIOH[AI]\ 8LW]\ IaL
]\O!I fS
 / /ICU!KHV\L MLIKH VM8 8IK! DI]\U
VMHS
  %
[A]\ MLUH ]\H S!dT IaVMdRQGVMIa[aKRNL [/VM MS
K!  
XcIVMII K! NL !dT M Sd\  D]^X_ U!KH V\L 
L K!
IK!aS
]\O!IDVMHS@IK!  
QGVMIa[a K8W ! MLVM  T /\VMWOHVMI MS
LIK!  
QGVMIa[aK!!dT NL[/VM MS8]\ O!I@IK! 
IaVMdE L_CWWKHV/E GI]@PH 8 K! /WNS
U!KHV\L 8 IK! GI]\U
Xc]\O!R LW]\IaLA oIK! 8QGVMIa[aK
]\
IK! f e
P!I
W! ML
IK! 
K! T  /\VMWOHVMI MS@QGVMIa[aKfW!  ML(
Xc]\D IK! / TU! KHV\ L M L
4VM RWNVMIa[aK! MSfVMID IK! R /HSf]^X
aLIU!KHV\L  RI ]RIaVMHLUHVM / IWNVMIa[aK! ML
K!  /hfVM T IK!  /@IaVMH LYXc / MSfI]GIK!  RL M[A]\HS@L /I]^X
bSO!!d8IK!  RIK! aS@U! KHV\L 
Ia VMHL UHVM  /I
IK! GL M[A]\HS
LIaVMI RNLOH L MS@
K!  /
a
WNVMIa [aK! ML
%
U!KHV\L T]^ X
IK! 
! ARI[A h![A W 
I]8V\LL /IIK! 
"HW! ML
Xc]\IK!  _CI 
]\UH  /aVMI]\
K! 

"
& %(' P!IDN LDL  /I 
VM? \VMWNSfNLLO! E o4 NL ML /ID@IK! 
[/V\L R]^X

IK! RL  M[A]\HS@U! KHV\L Eo	VMHS
Xc]\O!IKU!KHV\L 
K! 8PhUHV\ LL!d@NL
[/VM MS
]\O!IT?IK!  
Xc]\O! IKU!KH V\L 8Ph? [A]\QR UHVM!d8IK! 
QGVM Ia[a KRW ! ML	]^X_IK!  NLL O! M SRHLIOH[AI]\8 VMdEVMH LIIK!  #_C I W!  ML
IK! ! O!H [AI]\HVMW!  !I4NL[A]\QRUHVM MS
]D[aK! M[aj IK! CP!a VMH[aKDU!  MS N[AI]\ o\IK!   MLO!WIbXc]\Q
,%
VMdEVMHL IIK! CU! MSN [AI  MS UHVMIKT TIK!  gXc]\O!IKT U!KHV\L 
K!  
"% Ld\HVMWH NL4d\ /!  /aVMI MS
T[/V\L 
%&
VTQR NLU! MSN [AI]\ o!VMHSG NLC OHL MS8P hRIK!  DVMWNS!VMI]\8O! !II]T\VMWNS!VMI DVMWW4 /I MLC VMPH]ME 
]^X
,%
 ML O!W I !dT \VMWN SGP! Ia LVM DWNVMIa[aK!  MSG I]TIK!  
I
K! 
"%D P!I[A /WWNLC VMIIK!  D /H SG]^X
U!KH V\L 
XcI[Ah![AW 7b
aLICU! KHV\ L . acXc]\ VD!]\QGVMW LK!
K! ML  P!Ia LCL K!
XcISO!!dT IK! 

J1->D'04*J1-  HDLM' '

Ef WWOHL IaVMI ML IK! G MV\ S?VMH S
_CI R]\UH  /aVMI]\HL
][/[AO!!df ]\?IK! 8L]\O!a[A 
4d\O! .G
]\UH /aVMHS!L
L[A 
Xc]\ QGVMI]\M_CIK8 MV\[aK8IaVMHLI]\8  /U! ML / IaLCIK!  $ _]\aLI[/V\L 
S /WNV/hR 
K! 
K! M L O!QT PH / aL	VM  DVMWW
HVM]
Xc]\Q-IK!  D
Ea[
kVMIaV
 /WNS% oV\LCIC[A]\HLNLIaL]^XbQR]\ P! IaLUH /
E3baoHVMHS8LO! PHL M`E O! /IWhTQR]\ [/VMUHV\[AIaVMH [A 
a 
E M[AI]\ME
PH M[/VMOHL D]^ X_I K!    
]_

Z
 
'
'
q
 
 
 

 


 

%
 
 
 
 
 
G
 

 
 
q
 
	
 
q

 
E
 
 
e7Q

PHASE  1

PHASE  2

PHASE  3

PHASE  4

BIT

data setup for write

issued

1.8

1.5

1.1

0.5

0.5

MATCH_LOCAL

DATA

BPS_CNTRL

1.4

1.5

0.5

write

1.1

0.5

read

data written

data shifted

1.7

kNVMd\aVMQ

HL M[b
HS ]_JQR!d
ElHLIOH [AI]\M^
4d\O! # G
?I K! R  MV\S
IK! R MV\ S?SE /aL
Ph
W!  ML VM 8V\LL /I MS
U!KHV\L Eo	IK! RQGVMIa[aK
HL IOH[AI]\ D
[aK! MSO!W // oMVM HS VM 	OH L MS PhIK!  	Ph UHV\LL!dCW]\d\N[4I]d\  /!  /aVMI IK! mC i

G b
Ld\HVM WNL_CK!N[a KGS E  IK!  PhUHV\LLCQT O!WIU! W ARE /aLa4 d\O! G
IK!  G]\UH  /aVMH S!LTI]f MV\ [aK?IK! 8 !O!H [AI]\H VMW  !IaL$ _CWWC S  /UH /HS
K! G  /I  GS /WNV/ h
Xc]\
O!UH]\@IK! T ]\O! I !dGVMHSf U!WN V\[A /QR / IVMH SfNL PH]\O! H S@I]R IO!@  I]R IK!  T [AIN[/VMW4 UH VMIK
Xc]\IK! 
S MLd\
]^X
IK!  
 
K! C]\O!IU!O!I	]^X
IK! CQTO! WIU!W R /aL _CWW KHV/E  I]d\] IK!]\O!d\KR VKO! d\ SE /
Xc]\  PH / !dRS E / R]\I]T IK!  P!OH L ML
PH 

Xc]\Q
IK! 
C

@A0 -70

J1-< 4) 





"%* 


("%*

6%

=
. %*

 "
'
=
=

7/
6%""
=
6%""
=
7/
=
"%* 

lHLIOH [AI]\f
[aK!  MS O!W / QR! d

*:'
7/0=

. %
4VMP!W # G

 
q
a
 
q
 
 
 
 
=

4
&
4
/

$
4



;

;


4






 
s
SRC1 CONTROL

SRC1 DATA

bypass
logic

match_local

INSTRUCTION
SCHEDULER

ph4

src1 operand

bypass
mux

result from FU

issued to FU
G!lC 	IN[/VMW4i VMIK
4d\O! G
WN LIaL	IK! QG VY]\	 /E /IaL] [/[AO! ! d
4VMP!W BG
IK! /CQR ]\  S /Ia VM W  MSGS  ML [A U!I ]\
Xc]\WW]_L

Xc]\IK!  


 

RIK!  
TSO! !dD IK! 

Xc]\O! U!KH V\L MLCVMHS

XcI !d]^ XbVMWWH IK!  [A]\ I]\WH P!IaL W][/VMW% I]DIK!  H LIOH [AI]\G 
[aK!  MSO!W /NL
K!  LK!
[/VM  MSR]\O!ISO! !dT U!KHV\L 
K!  /C!  A_J\ VMWO!  MLCLK!]\ O!WNS8PH D LIaVMP! W DPH 
Xc]\ DIK! DLIaVMI
Co_CK! /8I K! DL[aK! MSO! W!dD PH /d\H L
]^XbU!KHV\L  
Xc]\ EoH SO!  DI]R IK!  DL /NVMW4H VMIO! D]^X_ IK!  D]\WNS  MLI
LCQR /I ]\! MSTPH 
aLIVMWd\]
 /
 IK!QfoHIK! RL [aK!  MS O!W !d
LIK!  TQTO!WIU!W
Xc]\IK! T  1LIaVMIaL]\! TU! KH V\L R  MVMW /
  Do!IK! T L[aK! MSO!W! d
LKHVM M LVTUH]\I_C IKfIK!  
IK! DQTO! WIU!W /	 NLVMWNL]8LIaVMI MSG
]^X
IK!N L	U!KHV\L 
K! LUH M[AN VMW%%]E ]\jO! UfaV/h
Xc]\IK!  LI]\ NLV\ [AI\VMI MSTVMH SRIK!  U!][A MLL
IK! ]\WNS MLI UH /H S!dD LI]\  MLUH M[AIE  ]^X
HS
NL	[/VM  MS
XZV\[AIIKHVMI	I	KHV\L
IK! 
]\O!II ]
SNLVMWW]_IK! L[aK! MSO! W!d ]^XbVM hTW]EV\S
Ia L]\UH /a VMHS!L MV\Sh
]\!]\I
K! NL	 NL ! / MS  MS
I]
U!KH V\L ME3 bIKH VMI
a[/VM  MSf]\ O!I
HL IOH[AI]\ 
[/VMQR RV
XcI /DIK!NLDLUH M[A
[RLI]\  
WNL]!o

HS
PH /!d8I]
XcO!H [AI]\ 
	IaL
IK! GL UH M [ANVMW%]E ]\j O!U
NLV\[AI\ VMI MS
aV/h
Xc]\DIK! R W]E V\S

e
r
 
q
 
s
 

 

%



 
q
 
 

 

%



 

	
 
 

 

 

P]
K!  
]\WNS ML IUH /HS!dGW ]EV\SfHLIOH[AI]\
IK! 
IK!NLNLOHL MSfI]8S NLVMWW]_ IK! 
 MLO!WI]^X
Xc]\WW]_L IK! NLW]E V\SGH LIOH [AI]\
VMhGL I]\ H LIOH[AI]\G IKH VMI
L[aK! MSO!W !dD]^X
Xc]\ QGVMI]\ d\ /! /aVMI MS VMPH]ME   /dE VMaS !dIK!  	UH /H S! dW]EV\S VMH S UH /HS
K! 
!d LI]\   HL IOH[AI]\ HL4NL OHL MSD
IK! L[aK!  MSO!W!d]^X
IK! LI]\ VMHS
W]EV\ S
HLIOH[AI]\HL
K! RL[aK! MS O!W! dR]^X
 ML UH M [AIE /W h
IK! R[A]\ I]\W	IaVMHLYXc /HLIOH[AI]\@NLVMWNL]f[/VM MS
]\O!I @IK!N LU!KHV\L  
LIK! TLI]\ R LKH VM MLDVR UH]\ I$ _CIKfIK! T IK!aSf  Do% IDVMWNL]GKHV\L
mO! ICIK! S hH VMQRN[CH VMIO! ]^Xb IK!  D %]E ]\jO!Uf aV/hRVMWW]_L
[A]\QRQR ]\R%]E]\jO!Uf a V/h
IK! V\LL / I]\T ]^X
VL I]\  /E  /$ _CK!W CIK! CW]E ]\jO! UTU!] [A MLLb Xc]\	IK!  IK! aS
  NL4d\]\!d
LCVTL I]\ $ _C WW%]\! WhR]ME  /NS  IK! WNV\LI  JHLIOH[AI]\GI]T PH D L[aK! MS O! W MS%oIK! 
]\
]\WNS ML I
aLIVMWd\ ]\ IK!Q-NLC LIWW% QGVM IaVM!  MS
Xc]\IK!  D  L

IK!NLD U!KHV\L Eo	IK! G  MLO! WIaVM I

L[aK!  MS O!W!dfVM 8SE /
Xc]\Q
QGVMIa[aK@W!  ML
La[
]\I]R IK! 
 /WNS! LCIK!]\O! d\KfIK! D MV\SfS E /aLI]R [/VMhG]\O! IIK! D NLLO! D]^X
VMHSfLa [AC
K!   MV\ SGUH]\ IaLC]^X_IK! \VM]\OH L[A]\I]\W
IK!  HLI OH[A I]\HLI ]TI K! D! O!H[AI]\HVMW4  ! IaL
P!IaLD HLN S GIK! f 

VM fVMWNL]@]\UH /! MS?O! UI]@HSN[/VMI G IK! GIhUH G ]^X
IK! GHLIOH[AI]\
NL LO! MS
PH /!d

 
 

 

%


 
 
	
 
 
V
 
 

 

 

%



 
q

 
 
  	 




8	


	





 







!

 

NLCOHL MSGI]GLQTO!WNVMI  IK! D WNV/hE]\O! IVMH S8E /
Xch
eTLQTO!WNVMI]\
E /aL ]\DQ
K! 
 
e
W /E  /W LQTO! WNVMI]\gXc]\ e@
G[Aa[AO!IaL	 QR] S  /WNLIK! 
IaLgXcO! H[A I]\HVMW Ih
	SE  /TW]\d\N[
K! D  /E /I
Ia VMHL NLI]\ CV\L	V  ML NLI ]\	R L /  ML _CIKT VE]\WIaVMd\ 
	[A]\ I]\ WW MS
_CIa[aKTVMHS
 MV\[aK
!] S  _CIKRV
[/VMUHV\[A IaVMH[A 
L4QR ]S /W I]D[A]\QRU! O!I 	IK! S  /WNV/h!L VMHSD!]S 
	K O
	 /]\O! !d
I4OHL MLIK! 	K!]\!d
\VMWO! ML
Xc ]\Q
IK! G  MLO!WI !df:! /I
_]\j
d\K! /T V\[/[AO! aV\ [Ah? [A]\O! WNS
KHV/E G PH / /V\[aK! /E MS
OHL!d?
E U!N [A EoP! O!ITN LD! ]\IRV@ [AIN[/VMW UH VMaVMQR /I /D?IK! f[A]\ QRU! W /I /WhfSd\IaVMW S MLd\
K! 
V\S\VMIaVMd\ D]^ X

e*]ME  /
EU!N[A NLIKHVMI INLC VMP!W  I]RLQTO!WNVMI `E O! I V
WNVMd\  ! /IWNLIC8V
oHV\LCI
WNL]!o! IK! U!  M[A MS  /H[A _CV\LL /I PhRIK!  
[A]\QRUHVMa VMI E /WhT LK!]\ I / I QR 
 h!
!
q
_CV\LL QTO!WN VMI  MSOHL !d

e6VMH S
XcO!H [AI]\! MST[A]\ M[AIWhDV
XcI /_XZVMP! N[/VMI]\
K!  K!U ]7Q
U!Q
L[A QR]ELCUHVMaVMQR / I /
QRN[A]\8 U!][A MLLXb NLLO! U!U!W MS8I]
i*]
]
_CIKfWN VMQTP%S! V
W 
Xc]\ L QTO!WNVMI ]\HL
 
e

K! @ 
EO!UH /a L[/VMWNVM 8
E QTO!WNVMI]\
d\ /! /aVMI  I ML ICE M[A I]\ aL

q/s

NL

OHL MSI]

O!IK! 8PH  /H [aK! QGVMj

U!]\d\aVMQGLTVMHS

 



	








	

 






 

q
G

 
	
L
 
	

 
 
 

 
 

 
s
a
 
Q

q
 
]
	
 

 
P
q
PC



 

	
 



 


	 



 '





'
" '
=
	=
% =
!
9(%


=

4VMP!W 

:

.

 *=

&655
"

"

(	
5

=
$5

 


$5
"



%
$5
l	% NLI]^X_m  /H[aK! QGVMjTi]\d\ aVMQGL

<

.%*

6%6
555
555
555
55
555
555
555
555
555
555
555
555

 MSI]? AR!V\[AIWh
QR] S
Xc]\G IK! fd\ /!  /N[@
[aK! MSO! W! d?  ! IRKH V\LGPH  / /
K! ?-QR] S / W
 / U! M L /IIK! G [AO!  /IDS  ML d\VMHS@NLLO! G VMWd\]\IK!Q
Xc]\
 O:[A]\ QRU!W / NLOH L MS
K!  
[A]\QRU! W !d IK! D L QTO! WNVM I]\C[A ]S DVMHSGIK! PH  /H[aK! QGVMjTU! ]\d\aVMQG L



'!

IK! 8
 [aK!  MS O!W!d8 !I
I]fLQTO! WNVMI 
WN LIaL IK! T\VM ]\OH LDPH /H [aK!QG VMj! L OHL MS
4VMP!W 
K!  8PH /H[aK!QGVMj!L
[A]ME  /TV@P! ]E V\SaVM!d\ 
CI I /QRU!ID KHV\ LTPH /  /QGV\S GI ]
\VMh!d
VM 8]^X
Xc]\#
P!WW]\ RHLIOH[AI]\HL
W /!d\I Ko
Xc ]\Q
]7]o
]7]7]T HLIOH[AI]\HL
Xc]\
CI]C
 @NLT IK! @LIa VMH S!VMaSJS! VMIaV?[A]\QRU!  MLL]\O! IWIhV/\VMWNVMP! W W_C IK
# 
IK! @ 
Ei	 

Eh!LI / Q
k1NL
Vf[AOHLI]\Q
PH /H[aK!QGVMj@ AR IaV\[AI MS
Xc]\Q
IK! 
-QGVMd\ 
UH /aVMI !d
[A]\QRU! M LL ]\R LIaVMHS!VMaS8L H[A INLK _CNS /WhTOH L MSTG Sd\IaVMWLd\HVMW%U! ] [A MLL!dT VMU! U!WN[/VMI]\HL
IUH /
Xc]\ QGLVR SN L[A  /I  
[A]EL ! DIaVMH LYXc]\Q
]\@ V.QR QRU!
R /WQGVMd\ D VMHSf IK! /8UH  /
Xc]\QGLCIK! 
K!  kK! h! LI]\!  
E h IK!  /IN[	PH /H[aK!QG VMj NL4O!
E /a L 	IaVMHLYXc]\ Q1V I]\IaVM W!]^X
Xc]\
]7]IQR ML
L / E /a VMW4I /aVM I ]\HLI ]G]\P!IaVMf QR MVM!! d^XcO!W S!VMIaV
	`E I]\IINL
Xc]\Q
IK! T
 i	
I /d\ /
 OfJ	]\QRU!W /K_C K!W  IK!  
PH /H[aK!QGVM jRLO!I 
NL
Xc]\IK!  
NLIK! DL]\O! a[A D[A]S  
K! 


EU! MV\S!LK! / /I VMWN [AO!WN VMI ]\ PH /H[aK! QGVMj
IK! 

 

+
 






;
 




8


/




4


	



$



8
8


$



4


8




s
 
q

 
 
 





s
 
q
 
 
s
"

 
C





 


 
 
)
 
 
 
q
 
 

Q
r
 


 
PE







!







Xc]\Q1IK!  ef
WNV/hE ]\O! I	P hTO! !!!dDIK! 
K! IaVM HL NLI ]\CW / E / W!!  /I WNLI	NL	 ARIaV\[AI MS
INLIK!  /@ [A]\ E  /I MSGI]RIK!  %
V\L
Xc]\Q
WNV/hE]\ O!I MSI]\
[A ]\QRQGVMHS
Xc ]\Q*I K! 
()

IK! #
L[AU!I
K! NL ! /IWNLI NLIK!  /RLQTO!WNVMI MS
 M`EO!  MSDPh
 
e
_CIKTI K! K! /WU
]^X
Xc]\QGLVM 
W Eo! VMH SGIK!  D MLO!WI! dD]\O! IU!O!Ic _CV/E  
OHL!dT IK! I M LI
[AQGS
E M[AI]\a LCLV/ E M SRGIK!  
!]\TL QTO!WNVM I]\HL]\ ?WNVMd\ /DP! W][aj! LAo4IK!  R]\O! IU!O! I
]\PHL / E M S
NLDLI]\ MS@?V8W]\d
W RVMHS
NL
IK! /8[A]\ QRUHVM MSRVM dEVM HLIIK!   ARUH  M[AI MST MLO!WIaL

# 

&
	 @	V\[aKf HS NSOHVMW[A /WW1 _CV\L  ARIaV\[AI MS8VMH S@LQT O! WNVMI MSTI]G I MLI IaL
XcO!H [AI]\
	[A] S MS?VMHSfIK! 
VMW Ih8PH 
Xc]\  TOHL!dG I @IK! T S MLd\
K!  TI MLIE M[AI]\aLc_  / 
KHVMHS
aVMU!K!N[/VMW4HVMW hn/  /	I /
_CV\ LCOH L MSTI]T ]\PH L /E  IK! PH  /KH V/]\C]^X
XZV\[A ]^Xb IK!  D
 e
IK!  U! ]\PH MS8 L d\H VMWN L

%	 	
#	f LD ITNLD !]\IR UH]ELLP!W 8 I]
LQT O!WNVMI 
Xc]\T /E /h@ UH]ELLP!W @[A]\ QTP!HVMI]\
C /]\aS /mO[ Z
]^X_!U!O!IaLVM HSf[A ]\
d\O!aVMI]\HL ]^X_ IK! 
 //oHIK!  K! /WU8]^X_ IK! 

EO! UH /aL[/VMWNVM

EQT O!WNVMI]\$ a 
 7bc_CV\L L]\O!d\KI
I /d\  / PH /H [aK!QGVMj! Lc_ / T LQTO! WNVMI MSG]\@ IK! 
VMHS
 
e
[A]\QRUHVMI P!W D I MLIE M[AI]\aL
Xc]\IK! 
aLI
]7]7]f[Ah![AW ML_ / Td\ /! /aVMI MSfVMIDO!
IK!  I /H VMW LIaVMI ML
IK! ]\ O!IU!O! IaLC VMHSG VMWNL]
_CIKR IK!  ARUH  M[AI MS
I QR EoEVMW]\!d
\VMWO! ML	]^X
IK! RQG VY]\
]^X
[A]\I ]\WP!IaL
K!  ML RI MLI
E M[AI]\aL_  / RIK! /
OHL MS
I]@LQTO! WNVMI 
IK! 
LQTO!WNV
Xc]\Q-IK!  
 //oHVMH SGIK!  D]\O! IU!O!I
 ARIaV\[A I MSR! /I WNL I]^X_IK! D /I  C /]\aS / mO[ Z
I ]\HL_ /  
[A]\QRUHVM M S8VMdEVMH LIIK!  
 ARUH M[AI MS8 MLO! WIaLI]RE  /
XchGIK!  
XcO!H [AI]\f]^X
IK! 
m

 
"
	]\I]\WoC
Ea[AC
W WIK! GQG VY]\D [A]\QR UH]\!  /IaLD ]^X
IK! G
	D
E a[
	K_  / DLQT O!WNVMI MSTL /UHVMaVMI /Wh
	]\I]\WVMHSGIK! D HL IOH[AI]\f
[aK!  MSO! W /
YOHLIWjE IK! 
 /WNS! LK_ /  /QR]ME MS
K!W  L QTO!WN VMI !dI K! D 
Ea[ 	]\ I]\WP! W][aj! LAoIK!  D
Ea[ kVMIaV
m




#
 

 
 

C
 

 
 
	
 

 

 
	
 
 

 

q
 
	
 
^
q
 
^

PAG
IK! @ M`EO! \VM W /IR [/VM UHV\[AIaVMH [A @]^X
I]IK!  f]\O!IU!O!I
V\S! S MS
W!  ML _ / 
IK! _]\aS
VMHS
 /WNS! LIK! /QG L /WE  ML_ / RLQTO!WNVMI MS8 P hfOH L!d8IK! 
IK! ._C I RSE  /aL
K! Gk VMIaV
]^X
[A /WWNLAo7_CIKRIK!  [A]\ QRUH /HLVMI MST [/VMUHV\[AIaVMH [A V\S! S MST ]\RIK! 
]\!  ]_]^XbS!VM IaV
! /IW NLI	]^X
K!  G MV\S? VMHS6 _CI GSE /aLD VMW]\! dD_C IK?IK! 
E /IN [/VMW	P!ITVMHS? LK! 
XcI
! U!O! I
W!  ML
ML /I O!UfS E /aLg Xc]\IK!  DP! I W! ML _ / 
U! M[aKHVM d\ 
VMWNL]TOHL MS88IK!  
LQT O!WNVMI]\
K! 
Xc]\QGVMI]\H LK_ / 
_]\aLI[/V\L DL QTO! WNVM I]\ _CV\L[/VM MST ]\O!I VMHSR IK!  /W /\VM ICS  /WNV/hT 
E3b
]\P!IaVM ! MS aLK!]_C8 G4 d\O!  #G

 

 
 
 
q
 
 

	 



 

	 	

K! QR ]EL I QRUH]\IaVM IUHVMaVMQR /I /%]\P! IaVM!  MS
Xc]\QJIK!  CLQT O!WNVMI]\HLH NLIK!   HLIOH[AI]\HL
i4 / 	h![AW  ca ib%]^ X
Xc]\	VMWWEIK!  eCPH /H[aK!QG VMj U!]\d\aVMQGL4 VM  HSN[/VMI MS
IK! C
k
i
K!  Ci
GI K! D4d\O! 

 
 
I
P
C
 
 

3.5

3

2.5

2

1.5

1

0.5

0

2.76

2.61

2.28

2.18

2.27

2.51

compress

dct

dhrystone

eqntott

gcc

sc

4  d\ O!   

l	  H L I  OH [A I  ]\ H LC i4  /  	 h! [A W  

a  i  b

VM 
G7 G
]^ X
V/ E  / a VM d\  
P e o1 _C  I K
 K!  R  i 
QG I ]W C
Xc  ]\ Q C
]^ Xg I K!  T PH  / H [a K! QG VM  j! LC a VM ! d\  
o/ \ VM   ]\ OH L UH VM a VM QR  / I  / a L
 W I K! ]\ O! d\ K I K! N L% QG V/ h L  /  / QJ VC W  I I W  4 W ] _  / % I KH VM  I K!  	 ]\ !  	 ]\ PH L  /  E  M S  
q/ s
QG V R  QT O! Q:  H L I  OH [A I  ]\ 
KH V/ E  D PH  /  / @ L [/ VM W  M Sf S ] _C f  f I K!  D KH VM a S  _C VM   T  QR U! W  / QT  / E Ia VM I  ]\ 
 K!  
N L L O!  
M [A h! [A W  	 N L ! ] _
 H L I  M V\ S ]^ X
I K!   U!  ]\ UH ]E L  M S Q
 ! W h$ E     L4 VM   C OH L  M S  H L I  M V\ SD ]^ X
G! oE VM H S

    ? N L N L L O!  M SR  / E  /  h
]\ C ]\ !  
 ! W hT ]\ !  D 4   k
I K!   QT O! W I  U! W   / 4 N L ! ]
W ]\ ! d\  / C U!  UH  / W  !  M S
[A h! [A W  E o% O! ! W  jE  
I K!  R  M VM  W   /  QR ] S  / WK _C K! N [a K
[A ]\ O! WN Sf N L L O!  R PH ]\ I K
 
]\ !  R [A h! [A W  
 W W I K!  M L  R KH V/ E  
I K!   L UH  /  M S O! U oE P! O! I	  I	 N L L I  W WH VD [A ]\ H L N S  / a VM P! W  C  QR U!  ]M E  / QT  / E I
[A ]\  I   P! O! I  M SD I ]D I K!  C W ] _  /   ! d ]^ X

P
s
 
 

 
s
 
q
 
s
 
q
 
q
 
C
 
 


 
s
 
 
 
 
Pe

* =

:

<

 	



 


	






4VMP!W 

Cl 
 

""

&&)
	$
$#)

)


5)

)

	)
5
&65)

EIaVMWWNL



 




 

:
[aK! MSO!W !dT  ! IL IaVM WW4QR MVMHL IKHVMI IK! T 
  JNLC!]\I VMP! W D I]R [A]\QRQR I	IK!  D MLO!WI]^X
IK! $GR HLI OH[AI ]\H LVMIIK!  DPH]\II]\Q* ]^X
IK! 
m
I]RIK!  
C /d\NLI /4W 
K! NL [A]\ O!WNSGKHVMU!UH /
KHV\L	VM THL IOH[A I]\ _CVMI!dI] PH  NLLO! MS
_CK! /T IK! 
 
QR]E LI P!W][aj%o\ ]\	IK! 
IK! PH ]\II]\Q

HL I OH[AI]\ RKHV\LPH / / RNLL O! M STP!O!ICIK!  D 
 
NLLIWW_CVMI! dDO! UH ]\GIK!  MLO!WI I]TVME PHV\[aj
_ / /
  -LIaVMWWNLDVMH S
K! / RN L!]@S  M [AI  / WNVMI]\ HL K!U
SO!!d
IK! GLUH / MS O!U oV\LD  /E  /
PH /I
V8L IaVMWWoIK! R HLI OH[A I ]\?
[aK! MSO! W /LIWW jE  / /UH L ]\
NLLO!! d8V/\VMWNVMP! W TH LIOH [AI]\H LI]GIK! 
KHV\ L	PH  / /TjE /U!I	 WNVMd\  /!]\O!d\ KRI]DQGVMjE CLO! IKHVMI
K!  S /U!IKT]^X
!O!H[AI ]\HVMW% ! IaL
IK! 
  
V/\VMWNVMP!W 
LO
[A  /ICO!QTPH /]^X
HL IOH[AI]\HLVM 
Xc]\NLLO!  DO! HS /! ]\QGVMW [Aa[AO! QGLIaVMH[A ML
K! TLIaVMW W	[/VM OHL  ML IK!  ToHI K! T k:VMH SfIK!  TJ LIaVMd\ ML]^XgIK! DU! UH /W! D I]8LIaVMWWS_CK! W DIK! 
! 
VMHSM^
LIaVMd\  ML[A]\I O!  AR  M[AO!I!d
S /UH /HS  / IH LIOH [AI]\H LAo\]\	 [AW]EL /Wh
K! LIaVM WWNLVM [/VMOHL MS
QGVM!WhP hD[AW]E L /Wh U!WNV\[A MS
e8O! WIU!W //oE4 k
O!! I/o! /Ia[
U!WNV\ [A M STHLI OH[A I]\ HL
d\KI !d
Xc]\VDWQRI MSD ML]\O!a[A a
V\L W]_: V\L
Xc]\ \VM]\ OHLD PH /H[aK!QGVMj!L VM TWNLI MSf 
K! RLIaVMW WNL
Xc]\Q
C8VMHS@aVM! d\ 
4VMP!W 
PH C QR U!]ME  MS O!UH]\RPh  /QRU! W]Mh! dL]^XcI
EI] V\LK! d\KTV\LKC3 P
K! ML C O!QTPH  /aL4[/VM
_CVM 
Xc]\ Q- MV\ [aKG]\ IK!  /
IKHVMI QR]ME MLIK! DS  /UH /H S  /IC HLIOH[AI]\HLCVMUH VMI
L[a K! MSO!W !d

4
;
4


 


#


&
s
 
 


 
	
 
 
 
 
m
 

 
 
d
 
 
b
 
s
 
]
 
r
 
Q
s
 

E

 
P7P

* =

:

<



*=

6





*=





	
 



 


	 







	
#
$
&65
5#

&)
)
)
5)
)
5)
)
4VMP! W 

&()
$
5)
	
&65)
#
&(	)
&(
$5)

&(#)
&($
&()

El maVMH [aK8
E IaVMINLIN[/L

"


(







*=
6#
#)
*)
)
*)
)
)
)



  

'






4VMP!W 
dEVMaS !dG IK! 
]^X
sq

 
O!HL
IK! fLQTO!WNVMI]\
P!aVMH[aK@U! MS N[AI]\ 8aVMI 

Xc]\ Q
IK! G\VM ]\OHLT UHVMaVMQR /I /aLDdE VMIK! / MS
EfW NLIaL
P!aVMH[aK@U!  MS N[AI]\ 
ILK!]_ LDVRE  /hGK!d\K@ V/E /aVMd\ 
a_CIK8 V
eG
 /I hGP!aVMH [aKG IaVMd\ /ICP! O[Z
 /b
UH /a[A /IaVMd\  
K! T maVM H[a Kfk / WNV/hGN LI K! D UH /a[A /IaVMd\ TS  /WNV/h8 [Ah![AW MLa
NLC /WNVMIE I]RIK! 
[Ah![AW  MLXb% MLO!W I!d
K!  C O!QTPH  /4NL LWd\KIWh
Xc]\Q:VP!aVMH [aKTQRNLU! MSN[AI]\
I]\Ia VMWHO!QT PH / 4]^X
K!d\K! /IKHVMR AR UH M [AI  MST V\LN L	IK! V/E  /aVMd\ PHV\STP!aVMH[aKRUH /HVMWIhT]^X
m]\IKG LK!]\O!WNS
[Ah![AW ML
QRU! ]ME 
IK! Cma VMH[aKT mhUH V\L L! dQR /IK!] SDSNL[AOHLL MSDT 
E  M[AI]\E
GNL4QRU! W /QT AI MS
K! 
P!W] [aj! L]^X
V/E /aVMd\  PHV\SRP!aVMH[aKRUH  /H VMW Ih
HLIOH[AI]\
Xc /Ia[aK!  MS
H[A]\ M[AIWh
NL	 IK!  O! QTPH /]^X
 ML O!W I! d
Xc ]\Q1VD QRNL U! M SN [A I M SP!aVMH[aK o U!WOH L VMGV\S!SI]\HVMW [Ah![AW 
Xc]\ IK!  H[A]\ M[AI	P!W][aj
IK! 8QRNLU! MSN[AI MS
IK! 8U! UH  /W! a
I K! fk*L IaVMd\ f ]^X
HL IOH[AI ]\HL
]^X
ZX
U!UH  8P!O! P!P! W 7 b
P!aVMH[aK
NL !]\I
]\?IK! RWN V\LIDUH ]ELI]\?]^X
IK! 
Xc /Ia[aK!  MSfP! W][aj%o IK! /?VM? V\S!S I]\H VMW[Ah![AW 
NL
	cXc /Ia[aK!  MSTO! UH]\ 8 M[A]ME  /h
PH D  
IK! UH /HVMW IhGV\LCIKHVMIP! W][aj _CWW% KHV/E   I]
V\S!S MSGI]
G!o IK!  ML [Ah![AW ML4 ]^XbUH  /HVMWIhRS]D !]\IC d\ /ICV\S!S MSR /E  /hIQT 
L	QR / I ]\! MSD 8
E M[AI ]\E
QRNLU!  MSN[AI]\ o P!O!I IK!  T
 VMWNL]GLIaVMWWNL SO!!d
P!aVMH[aK8QR NLU! M SN [AI ]\
]\@V
ZX_IK! / NLV
IKHVMICIQR E oEI K! / G!]
QR]\  H[A]\ M[AIHLIOH [AI]\HLC VM PH  /!d
Xc /Ia[aK!  MS
K OHLIK!  DLIaVMWWK! /WU


 







 

 
 



5
&
5

&


#
#

#


&

s
 

s
 
	
 
Q
r
 
 
	
 
 
s
 
G
 
X
 
 
 
 
 
 
 
K! NL NL VMQRU! WhG  /NS  / IgXc]\Q*IK! 
fK!N S!dT IK! 
PHV\S8P!aVMH[aKf UH / HVMWIh
PH /H[aK!QGVM jW_CK!N[a K@KHV\ LIK! RW ]_ MLIPH V\S@P! aVMH [aK@UH /HVMWIh@]^X
[Ah![AW ML
C3b

 LIaVMWWNLa4 VMP! W 
KHV\LCI K!  K!d\K! ML ICO!QTPH /]^X

PQ

 MLO!WIaLC]^X
a4VMP!W 

IK! 
E3ba oVMHS



  







C[Ah![AW 4WNVMI /H [Ah e8O! WIU!W /baoAIK!  / 	 NLV[aKHVMH [A 	]^X
I KIK! C[AO! / IS ML d\ .a
 MLO!WIaL
LC]\! WhWGRUH ]\IaLVM TV/\VMWNVMP!W Eo
Xc ]\Q*IK! 
VM !dRVM IIK! TL VMQR  D I QR 
!O! H[AI]\ HVMW	 ! IaL
IK!NLC[/VMOHL M LCV$^@ I D E / 
]_
]_
mO! ICIK! UH ]\WN[AhTGU! WNV\[A I]
KHVMHS W IK! NL ]ME  /
NL	QR]\ 
d\O! MLgXc]\Q-IK! D4 d\O! 
[A  /I/oVMHSGNLCVMII ML I MSRP hRIK!  
IKHVMfLO

 
%
 
W
r
i
t
e
 
O
v
e
r
f
l
o
w

0.891

1

0.8

0.6

0.4

0.2

0

0.001
compress

dct

4  d\ O!   

gcc

0.001
dhrystone
eqntott
C lK ^@   I  D  E  /  
] _
I K!  G PH  / H [a K! QG VM  j! LA o VM H S? ]\ ! W h
!  / d\ W  d\  P! W  T  ? I
_ ]f ]^ X
 K!    O! QT PH  / a L VM    E  /  h
L QG VM W W oE VM H SR S ]
! ]\ I
Xc ]\  QG VM H [A  

 K!  M ^@   I  f  E  /  
] _: N L
VM W QR ]E L I
# 
d\ ]E  M LC V\ L	 K!  d\ KG V\ L ]
 R I K!   [/ V\ L   ]^ X
[/ VM OH L  D VM  hR QG V Y ]\  S  / d\ a V\ S! VM I  ]\ 8 ]^ X_ I K!   UH  / 
Xc  ]\ Q- I K!  D L  QT O! WN VM I  ]\ T  O! H L VM    W N L I  M ST  G I K!  D VM U! UH  / H S 
 K!  D S  / Ia VM  W  M SR   M L O! W Ia L

R8 

0.031

0.028

0.09

sc

 
"


G
 
q
s
 
s
 
 

 
 

 
^
s
 
 
 

s
 
C
 
s
 
 
Q
r
 





 
 
 
  	 






IK! D
 [aK!  MS O!W!dD !I_Xc]\
IK! S  MLd\GVMHSTQRU! W /QR AIaVMI]\D ]^X
K!NL	IK! MLNL	U! ML / I  MS
Xc]\CIK! NL	 ML MVMa[aK._CV\LI]D !]\I LI]\U8 VMI
IK! D
 k
i?U!][A ML L]\ 
YOHLIVDIK! /]
K!  QR]\I\VMI]\
 / IN[/VMWHQR]S /WoE P!O!II]T L /  IK!  S MLd\GIK! ]\O! d\KGI]
IaL [A]\ QRU!W /I]\
K!  \VM]\OHL KHVMaS _CVM 
LCIKHVMIC /E]\W E MSTV\LCVD  MLO! WI_ / S NL[AOHLL MSTGIK!   IK! MLNLVMHSR IK!  DS M[ANL]\HL
Ia V\S / ]
YOHLI
 MS
K!  
[aK! MSO!W !d  ! I O!H L	VMIVLUH / MS
]^X
]7]D e
n VMHS
KHV\ LVMRV/E  /aVMd\  HLIOH[AI]\
i4 / 	h![AW  ]^X





	

	 

K! RUH]ELL P!W  R QRU! ]ME /QT /EIaLC 
PH /W]_ l

IK! RS  MLd\

]^X

IK! 8
 [aK!  MS O!W! dG ! IDVM RI /QRn/ MS

_ / 	U! ]\UH]E L MS
Xc]\4 IK!  
k 
i4 oM]\! Wh#EKH V/E 	 PH  / / QRU! W /QT /EI MS
E /I K!]\O!d\K
G  L
  LNL[/VMOH L MSGPhRIK! 
G IK! 
K!  WQRIaVMI]\R]\ 8IK! D O!QTPH  /]^X
[AO! / I S ML d\
L / NVMWHVMIO! D]^ XbI K! DN L LO!  DVMP! IaVMI]\8U! ][A MLLCGQRU! W /QT / I
!d IK!  
]\WNS  MLI
aLI
E@  -NLLO! ML
W I / HVM I E GVMWd\]\IK!QG LIKH VMIRVMWW]_-QR]\ RIKH VM
VMWd\ ]\ IK!Q
_CK!W 
 /QGVM !!dM_CI K! ?I K! GIQR 
WQR ID VMWW]\II MS@I]@IK!  8L[aK! MS O! W!d8 U!] [A MLL
LK!]\ O!WNS?PH 
 ARU!W]\  MS

 
 


	
	

 
	
 
Z
	

 
q
C
 
G
 





 
 

 
 
P
r
Q7]
IK! fL]\O! a[A 8]\UH  /aVMH S! L_C IKIK!  G MLO!WIaL
K! W _C I GU!  M[a KHVM d\ 8W ]\d\N[
Xc]\TO! U%S! VMI!d
Xc ]\Q1IK!  D  L	NLLI OHVM I M SGVMIC IK! I]\UG]^XbIK!  HLIOH[AI]\.^
HS ]_
K! NL
Xc]\a[A ML	IK! 
G  MLO!WI4P!OHL MLIKHVMIKHV/ E I]  MV\[aKDIK!  CPH]\II]\Q:]^X
Xc]\ PhUH V\LL! dI]!]_VMWNL]
IK! C
d\]RVM ]\O!HS8I K! D 
I]T  MV\[aKGIK!  I]\U
K!  D[A]\! d\ MLI]\8GIK!  DPh UH V\LLC QTO!WIU!W R /aL
[AO! / I WhT U! ]\K! P!IaLCIK! DQR]M! dD]^X_IK! $_C I  U! M[aKH VMd\ W]\d\N[I]TIK!  PH]\ II]\QfoHP!O!I
NLCVTSN LIH[AIUH]ELL P!W I h
Xc]\L]\QR /]\! I]TIhR GIK!  
XcO!IO! 

%]E]\j!d?VMI
IK! GK! d\K! /TV/E /aVMd\  GPHV\ S?P!aVMH [aK?UH  /HVMWIh?[Ah! [AW MLAo QRU! W /QT  A IaVMI]\@]^X
P!aVMH[aKGPhUHV\L L !dRVMWN L]TPH M[A]\QR ML	 QRUH  /aVMIE  

_]D QR]\ C[A]\QRUH]\! /IaLIKHVMI V\LLNLIIK!  
[aK!  MSO! W!d   !I LIWW! !  / MSD I]PH   S MLd\! MS
! TN LIK! ^@  I 
PHV\[aj
	]\I]\WW /QR / I]\!  MSG MVMW //oVMH S@IK!  T]\IK!  /NL IK! T W]\ d\N[
IKHVMI
S M[ANS  ML_CK! / IK! /IK!  T
 :[/ VM@LK!
k /UH /HS ! dRO!UH]\
XcISO! ! dGVG[Ah! [AW ]\ILK!]\O! WNSfLIaVMWW
IK! 8U!WN V\[A /QR /I]^ X
IK! f LTVMHS?IK! 8]\O!I!d
[/VMUHV\[AIaVMH [A E oIK! G]\O!IU!O! IGSE /aLD ]\IK! 
NLLO! 
]\UH /aVMHS
n/ UH]_ /[A]\HLO!QR U!I]\o%IK!  
]G QR! QR
UH]\IaL_C WWKHV/E DI]GPH  TS MLd\! MS
P!OHL MLDI ]fIK! 8 L
LKHVM !d@VMNLLO!  RUH ]\IR LK! ]\O! WNS
PH 8 SE  /
IK!]\O! d\KI
	LIaVMI 8S E /aL
C ML]\O!a[A  WQR Ia VMI ]\HLbXc]\a[A MSG IK! DLQT O!WNVMI]\HL	 ]^X_H SNS OHVMW%P!W] [aj! LHLI MV\SR]^X_IK!   /I  

[aK! MSO!W !d ! IV\LV L!d\W   /IIhEoMP! O!II LK!]\O! WNS
PH  I MSD]\O!I T IK! 
XcO!IO! 
Lb XcO!IK! /
XcO!W W
[aKHVM!d\ MLI ]RIK! 
[/VMI]\ GI]E]\ WS_]\ O!WNS8PH 
	[AOHLI ]\Q*S MLd\@VM DNLjGU! ]\! E o VTWNV/hE]\ O!I E /
`EO!I 
K! /WU
XcO!W
:S /IaVMW  MS@
EU!N[A RLQTO! WNVMI]\8]^Xg IK!  T AR IaV\[AI MSf[AIN[/VMW4 UHVMIKH LDLK! ]\O!WNS@PH 
[/VM MSG]\O! I]\f IK! D UH]_ /P!OH L MLVMHS
e8 /Ia VMWQR d\aVMI ]\8VMHVMWh! LNLLK! ]\O!WNS8PH 
[/VM  MSR]\O!I
IK!  QGVY]\	[AW ][a j
IaV\[A M L	I ]DQGVMjE LO! IKHVMIIK!  /hTVM  _CNS   /! ]\O!d\KR I]DKH VMH SW IK! VMQR]\O!I
]_C!dTIK!]\ O!d\ K8IK! /Q
]^X
[AO! / I


 
^
^
 
 

 
 
 
 
 
 

 
 

 


 

q

% /! /WWoH 
aV/hE o
	K!NLI  /HL /o
P!!]\OHLAo
h! 
! 6i ][A MLL]\ 
4
Gk  MLd\ J]^X

IK! 
 &	( '&%
&ao! UHVMd\ MLcE7]
q

	 
% $%
&

 MSI ]\GHS M V?V\L]\ II ]
! % 


 

m /jE /W / hEo
q/r\rq

 

V/hW]\/oH VMHSf 


%&)'

E7]
q/r\r

mCVMd\K!  /nMV\S /K
 	


 
#

 ! E  /aLIh

]^X

VMW

Xc]\!NVo

UHV\LL
[aK! MSO! W /_Xc]\VT
E O!UH /aL[/VMWNVM
i]E LI
k MLd\8VMHSGH VMWh! LNL	]^X_V
W WN VMQ1m]_C
Xc]\!NVoH !  Eo
VMW
]^X
LIK! MLNLAo! ! E /aLIh
efV\ LI /
k d\Ia VMW
E d\HVMW4 i ][A  ML L]\
q/r\r
Y M[AI

QR U!W /QT / Ia VMI
aV/R kVMd\W 
]\T]^X_ VMWQD /IhRH LIOH[AI]\. ^
Xc]\!NVo!  !  Eo
]^XgVMW
 /UH]\I/oH !E /a L I h
q/r\r

HS ]_

U! ]

	[A C

sq

VMW!i4]\UH ML[A O
q/r\rq

 /I	VMW

e8 /IaV

]_a[aK!I M[AIO! 

 	



%\oMUHVMd\ ML

Eo

e7E

PEo

\O!! 

mCVMd\K!  /nMV\S /K
%&)'

q/r\r

aV/hEo
V/ hW ]\/o
P! !]\ OHLAo	VMH S?

e8N [A]\U! ][A MLL]\ 
i MVMj?
efi

 &	( '&%
&ao 
VMfk /d\]!o
&

% $% 
	 

EK! /!d
		K! /K
OHVM!d
HVMWh! LNLVMHS@k  MLd\f]^X

EO!UH /aL[/VMWN VMG	]\QR U!O!I /
efV\ LI /
a[aK! I M[AIO! 
 ! Eo
q/r\r

i		 l
 
#




 	



nEo

]7]


Xc]\DV
VRkh HVMQRN[DH LIOH[AI]\@  !I
Xc]\!NVo
VMW
LDIK! MLNLAo ! E /aLIhf]^X

 









 
 

 

 
 
 
 
 






 
G
 
 
G
 
s
o
C
 

C


"

 
 

E

^
 
	
 

E
 

G

 
 
E
 

s

 

 
q
]

q

 

e


 
 
 
 
 
C
s
e
q
^
 


 



E
 

P

 
 

E
 
Q
q
e8 jE 
	W

LAo

\]\K!HL]\
q/r\rq

 #

%

%  

  



'&

i / IN[A 

Q7C
VMWWo	 !d\W A_]E]S

% / DVMHS8WNVM
\]\K!!h8p
 / k ML d\
 	

 
CVMd\ /ImO[Z
% /! /WW o\

VMWWN V\[A EoMVMHSD

EO!UH /aL[/VMWN VMe8 N[A ][A ]\QRU! O!I /
q/r\r

maVMH [aK8i  MSN[AI]\8 
EIaVMI /d\ MLVMH S8maVMH[aK

EQR IK
!   	ao UHVMd\ MLe
C7Co
QG
q/r
nC e@
C  /]\aS  /4 mO[ Z
6C7] e
mCVMd\K!  /nMV\S /K
Xc]\V
 /
&& $%
% Do ]ME  /QTPH /


" 
!  

	

q\q

Xc]\QGVMH [A f 	]\QR UHVMNL]\
\]\K!J% /! /WWVM HS
V\ S / GmCVMd\K!  /nMV\ S /K
i /

i][A MLL]\
e8]S /WNL
_CIK6V 
i][A MLL]\ 

EO!UH /aL[/VMWN VM

%&%
% DoUH VMd\  ML G7G
G3Q o
q/r\r
"$"$ "
 &	 &
!  
%&)'

% &
%d\KI! / 
mOH[A Gk
KO!H S /G
i% 
i][A MLL]\

		
 ! E /aL IhEoEO! d\OH LI
q/r\r

 

 




]^X

E /E /aVMW
 

%&)'

 ao
EIaVM

Xc]\aS

VMQGV\[aK!oH 
OHL I /K!]\O!I/o
 	

 
efVMd\N [4
CV/hE ]\O! I
E h!LI /Q

efV/hE]! o^
'& 


4V/hW]\

[A]\II/oVMHS
!   	AoH ! /P!OHVMh





K! 
q/r

]\]_CIn

VMWnVMHSRe


 %&)'
( 

#

\O!! 
	 /dEV\LAoH /\V\ S!Vo
q/r


e

 

	% /E /WH
E QTO!WNVMI]\
l\RH[A /QR / IaVMWe@ 
T
3 _CIa[aK
 %
&ao4 UHVMd\ ML
PE
PQoV\L
'& 
% &
&


q/s


EI /E /W ^
VMWWN V\[A 
i /
Xc]\ QGVMH [A DHVMWh! LNL]^Xg V8
E O! UH /aL[/VMWNVMDa[aK!I M[AIO! 
Xc]\!NVoH  !  Eo
IK! MLN LAoH ! E /a LI hD]^ XgVM W
q/r\r

efV\LI /

VMHS@V\S  / mCVMd\K! /nMV\S /K

EI /E /D^
VMW WNV\[A 
VG
E O!UH /aL[/VMWNVM
Xc]\QGVMH [A TLLO! ML ]^X
i /
%&%

#	  
e8N [A]\U!][A ML L]\ 
 &	&
&
% &
&

&
"$"$"

%&)'


PEo O! d\OH LI
o!UHVMd\ MLc C
 %&)'\o!E]\WO!QR  
q/r\r



Q

 

%
"

 
Z
 

r

 

 

 
 
 


 

q
]


 
 
^
 
 
 


 

'
C
 

 

^
 

%
%
"


'

E
 

q
C

 
 
 

G
 
 

q
E


 
p
 

 

 
 

 
 


 

 


 
 
 



Q
s
 

q
G


 
 
 
 




q

q
Q
r
 


 
 

L
E
 

q
e

 
 
 



%
%
"


q
r
E

C
r
G
 
mCVMd\K!  /nMV\S /K


  

 /

Q7E
k MLd\
VMHS
! %


%%$

kVMd\Wo*VMHS
VMWWN V\[A Eo6 a V/

EI /E /
V\S /
nfC  /]\ aS  /8m O[Z
]7]e
QR U!W  /QT /IaVM I]\? ]^X
'	 
% $% %&
AoHO! d\OH LI
q/r\r
&
 /W7^@ ML I CVMHSD pVMQR aVM D	LK! aVMd\K! NVM
^@ MLW  /hEo!C MV\ S !d! oHefD o
q/r\r

%& 

 #"$




	



'&

S!SNL]\


q
P

^
 
 
V
q
 



'


G
 

q
Q

 






 
 
 
E
 




8	


	

K! D S /IaVMW  MSR  M LO!WIaL]\P!IaVM!  MS

Xc]\ Q-IK!  \VM]\OHLLQTO!WNVMI]\R O!H LC VM D WNLI MSTK! / 

QG

 


 







	

 
!  

 
	 
   !#"
$$$ 
%' &  (  ) !#"
$ $ $  #*+  ,
- .%'&(
/%'&  !#"
#*+  ,
! 0!  !#"
12 3*+4 $',
5)&627   !#"
$1 $1# *+4  $4,
%  6  
7 !#"
#*+  ,
7 6  
7 !#"
#*+  ,
7   !#"
'.1824 #*9  4,
%'&!)0.%' & 6 ;:%!!2).% #"
<    4
	#*+   ,=*
76  >:%!!
).%#"
< ' 1 .1?	#*+   ,=*
@). %'  A )-@!B"
# *+  ',
%!!0 C
0&D"

44'  1' 4   $ 28E 1  $.?4 $4  
F*+4$,4=*G 1 ,4# *G ,41# *H', #*+,# *+ ,

E84' 4  44 4 $ 4 . 24 4  
3*H,4$=*H' $, =*9, 4=*H8,#*+4 ,

4E84 ' $ 1 
'$
4#*+ , #*+ ,# *+, #*+  ,# *+ ,

@).%' C
0&D"

5)&6C
0&D"

- 6C
0&D"

4?  $ $  1 41$ 1
#*+,8F*9. ',8 F*91, #*+ , #*GE ',
5)&6/) 7. % .%&
) 
	I2J  $ .<'$ 1 2	 13*+',
A )( 25 75 )&6/ & 
	$ 1 $1 <   $K 	$= *+
A )( 
)  !0 2@).%' ="
3*9 4
%'&!)0.%'&0&F "
  
A )( 
- 6 0 &
	I   <' 4 ?	=*H4 1
- 67 -.%%'=LM !!#"
43*9$ $$  ,
# *+1 44 1 ,
%/ON8%'PQR-/SB"
4#*+4$$
%/ON8%'PQTUT
S3"
4#*+4$$
!/  70/="
4#*+4$$
!TWYPZ

!!VWXT

$

<

Q
s


4




,
"


4


,
"


4




,
"


4


,
"

	


Q7e

 
	 
   !#"
14
%'& ( )   !#"
1 4# *+   ,
-  .%' &(
/%'& !# "
#*+  ,
! 0!  !#"
.4 $=*9 $ ,
5)&627   !#"
44 
#*+  ,
%  6 
7 !#"
# *+   ,
7  6 
7 !#"
# *+   ,
7   !#"
44=*G 1$  ,
%'&!)0.% '&  6 ;:%!!2) .%# "
< '  ?	# *+   ,#*
76  >:%!!
).%#"
<   4
	#*+  ,# *
@). %'  A )-@!B"
'$# *9 1'$$ ,
%!!0 C
0&D"


'44
.41 
1 . 4
 $1 4
=*9,1#*+  ,#*G4 ,3 *9, '=*9$, #*+  ,

 

@).%' C
0&D"

5)&6C
0&D"

- 6C
0&D"

.4
1 41 
$  
E 
41'$
F*+4,'=*+ $,43*94,2 1#*9 $,4#*+ ',

'.8
14
4#*9$, #*+,#*+ ,#*+ ,#*+,

4 44$  
'
$
#*+,4#*9 $4 ,3*9 4, 4#*9  ,1 =*+',
5)&6/) 7. % .%&
) 
	I2J  < 14
 	1 =*9 ,
A )( 25 75 )&6/ &
	I4 4< 
	F*H
A )( 
)  !0 2@).%' ="
4#*+ $ 
%'&!)0.%'&0&F "
4$4 
A )( 
-   60&
	4$ 4 < '  ?	=*9 
- 67 -.%%' =LM ! !#"
=*+4 $ 1,
4#*9 $1 ,
%/ON8%'PQR-/SB"
4#*9$4
%/ON8%'PQTUT
S3"
4#*9$4
!/  70/="
4#*9$4
!TWYPZ

!!VWXT

'$

<



4



,
"


4


,
"


4





,
"


4



,
"

	
' &#

Q3P

 
	 
   !#"
81'
%'& ( )   !#"
8 1 ' # *+  ,
-  .%' &(
/%'& !# "
#*+  ,
! 0!  !#"
1$.2=*G 4 ,
5)&627   !#"
4 4 14 #*9$ $ ,
%  6 
7 !#"
# *+   ,
7  6 
7 !#"
# *+   ,
7   !#"
44$$1# *+4  4,
%'&!)0.% '&  6 ;:%!!2) .%# "
< 14   ?	#*+   ,=*
76  >:%!!
).%#"
<   4
	#*+    ,#*
@). %'  A )-@!B"
4#*+  1,
%!!0 C
0&D"

E4'  124 4 . 2 4 1
 $4  4
F*+, #*9 $ ,44# *+',3 *9, #*9 ,# *+ ,

 4

44' . '$ $2 4 4 1? $1 4 
 $4  4
#*+14,4#*+ ,4 1#*+',
 E3*9 $,#*H' ,

$4$ 1$ $ 
 
$4#*9,1 1#*+ 1 ,#*+ 4 ,# *+, #*+ ,

@).%' C
0&D"

5)&6C
0&D"

- 6C
0&D"

44$1 1  '$ $ 42  $$ 
#*+,'=*+4 , #*G, 1#*9 , $=*9. ',
5)&6/) 7. % .%&
) 
	I2J4 44  < 1$ '? 	 =*9$ ,
A )( 25 75 )&6/ & 
	I4 4 1 4< 444 ?	#*+
A )( 
)  !0 2@).%' ="
4#*+4 4 
%'&!)0.%'&0&F "
4111  
A )( 
-   60&
	411 1  < 14  
	 =*+4  
- 67 -.%%' =LM ! !#"
1#*H'  ,
#*G  ',
%/ON8%'PQR-/SB"
4#*+44
%/ON8%'PQTUT
S3"
4#*+44
!/  70/="
4#*+44
!TWYPZ

!!VWXT

'

<



4



,
"


4


,
"


4




,
"


4


,
"

	
&



Q7Q

 
	 
   !#"
4.1 
%'& ( )   !#"
 4 .1  # *+  ,
-  .%' &(
/%'&  !#"
#*+  ,
! 0!  !#"
1'$12# *+1 $.' ,
5)&627   !#"
 ' 1E
#*H' $ ,
%  6 
7 !#"
# *+   ,
7  6 
7 !#"
# *+   ,
7   !#"
'$$' '$=*H4 1 ,
%'&!)0.%' & 6 ;:%!!2). %#"
<   $. H 2	#*+    ,=*
76  >:%!!
).%#"
<  $4 $. 1 ?	#*+  ,= *
@). %'  A )-@!B"
4 $ #*+.4 ,
%!!0 C
0&D"

481 ? 1 $  1 11 $ ?   $  4 E 1 
 .' 
=*+,2#*9 $ ,1#*9 1,2 #*+  ,3 *H'$ ,#*+ ,

4E4
 4 8 124 1 4$ 2 1   4 1 
4#*+1,24#*9 ,4 #*9$,2 F*+ ,3*H'$ ,

$E'11? $  E'$ $ . 
8F*9,1=*+1 1,# *+',#*+ ,#*+ ,

@).%' C
0&D"

5)&6C
0&D"

- 6C
0&D"

1$$ ? 1 4 $   1  
E'  4 . 
#*+,
E3*H 4,#*9$ ,2F*+ ,21# *H' ,
5)&6/) 7. % .%&
) 
	I2J E41 11 $<  1  $
 	1 #*+4  ,
A )( 25 75 )&6/ & 
	I ' 1E <E 41 1 1$
	F*+1
A )( 
)  !0 2@).%' ="
3*9$1 
%'&!)0.%'&0&F "
44$ $ .'1
A )( 
-   60&
	44 $ $ . 1<   $.H 2	 4#*+ .
- 67 -.%%' =LM ! !#"
F*+4 4 4 ,
4= *9  11 ,
%/ON8%'PQR-/SB"
4#*H'
%/ON8%'PQTUT
S3"
4#*H'
!/  70/="
4#*H'
!TWYPZ

!!VWXT

$1

<


 


4



,
"


4


,
"


4




,
"


4


,
"

	
'

@).%' C
0&D"

5)&6C
0&D"

- 6C
0&D"

 
	
   !#"
11$ 
%'& ( )    !#"
11 4 4 421 1#*9 4',
-  .%' &(
/%'& !# "
'4 . 2#*H' $ 1,
!0!  !#"
1$14$$ 1#*91 1'$ ,
5)&627   !#"
4'$  1 
4#*9 14 $ ,
%  6  
7 !#"
#*+  ,
7 6  
7 !#"
#*+  ,
7   !#"
411$ #*+4 1'$  ,
%'&!)0.% '&  6 ;:%!!2) .%# "
<  1 1$  2	#*+  ,= *
76  >:%!!
).%#"
< .  4 4?	#*+  ,= *
@). %'  A )-@!B"
1 '$#*+1  ,
%!!0 C
0&D"

$4$1 241  41 1 24 8  $? 4 $  $ 2  1 '    $ 
#*9,41#*9  ,43*+ 1,2 =*H ,= *G ,#*+ ,

8'$
4$$  4 141  4 $ 1 44 1? 14 1 
8F*+,4#*+  ,41#*+4 ,4 3*+1 ',2= *H8,

'$$.
 1.1 
  
44#*9 4,1F*9$ ,4# *+4 ,#*+ ,#*+  ,

1. ' 1 $ . K$ $   14? E 841 
#*+,4#*G ,' =*+11, 3*+4 , #*9 ,
5)&6/) 7. % .%&
) 
	I2J  4$  $<4  1  4
 	 =*H ,
A )( 25 75 )&6/ & 
	4 '$  1 <  4$  $
	#*9$
A )( 
)  !0 2@).%' ="
3*94 $
%'&!)0.%'&0&F "
44$ $ $$
A )( 
-   60&
	44$ $ $ $< 1  1$ 2	 =*+4 4
- 67 -.%%' =LM ! !#"
'=*+4 4 $,
4#*+1 $1 $ ,
%/ON8%'PQR-/SB"
4#*+4
%/ON8%'PQTUT
S3"
4#*+41
!/  70/="
4#*+4$.
!TWYPZ

!!VWXT

$1

<

Q
r


4



,
"


4


,
"


4




,
"


4


,
"

	


@).%' C
0&D"

5)&6C
0&D"

- 6C
0&D"

 
	 
 !#"
'$
%'& ( )   !#"
411 '  
11# *+ 4 4,
-  .%' &(
/%'& !# "
  2# *9 $ $,
!0!  !#"
$1$42# *+  $,
5) & 627 !#"
444 $ 1 '$=*H4  ,
%  6 
7 !#"
# *+   ,
7  6 
7 !#"
# *+   ,
7   !#"
 424= *+4 1,
%'&!)0.% '&  6 ;:%!!2). % #"
< 11 $  H	# *+   ,=*
76  >:%!!
).%#"
< $.  14$
	#*+   ,=*
@). %'  A )-@!B"
$ # *+ 4 4 ,
%!!0 C
0&D"

$E4$24 1 11? 44  . K. '$. $44  
#*9$.',4=*+ ',4 F*9$1,2 $=*+ ,1#*G4 ,# *+4,

.84

'44
4  $   1 1 K . $ 4   11 
#*G1,4#*9. ',4 #*+11,2 =*+4 ,2=*+1 ,

4 
 $ 4'4 1 
1#*+,1=*+ ,#*+ ,# *+ ,#*+,

81$ $2$   1' 4 1 4  4
#*+,#*+ ',$=*9$ ,1#*+4  , 1#*9$$ ,
5)&6/) 7. % .%&
) 
	I2J 1 1 4 <1 4$ 1 ?	 1#*+$,
A )( 25 75 )&6/ & 
	44 4 $1  < 1 142	# *9$
A )( 
)  !0 2@).%' ="
4#*+ 11 
%'&!)0.%'&0&F "
41  
A )( 
- 6 0& 
	4 1   <1 1$  82 	= *+4$.
- 67 -.%%' =LM ! !#"
'=*+4  ,
'=*+1  $1 ,
%/ON8%'PQR-/SB"
4#*+
%/ON8%'PQTUT
S3"
4#*+4
!/  70/="
4#*+'$
!TWYPZ

!!VWXT



<

r
]


4



,
"


4


,
"


4




,
"


4


,
"

	
JOURNAL OF COMPUTER SCIENCE AND ENGINEERING, VOLUME 12, ISSUE 1, MARCH 2012 
 
 

5 

Computer Architecture and Algorithms for 
High Performance Computing through Pa-
rallel and Distributed Processing 
P. Venkata Subba Reddy  

Abstract—There is a very high need of High Performance Computing (HPC) in Many applications like space science to Artificial 
Intelligence.  HPC  shall  be  achieved  through  Parallel  and  Distributed  Computing.  In  this  paper,  Parallel  and  Distributed 
algorithms  are  discussed  based  on  Parallel  and  Distributed  Processors  to  achieve  HPC.  The  Programming  concepts  like 
threads, fork, sockets and par do are discussed with some simple examples for HPC.  

 Index Terms— High Performance Computing, Parallel and Distributed Algorithms, Computer Architecture, Computer 
Programming 

——————————      —————————— 

1  INTRODUCTION 

 Computer  Architecture  and  Programming  play  an  im-
portant  role  for  High  Performance  computing  (HPC)  in 
large  applications  Space  science  to  Artificial  Intelligence 
[].  The  Algorithms  are  problem  solving  procedures  and 
later  these  algorithms  transform  in  to  particular  Pro-
gramming  language  for  HPC.  There  is  need  to  study   
algorithms  for  High  Performance  Computing.  These   
Algorithms  are  to  be  designed  to  computer  in  reasonable 
time  to solve  large problems  like wather  forecasting, Tsu-
nami,    Remote  Sensing,  National  calamities,  Defense, 
Mineral  exploration,  Finite-element,  Cloud  Computing, 
and  Expert  Systems  ect.  The  Algorithms  are  Non-
Recursive  Algorithms,  Recursive  Algorithms,  Parallel 
Algorithms and Distributed Algorithms. 
 
The   Algorithms must be supported the Computer Archi-
tecture.  The  Computer  Architecture  is  characterized  with 
Flynn’s[2]  Classification  SISD,  SIMD,  MIMD,  and  MISD. 
Most  of  the  Computer  Architectures  are  supported  with 
SIMD  (Single  Instruction  Multiple  Data  Streams).  The 
class  of  Computer  Architecture  is  VLSI  Processor,  Multi-
processor, Vector Processor and Multiple Processor[1,3]. 

2  ALGORITHMS 
  There  are  Non-Recursive  Algorithms,  Recursive  Algo-
rithms,  Parallel  Algorithms  and  Distributed  Algo-
rithms[5,10].  
In  the  following  Algorithms,  Computer  Programming 
and Architectures are discussed   
 
2.1  Non-Recursive Algorithms 
Non-Recursive  Algorithms  are  systematically  applied  to 
the problems by analyzing the efficiency.  
Consider  the  algorithm  of  finding  maximum  element  in 
the array A ([0 .. n-1])     
maxval  A[0] 
for  i0 to  n-1 do 

    if  A[i] > maxval 
         axval  A[i] 
return maxval 
 
The problem will be analyzed as 
 
C(n)  is  number  of  computations.  Where  n  is  input  size. 
The number of times of operations in execution 
 
            n-1 
C(n)=  ∑  1 = n=1 Є Ө(n) 
             i=1 
 
2.2  Recursive Algorithms 
 The  recursive algorithm  is binary expansion whose  num-
ber of executions is multiplication. 
For instance n! = n*n-1*n-2*…*1 
Consider the algorithm of finding n! 
 
If n=0 return 1 
Else return F(n-1)*n 
 
The number of multiplications M(n) needs to compute  
M(n)=M(n-1) +1  for n>0  
M(0)=0 
 
2.3 Parallel Algorithms 
Parallel  Algorithms  are  designed  to  apply  on  problem  to 
compute  parallel  whose  number  of  executions  is  inde-
pendent. 
For  instance  parallel  sum  of  odd  and  even  number  up  to 
A[n]. 
Consider  the  algorithm  to  find  parallel  sum  of  odd  and 
even numbers unto A[n]. 
odsum0, evensun  0 
for  i0 to  n-1 stem 2 do 
    oddsum oddsum +A[i}] 

© 2012  JCSE 
www.journalcse.co.uk 
 

6 

 

return oddsum 
for  i1 to  n-1 stem 2 do 
    evensum evensum +A[i}] 
return evensum 
 
Consider  the  algorithm  to  find parallel matrix multiplica-
tion of A[n, n] and B[n,n].  
 
for i0 to n-1 do in parallel 
      for j0 to n-1 do in parallel 
            C[I,j]0 
             for k0 to n-1 do in parallel 
                   C[I,j]  C[I,j] + A[I,k] * B[k,j] 
return C 
 
 
2.4  Distributed Algorithms 
Distributed    Algorithms  are  designed  to  apply  on  prob-
lems  to  compute  on  Distributed    Systems  whose  execu-
tions are independent and distributed . 
Consider the algorithm to compute two transactions   
The  algorithm  of  the  problems  is  designed  to  execute  on 
Distributed Systems.   
  
       if (fork()) 
      {  
      #Transaction1; 
}    
       else 
       { 
       #Transaction2 
} 

3  PARALLEL AND DISTRIBUTED ARCHITECTURE 
AND PROGRAMMING 

Parallel  and  distributed  Computer  Architecture  of  pro-
cessors  is  defined  through  Flynn’s  classification  (SISD, 
SIMD, MIMD, MISD) 
 
 
 
 
 
 
 
 

d 
MuliCompu-

d 
Multiproces-

Distributed 

Computers 

Loosely 

Parallel 

Tightly 

Couple

Couple

sors 

and 

Switched 

Bus 

Bus 

ters 

Switched 

 

Parallel and Distributed Computer Systems 
 
There  are  four    minimum  number  of  Architecture  are 
VLSI Processor, Multiprocessor, Multiple Processor(Multi 
Computer)   and Vector Processors    . These Architectures  
are discussed in the following 
 
3.1  VLSI Processor 
VLSI  Chip  has  Computer  components  such  as  Processor 
arrays  (  Processing  Elements),  Memory    array  and  large 
scale  switching  networks.  Communicate  the  PEs  for  im-
plementing Parallel Algorithms with VLSI Chip.  

 
 
 
 
 
 
 
 
 
 

PE 

PE 

PE 

PE 

PE 

PE 

PE 

PE 

PE 

PE 

PE 

PE 

PE 

PE 

PE 

PE 

 
4x4 mesh Processing Elements of VLSI Processor 
 
Consider  the  Parallel  algorithm  for  odd  and  even  sum  of 
n elements for VLSI Processor. 
Compute oddsum, evensum in parallel 
One of the PEs set to oddsum: 
The  Perl  program  for  above  parallel  algorithm  using 
threads is given by 
The “use thread” creates one or more threads.  
 
 use threads; 
           $thr1= threads->new(\&ascending); 
          $thr2= threads->new(\&decending); 
                   ; 
           sub ascending { 
                my $num=0; 
do { $num=$num+1; 
 print " $num\n"; 
           } 
            while ( $num<10) 
             } 
sub decending { 
              my $num=10; 
do {  print " $num\n"; 
$num=$num-1; 
           } 
            while (  $num>0)     
}  
$thr1-> join; 
$thr2->join; 
 
3.2  Multi Processor 
 Multiprocessor  Computer  Have  been  modeled  as  n  Pro-
cessor    and  Parallel  Random  Access  Machine  (PRAM) 
with  shared  memory.  The  Parallel  Algorithms  will  be 
implemented with PRAM. 
 
 
 
 
 
 
 

P 
P2

Pn

P1

P 

 

 

 

 

 

 

 
Shared 
P 
Multiprocessor System 
Memory 
 
Consider the Parallel Algorithm for computation for Mul-
tiprocessing System 
 oddsum0 

 

 

7 

for  i0 to  n-1 stem 2 do 
    oddsum oddsum +A[i}] 
return oddsum 
 
one of the PEs set to evensum: 
evensun  0 
for  i1 to  n-1 stem 2 do 
    evensum evensum +A[i}] 
return oddsum 
 
The Perl Program  for above problem using  fork  for Paral-
lel Processing 
 
3.3  Multi Computer System 
Multi  Computer  involves  sequence  of  routers  and  chan-
nels  for    as  number  of    Computer  Systems  with Message 
Passing  Interconnection  Network.      The  Parallel  Algo-
rithms will be implemented with this Network 
 
 
.  
 
 
 
 
 
 
 

            Interconnection Network 

                 Message-Passing 

M  P 

M  P 

P  M 

P  M 

 

 

 

 

 

 

 

P 

M 

P 

M 

P 

M 

P 

M 

P 

M 

P 

M 

 

 
Multi Computer System 
 
Multi computer System/ Distributed system 
 

Consider the Parallel/Distributed   algorithms in       Multi  
Computer/distributed    System.  These  algorithms  will  be 
computed    in  two ways Client/Server and Remote Proce-
dural Calls 
  
Remote Procedural Computation 
 
The Client  request  the Data  from  the Server and  the Serv-
er sends the Data from the Server buffer. 
The Perl Program gets Time from Server.  
  
 
 
 
 
 
 
Remote Procedural Computation 

   CCC    Request 
 
Host 
Kernel 

 
Reply 
  Target 
Kernel 

#Perl Client  Program 
#!/usr/bin/perl 
use IO::Socket; 
$socket = new IO::Socket::INET (  
                                  PeerAddr  => '127.0.0.1', 
                                  PeerPort  =>  7008, 
                                  Proto => 'tcp', 

 

 

$peer_address = $client_socket->peerhost(); 
$peer_port = $client_socket->peerport(); 

                               ) 
or die "Couldn't connect to Server\n"; 
    $socket->recv($recv_data,1024); 
    if($recv_data){ 
localtime()=$recv_data; 
    print "Recieved :$recv_data\n"; 
    } 
 
#Perl Server Program 
#!/usr/bin/perl 
use IO::Socket; 
$| = 1; 
$socket = new IO::Socket::INET ( 
                                  LocalHost => '127.0.0.1', 
                                  LocalPort => '7008', 
                                  Proto => 'tcp', 
                                  Listen => 5, 
                                  Reuse => 1 
                               ); 
die "Coudn't open socket" unless $socket; 
print "\nTCPServer Waiting for client on port 7008"; 
while(1) 
{ 
 
my($new_sock,$buf); 
        $buf=sum(); 
$client_socket = ""; 
 
 
$client_socket = $socket->accept(); 
 
 
 
 
 
print "\n I got a connection from ( $peer_address 
, $peer_port ) "; 
        $client_socket->send($buf); 
        close $client_socket; 
sub sum() { return 2+3;} 
} 
 
The Host Machine   sends the Data to the Target Machines 
and  Target  Machine  processes  the  Data  and  send  result 
Data to the Host Machines. 
The  Perl  program  for  distributed  algorithm  may  imple-
ment using socket & fork. 
 
3.4  Vector Processors 
Super Computers are model with Vector Processor. Super 
Computers are specified by 5-tuples 
 
M = <N, C, I, M, R> 
Where  
N=number of processors 
C= Set of instructions 
I is set of instructions for parallel execution 
M=  Set of  Processors 
R= Set of routing functions 
 
 
 
 
 

8 

 

Scalar 

al  

function-

 
             Scalar Processor          Vector Processor  
 
 
 
 
 
 
 
 
 
 
 
 
 

pipelines 
Scalar 

Main Memory 

tional pipelines 

gram&Data 

Vector  

regis-

ters 

control 

control 

unit 

Vector 

Vector 

func-

tional  pipelines 

unit 

 

 

Vector 

func-

(Pro-

Mass 

Storage 

Host 

Comput-

er 
                                                        I/O (User) 
 

 

The Architecture of Vector Supercomputer 

 
 Parallel  Algorithms  are  designed  to  compute    big  prob-
lems    like  Weather  forecasting,  Remote  sensing,  Mineral 
exploration,  Oceanography  ect  in  parallel  using  Super 
Computers. 
 Consider  the  algorithm  to  find  Parallel  Matrix  Multipli-
cation A[nxn] and B[nxn], where n is very large.  
The  Perl  program  for  above  parallel  algorithm  for  matix 
multiplication  is given by 
 
for i0 to n-1 do in parallel 
for j0 to n-1 do in parallel 
 
                  P[i,j] set to  C[i,j]0 
             for k0 to n-1 do in parallel 
A[I,k] and B[k,k]broadcast to P[i,j]  
                   C[I,j]  C[I,j] + A[I,k] * B[k,j] 
return C 
The  Perl  program  for  above  parallel  programming  for 
matix multiplication  using  par  do  in  FORTRAN    is  given 
by 
 
par do 300 i = 1, n 
       par do 200 j = 1, n 
            par do 100 k = 1, n 
      a(i,k) = a(i,k) + b(i,j) * c(j,k) 
                   100 continue 
             200 continue 
   300 continue 

4  CONCLUSION 

High  Performance  Computing  is  required  when  large 
computations  of  the  problems.  HPC  shall  be  performed 
through  the  Parallel  and  distributed  Algorithms.  The  Pa-
rallel  and Distributed  are  discussed  based  on     Computer 
Architecture.  The  Class  of  Algorithms  and  Class  of  Com-
puter  Architecture  are  discussed.  The  Programming  con-
cepts  like  threads,  fork,  sockets  and  Par Do  are  discussed 
for  HPC.  Some  simple  examples  are  discussed  for  HPC. 
The  examples  shall  be  extending  to  large  problems  like 

Grid  Computing  and  Cloud  Computing.  Usually  Fotran 
is used for HPC. The Perl  and Java Programming are laso 
usefull for HPC[11].  
  

ACKNOWLEDGMENT 

The  author  wishes  to  thank  B.Tech.,  M.Tech.,  and  Ph.D 
Students for helping me.  

REFERENCES 

[1] 

[7] 

 Kai  Hwang,  Advanced  Computer  Architecture,  McGraw-Hill, 
New Delhi, 1993. 
[2]  M.  J.  Flynn,  “Some  Computer  Organizations  and  Theire  Effec-
tiveness”,  IEEE  Transactions  on  Computers,  vol.29,  n0.9, 
pp.948-960, 1972. 
[3]  K.  Hwang,  Advanced  Parallel  Processing  and  SuperCompuer 
Architecture”, Proceedings of IEEE, vol.75, 1987. 
[4]  K. Hwang anf  F. A Briggs, Computer Architecture  and  Parallel 
Processing, McGraw-Hill, New Delhi, 1992. 
[5]  Aho, Hopecroft  and Ulman, Design  and  Analysis  of Computer 
Algorithms, pearson, 2002. 
[6]  Martin  Brown,  Perl  The  Complete  Reference,  Tata  Mc  Graw-
Hill, New Delhi,2001. 
John C. Knight, The current status of super computers Original 
Research  Article  Computers  &  Structures,  Volume  10,  Issues  1–2, 
Pp.401-409, April1878. 
[8]  Horst  D  Simon  Erich  Strohmaier,  Jack  J  Dongarra,  Hans  W 
Meuer,    The  marketplace  of  high-performance  computing 
Original  Research  ArticleParallel  Computing,  Volume  25,  Issues 
13–14, pp. 1517-1544, Decmber 1999. 
[9]  Guillermo L. Taboada, Sabela Ramos, Roberto R. Expósito, Juan 
Touriño,  Ramón  Doallo  ,Java  in  the  High-Performance  Com-
puting  arena:  Research,  practice  and  experience   Original  Re-
search ArticleScience of Computer Programming July 2011. 
[10]  N. Sim, D. Konovalov, D. Coomans  High-Performance GRID 
Computing in ChemoinformaticsComprehensive  Chemometrics,  
pp. 507-539, 2009. 
[11]  P. Venkata Subba Reddy, “Object-Oriented Software Engineer-
ing through Java and Perl”, CiiT International Journal of Soft-
ware Engineering and Technology, July 2010. 

 
 

Dr.  P.  Venkata  Subba  Reddy  was  Professor  and 
Head, Department of Computer Science and Engineering, MeRITS, 
Udayagiri,  India  during  2006-07. He  is  currently  Associate  Profes-
sor  in  Department  of  Computer  science  and  Engineering,  College 
of Engineering, Sri Venkateswara University, Tirpathi,  India work-
ing  since  1992.  He  did      Ph.D  in  Artificial  Intelligence,  1992).    Sri 
Venkateswara  University,  Tirpathi,  India.  .  He  did  Post  Doctor-
al/Visiting  fellowship  in  Fuzzy  Algorithms  under  Prof.  V.  Rajara-
man,  SERC,IISC/JNCASR,  Bangalore,  India.  He  is  actively  en-
gaged  in  Teaching  and  Research  work  to  B.Tech.,  M.Tech.,  and 
Ph.D  students.    He  is  actively  in  doing  research  in  the  areas  of 
fuzzy  systems,  database  systems,  Software  Engineering  ,  Expert 
Systems  and Natural  language  processing. He published  papers  in 
reputed  National  and  International  journals.  He  is  an  Editor  for 
JCSE 
 

 

                            Volume 3, Issue 2, February 2013                                    ISSN: 2277 128X 
International Journal of Advanced Research in 
  Computer Science and Software Engineering 
                                                      Research Paper 
                                Available online at: www.ijarcsse.com 
Performance Improvement of MIPS Architecture by Adding 
New Features 
                                Galani Tina                                                                                                   R.D.Daruwala 
                     Department of Electrical   
                                                                       Department of Electrical   
             VJTI College of Engg, Mumbai India                                                             VJTI College of Engg, Mumbai I ndia  
 
Abstract  -  RISC  or  Reduced  Instruction  Set  Computer  is  a  design  philosophy  that  has  become  a  mainstream  in 
Scientific  and  engineering  applications.  Increasing  performance  and  gate  capacity  of  recent  FPGA  devices  permits 
complex  logic  systems  to  be  implemented  on  a  single  programmable  device.  This  paper  targets  to  develop  a  32-  bit 
MIPS  RISC  processor  architecture  in  VHDL  language  that  detects  the  pipeline  hazards  during  the  multithreading 
and  reduce  Cycle  per  instruction  (CPI)  by  eliminating  the  pipeline  hazards.  The  module  functionality  and 
performance  issue  like area, power dissipation and propagation delay are analysed at 90nm process  technology using 
Virtex4 XC4VLX15 XILINX tool. 
 
Keywords: MIPS Processor, Reduced Instruction Set Computer (RISC), VHDL, Pipeline, Xilinx 12.1, FPGA   
 

I. 
INTRODUCTION 
Nowadays,  computersand mobile phones  is  indispensable  tool  for most of  everyday  activities. This places  an  increasing 
burden on  the embedded microprocessor  to provide high performance while  retaining  low power consumption and small 
die size, which increase the complexity of the device [3].  
However,  as  products  grow  in  complexity  more  processing  power  is  required  while  the  expectation  on  battery 
life  also  increases.  With  the  rapid  development  of  silicon  technology  RISC  includes  extensions  to  RISC  concepts  that 
help  achieve  given  levels  of  performance  at  significantly  lower  cost  than  other  systems.  The  main  features  of  RISC 
processor are the instruction set can be  hardwired [7] to speed instruction execution. With reconfigurable devices such as 
FPGAs based core design any set o f task can be configured. It sustains any system  level change without costly hardware 
replacement and thus the design process is very fast and cost effective.   
In  the  present  work,  the  design  of  modules  to  remove  pipeline  hazards  along  with  MIPS  processor  architecture  is 
presented.  It  has  a  complete  instruction  set,  instruction  and  data  memories,  32  general  purpose  registers,  Arithmetical 
Logical  Unit  (ALU)  for  basic  operation,  forwarding  unit,  hazard  detection  unit  and  flushing  unit.  The  instruction  cycle 
consists  of  five  stages  namely  fetch,  decode,  execute, memory  access  and  write  back. Control  unit  determines  the  types 
of  instruction  to  execute.  The  remainder  of  this  paper  is  organized  as  follows.  Section  II  explains  types  of  hazards  and 
design  modules  to  remove  hazards.  Section  III  presents  the  implementation  of  modules.  Section  IV  presents  the 
simulation  results  implemented  in  advanced  90nm  process  technology.  Section  V  discusses  summary  with  the 
implementation of the MIPS RISC design topology. The final section presents the Conclusion and References.  
 

II.  BACKROUND 
Pipelining,  a  standard  feature,  is  an  implementation  technique  used  to  improve  both  CPI  (Cycle  Per  Instruction)  and 
overall  system  performance   [1].  Pipelining  allows  a  processor  to  work  on  different  steps  of  the  instruction  at  the  same 
time,  thus  more  instruction  can  be  executed  in  a  shorter  period  of  time.  Thus  in  pipelining  each  module  of  MIPS 
processor  does  not  wait  for  the  previous  instruction  to  finish  before  it  can  execute.    Pipelining  the  MIPS  processor 
introduces  events  called  hazards  [1],  which  prevents  the  next  instruction  in  the  instruction  stream  from  being  executing 
during its designated clock cycle and reduce the  speed of the processor. The  types of hazards include structural, data an d 
control hazards.   
 

  Structural  hazards  arise  when  the  flow  of  instructions  requires  more  hardware  resources  than  those  available 
on the platform [1].  
  Data hazards arise when there is a data dependency between the current instruction and the previous instruction 
in the pipeline.  
  Control  hazards  arise  when  there  is  a  change  in  the  flow  of  the  program  (branch  instruction  that  changes  the 
PC).  
A.)  Structural  hazards:  Structural  hazards  arise  when  the  hardware  cannot  support  the  combination  of  instruction  that 
we  want  to  execute  in  the  same  clock  cycle.  Fortunately,  the  MIPS  instruction  set  is  designed  to  be  pipelined, 
making  it  fairly  easy  for  designers  to  avoid  structural  hazards  when  designing  a  pipeline.  However,  if  the  MIPS 

© 2013, IJARCSSE All Rights Reserved                                                                                                           Page | 423 

Tina et al., International Journal of Advanced Research in Computer Science and Software Engineering 3(2), 
February  - 2013, pp. 1-6 
processor  had  been  designedwith  one  memory  to  be  shared  between  both  instruction  and  data  instead  of  two 
memories (Instruction memory and Data memory), then structural hazard would occur.  
B.)  Data hazards: Data hazards arise when an instruction depends on the result of a previous instruction i n a way that is 
exposed  by  the  overlapping  of  the  instructions  in  the  pipeline   [1],  thus  causing  the  pipeline  to  stall  until  the  results 
are  made  available.  One  solution  to  this  type  of  data  hazard  is  called  forwarding,  which  supplies  the  resulting 
operand to the dependant instruction as soon it has been computed.  

 
Fig. 1 Pipelined Data Dependencies 
 

 

 
Fig. 2 Pipelined Data Dependencies Resolved with Forwarding 
 
While  forwarding  is  an  exceptional  solution  to  data  hazards  it does  not  resolve  all of  them  [1]. One  instance  is when  an 
instruction attempts to read a register value that is going to be supplied by a previous load instruction that writes the sam e 
register,  called  a  load-use  hazard.  At  the  same  time  the  load  instruction  is  reading  data  from  memory,  the  subsequent 

 

© 2013, IJARCSSE All Rights Reserved                                                                                                           Page | 424 

Tina et al., International Journal of Advanced Research in Computer Science and Software Engineering 3(2), 
February  - 2013, pp. 1-6 
instruction executing  in  the execution  stage with  the wrong data value. The only  solution here  is  to stall  the pipeline and 
wait  for  the  correct  data  value  being  used  as  an  operand.  In  order  to   detect  such  hazards,  MIPS  introduces  a  Hazard 
Detection Unit  [4]  during  the  instruction decode  stage  so  that  it  can  stall  the  pipeline  between  a  load  instruction  and  the 
immediate instruction attempting to use the same register. 
 

Fig. 3 Pipelined Data Dependencies Requiring Stall 
 

Fig. 4 Pipelined Data Dependencies Resolved with Stall 

 

 

 
C.)  Control  hazard:  The  last  type  of  hazard  is  a  control  hazard  also  known  as  a  branch  hazard.  These  hazards  occur 
when  there  is  a  need  to  make  a  decision  based  on  the  results  of  one  instruction  while  other  instructions  continue 
executing  [1].  For  example,  a  branch  on  equal  instruction  will  branch  to  a  non -sequential  part  of  the  instruction 
memory  if  the  two  register  values  compared  are  equal.  While  the  register  values  are  compared,  other  instructions 
continue  to  be  fetched  and  decoded.  If  the  branch  is  taken,  the  wrong  instructions  are  fetched  into  the  pipeline  and 
must  somehow  be  discarded.  Figure  5  shows  three  instructions  that  need  to  be  discarded  after  it  is  determined  the 
branch instruction will be taken.  
A  common  solution  to  these  hazards  is  to  continue  instruction  execution  as  if  the  branch  is  not  taken.  If  it  is  later 
determined  that  the  branch  is  taken,  the  instructions  that  were  fetched  and decoded must  be  discarded which  can be 
achieved by flushing some of the pipeline registers. Flushing means that all values stored in the pipeline registers are 
discarded or reset. However in order to reduce the branch hazard to 1 clock cyc le, the branch decision is moved from 
thememory  pipeline  stage  to  the  instruction  decode  stage.  By  simply  comparing  the  registers  fetch  it  can  be 
determined if a branch is to be taken or not.  

 

© 2013, IJARCSSE All Rights Reserved                                                                                                           Page | 425 

Tina et al., International Journal of Advanced Research in Computer Science and Software Engineering 3(2), 
February  - 2013, pp. 1-6 

 

Fig. 5 Pipelined Branch Instructions 
 
III.  IMPLEMENTATION
A  32-bit  MIPS  processor  was  designed,  tested  and  synthesized.  For  improving  performance  processor  has  following 
attributes: 
(i) 
Hazard Detection Unit and Correction 
(ii) 
Forwarding Unit 
(iii) 
Flushing Unit to flush IF/ID pipeline register [4] 
In implementation for lower CPI first detect a hazard and then forward the proper value to resolve the hazards. The types 
of hazard condition that can be detected are as following:  
Type 1a. EX/MEM.RegisterRd = ID/EX.RegisterRs  
Type 1b.  EX/MEM.RegisterRd = ID/EX.RegisterRt  
Type 2a.  MEM/WB.RegisterRd = ID/EX.RegisterRs  
Type 2b.  MEM/WB.RegisterRd = ID/EX.RegisterRt  
Because  some  instructions  do  not  write  registers,  this  policy  is  inaccurate;  sometimes  it  would  forward  when  it  was 
unnecessary. 
One  solution  is  simply  to  check  to  see  if  the  RegWrite  signal  will  be  active:  examining  the  WB  control  filed  of  the 
pipeline register during the EX and MEM stages determines if RegWrite signal is asserted  [1]. Besides of this check that 
an  instruction  in  the pipeline has $r0  register as  its destination, we want  to avoid  forwarding because  register $r0  is hard 
wired and always contains the value zero. 
The  hazard  conditions  above  thus  work  properly  as  long  as  we  add  EX/MEM  RegisterRd  !=  0  to  the  first  hazard 
condition  and MEM/WB  RegisterRd  !=  0  to  the  second.  Following  are  the  conditions  for  detecting  hazards  and  control 
signals for forwarding unit to resolve them:  
(1.)  EX hazard: 
If ((EX/MEM.RegWrite = 1) and (EX/MEM.RegisterRd != 0)and (EX/MEM.RegisterRd =  ID/EX.RegisterRs)) 
ForwardA = 10 
If ((EX/MEM.RegWrite =1) and (EX/MEM.RegisterRd != 0)and (EX/MEM.RegisterRd = ID/EX.RegisterRt))  
ForwardB = 10 
This  case  forwards  the  results  from  the  previous  instruction  to  the  either  input  of  the  ALU.  The  other  case  if 
previous  instruction  is  going  to  write  to  the  register  file  and  write  register  number  matches  the  read  register 
number  of  ALU  inputs,  then  activate  the  multiplexers  to  pick  the  value  instead  from  the  pipeline  register 
EX/MEM. 

 

(2.)  MEM hazard: 
If ((MEM/WB.RegWrite = 1) and (MEM/WB.RegisterRd != 0) and (MEM/WB.RegisterRd = 
ID/EX.RegisterRs)) 
Forward A = 01 
If ((MEM/WB.RegWrite = 1) and (MEM/WB.RegisterRd!= 0) and (MEM/WB.RegisterRd = 
ID/EX.RegisterRt)) 
Forward B = 01 
Another  potential  hazard  can  occur when  there  is  a  conflict between  the  result of  the  instruction  in  the WB  stage,  in  the 
MEM  stage,  and  the  source  operand  of  the  instruction  in  the  ALU  stage.  In  this  case  the  result  is  forwarded  from  the 
MEM stage. Thus the control for the MEM hazard would be the following:  
If ((MEM/WB.RegWrite = 1) and (MEM/WB.RegisterRd != 0) and (EX/MEM.Register.Rd != 
ID/EX.RegisterRs) and (MEM/WB.RegisterRd = ID/EX.RegisterRs)) then Forward A = 01 

© 2013, IJARCSSE All Rights Reserved                                                                                                           Page | 426 

Tina et al., International Journal of Advanced Research in Computer Science and Software Engineering 3(2), 
February  - 2013, pp. 1-6 
If ((MEM/WB.RegWrite = 1) and (MEM/WB.RegisterRd != 0) and (EX/MEM.Register.Rd != 
ID/EX.RegisterRt) and (MEM/WB.RegisterRd = ID/EX.RegisterRt)) then Forward B = 01 
Hazards occur when we  read a value  that was  just written  from memory, as  that value won’t be available  for  forwarding 
unit until  the end of memory stage. In order to detect such hazards, MIPS introduces a Hazard Detection Unit during the 
instruction decode stage so that it can stall the pipeline between load instruction and the immediate instruction attempting 
to use the same register [1]. The control for the Hazard Detection Unit is this single condition:  
If ((ID/EX.MemRead = 1) and ((ID/EX.RegisterRt = IF/ID.RegisterRs)or (ID/EX.RegisterRt = 
IF/ID.RegisterRt))) then stall the pipeline 

Fig. 6 Pipeline connection for Hazards Detection Unit and Forwarding Unit 
One way  to  improve branch performance  is  to  reduce  the cost of  the  taken branch.  If  the branch execution moves earlier 
in the pipeline, then fewer instructions need to be flushed. Thus far next PC assumed for a bran ch is selected in the MEM 
stage  but  if  we  move  branch  decision  in  the  ID  stage,  then  only  one  instruction  need  to  be  flushed.  For  this 
implementation just move the branch adder from the MEM stage to the ID stage because the PC value and the immediate 
field in the IF/ID pipeline are already available before ID stage.  

 

Fig. 7 MIPSPipelined Final Data path and Control 
 
For  branch  equal,  compare  the  two  registers  read  during  the  ID  stage  to  see  if  they  are  equal.  Equality  can  be  tested  by 
first  exclusive-ORing  their  respective  bits  and  then  ANDing  all  the  results.  By  moving  the  branch  execution  to  the  ID 
stage, there is only one instruction to flush if the branch is taken, the one currently being fetched  [1]. 
To  flush  instructions  in  the  IF  stage,  add  a  control  line,  called  IF.Flush,  that  zero  the  instruction  field  of  the  IF/ID 
pipeline register. Clearing the register transforms the fetched instruction into  nop, an instruction that does no operation to 
change state.   

 

© 2013, IJARCSSE All Rights Reserved                                                                                                           Page | 427 

Tina et al., International Journal of Advanced Research in Computer Science and Software Engineering 3(2), 
February  - 2013, pp. 1-6 

 

IV.  SIMULATION RESULTS 
The  performance of  the MIPS RISC processor   [7]  has been  evaluated  in  this  research  work by using  advanced XILINX 
VIRTEX4 XC4VLX15 technology. The design meets the need of high performance logic solution  for high volume, very 
low cost, consumer – oriented applications. 
The  synchronization  of  various  operation  are  done  using  CLK  signal   [7].  The  MIPS  processor  is  designed  with  three 
control signals RD, WR, RESET. If RESET is high then the processor will not perform any operation it will stay in ideal 
state. CLK is the external clock which is always equal to one which triggers the input and gives us the desired output.  
For  simulation,  a  number  of  instructions  from  instruction  memory  fed  into  the  CPU  and  th e  outputs  of  register  bank, 
ALU  results  and  pipeline  registers  value  were  monitored.  Theinstructions  that  were  tested  included  register  based  and 
immediate, reading and writing datamemory, and a  loop that would force the CPU to jump back tothe start of instruction 
memory  and  execute  those  same  instructions  again.  The  different  adds  were  important  becauseeach  exercised  different 
parts  of  the CPU  including  the  dataforwarding  unit, multiple  registers  and  different  functionswithin  the  ALU  itself.  The 
jump  instruction  was  very  important  also  in  that  it  exercised  the  branch  detection  unit,  hazard  detection  units  as  well  as 
the ability of the instruction    fetch stage to be able to jump to an address and continue execution with only the input of a  
single stall cycle. 

Fig. 8 Data Hazards and Forwarding Simulation 
 

Fig. 9 Data Hazards and Stalling Simulation 

 

 

© 2013, IJARCSSE All Rights Reserved                                                                                                           Page | 428 

Tina et al., International Journal of Advanced Research in Computer Science and Software Engineering 3(2), 
February  - 2013, pp. 1-6 

 
 

Fig. 10 Branch Hazard Simulation 
 
V.  SUMMARY 
This section presents  the performance of  the processor  in  terms of  its Total Power and Delay  that are obtained using  the 
Virtex4  XC4VLX15  XILINX tool.  Table  1  presents  the  maximum  power  dissipation,  area  occupied  and  time  taken  by 
each module to operate the processor. 
TABLE I    DELAY, TOTAL POWER AND AREA CALCULATION 

 

Topology 

Delay(ns) 

Slices Utilized 
(Area) 

AT 

AT2 

Power 
Dissipation 
10-9W 

Total Power 
(W) 

Instruction Fetch Unit 

1.779 

Instruction Decode 
Unit 

4.072 

Control Unit 

0.905 

Execution Unit 

4.495 

Data Memory Unit 

3.097 

49 

290 

15 

60 

83 

87.171 

155.08 

0.159 

0.165 

1180.88 

4808.54 

0.159 

0.168 

13.575 

12.29 

0.159 

0.164 

269.7 

1212.30 

0.159 

0.166 

257.051 

796.09 

0.159 

0.166 

Total 

14.348 

437 

1808.377 

6984.3 

 

0.829 

 
It  is  observed  that  total  delay  of  the  32-bit  MIPS  processor  is  which  results  after  addition  of  all  modules  in  the  design 
14.348  ns.  The  maximum  area  is  occupied  by  the  Decode  unit.  The  overall  power  dissipation  of  this  processor  is 
observed  to  be  0.829 W.  The  top-level  design  should  be  partitioned  such  that  the  synchronizer  blocks  are  contained  in 
individual modules outside of any functional blocks. This will help to achieve  the ideal clock domain scenario (one clock 
for  the  entire  design)  on  a  block  by  block  basis.By  synthesizing  the  Top  Level  module  of  Pipelining  design  the  overall 
delay  results  5.584  ns  while  by  synthesizing  the  Top  Module  of  Single  Cycle  Non-Pipelined  design  the  overall  delay 
results  13.613  ns.The  following  figures  show  the  variation  of  area,  power  dissipation  and  delay  of  the  whole  RISC 
processor and for each sub modules respectively. From the  graph it is seen that the maximum power dissipation and chip 
area is exhibited by the Decode unit.  
 

 

© 2013, IJARCSSE All Rights Reserved                                                                                                           Page | 429 

Tina et al., International Journal of Advanced Research in Computer Science and Software Engineering 3(2), 
February  - 2013, pp. 1-6 

Delay (ns)

Area

5

4

3

2

1

0

300

250

200

150

100

50

0

 

Fig. 11 Delay Estimation 

 
                                                               Fig. 12 Area Estimation 
 
VI.  CONCLUSION 
A 32-bit RISC processor  [2] with 16  instruction set  has been designed. Every  instruction  is executed  in one clock  cycles 
with  5-stage  pipelining.  The  design  is  verified  through  exhaustive  simulations.  The  processor  achieves  higher 
performance,  lower  area  and  lower power dissipation. The  simulation  results  show  that maximum  frequency of pipeline 
processor  is  increased  from73.461  MHz  to  179.092MHz.This  processor  can  be  used  as  a  systolic  core  to  perform 
mathematical computations  like  solving polynomial and differential equations. Apart   from  this  it can be used  in portable 
gaming kits. 
 

REFERENCES 
[1] David  A.  Patterson,  John  L.  Hennessy,  “Computer  Organization  and  Design  -  The  Hardware/Software  Interface” 
Second Edition (1998) Morgan Kaufmann Publisher, Inc.  
[2] Xiao  Li,  LongweiJi,  Bo  Shen,  Wenhong  Li,  Qianling  Zhang,  “VLSI  implementation  of  a  High -performance  32-bit 
RISC  Microprocessor”,  Communications,  Circuits  and  Systems  and  West  Sino  Expositions,  IEEE  2002 
International Conference on ,Volume  2, 2002 ,pp.1458 – 1461. 
[3] KusumlataPisda, DeependraPandey,  “Realization &  Study  of High  Performance MIPS RISCProcessor Design Using 
VHDL”,  International  Journal of Emerging  trends  in Engineering and Development, Volume 7,  I ssue 2, November 
2012, pp. 134 – 139, ISSN: 2249 – 6149.   
[4]  Kirat  Pal  Singh,  ShivaniParmar,  “VHDL  Implementation  of  a  MIPS  –  32  bit  Pipeline  Processor”,  International 
Journal of Applied Engineering Research, Volume 7, Issue 11,  ISSN:0973 – 4562. 
[5]  SamiappaSakthikumaran,S.Salivahanan  and  V.S.Kaanchana  Bhaaskaran,“16-Bit  RISC  Processor  Design  For 
Convolution Application”,IEEE International Conference on Recent Trends In Information Technology,  June 2011, 
pp.394-397.   
[6]  RupaliS.  Balpande  and  Rashmi  S.  Keote,  “Design  of  FPGA  based  Instruction  Fetch  &  Decode  Module  of  32 -bit 
RISC  (MIPS) Processor,  International Conference on Communication Systems  and Network Technologies pp. 409 
– 413. 
[7]  R. Uma,  “Design and Performance Analysis of 8  – bit RISC Processor using Xilinx Tool”,  International  Journal of 
Engineering  Research  and  Applications,  Volume  2,  Issue  2,  March  –  April  2012,  pp.  053  –  058,  ISSN:  2248  – 
9622. 
[8]  V.N.Sireesha and D.Hari Hara Santosh,  “FPGA  Implementation of a MIPS RISC Processor”,  International  Journal 
of Computer Technology and Applications, Volume 3, Issue 3, pp. 1251 – 1253, ISSN: 2229 – 6093. 

 

 

 

© 2013, IJARCSSE All Rights Reserved                                                                                                           Page | 430 

