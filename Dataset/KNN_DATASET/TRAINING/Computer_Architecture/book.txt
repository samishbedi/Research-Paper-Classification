Computer Architecture and Organization: From Software to
Hardware

Mano j Franklin
University of Maryland, College Park

c(cid:13)Mano j Franklin 2007

Preface

Introduction

Welcome! Bienvenidoes! Bienvenue! Benvenuto! This book provides a fresh introduction
to computer architecture and organization. The sub ject of computer architecture dates back
to the early periods of computer development, although the term was coined more recently.
Over the years many introductory books have been written on this fascinating sub ject, as the
sub ject underwent many changes due to technological (hardware) and application (software)
changes. Today computer architecture is a topic of great importance to computer science,
computer engineering, and electrical engineering. It bridges the yawning gap between high-
level language programming in computer science and VLSI design in electrical engineering.
The spheres of in(cid:13)uence exercised by computer architecture has expanded signi(cid:12)cantly in
recent years. A fresh introduction of the sub ject is therefore essential for modern computer
users, programmers, and designers.

This book is for students of computer science, computer engineering, electrical engineer-
ing, and any others who are interested in learning the fundamentals of computer architecture
in a structured manner. It contains core material that is essential to students in all of these
disciplines.
It is designed for use in a computer architecture or computer organization
course typically o(cid:11)ered at the undergraduate level by computer science, computer engineer-
ing, electrical engineering, or information systems departments. On successful completion
of this book you will have a clear understanding of the foundational principles of computer
architecture. Many of you may have taken a course in high-level language programming
and in digital logic before using this book. We assume most readers will have some fa-
miliarity with computers, and perhaps have even done some programming in a high-level
language. We also assume that readers have had exposure to preliminary digital logic de-
sign. This book will extend that knowledge to the core areas of computer architecture,
namely assembly-level architecture, instruction set architecture, and microarchitecture.

The WordReference dictionary de(cid:12)nes computer architecture as \the structure and or-
ganization of a computer’s hardware or system software." Dictionary.com de(cid:12)nes it as \the
art of assembling logical elements into a computing device; the speci(cid:12)cation of the rela-
tion between parts of a computer system." Computer architecture deals with the way in
which the elements of a computer relate to each other. It is concerned with all aspects of
the design and operation of the computer system. It extends upward into software as a
system’s architecture is intimately tied to the operating system and other system software.
It is almost impossible to design a good operating system without knowing the underlying
architecture of the systems where the operating system will run. Similarly, the compiler
requires an even more intimate knowledge of the architecture.

It is important to understand the general principles behind the design of computers, and
to see how those principles are put into practice in real computers. The goal of this book is

2

to provide a complete discussion of the fundamental concepts, along with an extensive set
of examples that reinforce these concepts. A few detailed examples are also given for the
students to have a better appreciation of real-life intricacies. These examples are presented
in a manner that does not distract the student from the fundamental concepts. Clearly, we
cannot cover every single aspect of computer architecture in an introductory book. Our goal
is to cover the fundamentals and to lay a good foundation upon which motivated students
can easily build later. For each topic, we use the following test to decide if it should get
included in the text:
is the topic foundational? If the answer is positive, we include the
topic.

Almost every aspect of computer architecture is replete with trade-o(cid:11)s, involving char-
acteristics such as programmability, software compatibility, portability, speed, cost, power
consumption, die size, and reliability. For general-purpose computers, one trade-o(cid:11) drives
the most important choices the computer architect must make: speed versus cost. For
laptops and embedded systems, the important considerations are size and power consump-
tion. For space applications and other mission-critical applications, reliability and power
consumption are of primary concern. Among these considerations, we highlight programma-
bility, performance, cost, and power consumption throughout the text, as they are funda-
mental factors a(cid:11)ecting how a computer is designed. However, this coverage is somewhat
qualitative, and not intended to be quantitative in nature. Extensive coverage of quantita-
tive analysis is traded o(cid:11) in favor of qualitative explanation of issues. Students will have
plenty of opportunity to study quantitative analysis in a graduate-level computer architec-
ture course. Additional emphasis is also placed on how various parts of the system are
related to real-world demand and technology constraints.

Performance and functionality are key to the utility of a computer system. Perhaps one
of the most important reasons for studying computer architecture is to learn how to extract
the best performance from a computer. As an assembly language programmer, for instance,
you need to understand how to use the system’s functionality most e(cid:11)ectively. Speci(cid:12)cally,
you must understand its architecture so that you will be able to exploit that architecture
during programming.

Coverage of Software and Hardware

Computer architecture/organization is a discipline with many facets, ranging from trans-
lation of high-level language programs through design of instruction set architecture and
microarchitecture to the logic-level design of computers. Some of these facets have more
of a software luster whereas others have more of a hardware luster. We believe that a
good introduction to the discipline should give a broad overview of all the facets and their
interrelationships, leaving a non-specialist with a decent perspective on computer architec-
ture, and providing an undergraduate student with a solid foundation upon which related
and advanced sub jects can be built. Traditional introductory textbooks focussing only on
software topics or on hardware topics do not ful(cid:12)ll these ob jectives.

3

Our presentation is unique in that we cover both software and hardware concepts. These
include high-level language, assembly language programming, systems programming, in-
struction set architecture design, microarchitecture design, system design, and digital logic
design.

There are four legs that form the foundation of computer architecture: assembly-level
architecture, instruction set architecture, microarchitecture, and logic-level architecture.
This book is uniquely concerned about all four legs. Starting from the assembly-level
architecture, we carry out the design of the important portions of a computer system all
the way to the lower hardware levels, considering plausible alternatives at each level.

Structured Approach

In an e(cid:11)ort to systematically cover all of these fundamental topics, the material has been
organized in a structured manner, from the high-level architecture to the logic-level archi-
tecture. Our coverage begins with a high-level language programmer’s view|expressing
algorithms in an HLL such as C|and moves towards the less abstract levels. Although
there are a few textbooks that start from the digital logic level and work their way to-
wards the more abstract levels, in our view the fundamental issues of computer architec-
ture/organization are best learned starting with the software levels, with which most of the
students are already familiar. Moreover, it is easier to appreciate why a level is designed
in a particular manner if the student knows what the design is supposed to implement.
This structured approach|from abstract software levels to less abstract software levels to
abstract hardware levels to less abstract hardware levels|is faithfully followed through-
out the book. We make exceptions only in a few places where such a deviation tends to
improve clarity. For example, while discussing ISA (instruction set architecture) design
options in Chapter 5, we allude to hardware issues such as pipelining and multiple-issue,
which in(cid:13)uence ISA design.

For each architecture level we answer the following fundamental questions: What is
the nature of the machine at this level? What are the ways in which its building blocks
interact? How does the machine interact with the outside world? How is programming
done at this level? How is a higher-level program translated/interpreted for controlling the
machine at this level? We are con(cid:12)dent that after you have mastered these fundamental
concepts, building upon them will be quite straightforward.

Example Instruction Set

As an important goal of this book is to lay a good foundation for the general sub ject of com-
puter architecture, we have refrained from focusing on a single architecture in our discussion
of the fundamental concepts. Thus, when presenting concepts at each architecture level,
great care is taken to keep the discussion general, without tailoring to a speci(cid:12)c architecture.
For instance, when discussing the assembly language architecture, we discuss register-based

4

approach as well as a stack-based approach. When discussing virtual memory, we discuss a
paging-based approach as well as a segmentation-based approach. In other words, at each
stage of the design, we discuss alternative approaches, and the associated trade-o(cid:11)s. While
one alternative may seem better today, technological innovations may tip the scale towards
another in the future.

For ease of learning, the discussion of concepts is peppered with suitable examples. We
have found that students learn the di(cid:11)erent levels and their inter-relationships better when
there is a continuity among many of the examples used in di(cid:11)erent parts of the book. For
this purpose, we have used the standard MIPS assembly language [ref ] and the standard
MIPS Lite instruction set architecture [ref ], a subset of the MIPS-I ISA [ref ]. We use MIPS
because it is very simple and has had commercial success, both in general-purpose com-
puting and in embedded systems. The MIPS architecture had its beginnings in 1984, and
was (cid:12)rst implemented in 1985. By the late 1980s, the architecture had been adopted by
several workstation and server companies, including Digital Equipment Corporation and
Silicon Graphics. Now MIPS processors are widely used in Sony and Nintendo game ma-
chines, palmtops, laser printers, Cisco routers, and SGI high-performance graphics engines.
More importantly, some popular texts on Advanced Computer Architecture use the MIPS
architecture. The use of the MIPS instruction set in this introductory book will therefore
provide good continuity for those students wishing to pursue higher studies in Computer
Science or Engineering.

In rare occasions, I have changed some terminology, not to protect the innocent but
simply to make it clearer to understand.

Organization and Usage of the Book

This book is organized to meet the needs of several potential audiences. It can serve as an
undergraduate text, as well as a professional reference for engineers and members of the
technical community who (cid:12)nd themselves frequently dealing with computing. The book
uses a structured approach, and is intended to be read sequentially. Each chapter builds
upon the previous ones. Certain sections contain somewhat advanced technical material,
and can be skipped by the reader without loss in continuity. These sections are marked
with an asterisk. We recommend, however, that even those sections be skimmed, at least
to get a super(cid:12)cial idea of their contents.

Each chapter is followed by a \Concluding Remarks" section and an \Exercises" section.
The exercises are particularly important. They help master the material by integrating a
number of di(cid:11)erent concepts. The book also includes many real-world examples, both his-
torical and current, in each chapter. Instead of presenting real-world examples in isolation,
such examples are included while presenting the ma jor concepts.

This book is organized into 9 chapters, which are grouped into 3 parts. The (cid:12)rst part
provides an overview of the sub ject. The second part covers the software levels, and the
third part covers the hardware levels. The coverage of the software levels is not intended

5

to make the readers pro(cid:12)cient in programming in these levels, but rather to help them
understand what each level does, how programs at immediately higher level are converted
to this level, and how to design this level in a better way.

A layered approach is used to cover the topics. Each new layer builds upon the previous
material to add depth and understanding to the reader’s knowledge.

Chapter 1 provides an overview of .... It opens with a discussion of the expanding role
of computers, and the trends in technology and software applications. It brie(cid:13)y introduces
..... Chapter 2 ... Chapter 3 ..... Most of the material in Chapter 3 should be familiar
to readers with a background in computer programming, and they can probably browse
through this chapter. Starting with Chapter 4, the material deals with the core issues in
computer architecture. Chapter 4 ..... Chapter 5 ..... Chapter 6 .....

The book can be tailored for use in software-centric as well as hardware-centric courses.
For instance, skipping the last chapter (or the last 3 chapters) makes the book becomes suit-
able for a software-centric course, and skipping chapter 2 makes it suitable for a hardware-
centric course.
\If you are planning for a year, sow rice;
if you are planning for a decade, plant trees;
if you are planning for a lifetime, educate people."
| Chinese Proverb

\Therefore, since brevity is the soul of wit, And tediousness the limbs and outward
(cid:13)ourishes, I wil l be brief "
| Wil liam Shakespeare, Hamlet

6

Soli Deo Gloria

Contents

1 Introduction

1.1 Computing and Computers

. . . . . . . . . . . . . . . . . . . . . . . . . . .

1.1.1 The Problem-Solving Process . . . . . . . . . . . . . . . . . . . . . .

1.1.2 Automating Algorithm Execution with Computers . . . . . . . . . .

1.2 The Digital Computer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.2.1 Representing Programs in a Digital Computer: The Stored Program
Concept . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.2.2 Basic Software Organization . . . . . . . . . . . . . . . . . . . . . . .

1.2.3 Basic Hardware Organization . . . . . . . . . . . . . . . . . . . . . .

1.2.4

Software versus Hardware . . . . . . . . . . . . . . . . . . . . . . . .

1.2.5 Computer Platforms . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.3 A Modern Computer System . . . . . . . . . . . . . . . . . . . . . . . . . .

1.3.1 Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.3.2

1.3.3

Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Starting the Computer System: The Boot Process

. . . . . . . . . .

1.3.4 Computer Network . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.4 Trends in Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.4.1 Hardware Technology Trends . . . . . . . . . . . . . . . . . . . . . .

1.4.2

Software Technology Trends . . . . . . . . . . . . . . . . . . . . . . .

1.5 Software Design Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.6 Hardware Design Issues

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.6.1 Performance

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.6.2 Power Consumption . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.6.3 Price . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

1

3

3

5

9

10

12

13

15

16

17

17

19

20

21

22

22

23

25

25

25

26

27

8

CONTENTS

1.6.4

1.6.5

Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.7 Theoretical Underpinnings . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.7.1 Computability and the Turing Machine

. . . . . . . . . . . . . . . .

1.7.2 Limitations of Computers . . . . . . . . . . . . . . . . . . . . . . . .

1.8 Virtual Machines: The Abstraction Tower . . . . . . . . . . . . . . . . . . .

1.8.1 Problem De(cid:12)nition and Modeling Level Architecture . . . . . . . . .

1.8.2 Algorithm-Level Architecture . . . . . . . . . . . . . . . . . . . . . .

1.8.3 High-Level Architecture . . . . . . . . . . . . . . . . . . . . . . . . .

1.8.4 Assembly-Level Architecture

. . . . . . . . . . . . . . . . . . . . . .

1.8.5

Instruction Set Architecture (ISA) . . . . . . . . . . . . . . . . . . .

1.8.6 Microarchitecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.8.7 Logic-Level Architecture . . . . . . . . . . . . . . . . . . . . . . . . .

1.8.8 Device-Level Architecture . . . . . . . . . . . . . . . . . . . . . . . .

1.9 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.10 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

I PROGRAM DEVELOPMENT | SOFTWARE LEVELS

2 Program Development Basics

2.1 Overview of Program Development . . . . . . . . . . . . . . . . . . . . . . .

2.1.1 Programming Languages . . . . . . . . . . . . . . . . . . . . . . . . .

2.1.2 Application Programming Interface Provided by Library . . . . . . .

2.1.3 Application Programming Interface Provided by OS . . . . . . . . .

2.1.4 Compilation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.1.5 Debugging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 Programming Language Speci(cid:12)cation . . . . . . . . . . . . . . . . . . . . . .

2.2.1

2.2.2

Syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3 Data Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3.1 Constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3.2 Variables

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3.3

IO Streams and Files . . . . . . . . . . . . . . . . . . . . . . . . . . .

27

27

27

27

28

30

33

33

37

38

38

39

39

40

41

41

43

45

46

47

50

50

50

50

50

50

50

50

51

52

58

CONTENTS

2.3.4 Data Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3.5 Modeling Real-World Data . . . . . . . . . . . . . . . . . . . . . . .

2.4 Operators and Assignments . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.5 Control Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.5.1 Conditional Statements

. . . . . . . . . . . . . . . . . . . . . . . . .

2.5.2 Loops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.5.3

2.5.4

Subroutines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Subroutine Nesting and Recursion . . . . . . . . . . . . . . . . . . .

2.5.5 Re-entrant Subroutine . . . . . . . . . . . . . . . . . . . . . . . . . .

2.5.6 Program Modules

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.5.7

Software Interfaces: API and ABI

. . . . . . . . . . . . . . . . . . .

2.6 Library API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.7 Operating System API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.7.1 What Should be Done by the OS? . . . . . . . . . . . . . . . . . . .

2.7.2

Input/Output Management . . . . . . . . . . . . . . . . . . . . . . .

2.7.3 Memory Management

. . . . . . . . . . . . . . . . . . . . . . . . . .

2.7.4 Process Management . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.8 Operating System Organization . . . . . . . . . . . . . . . . . . . . . . . . .

2.8.1

System Call Interface

. . . . . . . . . . . . . . . . . . . . . . . . . .

2.8.2 File System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.8.3 Device Management: Device Drivers . . . . . . . . . . . . . . . . . .

2.8.4 Hardware Abstraction Layer (HAL)

. . . . . . . . . . . . . . . . . .

2.8.5 Process Control System . . . . . . . . . . . . . . . . . . . . . . . . .

2.9 Ma jor Issues in Program Development . . . . . . . . . . . . . . . . . . . . .

2.9.1 Portability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.9.2 Reusability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.9.3 Concurrency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.10 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.11 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Assembly-Level Architecture | User Mode

3.1 Overview of User Mode Assembly-Level Architecture . . . . . . . . . . . . .

3.1.1 Assembly Language Alphabet and Syntax . . . . . . . . . . . . . . .

9

60

60

64

65

65

66

67

68

68

69

69

69

70

72

72

73

74

74

76

76

77

78

78

80

80

80

80

80

80

81

82

83

10

CONTENTS

3.1.2 Memory Model

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.3 Register Model

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.4 Data Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.5 Assembler Directives . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.6

Instruction Types and Instruction Set . . . . . . . . . . . . . . . . .

3.1.7 Program Execution . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1.8 Challenges of Assembly Language Programming . . . . . . . . . . .

3.1.9 The Rationale for Assembly Language Programming . . . . . . . . .

3.2 Assembly-Level Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2.1 Assembly-Level Interface Provided by Library . . . . . . . . . . . . .

3.2.2 Assembly-Level Interface Provided by OS . . . . . . . . . . . . . . .

3.3 Example Assembly-Level Architecture: MIPS-I . . . . . . . . . . . . . . . .

3.3.1 Assembly Language Alphabet and Syntax . . . . . . . . . . . . . . .

3.3.2 Register Model

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3.3 Memory Model

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3.4 Assembler Directives . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3.5 Assembly-Level Instructions . . . . . . . . . . . . . . . . . . . . . . .

3.3.6 An Example MIPS-I AL Program . . . . . . . . . . . . . . . . . . .

3.3.7

SPIM: A Simulator for the MIPS-I Architecture

. . . . . . . . . . .

3.4 Translating HLL Programs to AL Programs . . . . . . . . . . . . . . . . . .

3.4.1 Translating Constant Declarations . . . . . . . . . . . . . . . . . . .

3.4.2 Translating Variable Declarations . . . . . . . . . . . . . . . . . . . .

3.4.3 Translating Variable References . . . . . . . . . . . . . . . . . . . . .

3.4.4 Translating Conditional Statements

. . . . . . . . . . . . . . . . . .

3.4.5 Translating Loops

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.4.6 Translating Subroutine Calls and Returns . . . . . . . . . . . . . . .

3.4.7 Translating System Calls

. . . . . . . . . . . . . . . . . . . . . . . .

3.4.8 Overview of a Compiler . . . . . . . . . . . . . . . . . . . . . . . . .

3.5 Memory Models: Design Choices . . . . . . . . . . . . . . . . . . . . . . . .

3.5.1 Address Space: Linear vs Segmented . . . . . . . . . . . . . . . . . .

3.5.2 Word Alignment: Aligned vs Unaligned . . . . . . . . . . . . . . . .

3.5.3 Byte Ordering: Little Endian vs Big Endian . . . . . . . . . . . . . .

3.6 Operand Locations: Design Choices

. . . . . . . . . . . . . . . . . . . . . .

83

85

87

89

90

93

94

95

96

97

97

97

97

98

101

102

103

104

107

107

108

110

118

119

122

123

127

128

129

129

130

131

131

CONTENTS

3.6.1

Instruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.6.2 Main Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.6.3 General-Purpose Registers . . . . . . . . . . . . . . . . . . . . . . . .

3.6.4 Accumulator

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.6.5 Operand Stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.7 Operand Addressing Modes: Design Choices . . . . . . . . . . . . . . . . . .

3.7.1

Instruction-Residing Operands: Immediate Operands . . . . . . . . .

3.7.2 Register Operands . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.7.3 Memory Operands . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.7.4

Stack Operands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.8 Subroutine Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.8.1 Register Saving and Restoring

. . . . . . . . . . . . . . . . . . . . .

3.8.2 Return Address Storing . . . . . . . . . . . . . . . . . . . . . . . . .

3.8.3 Parameter Passing and Return Value Passing . . . . . . . . . . . . .

3.9 De(cid:12)ning Assembly Languages for Programmability . . . . . . . . . . . . . .

3.9.1 Labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.9.2 Pseudoinstructions . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.9.3 Macros

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.10 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.11 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Assembly-Level Architecture | Kernel Mode

4.1 Overview of Kernel Mode Assembly-Level Architecture . . . . . . . . . . . .

4.1.1 Privileged Registers

. . . . . . . . . . . . . . . . . . . . . . . . . . .

4.1.2 Privileged Memory Address Space . . . . . . . . . . . . . . . . . . .

4.1.3

IO Addresses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.1.4 Privileged Instructions . . . . . . . . . . . . . . . . . . . . . . . . . .

4.2 Switching from User Mode to Kernel Mode . . . . . . . . . . . . . . . . . .

4.2.1

Syscall Instructions: Switching Initiated by User Programs

. . . . .

4.2.2 Device Interrupts: Switching Initiated by IO Interfaces . . . . . . . .

4.2.3 Exceptions: Switching Initiated by Rare Events . . . . . . . . . . . .

4.3

IO Registers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.3.1 Memory Mapped IO Address Space

. . . . . . . . . . . . . . . . . .

11

131

131

132

132

133

136

137

137

138

140

141

142

143

145

146

146

146

146

147

147

149

150

151

152

152

152

153

154

156

157

158

158

12

CONTENTS

4.3.2

Independent IO Address Space . . . . . . . . . . . . . . . . . . . . .

4.3.3 Operating System’s Use of IO Addresses . . . . . . . . . . . . . . . .

4.4 Operating System Organization . . . . . . . . . . . . . . . . . . . . . . . . .

4.4.1

System Call Layer

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.4.2 File System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.4.3 Device Management: Device Drivers . . . . . . . . . . . . . . . . . .

4.4.4 Process Control System . . . . . . . . . . . . . . . . . . . . . . . . .

4.5 System Call Layer for a MIPS-I OS . . . . . . . . . . . . . . . . . . . . . . .

4.5.1 MIPS-I Machine Speci(cid:12)cations for Exceptions . . . . . . . . . . . . .

4.5.2 OS Usage of MIPS-I Architecture Speci(cid:12)cations . . . . . . . . . . . .

4.6

IO Schemes Employed by Device Management System . . . . . . . . . . . .

4.6.1

Sampling-Based IO . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.6.2 Program-Controlled IO . . . . . . . . . . . . . . . . . . . . . . . . .

4.6.3

Interrupt-Driven IO . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.6.4 Direct Memory Access (DMA)

. . . . . . . . . . . . . . . . . . . . .

4.6.5

IO Co-processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.6.6 Wrap Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.7 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.8 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Instruction Set Architecture (ISA)

5.1 Overview of Instruction Set Architecture . . . . . . . . . . . . . . . . . . . .

5.1.1 Machine Language . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.1.2 Register, Memory, and IO Models

. . . . . . . . . . . . . . . . . . .

5.1.3 Data Types and Formats

. . . . . . . . . . . . . . . . . . . . . . . .

5.1.4

Instruction Types and Formats . . . . . . . . . . . . . . . . . . . . .

5.2 Example Instruction Set Architecture: MIPS-I

. . . . . . . . . . . . . . . .

5.2.1 Register, Memory, and IO Models

. . . . . . . . . . . . . . . . . . .

5.2.2 Data Types and Formats

. . . . . . . . . . . . . . . . . . . . . . . .

5.2.3

Instruction Types and Formats . . . . . . . . . . . . . . . . . . . . .

5.2.4 An Example MIPS-I ML Program . . . . . . . . . . . . . . . . . . .

5.3 Translating Assembly Language Programs to Machine Language Programs

5.3.1 MIPS-I Assembler Conventions . . . . . . . . . . . . . . . . . . . . .

158

160

162

164

164

165

167

168

168

170

173

173

174

177

181

182

183

183

183

185

186

186

189

189

189

190

190

190

190

191

191

192

CONTENTS

5.3.2 Translating Decimal Numbers . . . . . . . . . . . . . . . . . . . . . .

5.3.3 Translating AL-speci(cid:12)c Instructions and Macros

. . . . . . . . . . .

5.3.4 Translating Labels . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.3.5 Code Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.3.6 Overview of an Assembler . . . . . . . . . . . . . . . . . . . . . . . .

5.3.7 Cross Assemblers . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.4 Linking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.4.1 Resolving External References

. . . . . . . . . . . . . . . . . . . . .

5.4.2 Relocating the Memory Addresses

. . . . . . . . . . . . . . . . . . .

5.4.3 Program Start-Up Routine

. . . . . . . . . . . . . . . . . . . . . . .

5.5

Instruction Formats: Design Choices . . . . . . . . . . . . . . . . . . . . . .

5.5.1 Fixed Length Instruction Encoding . . . . . . . . . . . . . . . . . . .

5.5.2 Variable Length Instruction Encoding . . . . . . . . . . . . . . . . .

5.6 Data Formats: Design Choices and Standards . . . . . . . . . . . . . . . . .

5.6.1 Unsigned Integers: Binary Number System . . . . . . . . . . . . . .

5.6.2

Signed Integers: 2’s Complement Number System . . . . . . . . . . .

5.6.3 Floating Point Numbers: ANSI/IEEE Floating Point Standard . . .

5.6.4 Characters: ASCII and Unicode

. . . . . . . . . . . . . . . . . . . .

5.7 Designing ISAs for Better Performance . . . . . . . . . . . . . . . . . . . . .

5.7.1 Technological Improvements and Their E(cid:11)ects

. . . . . . . . . . . .

5.7.2 CISC Design Philosophy . . . . . . . . . . . . . . . . . . . . . . . . .

5.7.3 RISC Design Philosophy . . . . . . . . . . . . . . . . . . . . . . . . .

5.7.4 Recent Trends

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.8 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.9 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

II PROGRAM EXECUTION | HARDWARE LEVELS

6 Program Execution Basics

6.1 Overview of Program Execution . . . . . . . . . . . . . . . . . . . . . . . . .

6.2 Selecting the Program: User Interface

. . . . . . . . . . . . . . . . . . . . .

6.2.1 CLI Shells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.2.2 GUI Shells

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

13

192

192

193

195

196

196

197

198

198

198

199

201

203

203

204

205

206

212

213

214

215

215

217

217

218

219

221

221

222

223

224

14

CONTENTS

6.2.3 VUI Shells

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.3 Creating the Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.4 Loading the Program . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.4.1 Dynamic Linking of Libraries . . . . . . . . . . . . . . . . . . . . . .

6.5 Executing the Program . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.6 Halting the Program . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.7

Instruction Set Simulator

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.7.1

6.7.2

Implementing the Register Space . . . . . . . . . . . . . . . . . . . .

Implementing the Memory Address Space . . . . . . . . . . . . . . .

6.7.3 Program Loading . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.7.4

Instruction Fetch Phase . . . . . . . . . . . . . . . . . . . . . . . . .

6.7.5 Executing the ML Instructions . . . . . . . . . . . . . . . . . . . . .

6.7.6 Executing the Syscall Instruction . . . . . . . . . . . . . . . . . . . .

6.7.7 Comparison with Hardware Microarchitecture . . . . . . . . . . . . .

6.8 Hardware Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.8.1 Clock . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.8.2 Hardware Description Language (HDL)

. . . . . . . . . . . . . . . .

6.8.3 Design Speci(cid:12)cation in HDL . . . . . . . . . . . . . . . . . . . . . . .

6.8.4 Design Veri(cid:12)cation using Simulation . . . . . . . . . . . . . . . . . .

6.8.5 Hardware Design Metrics

. . . . . . . . . . . . . . . . . . . . . . . .

6.9 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.10 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7 Microarchitecture | User Mode

7.1 Overview of User Mode Microarchitecture . . . . . . . . . . . . . . . . . . .

7.1.1 Dichotomy: Data Path and Control Unit

. . . . . . . . . . . . . . .

7.1.2 Register File and Individual Registers . . . . . . . . . . . . . . . . .

7.1.3 Memory Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.1.4 ALUs and Other Functional Units

. . . . . . . . . . . . . . . . . . .

7.1.5

Interconnects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.1.6 Processor and Memory Subsystems . . . . . . . . . . . . . . . . . . .

7.1.7 Micro-Assembly Language (MAL)

. . . . . . . . . . . . . . . . . . .

7.2 Example Microarchitecture for Executing MIPS-0 Programs . . . . . . . . .

225

227

227

228

230

231

231

233

234

235

235

235

236

237

237

237

237

238

238

238

239

239

241

243

243

244

245

246

247

249

249

251

CONTENTS

7.2.1 MAL Commands . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.2.2 MAL Operation Set

. . . . . . . . . . . . . . . . . . . . . . . . . . .

7.2.3 An Example MAL Routine . . . . . . . . . . . . . . . . . . . . . . .

7.3

Interpreting ML Programs by MAL Routines . . . . . . . . . . . . . . . . .

7.3.1

7.3.2

7.3.3

7.3.4

7.3.5

Interpreting an Instruction | the Fetch Phase . . . . . . . . . . . .

Interpreting Arithmetic/Logical Instructions

. . . . . . . . . . . . .

Interpreting Memory-Referencing Instructions . . . . . . . . . . . . .

Interpreting Control-Changing Instructions . . . . . . . . . . . . . .

Interpreting Trap Instructions . . . . . . . . . . . . . . . . . . . . . .

7.4 Memory System Organization . . . . . . . . . . . . . . . . . . . . . . . . . .

7.4.1 Memory Hierarchy: Achieving Low Latency and Cost

. . . . . . . .

7.4.2 Cache Memory: Basic Organization . . . . . . . . . . . . . . . . . .

7.4.3 MIPS-0 Data Path with Cache Memories

. . . . . . . . . . . . . . .

7.4.4 Cache Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.4.5 Address Mapping Functions . . . . . . . . . . . . . . . . . . . . . . .

7.4.6 Finding a Word in the Cache . . . . . . . . . . . . . . . . . . . . . .

7.4.7 Block Replacement Policy . . . . . . . . . . . . . . . . . . . . . . . .

7.4.8 Multi-Level Cache Memories

. . . . . . . . . . . . . . . . . . . . . .

7.5 Processor-Memory Bus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.5.1 Bus Width . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.5.2 Bus Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.6 Processor Data Path Interconnects: Design Choices . . . . . . . . . . . . . .

7.6.1 Multiple-Bus based Data Paths . . . . . . . . . . . . . . . . . . . . .

7.6.2 Direct Path-based Data Path . . . . . . . . . . . . . . . . . . . . . .

7.7 Pipelined Data Path: Overlapping the Execution of Multiple Instructions

.

7.7.1 De(cid:12)ning a Pipelined Data Path . . . . . . . . . . . . . . . . . . . . .

7.7.2

Interpreting ML Instructions in a Pipelined Data Path . . . . . . . .

7.7.3 Control Unit for a Pipelined Data Path . . . . . . . . . . . . . . . .

7.7.4 Dealing with Control Flow . . . . . . . . . . . . . . . . . . . . . . .

7.7.5 Dealing with Data Flow . . . . . . . . . . . . . . . . . . . . . . . . .

7.7.6 Pipelines in Commercial Processors

. . . . . . . . . . . . . . . . . .

7.8 Wide Data Paths: Superscalar and VLIW Processing . . . . . . . . . . . . .

7.9 Co-Processors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

15

253

253

253

254

256

257

258

259

260

260

260

263

264

264

264

267

268

269

269

270

270

272

272

273

274

275

279

279

280

284

286

287

288

16

CONTENTS

7.10 Processor Data Paths for Low Power . . . . . . . . . . . . . . . . . . . . . .

7.11 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.12 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8 Microarchitecture | Kernel Mode

8.1 Processor Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.1.1

Interpreting a System Call Instruction . . . . . . . . . . . . . . . . .

8.1.2 Recognizing Exceptions and Hardware Interrupts . . . . . . . . . . .

8.1.3

Interpreting an RFE Instruction . . . . . . . . . . . . . . . . . . . .

8.2 Memory Management: Implementing Virtual Memory . . . . . . . . . . . .

8.2.1 Virtual Memory: Implementing a Large Address Space . . . . . . . .

8.2.2 Paging and Address Translation . . . . . . . . . . . . . . . . . . . .

8.2.3 Page Table Organization . . . . . . . . . . . . . . . . . . . . . . . . .

8.2.4 Translation Lookaside Bu(cid:11)er (TLB)

. . . . . . . . . . . . . . . . . .

8.2.5

Software-Managed TLB and the Role of the Operating System in
Virtual Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.2.6

Sharing in a Paging System . . . . . . . . . . . . . . . . . . . . . . .

8.2.7 A Real-Life Example: a MIPS-I Virtual Memory System . . . . . . .

8.2.8

Interpreting a MIPS-I Memory-Referencing Instruction . . . . . . .

8.2.9 Combining Cache Memory and Virtual Memory . . . . . . . . . . .

8.3

IO System Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.3.1

8.3.2

Implementing the IO Address Space: IO Data Path . . . . . . . . .

Implementing the IO Interface Protocols: IO Controllers . . . . . . .

8.3.3 Example IO Controllers . . . . . . . . . . . . . . . . . . . . . . . . .

8.3.4 Frame Bu(cid:11)er: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.3.5

IO Con(cid:12)guration: Assigning IO Addresses to IO Controllers . . . . .

8.4 System Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.4.1

Single System Bus . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.4.2 Hierarchical Bus Systems

. . . . . . . . . . . . . . . . . . . . . . . .

8.4.3

Standard Buses and Interconnects

. . . . . . . . . . . . . . . . . . .

8.4.4 Expansion Bus and Expansion Slots . . . . . . . . . . . . . . . . . .

8.4.5

IO System in Modern Desktops . . . . . . . . . . . . . . . . . . . . .

8.4.6 Circa 2006

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.4.7 RAID . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

288

290

291

293

293

294

295

297

297

297

301

304

306

309

312

312

319

320

321

322

323

324

325

329

332

332

333

341

352

354

355

356

CONTENTS

8.5 Network Architecture

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.5.1 Network Interface Card (NIC)

. . . . . . . . . . . . . . . . . . . . .

8.5.2 Protocol Stacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.6

Interpreting an IO Instruction . . . . . . . . . . . . . . . . . . . . . . . . . .

8.7 System-Level Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.8 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.9 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9 Register Tranfer Level Architecture

9.1 Overview of RTL Architecture

. . . . . . . . . . . . . . . . . . . . . . . . .

9.1.1 Register File and Individual Registers . . . . . . . . . . . . . . . . .

9.1.2 ALUs and Other Functional Units

. . . . . . . . . . . . . . . . . . .

9.1.3 Register Transfer Language . . . . . . . . . . . . . . . . . . . . . . .

9.2 Example RTL Data Path for Executing MIPS-0 ML Programs

. . . . . . .

9.2.1 RTL Instruction Set . . . . . . . . . . . . . . . . . . . . . . . . . . .

9.2.2 RTL Operation Types . . . . . . . . . . . . . . . . . . . . . . . . . .

9.2.3 An Example RTL Routine . . . . . . . . . . . . . . . . . . . . . . . .

9.3

Interpreting ML Programs by RTL Routines

. . . . . . . . . . . . . . . . .

17

357

358

359

359

359

360

360

361

362

362

363

363

364

366

367

368

369

9.3.1

9.3.2

9.3.3

9.3.4

9.3.5

Interpreting the Fetch and PC Update Commands for Each Instruction 369

Interpreting Arithmetic/Logical Instructions

. . . . . . . . . . . . .

Interpreting Memory-Referencing Instructions . . . . . . . . . . . . .

Interpreting Control-Changing Instructions . . . . . . . . . . . . . .

Interpreting Trap Instructions . . . . . . . . . . . . . . . . . . . . . .

9.4 RTL Control Unit: An Interpreter for ML Programs . . . . . . . . . . . . .

9.4.1 Developing an Algorithm for RTL Instruction Generation . . . . . .

9.4.2 Designing the Control Unit as a Finite State Machine . . . . . . . .

9.4.3

9.4.4

Incorporating Sequencing Information in the Microinstruction . . . .

State Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9.5 Memory System Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9.5.1 A Simple Memory Data Path . . . . . . . . . . . . . . . . . . . . . .

9.5.2 Memory Interface Unit . . . . . . . . . . . . . . . . . . . . . . . . . .

9.5.3 Memory Controller . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9.5.4 DRAM Controller

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

371

372

373

374

375

375

377

380

381

382

383

383

383

383

18

CONTENTS

9.5.5 Cache Memory Design . . . . . . . . . . . . . . . . . . . . . . . . . .

9.5.6 Cache Controller: Interpreting a Read/Write Command . . . . . . .

9.6 Processor Data Path Interconnects: Design Choices . . . . . . . . . . . . . .

9.6.1 Multiple-Bus based Data Paths . . . . . . . . . . . . . . . . . . . . .

9.6.2 Direct Path-based Data Path . . . . . . . . . . . . . . . . . . . . . .

9.7 Pipelined Data Path: Overlapping the Execution of Multiple Instructions

.

9.7.1 De(cid:12)ning a Pipelined Data Path . . . . . . . . . . . . . . . . . . . . .

9.7.2

Interpreting ML Instructions in a Pipelined Data Path . . . . . . . .

9.7.3 Control Unit for a Pipelined Data Path . . . . . . . . . . . . . . . .

9.8 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9.9 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10 Logic-Level Architecture

10.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.1.1 Multiplexers

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.1.2 Decoders

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.1.3 Flip-Flops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.1.4 Static RAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.1.5 Dynamic RAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.1.6 Tri-State Bu(cid:11)ers . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.2 Implementing ALU and Functional Units of Data Path . . . . . . . . . . . .

10.2.1 Implementing an Integer Adder . . . . . . . . . . . . . . . . . . . . .

10.2.2 Implementing an Integer Subtractor

. . . . . . . . . . . . . . . . . .

10.2.3 Implementing an Arithmetic Over(cid:13)ow Detector . . . . . . . . . . . .

10.2.4 Implementing Logical Operations . . . . . . . . . . . . . . . . . . . .

10.2.5 Implementing a Shifter . . . . . . . . . . . . . . . . . . . . . . . . . .

10.2.6 Putting It All Together: ALU . . . . . . . . . . . . . . . . . . . . . .

10.2.7 Implementing an Integer Multiplier . . . . . . . . . . . . . . . . . . .

10.2.8 Implementing a Floating-Point Adder

. . . . . . . . . . . . . . . . .

10.2.9 Implementing a Floating-Point Multiplier . . . . . . . . . . . . . . .

10.3 Implementing a Register File . . . . . . . . . . . . . . . . . . . . . . . . . .

10.3.1 Logic-level Design . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.3.2 Transistor-level Design . . . . . . . . . . . . . . . . . . . . . . . . . .

383

384

384

385

387

390

390

393

393

394

395

397

397

399

400

402

403

404

405

405

406

415

416

419

419

419

420

425

425

425

426

429

CONTENTS

10.4 Implementing a Memory System using RAM Cells

. . . . . . . . . . . . . .

10.4.1 Implementing a Memory Chip using RAM Cells

. . . . . . . . . . .

10.4.2 Implementing a Memory System using RAM Chips . . . . . . . . . .

10.4.3 Commercial Memory Modules . . . . . . . . . . . . . . . . . . . . . .

10.5 Implementing a Bus

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.5.1 Bus Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.5.2 Bus Arbitration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.5.3 Bus Protocol: Synchronous versus Asynchronous . . . . . . . . . . .

10.6 Interpreting Microinstructions using Control Signals

. . . . . . . . . . . . .

10.6.1 Control Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.6.2 Control Signal Timing . . . . . . . . . . . . . . . . . . . . . . . . . .

10.6.3 Asserting Control Signals in a Timely Fashion . . . . . . . . . . . .

10.7 Implementing the Control Unit . . . . . . . . . . . . . . . . . . . . . . . . .

10.7.1 Programmed Control Unit: A Regular Control Structure . . . . . . .

10.7.2 Hardwired Control Signal Generator: A Fast Control Mechanism . .

10.7.3 Hardwired versus Programmed Control Units . . . . . . . . . . . . .

10.8 Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10.9 Exercises

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A MIPS Instruction Set

B Peripheral Devices

B.1 Types and Characteristics of IO Devices . . . . . . . . . . . . . . . . . . . .

B.2 Video Terminal

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.2.1 Keyboard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.2.2 Mouse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.2.3 Video Display . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.3 Printer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.4 Magnetic Disk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.5 Modem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

431

431

431

432

433

434

435

435

439

439

442

442

443

443

446

449

449

450

451

453

453

454

455

456

457

458

459

460

20

CONTENTS

Chapter 1

Introduction

Let the wise listen and add to their learning, and let the discerning get guidance

Proverbs 1: 5

We begin this book with a broad overview of digital computers This chapter serves
as a context for the remainder of this book.
It begins by examining the nature of the
computing process.
It then discusses the fundamental aspects of digital computers, and
moves on to recent trends in desktop computer systems. Finally, it introduces the concept
of the computer as a hierarchical system. The ma jor levels of this hierarchical view are
introduced. The remainder of the book is organized in terms of these levels.

\The computer is by al l odds the most extraordinary of the technological clothing
ever devised by man, since it is an extension of our central nervous system. Beside
it the wheel is a mere hula hoop...."
| Marshal l McLuhan. War and Peace in the Global Vil lage

Born a few years back, digital computer technology, in cohort with telecommunication
technology, has ushered us into the information age and is exerting a profound in(cid:13)uence
on almost every facet of our daily lives1 . Most of us spend a substantial time every day in
front of a computer (most of it on the internet or on some games!). Rest of the time, we are
on the cell phone or some other electronic device with one or more computers embedded
within. On a more serious note, we are well aware of the critical role played by computers
in (cid:13)ying modern aircraft and spacecraft; in keeping track of large databases such as airline
reservations and bank accounts; in telecommunications applications such as routing and
controlling millions of telephone calls over the entire world; and in controlling power stations
and hazardous chemical plants. Companies and governmental agencies are virtually crippled

1This too shall pass .....

1

2

Chapter 1.

Introduction

when their computer systems go down, and a growing number of sophisticated medical
procedures are completely dependent on computers. Biologists are using computers for
performing extremely complex computations and simulations. Computer designers are using
them extensively for developing tomorrow’s faster and denser computer chips. Publishers
use them for typesetting, graphical picture processing, and desktop publishing. The writing
of this book itself has bene(cid:12)tted substantially from desktop publishing software, especially
Latex. Thus, computers have taken away many of our boring chores, and have replaced
them with addictions such as chatting, browsing, and computerized music.

What exactly is a computer? A computer science de(cid:12)nition would be as follows: a com-
puter is a programmable symbol-processing machine that accepts input symbols, processes
it according to a sequence of instructions called a computer program, and produces the
resulting output symbols. The input symbols as well as the output symbols can represent
numbers, characters, pictures, sound, or other kinds of data such as chess pieces. The most
striking property of the computer is that it is programmable, making it a truly general-
purpose machine. The user can change the program or the input data according to speci(cid:12)c
requirements. Depending on the software run, the end user \sees" a di(cid:11)erent machine; the
computer user’s view thus depends on the program being run on the computer at any given
instant. Suppose a computer is executing a chess program. As far as the computer user
is concerned, at that instant the computer is a chess player because it behaves exactly as
if it were an electronic chess player2 . Because of the ability to execute di(cid:11)erent programs,
a computer is a truly general-purpose machine. The same computer can thus perform a
variety of information-processing tasks that range over a wide spectrum of applications|
for example, as a word processor, a calculator, or a video game|by executing di(cid:11)erent
programs on it; a multitasking computer can even simultaneously perform di(cid:11)erent tasks.
The computer’s ability to perform a wide variety of tasks at very high speeds and with high
degrees of accuracy is what makes it so ubiquitous.

\The computer is only a fast idiot, it has no imagination; it cannot originate action.
It is, and wil l remain, only a tool to man."
| American Library Association’s reaction to the UNIVAC computer exhibit at the
1964 New York World’s Fair

2 In the late 1990s, a computer made by IBM called Deep Thought even defeated the previous World Chess
Champion Gary Kasparov. It is interesting to note, however, that if the rules of chess are changed even
slightly (for example, by allowing the king to move two steps at a time), then current computers will have
a di(cid:14)cult time, unless they are reprogrammed or reconstructed by humans. In contrast, even an amateur
human player will be able to comprehend the new rules in a short time and play a reasonably good game
under the new rules!

1.1. Computing and Computers

3

1.1 Computing and Computers

The notion of computing (or problem solving) is much more fundamental than the notion
of a computer, and predates the invention of computers by thousands of years.
In fact,
computing has been an integral aspect of human life and civilization throughout history.
Over the centuries, mathematicians developed algorithms for solving a wide variety of math-
ematical problems. Scientists and engineers used these algorithms to obtain solutions for
speci(cid:12)c problems, both practical and recreational. And, we have been computing ever since
we entered kindergarten, using (cid:12)ngers, followed later by paper and pencil. We have been
adding, subtracting, multiplying, dividing, computing lengths, areas, volumes and many
many other things. In all these computations, we follow some de(cid:12)nite, unambiguous set of
rules that have been established. For instance, once the rules for calculating the area of a
complex shape have been established|divide it into non-overlapping basic shapes and add
up the areas of the shapes|we can calcuate the area of any complex shape.

A typical modern-day computing problem is much more complex, but works on the same
fundamental principles. Consider a metropolitan tra(cid:14)c control center where tra(cid:14)c video
images from multiple cameras are being fed, and a human operator looks at the images
and takes various tra(cid:14)c control decisions. Imagine automating this process, and letting a
computer do the merging of the images and taking various decisions! How should we go
about designing such a computer system?

1.1.1 The Problem-Solving Process

Finding a solution to a problem, irrespective of whether or not we use a computer, involves
two important phases, as illustrated in Figure 1.1:

(cid:15) Algorithm development

(cid:15) Algorithm execution

We shall take a detailed look at these two phases.

1.1.1.1 Algorithm Development

The (cid:12)rst phase of computing involves the development of a solution algorithm or a step-
by-step procedure that describes how to solve the problem. When we explicitly write down
the rules (or instructions) for solving a given computation problem, we call it an algorithm.
An example algorithm is the procedure for (cid:12)nding the solution of a quadratic equation.
Informally speaking, many of the recipes, procedures, and methods in everyday life are
algorithms.

What should be the granularity of the steps in an algorithm? This depends on the
sophistication of the person or machine who will execute it, and can vary signi(cid:12)cantly from

4

Chapter 1.

Introduction

Problem

Algorithm Development

Input Data

Algorithm

Algorithm Execution

Output Data (Results)

Figure 1.1: The Problem Solving Process

one algorithm to another; a step can be as complex as (cid:12)nding the solution of a sub-problem,
or it can be as simple as an addition/subtraction operation. Interestingly, an addition step
itself can be viewed as a problem to be solved, for which a solution algorithm can be
developed in terms of 1-bit addition with carry-ins and carry-outs. It should also be noted
that one may occasionally tailor an algorithm to a speci(cid:12)c set of input data, in which case
it is not very general.

Algorithm development has always been done with human brain power, and in all likeli-
hood will continue like that for years to come! Algorithm development has been recorded as
early as 1800 B.C., when Babylonian mathematicians at the time of Hammurabi developed
rules for solving many types of equations [4]. The word \algorithm" itself was derived from
the last name of al-Khw^arizm^i, a 9th century Persian mathematician whose textbook on
arithmetic had a signi(cid:12)cant in(cid:13)uence for more than 500 years.

1.1.1.2 Algorithm Execution

Algorithm execution|the second phase of the problem-solving process|means applying
a solution algorithm on a particular set of input values, so as to obtain the solution of
the problem for that set of input values. Algorithm development and execution phases
are generally done one after the other; once an algorithm has been developed, it may be
executed any number of times with di(cid:11)erent sets of data without further modi(cid:12)cations.
However, it is possible to do both these phases concurrently, in a lock-step manner! This
typically happens when the same person performs both phases, and is attempting to solve
a problem for the (cid:12)rst time.

The actions involved in algorithm execution can be broken down into two parts, as
illustrated in Figure 1.2.

(cid:15) Sequencing through the algorithm steps: This part involves selecting from the algo-
rithm the next step to be executed.

1.1. Computing and Computers

5

(cid:15) Executing the next step of the algorithm, as determined by the sequencing part.

Algorithm

Input Data

Determine Next Step

Step

Data

Execute the Step

Output Data (Results)

Figure 1.2: The Algorithm Execution Process

For hundreds of years, people relied mainly on human brain power for performing both of
these parts. As centuries went by (and the gene pool deteriorated), a variety of computing
aids were invented to aid human brains in executing the individual steps of solution algo-
rithms. The Chinese abacus and the Japanese soroban were two of the earliest documented
aids used for doing the arithmetic calculations speci(cid:12)ed in algorithm steps. The slide rule
was a more sophisticated computing aid invented in the early 1600s by William Oughtred,
an English clergyman; it helped to perform a variety of computation operations including
multiplication and division. Later examples of computing aids included Pascaline, the
mechanical adder built in 1642 by the French mathematician Blaise Pascal (to assist his
father in adding long columns of numbers in the tax accounting o(cid:14)ce) and the stepped-
wheel machine of Gottfried Wilhelm Leibniz in 1672 (which could perform multiplication
and division in addition to addition and subtraction).

As problems increased in complexity, the number of steps required to solve them also
increased accordingly. Several mechanical and electrical calculators were commercially pro-
duced in the 19th century to speed up speci(cid:12)c computation steps. The time taken by a
calculator to perform a computation step was in the order of a few milliseconds, in contrast
to the several seconds or minutes taken by a person to perform the same step. It is impor-
tant to note that even after the introduction of calculators, the sequencing part of algorithm
execution was still done by people, who punched in the numbers and the operations. It is
also important to note that the granularity of the steps in an algorithm is related to the
capabilities and sophistication of the calculating aids used. Thus, a typical calculator lets
us specify algorithm steps such as multiplication and square root, for instance, whereas an
abacus can perform only more primitive computation steps.

1.1.2 Automating Algorithm Execution with Computers

We saw that calculators and other computing aids allowed an algorithm’s computation
steps to be executed much faster than what was possible without any computing aides.
However, the algorithm execution phase still consumed a signi(cid:12)cant amount of time for

6

Chapter 1.

Introduction

the following reasons: (i) the sequencing process was still manual, and (ii) the execution of
each computation step involved manual inputting of data into the calculating aid. Both of
these limitations can be overcome if the sequencing process is automated by means of an
appropriate machine, and the data to be processed is stored in the machine itself. This is
the basic idea behind computers.

\Stripped of its interfaces, a bare computer boils down to little more than a pocket
calculator that can push its own buttons and remember what it has done."
{ Arnold Penzias.
Ideas and Information.

One of the earliest attempts to automate algorithm execution was that of Charles Bab-
bage, a 19th century mathematics professor. He developed a mechanical computing ma-
chine called Difference Engine. This computer was designed to execute only a single
algorithm|the method of ((cid:12)nite) di(cid:11)erences using polynomials. Although this algorithm
used only addition and subtraction operations, it permitted many complex and useful func-
tions to be calculated.
(Chapter 1 of [2] provides a good description of the use of this
algorithm in calculating di(cid:11)erent functions.) The Difference Engine performed the se-
quencing process automatically, in addition to performing the operation speci(cid:12)ed in each
computation step. This is a ma jor advantage because it allows the algorithm execution
phase to be performed at machine speeds, rather than at the speed with which it can be
done manually. One limitation of executing a single algorithm, however, is that only a few
problems can be solved by a single algorithm; such a computing machine is therefore not
useful for general-purpose computing.

After a few years, Babbage envisioned the Analytical Engine, another massive brass,
steam-powered, mechanical (digital) computing machine. The radical shift that it intro-
duced was to have the machine accept an arbitrary solution algorithm (in punched card
format), and execute the algorithm by itself. This approach allows arbitrary algorithms to
be executed at the speed of the machine, making the machine a general-purpose computer.
The radical idea embodied in the Analytical Engine was the recognition that a machine
could be \programmed" to perform a long sequence of arithmetic and decision operations
without human intervention.

\What if a calculating engine could not only foresee but could act on that foresight?"
{ Charles Babbage. November 1834.

The Analytical Engine served as a blueprint for the (cid:12)rst real programmable computer,
which came into existence a century later3 . The basic organization proposed by Babbage
is given in Figure 1.3. The main parts are the mill, the store, the printer and card punch,
the operation cards, and the variable cards. The instructions were given to the machine on
punch cards, and the input data was supplied through the variable cards. Punched cards had

3Primitive forms of \programmable" machines had existed centuries ago, dating back to Al-Jazari’s
musical automata in the 13th century and even to Heron’s mobile automaton in the 1st century.

1.1. Computing and Computers

7

been recently invented by Jacquard for controlling weaving looms. Augusta Ada, Countess
of Lovelace as well as a mathematician, was one of the few people who fully understood
Babbage’s vision. She helped Babbage in designing the Analytical Engine’s instruction set,
and in describing, analyzing, and publicizing his ideas.
In an article published in 1843,
she predicted that such a machine might be used to compose complex music, to produce
graphics, and would be used for both practical and scienti(cid:12)c use. She also created a plan
for how the engine might calculate a mathematical sequence known as Bernoulli numbers.
This plan is now regarded as the (cid:12)rst \computer program," and Ada is credited as the (cid:12)rst
computer programmer.

\The Analytical Engine has no pretensions whatever to originate anything. It can
do whatever we know how to order it to perform."

\Supposing, for instance, that the fundamental relations of pitched sounds in the
science of harmony and of musical composition were susceptible of such expression
and adaptations, the engine might compose elaborate and scienti(cid:12)c pieces of music
of any degree of complexity or extent."
| Countess Ada Lovelace

Mill

(Arithmetic/
Logic Unit)

Data

Store

(Main
Memory)

Printer and
Card Punch

(Output Unit)

Instructions

Operation
Cards

Variable
Cards

Program

Figure 1.3: Basic Organization of Babbage’s Analytical Engine

Automated algorithm execution has two side-e(cid:11)ects that we need to keep in mind. First,
it forces the algorithm development and algorithm execution phases to happen one after
the other. It also implies that the algorithm must allow for the occurrence of all possible
inputs. Hence computer algorithms are seldom developed to solve just a single instance
of a problem; rather they are developed to handle di(cid:11)erent sets of input values. Thus, in
moving from the manual approach to the automated approach, we are forced to sacri(cid:12)ce
the versatility inherent in the concurrent development and execution of an algorithm. The
big gain, however, is in the speed and storage capabilities o(cid:11)ered by the computer machine.

Another side-e(cid:11)ect of automated algorithm execution is that for a machine to follow
an algorithm, the algorithm must be represented in a formal and detailed manner: the less

8

Chapter 1.

Introduction

sophisticated the follower, the more detailed the algorithm needs to be! Detailed algorithms
written for computers are called computer programs. By de(cid:12)nition, a computer program
is an expression of an algorithm in a computer programming language, which is a precise
language that can be made understandable to a computer. Because of the extensive e(cid:11)orts
involved in developing a computer program to make it suitable for execution in a computer,
the program itself is often developed with a view to solve a range of related problems rather
than a single problem. For instance, it may not be pro(cid:12)table to develop a computer program
to process a single type of bank transaction; instead, it is pro(cid:12)table to develop the program
with the ability to process di(cid:11)erent types of transactions.

In spite of these minor side-e(cid:11)ects, the idea of using computers to perform automated
algorithm execution has been found to have tremendous potential. First of all, once an
algorithm has been manually developed to solve a particular problem, computers can be used
to execute the algorithm at very high speeds. This makes it possible to execute long-running
algorithms that require billions of operations, which previously could never be executed in
a reasonable period of time4 . In fact, a lion’s share of computer development took place in
the 1930s and 1940s, mostly in response to computation problems that arose in the WW
II e(cid:11)ort, such as ballistic computations and code-breaking. The ability to execute complex
algorithms in real-time is the leading reason for the acceptance of computers in many
embedded applications, such as automobiles and aircraft. Secondly, the same computing
machine can be used to execute di(cid:11)erent algorithms at di(cid:11)erent times, thus having a truly
general-purpose computing machine. Thirdly, computers are immune to emotional and
physical factors such as distraction and fatigue, and can provide accurate and reliable results
almost all the time5 . Finally, embedded applications often involve working in hazardous
environments where humans cannot go, and computers are good candidates for use in such
environments.

At this stage, it is instructive to contrast the computing machine against other types of
machines such as clocks, which predate the computer by hundreds of years. Such machines
are constructed to perform a speci(cid:12)c sequence of internal actions to solve a speci(cid:12)c problem.
For instance, the hands of a clock go around at (cid:12)xed speeds; this is in fact a mechanical
implementation of an algorithm to keep track of time. A digital clock keeps track of time
using a quartz crystal and digital circuitry. Such machines can only do the one thing they
are constructed to do. A computing machine, on the other hand, is general-purpose in that
it can perform a large variety of widely di(cid:11)ering functions, based on the algorithm that
it is operating upon at any given time. Because the algorithm can be changed, di(cid:11)erent
functions can be implemented by acquiring a single hardware system and then developing
di(cid:11)erent algorithms to perform di(cid:11)erent functions in the hardware. Thus, by executing a

4 Interestingly, even now, at a time when computers have become faster by several orders of magnitude,
there are prodigies like Sakuntala Devi [] who have demonstrated superiority over computers in performing
certain kind of complex calculations!
5We should mention that computers are indeed susceptible to some environmental factors such as elec-
trical noise and high temperatures. Modern computers use error-correcting codes and other fault tolerance
measures to combat the e(cid:11)ect of electrical noise and other environmental e(cid:11)ects.

1.2. The Digital Computer

9

computer program for keeping track of time, a computer can implement a clock! This feature
is the crucial di(cid:11)erence between general-purpose computing machines and special-purpose
machines that are geared to perform speci(cid:12)c functions.

We have described some of the landmark computers in history. Besides the few comput-
ers mentioned here, there are many other precursors to the modern computer. Extensive
coverage of these computers can be found in the IEEE Annals of the History of Computing,
now in its 28th volume [ref ].

\Computers in the future wil l weigh no more than 0.5 tons."
| Popular Mechanics: Forecasting Advance of Science, 1949

1.2 The Digital Computer

We saw how computers play a ma jor role in executing algorithms or programs to obtain
solutions for problems. Solving a problem involves manipulating information of one kind
or other. In order to process information, any computer|mechanical or electrical|should
internally represent information by some means. Some of the early computers were analog
computers, in that they represented information by physical quantities that can take values
from a continuum, rather than by numbers or bit patterns that represent such quantities.
Physical quantities can change their values by an arbitrarily small amount; examples are the
rotational positions of gears in mechanical computers, and voltages in electrical computers.
Analog quantities represent data in a continuous form that closely resemble the information
they represent. The electrical signals on a telephone line, for instance, are analog-data
representations of the original voices. Instead of doing arithmetic or logical operations, an
analog computer uses the physical characteristics of its data to determine solutions. For
instance, addition could be done just by using a circuit whose output voltage is the sum of
its input voltages.

Analog computers were a natural outcome of the desire to directly model the smoothly
varying properties of physical systems. By making use of di(cid:11)erent properties of physical
quantities, analog computers can often avoid time-consuming arithmetic and logical opera-
tions. Although analog computers can nicely represent smoothly changing values and make
use of their properties, they su(cid:11)er from the di(cid:14)culty in measuring physical quantities pre-
cisely, and the di(cid:14)culty in storing them precisely due to changes in temperature, humidity,
and so on. The subtle errors introduced to the stored values due to such noise are di(cid:14)cult
to detect, let alone correct.

The 20th century saw the emergence of digital computers, which eventually replaced
analog computers in the general-purpose computing domain. Digital computers represent
and manipulate information using discrete elements called symbols. A ma jor advantage of
using symbols to represent information is resilience to error. Even if a symbol gets distorted,
it can still be recognized, as long as the distortion does not cause it to appear like another

10

Chapter 1.

Introduction

symbol. This is the basis behind error-correcting features used to combat the e(cid:11)ects of
electrical noise in digital systems. Representing information in digital format has a side-
e(cid:11)ect, however. As we can only have a limited number of bits, only a (cid:12)nite number of values
can be uniquely represented. This means that some of the values can be represented with
high degree of precision, whereas the remaining ones will need to be approximated.

Electronic versions of the digital computer are typically built out of a large collection
of electronic switches, and use distinct voltage states (or current states) to represent dif-
ferent symbols. Each switch can be in one of two positions, on or o(cid:11); designing a digital
computer will therefore be a lot simpler if it is restricted to handling just two symbols.
So most of the digital computers use only two symbols in their alphabet and are binary
systems, although we can design computers and other digital circuits that handle multiple
symbols with multiple-valued logic. The two symbols of the computer alphabet are usu-
ally represented as 0 and 1; each symbol is called a binary digit or a bit. Computers often
need to represent di(cid:11)erent kinds of information, such as instructions, integers, (cid:13)oating-point
numbers, and characters. Whatever be the type of information, digital computers repre-
sent them by concatenations of bits called bit patterns, just like representing information in
English by concatenating English alphabets and puctuation marks. The (cid:12)nite number of
English alphabets and puctuation marks do not impose an inherent limit on how much we
can communicate in English; similarly the two symbols of the computer alphabet do not
place any inherent limits on what can be communicated to the digital computer. Notice,
however, that information in the computer language won’t be as cryptic as in English, just
like inofmration in English is not as cryptic as in Chinese (which has far more symbols).

\Even the most sophisticated computer is real ly only a large, wel l-organized volume
of bits."
| David Harel. Algorithmics: The Spirit of Computing

By virtue of their speed and other nice properties, these electronic versions completely
replaced mechanical and electromechanical versions. At present, the default meaning of the
term \computer" is a a general-purpose automatic electronic digital computer.

1.2.1 Representing Programs in a Digital Computer: The Stored Pro-
gram Concept

We saw that a computer solves a problem by executing a program with the appropriate set
of input data. How is this program conveyed to the computer from the external world? And,
how is it represented within the computer? In the ENIAC system developed at University of
Pennsylvania in early 1940s, for instance, the program was a function of how its electrical
circuits were wired, i.e., the program was a function of the physical arrangement of the
cables in the system. The steps to be executed were speci(cid:12)ed by the connections within the
hardware unit. Every time a di(cid:11)erent program needed to be executed, the system had to be
rewired. Conveying a new program to the hardware sometimes took several weeks! Other

1.2. The Digital Computer

11

early computers used plug boards, punched paper tape, or some other external means to
represent programs. Developing a new program involved re-wiring a plugboard, for instance.
And, loading a program meant physically plugging in a patch board or running a paper
tape through a reader.

A marked change occurred in the mid-1940s when it was found that programs could be
represented inside computers in the same manner as data, i.e., by symbols or bit patterns.
This permits programs to be stored and transfered like data. This concept is called the
stored program concept, and was (cid:12)rst described in a landmark paper by Burks, Goldstein,
and von Neumann in 1946 [1].
In a digital computer implementing the stored program
concept, a program will be a collection of bit patterns. When programs are represented
and stored as bit patterns, a new program can be conveyed to the hardware very easily.
Moreover, several programs can be simultaneously stored in the computer’s memory. This
makes it easy not only to execute di(cid:11)erent programs one after the other, but also to switch
from one program to another and then back to the (cid:12)rst, without any hardware modi(cid:12)cation.
Stored program computers are truly \general-purpose," as they can be easily adapted to do
di(cid:11)erent types of computational and information storage tasks. For instance, a computer
can instantaneously switch from being a word processor to a telecommunications terminal,
a game machine, or a musical instrument! Right from its inception, the stored program
concept was found to be such a good idea that it has been the basis for virtually every
general-purpose computer designed since then. In fact it has become so much a part of the
modern computer’s functioning that it is not even mentioned as a feature!

In a stored program computer, the program being executed can even manipulate another
program as if it were data|for example, load it into the computer’s memory from a storage
device, copy it from one part of memory to another, and store it back on a storage device.
Altering a program becomes as easy as modifying the contents of a portion of the computer’s
memory. The ability to manipulate stored programs as data gave rise to compilers and
assemblers that take programs as input and translate them into other languages.

The advent of compilers and assemblers have introduced several additional steps in
solving problems using modern digital computers. Figure 1.3 depicts the steps involved in
solving a problem using today’s computers. First, an algorithm, or step-by-step procedure,
is developed to solve the problem. Then this algorithm is expressed as a program in a
high-level programming language by considering the syntax rules and semantic rules of the
programming language. Some of the common high-level languages are C, FORTRAN, C++,
Java, and VisualBasic.

The source program in the high-level language is translated into an executable program
(in the language of the machine) using programs such as compilers, assemblers, and linkers.
During this compilation process, syntax errors are detected, which are then corrected by the
programmer. Once the syntax errors are corrected, the program is re-compiled. Once all
syntax errors are corrected, the compiler produces the executable program. The executable
program can be executed with a set of input values on the computer to obtain the results.
Semantic errors manifest as run-time errors, and are corrected by the programmer.

12

Chapter 1.

Introduction

Problem

Algorithm Development

Human

Algorithm

Program Development

Human

Source Program

Program Translation

Program

Syntax
Errors

Semantic
Errors

Input Data

Executable Program

Program Execution

Hardware

Output Data (Results)

Figure 1.4: Basic Steps in Solving a Problem using a Computer

1.2.2 Basic Software Organization

As discussed in the previous section, today’s computers use the stored program concept.
Accordingly the software consists of symbols or bit patterns that can be stored in storage
devices such as CD-ROMs, hard disks, and (cid:13)oppy disks. A program consists of two parts|
instructions and data|both of which are represented by bit patterns. The instructions
indicate speci(cid:12)c operations to be performed on individual data items. The data items can
be numeric or non-numeric.

It is possible to write stand-alone programs that can utilize and manage all of the system
resources, so as to perform the required task. This is commonly done in controllers and
embedded computers, which typically store a single program in a ROM (read-only memory),
and run the same program forever. In the mid and higher end of the computer spectrum,
starting with some embedded computers, a dichotomy is usually practiced, however, for
a variety of reasons.
Instead of writing stand-alone programs that have the ability to
access and control all of the hardware resources, the access and control of many of the
hardware resources (typically IO devices) are regulated through a supervisory program
called the operating system. When a program needs to access a regulated hardware resource,
it requests the operating system, which then provides the requested service if it is legitimate
request. This dichotomy has led to the development of two ma jor kinds of software|user
programs and kernel programs|as shown in Figure 1.4.

The operating system is one of the most important pieces of software to go into a modern
computer system. It provides other programs a uniform software interface to the hardware

1.2. The Digital Computer

13

User programs

Kernel programs

(Application software)

(Operating system)

Figure 1.5: Basic Software Organization in a Digital Computer

resources. In addition to providing a standard interface to system resources, in multitasking
environments, the operating system enables multiple user programs to share the hardware
resources in an orderly fashion6 . This sharing increases overall system performance, and
ensures security and privacy for the individual programs. To do this sharing in a safe and
e(cid:14)cient manner, the operating system is the software that is \closest to the hardware". All
other programs use the OS as an interface to shared resources and as a means to support
sharing among concurrently executing programs.

A hardware timer periodically interrupts the running program, allowing the processor to
run the operating system. The operating system then decides which of the simultaneously
active application programs should be run next on the processor; it takes this decision with
a view to minimize processor waste time. Peripheral devices also interrupt the running
program, at which times the operating system intervenes and services the devices.

For similar reasons, memory management and exception handling functions are also
typically included in the operating system. Memory management involves supporting a large
virtual memory address space with a much smaller physical memory, and also sharing the
available physical memory among the simultaneously active programs. Exception handling
involves dealing with situations that cause unexpected events such as arithmetic over(cid:13)ow
and divide by zero.

1.2.3 Basic Hardware Organization

Even the most complex software, with its excellent abstraction and generality features, is
only like the mental picture an artist has before creating a masterpiece. By itself it does
not solve any problem. For it to be productive, it must be eventually executed on suitable
hardware with proper data, just like the artist executing his/her mental picture on a suitable
canvas with proper paint. The hardware is thus an integral part of any computer system.

\You’l l never plow a (cid:12)eld by turning it over in your mind."
| An Irish Proverb

While nearly every class of computer hardware has its own unique features, from a func-

6Even in multitasking computers, hardware diagnostic programs are often run entirely by themselves,
with no intervention from the operating system.

14

Chapter 1.

Introduction

tional point of view (i.e, from the point of view of what the ma jor parts are supposed to
do), the basic organization of modern computers|given in Figure 1.5|is still very similar
to that of the Analytical Engine proposed in the 19th century. This organization consists
of three functionally independent parts: the CPU (central processing unit), the memory
unit, and the input/output unit. The actions performed by the computer are controlled
and co-ordinated by the program that is currently being executed by the CPU. The in-
put/output unit is a collection of diverse devices that enable the computer to communicate
with the outside world. Standard input/output devices include the keyboard, the mouse,
the monitor, and so on. Programs and data are brought into the computer from the external
world using the input devices and their controllers. The input unit’s function is to accept
information from human users or other machines, through devices such as the keyboard, the
mouse, the modem, and the actuators. The results of computations are sent to the outside
world through the output unit. Some of the input/output devices are storage devices, such
as hard disks, CD-ROMs, and tapes, which can store information for an inde(cid:12)nite period
of time.

CPU

Data Path

Control Unit

Memory

Input

Output

Storage

Figure 1.6: Basic Hardware Organization of a Digital Computer

When the program and data are ready to be used, they are copied into the memory unit
from either the external environment or a storage device. The memory unit stores two types
of information: (i) the instructions of the program being executed, and (ii) the data for the
program being executed. The CPU executes a memory-resident program by reading the
program instructions and data from the memory. The execution of the program is carried
out by the CPU’s control unit, which reads each instruction in the program, decodes the
instruction, and causes it to be executed in the data path. The control unit is the brain of
the system, and behaves like a puppeteer who pulls the right strings to make the puppets
behave exactly as needed.

It is interesting to compare and contrast this organization with that of a human being’s
information processing system, which among other things, involves the brain. The main
similarity lies in the way information is input and output. Like the digital computer, the
human information processing system obtains its inputs through its input unit (the sense
organs), and provides its outputs through its output unit by way of speech and various

1.2. The Digital Computer

15

motions. The dissimilarity lies both in the organization of the remaining parts and in the
way information is stored and processed. All of the information storage and processing
happens in a single unit, the brain. Again, the brain stores information not as 0s and 1s in
memory elements, but instead by means of its internal connectivity. Information processing
is done in the brain on a massively parallel manner. This is in contrast to how information
processing is done in a digital computer, where the information is stored in di(cid:11)erent memory
units from where small pieces are brought into the CPU and processed 7 .

1.2.4 Software versus Hardware

Software consists of abstract ideas, algorithms, and their computer representations, namely
programs. Hardware, in contrast, consists of tangible ob jects such as integrated circuits,
printed circuit boards, cables, power supplies, memories, and printers. Software and hard-
ware aspects are intimately tied together, and to achieve a good understanding of computer
systems, it is important to study both, especially how they integrate with each other.
Therefore, the initial portions of this book deal with software and programming, and the
latter portions deal with the hardware components. This introductory chapter introduces a
number of software and hardware concepts, and gives a broad overview of the fundamental
aspects of both topics. More detailed discussions follow in subsequent chapters.

The boundary between the software and the hardware is of particular interest to systems
programmers and compiler developers.
In the very (cid:12)rst computers, this boundary|the
instruction set architecture|was quite clear; the hardware presented the programmer with
an abstract model that took instructions from a serial program one at a time and executed
them in the order in which they appear in the program. Over time, however, this boundary
blurred considerably, as more and more hardware features are exposed to the software, and
hardware design itself involves software programming techniques. Nowadays, it is often
di(cid:14)cult to tell software and hardware apart, especially at the boundary between them. In
fact, a central theme of this book is:
Hardware and software are logical ly equivalent.

Any operation performed by software can also be built directly into the hardware. Em-
bedded systems, which are more specialized than their general-purpose counterpart, tend to
do more through hardware than through software. In general, new functionality is (cid:12)rst in-
troduced in software, as it is likely to undergo many changes. As the functionality becomes
more standard and is less likely to change, it is migrated to hardware.

\Hardware is petri(cid:12)ed software."
| ????

Of course, the reverse is also true: Any instruction executed by the hardware can also

7Research is under way to develop computers made from quantum circuits, and even biological circuits.
In the next decade, we may very well have computers made with such \hardware", and working with di(cid:11)erent
computation models!

16

Chapter 1.

Introduction

be simulated in software. Suppose an end user is using a computer to play a video game.
It is possible to construct an electronic circuit to directly handle video games, but this is
seldom done. Instead a video game program is executed to simulate a video game. The
decision to put certain functions in hardware and others in software is based on such factors
as cost, speed, reliability, and frequency of expected changes. These decisions change with
trends in technology and computer usage.

1.2.5 Computer Platforms

Classi(cid:12)cation is fundamental to understanding anything that comes in a wide variety. Auto-
mobiles can be classi(cid:12)ed according to manufacturer, body style, pickup, and size. Students
often classify university faculty based on their teaching style, sense of humor, and strictness
of grading. They classify textbooks according to price, contents, and ease of understanding.
Likewise, computers come in various sizes, speeds, and prices, from small-scale to large-
scale. Table 1.1 gives a rough categorization of today’s computers. This categorization is
somewhat idealized. Within each category, there is wide variability in features and cost; in
practice the boundary between two adjacent categories is also somewhat blurred. The ap-
proximate price (cid:12)gures in the table are only intended to show order of magnitude di(cid:11)erences
between di(cid:11)erent categories. All computers are functionally similar, irrespective of where
they line up in the spectrum. The general principles of computer architecture and organi-
zation are the same for the entire computer spectrum, from workstations to multiprocessors
and distributed computer systems.

Category

Price Typical applications

Disposable computer
Embedded computer
Entertainment PC
Desktop or laptop PC
Server
Collection of workstations
Mainframe
Supercomputer

$1 Greeting cards, watches
$10 Home appliances, cars
$100 Home video games
$1000 Word processing, CAD design
$10,000 Network server
$100,000 LAN
$1,000,000 Bank accounts, airline reservations
$10,000,000 Weather forecast, oil exploration

Table 1.1: Di(cid:11)erent categories of currently available computers

At one end of the spectrum we have disposable computers like the ones used in greeting
cards, inexpensive watches, and other similar applications. These are quite inexpensive
because they use a single chip with small amounts of memory, and are produced in large
quantities. Then there are a wide variety of embedded computers, used in applications such
as automobiles and home appliances. The entertainment PCs are computer systems that
are optimized for games, personal communications, and video playback. They typically

1.3. A Modern Computer System

17

have high-quality graphics, video, and audio so as to support high clarity and realism.
Desktop computers and laptop computers are typically intended for a single user to run
applications such as word processing, web browsing, and receiving/sending email. These
computers come with di(cid:11)erent features and costs.
In the immediately higher category,
we have servers. A server is a high-performance computer that serves as a gateway in a
computer network. At the other end of the spectrum, we have the supercomputers, which
are used for applications involving very complex calculations, such as weather prediction
and nuclear explosion modeling. The lower end of the spectrum often provides the best
price/performance ratio, and the decision on which system to purchase is often dictated by
such issues as software and ob ject code compatibility.

1.3 A Modern Computer System

As discussed earlier, computers come in various sizes and kinds. Among these, perhaps the
most commonly seen one and one that comes to mind vividly when one thinks of a computer,
is a desktop computer. Desktop computers are designed to be truly general-purpose. For
these reasons, we provide a detailed description of a typical desktop computer system in
this section. In fact, many of the issues discussed here are applicable to all members of the
computer spectrum.

1.3.1 Hardware

Figure 1.6 shows a typical desktop computer. It has a system unit which is the case or
box that houses the motherboard, other printed circuit boards, the storage devices, and the
power supply. The system unit is generally designed in such a way that it can be easily
opened to add or replace modules. The di(cid:11)erent components in the system unit are typically
connected together using a bus, which is a set of wires for transferring electrical signals. Each
printed circuit board houses a number of chips, some of which are soldered and the rest are
plugged into the board. The latter permits the user to upgrade the computer components.
Circuits etched into the boards act like wires, providing a path for transporting data from
one chip to another.

Figure 1.7: Photograph of a Typical Desktop Computer System

Processor: The processor, also called the central processing unit (CPU), is perhaps the
most important part of a computer. It carries out the execution of the instructions of a
program.

18

Chapter 1.

Introduction

Chip Sets: The chipsets provide hardware interfaces for the processor to interact with
other devices, such as DRAM and graphics cards.

Motherboard: The motherboard is the main printed circuit board, and holds the com-
puter’s processor chip(s), ROM chips, RAM chips, and several other key electronic com-
ponents. The processor is an important part of a computer, and can be a single chip or
a collection of chips. ROM chips typically contain a small set of programs that start the
computer, run system diagnostics, and control low-level input and output activities. These
programs are collectively called BIOS (basic input output system) in PCs. The instructions
in the ROM chips are permanent, and the only way to modify them is to reprogram the
ROM chips. RAM chips are volatile and hold program and data that is temporary in na-
ture. A battery powered real-time clock chip keeps track of the current date and time. The
motherboard also typically contains expansion slots, which are sockets into which expansion
cards such as video card, sound card, and internal modem, can be plugged in. An expansion
card has a card edge connector with metal contacts, which when plugged into an expansion
slot socket, connect the circuitry on the card to the circuitry on the motherboard. The
number of expansion slots in the motherboard determines its expandability.

Figure 1.8: Photograph of a Motherboard

Storage Devices: The commonly used storage devices are (cid:13)oppy disk drives, hard disk
drives, CD-ROM drives, and ZIP drives. A (cid:13)oppy disk drive is a device that reads and
writes data on (cid:13)oppy disks. A typical (cid:13)oppy disk drive uses 3 1
2 -inch (cid:13)oppy disks each of
which can store up to 1.44 MB. A hard disk drive can store billions of bytes on a non-
removable disk platter. A CD-ROM drive is a storage device that uses laser technology to
read data from a CD-ROM. The storage devices are typically mounted in the system unit.
The ones involving removable media such as the (cid:13)oppy disk drive, the CD-ROM drive, and
the ZIP drive are mounted on the front side of the system unit, and the hard disk drives
are typically mounted inside the system unit.

Input/Output Devices: Two of the commonly used input devices in a desktop computer
are the keyboard and the mouse. A computer keyboard looks similar to that of a typewriter,
with the addition of number keys, as well as several additional keys that control computer-
speci(cid:12)c tasks. The mouse is useful in manipulating ob jects depicted on the screen. Other
commonly used input device is the microphone. The primary output device in a desktop
computer is the monitor, a display device that forms an image by converting electrical
signals from the computer into points of colored light on the screen. Its functioning is very
to a television picture tube, but has a much higher resolution so that a user sitting at close
quarters can clearly see computer-generated data such as text and images. Other frequently
used output devices are the printer and the speakers.

1.3. A Modern Computer System

19

Device Controllers: Each device|keyboard, mouse, printer, monitor, etc|requires spe-
cial control ler circuitry for transferring data from the processor and memory to the device,
and vice versa. A device controller is designed either as a chip which is placed in the
motherboard or as a printed circuit board which is plugged into an expansion slot of the
motherboard. The peripheral devices are connected to their respective controllers in the
system unit using special cables to sockets called expansion ports. The ports are located
on the backside of the system unit and provide connections through holes in the back of
the system unit. Parallel ports transfer several bits simultaneously and are commonly used
to connect printers to the computer. Serial ports transfer a single bit at a time, and are
commonly used to connect mice and communication equipment to the computer. Device
controllers are very complex. Each logical command from the processor must typically be
decomposed into long sequences of low-level commands to trigger the actions to be per-
formed by the device and to supervise the progress of the operation by testing the device’s
status. For instance, to read a word from a disk, the disk controller generates a sequence of
commands to move the read/write arm of the disk to the correct track, await the rotational
delay until the correct sector passes under the read/write arm, transfer the word, and check
for a number of possible error conditions. A sound card contains circuitry to convert digital
signals from the computer to sounds that play through speakers or headphones that are
connected to the expansion ports of the card. A modem card connects the computer to the
telephone system so as to transport data from one computer to another over phone lines.
A network card, on the other hand, provides the circuitry to connect a computer to other
computers on a local area network.

1.3.2 Software

A desktop computer typically comes with pre-installed software. This software can be
categorized into two categories|application software and systems software.

Application Software: Application programs are designed to satisfy end-user needs by
operating on input data to perform a given job, for example, to prepare a report, update
a master payroll (cid:12)le, or print customer bills. Application software may be packaged or
custom. Packaged software includes programs pre-written by professional programmers,
and are typically o(cid:11)ered for sale in a (cid:13)oppy disk or CD-ROM. Custom software includes
programs written for a highly specialized task.

Systems Software: Systems software enables the application software to interact with
the computer, and helps the computer manage its internal and external resources. Systems
software is required to run applications software; however, the converse is not true. Systems
software can be classi(cid:12)ed into three types|utility programs, language translators, and the
operating system. Utility programs are generally used to support, enhance, or expand the
development of application programs. Examples consist of editors and programs for merging

20

Chapter 1.

Introduction

(cid:12)les. A language translator or compiler is a software program that translates a program
written in a high-level language such as C into machine language, which the hardware
can directly execute. Thus a compiler provides the end user with the capability to write
programs in a high-level language.

Operating System: The operating system is a ma jor component of the systems software.
Desktop operating systems allocate and control the use of all hardware resources:
the
processor, the main memory, and the peripheral devices. They also add a variety of new
features, above and beyond what the hardware provides. Running the shell provides the end
user with a more \capable" machine, in that the computer system provides direct capability
to specify commands by typing them on a keyboard. The GUI (graphical user interface)
goes one step further by providing the user with a graphical view of the desktop, and letting
the user enter commands by clicking on icons. The multitasking feature of the OS provides
the user with the capability to run multiple tasks \concurrently". The (cid:12)le system of the OS
provides the user with a structured way of storing and accessing \permanent" information.
The operating system is thus an important part of most computer systems because it exerts
a ma jor in(cid:13)uence on the overall function and performance of the entire computer. Normally,
the OS is implemented in software, but there is no theoretical reason why it could not be
implemented in hardware!

Device Driver (Software Driver): Most application programs need to access input/output
devices and storage devices such as disks, terminals, and printers. Allowing these programs
to perform the low-level IO activity required to directly control an input/output device
is not desirable for a variety of reasons. First, most application programmers would (cid:12)nd
it extremely di(cid:14)cult to do the intricate actions required to directly control an IO device.
Second, inappropriate accesses of the IO devices by amateur or malicious programmers can
wreck plenty of havoc. The standard solution adopted in computer systems is therefore to
provide a more abstract interface to the application programmer, and let an interface pro-
gram perform the required low-level IO activity. This interface program is called a device
driver or software driver. Each device requires speci(cid:12)c device driver software, because
each device has its own speci(cid:12)c commands whereas an application program uses generic
commands. The device driver receives generic commands from the application program
and converts them into the specialized commands for the device, and vice versa.

1.3.3 Starting the Computer System: The Boot Process

Now that you have a good understanding of the role of an operating system in a modern
computer, it would be interesting to learn how the operating system is activated each time
a computer is turned on. When a computer is turned o(cid:11), the data in the registers and
memory are lost. Thus when the computer is turned on, the OS program is not residing in
the main memory, and needs to be brought into main memory from a storage device such as

1.3. A Modern Computer System

21

a diskette or hard disk. In modern computers, this copying is done by executing a program
called the bootstrap program or boot program for short. How can the computer execute
this copy program if the memory contains no useful contents? To solve this dilemma, a
portion of the memory is implemented using non-volatile memory devices such as a read-
only memory (ROM). This memory contains the boot program. When the computer is
turned on, it starts executing instructions from the starting address of the boot program.
The boot program contains code to perform diagnostic tests of crucial system components
and load the operating system from a disk to the main memory. This bootstrap loader may
be comprehensive enough to copy the nucleus of the operating system into memory. Or it
may (cid:12)rst store a more comprehensive loader that, in turn, installs the nucleus in memory.
Once loaded, the OS remains in main memory until the computer is turned o(cid:11).

For copying the OS from a disk drive to the RAM, the computer needs to know how
the disk has been formatted, i.e., the number of tracks and sectors and the size of each
sector.
If information about the hard disk were stored in the ROM, then replacing the
hard disk becomes a di(cid:14)cult proposition, because the computer will not be able to access
the new hard disk with information about the old disk. Therefore, a computer must have
a semi-permanent medium for storing boot information, such as the number of hard disk
drive cylinders and sectors. For this purpose, it uses CMOS (complementary metal oxide
semiconductor) memory, which requires very little power to retain its contents and can
therefore be powered by battery. The battery helps the CMOS memory to retain vital
information about the computer system con(cid:12)guration, even when the computer is turned
o(cid:11). When changing the computer system con(cid:12)guration, the information stored in the CMOS
memory must be updated, either by the user or by the plug-and-play feature.??

1.3.4 Computer Network

Till now we were mostly discussing stand-alone computers, which are not connected to any
computer network. Most of today’s desktop computers are instead connected to a network,
and therefore it is useful for us to have a brief introduction to this topic. A computer network
is a collection of computers and other devices that communicate to share data, hardware,
and software. Each device on a network is called a node. A network that is located within
a relatively limited area such as a building or campus is called a local area network or
LAN, and a network that covers a large geographical area is called a wide area network
or WAN. The former is typically found in medium-sized and large businesses, educational
institutions, and government o(cid:14)ces. Di(cid:11)erent types of networks provide di(cid:11)erent services,
use di(cid:11)erent technology, and require users to follow di(cid:11)erent procedures. Popular network
types include Ethernet, Token Ring, ARCnet, FDDI, and ATM.

Give a (cid:12)gure here

A computer connected to a network can still use all of its local resources, such as hard
drive, software, data (cid:12)les, and printer.
In addition, it has access to network resources,
which typically include network servers and network printers. Network servers can serve as

22

Chapter 1.

Introduction

a (cid:12)le server, application server, or both. A (cid:12)le server serves as a common repository for
storing program (cid:12)les and data (cid:12)les that need to be accessible from multiple workstations|
client nodes|on the network. When an individual client node sends a request to the (cid:12)le
server, it supplies the stored information to the client node. Thus, when the user of a client
workstation attempts to execute a program, the client’s OS sends a request to the (cid:12)le server
to get a copy of the executable program. Once the server sends the program, it is copied
into the memory of the client workstation, and the program is executed in the client. The
(cid:12)le server can also supply data (cid:12)les to clients in a similar manner. An application server, on
the other hand, runs application software on request from other computers, and forwards
the results to the requesting client.

In order to connect a computer to a network, a network interface card (NIC) is required.
This interface card sends data from the workstation out over the network and collects
incoming data for the workstation. The NIC for a desktop computer can be plugged into
one of the expansion slots in the motherboard. The NIC for a laptop computer is usually
a PCMCIA card. Di(cid:11)erent types of networks require di(cid:11)erent types of NICs.

A computer network requires a network operating system to control the (cid:13)ow of data,
maintain security, and keep track of user accounts. Commonly used operating systems such
as UNIX, Windows XP, and Windows Vista already include networking capability. There
are also software packages such as Novel l Netware that are designed exclusively for use as
network operating system. A network operating system usually has two components: server
software and client software. The server software is installed in the server workstation; it
has features to control (cid:12)le access from the server hard drive, manage the print queue, and
track network user data such as userids and passwords. The client software is installed
on the local hard drive of each client workstation; it is essentially a device driver for the
NIC. When the client workstation boots up, the network client software is activated and
establishes the connection between the client and the other devices on the network.

1.4 Trends in Computing

Computer systems have undergone dramatic changes since their inception a few decades
ago. It is di(cid:14)cult to say whether it is the hardware that drives the software or if it is the
other way around. Both are intimately tied to each other; trends in one do a(cid:11)ect the other
and vice versa. We shall (cid:12)rst discuss trends in hardware technology.

1.4.1 Hardware Technology Trends

Ever since transistors began to be integrated in a large scale, producing LSI and VLSI (Very
Large Scale Integration) circuits, there have been non-stop e(cid:11)orts to continually reduce the
transistor size. Over the last three decades, the feature size has decreased nearly by a factor
of 100, resulting in smaller and smaller transistors. This steady decrease in transistor sizes,

1.4. Trends in Computing

23

coupled with occasional increases in die sizes, have resulted in more and more transistors
being integrated in a single chip. This has translated .......

In 2008, Intel r(cid:13)released the (cid:12)rst processor chip that integrates more than 2 billion
transistors|the quad-core Tukwila. As of 2008, it is also the biggest microprocessor made,
with a die size of 21.5 (cid:2) 32.5mm2 .
Below we highlight some of the main trends we see in hardware technology today:

(cid:15) Clock speed: Clock speed had been steadily increasing over the last several decades;
however, the current trend hints more of a saturation.
In 2007, IBM released the
dual-core Power6 processor, which operates at an astonishing 4.7 GHz clock.

(cid:15) Low-power systems: In the late 1990s, as the number of transistors as well as the
clock speed steadily increased, power consumption|especially power density|began
to increase at an alarming rate. High power densities translated to higher temper-
atures, necessitating expensive cooling technologies. Today, power consumption has
become a (cid:12)rst-order design constraint, and power-aware hardware designs are com-
monplace.

(cid:15) Large memory systems: Memory size has always been increasing steadily, mir-
roring the downward trend in price per bit. However, memory access time increases
with size, necessitating the use of cache memories to reduce average memory latencies.
Nowadays, it is commonplace to see multi-level cache organizations in general-purpose
computers.

(cid:15) Multi-core processors: The current trend is to incorporate multiple processor cores
on the same die. These cores parallely execute multiple threads of execution.

(cid:15) Embedded systems: Although embedded systems have been around for a while,
their popularity has never been as high as it is today. Cell phones, automobile controls,
computer game machines | you name it! | all have become so sophisticated, thanks
to advances in embedded system technology.

Some of these trends become clear when we look at the microprocessors developed over
the last four decades for desktop systems by one of the ma jor processor manufacturers,
Intel r(cid:13). Table 1.2 succinctly provides various features of these processors.

1.4.2 Software Technology Trends

As processors and memory became smaller and faster | providing the potential for signi(cid:12)-
cant boosts in performance | application programs and operating systems strived to o(cid:11)set
that bene(cid:12)t by becoming bigger and sluggish. The time to boot up a computer, for instance,
has remained steady|if not increased|over the years. This does not mean that software

24

Chapter 1.

Introduction

Processor

Word Intro

Feature

Die Number of

Clock

Name
4004
8008
8080
8085
8086
80286
Intel386T M
Intel486T M
Pentium r(cid:13)
Pentium r(cid:13)Pro
Pentium r(cid:13)II
Pentium r(cid:13)III
Pentium r(cid:13)4
Pentium r(cid:13)D

size Year
1971
4
1972
8
1974
8
8
1976
1978
16
1982
16
1985
32
32
1989
1993
32
1995
32
1997
32
32
1999
2000
32
64
2005

size ((cid:22)m)
10
10
6
3
3
1.5
1.5
1
0.8
0.35
0.35
0.18
0.18
0.09

area (mm2 ) Transistors
Freq.
2300
13.5
108 KHz
3500
200 KHz
6000
2 MHz
6500
2 MHz
29K 4.77 MHz
134K
6 MHz
16 MHz
275K
1.2M
25 MHz
3.1M
66 MHz
5.5M 200 MHz
7.5M 300 MHz
28M 733 MHz
42M
2 GHz
3.2 GHz
230M

81
294
195
131
106
217
206

Table 1.2: Progression of Intel r(cid:13)Microprocessors Designed for Desktop Systems

technology has made no progress. On the contrary, there have been tremendous improve-
ments in software application development. The driving philosophy in software technology
development has also been performance and e(cid:14)ciency, but of the programmer!.

Below we highlight a few of the current trends in software applications:

(cid:15) Application Nature:

{ Multimedia

{ Graphics

{ Bioinformatics

{ Web-based

(cid:15) Programming Methodology and Interface:

{ Ob ject-oriented programming: Java, C#

{ Visual programming: Scripting, HTML

{ Multi-threading

{ Application Programming Interface (API): POSIX, Windows API

(cid:15) Operating System:

{ Linux

1.5. Software Design Issues

25

{ Windows Vista
{ Mac OS X

(cid:15) Security:

1.5 Software Design Issues

A good software piece is not one that just works correctly. Modularity, simplicity,

1.6 Hardware Design Issues

Among the two phases in the life of a program | its development and execution | hardware
designers are concerned with the execution phase. As we will see in Section 1.8, hardware
design is carried out in various stages, at di(cid:11)erent levels of abstraction. When designing
hardware, the factors that stand at the forefront are performance, power consumption, size,
and price; well designed hardware structures are those that have adequate performance and
long battery life (if running o(cid:11) a battery), and are compact and a(cid:11)ordable. Other issues
that become important, depending on the computing environment, are binary compatibility,
reliability, and security.

1.6.1 Performance

The speed with which computer systems execute programs has always been a key design
parameter in hardware design. We can think of two di(cid:11)erent metrics when measuring the
speed of a computer system: response time and throughput. Whereas response time refers
to how long the computer takes to do an activity, throughput refers to how much the com-
puter does in unit time. The response time is measured by time elapsed from the initiation
of some activity until its completion. A frequently used response time metric is program
execution time, which speci(cid:12)es the time the computer takes for executing the program once.
The execution time of a program, ET , can be expressed as the product of three quantities:
(i) the number of instructions executed or instruction count (I C ), (ii) the average number
of clock cycles required to execute an instruction or cycles per instruction (C P I ), and (iii)
the duration of a clock cycle or cycle time (C T ). Thus,
ET = I C (cid:2) C P I (cid:2) C T
Although this simple formula seems to provide a good handle on program execution time,
and therefore on computer performance, the reality is not so simple! The instruction count
of a program may vary depending on the data values supplied as input to the program. And,
the cycles per instruction obtained may vary depending on what other programs are simul-
taneously active in the system8 . Finally, we have computer systems that dynamically adjust

8Nowadays, almost all computer systems execute multiple programs at the same time.

26

Chapter 1.

Introduction

the clock cycle time|dynamic frequency scaling|in order to reduce power consumption.

While reporting program execution time, the standard practice used to deal with the
(cid:12)rst problem is to measure the execution time with a standard set of input data. The
second and third problems are avoided by not running only one benchmark program at a
time and by not exercising dynamic frequency scaling.

Throughput, the other metric of performance, speci(cid:12)es the number of programs, jobs,
or transactions the computer system completes per unit time.
If the system completes
C programs during an observation period of T seconds, its throughput X is measured as
C/T programs/seconds. For processors, a more commonly used throughput measure is the
number of instructions executed in a clock cycle, referred to as its IPC (instructions per
cycle).

Although throughput and response time are related, improving the throughput of a
computer system does not necessarily result in reduced response time. For instance, the
throughput of a computer system improves when we incorporate additional processing cores
and use these cores for executing independent tasks, but that does not decrease the execution
time of any single program. On the other hand, replacing a processor with a faster one would
invariably decrease program execution time as well as improve throughput.

1.6.2 Power Consumption

After performance, power consumption is perhaps the biggest design issue to occupy the
hearts and minds of computer designers.
In fact, in some application domains, power
consumption has edged out performance as the most important design factor. Why is
power such an important issue? This is because it directly translates to heat production.
Most of the integrated circuits will fail to work correctly if the temperature rises beyond a
few degrees.

Again, the designer’s goal is to reduce the power consumption occurring during program
execution, as program development can be carried out in a di(cid:11)erent system where power
consumption may not be burning issue.

Power consumption has two components: dynamic and static. Dynamic power relates to
power consumed when there is switching activity (or change of state) in a system, whereas
static power relates to power consumed even when there is no switching activity in the
system. Dynamic power is directly proportional to the extent of switching activity in the
system and the clock frequency of operation. It is also proportional to the capacitance in
the circuits and wires, and to the square of the supply voltage of the circuits.

Static power consumption occurs due to leakage currents in the system. With continued
scaling in transistor technology|reduction in transistor sizes|static power consumption
is becoming comparable to dynamic power consumption. Static power consumption is also
related to the supply voltage. Therefore, to develop low-power systems, computer hardware
designers strive to reduce the supply voltage of the circuits as well as reduce the amount of

1.7. Theoretical Underpinnings

27

hardware used to perform the required functionality.

1.6.3 Price

Price is an important factor that makes or breaks the success of any computer system. Be-
tween two comparable computer systems, all things being equal, price will be an important
factor. The ma jor factors a(cid:11)ecting the price are design cost, manufacturing cost, and pro(cid:12)t
margin, all of which may be impacted by the sales volume. In general, price increases expo-
nentially with the complexity of the system. Therefore, it is imperative to reduce hardware
complexity at all costs.

1.6.4 Size

Size is an important design consideration, especially for laptops and embedded systems.

1.6.5 Summary

From the above discussion, it is apparent that design the computer hardware is a complex
process, where one has to focus on several factors at the same time. Often, focusing on one
factor comes at the expense of others. For instance, attempting to improve performance by
using substantial amounts of hardware generally results in high power consumption as well
as size and price. A good design will attempt to achieve good performance without increase
in hardware complexity, thereby conserving power, and reducing the size and price as well.

1.7 Theoretical Underpinnings

1.7.1 Computability and the Turing Machine

The (cid:13)edgling days of computers saw them only solving problems of a numerical nature;
soon they began to process various kinds of information. A question that begs an answer is:
What kinds of problems can a computer solve? The answer, as per computer science theory,
is that given enough memory and time, a computer can solve all problems for which a (cid:12)nite
solution algorithm exists. One of the computer pioneers who de(cid:12)ned and formalized compu-
tation was the British mathematician Alan Turing. While a graduate student at Princeton
University in 1936, Turing published a seminal paper titled \On Computable Numbers
with an Application to the Entscheidungsproblem," which laid a theoretical foundation for
modern computer science. In that paper he envisioned a theoretical machine, which later
became known as a Turing Machine, that could read instructions from a punched paper tape
and perform all the critical operations of a computer. One of Turing’s remarkable achieve-
ments was to prove that a universal Turing machine|a simulator of Turing machines|can

28

Chapter 1.

Introduction

perform every reasonable computation [?].
If given a description of a particular Turing
machine TM, the universal Turing machine simulates all the operations performed by TM.
It can do anything that any real computer can do, and therefore serves as an abstract model
of all general-purpose computers. Turing’s paper also established the limits of computer
science by mathematically demonstrating that some problems do not lend themselves to
algorithmic representations, and therefore cannot be solved by any kind of computer.

1.7.2 Limitations of Computers

\Computers are useless. They can only give you answers."
| Pablo Picasso (1881 - 1973).

For the computer to solve a problem, it is imperative to (cid:12)rst develop a solution algorithm,
or step-by-step procedure, for the problem. Although a general-purpose computer can be
used to solve a wide variety of problems by executing appropriate algorithms, there are
certain classes of problems that cannot be solved using a computer, either in principle or
in practice! Such problems may be grouped into three categories:

(cid:15) Undecidable or non-computable problems

(cid:15) Unsolvable problems

(cid:15) Intractable problems

Well−defined problems

Computable

Noncomputable

Undecidable or Non-computable
Problems: This category includes
problems that have been proven not
to have (cid:12)nite solution algorithms.
Kurt G(cid:127)odel, a famous mathemati-
cian, proved in the 1930s his famous
incompleteness theorem [ref ]. An
important consequence of G(cid:127)odel’s
theorem is that there is a limit
on our ability to answer questions
about mathematics.
If we have a
mathematical model as complex as
the set of integers, then there is no
algorithmic way by which true state-
ments can be distinguished from
false ones.
In practical terms, this
means that not all problems have an
algorithmic solution, and therefore a computer cannot be used to solve any arbitrary prob-
lem. In particular, a computer cannot (cid:12)nd proofs in su(cid:14)ciently complex systems. A. M.

Figure 1.9: Categorization of Well-De(cid:12)ned Problems
based on Computability

Partially
computable

Tractable

NP−complete

Unsolvable

1.7. Theoretical Underpinnings

29

Turing and Alonzo Church demonstrated a set of undecidable problems in 1936. One of
these is what has become known as the Turing machine halting problem, which states that
no algorithm exists to determine if an arbitrary Turing machine with arbitrary input data
will ever halt once it has started working. A practical implication of this result is that given
a su(cid:14)ciently complex computer program with loops, it is impossible to determine if under
certain inputs the program will ever halt. Since the 1930s, a number of other problems
have been proven to be undecidable. It will never be possible in a logically consistent sys-
tem to build a computer, however powerful, that by manipulating symbols can solve these
undecidable problems in a (cid:12)nite number of steps!

Unsolvable Problems: This category includes well-de(cid:12)ned problems that have not been
proved to be undecidable, but for which no (cid:12)nite algorithm has yet been developed. An
example is Goldbach’s conjecture, formulated by the 18th century mathematician Christian
Goldbach. The conjecture states that every even integer greater than 2 is the sum of exactly
two prime numbers. Although this conjecture has been veri(cid:12)ed for a large number of even
integers, it has not yet been proved to be true for every even integer, nor has any (cid:12)nite
algorithm been developed to prove this conjecture. An algorithm that examines all even
integers is not (cid:12)nite, and therefore will not terminate. An unsolved problem may eventually
be solved or proved to be undecidable.

Intractable Problems: This category includes problems that have a (cid:12)nite solution al-
gorithm, but executing the best available algorithm requires unreasonable amounts of time,
computer memory, and/or cost.
In general, this is the case when the complexity of the
best available algorithm grows exponentially with the size of the problem. An example of
an intractable problem is the traveling salesman problem. The ob jective in this problem is
to (cid:12)nd a minimum-distance tour through a given set of cities. The best available solution
algorithms for this problem are exponential in n, where n is the number of cities in the tour.
This means that the execution time of the algorithm increases exponentially as n increases.
For reasonably large values of n, executing such an algorithm becomes infeasible. Many
problems that occur in real life are closely related to the traveling salesman problem; two
common examples are the scheduling of airline (cid:13)ights, and the routing of wires in a VLSI
chip. An intractable problem becomes more tractable with technological advances that
make it feasible to design faster computers. Algorithm developers often tackle intractable
problems by devising approximate or inexact solution algorithms. These approximate al-
gorithms often involve the use of various heuristics, and are near-optimal most of the time.
Simulated annealing is an example of such an algorithm. Recent research seems to indicate
that quantum computing has the potential to solve many of the intractable problems more
e(cid:14)ciently.

30

Chapter 1.

Introduction

1.8 Virtual Machines: The Abstraction Tower

If we look at the computer as a physicist would do, we will see that a digital computer
executes an algorithm by controlled movement of electrons through silicon substrates and
metal wires. A complete description of a computer could be given in terms of all of its silicon
substrates, impurity dopings, wire connections, and their properties. Such a view, although
very precise, is too detailed even for computer hardware designers, let alone the program-
mers. Hardly any programs would have been written in all these years if programmers were
given such a speci(cid:12)cation!

Like many other machines built today, computers are incredibly complex. The functions
involved in developing programs and in designing the hardware to execute them are so
diverse and complex that it is di(cid:14)cult for a user/programmer/designer to have mastery
of all of the functions. A practical technique for dealing with complexity in everyday
life is abstraction9 . An automobile driver, for instance, need not be concerned with the
details of how exactly the automobile engine works. This is possible because the driver
works with a high-level abstract view of the car that encapsulates the essentials of what
is required for driving the vehicle. The car mechanic, on the other hand, has a more
detailed view of the machine.
In a similar manner, abstraction is used to deal with the
complexity of computers. That is, computer software and hardware can be viewed as a
series of architectural abstractions or virtual machines. Di(cid:11)erent users see di(cid:11)erent (virtual)
machines depending on the level at which they use the computer. For instance, a high-
level language programmer sees a virtual machine that is capable of executing statements
speci(cid:12)ed in a high-level language. An assembly language programmer, on the other hand,
sees a di(cid:11)erent machine with registers and memory locations that can execute instructions
speci(cid:12)ed in an assembly language. Thus, the study of a computer system is (cid:12)lled with
abstractions. There is yet another advantage to viewing the computer at several abstraction
levels. Programs developed for a particular abstraction level can be executed in di(cid:11)erent
platforms|which di(cid:11)er in speed, cost, and power consumption|that implement the same
abstract machine.

The user who interacts with a computer system at a particular abstraction level has
a view of what its capabilities are at this level, and this view results directly from the
functions that the computer can perform at this level. Conceptually, each architectural
abstraction is a set of rules that describes the logical function of a computer as observable by
a program running on that abstract machine. The architecture does not specify the details of
exactly how its functions will be performed; it only speci(cid:12)es the architecture’s functionality.
Implementation issues related to the functionality are left to the lower-level machines. The

9An abstraction is a representation that hides details so that one can focus on a few concepts at a time.
Formal abstractions have a well-de(cid:12)ned syntax and semantics. Hence, they provide a way of conveying the
information about a system in a consistent way that can be interpreted unambiguously. This abstraction (or
speci(cid:12)cation) is like a contract: it de(cid:12)nes how the system behaves. The abstraction de(cid:12)nes how the outside
world interacts with the system. The implementation, on the other hand, de(cid:12)nes how the system is built,
as seen from the inside.

1.8. Virtual Machines: The Abstraction Tower

31

entire point of de(cid:12)ning each architectural abstraction is to insulate programmers of that
level from those details. The instruction set architecture, for instance, provides a level of
abstraction that allows the same (machine language) program to be run on a family of
computers having di(cid:11)erent implementations (i.e., microarchitectures).

\‘There are many paths to the top of the mountain,
but the view is always the same."
| Chinese Proverb

In order to make it easier for comprehension purposes, we have organized the computer
virtual machines along a single dimension as an abstraction tower, with one machine \above"
the other. Each virtual machine except the one at the lowest level is implemented by the
virtual machine below it. This approach is called hierarchical abstraction. By viewing the
computer as a hierarchy of abstractions, it becomes easier to master the complexity of
computers and to design computer systems in a systematic, organized way.

Appropriate interfaces are used to specify the interaction between di(cid:11)erent abstractions.
This implementation is done by translating or interpreting the steps or instructions of one
level using instructions or facilities from the lower level. A particular computer designer
or user needs to be familiar only with the level at which he/she is using the computer.
For instance, a programmer writing a C program can assume that the program will be
executed on a virtual machine that directly executes C programs. The programmer need
not be concerned about how the virtual machine implements C’s semantics. Similarly,
in a multitasked computer system, each active program sees a separate virtual machine,
although physically there may be only a single computer! Some of the machine levels
themselves can be viewed as a collection of multiple abstractions. One such breakdown
occurs in the assembly-level language machine where we further break it into User mode
and Kernel mode.

Figure 1.10 depicts the principal abstraction levels present in modern computers.
In
the (cid:12)gure the planes depict the abstract machines. For each machine level, we can write
programs speci(cid:12)c to that level to control that machine. The solid blocks depict the peo-
ple/software/hardware who transform a program for one machine level to a program for the
machine level below it. For the sake of clarity and to put things in proper perspective, the
(cid:12)gure also includes a few levels (at the top) that are currently implemented by humans. It
is important to note that these abstract machines are somewhat di(cid:11)erent from the virtual
machines seen by end users when they run di(cid:11)erent programs on a computer system. For
instance, when you run a MIPS assembler program on a host computer system, you do not
\see" that host as a MIPS assembly-level machine or a MIPS ISA-level machine. Instead,
your view of the host is simply that of a machine capable of taking a MIPS assembly lan-
guage program as input, and churning out an equivalent MIPS machine language program
as output! You can even run that assembler program without knowing anything about the
MIPS assembly language or machine language. The person who wrote the MIPS assem-
bler, on the other hand, does see the MIPS assembly-level architecture as well as the MIPS

32

Chapter 1.

Introduction

ISA. Finally, the MIPS assembler program may have been originally written in an HLL,
for example C, in which case its developer also sees an abstract C machine that takes as
commands C statements!

The Use of a Language for an Abstraction: Each virtual machine level provides an
abstraction that is suitable for the computer user/designer working at that level. In order to
make use of a particular abstraction, it must be possible to specify commands/instructions
to that virtual machine. Without a well-de(cid:12)ned language, it becomes di(cid:14)cult to specify a
program of reasonable size and complexity. Most of the abstraction levels therefore provide
a separate language to enable the user at that level to specify the actions to be performed
by the virtual machine. The language speci(cid:12)es what data can be named by a program at
that level, what operations can be performed on the named data, and what ordering exists
among the operations. The language must be rich enough to capture the intricacies of the
corresponding virtual machine. When describing each of the abstraction levels, we will show
how the language for that level captures the essence of that level, and how it serves as a
vehicle to represent the commands speci(cid:12)ed by the user of that level.

\The limits of my language mean the limits of my world."
| L. Wittgenstein. Tractatus Logicio-Philosophicus

Translators, Interpreters, Emulators, and Simulators: An important aspect of the
layered treatment of computers is that, as already mentioned, the commands speci(cid:12)ed at
each abstraction level need to be converted to commands speci(cid:12)c to the immediately lower
level. Such a transformation makes the computer behave as a di(cid:11)erent machine than the
one for which the original program was written. This transformation process can be done by
translation or interpretation. In computer parlance, the term translation indicates taking a
static program or routine, and producing a functionally equivalent static program, usually
at a lower level. A static program is one that is not being executed currently. Thus,
translation of a loop involves translating each command of the loop exactly once. The term
interpretation, on the other hand, indicates taking individual steps of a dynamic program
and producing an equivalent sequence of steps, usually at a lower level. A dynamic program
is one that is in the process of being executed. Thus, interpretation of a loop involves
interpreting each command of the loop multiple times, depending on the number of times
the loop gets executed. Because of this dynamic nature, an interpreter essentially makes
one machine (the host machine) appear as another (the target machine). A translator is
almost always implemented in software, whereas an interpreter is implemented in software
or hardware. A software interpreter is often called a simulator, and a hardware interpreter is
often called an emulator. Of course, it is possible to build interpreters that use a combination
of software and hardware techniques. A simulator is often used to illustrate the working of
a virtual machine. It is also used to allow programs compiled for one machine to execute
on another machine. For instance, a simulator can execute programs written for an older

1.8. Virtual Machines: The Abstraction Tower

33

machine on a newer machine.

Advantages of using interpretation include (i) the ability to execute the (source) program
on di(cid:11)erent platforms, without additional compilation steps, and (ii) the ease of carrying
out interactive debugging. The main disadvantage is performance.

1.8.1 Problem De(cid:12)nition and Modeling Level Architecture

\At the highest level, the description is greatly chunked, and takes on a completely
di(cid:11)erent feel, despite the fact that many of the same concepts appear on the lowest
and highest levels."
| Douglas R. Hofstadter, in G(cid:127)odel, Escher, Bach: An Eternal Golden Band

The highest abstraction level that we can think of is the level at which problem de(cid:12)nition
and modeling are done. At this level, we can view the computer system as a machine that can
solve well-de(cid:12)ned computer problems. We loosely de(cid:12)ne a well-de(cid:12)ned computer problem
as one that can be represented and manipulated inside a computer.

The problem modeling person, therefore, takes complex real-life problems and precisely
formulates them so as to be solved on the computer. This process involves representing
the real-life problem’s data by some form of data that can be manipulated by a computer.
This process is called abstraction or modeling| creating the right model for the problem
so as to make it possible to eventually develop an appropriate algorithm to solve it. Notice
that in this context, modeling often implies simpli(cid:12)cation, the replacement of a complex
and detailed real-world situation by a comprehendable model within which we can solve a
problem. That is, the model captures the essence of the problem, \abstracting away" the
details whose e(cid:11)ect on the problem’s solution is nil or minimal.

Almost any branch of mathematics or science may be utilized in the modeling process.
Problems that are numerical in nature are typically modeled by mathematical concepts
such as simultaneous linear equations (e.g., (cid:12)nding currents in electrical circuits, or (cid:12)nding
stresses in frames made of connected beams) and di(cid:11)erential equations (e.g., predicting
population growth, or predicting the rate at which chemicals will react). Several problems
can be modeled as graph theoretical problems. Symbol and text processing problems can
be modeled by character strings and formal grammars. Once a problem is formalized, the
algorithm developer at the lower level can look for solutions in terms of a precise model and
determine if an algorithm already exists to solve that problem. Even if there is no known
solution, knowledge of the model properties might aid in developing a solution algorithm.

1.8.2 Algorithm-Level Architecture

The architecture abstraction below the problem de(cid:12)nition level is the algorithm-level archi-
tecture. At this level, we see the computer system as a machine that is capable of executing

34

Chapter 1.

Introduction

algorithms. An algorithm, as we saw earlier, is a step-by-step procedure that can be carried
out mechanically so as to get a speci(cid:12)c output from a speci(cid:12)c input. A key feature of com-
puter algorithms is that the steps are precisely de(cid:12)ned so as to be executed by a machine.
In other words, it describes a process so unambiguously that it becomes mechanical, in the
sense that it does not require much intelligence, and can be performed by rote or a machine.
Computer scientists also require that an algorithm be (cid:12)nite, meaning that (i) the number
of steps must be (cid:12)nite so that it terminates eventually, and (ii) each step must require only
(cid:12)nite time and computational resources.

The basic actions involved in a computer algorithm are:

(cid:15) Specify data values (using abstract data types)

(cid:15) Perform calculations and assign data values

(cid:15) Test data values and select alternate courses of actions including repetitions

(cid:15) Terminate the algorithm

The algorithm-level architecture supports abstract data types and abstract data struc-
tures; the algorithm designer formulates suitable abstract data structures and develops an
algorithm that operates on the data structures so as to solve the problem. Providing ab-
stract data types enables the algorithm designer to develop more general algorithms that
can be used for di(cid:11)erent applications involving di(cid:11)erent data types. This is often called
algorithm abstraction. For instance, a sorting algorithm that has been developed without
specifying the data types being sorted, can be programmed to sort a set of integers or a
set of characters. Similarly, when considering a data structure, such as an array, it is often
more productive to ignore certain details, such as the exact bounds of its indices. This is
often called data abstraction.

Algorithm E(cid:14)ciency: Computer theorists are mainly concerned with discovering the
most e(cid:14)cient algorithms for a given class of problems. The algorithm’s e(cid:14)ciency relates its
resource usage, such as execution time or memory consumption, to the size of its input data,
n. The e(cid:14)ciency is stated using the \Big O" notation, O(n). For example, if an algorithm
takes 4n - 2n + 2 steps to solve a problem of size n, we can say that the number of steps is
O(n2 ). Programmers use their knowledge of well-established algorithms and their respective
complexities to choose algorithms that are best suited to the circumstances. Examples of
such algorithms are quick-sort for sorting data (which has an an (n log n) average running
time), and binary search for searching through sorted data (which has an O(log 2n) time).
Algorithms can be speci(cid:12)ed in di(cid:11)erent ways. Two common methods are pseudocode
descriptions and (cid:13)owchart diagrams. A pseudocode description uses English, mathematical
notations, and a limited set of special commands to describe the actions of an algorithm. A
(cid:13)owchart diagram provides the same information graphically, using diagrams with a (cid:12)nite
set of symbols in the place of the more elegant features of the pseudocode. A computer

1.8. Virtual Machines: The Abstraction Tower

35

cannot directly understand either pseudocode or (cid:13)owcharts, and so algorithm descriptions
are translated to computer language programs, most often by human programmers. Thus,
a computer program is an embodiment of an algorithm; strictly speaking, an algorithm is
a mental concept that exists independently of any representation.

1.8.2.1 Computation Models

Another important tenet of an algorithm-level architecture is the computational model
it supports. A computational model conceptualizes computation in a particular way by
specifying the kinds of primitives, relationships, and events that can be described in an
algorithm. A computational model will generally have the following features:

(cid:15) Primitives: They represent the simplest ob jects that can be expressed in the model.
Examples of primitives found in most of the computation models are constants and
variables.

(cid:15) Methods of combination: They specify how the primitives can be combined with
one another to obtain compound expressions.

(cid:15) Methods of abstraction: They specify how compound ob jects can be named and
manipulated as units.

The computational model determines the kind of computations that can be speci(cid:12)ed by
an algorithm. For example, if we consider a geometric computational model that supports
only ruler and compass construction primitives, then we can specify algorithms (rules)
for bisecting a line segment, bisecting an angle, and other similar problems. We cannot,
however, specify an algorithm to trisect an angle. For solving this problem, we require
additional primitives such as a protractor. For arithmetic computation we can use models
incorporating di(cid:11)erent primitives such as an abacus, a slide rule, or even a calculator. With
each of these computation models, the type of arithmetic problems that can be solved is
di(cid:11)erent. The algorithms for solving a speci(cid:12)c problem would also be di(cid:11)erent.

Algorithm development is always done for a speci(cid:12)c algorithm-level architecture having
an underlying computational model. Three basic computational models are currently in
use, some of them being more popular than the others: imperative, functional, and logic.
These models of computation are equivalent in the sense that, in principle, any problem
that has a solution in one model is solvable in every one of the other models also.

Imperative Model: The imperative model of computation is based on the execution
of a sequence of instructions that modify storage called state. The basic concept is the
notion of a machine state (comprising variables in the high-level architecture, or registers
and memory locations in the assembly-level architecture). Program development consists
of specifying a sequence of state changes to arrive at the solution. An imperative program

36

Chapter 1.

Introduction

would therefore consist of a sequence of statements or instructions and side-e(cid:11)ect-prone
functions; the execution of each statement (or instruction) would cause the machine to
change the value of one or more elements of its state, thereby taking the machine to a new
state. A side-e(cid:11)ect-prone function is one whose execution can result in a change in the
machine state. Historically, the imperative model has been the most widely used model;
most computer programmers start their programming career with this computational model.
It is the closest to modeling the computer hardware. This tends to make it the most e(cid:14)cient
model in terms of execution speed. Commonly used programming languages such as C,
C++, FORTRAN, and COBOL are based on this computational model.

Applicative (Functional) Model: The functional model has its foundation in mathe-
matical logic. In this model, computing is based on recursive function theory (RFT), an
alternative (and equivalent) model of e(cid:11)ective computability. As with the Turing machine
model, RFT can express anything that is computable. Two of the prominent computer
scientists who pioneered this computational model are Stephen Kleene and Alonso Church.
The functional model consists of a set of values, functions, and the application of side-e(cid:11)ect-
free functions. A side-e(cid:11)ect-free function is one in which the entire result of computation is
produced by the return value(s) of the function. Side-e(cid:11)ect-free functions can only access
explicit input parameters; there are no global variables in a fully functional model. And, in
the purest functional models, there are no assignment statements either. Functions may be
named and may be composed with other functions. Functions can take other functions as
arguments and return functions as results. Programs consist of de(cid:12)nitions of functions, and
computations are application of functions to values. A classic example of a programming
language that is built on this model is LISP.

Rule-based (Logic) Model: The logic model of computation is a formalization of the
logical reasoning process. It is based on relations and logical inference. An algorithm in
this model involves a collection of rules, in no particular order. Each rule consists of an
enabling condition and an action. The execution order is determined by the order in which
the enabling conditions are satis(cid:12)ed. The logic model is related to relational data bases and
expert systems. A programming language designed with this model in mind is Prolog.

Computational Model Extensions: Apart from the three popular computational mod-
els described above, many other computational models have been proposed. Many exten-
sions have also been proposed to computational models to improve programmer e(cid:14)ciency or
hardware e(cid:14)ciency. Two important such extensions are the ob ject-oriented programming
model and the concurrent programming model.

(cid:15) Ob ject-Oriented Model: In this model, an algorithm consists of a set of ob jects
that compute by exchanging messages. Each ob ject is bound up with a value and
a set of operations that determine the messages to which it can respond. Functions

1.8. Virtual Machines: The Abstraction Tower

37

are thus designed to operate on ob jects. Ob jects are organized hierarchically. That
is, complex ob jects are designed as extensions of simple ob jects; the complex ob ject
will \inherit" the properties of the simple ob ject. The ob ject-oriented model may be
implemented within any of the other computational models. Imperative programming
languages that use the ob ject-oriented approach are C++ and Java.

(cid:15) Concurrent Model: In this model, an algorithm consists of multiple processes or
tasks that may exchange information. The computations may occur concurrently or
in any order. The model is primarily concerned with methods for synchronization and
communication between processes. The concurrent model may also be implemented
within any of the other computational models. Concurrency in the imperative model
can be viewed as a generalization of control. Concurrency is particularly attractive
within the functional and logic models, as subexpression evaluation and inferences
may then be performed concurrently. Hardware description languages (HDLs) such
as Verilog and VHDL use the concurrency model, as they model hardware components,
which tend to operate concurrently.

1.8.3 High-Level Architecture

The abstraction level below the algorithm-level architecture is the high-level architecture.
This is the highest level that we study in this book, and is de(cid:12)ned by di(cid:11)erent high-level
languages, such as C, C++, FORTRAN, Java, LISP, Prolog, and Visual Basic. This level
is used by application programmers and systems programmers who take algorithms and
formally express them in a high-level language. HLL programmers who develop their own
algorithms often perform both these steps concurrently. That is, the algorithm development
is done side by side with HLL program development.

To the HLL programmer the computer is a machine that can directly accept programs
written in a high-level language that uses alphabets as well as symbols like +, (cid:0), etc. It
is de(cid:12)nitely possible to construct a computer hardware that directly executes a high-level
language; several LISP machines were developed in the 1970s and 1980s by di(cid:11)erent vendors
to directly execute LISP programs. Directly running high-level programs on hardware is not
commonplace, however, as the hardware can run only programs written in one speci(cid:12)c high-
level language. More commonly, programs written in high-level languages are translated to
a lower level by translators known as compilers. We shall see more details of the high-level
architecture in Chapter 2.

For a computer to solve a problem, the algorithm must be expressed in an unambiguous
manner, so that computers can faithfully follow it. This implies espressing the algorithm
as a program as per the syntax and semantics of a programming language.

38

Chapter 1.

Introduction

1.8.4 Assembly-Level Architecture

The next lower level, called the assembly-level architecture, implements the high-level ar-
chitecture. The architecture at this level has a notion of storage locations such as registers
and memory. Its instructions are also more primitive than HLL statements. An instruction
may, for instance, add two registers, move data from one memory location to another, or
determine if a data value is greater than zero. Primitive instructions such as these are
su(cid:14)cient to implement high-level language programs. The language used to write programs
at this level is called an assembly language. In reality, an assembly language is a symbolic
form for the language used in the immediately lower level, namely the instruction set ar-
chitecture. Often, the assembly-level architecture also includes some instructions that are
not present in the instruction set architecture.

The assembly-level architecture and the instruction set architecture are usually hybrid
levels in that each of these architectures typically includes at least two modes|the User
mode and the Kernel mode. Both modes have many common instructions; however, each
mode also has a few instructions of its own. The extra instructions in the Kernel mode
include, for instance, those for reading or writing to IO addresses, managing memory al-
location, and creating multiple processes. The extra instructions in the User mode are
called system cal l instructions. In the microarchitecture, these instructions are interpreted
by executing an interpreter program in the Kernel mode at the ISA level. This interpreter
program is called the operating system kernel. Notice that the operating system itself may
have been originally written in a high-level language, and later translated to the lower
levels. The instructions that are common to both modes are interpreted directly by the
microarchitecture, and not by the OS. Thus, the system call instructions of the User mode
are interpreted by the OS and the rest are interpreted directly by the microarchitecture.

1.8.5

Instruction Set Architecture (ISA)

The next lower level is called instruction set architecture (ISA). The language used to
specify programs at this level is called a machine language. The memory model, IO model,
and register model in the ISA are virtually identical to the ones in the assembly-level
architecture. However, when specifying register and memory addresses in machine language,
they are speci(cid:12)ed in binary encoding. The instructions in the ISA are also mainly binary
encodings of the instructions present in the assembly-level architecture. There may be a
few minor di(cid:11)erences in the instruction set. Usually, the assembly-level architecture has
more instructions than what is available in the ISA. Moreover, most assembly languages
permit programmers to de(cid:12)ne their own macros. These enhancements make it much easier
to program in an assembly language, compared to a machine language. Programs written in
an assembly language are translated to machine language using a program called assembler.

The instruction set architecture is sometimes loosely called architecture. Di(cid:11)erent ISAs

1.8. Virtual Machines: The Abstraction Tower

39

di(cid:11)er in the number of operations, data types, and addressing modes they specify. ISAs that
include fewer operations and addressing modes are often called RISC (Reduced Instruction
Set Computer) ISAs. Those with a large repertoire of operations and addressing modes are
often called CISC (Complex Instruction Set Computer) ISAs. The most commonly found
ISA is the IA-32 ISA|more often known by its colloquial name, x86|introduced by Intel
Corporation in 1979. Other ISAs that are in use today are IA-64, MIPS, Alpha, PowerPC,
SPARC, and PA-RISC.

1.8.6 Microarchitecture

The microarchitecture is the abstraction level immediately below the ISA; it serves as a
platform for interpreting machine language instructions. A microarchitecture speci(cid:12)cation
includes the resources and techniques used to realize the ISA speci(cid:12)cation, along with the
way the resources are organized to realize the intended cost and performance goals. At this
level the viewer sees hardware ob jects such as instruction fetch unit, register (cid:12)les, ALUs,
latches, cache memory, memory systems, IO interfaces, and interconnections. A register (cid:12)le
is a collection of registers from which a single register can be read or written by specifying
a register number. An ALU (Arithmetic Logic Unit) is a combinational logic circuit that is
capable of performing simple arithmetic and logical operations that are speci(cid:12)ed in machine
language instructions. The register (cid:12)le, the ALU, and the other components are connected
together using bus-type or point-to-point interconnections to form a data path. The basic
operation of the data path consists of fetching an instruction from main memory, decoding
its bit pattern to determine what it speci(cid:12)es, and to carry out its execution by fetching the
required operands, using the ALU to operate on the operand values, and storing back the
result in the speci(cid:12)ed register or memory location.

The actual interpretation of the machine language instructions is done by a control unit,
which controls and coordinates activities of the data path. It issues commands to the data
path to fetch, decode, and execute machine language instructions one by one. There is a
fundamental break at the instruction set architecture. Whereas the architectures above it
are usually implemented by translation, the ISA and the architectures below it are always
implemented by interpretation.

1.8.7 Logic-Level Architecture

Descending one level lower into the hardware, we get to the logic-level architecture. This
architecture is an abstraction of the electronic circuitry of a computer, and refers to the
actual digital logic and circuit designs used to realize the computer microarchitecture. The
designer of this architecture uses gates, which accept one or more digital inputs and produce
as output some logical function of the inputs. Several gates can be connected to form a mul-
tiplexer, decoder, PLA, or other combinational logic circuits such as an adder. The outputs
of an adder, subtractor, and other functional units can be passed through a multiplexer

40

Chapter 1.

Introduction

to obtain an ALU. Similarly, a few gates can be connected together with some feedback
arrangement to form a (cid:13)ip-(cid:13)op or 1-bit memory, which can be used to store a 0 or a 1.
Several (cid:13)ip-(cid:13)ops can be organized to form a register, and several registers can be organized
to form a register (cid:12)le. Memory systems are built in a similar manner, but on a much larger
scale. Thus, we use a hierarchical approach for implementing the individual blocks of the
microarchitecture in the logic-level architecture. We will examine gates and the logic-level
architecture in detail in Chapter 9 of the book. This is the lowest architecture abstraction
that we will study in detail in this book.

Synchronous vs Asynchronous (self-timed): Currently digital computers are typi-
cally designed as synchronous or clocked sequential circuits, meaning that they use clock
signals to co-ordinate and synchronize the di(cid:11)erent activities in the computer. Changes in
the machine state occur only at discrete times that are co-ordinated by the clock signals.
Thus, the basic speed of a computer is determined by the time of one clock period. The
clock speed of the processor used in the original IBM PC was 4.77 MHz. The clock speeds
in current state-of-the-art computers range from 1 to 3.8 GHz. If all other speci(cid:12)cations
are identical, higher clock speeds mean faster processing. An alternative approach is to use
asynchronous or self-timed sequential circuits.

1.8.8 Device-Level Architecture

For the sake of completeness, we mention the existence of a machine level below the logic-
level architecture, called device-level architecture. The primitive ob jects at this level are
devices and wires. The prevalent devices in today’s technologies are transistors. The de-
signer of this architecture uses individual transistors and wires to implement the digital
logic circuits speci(cid:12)ed in the logic-level architecture. The designer also speci(cid:12)es how the
transistors and wires should be laid out. With today’s technology, millions and millions of
transistors can be integrated in a single chip; such a design is called VLSI (Very Large Scale
Integration). Accordingly, device-level architecture is also called VLSI architecture.

One possible way of designing a device-level architecture involves taking the logic-level
architecture and implementing it as follows: connect a few transistors together to form
device-level circuitry that implement logic gates such as inverter, AND, OR, NAND, and
NOR. Then, implement each logic gate in the logic-level architecture by the equivalent
device-level cicuitry. In current practice, a di(cid:11)erent approach is taken. Instead of attempting
to implement the logic-level architecture, the VLSI designer takes the microarchitecture, and
implements the functional blocks in the microarchitecture with device-level circuitry. By
bypassing the logic-level architecture, this approach leads to a more e(cid:14)cient design.

Di(cid:11)erent types of transistor devices are available: BJT (Bipolar Junction Transistor),
MOSFET (Metal Oxide Semiconductor Field E(cid:11)ect Transistor), etc. Digital computer
applications invariably use MOSFETs. Again, di(cid:11)erent design styles are available with
MOSFETs. The most prevalent style is the CMOS (Complementary Metal Oxide Semicon-

1.9. Concluding Remarks

41

ductor) approach, which uses a PMOS network and a complementary NMOS network.

If we want to study the design of transistors, that leads us into solid-state physics, which
deals with low-level issues such as electrons, holes, tunneling, and quantum e(cid:11)ects. At this
low abstraction level the machine looks more analog than digital! This level is clearly
outside the scope of this book.

1.9 Concluding Remarks

We have barely scratched the surface of computing, but we have laid a solid foundation
for computing and computers. This chapter began with the general topic of computing
and the role of computers in computing applications. Perhaps now we can answer the
question: what exactly is a computer? To a large extent, the answer depends on the level
at which we view the computer. At the high-level view, it is a machine that accepts high-
level language programs, and directly executes them. At the logic-level view, computers are
digital electronic circuits consisting of di(cid:11)erent types of gates that process 0s and 1s. Viewed
in the above light, we arrive at a de(cid:12)nition of computer architecture as being concerned
with the design and application of a series of virtual machines, starting from high-level
architecture to logic-level architecture.

Besides the abstractions described in the previous section, a computer can have addi-
tional abstractions, such as user interfaces and data communication facilities. The key thing
to remember is that computers are generally designed as a series of architectural abstrac-
tions, each one implementing the one immediately above it. Each architecture represents
a distinct abstraction, with its own unique ob jects and operations. By focusing on one
architecture at a time, we are able to suppress irrelevant details, thereby making it easier
to master this complex sub ject. All of the architectures are important for mastery of the
sub ject; this textbook studies four architectures in detail: the assembly-level architecture,
the instruction set architecture, the microarchitecture, and the logic-level architecture. A
synthesis of these four architectures will give a depth and richness of understanding that
will serve well, irrespective of whether your main interest is in computer science, computer
engineering, or electrical engineering.

1.10 Exercises

1. Explain the role played by the operating system in a computer.

2. What is meant by a virtual machine in the context of computers?

3. Explain what is meant by the stored program concept.

Chapter 1.

Introduction

42

Problem−Level Architecture

Algorithm−Level Architecture

Problem

Algorithm Developer

Algorithm

HLL Programmer

High−Level Architecture
main() {
int a, b;
Application Program (HLL)
a = read(0, &b, 1);

Library Program (HLL)

OS Program (HLL)

Compiler (Translator)

Assembly−Level Architecture

User Mode

Kernel Mode

main:

LW  R4, 0(R29)
ADD R1, R3, R4
SYSCALL

User Program (AL)

OS Program (AL)

Assembler (Translator)

Machine−Level Architecture
(Instruction Set Architecture)

User Mode

Kernel Mode

Designed by Instruction Set Architect

User Program (ML)

Interpretation
by OS code

OS Program (ML)

Control Unit (Interpretor)

Microarchitecture

PC     MAR

Microarchitectural       Data Path
(RFs, Caches,      ALUs, Buses)

Designed by Microarchitect

Microinstruction

Microsequencer (Interpretor)

Logic −Level Architecture
PC_out, MAR_in

Logic−Level         Data Path
(Gates, Flip−flops, MUXes, ROMs)

Designed by Logic Designer

Control Signals

Device Control Inputs (Implementor)

Device−Level Architecture

Device −Level            Data Path
(Transistors, Wires, Layouts)

Designed by VLSI Designer

Figure 1.10: Machine Abstractions relevant to Program Development and Execution, along
with the Ma jor Components of each Abstract Machine.

Part I

PROGRAM DEVELOPMENT |
SOFTWARE LEVELS

Finish your outdoor work and get your (cid:12)elds ready; after that, build your house.

Proverbs 24: 27

This part deals with the software levels of computers.
In particular, it discusses the
high-level language (HLL)-level architecture, the assembly-level architecture, and the in-
struction set architecture (ISA). Chapter 2 gives a brief overview of program development.
This discussion is focused primarily on the high-level architecture. The detailed discus-
sion of computer architecture begins in this chapter with background information on the
high-level architecture, which is usually covered in a pre-requisite course to computer
architecture. This material is included in the book for completeness and to highlight
some of the software issues that are especially critical to the design of computer systems.
Chapter 3 provides a detailed treatment of the assembly-level architecture. In particular,
it describes the memory model, the register model, instruction types, and data types. It
also discusses programming at this level. Chapter 4 covers the Kernel mode, and di(cid:11)er-
ent ways of carrying out IO operations. Chapter 5 discusses ISA. It covers instruction
encoding, data encoding, translation from assembly language to machine language, and
di(cid:11)erent approaches to instruction set design.

44

Chapter 2

Program Development Basics

Let the wise listen and add to their learning, and let the discerning get guidance

Proverbs 1: 5

Software development is a fundamental aspect in computing; without software, comput-
ing would be limited to a few (cid:12)xed algorithms that have been hardwired into the hardware
system. The phenominal power of computers is due to their ability to execute di(cid:11)erent
programs at di(cid:11)erent times or even concurrently. Most of today’s program development
| programming | is done in one of the high-level languages. Therefore, much of the
discussion in this chapter is focused on high-level languages. These languages languages
abstract away the hardware details, making it possible to develop portable programs, i.e.,
programs that are not tied to any speci(cid:12)c hardware platform and can therefore be made
to execute on di(cid:11)erent hardware platforms. It is this high degree of abstraction that gives
them the name \high-level languages." Programming at a high level allows programmers
not to be concerned with the detailed machine-level speci(cid:12)cations, which in turn improves
their e(cid:14)ciency (if not the e(cid:14)ciency of the code!).

Many high-level languages are popular today: C, C++, Java, FORTRAN, VisualBA-
SIC, etc. Our ob jective in this chapter is not to teach programming; we assume that you
are already familiar with at least one high-level language, and have done some entry-level
programming. Our intent is to review important concepts that are common to program
development | irrespective of the language | and to lay a foundation for the material
presented in the subsequent chapter, which deals with the assembly-level architecture and
translation of programs from high-level languages to assembly languages.
In that vein,
we touch upon basic issues in software engineering as well; however, advanced software
engineering concepts are clearly out of the scope of this book.

45

46

Chapter 2. Program Development Basics

2.1 Overview of Program Development

\Programs should be written for people to read, and only incidentally for machines to
execute."

| Structure and Interpretation of Computer Programs by Harold Abelson and Gerald
Jay Sussman

There is no single way to develop computer programs; programmers di(cid:11)er quite a bit
when it comes to how they develop programs. A software engineering approach to pro-
gramming .................. We shall start with an overview of the important aspects in program
development. Below we highlight these aspects.

(cid:15) Problem modeling: The model is created by understanding the complete problem
to be solved, and making a formal representation of the system being designed.

(cid:15) Algorithm development: Once a formal model of the problem is developed, the
next step is to develop an appropriate algorithm for solving the problem. Algorithm
development involves de(cid:12)ning the following:

{ Data structures: the format and type of data the program will represent and
manipulate.

{ Inputs: the kind of data the program will accept.

{ Outputs: the kind of data the program will output.

{ User interface: the design of the screen the end user will see and use to enter
and view data.

{ Algorithm: the methods of manipulating the inputs and determining the outputs.

Algorithm development includes a lion’s share of the problem solving e(cid:11)ort. It is a
creative process, and has not yet been automated! The reason for this, of course, is
that for automating something, an algorithm has to be developed for performing it,
which means that we would require an algorithm for writing algorithms! Therein lies
the di(cid:14)culty. An array of guidelines have been developed, however, to make it easier
for an algorithm developer to come up with an algorithm for a new problem. Some
of these guidelines are given below:

{ See if any standard techniques (or \tricks") can be used to solve the problem.

{ See if the problem is a slight variation of a problem for which an algorithm has
already been developed. If so, try to adapt that algorithm.

{ Divide-and-conquer approach: See if the problem can be broken into subprob-
lems.

{ Develop a simpli(cid:12)ed version of the problem, and develop an algorithm for the
simpli(cid:12)ed problem. Then adapt the algorithm to (cid:12)t the original problem.

2.1. Overview of Program Development

47

A problem can often be solved by more than one functionally correct algorithm.
Choosing between two algorithms often depends on the requirements of a particular
application. For instance, one algorithm may be good for small data sets, whereas
the other may be good for large data sets.

(cid:15) Programming:

(cid:15) Debugging: Virtually all programs have defects in them called bugs, and these need
to be eliminated. Bugs can arise from errors in the logic of the program speci(cid:12)cation
or errors in the programming code created by a programmer. Special programming
tools assist the programmer in (cid:12)nding and correcting bugs. Some bugs are di(cid:14)cult
to locate and (cid:12)xing them is like solving a complex puzzle.

(cid:15) Testing: Alpha and beta testing. Alpha testing is a small scale trial of the program.
The application is given to a few expert users to assess whether it is going to meet their
needs and that the user interface is suitable. Bugs and missing features due to the
application being un(cid:12)nished will be found. Any errors in the code and speci(cid:12)cation
will be corrected at this stage. Beta testing is a more wide-ranging trial where the
application is given to a selection of users with di(cid:11)erent levels of experience. This
is where the bulk of the remaining bugs are found; some may remain undetected or
un(cid:12)xed.

(cid:15) Software delivery: The completed software is packaged with full documentation
and delivered to the end users. When they use the software, bugs that were not found
during testing may appear. As these new bugs are reported an updated version of the
software with the reported bugs corrected is shipped as a replacement.

Program development, as we saw in Chapter 1, .........

2.1.1 Programming Languages

Programming languages are the vehicle we use to express the tasks a computer must per-
form. It serves as a framework within which we organize our ideas about computer processes.

What makes a programming language powerful? A powerful programming language is
more than just a means for instructing a computer to perform various tasks. The power
of the language depends on the means it provides for combining simple ideas to form more
complex ideas. Every powerful language has three mechanisms for accomplishing this:

* primitive expressions, which represent the simplest entities the language is concerned
with,

* means of combination, by which compound elements are built from simpler ones, and

* means of abstraction, by which compound elements can be named and manipulated
as units.

48

Chapter 2. Program Development Basics

In programming, we deal with two kinds of primitives: instructions and data. Informally,
data is \stu(cid:11) " that we want to manipulate, and instructions are descriptions of the rules
for manipulating the data. Thus, any powerful programming language should be able to
describe primitive data and primitive instructions, and should have methods for combining
and abstracting instructions and data.

A computer program is nothing but an algorithm expressed in the syntax of a program-
ming language. For executing an algorithm, it is imperative that it be (cid:12)rst expressed in a
formal language. The familiar hello, world! program given below, when executed on a
computer, will display the words \hello, world!" on the display. This program uses the
syntax and semantics of the C programming language.

Program 1 The Hello World! program in C.

main() {
// Display the string
printf("hello, world!");

}

The same program, when expressed in a di(cid:11)erent programming language, will have some
di(cid:11)erences, but the underlying algorithm will be the same. For instance, when expressed in
Java, the same algorithm may look as follows:

Program 2 The Hello World! program in Java.

class helloworld {
public static void main(String[] args) {
// Display the string
System.out.println("hello, world!");

}

}

The features supported by a programming language form an important aspect of pro-
gramming, as the programmer expresses the entire algorithm by means of the programming
language chosen. In this section, we discuss features that are common to many high-level
programming languages.

(cid:15) Alphabet: High-level languages generally use a rich alphabet, such as the ones used
in natural languages, along with many of the symbols used in mathematics. Common
languages such as C and Java use the English alphanumerical character set as the
alphabet.

2.1. Overview of Program Development

49

(cid:15) Syntax: Syntax speci(cid:12)es the rules for the structure of programs.
It speci(cid:12)es the
delimiters, keywords, etc, and the possible combinations of symbols. Programming
languages can be textual or graphical. Textual languages use sequences of text includ-
ing words, numbers, and punctuation, much like written natural languages. Graphical
languages use symbols and spatial relationships between symbols to specify programs.
A languages syntax can be formalized by a grammar or syntax chart.

(cid:15) Semantics: While the syntax of a language refers to the appearance of programs
written in that language, semantics refers to the meanings of those programs. The
semantics of a language specify the meaning of di(cid:11)erent constructs in the language,
and therefore the behavior of the program when executed. The semantics of a lan-
guage draw upon linguistics and mathematical logic, and have a connection with the
computational model(s) supported by the language.

(cid:15) Data types and data abstraction: All programming languages provide a set of
basic data types such as integers, (cid:13)oating-point numbers, and characters. A data
type speci(cid:12)es the set of values a variable of that type can have. It also de(cid:12)nes how
operations such as + and (cid:0) will be carried out on variables of that type. In Pascal, for
instance, the expression i + j indicates integer addition if i and j are de(cid:12)ned to be
integers, and (cid:13)oating-point addition if they are de(cid:12)ned to be reals. Type checking
is supported by Pascal to ensure that such operations are applied to data of the same
type; more weakly typed languages such as C relax this restriction somewhat. Ob jects
are organized hierarchically

Most of the programming languages allow the programmer to de(cid:12)ne complex data
types out of simpler ones. Examples are the record data type in Pascal and the
struct data type in C. Ob ject-oriented languages such as C++ and Java extend
this concept further, by allowing the programmer to de(cid:12)ne a set of operations for
each of the newly de(cid:12)ned data type. The data type, along with the associated set of
operations, is called an object. Program statements are only allowed to manipulate
data ob jects according to the operations de(cid:12)ned for that ob ject.

Finally, most modern programming languages allow the programmer to de(cid:12)ne abstract
data types, thereby creating an extended language. An abstract data type is a data
type that is de(cid:12)ned in terms of the operations that it supports and not in terms
of its structure or implementation. In the context of programming languages, data
abstraction means hiding the details concerning the implementation of data values
in computers. Data abstraction thus makes it possible to have a clear separation
between the properties of a data type (which are visible to the user interface) and its
implementation details. Thus, abstraction forms the basic platform for the creation
of user-de(cid:12)ned ob jects.

If a programming language does not directly support data abstraction, the program-
mer may explicitly design and use abstract data types, by using appropriate coding.

(cid:15) Control abstraction:

50

Chapter 2. Program Development Basics

(cid:15) Library API:

(cid:15) OS API:

In the discussion that follows, we will provide example code snippets in both C and
Java. We selected these two languages because of their popularity. The reader who is not
familiar with any of these languages should not be distracted by this choice; the syntax
and semantics of these languages are easy to understand and are similar to those of other
high-level languages.
In any case, we will restrict our discussion to simple constructs in
these languages.

2.1.2 Application Programming Interface Provided by Library

2.1.3 Application Programming Interface Provided by OS

2.1.4 Compilation

2.1.5 Debugging

\If builders built houses the way programmers built programs, the (cid:12)rst woodpecker
to come along would destroy civilization."
| Gerald Weinberg

\Do not look where you fell, but where you slipped." African proverb

2.2 Programming Language Speci(cid:12)cation

2.2.1 Syntax

The syntax of a language refers to ....
It a(cid:11)ects the readability of the program. It also
impacts the ease with which a compiler can parse the program.

2.2.2 Semantics

2.3 Data Abstraction

Declaration and manipulation of data values is at the heart of computer algorithms. The
data types and structures used by algorithms are somewhat abstract in nature. A ma jor part
of the programming job involves implementing these abstractions using the more primitive
data types and features provided by the programming language. All programming languages
provide several primitive data types, and means to combine primitive data types into data
structures. Let us look at these primitive data types.

2.3. Data Abstraction

2.3.1 Constants

51

We shall start our discussion of data types with constants. Constants are ob jects whose
values do not change during program execution. Many calculations in real-world problems
involve the use of constants. Although a constant can be represented by declaring a variable
and initializing it to the appropriate value, this may not be the most e(cid:14)cient way from the
execution point of view. Most assembly languages do not treat constant and variable data in
the same manner. Assembly languages support a special immediate addressing mode that
lets a constant value to be directly speci(cid:12)ed as part of an instruction rather than storing that
constant’s value in a memory location and accessing it as a variable. By understanding how
constants are represented at the assembly language and machine language levels, constants
may be appropriately presented in the HLL source code to produce smaller and faster
executable programs.

2.3.1.1 Literal Constants and Program E(cid:14)ciency

High-level programming languages and most modern assembly languages allow you to spec-
ify constant values just about anywhere you can legally read the value of a memory variable.

2.3.1.2 Manifest Constants

A manifest constant is a constant value associated with a symbolic name. During program
translation, the translator will directly substitute the value everywhere the name appears
within the source code. Manifest constants allow programmers to attach meaningful names
to constant values so as to create easy-to-read and easily maintained programs.

2.3.1.3 Read-Only Memory Ob jects

C++ programmers may have noticed that the previous section did not discuss the use of
C++ const declarations. This is because symbols you declare in a C++ const statement
aren’t necessarily manifest constants. That is, C++ does not always substitute the value
for a symbol wherever it appears in a source (cid:12)le. Instead, C++ compilers may store that
const value in memory and then reference the const ob ject as it would a static variable.
The only di(cid:11)erence, then, between that const ob ject and a static variable is that the C++
compiler doesn’t allow you to assign a value to the const ob ject at runtime.

C++ sometimes treats constants you declare in const statements as read-only variables
for a very good reasonit allows you to create local constants within a function that can
actually have a di(cid:11)erent value each time the function executes (although while the function
is executing, the value remains (cid:12)xed). Therefore, you cannot always use such "constants"
within constant expressions in C++ and expect the C++ compiler to precompute the
expression’s value.

52

Chapter 2. Program Development Basics

2.3.1.4 Enumerated Types

Well-written programs often use a set of names to represent real-world quantities that
don’t have an explicit numeric representation. An example of such a set of names might be
various car manufacturers, such as GM, Ford, and Chrysler. Even though the real world
does not associate numeric values with these manufacturers, they must be must encoded
with numerical values if they are to be represented in a computer system. (Of course, it
is possible to represent them as \text" by representing each character in the name using
ASCII, but that would slow down program execution.) The internal value associated with
each symbol can be arbitrary; the important point is that the same unique value is used
every time a particular symbol is used. Many programming languages provide a feature
known as the enumerated data type that will automatically associate a unique value with
each name in a list. The use of enumerated data types helps the programmer to specify the
data using meaningful names rather than \magic" numbers such as 0, 1, and 2.

2.3.1.5 Boolean Constants

Many high-level programming languages provide Boolean or logical constants that can rep-
resent the values True and False. Because there are only two possible Boolean values, their
representation requires only a single bit at the machine language. However, because most
machine languages do not permit storage allocation at the granularity of a single bit, most
programming languages use a whole byte or even a larger ob ject to represent a Boolean
value. The behavior of the unused bits in a Boolean ob ject depends on the programming
language. Many languages treat the Boolean data type as an enumerated type.

2.3.2 Variables

Irrespective of the speci(cid:12)c high-level language used, the programmer sees an abstract ma-
chine that supports data structures and operations that can be performed on the data
structures. In most high-level languages, the data structures are declared a certain type.
The type indicates both the characteristics of ob jects that can be represented and the kinds
of operations that can be performed on the ob jects.

As mentioned earlier, manipulation of data values is at the heart of every computer pro-
gram. It is therefore of utmost importance that high-level languages provide programmers
an e(cid:14)cient and easy way of specifying data values that can be modi(cid:12)ed. Most HLLs allow
the programmer to refer to a data value symbolical ly by a name. Variables are used for a
variety of data values including input values, output values, loop counts, and intermediate
results of computation. Consider a simple program|one that counts the number of words
in a text (cid:12)le. This program would need to know the name of the (cid:12)le|information the
program end user would need to supply. The program would need a variable to keep track
of the number of words counted so far.

2.3. Data Abstraction

53

Declaring and manipulating variables is a central concept in HLL programming. The
HLL variables have some similarity to the variables used in algebra and other branches
of mathematics, although there are a few notable di(cid:11)erences.
In addition to specifying
the name of a variable, a variable declaration includes specifying the type, scope, and
storage class of the variable. The position of a variable declaration statement in a program
implicitly speci(cid:12)es the scope of the variable. The declaration is for the bene(cid:12)t of the
compiler, which must know how much space to allocate for each variable. Di(cid:11)erent variable
types require di(cid:11)erent amounts of space. For example, C permits di(cid:11)erent variable types
such as integers, characters, and (cid:13)oats. The declaration of a variable is accomplished by
specifying its name and type. For example, in C the declaration

int n;

declares an integer variable named n.

Most high-level languages allow a variable to be initialized at the time of declaration.
In C, the declaration

int n = 5;

declares an integer variable named n, and calls for initializing its value to 5. Once a variable
has been assigned a value, it retains that value until it is modi(cid:12)ed, either by a direct
assignment or an indirect assignment through a pointer.

2.3.2.1 Data Type

The data type of a variable de(cid:12)nes the set of values that the variable may ever assume, and
the semantics of possible arithmetic/logical operations on those values. In other words, a
variable type is a formally speci(cid:12)ed set of values and operations. For example, a variable
of type boolean (or logical) can assume either the value true or the value false, but no
other value. Logical operations such as fand, or, notg, and the assignment operation can be
performed on it. In addition, the data type indirectly speci(cid:12)es the number of bytes occupied
by the variable, and the methodology to be used for carrying out arithmetic operations on
the variable. Some of the commonly used data types are discussed next.

Signed Integers and Unsigned Integers: These are fundamental data types; virtually
every HLL supports them. Most HLLs support di(cid:11)erent word sizes for integer variables.
For instance, C has short and int variable types for representing 16-bit integers and 32-bit
integers, which can take positive as well as negative values. C also lets the programmer
declare unsigned integer types by adding the pre(cid:12)x unsigned before short or int.

\Good things, when short, are twice as good" Baltasar Gracian, The Art of Worldly
Wisdom

54

Chapter 2. Program Development Basics

double value; /* or your money back! */
short changed; /* so triple your money back! */

| Larry Wal l (the Perl Guy) in cons.c from the Perl source code

Floating Point Numbers (for Increased Range): The range of signed integers rep-
resentable in a 32-bit (cid:12)xed-point format is approximately (cid:0)2:15 (cid:2) 10 9 to 2:15 (cid:2) 109 . Many
computation problems require the ability to represent numbers that are of much greater
or smaller magnitude than the integers in this range. Examples are Avogadro’s number,
6:02 (cid:2) 1023 ; mass of a proton, 1:673 (cid:2) 10(cid:0)24 g; and the US National Debt a few years back,
$17,383,444,386,952.37. In order to represent very large integers and very small fractions,
most high-level languages support (cid:13)oating-point (FP) variable types, in which the e(cid:11)ective
position of the radix point can be changed by adjusting an exponent. The radix point is
said to (cid:13)oat, and the numbers are called (cid:13)oating-point numbers. This distinguishes them
from (cid:12)xed-point numbers, whose radix point is always in the same position. An FP number
is written on paper as follows:
(Sign)Signi(cid:12)cand (cid:2) BaseExponent

The base is the radix of the FP number, the signi(cid:12)cand identi(cid:12)es the signi(cid:12)cant digits of
the FP number, and the sign identi(cid:12)es the overall sign of the FP number. The exponent,
along with the base, determines the scale factor, i.e., the factor by which the signi(cid:12)cand is
multiplied to get the actual value. In C, (cid:13)oating-point variables can be declared as follows:

float f;
double d;

/* single precision floating-point */
/* double precision floating-point */

Character: Textual information has become one of the frequently utilized forms of infor-
mation for both storage and manipulation. This seems counterintuitive, given that comput-
ers have historically been used to \compute," or perform calculations. However, when we
consider the facts that programs are input in text form, that compilers operate on strings
of characters, and that computation answers are generally provided via some type of tex-
tual information, then the amount of textual information processed by computers begins
to be appreciated. Furthermore, the preparation of letters, reports, and other documents
has become a ma jor application of computers. The basic unit of textual information is a
character. Most high-level languages provide variable type(s) to represent character and/or
strings of characters. In C, a character variable can be declared as follows:

char c;

If you lost wealth, you lost nothing
If you lost health, you lost something
If you lost character, you lost everything.

| An old proverb

2.3. Data Abstraction

55

Pointer: The last data type that we will discuss in this section is the pointer. A pointer
is a variable used to hold the (memory) address of another variable. In de(cid:12)ning a pointer,
the high-level language assumes a limited knowledge of the memory model of the lower
level assembly-level architecture that implements it. Only some high-level languages sup-
port pointers. Example are Pascal and C. Pointers are helpful for building complex data
structures such as linked lists and trees. A pointer variable that points to a character can
be declared in C as follows:

char *cptr;

In this declaration, cptr is the pointer variable; the implicit assumption is that whatever
cptr is pointing to should be interpreted as a data item of type char.

Array: This is a data structure, i.e., a collection of variables.

Structure: This is a data structure, i.e., a collection of variables.

2.3.2.2 Scope

Another important piece of information speci(cid:12)ed in a variable declaration is the variable’s
scope, which de(cid:12)nes where and when it is active and available in the program.

2.3.2.3 Static Scoping

With static scoping, a variable always refers to its nearest enclosing binding. Because
matching a variable to its binding only requires analysis of the program text, this type of
scoping is sometimes also called lexical scoping. Static scope allows the programmer to
reason as if variable bindings are carried out by substitution. Static scoping also makes it
much easier to make modular code and reason about it, since its binding structure can be
understood in isolation. Correct implementation of static scope in languages with (cid:12)rst-class
nested functions can be subtle, as it requires each function value to carry with it a record
of the values of the variables that it depends on. When (cid:12)rst-class nested functions are not
used or not available (such as in C), this overhead is of course not incurred. Variable lookup
is always very e(cid:14)cient with static scope, as the location of each value is known at compile
time.

Most high-level languages that have static scoping allow at least the following two scopes
for variables:

(cid:15) global
(cid:15) local

56

Chapter 2. Program Development Basics

A global variable can be accessed throughout the program (that is, by all modules or
functions in the program). Because of this, declaring too many global variables makes it
di(cid:14)cult to debug and track variable values.

A local variable can be accessed only within the block in which it is declared. When
a local variable has the same name as that of a global variable, the global variable is not
visible in the block where the local variable is visible.

In some high-level languages, the scope of a variable is not explicitly declared; instead,
it is implicitly de(cid:12)ned by where exactly the variable is declared.

int i, j;

/* global variables; static storage class */

main()
f

int i, *iptr;
static int s;

/* local variables; automatic storage class */
/* local variable; static storage class */

iptr = (int *)malloc(40); /* dynamic storage class */

g

2.3.2.4 Dynamic Scoping

In dynamic scoping, each identi(cid:12)er has a global stack of bindings.
Introducing a local
variable with name x pushes a binding onto the global x stack (which may have been
empty), which is popped o(cid:11) when the control (cid:13)ow leaves the scope. Evaluating x in any
context always yields the top binding. Note that this cannot be done at compile time
because the binding stack only exists at runtime, which is why this type of scoping is called
dynamic scoping.

ince a section of code can be called from many di(cid:11)erent locations and situations, it can
be di(cid:14)cult to determine at the outset what bindings will apply when a variable is used.
This can be bene(cid:12)cial; application of the principle of least knowledge suggests that code
avoid depending on the reasons for (or circumstances of ) a variable’s value, but simply use
the value according to the variable’s de(cid:12)nition. This narrow interpretation of shared data
can provide a very (cid:13)exible system for adapting the behavior of a function to the current
state (or policy) of the system. However, this bene(cid:12)t relies on careful documentation of all
variables used this way as well as on careful avoidance of assumptions about a variable’s
behavior, and does not provide any mechanism to detect interference between di(cid:11)erent parts
of a program. As such, dynamic scoping can be dangerous and many modern languages do
not use it. Some languages, like Perl and Common Lisp, allow the programmer to choose
lexical or dynamic scoping when (re)de(cid:12)ning a variable. [edit]

Implementation

Dynamic scoping is extremely simple to implement. To (cid:12)nd an identi(cid:12)er’s value, the

2.3. Data Abstraction

57

program traverses the runtime stack, checking each activation record (each function’s stack
frame) for a value for the identi(cid:12)er. This is known as deep binding. An alternate strategy
that is usually more e(cid:14)cient is to maintain a stack of bindings for each identi(cid:12)er; the stack
is modi(cid:12)ed whenever the variable is bound or unbound, and a variable’s value is simply
that of the top binding on the stack. This is called shallow binding. Note that both of
these strategies assume a last-in-(cid:12)rst-out (LIFO) ordering to bindings for any one variable;
in practice all bindings are so ordered. [edit]

Example

int x = 0; int f () return x;

int g () int x = 1; return f();

With static scoping, calling g will return 0 since it has been determined at compile time
that the expression x in any invocation of f will yield the global x binding which is una(cid:11)ected
by the introduction of a local variable of the same name in g.

With dynamic scoping, the binding stack for the x identi(cid:12)er will contain two items when
f is invoked: the global binding to 0, and the binding to 1 introduced in g (which is still
present on the stack since the control (cid:13)ow hasn’t left g yet). Since evaluating the identi(cid:12)er
expression by de(cid:12)nition always yields the top binding, the result is 1.

2.3.2.5 Lifetime

A variable’s lifetime or storage class determines the period during which that variable exists.
Some variables exist brie(cid:13)y, some are repeatedly created and destroyed, and others exist for
the entire program execution. A variable’s storage class determines if the variable loses its
value when the block that contains it has completed execution. Most high-level languages
support the following three storage classes:

(cid:15) automatic
(cid:15) static
(cid:15) dynamic
(cid:15) persistent

Automatic variables begin to exist when control reaches their block, and lose their values
when execution of their block completes. Examples are the local variables declared within
subroutines1 . Static variables, on the other hand, begin to exist when the program starts
running, and continue to retain their values till the termination of the program. Dynamic
variables are implicit variables pointed to by pointer variables, and do not exist when
the program starts running. They are created during the execution of the program, and
continue to exist and retain their values until they are explicitly destroyed by the program.
In C, a dynamic variable is created during program execution by calling the library function

1Some high-level languages permit a local variable to retain its value between invocations of the subroutine
by declaring the local variable as a static variable.

58

Chapter 2. Program Development Basics

malloc() or calloc(), which returns the starting address of the memory block assigned
to the variable. The allotted memory locations are explicitly freed by calling the library
routine free(). Once some memory locations are freed, those locations should no longer
be accessed. Doing so may cause a protection fault or, worse yet, corrupt other data in the
program without indicating an error. The following example code illustrates the creation
and destruction of a dynamic variable using malloc() and free(), respectively.

char *cptr;

/* allocate a dynamic char array having size elements */
cptr = (char *)malloc(size * sizeof(char));
...
free(cptr);

/* free the block of memory pointed by cptr */

A persistent variable is one that keeps its value after program execution, and that has
an initial value before program execution. The most common persistent variables are (cid:12)les.

2.3.3

IO Streams and Files

The variable types that we saw so far manage data that originate within the program. If
a program operates only on internally generated data, then it is geared to solve only a
particular instance of a problem (i.e., solving a problem for a particular set of input values
only), and is not likely to be very useful. Instead, if the program accepts external inputs,
then it can solve di(cid:11)erent instances of a problem. Thus, it is important for programs to
have the ability to accept input data. On a similar note, the moment a program completes
its execution, its variables cease to exist, and the variable values disappear, without leaving
a trace of the computation results. For the purposes of performing input and output,
high-level languages provide data types that deal with IO streams and (cid:12)les.

In C, the stdio library supports IO streams and (cid:12)les. Each program can access a
collection of virtual IO devices (stdin, stdout, and stderr) that may be controlled by
a simple set of library functions. The main reason for including IO routines in a library
is their complexity: rather than force every application programmer to write these com-
plex routines, simple economics suggest including them in the library software. Example
library functions in C that deal with IO streams are getchar(), putchar(), scanf(), and
printf(). getchar() allows application programs to read a single character from standard
input, and putchar() allows application programs to write a single character to standard
output; scanf() and printf() are useful for performing formatted IO operations with
standard input and standard output, respectively 2 .

2The behavior of these and other similar functions is precisely de(cid:12)ned by the ANSI C standard. Standards
have been developed for high-level languages by national and international organizations such as ANSI
(American National Standards Institute), IEEE (Institute of Electrical and Electronic Engineers), and ISO

2.3. Data Abstraction

59

For clari(cid:12)cation, we present below a simple C program to copy standard input to
standard output, one character at a time, using the C library routines getchar() and
putchar().

#include <stdio.h> /* contains definitions such as EOF */

main()
f

int c;

while ((c = getchar()) != EOF)
putchar(c);

g

Although IO streams (standard input and standard output) can be used to supply
external input to programs and to obtain the results of computation, they are cumbersome
to handle large amounts of data. Inputting large amounts of data through a keyboard every
time a program is executed is impractical. Moreover, the results sent to the standard output
do not leave a \permanent" record. For these reasons, most high-level languages provide a
data type called (cid:12)le, which provides a permanent way of storing data. Unlike other data
structures provided by an HLL, (cid:12)les may be present before the execution of a program, and
do not vanish when a program terminates; in other words, they persist.

\The palest ink is better than the best memory."
| Chinese proverb

The HLL application programmer is provided with an abstraction of a uniform space
of named (cid:12)les. Thus, HLL application programmers do not concern themselves with any
speci(cid:12)c IO devices; instead they can rely on a single set of (cid:12)le-manipulation OS routines
for (cid:12)le management (and IO device management in an indirect manner). This is sometimes
referred to as device-independent IO. For example, a character may be printed on a
printer by writing the character to the \printer (cid:12)le".

Application programs generally read (formatted) data from one or more (cid:12)les, process the
data, and then write the result to one or more other (cid:12)les. For example, an accounts payable
program reads a (cid:12)le containing invoices and another containing purchase orders, correlates
the data, and then prints a check and writes to a (cid:12)le to describe the expenditures. A
compiler reads an HLL source program (cid:12)le, translates the program into machine language,
and writes the machine language program into an executable (cid:12)le.

The actions that an HLL program is allowed to perform on a (cid:12)le are restricted in certain
ways. First of all, before accessing a (cid:12)le, it has to opened. Secondly, most of the HLLs permit

(International Standards Organization). Adherence to a standard facilitates the development of portable
programs, which can run in di(cid:11)erent computer systems with little or no change. Portability is particularly
important, given that software is a ma jor investment for many computer users.

60

Chapter 2. Program Development Basics

only sequential access to the data present in a (cid:12)le. After completing all accesses to a (cid:12)le,
the (cid:12)le is closed. The activities of opening, closing, reading from, and writing to (cid:12)les are
done using special function calls, which are also typically implemented as part of the library
routines. Example library functions in C that deal with (cid:12)le accesses are fopen(), fclose(),
fscanf(), and fprintf().

Instead of specifying a (cid:12)le’s name every time it is accessed, C provides a (cid:12)le pointer data
type. Unlike other pointer variables, which point to the starting address of a variable, a (cid:12)le
pointer does not point to the (cid:12)le, but rather to a data structure that contains information
about the (cid:12)le. The following C code illustrates how a (cid:12)le pointer is declared, initialized,
and used.

#include <stdio.h>

main()
f

FILE *fp;
int c;

/* special data type for accessing a file */

fp = fopen("fname", "r"); /* open file "fname" in read mode */
c = getc(fp); /* read next character from file pointed by fp */
fclose(fp);
/* close file pointed by fp */

g

HLLs such as C provide a uniform interface for both IO streams and (cid:12)les. The generic
functions used to interact with the IO streams are similar to those provided for the ma-
nipulation of (cid:12)les. In fact, if sequential access to (cid:12)les is assumed, there is practically no
di(cid:11)erence between a virtual device and a (cid:12)le, and hence virtual devices may be manipulated
using the same library routines as those used to access (cid:12)les.

The library routines can be linked to the application program statically at link time or
dynamically at run time.

2.3.4 Data Structures

2.3.5 Modeling Real-World Data

As we saw in Chapter 1, problem solving using a computer involves executing an algorithm
with appropriate input data. For a digital computer to solve real-world problems, the
real-world data has to be converted to a digital form that can be easily represented and
manipulated inside the computer. To begin with, all of the analog data has to be converted
to digital data, possibly resulting in some loss of precision depending on the number of
bits used to represent the digital data. The digital data itself can be represented in many

2.3. Data Abstraction

61

di(cid:11)erent ways. In fact, one of the most important steps in developing a computer algorithm
is to carefully consider what real-world data the algorithm needs to process and then choose
an appropriate internal representation for that data. For some type of data, an internal
representation is fairly obvious; for other types of data, many alternatives exist.

Depending on the architectural level at which programming is done, the details of data
representation will di(cid:11)er.
In this section, we concern ourselves only with how data is
represented at the algorithm development level, arguably the highest level that we can
think of.

2.3.5.1

Images

Images have become an important type of data these days, especially with the popularity
of the internet.
Images form an important part of many documents and presentations.
Images vary greatly, based on size, color, textures, and shapes of ob jects. Di(cid:11)erent formats
are used to represent images, depending on these characteristics as well as processing and
performance requirements. There are two fundamentally di(cid:11)erent ways the computer stores
and manipulates images.

(cid:15) Vector image | Draw-type: In this approach, an image is viewed as a collection of
lines, shapes, and ob jects. Lines and curves can be easily de(cid:12)ned by mathematical
ob jects called vectors. Geometrically de(cid:12)nable shapes can be easily represented as
mathematical ob jects by a small number of parameters. For example, a line can be
completely speci(cid:12)ed by noting the co-ordinates of its end points. A circle can similarly
be speci(cid:12)ed by noting the co-ordinates of its center and the length of its radius.

A draw-type image, often referred to as a vector or scalable image, contains a set
of ob jects whose characteristics are stored mathematically. Each individual ob ject
within the image retains its own characteristics, such as the co-ordinates of its vertices
(corners), the thickness and colour of its outline, the color of its interior, etc. This
makes it possible to target editing actions at speci(cid:12)c elements of an image. Vector
graphics are resolution independent and can be scaled to any size and printed at any
resolution without losing clarity. Vector graphics are best for type and graphics that
must retain crisp lines when scaled to various sizes. Examples of commonly used
software for producing vector images include: CorelDRAW!, Adobe Illustrator, Aldus
Freehand, Microsoft Draw, and AutoDesk AutoCAD.

(cid:15) Raster image (pixelmap, bitmap) | Paint-type: In this approach, an image is viewed
as a rectangular grid of tiny squares called pixels. Each pixel has a speci(cid:12)c location
and color value. When viewed in this manner, the viewer does not \see" ob jects or
shapes! As a result, a raster image can lose detail and appear jagged if viewed at
a high magni(cid:12)cation or printed at too low a resolution. Raster images are best for
representing subtle gradations of shades and color such as in photographs. Raster
images can either be created entirely by computer, or can be sampled initially from

62

Chapter 2. Program Development Basics

other sources, for example, scanning a photograph or capturing a frame of video.
When displaying on computer monitors, all images | vector images and raster images
| are displayed as pixels on-screen.

Image Formats

Several di(cid:11)erent formats are used to represent images. Some of these are lossless in that
they retain all of the information that has been captured about an image. Others are lossy
and approximate some of the information so as to reduce the amount of data required to
store and manipulate the image.

Bitmap (BMP):
It is the Microsoft Windows standard for image (cid:12)les. The bitmap
format stores the image in uncompressed form, and so it is lossless. As the image is not
compressed, it renders good images; however, the data (cid:12)le is likely to be quite large. What
you store is pretty much what you see! Although they do have the ability to support up to
16.7 million colors, there really is no reason why the average user should need to use this
format for image manipulation. Moreover, the bitmap format does not support animation,
transparency, and interlacing.

Graphics Interchange Format (GIF): The GIF is a lossless image format that is the
most common format found on the web. Due to having a 256 maximum color range this
format is ideal for making small icons, buttons, or other graphics that have large blocks of
the same color, but not for images that are required to be photographic quality. Therefore,
if you are working with images from a digital camera, do not use GIF as the (cid:12)le format.
This (cid:12)le format supports transparency, interlace, and can be animated, which makes it a
excellent format for putting images on a website. To reduce the data size, GIF does use a
non-lossy compression algorithm. This means that the compressed image can be converted
back to the original image with no loss of detail. The algorithm works by noting sections of
the image that are the same color, and works quite well for images that have large areas of
solid color with little to no texture. The GIF format is an excellent choice for images that
are cartoonlike in appearance such as banner ads, logos, and text on a solid background.
It is a poor choice for real-life depictions such as photographs of nature or people which
tend to have lots of detailed variations. CompuServe Graphics Interlaced Format. Designed
for transmission over modem links, GIF (cid:12)les are compact images that can be stored in an
interlaced format. This means that one line of pixels in every four will be decoded (cid:12)rst,
allowing the user to see what the image will look like before the whole (cid:12)le is downloaded.
It has a maximum of 8-bit (256 colours). This type of image is used on the World Wide
Web.

Joint Photographic Experts Group (JPEG): The JPEG format was developed by
the Joint Photographic Experts Group committee. It is a lossy (cid:12)le format that was designed

2.3. Data Abstraction

63

with photographic images in mind. A JPEG is capable of storing millions of colors, making
it a great format for saving digital camera photographs and capturing the proper hues and
color that we see in real life. JPEG does support compression, but the more you compress
this type of image, the more loss of detail will occur. The predominant uses for JPEGs are
for photographic images on websites and for storing pictures from digital cameras. Though
the TIFF format is a higher quality format, the size of the resulting TIFF image, makes the
JPEG the more practical choice. This format does not support does not support animation
or transparency, but can be interlaced. It is a lossy compression meaning the compressed
image will not render in as much detail as the original from which it was created. The
JPEG format is an excellent choice for photograpic images which depict the real world such
as nature or people and also for complex backgrounds with lots of texture and detailed
variation.

MPEG: An MPEG (cid:12)le uses a complex algorithm like a JPEG (cid:12)le does { it tries to elim-
inate repetition between frames to signi(cid:12)cantly compress video information. In addition,
it allows a soundtrack (which animated GIFs do not). Because a typical sequence has
hundreds or thousands of frames, (cid:12)le sizes can still get quite large.

Shockwave: Shockwave provides a vector-based animation capability. Instead of speci-
fying the color of every pixel, a Shockwave (cid:12)le speci(cid:12)es the coordinates of shapes (ob jects
like lines, rectangles, circles, etc.) as well as the color of each shape. Shockwave (cid:12)les can be
extremely small. They allow animation and sound. The images are also scalable; because
they are vector-based, the image can be enlarged and it will still look great.

Tagged Image File Format (TIFF): The TIFF format is a lossless image format that
is the considered the best choice for photographic image quality. Most digital cameras give
the option of using TIFF as the format it saves (cid:12)les in. The main problem with this (cid:12)le
format is that most applications do not compress the TIFF (cid:12)les, and so they can be quite
large. This is not much of a problem for storing the pictures on a computer, but with
limited (cid:13)ash memory sizes for cameras it could limit the amount of pictures that can be
stored on one card. If you have the storage, then the TIFF format is highly recommended,
but if you do not have the space, then go with a JPEG as you most likely will not notice
a di(cid:11)erence in image quality. This format does not support animation, transparency, and
can not be interlaced.

Encapsulated PostScript (EPS): Adobe PostScript is the industry standard page de-
scription language. EPS (cid:12)les can contain both paint and draw type information, and can
often become extremely large.

Name Extension Compressed Loss Animated Max Colors Transparency Interlaced

64

Chapter 2. Program Development Basics

Graphics Interchange Format .GIF Yes Lossless Yes 256 Yes Yes Joint Photographic
Experts Group .JPG Yes Lossy No 16.7 Million No Yes Portable Network Graphics .PNG
Yes Lossless Yes 256/16.7 Mil Yes Yes Bit-Map .BMP Rarely Lossless No 16.7 Million No
No Tagged Image File Format .TIFF or .TIF Yes Lossless No 16.7 Million No No

Figure 4: Graphic Formats and their Attributes

2.3.5.2 Video

Video data is a natural extension of graphical data. It is a sequence of still images that
depict how the scene changes with the passage of time. If these still images are displayed
at the rate of 30 or more images per second, then, to the human eye, they appear as a
continuous motion picture.

2.3.5.3 Audio

Another real-world entity that we often represent and process inside a computer is audio.
Although real-world audio is analog in nature, inside the computer it is stored in digital form,
called digital audio. Digitization is done by electronically sampling the analog waveform at
regular time intervals; the time intervals should be small enough to capture every nuance in
the analog signal. Each sample is then approximated to one of the allowable discrete values
using an analog-to-digital (A/D) converter. These discrete values are then represented in a
binary format.

2.4 Operators and Assignments

Apart from providing di(cid:11)erent data types, high-level languages also provide a set of arith-
metic operators and logical operators that allow the programmer to specify manipulation
of variables. Operators act on the values assigned to variables. Variables, along with the
operators, allow the HLL programmer to express the computation speci(cid:12)ed in the algorithm
to be implemented.

The assignment statement in C involves the evaluation of expressions composed of op-
erators, variables, and constants. In C, as in most HLLs, all operators are either monadic
or dyadic, i.e, involve one or two operands.

Another operator available in C and other languages that support pointers is the address-
of operator, \&", to take the address of a static variable.

2.5. Control Abstraction

65

2.5 Control Abstraction

What distinguishes a computer from a simple calculator is its ability to make decisions:
based on the input data and the values created during the computation, di(cid:11)erent instruc-
tions are executed.

2.5.1 Conditional Statements

I shal l be tel ling this with a sigh
Somewhere ages and ages hence:
Two roads diverged in a wood, and I
I took the one less traveled by,
And that has made al l the di(cid:11)erence.

Robert Frost

I have always known
That at last I would
Take this road, but yesterday
I did not know that it would be today."

| The Road Not Taken by

of Ise) by Ariwara no Narihira (9th century Japan)

| Ise Monogatari (The Tales

if-else: Often it is necessary to have the program execute some statements only if a
condition is satis(cid:12)ed. All high-level languages provide if constructs to program conditional
execution. The C if statement has the syntax

if (expr)
stmt;

The semantics of this statement is: if expr evaluates to true, execute stmt, otherwise skip
stmt. Sometimes, a program may need to choose one of two alternative (cid:13)ows of control,
depending on the value of a variable or expression. For this purpose, the C language provides
an if-else construct, which has the syntax

if (expr)
stmt1;

else

stmt2;

The semantics of this statement is:
execute stmt2.

if expr evaluates to true, execute stmt1, otherwise

66

Chapter 2. Program Development Basics

switch: High-level languages also generally provide a construct for specifying multi-way
decisions. The C language provides a switch construct, which compares the value of an
expression against a list of supplied constant integer values, and depending on the match
selects a particular set of statements. Its syntax is given below.

switch (expr)
f

case const-expr1:
stmt1;
break;
case const-expr2:
stmt2;
break;
default:
stmt3;

g

The semantics of this switch statement is: if expr when evaluated matches the evaluated
value of const-expr1 or const-expr2, then control branches to stmt1 or stmt2, respectively.
If there is no match, then control branches to stmt3, which follows the default label. After
executing the statements associated with the matching case, control falls through to the
next case unless control is explicitly taken out of the switch construct using statements
such as break, goto, return, or exit().

2.5.2 Loops

while: Most programs require some action to be repeated a number of times, so long as
a particular condition is satis(cid:12)ed. Although it may be possible to include the same code
the required number of times, that would be quite tedious and result in longer programs.
Moreover, this may not be possible in cases where we do not know at programming time
the number of times the action needs to be repeated. High-level languages provide loop
constructs to express such code in a concise manner. The C while loop statement has the
syntax

while (expr)
stmt;

The semantics of this statement is: if expr evaluates to a non-zero value, execute stmt, and
repeat this process until expr evaluates to zero. stmt normally modi(cid:12)es a variable that is
contained in expr so that expr will eventually become false and therefore terminate the loop.

for: Although a while loop is su(cid:14)cient for implementing any kind of loops, most HLLs
provide a for loop construct to express some loops in a more elegant manner. The C for

2.5. Control Abstraction

statement has the syntax

for (initial expr; expr2; update expr)
stmt;

67

The semantics of this for loop can be easily understood when considering its equivalent
while loop, given below:

initial expr;
while (expr2)
f

stmt;
update expr;

g

In this loop, initial expr is the initializing statement; expr2 calculates the terminating con-
dition; and update expr implements the update of the loop variables.

2.5.3 Subroutines

The next topic that we like to discuss is the subroutine, procedure, function, or method
concept, an important innovation in the development of programming languages.
In a
given program, it is often necessary to perform a speci(cid:12)c task on di(cid:11)erent data values.
Such a repeated task is normally implemented as a subroutine, which can be called from
di(cid:11)erent places in a program. For example, a subroutine may evaluate the mathematical
sine function or sort a list of values into increasing or decreasing order. At any point in
the program the subroutine may be invoked or cal led. That is, at that point, the computer
is instructed to suspend the execution of the cal ler and execute the subroutine. Once the
subroutine’s execution is completed, control returns to the point from where the call was
made.

The two principal reasons for the use of subroutines are economy and modularity. A
subroutine allows the same piece of code to be called from di(cid:11)erent places in the program.
This is important for economy in programming e(cid:11)ort, and for reducing the storage require-
ments of the program. Subroutines also allow large programming tasks to be subdivided into
smaller units. This use of modularity greatly eases the programming task. The following C
code illustrates the use of subroutines in constructing a modular program.

main()
f

float a, b, c, d;

b = bigger(a, b);

68

Chapter 2. Program Development Basics

d = bigger(c, d);

g

bigger(float p, float q)
f

if (p >= q)
return p;

else

return q;

g

In this code, the function bigger() is used to determine which of its two input pa-
rameters (p or q) is bigger. This function is called twice from the function main().
If
the language does not support subroutines/functions, then most of the body of function
bigger() will need to be repeated twice in main().

The subroutine concept thus permits control abstraction; the code fragment within the
subroutine can be referred by the subroutine name at the calling place, where it is thought in
terms of its function rather than its implementation. In structured programming languages,
subroutines (and macros) are the main mechanism for control abstraction.

2.5.3.1 Parameter Passing

2.5.4 Subroutine Nesting and Recursion

If a subroutine calls a subroutine (either itself or another subroutine), then that is called
subroutine nesting. Conceptually, subroutine nesting can be carried out to any depth. The
(cid:12)rst subroutine to complete will be the one that was called last, causing control to return
to the one that called it.

If nesting involves calling the same subroutine itself, either directly or through other
subroutines, then we call that recursion. Recursion is an important concept in computer
science, analogous to the concept of induction in mathematics. Mathematical problems that
can be explained by induction invariably have an elegant recursive solution algorithm.

A rose is a rose is a rose

| Gertrude Stein

2.5.5 Re-entrant Subroutine

In a multitasked environment, many programs (or processes) can be simultaneously active
in a computer system. Many of the concurrently active processes may share a few routines,
especially those belonging to libraries and the OS. Consider the scenario where a context

2.6. Library API

69

switch happens when a process is in the middle of the execution of a subroutine. Before
the control is restored to this process, it is quite possible for another process to execute
the same subroutine. In order for a subroutine to be executed by another process in this
manner, the subroutine must be re-entrant, i.e., it must not update global variables.

2.5.6 Program Modules

We can let a subroutine to call subroutines that are physically present in other modules.
The caller and callee subroutines may be written by di(cid:11)erent programmers, at di(cid:11)erent
times! This calls for de(cid:12)ning and using speci(cid:12)c interfaces for each subroutine that is likely
to be called from other modules.

2.5.7 Software Interfaces: API and ABI

Application programs do not directly implement all of the functionality required. Instead,
they frequently invoke the services of library routines and OS routines.

2.5.7.1 Application Binary Interface (ABI)

An application binary interface (ABI) speci(cid:12)es the machine language interface provided by
an execution environment, which is usually a hardware platform along with the operating
system running on it (e.g., Linux ABI for the ARM Architecture). Thus, the ABI refers to
the speci(cid:12)cations to which an executable should conform in order to execute in a speci(cid:12)c
execution environment. The ABI includes the user mode instruction set architecture (the
memory address space, the number, sizes and reserved uses of registers, and the instruction
set), the ISA-level system call interface supported by the operating system (including the
system call numbers and how an application should make system calls to the operating
system), and the binary format of executable (cid:12)les. ABIs also cover other details of system
calls, such as the calling convention, which tells how functions’ arguments are passed and
return values retrieved. ABIs deal with run-time compatibility; a program binary targeted
to an ABI can run (without relinking or recompilation) on any system that supports the
same ABI. Application binary interfaces are also known as Abstract Machine Interface.

2.6 Library API

A practical extension of the above modularization concept is the use of library modules. A
library is a group of commonly used subroutines or functions bundled into a single module.
The basic libraries contain routines that read and write data, allocate and deallocate mem-
ory, and perform complex operations such as square root calculation and sorting. Other
libraries contain routines to access a database or manipulate terminal windows. Apart from

70

Chapter 2. Program Development Basics

language-dependent library such as the C stdio which provides IO routines, we also have
language-independent but OS-dependent libraries such as the Solaris thread library which
provides support functions for multithreading, and language- and platform-independent li-
braries such as the MPI (which supports the message-passing model of parallel processing)
and the OpenGL (which supports advanced graphics functions).

2.7 Operating System API

A hardware system, augmented by an operating system, cannot do much unless application
programs are loaded in it. Application programs are what the end users run when .............
In modern computers, application programs never run in a \vacuum" either. They rely
heavily on support from pre-written software such as library routines and the operating
system. When the application program wants to carry out a functionality that has already
been implemented in a library routine or in the OS, it simply invokes the functionality and
does not directly code the functionality.

An application programming interface (API) speci(cid:12)es a language and message format
used by an application program to communicate with a systems program (library, OS) that
provides services to it. Three commonly used OS APIs are the POSIX API for UNIX,
Linux, and MAC OS X systems; the Win32 API for Windows systems; and the Java API
for the Java virtual machine. It is the interface by which an application gains access to
operating systems.

The application program interface (API) de(cid:12)nes the calls that can be made from an
application to the operating system. Notice that adherence to an API does not ensure
runtime compatibility.

An API speci(cid:12)es a set of calling conventions that de(cid:12)nes how a service is invoked through
a software package. The calls, subroutines, interrupts, and returns that comprise a doc-
umented interface so that a higher-level program such as an application can make use of
the services of another application, operating system, network operating system, driver, or
other lower-level software program.

In the software (cid:12)eld, APIs are structured abstraction layers that conceal the gory details
of an individual application, operating system or hardware item and the world outside that
software or hardware.

All programs using a common API will have similar interfaces. This makes it easier to
port programs across multiple systems and for users to learn new programs.

An application program interface or application programming interface (API) is the
speci(cid:12)c method prescribed by a computer operating system or by an application program
by which a programmer writing an application program can make requests of the operating
system or another application.

We just saw the use of library routines in supporting IO streams and (cid:12)les. The library

2.7. Operating System API

71

routines do not directly access the IO devices, however. As we saw in Section 1.2.2, access to
and control of many of the hardware resources are regulated through the operating system.
This is because the low-level details of the system’s operation are often of no interest to
the computation at hand, and it is better to free the application programmer and the
library developer from dealing with the low-level details. The problem is further aggravated
because the program must be able to deal with a multitude of di(cid:11)erent IO interfaces, not
only for di(cid:11)erent device types but also for di(cid:11)erent models within the same class of devices.
It would not be practical to require each programmer to know the operational details of
every device that will be accessed by a program. Moreover, the number and type of devices
may change in course of time.
It is important that such changes remain transparent to
application programs.

To satisfy the preceding requirements of device independence, the operating system pro-
vides an abstract interface to the application programmer. This interface, called application
programming interface (API) of the OS, presents a greatly simpli(cid:12)ed view of the hardware
resources of the computing environment. The OS’ API is simply a set of commands that
can be issued by the application program to the operating system. If the OS performs these
jobs, it simpli(cid:12)es the job of the application programmer, who need not get involved with
the details of hardware devices. Further, when an application program uses the API, it is
shielded from changes in the computer hardware as new computers are developed. To be
speci(cid:12)c, the operating system and its device drivers can be changed to support new com-
puter hardware while preserving the API unchanged and allowing application programs to
run unchanged on the new computer. The API provided by the OS is formally de(cid:12)ned by
a set of human readable function call de(cid:12)nitions, including call and return parameters.

API

Application Programs

Library Routines
System Calls

System Call Interface

Operating System Routines

Figure 2.1: Illustration of API

Applications programs written with a particular API in mind can be run only on systems
that implement the same API. The APIs de(cid:12)ned by most operating systems are very similar,
and di(cid:11)er only in some minor aspects. This makes it easy to port application programs
developed for one OS to another. Examples of APIs are the Portable Operating System

72

Chapter 2. Program Development Basics

Interface (POSIX) and the Win32 API. The POSIX standard is based on early UNIX
systems and is widely used in UNIX-based operating systems such as FreeBSD and Linux.
Many non-UNIX systems also support POSIX. The Win32 API is the one implemented in
a Microsoft Windows environment. It supports many more functions (about 2000) than
POSIX, as it includes functions dealing with the desktop window system also.

2.7.1 What Should be Done by the OS?

(cid:15) Simple vs Complex Function

(cid:15) Speci(cid:12)c vs Generic Function

(cid:15) Security

The commands supported by an OS’ API can be classi(cid:12)ed into 3 classes:

(cid:15) Input/Output management

(cid:15) Memory management

(cid:15) Process management

2.7.2

Input/Output Management

The most commonly used part of the OS’ API is the part that deals with input/output.
The basic abstraction provided for application programmers and library programmers to
perform input and output operations is called a (cid:12)le. The (cid:12)le abstraction supported by the
API is more basic than the one supported by library routines in that it is simply a sequence
of bytes (with no formatting) in an IO device. This de(cid:12)nition of the (cid:12)le as a stream
of bytes imposes little structure on a (cid:12)le; any further structure is up to the application
programs, which may interpret the byte stream as they wish. The interpretation has no
bearing on how the OS stores the data. Thus, the syntax for accessing the data in a (cid:12)le is
de(cid:12)ned by the API, and is identical for all application programs, but the semantics of the
data are imposed by the application program. For instance, the text formatting program
LaTeX expects to (cid:12)nd \new-line" characters at the end of each line of text, and the system
accounting program acctcom expects to (cid:12)nd (cid:12)xed-length records in the (cid:12)le. Both programs
use the same API commands to access the data in the (cid:12)le as a byte stream, and internally,
they parse the stream into the appropriate format.

The (cid:12)le abstraction provided by the API is also device-independent like the one pro-
vided by the standard library. That is, it hides all device-speci(cid:12)c aspects of (cid:12)le manipulation
from HLL application and library programmers, and provides instead an abstraction of a
simple, uniform space of named (cid:12)les.

2.7. Operating System API

2.7.2.1 Example

73

For illustration, let us consider the UNIX API. In this API, all input and output operations
are done by reading or writing (cid:12)les, because all IO devices|including the user program’s
terminal|are considered as (cid:12)les. This means that a single, homogeneous interface handles
all communication between a program and peripheral devices.

The lowest level of IO in the UNIX API provides no bu(cid:11)ering or any other services. All
input and output operations are done by two API functions called read and write, which
specify a (cid:12)le descriptor and the number of bytes to be transferred. The second argument
is a bu(cid:11)er in the application program’s memory space where the data is to come from or
go to. The third argument is the number of bytes to be transferred. The calls are done as
follows:

numbytes read = read(fd, buf, numbytes);
numbytes written = write(fd, buf, numbytes);

Each of these system calls returns a byte count indicating the number of bytes actually
transferred. When reading, the number of bytes returned may be less than the number
asked for. A return value of zero indicates end of (cid:12)le, and a (cid:0)1 indicates an error 3 .

2.7.2.2 A Trace of a System Call

When an application program executes a read() system call to read data from a (cid:12)le, a
set of OS functions are called, which may eventually result in calling an appropriate device
driver. Figure 2.2 illustrates a situation in which an application program calls the OS twice
for reading from standard input. During the (cid:12)rst time, it calls the scanf() library function,
which calls the read() system call de(cid:12)ned in the API. During the second time, it directly
calls the read() system call.
In both cases, the read() routine calls the keybd read()
device driver to perform the actual read operation.

If a di(cid:11)erent keyboard is used in the future, only the keyboard device drivers need to be
changed; the application program, the library routines, and the device-independent part of
the OS require no change.

2.7.3 Memory Management

High-level languages that support pointers naturally allow dynamic allocation (and deal-
location) of memory. Applications programmers typically do allocation and deallocation
using library functions such as malloc() and free(), as we already saw in Section 2.1.
The malloc(n) function, for instance, returns a pointer to an unused block of n bytes.

3More details of read and write system calls can be obtained in a UNIX/Linux system by typing man 2
read and man 2 write, respectively.

74

Chapter 2. Program Development Basics

Library
Interface

API

scanf()
{

read();

}

main()
{

scanf();

read();

}

read()
{

keybd_read();

keybd_read()
{

}

}

Application Program

Library Routines

OS Kernel
(Device−Independent IO)

Device Drivers
(Device−Dependent IO)

Figure 2.2: A Trace of Routines Executed when Calling API Function read()

obtains large chunks of memory address space from the OS using the sbrk() system call
provided by the API, and manages them. The system call sbrk(b) returns a pointer to b
more bytes of memory.

2.7.4 Process Management

process creation

context switch

process control block

2.8 Operating System Organization

In modern computers, the operating system is the only software component that runs in the
Kernel mode. It behooves us therefore to consider the structure and implementation of an
operating system. In particular, it is important to see how the system calls speci(cid:12)ed in the
application programming interface (API) (provided to user programs and library functions)

2.8. Operating System Organization

75

are implemented by operating systems4 . The exact internal details of an operating system
vary considerably from one system to another. It is beyond the scope of this book to discuss
di(cid:11)erent possibilities. Figure 4.7 gives a possible block diagram, which is somewhat similar
to that of a standard UNIX kernel.
In the (cid:12)gure, the kernel mode software blocks are
shown shaded. The main components of the OS software include a system call interface,
(cid:12)le system, process control system, and the device management system (device drivers).
This organization uses a layered approach, which makes it easier to develop the OS and to
introduce modi(cid:12)cations at a later time. This also makes it easier to debug the OS code,
because the e(cid:11)ect of bugs may be restricted to a single layer. The (cid:12)gure also shows the
relationship of the OS to user programs, library routines, and the hardware.

HCI

User Mode
Software

ABI

OS Interface

Application Programs and Shell

Dynamically Linked Libraries
System Calls

System Call Layer

Device−
Independent

File System

Kernel Mode
Software

IO Management
Character−
Network−
Oriented
Oriented

Block−
Oriented

Process Control System

Inter−process
Communication

Scheduler

Memory
Management

User Mode
Instructions

Device−
Dependent

Device Drivers

OS Kernel

s
t
n
e
v
E
 
l
a
n
o
i
t
p
e
c
x
E

ISA

Kernel Mode

User Mode

Device Controllers

Hardware

IO Registers

IO Control Logic

IO Devices

Privileged Registers and Memory User Mode Registers and Memory
Control Unit, ALU, and Memory Controllers

Device Interrupts

Program Control Transfers Initiated by Software

Program Control Transfers Initiated by Hardware

Hardware Accessed/Controlled

Hardware Buses

Hardware Connections

Figure 2.3: Block Diagram Showing How a UNIX-like Kernel Implements the API

4Operating systems typically play two roles|controlling the environment provided to the end user and
controlling the environment provided to application programs. The former involves tasks such as maintaining
a (cid:12)le structure and supporting a graphical user interface. The latter involves taks such as reading a speci(cid:12)ed
number of bytes from a (cid:12)le on behalf of an application program. In this book, we are concerned only with
the latter role, as it is closer to computer architecture.

76

Chapter 2. Program Development Basics

2.8.1 System Call Interface

The system call interface provides one or more entry points for servicing system call instruc-
tions and exceptions, and in some cases device interrupts also. The system call interface
code copies the arguments of the system call and saves the user process’ context. It then
uses the system call type to look up a system cal l dispatch vector to determine the ker-
nel function to be called to implement that particular system call, interrupt, or exception.
When this kernel function completes, the system call interface restores the user process’
context, and switches to User Mode, transferring control back to the user process. It also
sends the return values and error status to the user program.

We can summarize the functions performed by the system call interface:

(cid:15) Determine type of syscall

(cid:15) Save process context

(cid:15) Call appropriate hander

(cid:15) Restore process context

(cid:15) Return to user program

2.8.2 File System

The API provided to application programs by the operating system, as we saw earlier, in-
cludes device-independent IO. That is, the interface is the same, irrespective of the physical
device that is involved in the IO operation. The (cid:12)le abstraction part of the API is sup-
posed to hide all device-speci(cid:12)c aspects of (cid:12)le manipulation from application programmers,
and provide them with an abstraction of a simple, uniform space of named (cid:12)les. Thus,
application programmers can rely on a single set of (cid:12)le-manipulation OS routines for (cid:12)le
management (and IO device management in an indirect manner). This is sometimes referred
device-
independent
to as device-independent IO.
IO

As we saw in Section 3.4.7, application programs access IO (i.e., (cid:12)les) through read and
write system call instructions. The read and write system call instructions (of the User
mode) are implemented in the Kernel mode by the (cid:12)le system part of the OS, possibly with
the help of appropriate device drivers.

Files of a computer installation may be stored on a number of physical devices, such as
disk drives, CD-ROM drives, and magnetic tapes, each of which can store many (cid:12)les. If the
IO device is a storage device, such as a disk, the (cid:12)le can be read back later; if the device is
a non-storage device such as a printer or monitor, the (cid:12)le cannot be read back. Di(cid:11)erent
(cid:12)les may store di(cid:11)erent kinds of data, for example, a picture, a spreadsheet, or the text of
a book chapter. As far as the OS is concerned, a (cid:12)le is simply a sequence of bytes written
to an IO device.

2.8. Operating System Organization

77

The OS partitions each (cid:12)le into blocks of (cid:12)xed size. Each block in a (cid:12)le has an address
that uniquely tells where within the physical device the block is located. Data is moved
between main memory and secondary storage in units of a single block, so as to take
advantage of the physical characteristics of storage devices such as magnetic disks and
optical disks.

File management related system calls invoked by application programs are interpreted
by the (cid:12)le system part of the OS, and transformed into device-speci(cid:12)c commands. The
process of implementing the open system call thus involves locating the (cid:12)le on disk, and
bringing into main memory all of the information necessary to access it. The OS also
reserves for the (cid:12)le a bu(cid:11)er space in its memory space, of size equal to that of a block.
When an application program invokes a system call to write some bytes to a (cid:12)le, the (cid:12)le
system part of the OS writes the bytes in the bu(cid:11)er allotted for the (cid:12)le. When the bu(cid:11)er
becomes full, the (cid:12)le system copies it into a block in a storage device (by invoking the
device’s device driver); this block becomes the next block of the (cid:12)le. When the application
process invokes the close system call for closing a (cid:12)le, the (cid:12)le system writes the (cid:12)le’s bu(cid:11)er
as the (cid:12)nal block of the (cid:12)le, irrespective of whether the bu(cid:11)er is full or not, prior to closing
the (cid:12)le. Closing a (cid:12)le involves freeing up the table space used to hold information about
the (cid:12)le, and reclaiming the bu(cid:11)er space allotted for the (cid:12)le.

2.8.3 Device Management: Device Drivers

The device management part of the OS is implemented as a collection of device drivers.
Most computers have input/output devices such as terminals and printers, and storage
devices such as disks. Each of these devices requires speci(cid:12)c device driver software, which
acts as an interface between the device controller and the (cid:12)le system part of the OS kernel.
A device driver is needed because each device has its own speci(cid:12)c commands instead of
generic commands. A printer device driver, for instance, contains all the software that is
speci(cid:12)c to a prticular type of printer such as a Postscript printer. Thus, the device drivers
form the device-dependent part of the IO software. By partitioning the kernel mode software
into device-independent and device-dependent components, the task of adding a new device
to the computer is greatly simpli(cid:12)ed.

The device drivers form a ma jor portion of the kernel mode software. Each device driver
itself is a collection of routines, and can have multiple entry points. The device driver
receives generic commands from the OS (cid:12)le system and converts them into the specialized
commands for the device, and vice versa. To the maximum extent possible the driver
software hides the unique characteristics of a device from the OS (cid:12)le system.

Device drivers can be fairly complex. Many parameters may need to be set prior to
starting a device controller, and many status bits may need to be checked after the comple-
tion of each device operation. Many device drivers such as the keyboard driver are supplied
as part of the pre-installed system software. Device drivers for other devices need to be
installed as and when these devices are installed.

78

Chapter 2. Program Development Basics

The routines in a device driver can be grouped into three kinds, based on functionality:

(cid:15) Autocon(cid:12)guration and initialization routines

(cid:15) IO initiation routines

(cid:15) IO continuation routines (interrupt service routinestem Autocon(cid:12)guration and initial-
ization routines

(cid:15) IO initiation routines

(cid:15) IO continuation routines (interrupt service routines)

The autocon(cid:12)guration routines are called at system reboot time, to check if the corre-
sponding device controller is present, and to perform the required initialization. The IO
initiation routines are called by the OS (cid:12)le system or process control system in response to
system call requests from application programs. These routines check the device status, and
initiate IO requests by sending commands to the device controller. If program-controlled
IO transfer is used for the device, then the IO initiation routines perform the IO transfers
also. By contrast, if interrupt-driven IO transfer is used for the device, then the actual IO
transfer is done by the interrupt service routines when the device becomes ready and issues
an interrupt.

2.8.4 Hardware Abstraction Layer (HAL)

The hardware abstraction layer provides a slightly abstract view of the hardware to the
OS kernel and the device drivers. By hiding the hardware details, it provides a consistent
hardware platform for the OS. This makes it easy to port an OS across a family of hardware
platforms that have the same user mode ISA, but di(cid:11)er in the kernel mode ISA (such as
di(cid:11)erent MMU architectures).

2.8.5 Process Control System

2.8.5.1 Multi-Tasking

When a computer system supports multi-tasking, each process sees a separate virtual ma-
chine, although the concurrent processes are sharing the same physical resources. Therefore,
some means must be provided to separate the virtual machines from each other at the phys-
ical level. The physical resources that are typically shared by the virtual machines are the
processor (including the registers, ALU, etc), the physical memory, and the IO interfaces.
Of these, the processor and the IO interfaces are typically time-shared between the pro-
cesses (temporal separation), and the physical memory is partitioned between the processes

2.8. Operating System Organization

79

(spatial separation)5 . To perform a context switch of the virtual machines, the time-shared
resources must be switched from one virtual machine to the next. This switching must be
managed in such a way that the virtual machines do not interact through any state infor-
mation that may be present in the physically shared resources. For example, the ISA-visible
registers must be saved and restored during a context switch so that the new context cannot
access the old context’s register state.

Decisions regarding time-sharing and space-sharing are taken in the Kernel mode by the
operating system, which is responsible for allocating the physical resources to the virtual
machines. If a user process is allowed to make this decision, then it could possibly encroach
into another process’ resources, and tamper with its execution. The operating system’s
decisions, however, need to be enforced when the system is in the User mode. This enforce-
ment is done using special hardware (microarchitectural) support so that the enforcement
activity does not reduce performance.

2.8.5.2 Multi-Programming

Some applications can be most conveniently programmed for two or more cooperating pro-
cesses running in parallel rather than for a single process. In order for several processes to
work together in parallel, certain new Kernel mode instructions are needed. Most modern
operating systems allow processes to be created and terminated dynamically. To take full
advantage of this feature to achieve parallel processing, a system call to create a new pro-
cess is needed. This system call may just make a clone of the caller, or it may allow the
creating process to specify the initial state of the new process, including its program, data,
and starting address. In some cases, the creating (parent) process maintains partial or even
complete control over the created (child) processes. To this end, Kernel mode instructions
are added for a parent to stop, restart, examine, and terminate its children.

5Time-sharing the entire physical memory is not feasible, because it necessitates saving the physical
memory contents during each context switch.

80

Chapter 2. Program Development Basics

2.9 Ma jor Issues in Program Development

2.9.1 Portability

2.9.2 Reusability

2.9.3 Concurrency

2.10 Concluding Remarks

2.11 Exercises

Chapter 3

Assembly-Level Architecture |
User Mode

He who scorns instruction wil l pay for it, but he who respects a command is rewarded.

Proverbs 13: 13

This is the (cid:12)rst of several chapters that address core issues in computer architecture.
The previous chapter discussed high-level architectures. This chapter is concerned with the
immediately lower-level architecture that is present in essentially all modern computers: the
assembly-level architecture. This architecture deals with the way programs are executed
in a computer from the assembly language programmer’s viewpoint. We describe ways in
which sequences of instructions are executed to implement high-level language statements.
We also discuss commonly used techniques for addressing memory locations and registers.
A proper understanding of these concepts is an essential part of the study of computer
architecture, organization, and design. We introduce new concepts in machine-independent
terms to emphasize that they apply to all computers.

The vast ma jority of today’s programs are written in a high-level language such as C,
FORTRAN, C++, and Java. Before the introduction of high-level languages, early pro-
grammers and computer architects were using languages of a di(cid:11)erent type, called assembly
languages. The main purpose of discussing assembly-level architectures in this book is
to provide an adequate link to instruction set architectures and microarchitectures, which
provide a closer view on how computers are built and how they operate. To execute any
high-level program, it must (cid:12)rst be translated into a lower level program (most often by a
compiler and occasionally by assembly language programmers). Knowledge of the assembly-
level architecture is a must, both for compiler writers and for assembly language program-
mers. The relationship between high-level, assembly, and machine language features is a key

81

82

Chapter 3. Assembly-Level Architecture | User Mode

consideration in computer architecture. Much of the discussion in this chapter is applicable
to both the assembly-level architecture and the instruction set architecture, as the former
is a symbolic representation of the latter.

The ob jective of this chapter on assembly-level architecture is not to make you pro(cid:12)cient
in assembly language programming, but rather to help you understand what this virtual
machine does, and how high-level language programs are converted to assembly language
programs. We include a number of code fragments that are short and useful for clari(cid:12)cation.
These code fragments are meant to be conceptual, rather than to be cut and pasted into
your application programs. If you like to write intricate assembly language programs, it is
better to follow up this material with a good book on assembly language programming.

3.1 Overview of User Mode Assembly-Level Architecture

We shall (cid:12)rst present an overview of the basic traits of an assembly language machine. You
will notice that these basic traits closely resemble those of the generic computer organization
described in Section 1.2. An assembly language machine is designed to support a variety of
high-level languages, and is not tailored to a particular high-level language. Thus, programs
written in di(cid:11)erent high-level languages can be translated to the same assembly language.

It is also interesting to note that many of the popular assembly-level architectures are
quite similar to each other, just like the case of many popular high-level architectures.
Therefore once you master one, it is easy to learn others. This similarity occurs because
any given assembly-level architecture closely follows an instruction set architecture (ISA),
and ISA design is driven by hardware technology and application requirements. Di(cid:11)erent
ISAs have many things in common, because they target similar application domain, and
are interpreted by hardware machines built using similar hardware technologies.

\Real programmers can write assembly code in any language. :-)"
{ Larry Wal l (the Perl guy)

\Al l people smile in the same language."
{ Author Unknown

As mentioned in Section 1.8, an architecture speci(cid:12)es what data can be named by a
program written for that architecture, what operations can be performed on the named
data, and what ordering exists among the operations. When writing an assembly language
program, the locations that can be named are the (virtual) memory address space, and
the registers. The value returned by a read to an address is the last value written to that
address. In most languages, sequential order is implied among the instructions. That is,
instructions are to be executed one after the other, in the order in which they are speci(cid:12)ed
in the program.

3.1. Overview of User Mode Assembly-Level Architecture

83

3.1.1 Assembly Language Alphabet and Syntax

The programmer must tell the computer, through the statements/instructions in the pro-
gramming language used, everything that the computer must do. We shall look at the
di(cid:11)erent facets of an assembly language.

3.1.1.1 Alphabet

High-level languages, as we saw in Chapter 2, are somewhat close to natural languages,
and are substantially removed from the underlying hardware levels. An assembly language
also uses symbols and words from natural languages such as English, but is closer to the
underlying hardware1 . Formally speaking, an assembly language consists of a set of symbolic
names and a set of rules for their use. The symbolic names are called mnemonics (which
mean \aid to memory" in the Greek language).

3.1.1.2 Syntax

The syntax of a language is the set of rules for putting together di(cid:11)erent tokens to produce
a sequence of valid statements or instrucions. In the case of assembly languages, this deals
with the rules of using the mnemonics in the speci(cid:12)cation of complete instructions and
assembly language programs. The syntax followed by assembly languages is quite di(cid:11)erent
from that followed by high-level languages. Instructions are generally speci(cid:12)ed in a single
line by an opcode mnemonic followed by zero or more operand mnemonics. The opcode (cid:12)eld
may be preceded by a label, and the instruction may be followed by a comment, which starts
with a character such as \;" or \#". Most of the assembly languages require the label to
start in the (cid:12)rst column of the line, and instructions to start only from the second column
or later. You might wonder why the syntax of an assembly language is so restrictive. The
reason is to simplify the assembler, which was traditionally written in assembly language to
occupy very little space in memory. Apart from instructions, an assembly language program
also contains assembler directives, which separate data values from instructions and specify
information regarding data values.

3.1.2 Memory Model

An assembly-level architecture de(cid:12)nes a memory model consisting of a large number of
locations, each of which is a (cid:12)xed-size group of storage cel ls. Each cell can store a single bit

1Speci(cid:12)cally, an assembly language is a symbolic representation of the machine language|which uses
only bit patterns (1s and 0s) to specify information. It uses a richer set of symbols (including the English
alphabet) instead of bits, and gives symbolic names to commonly occurring bit patterns, such as opcodes
and register speci(cid:12)ers, which make it easier for humans to read and comprehend them. For example,
in assembly language, we use instructions such as add $at, $v0, $v1 in place of bit patterns such as
000000000010001000011000000000000.

84

Chapter 3. Assembly-Level Architecture | User Mode

of information|a 0 or a 1. Most assembly languages consider a location to store 8 bits, or
a byte. Because a single bit represents a very small amount of information, bits are seldom
handled individually; the usual approach is to read or write one or more locations at a time.
Most assembly languages provide instructions for manipulating data items of di(cid:11)erent sizes,
such as 8 bits, 16 bits, 32 bits, and 64 bits.

For the purpose of reading or writing memory locations, each memory location is given a
unique address. The collection of all memory locations is often called the memory address
space. Although assembly languages have provision to deal with numerical addresses, it is
customary to use labels to refer to memory locations. As labels can be constructed using
alphabetical letters (in addition to numerical digits if required), they are easier for the
programmer to keep track of.

The instructions and data of an assembly language program are strongly tied to the
memory address space. Each instruction or data item is viewed as occupying one location
(or a contiguous set of locations) in the memory address space. Although the assembly
language programmer can assign instructions and data items to memory locations in a
random manner, for functional reasons it is better to organize the locations into a few
sections, much like how the ma^(cid:16)tre d’h^otel organizes a restaurant dining area into smoking
and non-smoking sections. Each section holds a chunk of code or data that logically belongs
together. Some of the sections commonly used by assembly language programs are text
(code), data, heap, and stack, as illustrated in Figure 3.1.

Memory Address Space

Addresses
0x00000000

Text

Data

Stack

Direction of growth
at run−time

Figure 3.1: Organizing the Memory Address Space as Multiple Sections

3.1. Overview of User Mode Assembly-Level Architecture

85

The text section, as its name implies, is used to allocate instructions, and is read-
only from the application program’s point of view. The operating system can still write
to that section, and uses this ability when loading an application program into memory.
Another point to note is that in machines that allow self-modifying code, the text section
is read-write.

The data section is generally used to store data items that are required throughout the
activation of the program. Such items include, for instance, statically allocated global data
items and dynamically allocated data items. This section is allowed to grow at run-time as
and when allocating new data items dynamically.

The stack section is generally used to store data items that are required only during
the activation of a subroutine; such items include local variables and parameters to be
passed to other subroutines. The stack section is therefore empty at programming time. At
program run-time, it starts empty, and then grows and shrinks as subroutines are called and
exited. Every time a subroutine is called, the stack section grows by an amount called stack
frame or activation record. The stack frame thus constitutes a private work space for the
subroutine, created at the time the subroutine is called and freed up when the subroutine
returns. Historically, most machines assume the stack frames to grow in the direction of
decreasing memory addresses, although a few machines assume the opposite.

3.1.3 Register Model

Most assembly-level architectures include a few registers to store information that needs
to be accessed frequently. In the lower-level hardware implementations, these registers are
implemented in a manner that permits faster access to them compared to accessing the
main memory. This is because of the following reasons, which stem from having only a few
registers:

(cid:15) In a microarchitecture, the decoder or selector used to select a register will be much
smaller than the one used to select a memory location.

(cid:15) In a microarchitecture, the registers are typically implemented inside the processor
chip, and so no o(cid:11)-chip access is required to access a register.

(cid:15) In a device-level architecture, the technology used to implement registers ((cid:13)ip-(cid:13)ops
or SRAMs) is faster than the one used to implement memories (DRAMs), which are
typically designed for achieving high density.

Apart from these access time advantages, there is yet another advantage at the lower level:
a register can be speci(cid:12)ed with just a few bits in a machine language. This is much less than
the 32 or 64 bits required to specify a memory location. For example, in an instruction
set architecture that speci(cid:12)es 32 registers, only log 2 32 = 5 bits are needed to specify a
register. Notice that all of these advantages of registers would be lost, if too many registers
are speci(cid:12)ed.

86

Chapter 3. Assembly-Level Architecture | User Mode

Registers are used for a variety of applications, and generally have names that denote
their function. Below, we give the names of commonly used registers and a brief description
of their function. Notice that not all machines may have every one of these registers.

(cid:15) Program Counter (PC): This register is speci(cid:12)ed in virtually every machine, and is
used to store the address of the memory location that contains the next instruction
to be executed.

(cid:15) Accumulator (ACC): Many of the early machines speci(cid:12)ed a special register called the
accumulator to store the result of all arithmetic and logical operations.

(cid:15) Stack Pointer (SP): This register is used to store the address of the topmost location
of the stack section of the memory.

(cid:15) Link Register: This register is used to store the return address when calling a sub-
routine.

(cid:15) General-Purpose Registers (GPRs): Modern architectures invariably specify a number
of GPRs to store key local variables and the intermediate results of computation.
Examples are the AX, BX, CX, and DX registers in the IA-32 architecture, and registers $0
through $31 in the MIPS-I architecture. Most architectures specify separate registers
for holding integer numbers and (cid:13)oating-point numbers. On some architectures, the
GPRs are completely symmetric and interchangeable. That is, to hold a temporary
result, the compiler can equally use any of the GPRs; the choice of register does not
matter. On other architectures some of the GPRs may have some special functions
too. For example, in the IA-32 architecture, there is a register called EDX, which can
be used as a GPR, but which also receives half the product in a multiplication and
half the dividend in a division. Similarly, in the MIPS-I architecture, register $31
is a GPR, but is used to store the return address when executing a subroutine call
instruction.

(cid:15) Flags Register: If speci(cid:12)ed, this register stores various miscellaneous bits of informa-
tion (called (cid:13)ags or condition codes), which re(cid:13)ect di(cid:11)erent properties of the result
of the most recent arithmetic or logical operation, and are likely to be needed by
subsequent instructions. Typical condition code bits include:
N | set if the previous result was negative
Z | set if the previous result was zero
V | set if the previous result caused an over(cid:13)ow 2

2Over(cid:13)ow occurs in a computer because of using a (cid:12)xed number of bits to represent numbers. When
the result of an arithmetic operation cannot be represented by the (cid:12)xed number of bits allotted for the
result, then an over(cid:13)ow occurs. The over(cid:13)ow event can be handled in 3 ways: (i) the semantics of the
instruction that generated the over(cid:13)ow may include speci(cid:12)cations on how the over(cid:13)ow is treated. (ii) the
over(cid:13)ow triggers an exception event, transferring control to the operating system, which then handles the
exception event. (iii) the V (cid:13)ag is set to 1 so as to permit subsequent instructions of the application program
to monitor the V (cid:13)ag and take appropriate action.

3.1. Overview of User Mode Assembly-Level Architecture

87

C | set if the previous result caused a carry out of the most signi(cid:12)cant bit (MSB)
A | set if the previous result caused a carry out of bit 3 (auxiliary carry)
P | set when the previous result had even parity.
Flags are set implicitly by certain arithmetic and logical instructions. For example,
after a compare instruction is executed, the Z (cid:13)ag is used to indicate if the two num-
bers are equal, and the N (cid:13)ag is used to indicate if the second number is bigger than
the (cid:12)rst number. A subsequent instruction can test the value of these (cid:13)ags, and take
appropriate action. Similarly, the C (cid:13)ag is useful in performing multiple-precision
arithmetic. The required multiple-precision addition is done in several steps, with
each step doing a single-precision addition. The C (cid:13)ag generated in one step serves
as a carry input for the next step. The A (cid:13)ag is useful for performing arithmetic
operations on packed decimal numbers.

3.1.4 Data Types

In Chapter 2 we saw that declaring and manipulating variables were key concepts in high-
level languages; each variable has a type associated with it. By contrast, the assembly-
level architecture does not have a notion of variables!
Instead, the assembly language
programmer considers the contents of a register, a memory location, or a contiguous set of
memory locations as a data item, and manipulates the contents of these storage locations
using instructions.

When a compiler translates an HLL program into an assembly language program, it
maps HLL variables to memory locations in the assembly-level architecture. The number of
memory locations allocated depends on the variable’s type. The memory section or region
in which locations are allocated depends on the variable’s storage class.

Assembly-level architectures support a variety of data types, such as characters, signed
integers, unsigned integers, and (cid:13)oating-point numbers. Support for a particular data type
comes primarily in the form of instruction opcodes that interpret a bit pattern as per
the de(cid:12)nitions of that data type. For example, an assembly language may provide two
di(cid:11)erent ADD instructions|add and fadd|one that interprets bit patterns as integers, and
one that interprets them as (cid:13)oating-point numbers. Remember that in an assembly-level
architecture, the data in a particular storage location is not self-identifying. That is, the
bits at that storage location do not specify a data type, and therefore have no inherent
meaning. The meaning is determined by how an instruction uses them.
It is up to the
assembly language programmer (or compiler) to use the appropriate opcodes to interpret
the bit patterns correctly. Thus, it is the job of the assembly language programmer (or
the compiler) to ensure that bit patterns representing integer variables are added together
using an integer ADD instruction.

To illustrate this further, we shall use an example. Figure 3.2 shows how two di(cid:11)erent
ADD instructions can produce two di(cid:11)erent results when adding the same two bit patterns
01010110 and 00010100. In the (cid:12)rst case, an integer add instruction treats the two patterns

88

Chapter 3. Assembly-Level Architecture | User Mode

as (binary encoded) integers 86 and 20, and obtains the result pattern 01101010, which has
a decimal value 106 when interpreted as an integer. In the second case, a BCD 3 add instruc-
tion treats the two patterns as (BCD encoded) numbers 56 and 14, and obtains a di(cid:11)erent
result pattern of 01110000, which represents the decimal number 70 when interpreted as a
BCD number.

Binary Number System
(Unsigned Integers)

Binary Coded Decimal System
(BCD Integers)

01010110
00010100
01101010

(86)
(20)
(106)

1

01010110
00010100
01110000

(56)
(14)
(70)

Decimal

Decimal

Figure 3.2: An Example Illustrating how the same two Bit Patterns can be added di(cid:11)erently
to yield Di(cid:11)erent Results

Below, we highlight the data types that are typically supported in assembly-level archi-
tectures.

(cid:15) Unsigned Integer and Signed Integer: Integers are among the most basic data
types, and all machines support them. Some machines provide support for unsigned
integers as well as signed integers. This support comes primarily in the way of in-
structions that have a di(cid:11)erent semantic for recognizing arithmetic over(cid:13)ows.

(cid:15) Floating-Point Number: Most machines support (cid:13)oating-point data type by in-
cluding speci(cid:12)c instructions for performing (cid:13)oating-point arithmetic. Many machines
also have separate registers for holding integer values and (cid:13)oating-point values. As
mentioned in Chapter 2, an FP number is written on paper as follows:
(S ign)S ignif icand (cid:2) B aseE xponent
The base in the above equation is the radix of the system, which is a constant for a
particular assembly-level architecture, and is usually chosen as 2. The signi(cid:12)cand is
used to identify the signi(cid:12)cant digits of the FP number; the number of bits allotted
for the signi(cid:12)cand determines the precision.

(cid:15) Decimal Number: Some HLLs, notably COBOL, allow decimal numbers as a data
type. Assembly-level architectures that were designed to be COBOL-friendly often
directly support decimal numbers, typically by encoding a decimal digit in 4 bits
and then packing two decimal digits per byte (BCD format). However, instead of

3A BCD number is a binary coded decimal number. The BCD format is explained later in this section.

3.1. Overview of User Mode Assembly-Level Architecture

89

providing arithmetic opcodes that work correctly on these packed decimal numbers,
they typically provide special decimal-arithmetic-correction opcodes that can be used
after an integer addition to obtain the correct BCD answer! These opcodes use the
carry out of bit 3, which is available in the A (auxiliary carry) (cid:13)ag.

(cid:15) Character: Most assembly-level architectures support non-numeric data types such
as characters. It is not uncommon for an assembly-level architecture to have special
opcodes that are intended for handling character strings, that is, consecutive runs
of characters. These opcodes can perform copy, search, edit, and other functions on
strings.

If an architecture does not support a particular data type, and an arithmetic operation
needs to be performed for that data type, the assembly language programmer (or compiler)
may have to synthesize that operation using the available instructions. For example, if an
architecture does not support the (cid:13)oating-point data type, and there is a need to add two
bit patterns as if they are stored in the (cid:13)oating-point format, then the programmer needs
to write a routine that separates the exponents and signi(cid:12)cands, equalizes the exponents by
modifying the signi(cid:12)cands, and then performs the addition and re-normalization operations.
Similarly, if the architecture does not support the BCD (binary coded decimal) data type,
and if BCD arithmetic needs to be performed on a bit pattern, then the assembly language
programmer (or compiler) needs to synthesize that arithmetic operation using sequences of
existing instructions.

3.1.5 Assembler Directives

An assembly language program instructs the assembly-level machine two things: (i) how to
initialize its storage locations (registers and memory), and (ii) how to manipulate the values
in its storage locations. The (cid:12)rst part is conveyed by means of statements called assembler
directives, and the second part is conveyed by means of assembly-level instructions. (Apart
from directives that are used to initialize storage locations, there are some other directives
that are executed by the assembler and are not assembled. There are also some directives
that serve as programming tools, to simplify the process of writing the program.) We shall
discuss directives in this section. Instructions will be discussed in the next section.

Without initialization, the values in registers and memory locations would be unde(cid:12)ned.
The bulk of the initialization done by the directives involves the memory space and those
registers that point to di(cid:11)erent sections in the memory space, such as the program counter,
the global pointer, and the stack pointer. Speci(cid:12)cally, they indicate which static sections
the program will use (.text, .rdata, .data, etc 4 ), how big these sections are, where their
boundaries are, and what goes into the di(cid:11)erent data sections.

4Dynamic sections such as heap and stack are created during execution, and are therefore not speci(cid:12)ed
by directives.

90

Chapter 3. Assembly-Level Architecture | User Mode

For example, to place the subsequent statements of the program in the .data section of
memory, we can use a directive such as:

.data
And, to initialize the next 4 bytes to 58, we can use a directive such as:
.word 58

When an assembly language program is translated to machine language, the directives
are translated to form the machine language program’s header, section headers, and various
data sections. A directive therefore does not translate to machine language instructions. At
the execution time of a machine language program, the initialization task is performed by
the loader part of the OS, which reads the program’s header, section headers, and sections
to do so.

3.1.6

Instruction Types and Instruction Set

Apart from assembler directives, an assembly language program includes assembly-level
instructions also. In fact, the instructions form the crux of the program; a program without
instructions would only be initializing the registers and memory! As mentioned before,
instructions manipulate the values present in registers and memory locations. For example,
to copy the contents of memory location A to register $t0, we can use an instruction such
as:

lw
$t0, A
Here, the mnemonic lw is an abbreviation for load word. Similarly, to add the contents of
registers $t1 and $t2 and to place the result in register $t3, we can use an instruction such
as:

add

$t3, $t1, $t2

A typical program involves performing a number of functionally di(cid:11)erent steps, such
as adding two numbers, testing for a particular condition, reading a character from the
keyboard, or sending a character to be displayed on a video screen. Each assembly-level
architecture has its own set of instructions, called its instruction set. In practice, many
of the instruction sets are quite similar. The instructions in an instruction set can be
functionally classi(cid:12)ed into four categories:

(cid:15) Data transfer

(cid:15) Data manipulation

(cid:15) Program sequencing and control

(cid:15) Trap or syscall

Data Transfer: Data transfer instructions copy data from one storage location to an-
other, without changing the data stored in the source location. Typical transfers of data

3.1. Overview of User Mode Assembly-Level Architecture

91

are between registers and main memory locations; between registers and stack locations; or
or between registers themselves. Variables declared in a high-level language are generally
allocated locations in main memory (some are allocated in general-purpose registers), and
so most of the data reside initially in main memory. From the main memory, data is often
copied to general-purpose registers or the stack, prior to operating on them. Data transfer
instructions are quite useful in this endeavor. Some of the commonly used data transfer
instructions in di(cid:11)erent assembly languages and their semantics are given in Table 3.1.

Mnemonic

MOV
LOAD
STORE
PUSH
POP
XCH

Semantics
Copy data from one register/memory location to another
Copy data from a memory location to a register
Copy data from a register to a memory location
Copy data from a register/memory location to top of stack
Copy data from top of stack to a register/memory location
Exchange the contents of two register/memory locations

Table 3.1: Common Data Transfer Instructions in Di(cid:11)erent Assembly Languages

Data Manipulation: Data manipulation instructions perform a variety of operations
on data and modify them. These are instrumental for implementing the operators and
assignments of HLL programs. There are three types of data manipulation instructions:
arithmetic, logical, and shift. Some of the commonly used data manipulation instructions
in di(cid:11)erent assembly languages and their semantics are given in Table 3.2. The input
operands for these instructions are speci(cid:12)ed as part of the instruction, or are available in
storage locations such as memory locations, registers, or stack. The result of the instruction
is also stored in a storage location. Recently, many assembly languages have included
instructions that are geared for speeding up multimedia applications.

Program Sequencing and Control: Normally, the instructions of a program are exe-
cuted one after the other, in a straightline manner. Control-changing instructions are used
to deviate from this straightline sequencing. A conditional branch instruction is an instruc-
tion that causes a control (cid:13)ow deviation, if and only if a speci(cid:12)c condition is satis(cid:12)ed. If
the condition is not satis(cid:12)ed, instruction sequencing proceeds in the normal way, and the
next instruction in sequential address order is fetched and executed. The condition can
be the value stored in a condition code (cid:13)ag, or the result of a comparison. Conditional
branch instructions are useful for implementing if statements and loops. Besides condi-
tional branches, instruction sets also generally provide unconditional branch instructions.
When an unconditional branch is encountered, the sequencing is changed irrespective of any
condition. Finally, assembly languages also include cal l instructions. and return instructions

92

Chapter 3. Assembly-Level Architecture | User Mode

Mnemonic
ADD
SUB
MULT
DIV

INC

DEC

ABS
AND
OR
XOR
LSHIFT
RSHIFT
LROT
RROT

Semantics
Add two operand values and store the result value
Subtract one operand value from another and store the result value
Multiply two operand values and store the result value
Divide an operand value by another and store the quotient
remainder values
Increment an operand value and store the result value
in the same location
Decrement an operand value and store the result value
in the same location
Find the absolute value of the operand value and store the result
And two operand values and store the result value
Or two operand values and store the result value
Exor two operand values and store the result value
Left shift one operand value by another and store the result value
Right shift one operand value by another and store the result value
Left rotate one operand value by another and store the result value
Right rotate one operand value by another and store the result value

Table 3.2: Common Data Manipulation Instructions in Di(cid:11)erent Assembly Languages

to implement subroutine calls and returns. Some of the commonly used program control
and sequencing instructions in di(cid:11)erent assembly languages and their semantics are given
in Table 3.3. The (cid:12)rst group of instructions in the table are conditional branch instructions;
the second group are unconditional branches, and the third group deal with subroutine calls
and returns.

Trap or Syscall Instructions: The instructions we saw so far cannot do IO operations
or terminate a program. For performing such operations, an application program needs
to call the services of the operating system (OS). Trap or syscall instructions are used
to transfer control to the OS, in order for the OS to perform some task on behalf of the
application program. That is, this instruction is used to invoke an OS service. Once the OS
completes the requested service, it can return control back to the interrupted application
program by executing an appropriate Kernel mode instruction. If a single trap instruction
is provided for specifying di(cid:11)erent types of services, then the required service is speci(cid:12)ed as
an operand of the instruction.

3.1. Overview of User Mode Assembly-Level Architecture

93

Mnemonic
JZ
JNZ
JC
JNC
BEQ
BNE
BLT
JMP
B
JR
CALL
JAL
RETURN

Semantics
Jump if Z (cid:13)ag is set
Jump if Z (cid:13)ag is not set
Jump if C (cid:13)ag is set
Jump if C (cid:13)ag is not set
Branch if both operand values are equal
Branch if both operand values are not equal
Branch if one operand value is less than the other
Jump unconditionally
Branch unconditionally
Jump to address in speci(cid:12)ed register
Call speci(cid:12)ed subroutine
Jump and link to speci(cid:12)ed subroutine
Return from subroutine

Table 3.3: Some of the Common Program Sequencing and Control Instructions in Di(cid:11)erent
Assembly Languages

3.1.7 Program Execution

Consider a simple assembly language program to add the contents of memory locations 1000
and 1004, and store the result in memory location 1008.

.text
start: lw
lw
add
sw
sys halt

$t0, 1000
$t1, 1004
$t1, $t1, $t0
$t1, 1008

The programmer’s view is that when this program is being executed, it is present in the
computer’s main memory. The (cid:12)rst line in the program, namely .text, is a directive
indicating that the program be placed in the .text section of main memory. This line
is called an assembler directive, and is not part of the executed program. The next line,
which contains the (cid:12)rst instruction, begins with the label
start. This label indicates the
memory address of the (cid:12)rst instruction. Figure 3.3 shows how this small program might be
loaded in the .text section of the memory space, when it is about to be executed. The (cid:12)ve
instructions of the program have been placed in successive memory locations, in the same
start. Notice that the label
order as that in the program, starting at location
start
does not explicitly appear in the program stored in memory, nor does the directive .text
in the (cid:12)rst line. The comment portions of the statements also do not appear in memory.

94

Chapter 3. Assembly-Level Architecture | User Mode

Address
0

__start

__start

Address

Main Memory

lw  $t0, 1000
lw  $t1, 1004
add $t1, $t1, $t0
sw  $t1, 1008
sys_halt

.text
section

.data
section

1000
1004
1008

M  −1

Data

pc

$t0
$t1
$t2
$t3
$t4
$t5

Figure 3.3: A Possible Placement of the Sample Program in Memory

Let us consider how this program is executed. To begin executing the program, the
address of its (cid:12)rst instruction (P ) is placed into pc. This instruction is executed and the
contents of pc are advanced to point to the next instruction. Then the second instruction
is executed, and the process is continued until the computer encounters and executes the
sys halt instruction. The last instruction transfers control to the OS, and tells it to
terminate this program. As you can see, instructions are executed in the order of increasing
addresses in memory. This type of sequencing is called straight-line sequencing.

3.1.8 Challenges of Assembly Language Programming

Because an assembly language is less abstract than a high-level language, programming in
assembly is considerably more di(cid:14)cult than programming in a high-level language. The
lack of abstraction manifests itself in di(cid:11)erent ways:

(cid:15) First, the storage resources are more concrete at the assembly level, which has the
notion of registers and memory locations, as opposed to variables. The programmer
must manage the registers and memory locations at every step of the way. In machines
with condition codes, the programmer must keep track of the status of condition codes
and know what instructions a(cid:11)ect them before executing any conditional branches.
This sounds tedious, if not di(cid:14)cult.

3.1. Overview of User Mode Assembly-Level Architecture

95

(cid:15) Another important di(cid:11)erence is that the data items in an assembly language program
are not typed, meaning they are not inherently speci(cid:12)ed as belonging to a particular
type such as integer or (cid:13)oating-point number. Assembly languages provide di(cid:11)erent
instructions for manipulating a data item as an integer or another data type. Thus,
it is the responsibility of the programmer to use the appropriate instructions when
manipulating data items. Even then, the number of data types supported in an
assembly-level architecture is fewer than that in the high-level architecture; only a
few simple data types such as integers, (cid:13)oating-point numbers, and characters are
supported. Thus, all of the complex data types and structures supported at the
HLL level must be implemented by the assembly language programmer using simple
primitives.

(cid:15) In assembly language programs, most of the control (cid:13)ow changes must be imple-
mented with branch instructions whose semantics are similar to those of the \go to"
statements used in HLLs.

(cid:15) The amount of work speci(cid:12)ed by an assembly language instruction is generally smaller
than that speci(cid:12)ed by an HLL statement5 . This means that several assembly language
instructions are usually needed to implement the equivalent of a typical HLL state-
ment.

(cid:15) Assembly language programs take much longer to debug, and are much harder to
maintain.

(cid:15) Finally, it is di(cid:14)cult for assembly language novices, particularly those with high-level
language experience, to think at a low enough level.

All of these factors make it di(cid:14)cult to program in assembly language. Apart from these
di(cid:14)culties, there is a practical consideration: an assembly language program is inherently
tied to a speci(cid:12)c instruction set, and must be completely rewritten to run on a machine
having a di(cid:11)erent instruction set. Because of these reasons, most of the programming done
today is in a high-level language.

3.1.9 The Rationale for Assembly Language Programming

Under the above circumstances, why would anyone want to program in assembly language
today? There are at least two reasons:

1. Speed and code size

2. Access to the hardware

5Exceptions to this general case are the vector instructions and the MMXT M instructions.

96

Chapter 3. Assembly-Level Architecture | User Mode

First of all, an expert assembly language programmer can often produce code that is much
smaller and much faster than the code obtained by compiling an equivalent HLL program.
For some applications, speed and code size are critical. Many embedded applications, such
as the code in a smart card, the code in a cellular telephone, the code in an anti-lock brake
control, device drivers, and the inner loops of performance-critical applications fall in this
category. Second, some functions need complete access to the hardware features, something
usually impossible to specify in high-level languages. Some hardware features have no ana-
logues in high-level languages. The low-level interrupt and trap handlers in an operating
system, and the device drivers in many embedded real-time systems fall into this category.
Similarly, an assembly language programmer may be able to make better use of special in-
structions, such as string copy instructions, pattern-matching instructions, and multimedia
instructions such as the MMXT M instructions. Many of these special instructions do not
have a direct equivalent in high-level languages, thereby forcing the HLL programmer to
use loops. Compilers, in most cases, cannot determine that such a loop can be replaced by
a single instruction, whereas the assembly language programmer can easily determine this.

3.2 Assembly-Level Interfaces

The assembly-level architecture attributes that we saw so far pertain to the assembly lan-
guage speci(cid:12)cation part of the architecture.
In many contexts, by assembly-level archi-
tecture, we just mean this assembly language speci(cid:12)cation. For serious programming in
assembly, we need to enhance the architecture by the following two additional parts:

(cid:15) assembly-level interface provided by libraries

(cid:15) assembly-level interface provided by the OS

Assembly Language
Programmer

Assembly Language
Specification
(User Mode)

Assembly−Level
Interface
(Library)

Assembly−Level
Interface
(OS)

Figure 3.4: Three Di(cid:11)erent Parts of Assembly-Level Architecture

3.3. Example Assembly-Level Architecture: MIPS-I

97

3.2.1 Assembly-Level Interface Provided by Library

3.2.2 Assembly-Level Interface Provided by OS

3.3 Example Assembly-Level Architecture: MIPS-I

We shall next take a more detailed look at assembly-level architectures. It is easier to do
so using an example architecture. The example architecture that we use is the well-known
MIPS-I assembly-level architecture [ref ]. We use the MIPS-I architecture because it is one
of the simplest architectures that has had good commercial success, both in the general-
purpose computing world and in the embedded systems world. The MIPS instruction set
architecture had its beginnings in 1984, and was (cid:12)rst implemented in 1985. By the late
1980s, this architecture had been adopted by several workstation and server companies,
including Digital Equipment Corporation and Silicon Graphics. Today MIPS processors are
widely used in Sony and Nintendo game machines, palmtops, laser printers, Cisco routers,
and SGI high-performance graphics engines. Although it is not popular anymore in the
desktop computing world, the availability of sophisticated MIPS-I simulators such as SPIM
makes it possible for us to develop MIPS-I assembly language programs and simulate their
execution. All of these features makes MIPS-I an excellent architecture for use in Computer
Architecture courses. Below, we look at some of the important aspects of the MIPS-I
architecture; interested readers may refer to [refs] for a more detailed treatment.

3.3.1 Assembly Language Alphabet and Syntax

The MIPS-I assembly language format is line oriented; the end of a line delimits an in-
struction. Each line can consist of up to four (cid:12)elds, as shown in Figure 3.5: a label (cid:12)eld,
an opcode (cid:12)eld, an operand (cid:12)eld, and a comment (cid:12)eld. The language is free format in the
sense that any (cid:12)eld can begin in any column, but the relative left-to-right ordering of the
(cid:12)elds must be maintained. For the sake of clarity, we will align the (cid:12)elds in each of our
code snippets.

Label

Opcode

Destination
Operand

Source
Operands

Comment

label1:

add

$t3, $t1, $t2

# add contents of $t1 and $t2, and put result in $t3

label2:

.word 20

# call next loc as label2, initialize 4−byte word to 20

Assembler Directive

Figure 3.5: Format of a MIPS-I Assembly Language Statement

98

Chapter 3. Assembly-Level Architecture | User Mode

A label is a symbolic name used to identify a memory location that is explicitly referred
to in the assembly language program. A label consists of any sequence of alphanumerical
characters, underscores ( ), dollar signs ($), or periods (.), as long as the (cid:12)rst character is
not a digit. A label must be followed by a colon. Labels are particularly useful for speci-
fying the target of a control-changing instruction and for specifying the memory location
corresponding to a variable.

After the optional label (cid:12)eld, the next (cid:12)eld speci(cid:12)es an opcode or an assembler directive.
An opcode speci(cid:12)es the operation to be done by the instruction.

The operand (cid:12)eld in an assembly language statement speci(cid:12)es the destination operand
and source operand(s) of the instruction. Operands are separated by commas, and the
destination operand (if present) appears in the leftmost position in the operand (cid:12)eld, except
for store instructions. For instance, in the assembly language instruction \add $t3, $t1,
$t2," the source operands are registers $t1 and $t2, and the destination operand is register
$t3. Numbers are interpreted as decimal, unless preceded by 0x or succeeded by H, either
of which denotes a hexadecimal number. Multiple instructions can be written in a single
line, separated by semicolons.

The comment (cid:12)eld, which comes last, begins with a sharp sign (#), and terminates at
the end of the line. Thus, all text from a # to the end of the line is a comment 6 . Just
as in high-level languages, the comments are only intended for human comprehension, and
are ignored by the assembler. A good comment helps explain a non-intuitive aspect of one
or more instructions. By providing additional insight, such a comment provides important
information to a future programmer who wants to modify the program.

3.3.2 Register Model

The MIPS-I assembly-level architecture models 32 general-purpose 32-bit integer registers
named $0 - $31, 32 general-purpose 32-bit (cid:13)oating-point registers named f0 - f31, and
three special integer registers named pc, hi, and lo. Two of the general-purpose integer
registers ($0 and $31) are also somewhat special. Register $0 always contains the value
0, and can never be changed. If an instruction speci(cid:12)es $0 as the destination register, the
register contents will not change when the instruction is executed. Register $31 can be used
as a general-purpose register, but it has an additional use as a link register for storing a
subroutine return address, as we will see in Section 3.4.6.

The (cid:13)oating-point arithmetic instructions use the FP registers as the source as well
as the destination. However, they can specify only the 16 even-numbered FP registers,
$f0, $f2, $f4, ...., $f30. When specifying an even-numbered FP register, if the operand
is a double-precision FP number, the remaining 32 bits of the number are present in the
subsequent odd-numbered FP register. For instance, when $f0 is speci(cid:12)ed for indicating a

6Although a line can contain nothing other than a comment, starting a comment from the very (cid:12)rst
column of a line is not a good idea. This is because most assemblers invoke the C preprocessor cpp, which
treat them as preprocessor commands.

3.3. Example Assembly-Level Architecture: MIPS-I

99

31

PC

0

zero
at
v0
v1
a0
a1
a2
a3
t0
t1
t2
t3
t4
t5
t6
t7
s0
s1
s2
s3
s4
s5
s6
s7
t8
t9
k0
k1
gp
sp
s8/fp
ra

lo

hi

$0
$1
$2
$3
$4
$5
$6
$7
$8
$9
$10
$11
$12
$13
$14
$15
$16
$17
$18
$19
$20
$21
$22
$23
$24
$25
$26
$27
$28
$29
$30
$31

fv0

fv1

ft0

ft1

ft2

ft3

fa0

fa1

ft4

ft5

fs0

fs1

fs2

fs3

fs4

fs5

FpCond

$f0
$f1
$f2
$f3
$f4
$f5
$f6
$f7
$f8
$f9
$f10
$f11
$f12
$f13
$f14
$f15
$f16
$f17
$f18
$f19
$f20
$f21
$f22
$f23
$f24
$f25
$f26
$f27
$f28
$f29
$f30
$f31

Figure 3.6: MIPS-I User Mode Register Name Space

double-precision operand, the 64-bit number is present in the register pair f$f0, $f1g.

Explain HI and LO registers

MIPS-I Assembly Language Conventions for Registers

The MIPS-I assembly language follows some conventions regarding the usage of registers.
These conventions are not part of the assembly-level architecture speci(cid:12)cations. This means
that if you write stand-alone assembly language programs that do not adhere to these
conventions, they are still guaranteed to be assembled and executed correctly. However,
if you do not adhere to these conventions, you cannot use the standard libraries and the
standard operating system, because they have been already compiled with the MIPS-I
compiler, which follows these conventions. Some of the important conventions are given
below:

100

Chapter 3. Assembly-Level Architecture | User Mode

(cid:15) Register $at ($1) is reserved for use by the assembler for computing certain memory
addresses.

(cid:15) Register $sp ($29) is reserved for use as the stack pointer.

(cid:15) The (cid:12)rst four integer parameters of a subroutine are passed through registers $a0-$a3
($4-$7). Thus, the subroutines in the standard libraries and operating system have
been developed with the assumption that their (cid:12)rst 4 parameters will be present in
these registers. Similarly, the (cid:12)rst two (cid:13)oating-point parameters are passed through
FP registers $fa0 and $fa1 ($f12 and $f14). The remaining parameters are passed
through the stack frame.

(cid:15) The integer return values of a subroutine are passed through registers $v0 and $v1
($2 and $3).

(cid:15) Register $v0 ($2) is used to specify the exact action required from the operating
system (OS) when executing a syscall instruction.

(cid:15) Registers $k0 and $k1 ($26 and $27) are reserved for use by the operating system.

Table 3.4 gives the names by which the MIPS registers are usually known. These names
are based on the conventional uses for the di(cid:11)erent registers, as explained above.

Register
Number

$0
$1
$2-$3
$4-$7
$8-$15
$16-$23
$24-$25
$26-$27
$28
$29
$30
$31
$f0, $f2
$f4, $f6, $f8, $f10
$f12, $f14
$f16, $f18
$f20, $f22, $f24, $f26, $f28, $f30

Register
Name
$zero
$at
$v0-$v1
$a0-$a3
$t0-$t7
$s0-$s7
$t8-$t9
$k0-$k1
$gp
$sp
$s8/$fp
$ra
fv0-fv1
ft0-ft3
fa0-fa1
ft4-ft5
fs0-fs5

Typical Use

Zero constant, destination of nop instruction
Assembler temporary; reserved for assembler
Values returned by subroutines
Arguments to be passed to subroutines
Temporaries used by subroutines without saving
Saved by subroutines prior to use
Temporaries used by subroutines without saving
Kernel uses for interrupt/trap handler
Global pointer
Stack pointer
Saved by subroutines, frame pointer
Return address for subroutines
Values returned by subroutines
Temporaries used by subroutines without saving
Arguments to be passed to subroutines
Temporaries used by subroutines without saving
Saved by subroutines prior to use

Table 3.4: Conventional Names and Uses of MIPS-I User Mode Registers

3.3. Example Assembly-Level Architecture: MIPS-I

101

3.3.3 Memory Model

The MIPS-I assembly-level architecture models a linear memory address space (i.e., a (cid:13)at
address space) of 231 memory locations that are accessible to user programs. These locations
have addresses ranging from 0 to 0x7fffffff, as indicated in Figure 3.7. Each address refers
to a byte, and so a total of 2 Gbytes of memory can be addressed. Although each address
refers to a single byte, MIPS-I provides instructions that simultaneously access one, two,
three, or four contiguous bytes in memory. The most common access size is 4 bytes, which
is equal to the width of the registers. Although a location can be speci(cid:12)ed by its address,
the common form of speci(cid:12)cation in assembly language is by a label or by an index register
along with an o(cid:11)set. In the latter case, the memory address is given by the sum of the
register contents and the o(cid:11)set.

Memory Address Space

Addresses
0x00000000

0x00400000

0x10000000

Reserved

.text

.data

Heap

Direction of growth
at run−time

Stack

0x7fffffff

Figure 3.7: Organization of MIPS-I User Memory Address Space

MIPS-I Assembly Language Conventions for Memory

Like the case with registers, there are MIPS-I assembly language conventions for the memory
address space also. Again, these conventions are not part of the assembly-level architecture
speci(cid:12)cations. Figure 3.7 indicates the conventional organization of the MIPS-I memory
address space, in terms of where the di(cid:11)erent sections of the memory start. The conventional
starting points for the .text and .data sections are at addresses 0x400000 and 0x10000000,

102

Chapter 3. Assembly-Level Architecture | User Mode

respectively. The user run-time stack starts at address 0x7fffffff, and grows towards the
lower memory addresses. General-purpose register $sp is generally used as the stack pointer;
stack operands are accessed by specifying an o(cid:11)set value that is to be added to the stack
pointer. Thus, operands that are buried within the stack can also be accessed. Local
variables of a subroutine are allocated on the stack. If a subroutine needs the stack to grow,
for allocating local variables and temporaries, it decrements $sp by the appropriate amount
at the beginning, and increments $sp by the same amount at the end.

3.3.4 Assembler Directives

The MIPS-I assembly-level architecture supports several assembler directives, all of which
are written with a dot as their (cid:12)rst character. Below, we describe some of the commonly
used directives.

(cid:15) .rdata: indicates that the subsequent items are to be stored in the read-only data
section.

(cid:15) .data: indicates that the subsequent items are to be stored in the .data section.

(cid:15) .text: indicates that the subsequent items are to be stored in the .text section.

(cid:15) .comm, .lcomm: are used to declare uninitialized, global data items. These direc-
tives are commonly used when the initial value of a variable is not known at program-
ming time. An item declared with the .comm directive can be accessed by all modules
that declare it. (The linker allocates memory locations for such an item in the .bss
or .sbss section.) An item declared with the .lcomm directive is a global variable
that is accessible within a single module. (The assembler allocates memory locations
for such an item in the .bss section or in the .sbss section.)

(cid:15) .byte, .half, .word: These directives are used to set up data items that are 1, 2,
and 4 bytes, respectively. In contrast to the .comm directive, these directives provide
the ability to initialize the data items. Example declarations using these directives
are given below. The last two declarations correspond to arrays, and are extensions
of the one used for specifying a single item.

b:

h:

w:

.byte

.half

.word

5

5

5

ba:

.byte

0:5

wa:

.word

1:2, 4

# Allocate an 1-byte item with initial value 5
# at next memory location, and name it b
# Label next memory location as h;
# allocate next 2 bytes for a 2-byte item with initial value 5
# Label next memory location as w;
# allocate next 4 bytes for a 4-byte item with initial value 5
# Label next memory location as ba;
# allocate next 5 bytes to 5 1-byte items with initial value 0
# Label next memory location as wa;
# allocate next 12 bytes to 3 4-byte items with initial values 1, 1, 4

3.3. Example Assembly-Level Architecture: MIPS-I

103

In these directives, if no integer value is speci(cid:12)ed after .byte, .half, .word, then the
item is initialized to zero.

(cid:15) .(cid:13)oat:
is used to specify a 4-byte data item that is initialized to a single-precision
(cid:13)oating-point number.

(cid:15) .ascii and .asciiz: are used to declare ASCII strings, without and with a terminating
null character, respectively. In the following example, both directives de(cid:12)ne the same
string.

a:
z:

.ascii "goodn0"
.asciiz "good"

# Place string \good" in memory at location a
# Place string \good" in memory at location z

(cid:15) .space: is used to increment the current section’s location counter by a stipulated number of
bytes. This directive is useful to set aside a speci(cid:12)ed number of bytes in the current section.

s:

.space 40

# Label current section’s location counter as s
# and increment it by 40 bytes

(cid:15) .globl:
indicates that the subsequent variable name or label is globally accessible,
and can be referenced in other (cid:12)les. For example,

.data
.globl g
.word
0

# Subsequent items are stored in the data section
# Label g (in this case, a variable) is global
# Declare a 4-byte item named g with initial value 0

.text
.globl f
subu
$sp, 24

# Subsequent items are stored in the text section
# Label f (in this case, the beginning of a function) is global
# Decrement stack pointer register by 24

g:

f:

(cid:15) .align: is used to specify an alignment. The alignment is speci(cid:12)ed as a power of 2.

3.3.5 Assembly-Level Instructions

We have already seen some of the instructions and addressing modes of the MIPS-I assembly-
level architecture. The MIPS-I architecture supports the following addressing modes: reg-
ister direct, memory direct, register-relative, immediate, and implicit. Let us now examine
the MIPS-I instruction set in a more comprehensive manner 7 .
The MIPS-I architecture is a load-store architecture, which means that only load
instructions and store instructions can access main memory. The commonly used lw (load
word) instruction fetches a 32-bit word from memory. The MIPS-I architecture also provides

7The assembly language examples in this book use only a subset of the MIPS-I assembly-level instruction
set. A complete description of the MIPS-I assembly-level instruction set is available in Appendix *.

104

Chapter 3. Assembly-Level Architecture | User Mode

separate load instructions for loading a single-byte (lb opcode) and a 2-byte half-word (lh
opcode).

As we saw earlier, conventional MIPS assembly language designates a portion of the
memory address space as a stack section. Special stack support such as push and pop
instructions are not provided, however. Locations within the stack section of the memory
space are accessed just like the remaining parts of memory.

Arithmetic and Logical Instructions: MIPS-I provides many instructions for per-
forming arithmetic and logical operations on 32-bit operands. All arithmetic instructions
and logical instructions operate on register values or immediate values. While the register
operands are always 32 bits wide, the immediate operands can be shorter. These instruc-
tions explicitly specify a destination register, and two source operands, of which one is a
register and the other is a register or immediate value.

Control-changing Instructions: The MIPS-I architecture includes several conditional
as well as unconditional branch instructions. The conditional branch instructions base their
decisions on the contents of one or two registers.

System Call Instructions: The MIPS-I architecture provides a syscall instruction for
user programs to request a variety of services from the operating system (OS). The speci(cid:12)c
service requested is indicated by a code value stored in register $v0. The list of services
supported and the speci(cid:12)c code for each service varies from one OS to another 8 .

SPIM’s ABI | System Calls Supported by SPIM: The SPIM simulation tool
includes a very simple operating system that supports a small set of system calls. These
system calls are listed in Table 3.5. If you are familiar with the standard calls supported
by di(cid:11)erent UNIX-style and Windows-style operating systems, you will notice that these
system calls are somewhat di(cid:11)erent. Apart from the much smaller set of calls supported,
the functionality provided by the SPIM calls is not very primitive and is more at the level
of library routines.

3.3.6 An Example MIPS-I AL Program

We are now familiar with the syntax of the MIPS-I assembly language. Next, we shall put
together some of the concepts we learned by writing a simple MIPS-I assembly language

8The di(cid:11)erences in the range of services and system call codes between di(cid:11)erent OSes imply that if a user
program uses syscall instructions to directly request the OS for services|instead of going through library
functions|then the program may not be portable across di(cid:11)erent OSes. Thus, a syscall-laden assembly
language program targeted speci(cid:12)cally for the SPIM OS will most likely not run correctly on an ULTRIX
OS host machine.

3.3. Example Assembly-Level Architecture: MIPS-I

105

OS Service Code in $v0
1
print int
2
print float
3
print double
4
print string
5
read int
6
read float
7
read double

read string

sbrk
exit

8

9
10

Arguments
$a0: integer to be printed
$fa0: (cid:13)oat to be printed
$fa0: double to be printed
$a0: address of string to be printed

Return Value

$v0: integer read
$fv0: (cid:13)oat read
$fv0: double read

$a0: address for placing read string;
$a1: number of bytes
$a0: amount

$v0: address

Table 3.5: System Calls Supported by SPIM

program. Let us write an assembly language program that prints the familiar \hello,
worldnn" string, whose C code and Java code are given below.

Program 4 The Hello World! program in Java.

main() {
// Display the string
printf("hello, world!");

}

class helloworld {
public static void main(String[] args) {
// Display the string
System.out.println("hello, world!");

}

}

For writing the MIPS AL program, we use the ABI (application binary interface) sup-
ported by SPIM. This will help motivated students to ‘execute’ this program using any of
the fspim, xspim, PCSpimg tool set. One point to note when using these tools to ‘execute’
assembly language programs is that prior to execution, the tools assemble the program into
the equivalent machine language program. Thus, the tools directly simulate the execution
of machine language instructions, and not the assembly language instructions. The mem-

106

Chapter 3. Assembly-Level Architecture | User Mode

ory map displayed by these tools therefore indicates machine language instructions, and not
assembly language instructions.

##################################################################################
# data section
##################################################################################
# Store subsequent items in the .data section
.data
# Label next location as string1
string1:
.asciiz "hello, worldnn" # Allocate subsequent bytes and store string "hello, worldnn"

##################################################################################
# text section
##################################################################################
# Store subsequent items in the .text section
.text
#
.globl
# Program execution begins here
start:
# Place the memory address labeled string1 in $a0
la
# Place code for print string system call in $v0
li
# Call OS to print the string
syscall

$a0, string1
$v0, 4

start

li
syscall

$v0, 10

# Place code for exit system call in $v0
# Call OS to terminate the program

This MIPS-I AL program contains only 2 sections: .data and .text. The .data section
has a single declaration|that of the \hello, worldnn" string, which is declared using the
.asciiz directive, and given the label string1. The string therefore starts at label string1.

The .text directive tells the assembler to place the subsequent items in the .text
section. The
start label indicates that execution of the program should begin at that
point. Although we have placed the
start label just before the very (cid:12)rst instruction in
this program, this is not a rule; the
start label can be placed anywhere within the .text
section. In order to print the string, we have to use the services of the operating system
via a syscall instruction.
In this example code, we have used the syscall convention
followed by SPIM, a simulator for the MIPS-I architecture. The address of the string (i.e.,
label string1) is placed in register $a0, and the code for the print string system call in
SPIM (which is 4) is placed in register $v0, prior to the syscall instruction. Notice that
this program will not run on a standard MIPS host, because standard OSes use a di(cid:11)erent
ABI. The standard ABI does not support the print string system call, and instead uses
the code 4 for the write system call.

Finally, after printing \hello, worldnn", the program should terminate. It does so, via
another syscall instruction, after placing in register $v0 the value 10, which is the code
for the exit system call in SPIM. (The standard ABI de(cid:12)nes a code value of 1 for the exit
system call, in contrast to the value of 10 used by SPIM’s ABI.)

3.4. Translating HLL Programs to AL Programs

107

3.3.7 SPIM: A Simulator for the MIPS-I Architecture

Let us elaborate on the SPIM simulator, as it is a very useful tool for learning MIPS-I
assembly language programming. This simulator was developed by Prof. James Larus at
University of Wisconsin-Madison, and is available to the public from an ftp site at that
University. The simulator comes in 3 di(cid:11)erent (cid:13)avors: spim, xspim, and PCSpim. Among
these, spim is the most basic one, providing a terminal-style interface on Unix/Linux hosts,
and a DOS interface or console interface on Windows hosts. The xspim and PCSpim tools
are fancier and provide graphical user interfaces (GUI). xspim runs on Unix/Linux hosts
and provides an X window interface, whereas PCSpim runs on Windows hosts and provides
a Windows interface.

To execute the above program with xspim in a Unix/Linux host, type the command:
xspim -notrap &. The -notrap option tells SPIM not to add its own start-up code, and
to begin execution at the start label. An xspim window pops up, as shown in Figure 3.8.
You can use the load button in this window to read and assemble your MIPS-I assembly
language program. If any errors are generated during assembly, they are indicated in the
bottom portion of the xspim window. After (cid:12)xing the bugs in the AL program (cid:12)le, you can
reload the program (cid:12)le into xspim by (cid:12)rst clearing the memory and register contents|using
the memory & registers option in the clear button|and then using the load button to
reload the program.

If SPIM has successfully assembled your program, then you can use the run or step
buttons in the xspim window to execute your program.
In this case, a console window
will be automatically opened to display hello world. xspim provides many other useful
features such as breakpoints and a debugger, which make it even more attractive than a
real MIPS host for developing MIPS-I assembly language programs. You are encouraged to
consult the SPIM manual for learning and using these features.

3.4 Translating HLL Programs to AL Programs

We just saw the rudiments of writing a complete program in MIPS-I assembly language, and
‘executing’ it on the SPIM simulator. We shall next take a look at how a high-level language
program is translated to assembly language. After all, most of today’s programming is done
in a high-level language. We shall revisit direct programming in assembly language when
we discuss device drivers and exception handlers in the next chapter.

The translation of a program from a high-level language to the assembly language is
typically done by a program called a compiler. A detailed treatment of the algorithms
used by a compiler is beyond the scope of this book; therefore we restrict our discussion
to a sketch of the important ideas. For illustrating the translation process, we again use
the MIPS-I assembly language. We illustrate the utility of various instruction types with
practical examples of translation from C language to MIPS-I assembly language.

108

Chapter 3. Assembly-Level Architecture | User Mode

Figure 3.8: A Sample xspim Window

3.4.1 Translating Constant Declarations

Constants are ob jects whose values do not change during program execution. Translating a
constant declaration typically involves assigning memory location(s) for the constant. The
number of locations required for a constant depends on its size. The memory location(s)
will hold the constant’s value (a pattern of 0s and 1s) during the execution of the program.
Notice that in machines that support only aligned words, integers and (cid:13)oating-point con-

3.4. Translating HLL Programs to AL Programs

109

stants need to start on a word boundary. The standard practice is to allocate them in the
.rdata section of memory.

#define const 32000

Sample Constant Declaration

One way of translating this HLL constant declaration to assembly language is given
below. In this translation, we have used a label to specify a memory address that will store
the value of the constant. For clarity, we have named this label with the same name as
the HLL constant name. The .rdata directive tells that the subsequent items need to be
placed in the .rdata (read-only data) section 9 . The HLL constant const is allocated space
in memory with the .word directive, which allocates 4 bytes. Subsequent instructions that
use this constant value will read it from memory.

# Assign memory locations for the constants and initialize them
# Subsequent items are stored in the .rdata section
.rdata
# Label next memory location as const;
const:.word
# allocate next 4 bytes for constant const

32000

It is important to note here that a compiler may not always translate constants in this
manner. Small constants are often not often allocated to memory locations or registers.
Instead, the compiler explicitly speci(cid:12)es the value of the constant in instructions that use
that constant. Consider the following code snippet.

#define small 2
main()
f

var i, j;
i = small;
j = small * small;

g

Declaration and Uses of a Small Constant

This code can be re-written as follows:

main()
f

g

var i, j;
i = 2;
j = 2 * 2;

9Certain run-time tables are also allocated in the same section of memory as the constants. Such an
example is available in page 122, where we discuss jump tables for translating switch statements.

110

Chapter 3. Assembly-Level Architecture | User Mode

Substituting Occurrences of a Small Constant by its Value

References to small constants can often be directly speci(cid:12)ed in an instruction itself as
immediate operands. This is discussed in Section 3.7.1.

||||||||||||||||||||||||{

Floating-Point Constants: Floating-point constants take up many bits even if the value
they represent is small. In modern architectures, the minimum number of bits required to
represent a (cid:13)oating-point number is 32 bits. Therefore, (cid:13)oating-point constants are seldom
explicitly speci(cid:12)ed within instructions as immediate values. Instead, they are allocated to
memory locations just like what is done for large integer constants. Consider, for example,
the following C program:

#include <stdlib.h>
#include <stdio.h >
int main( int argc, char **argv, char **envp )
{

static int j;
static double i = 1.0;
static double a[8] = {0,1,2,3,4,5,6,7};

j = 0;
a[j] = i + 1.0;

}

|||||||||||||||||||||||||{

3.4.2 Translating Variable Declarations

High-level language programs typically have a number of variable declarations, both global
and local. Global variables are visible throughout the program, whereas local variables are
visible only when the block in which they are declared are active. Translation of a vari-
able declaration typically involves assigning memory location(s) for the variable; optimizing
compilers may allocate some variables to registers in order to speed up the program’s exe-
cution. The number of locations required for a variable depends on its type. The memory
location(s) or register(s) will hold the variable’s value (a pattern of 0s and 1s) during the
execution of the program. Notice that in machines that support only aligned words, integers
and (cid:13)oating-point numbers need to start on a word boundary.

3.4. Translating HLL Programs to AL Programs

111

Variables declared in a high-level language are generally allocated locations in one of
three sections in memory: the .data section, the run-time stack section, and the heap
section. Variables that persist across function invocations, such as global variables and
static variables, are allocated memory locations in the .data section. Variables that do not
persist across function invocations, such as local variables, are generally allocated locations
in the stack section. Dynamic variables created at execution time and accessed through
pointers are allocated memory locations in the heap section.

3.4.2.1 Global Variables

Consider the following global variable declarations in C.

int i = 0;
int a = 12;
struct f
char name[6];
int length;
g record;
float f = 0.5;
char *cptr;

Sample Global Variable Declarations

One way of translating these HLL variable declarations to assembly language is given
below. Again, we use labels to specify memory addresses that correspond to variables. For
clarity, in our examples, the label assigned to the memory location corresponding to an
HLL variable generally has the same name as the HLL variable name. The .data directive
tells that the subsequent items need to be placed in the .data section. We have allocated a
contiguous space in memory for HLL variables i, a, record, f, and cptr. Variables i and
a require four bytes each, and are allocated memory using the .word directive. The struct
variable record has 2 (cid:12)elds: name and length. The (cid:12)eld name, an array of six characters,
requires one byte per character. The (cid:12)eld record.length is an integer, and therefore starts
at the next word boundary. Thus, a total of 12 bytes are allocated for the HLL variable
record. The HLL variable f of type float is allocated with the .float directive, and
occupies 4 bytes in memory. Finally, the HLL pointer variable cptr is allocated space
in memory with the .word directive. Notice that in an assembly language program, the
memory location assigned to an HLL variable does not identify the data type. Here, the
same directive is used, for instance, for allocating integers as well as pointers.

# Assign memory locations for the global variables and initialize them
# Subsequent items are stored in the .data section
.data
# Label next memory location as i;
.word
# allocate next 4 bytes for int variable i

0

i:

112

Chapter 3. Assembly-Level Architecture | User Mode

a:

.word

12

record:
.byte
.word

0:6
0

f:

.float 0.5

# Label next memory location as a;
# allocate next 4 bytes for int variable a

# Label next memory location as record
# Allocate next 6 bytes for record.name
# Allocate next 4 bytes for record.length

# Label next memory location as f;
# allocate next 4 bytes for (cid:13)oat variable f

cptr: .word

NULL

# Label next memory location as cptr;
# allocate next 4 bytes for pointer variable cptr

For the HLL variables, we have allocated memory locations in sequential order in the
.data section, but the exact addresses are not speci(cid:12)ed. When this program is translated
to machine language by the assembler, speci(cid:12)c addresses will be assigned. You can verify
this by loading the above code in SPIM, and looking at the DATA display of the xspim
window. Figure 3.9 gives one such memory allocation assuming these data items to start at
location 0x10010000. The assembler has allocated a contiguous space in memory for all of
the items declared in the .data portion of the assembly code given above. Thus, variables
i and a, which require four bytes each, are mapped to locations 0x10010000-0x10010003
and 0x10010004-0x10010007, respectively. The locations corresponding to struct variable
record begin at address 0x10010008, with the (cid:12)rst 6 locations corresponding to the 6
characters in record.name. Because of the automatic alignment of the .word directive on
a word boundary, the memory locations corresponding to record.length start at the next
word boundary, 0x10010010. Locations 0x1001000e and 0x1001000f are therefore unused.
Had we placed the .align 0 directive after the .data directive in the above code, then
record.length would have been mapped to locations 0x1001000e-0x10010011. However,
all accesses to record.length then become complicated, as we will see later.

In the above assignment in a machine language, all of the initialized variables (i, a, and
f) and the uninitialized variables (record and cptr) were allocated together in the same
section of memory. When a machine language program is shipped, the .text section as well
as the initialized .data section need to be shipped. The assembly language programmer
can help to reduce the size of the shipped program by allocating uninitialized data items
using the .comm or .lcomm directives instead of the .word directive. The assembler and
linker would then allocate memory locations for such data items in a separate data section,
called .bss. This uninitialized data section need not be included in the shipped program.
Notice that the .comm directive is not supported by SPIM; so, you cannot try this with
SPIM.

3.4. Translating HLL Programs to AL Programs

113

AL Sections

AL Labels

Memory Address Space HLL Variables

.data

i:
a:
record:

f:
cptr:

0
0

0
12

0
0

0
0

0
0.5
NULL

0
0

i
a

name

length
f
cptr

record

Figure 3.9: A Memory Map View of the Assembly Language Program Snippet Implementing
Global Variables

3.4.2.2 Local Variables

Next let us turn our attention to local variables (or automatic variables), which are de(cid:12)ned
inside a subroutine, and are active only when the subroutine is active. Two tpes of allocation
are possible for local variables: static allocation and global allocation.

Static Allocation:
In this type of allocation, the local variables are assigned memory
locations in the .data section, just like the globals. This avoids the overhead of creation
and destruction of a work space for every subroutine instance. However, this is possible
only if the subroutine is non-recursive and non-reentrant, such as in Fortran 77 (an earlier
version of Fortran). In these languages only one instance of a given subroutine can be active
at a time.

Dynamic Allocation: Although we can assign them memory locations along with the
globals, such an approach has some drawbacks:

(cid:15) The local variables of all subroutines will be occupying memory space throughout the
entire execution of the program, irrespective of whether they are active or not.

(cid:15) More importantly, if a subroutine is recursive, then multiple instances of a local vari-
able de(cid:12)ned in that subroutine will map to the same memory location. The newer

114

Chapter 3. Assembly-Level Architecture | User Mode

instances of the subroutine may therefore overwrite the values stored by the earlier,
but still active, instances of the subroutine.

In the ensuing discussion, we only consider dynamic allocation. Conceptually, the local
variables of a subroutine instance should be \created" only when the subroutine instance
comes into existence, and should be \destroyed" when the instance (cid:12)nishes. To do this, the
allocation of these variables should happen at run-time as opposed to during translation.
Thus, when a subroutine calls another, a new set of local variables are created for the callee,
although the caller’s local variables are still active. The (cid:12)rst subroutine to complete will
be the one that is called last, and the local variables to be destroyed (cid:12)rst are the ones that
were created last in the nested call sequence. That is, the local variables are created and
destroyed in a last-in (cid:12)rst-out (LIFO) order. This suggests that the local variables could
be allocated at run-time in a stack-like structure; the LIFO nature of stack-like structures
(cid:12)ts naturally with the LIFO nature of subroutine calls and returns.

A natural place to allocate the local variables, then, is in the stack section discussed
in Section 3.1.2. A convenient way of doing this allocation is to designate for each active
subroutine a contiguous set of locations in the stack section called a stack frame or
activation record. The stack frame constitutes a private work space for the subroutine,
created when entering the subroutine and freed up when returning from the subroutine. If
the subroutine requires more space for its local variables, it can obtain the space it needs
by raising the top of stack.

We shall illustrate the use of stack frames using an example code. Consider the following
C code. We have included only the main() function for simplicity. This function declares
two local variables, x and y.

int i = 0;
int a = 12;

main()
f

int x = 5;
int y;

...

g

Example Global Variable and Local Variable Declarations

One way of translating these variable declarations to assembly language is given below. A
stack frame of 12 bytes is created for the main subroutine by the subu $sp, 12 instruction,
which forms the subroutine prologue code. The local variables x and y are allocated 4 bytes

3.4. Translating HLL Programs to AL Programs

115

each in the stack section. Unlike the global variables, the exact addresses assigned to
these local variables are not determined when generating the equivalent machine language
program. These addresses will be determined only at run-time, based on where the stack
frame for main() gets allocated. Figure 3.10 illustrates this memory allocation. Part
(a) of the (cid:12)gure shows the memory map prior to entering main(), and part (b) shows
the same after entering main() and executing the subu $sp, 12 instruction. The newly
created stack frame for main() is deleted prior to leaving main() by executing the addu
$sp, 12 instruction, which forms the subroutine epilogue code. You can load this assembly
language program in xspim and ‘execute’ it to see the allocation of local variables. When
running programs having a main label, it is better to run xspim without the -notrap option,
allowing SPIM to append a start-up code at the beginning of the program. The start-up
code performs some initialization, and then calls the main subroutine.

# Assign memory locations for the global variables
# Initialize memory locations if necessary
# Store subsequent items in the data section
.data
# Allocate a 4-byte item with initial value 0
.word
# at next memory location, and label it i
# Allocate a 4-byte item
# at next memory location, and label it a

.word

0

12

i:

a:

.text
.align 2
.globl main
.ent
main

main:

# Entry point of subroutine main (optional)

# Assign memory locations for the local variables of main()
# Decrement $sp to allocate a 12-byte stack frame
subu
$sp, 12
.frame $sp, 12, $ra # Stack frame is accessed with $sp; frame size is 12 bytes;
# return address is in $ra

li
sw
...
...
addu
jr
.end

$t1, 5
$t1, 8($sp) # Initialize local variable x to 5

$sp, 12
$ra
main

# Increment $sp to delete the current stack frame
# Return from subroutine main
# End point of subroutine main (optional)

3.4.2.3 Dynamic Variables

A global variable is assigned a (cid:12)xed memory location (or set of neighboring locations) at
compile time, and remains (cid:12)xed throughout the execution of the program. Such variables
are assigned memory locations within the .data section. A local variable, on the other
hand, begins to exist from the time the corresponding subroutine is called. It is disposed

116

Chapter 3. Assembly-Level Architecture | User Mode

AL Sections

AL Labels

Memory Address Space

HLL Variables

AL Sections

AL Labels

Memory Address Space

HLL Variables

.data

i:
a:

0
12

i
a

.data

i:
a:

0
12

i
a

global variables

stack

Stack frame
main()
for

stack

y
x

local variables
of
main()

Register

sp

Register

sp

(a) Memory Map Before Entering main()

(b) Memory Map After Entering main()

Figure 3.10: A Memory Map View of the Assembly Language Program Snippet Implement-
ing Local Variables

of when control is passed back to the calling routine. A convenient place to allocate such a
variable is in the stack frame that will be created when the subroutine is called.

Finally, a dynamic variable (pointed to by a pointer) is created during program execution
(by calling a library function such as malloc() or calloc() in C, which returns the memory
address assigned to the variable).
Such a variable continues to exist until the allotted
memory locations are explicitly freed (by calling a library routine such as free()). Like
local variables, dynamic variables are also not assigned memory locations at compile time,
and are instead assigned memory locations at run-time as and when they are created. Unlike
local variables, however, the run-time allocation is done not in the stack section, but in the
heap section. This is because these data items need to be active even after the subroutines
in which they were created (cid:12)nish execution.

We shall illustrate the allocation of dynamic variables using an example code. Consider
the following C code. We have included only the main() function for simplicity. This
function declares two local variables, x and y.

int i = 0;

3.4. Translating HLL Programs to AL Programs

117

main()
f

g

int x, *y;

y = (int *)malloc(8);
...

Example Global Variable, Local Variable, and Dynamic Variable Declarations

# Assign memory locations for the global variables
# Initialize memory locations if necessary
# Store subsequent items in the data section
.data
# Allocate a 4-byte item with initial value 0
.word
# at next memory location, and label it i
# Allocate a 4-byte item
# at next memory location, and label it a

.word

0

12

i:

a:

.text
.align 2
.globl main
main
.ent

main:

# Entry point of subroutine main

# Assign memory locations for the local variables of main()
# Decrement $sp to allocate a 12-byte stack frame
subu
$sp, 12
# for storing $ra and for variables x and y
.frame $sp, 12, $ra # Stack frame is accessed with $sp; frame size is 12 bytes
# return address is in $ra
$ra, 0($sp) # Save register $ra’s contents on stack
$a0, 8
# Call subroutine malloc to allocate a 4-byte item
malloc
$v0, 4($sp) # Update variable y with the address of dynamic variable

$ra, 0($sp) # Load register $rawith previously saved value
# Increment $sp to delete the current stack frame
$sp, 12
$ra
main

# End point of subroutine main

sw
li
jal
sw
...
...
lw
addu
jr
.end

3.4.2.4 Symbol Table

Keeping track of the mapping between the HLL variables and memory locations can be quite
tedious for an assembly language programmer, but not for the compiler. Most assembly

118

Chapter 3. Assembly-Level Architecture | User Mode

AL Sections

AL Labels

Memory Address Space

HLL Variables

AL Sections

AL Labels

Memory Address Space

HLL Variables

.data

i:
a:

0
12

Stack frame
main()
for

stack

i
a

y
x

i:
a:

.data

heap

0
12

i
a

global variables

dynamic variable

Stack frame
main()
for

stack

y
x

local variables
of
main()

Register

sp

Register

sp

(a) Memory Map After Entering main()

(b) Memory Map After Executing malloc()

Figure 3.11: A Memory Map View of the Assembly Language Program Snippet Implement-
ing a Dynamic Variable

languages provide the ability to symbolically specify memory locations by labels. Each
symbolic name in the assembly language program is eventually replaced by the appropriate
memory address during the assembly process. The compiler typically keeps track of variable
allocations through a data structure called symbol table. Each entry of the symbol table
corresponds to one variable, and has enough information for the compiler to remember
the memory locations or registers allocated for the variable. As and when the compiler
encounters a variable declaration, it creates a new entry for the variable in the symbol
table. The symbol table information is useful for displaying the addresses of symbols during
debugging or analyzing a program.

3.4.3 Translating Variable References

The compiler (or assembly language programmer) needs to generate the proper sequence of
data movement operations whenever an HLL variable is referenced. For doing this, it has
to remember where in memory (or register) the variable has been allocated. For variables
encompassed in complex data structures such as array elements and structure elements,
accessing the variable also involves calculating the correct address of the variable. Consider

3.4. Translating HLL Programs to AL Programs

119

the following assignment statements involving the previously declared global variables.

record.length = i;
cptr = &(record.name[3]);
record.name[5] = *cptr;

Some machines provide instructions to perform memory-to-memory arithmetic. In such
machines we may be able to translate arithmetic expressions without copying the variable
values to registers. However, in load-store machines such as the MIPS-I, we must (cid:12)rst
load the variable values into registers from their corresponding memory locations. Let us
translate the above assignment statements to MIPS-I assembly code. Notice that these
statements cannot be executed in SPIM without including the appropriate .data declara-
tions.

.text
lw
la
sw
addu
sw
lw
lw
sw

# Store subsequent items in the text section
# Copy the value in memory location named i into $t1
$t1, i
$t2, record # Copy the memory address named record into $t2
$t1, 8($t2) # Copy $t1 into memory location allocated to record.length
$t3, $t2, 3 # Calculate address of record.name[3]
# Store the address into memory location named cptr
$t3, cptr
# Copy the value in memory location cptr to $t3
$t3, cptr
$t4, 0($t3) # Copy the value in mem location pointed to by cptr to $t4
$t4, 5($t2) # Store $t4 into memory location allocated to record.name[5]

3.4.4 Translating Conditional Statements

All high-level languages support if statements, which cause some statements to be executed
only if a condition is satis(cid:12)ed. Consider the following C code:

if (i < a)
cptr = record.name;

else

cptr = &(record.name[1]);

To translate this if statement to assembly language, it is easier to (cid:12)rst rewrite this code
using goto statements, because assembly languages usually do not provide if-else con-
structs. The above code can be rewritten as follows:

if (i >= a)
goto else1;
cptr = record.name;
goto done;
else1: cptr = &(record.name[1]);
done:
...

120

Chapter 3. Assembly-Level Architecture | User Mode

We can implement this modi(cid:12)ed C code in assembly language with the use of condi-
tional branch (or conditional jump) instructions, which permit the skipping of one or more
instructions. The exact manner in which a conditional branch checks conditions depends
on the machine. Some machines provide condition codes that are set by arithmetic instruc-
tions and can be tested by conditional branch instructions. Some others like MIPS-I do not
provide condition codes, and instead let conditional branches check the value in a register.

A translation for the above C code to MIPS-I assembly code is given below. In this code,
the if condition evaluation is done using a bge instruction. Notice that in the high-level
language, when an if condition is satis(cid:12)ed, the statement(s) following the if statement is
(are) executed. In the assembly language, by contrast, when a branch condition is satis(cid:12)ed,
the instruction(s) following the branch instruction is (are) skipped. Therefore, we need to
use the complement of the if condition as the branch condition. In this example, the C code
checks for the condition if (i < a); the assembly code checks for the branch condition
bge (branch if greater or equal). Also notice that in the high-level language, once execution
goes to the then portion, the else portion is automatically skipped. However, the assembly
language does not provide such a support, and so an unconditional branch instruction is
used just before the else portion to skip the else portion whenever control goes to the then
portion.

.text
lw
lw
bge
la
sw
b
else: la
addu
sw
done: ...

# Copy the value in memory location i into $t1
$t1, i
# Copy the value in memory location a into $t2
$t2, a
$t1, $t2, else # Branch to label else if $t1 (i) (cid:21) $t2 (a)
# Copy the memory address named record into $t1
$t1, record
# Copy the value in $t1 into memory location named cptr
$t1, cptr
# Branch to label done (to skip the else portion)
done
# Copy the memory address named record into $t1
$t1, record
# Increment $t1 so as to obtain address of record.name[1]
$t1, 1
# Copy the value in $t1 into memory location named cptr
$t1, cptr

A series of if-else statements based on a single variable can be expressed as a multi-
way branching statement using the C switch statement. Consider the following C code. It
contains a switch statement, with 5 di(cid:11)erent cases including the default case.

switch (record.length)
f
case 0:
case 1:

*record.name = ’L’;
break;

case 2:
case 3:

3.4. Translating HLL Programs to AL Programs

121

*record.name = ’M’;
break;
default:
*record.name = ’H’;

g

A trivial way of translating this switch statement is to (cid:12)rst express it as a series of if-
else statements, and then translate them as we just did. However, such an approach may
be very ine(cid:14)cient, especially if there are many cases to consider. A more e(cid:14)cient translation
can be performed by incorporating a software structure called jump table. A jump table
is an array that stores the starting addresses of the code segments corresponding to the
di(cid:11)erent cases of a switch statement. It is indexed by the value of the switch statement’s
control variable.

In the next page we show an assembly language translation of the switch statement
given above. This code begins with the jump table declaration, which is stored in the read-
only data section. The jump table starts at label JT, and contains 4 entries, corresponding
to values 0-3 for record.length. These 4 entries are initialized to 4 labels that are declared
in the .text section.

The .text portion (cid:12)rst reads the value of record.length from memory. Then the
default case is handled by checking if the value of this variable is greater than or equal
to 4.
If the
If this condition is satis(cid:12)ed, then control is transferred to label default.
condition is not satis(cid:12)ed, then we need to index into the jump table. The jump table index
is calculated by scaling the value of record.length by 4, as each table entry occupies 4
bytes. For instance, if record.length has a value of 3, then the mul instruction scales it by
4 to obtain an index of 12. The subsequent lw instruction adds this o(cid:11)set to the jump table
starting address JT, and loads the target address into $t6. The next instruction performs
an unconditional jump to the target address stored in $t6.

# Store subsequent items in the read only data section
.rdata
#####################################################################
# Begin jump table here, and initialize it with target addresses
JT:

case0
.word
case1
.word
case2
.word
.word
case3
#####################################################################

# Store subsequent items in the text section
.text
# Align next item on a 22 byte (32-bit word) boundary
.align 2
# Load starting address of variable record in $t1
la
$t1, record
# Copy contents of memory location record.length to $t6
$t6, 8($t1)
lw
$t6, 4, default # Branch to label default if record.length (in $t6) (cid:21) 4
bge

122

Chapter 3. Assembly-Level Architecture | User Mode

mul
lw
j

# Scale by 4 to obtain the jump table index
$t6, $t6, 4
$t6, JT($t6) # Copy jump target address from jump table to $t6
$t6

# case 0
# case 1

$t15, ’L’
$t15, 0($t1) # *record.name = ’L’
# break
done

# case 2
# case 3

$t15, ’M’
$t15, 0($t1) # *record.name = ’M’
# break
done

# default

$t15, ’H’
$t15, 0($t1) # *record.name = ’H’

case0:
case1:
li
sb
b

case2:
case3:
li
sb
b

default:
li
sb

done:

3.4.5 Translating Loops

Loops form a ma jor portion of most programs. Therefore, it is important to translate
them in an e(cid:14)cient manner. Consider the following C code that adds the elements of
record.name, and stores the result in the memory location pointed by variable cptr. This
code uses the for loop construct.

*cptr = 0;
for (i = 0; i < record.length; i++)
*cptr += record.name[i];

Most assembly languages do not provide a single instruction that can directly implement
complex HLL constructs such as for loops. We can, however, rewrite the above loop in
terms of an if statement with an embedded goto, as follows:

loop:

*cptr = 0;
i = 0;
if (i < record.length)
f

*cptr += record.name[i];
i++;

3.4. Translating HLL Programs to AL Programs

123

goto loop;

g

One possible translation for this rewritten loop is given below. In this assembly language
program, register $t2 is used to store the current value of memory word pointed by cptr,
and register $t3 is used to store the latest value of i.

# Store subsequent items in the text section
.text
# Align next item on a 22 byte (32-bit word) boundary
.align 2
$t1, record # Load starting address of variable record in $t1
la
# Initialize copy of *cptr in $t2
li
$t2, 0
# Initialize copy of i in $t3
$t3, 0
li
$t4, 8($t1) # Copy contents of memory location record.length to $t4
lw
$t3, $t4, done# Branch to label done if i (in $t3) (cid:21) copy of record.length
loop: bge
$t5, 0($t1) # Copy record.name[i] (pointed by $t1) from memory to $t5
lw
$t2, $t2, $t5 # Add copy of record.name[i] in $t5 to running total in $t2
add
# Increment $t1 to point to next element of record.name[]
$t1, 4
addu
# i++; do this on copy of i in $t3
$t3, 1
add
# Branch to label loop
loop
b
# Store copy of i in $t3 to memory location named i
done: sw
$t3, i
# Copy pointer value stored in mem addr cptr to $t4
$t4, cptr
lw
$t2, 0($t4) # Store calculated sum ($t2) to memory location pointed by cptr
sw

This loop causes a straight-line sequence of instructions to be executed repeatedly, as
many times as needed (record.length in this case). The loop starts at location loop and
ends at the instruction b loop. During each pass through this loop, record.name[i]’s
address is (cid:12)rst determined by adding 4 to $t1, and then record.name[i] is fetched and
added to the running total present in $t2. Thus, conditional branch instructions are very
useful in implementing the if statements and loops present in high-level programs.

In the above loop, a form of indirect addressing (indexed addressing, to be precise) is
used in the loop to access record.name[i]. Indirect addressing permits a di(cid:11)erent memory
address to be speci(cid:12)ed during each iteration of the loop, by varying the contents of the index
register ($t1 in this case). If an assembly language does not support any form of indirect
addressing, then the only way to change the memory address during each iteration is to use
self-modifying code, i.e., the program considers portions of itself as data, and modi(cid:12)es itself
during execution.

3.4.6 Translating Subroutine Calls and Returns

We just saw how control (cid:13)ow changes introduced by if statements and loops can be im-
plemented in assembly language. Next we discuss a more complicated type of control (cid:13)ow
change, the one involving subroutine calls and returns. To implement the special type of
branching required to implement subroutines, two basic instructions are used: a cal l instruc-
tion that transfers control from the calling program to the subroutine’s starting location,

124

Chapter 3. Assembly-Level Architecture | User Mode

and a return instruction that returns control from the subroutine to the place from which
it was called, as illustrated in Figure 3.12. In MIPS-I AL, these two instructions are called
jal and jr $ra, respectively. Implementing subroutines in an assembly language is more
complicated because of the following reasons:

Calling
Program

jal P

jal P

Called
Subroutine

P:

jr  $ra

Figure 3.12: Transfer of Control during Subroutine Linkage

(cid:15) The HLL subroutine may have local variables declared within it, for which storage
space needs to be allocated by the assembly language program. Because of the possi-
bility of recursion, each instance of a subroutine requires a new set of storage locations
for its local variables.

(cid:15) Temporary values created within an assembly language subroutine may need to be
stored in memory locations and registers. We cannot specify a (cid:12)xed set of memory
locations for a subroutine, again because of recursion. Conceptually, each run-time
instance of a subroutine requires a few \fresh" memory locations and registers.

(cid:15) A subroutine may be called from di(cid:11)erent places in a program. When the subroutine
(cid:12)nishes execution, control must return to the instruction that immediately follows the
call instruction that passed control to the subroutine.

(cid:15) Most subroutines need the calling program to pass parameters to them at the time
they are called. Also, often, a subroutine may need to pass a return value to the
calling program.

We shall take a detailed look at each of these issues and the solutions developed to
handle them. One important aspect that guides these solutions is that the development of
an assembly language subroutine|be it by a programmer or a compiler|is often done in
isolation to the development of its calling routine(s). This means that the calling routines as
well as the subroutines must adhere to a set of well-de(cid:12)ned speci(cid:12)cations or conventions. If
the calling routine was developed with a particular convention in mind, and the subroutine
was developed with another, then the program as a whole may not guarantee correct results.

3.4. Translating HLL Programs to AL Programs

125

The crux of the (cid:12)rst two problems mentioned above is that each invocation of a subrou-
tine needs a separate working environment. This environment consists of a set of registers
and memory locations that can be used for allocating local variables, and storing temporary
values. To solve the last two problems, we need to provide a well-de(cid:12)ned communication
mechanism between the working environments of the caller and the callee.

We already saw in Section 3.4.2 how local variables of a HLL program are allocated
storage space in the corresponding assembly language program. The conventional method
is to build a new stack frame every time a subroutine is called, and to allocate speci(cid:12)c
locations within the stack frame for each local variable. The stack frames are created within
the stack section of the memory address space, and are organized as a LIFO structure. The
provision of an independent stack frame for each active subroutine enables each subroutine
instance to have its own set of storage locations for allocating its local variables. In the
following subsections, we will see how the stack frame concept has become the backbone
for solving all of the above mentioned problems associated with implementing subroutines.
Frame layouts vary between assembly languages and compilers. If a frame pointer is used,
then the frame pointer of the previous frame is stored in the current frame, and restored
when returning from a subroutine.

Return Address Storing

Because a subroutine may be called from di(cid:11)erent places in a program, provision must be
made for returning to the appropriate location in the program after the subroutine completes
execution. To do this, the return address must be saved somewhere before transferring
control to the subroutine. The MIPS-I assembly-level architecture has earmarked register
$ra for storing the return address. The semantics of a call instruction (named jal for jump
and link) specify that the return address is automatically stored in general-purpose register
$ra. If a subroutine needs to call another, then the assembly language programmer (or
compiler) writing the caller routine should include instructions to save the contents of $ra
on the stack frame (before performing the call) and to restore the contents of $ra (after
returning from the callee). Typically, these instructions are placed at the beginning and
end, respectively, of the calling subroutine.

Consider the MIPS-I AL program that we saw in page 115. In this program, the routine
main() calls subroutine malloc(). Prior to the jal malloc instruction, the return address
of main() is saved in main()’s stack frame using the instruction sw $ra, 8($sp). When
executing the jal malloc instruction, the contents of $ra will be overwritten with the
return address of malloc(). Therefore, prior to returning from main(), the correct return
address of main() is restored into $ra using the instruction lw $ra, 8($sp).

126

Chapter 3. Assembly-Level Architecture | User Mode

Parameter Passing and Return Value Passing

When calling a subroutine, the calling routine must provide to the callee the parameters,
that is, the operands or their addresses, to be used in the computation. Later, the sub-
routine may return the results of the computation to the calling routine. This exchange of
information between a calling routine and a subroutine is referred to as parameter passing.
The MIPS-I AL convention stipulates the programmer to place the parameters in registers
$a0 through $a3, where they can be accessed by the instructions of the subroutine. Simi-
larly, the programmer should place the return values in registers $v0 and $v1. If more than
4 parameters are present, the rest of the parameters are passed through the stack frame.
Figure 3.18 shows a parameter being passed to subroutine P through register $a0. The
return value is passed by the subroutine back to the calling program through register $v0.

Calling
Routine

li
jal

$a0, 1
P

# place argument in $a0
# call subroutine P
# use return value stored in $v0

Called
Subroutine

P:

# use parameter stored in $a0

# place return value in $v0
$ra # return

jr

Figure 3.13: Passing Parameters and Return Value through Registers

Register Saving and Restoring

As discussed, the development of an assembly language subroutine|whether by a program-
mer or a compiler|is often done in isolation to the development of the calling program.
This means that at the time of subroutine development, it is di(cid:14)cult to identify the registers
that are not used in the calling environment, and are therefore available for its use. If the
subroutine developer blindly uses an arbitrary register for storing a temporary value, there
is a possibility of overwriting useful information belonging to the calling environment.

The approach followed in MIPS-I is to let the programmer temporarily save the values
pertaining to the caller, prior to the callee using them. Once the callee has completed its
usage of a register set, the programmer restores their original values. Again, a convenient
place to temporarily save the register values is the stack frame. Figure 3.17 shows the layout
of a stack frame in which space has been set apart for saving general-purpose registers as
well as (cid:13)oating-point registers. This is the approach followed in the MIPS-I ALA.

Performing the saving (and restoring) of the register values at the proper times is the
responsibility of the assembly language programmer. This saving (and restoring) can be
done by the caller (cal ler save), by the callee (cal lee save), or by a combination of the two.

3.4. Translating HLL Programs to AL Programs

127

It is important to note that it is not necessary to save the entire set of registers in the name
space. The MIPS-I AL earmarks some of the registers for caller save, and some others for
callee save.

Lower Addresses

Current Stack Frame

$sp

Space for storing
callee arguments

Space for storing
FPRs

Space for storing
GPRs

Space for storing
Temporary Values

Space for allocating
Local Variables

Higher Addresses

Figure 3.14: Layout of a Typical Stack Frame

Notice that the need for saving and restoring .... or memory locations, the MIPS-I
compiler avoids this problem by utilizing a separate stack frame for each active subroutine.

3.4.7 Translating System Calls

Consider the C program given in page 53 (and reproduced below), for copying characters
from standard input to standard output. This program directly calls the operating system
services using the read() and write() system calls, instead of going through library rou-
tines. These system calls can be implemented in an assembly language with the use of trap
or syscall instructions. The syscall instruction is quite di(cid:11)erent from a library function call,
although from the assembly language programmer’s point of view there may not be a big
di(cid:11)erence. Consider the following C code which uses the system calls read() and write()
to read a sequence of characters from stdin and write them to stdout.

main()

128

Chapter 3. Assembly-Level Architecture | User Mode

f

g

char c;

while (read(0, &c, 1) > 0) /* read one char from stdin */
write(1, &c, 1);
/* write one char to stdout */

A MIPS-I AL translation of the above C code is given below. This code uses the syscall
instruction to request the OS to perform the read() and write() system calls. Most
commercial operating systems use the same set of values for read code and write code
(3 and 4, respectively). Notice that this AL program cannot be ‘executed’ on a SPIM
simulator, because SPIM does not support these system calls.

.text
.globl main

main: add
li
loop: li
li
syscall

$a1, $sp, 4
$a2, 1
$a0, 0
$v0, 3

# Place the address of c in $a1
# Place the number of bytes to be read (i.e., 1) in $a2
# Place the (cid:12)le descriptor (0) in $a0
# Place the code for read() system call in $v0
# Call OS routine to perform the read

blez
li
li
syscall
b

$v0, done
$a0, 1
$v0, 4

loop

# Break out of while loop if syscall returned zero
# Place the (cid:12)le descriptor (1) in $a0
# Place the code for write() system call in $v0
# Call OS routine to perform the write
# Go back to while loop

done: jr

$ra

# Return from main() function

3.4.8 Overview of a Compiler

[This section needs to be written.]

The compilation process involves a series of phases.

(cid:15) Lexical analyzer (or lexer)

(cid:15) Syntax analyzer (or parser)

(cid:15) Intermediate code generator

(cid:15) Code optimizer: Code optimization is an important phase because for most of the
applications, once a program is developed, the same program is executed thousands
or millions of times.

3.5. Memory Models: Design Choices

129

(cid:15) Code generator

3.4.8.1 Just-In-Time (JIT) Compiler

The oldest implementation of Java is the interpreter. Every Java command is interpreted
to an equivalent sequence of host machine instructions, and is run in the order in which it
arrives. This is a really slow way of doing things.

Then came the JIT (just in time) compiler. Every time the Java execution runtime
environment would run into a new class|classes are functional groups within the Java
program|the JIT compiler would compile it right there. Once something is compiled, it
runs with native commands, and it is fast. Spending a little bit of time up front can save
a lot of execution time later. That did improve matters, but it still did not get the top
performance, because some things that would only run once could take longer to compile
than it would take to run them with the interpreter. This means you could wind up with
a net loss.

With that observation came the dynamic compiler, which compiles only those things
that matter and leaves the rest alone. The dynamic compiler decides whether to compile
each class. It has two weapons in its arsenal: an interpreter and a JIT, and it makes an
intelligent decision on a class-by-class basis whether to use the one weapon or the other.
The dynamic compiler makes that decision by "pro(cid:12)ling," or letting the code run a few
internal loops before deciding whether or not to compile that section of code. The decision
may be wrong, but statistically the dynamic compiler is right much more often than not;
in fact, the longer you run the code, the more likely it is to get it right.

3.5 Memory Models: Design Choices

We have seen the basics of translating high-level language programs to assembly language
programs. Much of this discussion was centered around the MIPS-I ALA, a somewhat simple
architecture. A wide variety of architectures have been in use over the last several decades.
They di(cid:11)er in terms of the register model, the memory model, and/or the instruction set.
In this section, we shall focus on the memory model. In particular, we shall investigate
di(cid:11)erent options for various facets of the memory model, such as the address space, the
endian-ness, and the support for word alignment.

3.5.1 Address Space: Linear vs Segmented

We have already seen that the memory address space is a collection of unique addresses,
each of which corresponds to a location that can store a single word of information. The
address space can be organized in more than one way. The organization we saw so far
is the linear (one-dimensional array) organization, in which consecutive numbers ranging

130

Chapter 3. Assembly-Level Architecture | User Mode

from 0 to M (cid:0) 1 (where M is the number of memory locations) form the addresses of
successive memory locations. The M addresses (0 ! M (cid:0) 1) thus constitute a linear address
space (a.k.a. (cid:13)at address space) for the computer. Thus, the memory model adopted by
conventional assembly language (and machine language) architectures is a linear memory
model. To specify a memory address in this model, log 2M bits are required. For example,
if the address space of an assembly-level architecture includes 2 32 memory locations, then
32 bits are required to specify a memory address in that machine. For this model, we will
use the notation (X) to denote the contents of memory location X.

In some architectures, this memory is organized in a 2-dimensional manner as a collection
of segments, each of which is a linear address space for storing logically related words. The
segments are typically of di(cid:11)erent sizes. One segment may hold the program, another may
hold the data, and so on.
In this memory model, a memory location is referred to by
specifying a segment address and a displacement within the segment.

Expand, saying the adv and disadv of segmented memory model

3.5.2 Word Alignment: Aligned vs Unaligned

Many assembly-level architectures require words to be aligned to their natural boundaries;
that is, an N -bit word may (cid:12)t within a memory location, but not across two locations. A
sketch of the linear memory model, along with an illustration of aligned words and unaligned
words is given in Figure 3.15.

Address

log M
2

Aligned

N −bit word

Unaligned

N −bit word

0
4

M−4

N bits

N

Data

Figure 3.15: Linear Memory Model and Alignment of Words in Memory. Some Assembly-
Level Architectures specify words to be aligned in Memory

3.6. Operand Locations: Design Choices

131

3.5.3 Byte Ordering: Little Endian vs Big Endian

In an assembly-level architecture specifying a byte-addressable memory address space, a
word typically spans multiple (adjacent) locations in memory. Each byte within such a
word will then have a distinct memory address, although the word is usually referenced by
a single address (either the lowest address or the highest address in the range). Two options
are available for placing the bytes within the word in memory. In the (cid:12)rst option, called
little-endian organization, the least signi(cid:12)cant byte of the word is assigned the lowest (byte)
address. In the second option, called big-endian organization, the most signi(cid:12)cant byte is
assigned the lowest address.

3.6 Operand Locations: Design Choices

In this section we will study the di(cid:11)erent options that exist for holding the operands of
an instruction. The MIPS-I architecture limited itself to using the main memory, the
general-purpose registers, a few special registers, and even part of the instruction for holding
the operands; for data manipulation instructions, operand storage was limited to general-
purpose registers. Many other machines specify additional locations for operand storage,
such as accumulator and operand stack. We will see each of these options in detail.

3.6.1

Instruction

Small constants are frequently used as instruction operands. A convenient place for storing
these operands is within the instruction itself. Such operands are called immediate operands
because they are available for use immediately after the instruction has been fetched from
memory. Many small constants such as 0, 1, (cid:0)1, 2, and 4 occur frequently in programs, and
the immediate operand approach o(cid:11)ers a convenient way to store such an operand within
the instruction itself. A MIPS-I instruction using an immediate operand is given below.

li

$t1, 5

# 5 is an immediate operand

3.6.2 Main Memory

Main memory is the default location for maintaining the values of the variables declared in
a program. For example, when a C compiler encounters the variable declaration
int c;
it assigns a memory location for variable c. The values of the variables are initialized/updated
by writing values to the relevant memory locations. Before manipulating a data item, it is
often copied to other storage locations such as registers and operand stack.

132

Chapter 3. Assembly-Level Architecture | User Mode

3.6.3 General-Purpose Registers

Besides memory locations, registers are another common place for storing operands.
In
load-store architectures such as MIPS-I, operands have to be copied to registers before they
can be used in arithmetic and logical operations. Registers are also commonly used for
storing temporary values and frequently used values. The more the number of registers
de(cid:12)ned in the architecture, the easier it is to keep the frequently used values in registers.

Modern computers invariably support a number of general-purpose registers at the as-
sembly language and ISA levels|typically 8 to 64|which can be used to temporarily store
frequently used operands. When operands are present in registers, it is possible to specify
multiple operands explicitly in an instruction, because a register can be speci(cid:12)ed by just a
few bits. Therefore, we can have instructions such as

add
add

# an x86 AL add instruction
BX, CX
$t0, $t1, $t2# a MIPS-I AL add instruction

Because of these multiple operand or address instructions, machines that use general-
purpose registers are often called multiple address machines. Among the multiple ad-
dress machines, some permit data manipulation only on register operands (as well as im-
mediate operands).
In such machines, all memory operands must be copied to registers
using explicit LOAD instructions prior to using them for data manipulation. Similarly, the
results of data manipulation instructions are also stored only in registers, from where they
are copied to memory locations (if needed) using explicit STORE instructions. Machines of
this kind are called load-store architectures (to re(cid:13)ect the fact that only LOAD and STORE
instructions can access memory) or register-register architectures (to re(cid:13)ect the fact
that all data manipulation instructions use only register operands (immediate operands as
well) and store the result in a register). Examples of load-store architectures are MIPS-I
and Alpha.

Machines that permit data manipulation instructions to access register operands as well
as memory operands are called register-memory architectures. An example is the IA-
32 machine. Notice that a register-memory machine can have instructions that use only
register operands.

3.6.4 Accumulator

The early computers made very judicious use of registers, because registers required a non-
trivial amount of hardware. In these machines, one of the registers, called the Accumulator,
was designated as the one that would be utilized in all arithmetic and logic operations.
On machines that operate in this manner, instructions requiring a single operand, such
as COMPLEMENT and INCR, (cid:12)nd the operand in the Accumulator. The result is also writ-
ten to the Accumulator.
Instructions requiring two operands also use the value in the
Accumulator as one of the operands. The other operand is identi(cid:12)ed by a single address in

3.6. Operand Locations: Design Choices

133

the instruction; hence machines that always use the Accumulator are often called single-
address machines. For example, the single-address instruction

add

X;

# Add the contents of mem loc X to ACC

means: \Add the contents of memory location X to the contents of accumulator and place
the sum into accumulator." To move the contents of memory location X into accumulator,
we can specify a single address instruction as follows:

lw

X;

# Copy the contents of mem loc X to ACC

Similarly, to move the contents of Accumulator to memory location Y, we can specify a
single address instruction as follows:

sw

Y;

# Copy the contents of ACC to mem loc Y

Using only single address instructions, we can add the contents of memory locations X
and Y, and place the result in memory location Z by executing the following sequence of
instructions:

lw
add
sw

X
Y
Z

Note that the operand speci(cid:12)ed in the operand (cid:12)eld may be a source or a destination,
depending on the opcode of the instruction. The lw instruction explicitly speci(cid:12)es the source
operand, memory address X, and implicitly speci(cid:12)es the destination, the Accumulator. On
the other hand, the sw instruction explicitly speci(cid:12)es the destination, memory location Z,
and implicitly speci(cid:12)es the source, the Accumulator. A single address machine built in
the mid-1960s that enjoyed wide popularity was the PDP-8, made by Digital Equipment
Corporation.

3.6.5 Operand Stack

The approaches we saw so far use some type of registers|general-purpose registers or an
accumulator|in addition to memory locations to hold instruction operands. A radically
di(cid:11)erent approach is to use an operand stack to hold the operands. An operand stack
is di(cid:11)erent from the stack frames that we saw earlier. It is a storage structure in which
accesses are allowed only to the top location of the stack (sometimes to the topmost two
locations)10 . Only two types of accesses are permitted to the top of the stack: push and

10 In computer science and engineering (cid:12)elds, this type of storage mechanisms are also known by the term
last-in (cid:12)rst-out (LIFO); at any time the (cid:12)rst data item that will be taken out of the stack will be the
last one that was placed in the stack.

134

Chapter 3. Assembly-Level Architecture | User Mode

pop. These operations are analogous, respectively, to the store and load operations de(cid:12)ned
on the memory address space. The push operation places a new data item to the top of
stack, causing the stack to \grow". The pop operation removes the top item from the stack,
causing the stack to \shrink". Thus, the push and pop operations cause a change in the size
of the operand stack structure. This is unlike the memory address space, which permits the
usual load and store operations, which cause no change to the size of the structure. This
peculiar nature of the operand stack can be clari(cid:12)ed with a real-world example. Consider
a pile of trays in a cafeteria: clean trays are added (pushed) to the pile at the top, causing
it to grow; and customers pick up (pop) trays from the top of the pile, causing it to shrink.

Example push and pop instructions are given below:

push
pop

X
X

# Copy contents of mem loc X to top of operand stack
# Copy contents of top of operand stack to mem loc X

Apart from these push and pop operations, a machine that supports an operand stack typi-
cally provides many arithmetic and logical instructions that operate on operands stored in
the operand stack. One such instruction could be
add

which means pop the contents of the two topmost locations of the operand stack, add them

together, and push the result to the new top of the operand stack. This instruction is
interesting in that it does not explicitly specify any operands, but the operands are implied
to be on the top of the operand stack. Because only the topmost item(s) of the operand
stack are accessible, it is important to push data items to the operand stack in the proper
order; otherwise it becomes di(cid:14)cult to use instructions that manipulate data stored in the
operand stack.

Only a few architectures support an operand stack. By contrast, almost all architectures
support stack frames for allocating the local variables of subroutines. Some of the architec-
tures that support an operand stack de(cid:12)ne it as a separate address space (as illustrated in
Figure 3.16(i)), whereas others de(cid:12)ne it as part of the memory address space. In the latter
case, the operand stack of a subroutine is usually implemented on top of the subroutine’s
stack frame, as illustrated in Figure 3.16(ii). The FP register points to the bottom of the
current stack frame, and the SP register points to the top of the operand stack, which is
implemented on top of the current stack frame. The Java Virtual Machine (JVM) de(cid:12)nes
an operand stack in this manner.

A pure stack machine takes the operand stack approach to the extreme. It does not
de(cid:12)ne any general-purpose registers, and performs arithmetic and logical operations only
with operands present in (the top one or two locations of ) the operand stack. It does specify
a memory model, along with push and pop instructions to move data between the operand
stack and memory locations. None of the data manipulation instructions in such a machine
speci(cid:12)es operands in an explicit manner. Because of these \zero address instructions",
machines that perform data manipulation solely on the operand stack are often called zero

3.6. Operand Locations: Design Choices

135

Stack Space

Memory Space

Direction of
stack growth

TOS

BOS

Direction of
stack growth

Operand
Stack

Stack Frame

SP (TOS)

FP

BOS

(i)

(ii)

Figure 3.16: Di(cid:11)erent Ways of Implementing an Operand Stack: (i) Separate Stack Address
Space; (ii) Within Memory Address Space, on Top of Current Stack Frame

address machines.

Example: Write an assembly language program to evaluate the following C language expres-
sion in a zero address machine.
D = (A + B + C ) (cid:2) (A + B )
Assume that variables A, B , C , and D have been declared as int.

A:
B:
C:
D:

.data
.word
.word
.word
.word
.text
push
push
add
push
add
push
push
add
mult
pop

5
10
8
0

A
B

C

A
B

D

# Copy contents of mem location A to TOS
# Copy contents of mem location B to TOS

# Copy contents of mem location C to TOS

# Copy contents of mem location A to TOS
# Copy contents of mem location B to TOS

The pure stack approach does have some drawbacks:

(cid:15) If the operand stack is implemented in a memory structure outside the processor,
all instructions that access the operand stack require o(cid:11)-chip access to fetch/store

136

Chapter 3. Assembly-Level Architecture | User Mode

operands, and this results in poor performance. The solution often adopted in hard-
ware implementations of stack-based instruction sets is to incorporate the top portion
of the operand stack as microarchitectural registers inside the processor; the rest of
the operand stack is incorporated in memory outside the processor chip. The hard-
ware registers that incorporate the top of the stack can be accessed in a single cycle.
As more and more data items are pushed onto the operand stack, these hardware
registers get (cid:12)lled up, necessitating the transfer of its bottom portion to the stack
in memory. The microarchitecture (and not the assembly language programmer) is
responsible for performing this transfer. The assembly language programmer is not
even aware of these hardware registers.

(cid:15) A second drawback of using an operand stack is the inability to reuse temporary val-
ues created during computations. Consider the expression that you evaluated just
now:

F = (A + B + C ) (cid:2) (A + B )
The A + B portion of this expression needs to be computed only once if the result is
stored in a general-purpose register, whereas it needs to be computed twice if operands
are stored only on the operand stack, unless the calculated value of A + B is popped
into a memory location, and pushed back to the top of stack when needed again.

(cid:15) Lastly, when all instructions use a common resource such as the top of the operand
stack,
it becomes di(cid:14)cult to execute multiple instructions in parallel
in a high-
performance processor implementation.

The proponents of pure stack machines counter these arguments. Their main claims are
that the stack machine approach is clean, simple, elegant! These features make it an easy
target for compilers. Thus, in the last decade, Sun Microsystems introduced a stack-based
machine called Java Virtual Machine (JVM), which has become popular in the world wide
web and embedded computing applications.

3.7 Operand Addressing Modes: Design Choices

An assembly language instruction speci(cid:12)es the assembly-level machine to do a speci(cid:12)c op-
eration on one or more operands. The operands can be present in general-purpose registers,
memory, accumulator, stack, or the instruction itself. The exact location of the operands
depends on the addressing modes speci(cid:12)ed in the instruction. An addressing mode speci(cid:12)es
a rule for interpreting or modifying an operand address (cid:12)eld to obtain the operand. For
instance, the operand or its location can be assumed, as in the CLA (clear accumulator)
instruction, or the operand location can be identi(cid:12)ed in the instruction itself, as in the \add
$t1, $t2, $t3" instruction.

We have already used several addressing modes when writing MIPS-I assembly-level
programs. Some instruction sets are equipped with even more powerful addressing modes,

3.7. Operand Addressing Modes: Design Choices

137

which give more (cid:13)exibility to the assembly language programmer. These addressing modes
include capabilities such as pointers to memory, counters for loop control, indexing of data,
and relocation of programs. In this section we will examine the common addressing modes
for specifying operands in assembly languages. It is important to note that many of these
addressing modes are not present in the instruction set architecture. Additional addressing
modes are provided in the assembly-level architecture to make it easier to program at the
assembly level. Fewer addressing modes are supported by the instruction set architecture
so as to make it easier to implement the architecture in hardware. Addressing modes that
are unsupported by the instruction set architecture are synthesized using the supported
addressing modes.

3.7.1

Instruction-Residing Operands: Immediate Operands

We shall start with addressing mode(s) for operands speci(cid:12)ed within the instruction (im-
mediate operands). In the MIPS-I instruction

li

$t1, 5

# 5 is an immediate operand

the operand 5 is speci(cid:12)ed using the immediate addressing mode. Almost all machines pro-
vide the immediate addressing mode, for specifying small integer constants. If an instruction
set does not support the immediate addressing mode, it would be di(cid:14)cult to specify such
constants. One option would be to hardwire a few memory locations or registers with fre-
quently required constants. The rest of the constants will then need to be synthesized from
the hardwired constants. Many of the newer machines use such a hardwired register for
storing the constant zero, and use the immediate addressing mode for specifying the re-
maining constants. We have already seen that the MIPS-I architecture has such a register,
called $zero.

3.7.2 Register Operands

In register-based architectures such as MIPS-I, many of the operands reside in registers. The
common method for specifying register operands is to employ the register addressing
mode, which is perhaps the most frequently used addressing mode in a register-based
machine. In this addressing mode, the name or address of the register is speci(cid:12)ed in the
instruction. We have already used this addressing mode several times in this chapter. For
example, the following MIPS-I instruction uses the register addressing mode to specify 2
operands|a source operand and a destination operand.

move

$t1, $t2

# The operand addresses are registers $t1 and $t2

In some architectures such as the IA-32, some of the registers are special. Often, when
using such as Apart from the register addressing mode,

138

Chapter 3. Assembly-Level Architecture | User Mode

3.7.3 Memory Operands

Memory operands have the largest variety of addressing modes, including ones with indi-
rection. We shall look at the commonly used ones here.

Memory Direct Addressing:
In this addressing mode, the entire address of the memory
operand is given explicitly as part of the instruction. Example instructions that use the
memory direct addressing mode to fetch a memory operand are given below:

lw
lw

# Source operand is in memory location label1
$t1, label1
$t1, 0x10000000# Source operand is in memory location 0x10000000

In the second instruction, the source operand is present in memory location whose address
is 0x10000000. This address is explicitly speci(cid:12)ed in the instruction. Memory direct ad-
dressing has two limitations: (i) At the ISA level, the entire address of the memory operand
has to be encoded in the instruction, which makes the instruction long. (ii) The address
must be determined and (cid:12)xed at the time of programming or the assembly process. Once
(cid:12)xed, this address cannot be changed when the program is being run, unless the architecture
permits self-modifying code11 . Therefore, memory direct addressing is limited to accessing
global variables whose addresses are known at the time of programming or the assembly
process.

Register Indirect Addressing:
In this addressing mode, the instruction speci(cid:12)es a
register as in register direct addressing, but the speci(cid:12)ed register contains the address of
the memory location where the operand is present. Thus, the e(cid:11)ective address of the
memory operand is in the register whose name or number appears in the instruction. This
addressing mode is useful for implementing the pointer data type of high-level languages.
For example,

lw

$t1, ($t2)

# Memory address of source operand is in register $t2

The big advantage of register indirect addressing is that it can reference memory without
paying the price of specifying a full memory address in the instruction. Second, by modifying
the contents of the speci(cid:12)ed register, it is possible to access di(cid:11)erent memory words on
di(cid:11)erent executions of the instruction. The utility of indirect addressing was demonstrated
in the example loop code in Section 3.4.5, which involved (cid:12)nding the sum of the elements
of an array. In that program, the add $t1, 4 instruction (which uses register addressing)
causes the address speci(cid:12)ed by the lw instruction (which uses a form of indirect addressing)

11 In an architecture that permits self-modifying code, the program is allowed to use the .text section as
data as well. The program can thus modify some of its instructions at run time, by writing to the memory
locations allotted to those instructions. Self-modifying code was common in the early days of computer
programming, but is not in vogue any more because of debugging di(cid:14)culties.

3.7. Operand Addressing Modes: Design Choices

139

to point to the next element of the array. What is interesting to note is that the loop body
does not explicitly specify any memory addresses.

Autoincrement Addressing: This is an extension of register indirect addressing. When
this mode is speci(cid:12)ed, the speci(cid:12)ed register is incremented by a (cid:12)xed amount (usually 1),
after using the current value of the register for determining the e(cid:11)ective address of the
operand. For example,

lw

$t1, ($t2)+

# lw $t1, ($t2)
# addu $t2, 1

When this instruction is executed, the current value of $t2 is used for determining the
memory address of the operand, and then $t2 is incremented. This addressing mode is not
speci(cid:12)ed in the MIPS-I assembly language. It was common in earlier assembly languages,
particularly for use inside loops.

Indexed or Register Relative Addressing: This is an extension of register indirect
addressing. The e(cid:11)ective address of the memory operand is given by the sum of an index
register value and an o(cid:11)set value speci(cid:12)ed in the instruction. For example,

add
add
add

$t1, 100($t2) # Mem addr of operand is 100 + contents of $t2
$t1, label($t2)# Mem addr of operand is addr of label + contents of $t2
$t1, $t3($t2) # Mem addr is contents of $t2 + contents of $t3

The (cid:12)rst instruction uses a single index register ($t2) and an o(cid:11)set, which are added
together to obtain the memory address of the operand. Notice that the value of the index
register does not change. The second instruction speci(cid:12)es a label as the o(cid:11)set. This mode is
particularly useful for accessing arrays; the index register serves as an index into the array
starting at label1. The third instruction uses two index registers, the contents of which
are added together to obtain the memory address of the operand; this indexed addressing
mode is not supported in the MIPS-I assembly language. Some assembly languages even
permit the index register to be a special register, such as the PC. When register PC is used as
an index register, it is often called PC-relative addressing. Some machines dedicate one
register to function solely as an index register. This register is addressed implicitly when
an index mode is speci(cid:12)ed. In other machines, other special registers or GPRs can be used
as an index register. In such a situation, the index register must be explicitly speci(cid:12)ed in
the instruction.

Memory Indirect Addressing:
In this addressing mode, the instruction speci(cid:12)es the
memory address where the e(cid:11)ective address of the memory operand is present. This ad-
dressing mode is not supported in the MIPS-I assembly language. An example MIPS-like
instruction that uses the memory indirect addressing mode is given below.

140

Chapter 3. Assembly-Level Architecture | User Mode

lw

$t1, (label1) # Mem addr of operand is in mem location label1

3.7.4 Stack Operands

Finally, let us look at stack operands. In the \pure" stack machines that we saw in Section
3.6.5, a stack operand can be accessed only if it is currently at the top of the stack. In
such machines, the location of stack operands need not be speci(cid:12)ed explicitly, and instead
can be speci(cid:12)ed implicitly. This type of addressing is called implicit addressing mode. The
operand or its e(cid:11)ective address is implicitly speci(cid:12)ed by the opcode. An example of such
an instruction is given below.

add

# Both operands are on top of stack

Most of the register-based machines provide a stack model to the programmer, but use
a less strict access mechanism for stack operands. They have a program-managed stack
pointer register, which can be used as an index register to access stack operands that are
not necessarily at the top of the stack.

Registers

$t0
$t1
$t2
$t3
$sp
pc

200
200
300
500

Addr

200

Main
Memory
50

300

400

500

1000

500

200

Example: Consider the register map and memory gap given. Each of the memory locations
explicitly shown represents 4 bytes. What is the value of the operand that is written to $t1
in each of the following MIPS-like instructions?

1. move
$t1, $t2
This is register direct addressing, and the operand value is 200.

2. lw
$t1, 300
This is memory direct addressing, and the operand value is 500.

3. li
$t1, 300
This is immediate addressing, and the operand value is 300.

3.8. Subroutine Implementation

141

4. lw
$t1, ($t2)
This is register indirect addressing, and the operand value is the contents of memory
location 200, which is 50.

5. lw
$t1, $t2($t3)
This is register indexed addressing, and the operand value is the contents of memory
location 400 (obtained by adding the contents of $t2 and $t3), which is 1000.

6. lw
$t1, (500)
This is memory indirect addressing; the e(cid:11)ective address of the operand is the contents
of memory location 500, and the operand value is 50.

3.8 Subroutine Implementation

In structured programming languages, subroutines (and macros) are the main mechanism
for control abstraction, which permits associating a name with a potentially complex code
fragment that can be thought in terms of its function rather than its implementation. In
Section ***, we saw how a subroutine is implemented in the MIPS-I assembly language.
In this section, we take a broader look at this topic, which is at the core of structured
programming. The two things that (i) a subroutine can be called from di(cid:11)erent places
in the program, and (ii) after the completion of the subroutine, control returns to the
calling place. For the proper functioning of a subroutine, at the assembly language level, a
subroutine requires its own storage space for storing the following: its return address of the
subroutine, its local variables, links to variables in non-local scopes 12 , and temporary values
it produces. In addition, it may require register space to store frequently used values.

Speci(cid:12)cally, we discussed three sub-topics there: return address saving, parameter pass-
ing, and saving (and restoring) of registers. In this section, we shall look at some design
choices in these areas.
Important issues to consider in implementing subroutines in an
assembly language are:

(cid:15) The HLL subroutine may have local variables declared within it, for which storage
space needs to be allocated by the assembly language program. Because of the possi-
bility of recursion, each instance of a subroutine requires a new set of storage locations
for its local variables.

(cid:15) Temporary values created within an assembly language subroutine may need to be
stored in memory locations and registers. We cannot specify a (cid:12)xed set of memory
locations for a subroutine, again because of recursion. Conceptually, each run-time
instance of a subroutine requires a few \fresh" memory locations and registers.

12 Some high-level languages such as

142

Chapter 3. Assembly-Level Architecture | User Mode

(cid:15) A subroutine may be called from di(cid:11)erent places in a program. When the subroutine
(cid:12)nishes execution, control must return to the instruction that immediately follows the
call instruction that passed control to the subroutine.

(cid:15) Most subroutines need the calling program to pass parameters to them at the time
they are called. Also, often, a subroutine may need to pass a return value to the
calling program.

We shall take a detailed look at each of these issues and the solutions developed to
handle them. One important aspect that guides these solutions is that the development of
an assembly language subroutine|be it by a programmer or a compiler|is often done in
isolation to the development of its calling routine(s). This means that the calling routines as
well as the subroutines must adhere to a set of well-de(cid:12)ned speci(cid:12)cations or conventions. If
the calling routine was developed with a particular convention in mind, and the subroutine
was developed with another, then the program as a whole may not guarantee correct results.

The crux of the (cid:12)rst two problems mentioned above is that each invocation of a subrou-
tine needs a separate working environment. This environment consists of a set of registers
and memory locations that can be used for allocating local variables, and storing temporary
values. To solve the last two problems, we need to provide a well-de(cid:12)ned communication
mechanism between the working environments of the caller and the callee.

We already saw in Section 3.4.2 how local variables of a HLL program are allocated
storage space in the corresponding assembly language program. The conventional method
is to build a new stack frame every time a subroutine is called, and to allocate speci(cid:12)c
locations within the stack frame for each local variable. The stack frames are created within
the stack section of the memory address space, and are organized as a LIFO structure. The
provision of an independent stack frame for each active subroutine enables each subroutine
instance to have its own set of storage locations for allocating its local variables. In the
following subsections, we will see how the stack frame concept has become the backbone
for solving all of the above mentioned problems associated with implementing subroutines.

3.8.1 Register Saving and Restoring

As discussed, the development of an assembly language subroutine|whether by a program-
mer or a compiler|is often done in isolation to the development of the calling program.
This means that at the time of subroutine development, it is di(cid:14)cult to identify the regis-
ters that are not used in the calling environment, and are therefore available for its use. If
the subroutine developer blindly uses an arbitrary register for storing a temporary value,
there is a possibility of overwriting useful information belonging to the calling environment.
Notice that a similar problem is avoided for memory values by utilizing a separate stack
frame for each active subroutine. One possibility is to provide a similar arrangement for
registers. Sun Microsystems’ SPARC architecture does precisely that. It uses the concept
of multiple register windows. Each active subroutine has its own private register name

3.8. Subroutine Implementation

143

space called register window. Every time a subroutine is called, a new register window is
made available to the newly activated subroutine. When the subroutine (cid:12)nishes execution,
its register window ceases to exist.

A more conventional approach for furnishing registers to a subroutine is to let both the
caller and the callee use the same register set, but provide a means to temporarily save the
values pertaining to the caller, prior to the callee using them. Once the callee has completed
its usage of a register set, their original values are restored. Again, a convenient place to
temporarily save the register values is the stack frame. Figure 3.17 shows the layout of a
stack frame in which space has been set apart for saving general-purpose registers as well
as (cid:13)oating-point registers. This is the approach followed in the MIPS-I ALA.

Performing the saving (and restoring) of the register values at the proper times is the
responsibility of the assembly language programmer. This saving (and restoring) can be
done by the caller (cal ler save), by the callee (cal lee save), or by a combination of the two.
It is important to note that it is not necessary to save the entire set of registers in the
name space. Strictly speaking, in order to ensure program correctness, we need to save only
those registers that contain useful values (such registers are called live registers) and are
about to be overwritten. However, it is di(cid:14)cult for either the caller or the callee to verify
both of these requirements: liveness of a register as well as the de(cid:12)niteness of overwriting it.
The caller knows about the liveness of a register, but not its probability to be overwritten
by the callee. The callee, on the other hand, knows when the register will be overwritten,
but it does not know if the register is live! Thus, whether we use caller save or callee
save, some ine(cid:14)ciency is bound to occur. Assembly languages like MIPS-I AL incorporate
some conventions about register usage to trim this ine(cid:14)ciency: some of the registers are
earmarked for caller save, and some others are earmarked for callee save.

3.8.2 Return Address Storing

Because a subroutine may be called from di(cid:11)erent places in a program, provision must be
made for returning to the appropriate location in the program after the subroutine completes
execution. To do this, the return address must be saved somewhere before transferring
control to the subroutine. This saving can be done either by inserting extra instruction(s)
before the call instruction, or by specifying it as part of the call instruction’s semantics.
Most of the machines follow the latter approach. The way in which a computer supports
control (cid:13)ow changes to and from subroutines is referred to as its subroutine linkage
method.

What would be a good place to store the return address? A commonly used method
is to store it in a special register, called a link register. This provides a fast mechanism to
store and retrieve the return address. However, it does not allow subroutine nesting; i.e.,
one subroutine calling another. When a nested subroutine call is made, the return address
of the second call also gets stored in the link register, overwriting its previous contents
(the return address of the (cid:12)rst call). Hence it is essential to save the link register contents

144

Chapter 3. Assembly-Level Architecture | User Mode

Lower Addresses

Current Stack Frame

$sp

Space for storing
callee arguments

Space for storing
FPRs

Space for storing
GPRs

Space for storing
Temporary Values

Space for allocating
Local Variables

Higher Addresses

Figure 3.17: Layout of a Typical Stack Frame

somewhere else before calling another subroutine.

Conceptually, subroutine nesting can be carried out to any depth. At any point in time,
the (cid:12)rst subroutine to complete will be the last one to be called. Its return address is the
last one generated by the nested call sequence. That is, the return addresses are generated
and used in a last-in (cid:12)rst-out (LIFO) order. This suggests that it would be ideal to save
the return addresses in a stack-like structure; the LIFO nature of stack pushes and pops
(cid:12)ts naturally with the LIFO nature of subroutine calls and returns. Instead of de(cid:12)ning a
separate stack structure for storing the return addresses, however, we can conveniently store
a subroutine’s return address in its stack frame itself, as we saw for the MIPS-I assembly
language.

If a subroutine needs to call another, then the assembly language programmer (or com-
piler) writing the caller routine includes instructions to save the contents of $ra on the stack
frame (before performing the call) and to restore the contents of $ra (after returning from
the callee). Typically, these instructions are placed at the beginning and end, respectively,
of the calling subroutine.

3.8. Subroutine Implementation

145

3.8.3 Parameter Passing and Return Value Passing

Finally, when calling a subroutine, the calling routine must provide to the callee the pa-
rameters, that is, the operands or their addresses, to be used in the computation. Later,
the subroutine may return the results of the computation to the calling routine. This ex-
change of information between a calling routine and a subroutine is referred to as parameter
passing. Parameter passing may occur in several ways. The parameters may be placed in
registers or in the stack, where they can be accessed by the subroutine. Figure 3.18 shows
a parameter being passed to subroutine P through register $a0. The return value is passed
by the subroutine back to the calling program through register $v0.

Calling
Routine

li
jal

$a0, 1
P

# place argument in $a0
# call subroutine P
# use return value stored in $v0

Called
Subroutine

P:

# use parameter stored in $a0

# place return value in $v0
$ra # return

jr

Figure 3.18: Passing Parameters and Return Value through Registers

Many assembly languages have conventions about which registers are used for passing
parameters and return values. For instance, the MIPS-I AL, as we saw, designates 4 regis-
ters, $a0-$a3 for passing parameters and 2 registers, $v0 and $v1, for passing return values.
When one subroutine wants to call another, it may need to save the incoming parameters
(present in registers $a0-$a3) in the stack frame, and copy the outgoing parameters (for
the callee) onto the same registers. Assembly languages with register windows avoid this
overhead by overlapping a portion of two adjacent register windows.

Passing parameters through general-purpose registers is straightforward and e(cid:14)cient.
However, if many parameters are involved, there may not be enough general-purpose reg-
isters available for this purpose. In such a situation, the parameters may be placed on the
caller subroutine’s stack frame, from where the callee subroutine can access them. This is
depicted in the stack frame layout given in Figure 3.17. The stack frame provides a very
(cid:13)exible alternative, because it can handle a large number of parameters. Before calling the
subroutine, the calling program copies all of the parameters to the stack frame. The called
subroutine can access the parameters from the stack frame. Before returning to the calling
program, the return values can also be placed on the stack frame.

146

Chapter 3. Assembly-Level Architecture | User Mode

3.9 De(cid:12)ning Assembly Languages for Programmability

When assembly languages were (cid:12)rst introduced, they were very similar to the lower-level
machine language (ML) they corresponded to, except that they used alphanumeric symbols
instead of binary codes. Thus, instead of coding in a machine language the bit pattern
10001100010000010101010110000010, the programmer could code the same instruction
in an assembly language as lw R1, 0x5582(R2). The assembler would translate each AL
instruction into precisely one ML instruction. With improvements in assembler technology,
this strict correspondence to machine language became relaxed. We now have powerful
assembly languages that provide several additional features. We shall discuss some of these
features below.

3.9.1 Labels

In order to do this, the assembly language programmer has to keep track of the memory
locations that correspond to di(cid:11)erent variables. Keeping track of the memory locations
assigned to variables can be quite tedious for an assembly language programmer. Most
assembly languages therefore provide the ability to symbolically specify the memory location
corresponding to an HLL variable. Each symbolic name in the assembly language program
is eventually replaced by the appropriate memory address during the assembly process.

3.9.2 Pseudoinstructions

Pseudoinstructions are instructions that are present in the assembly-level architecture, but
not in the instruction set architecture. Such instructions are not implemented in the hard-
ware, but are synthesized by the assembler using sequences of instructions present in the
machine language. Modern assembly languages often support di(cid:11)erent types of pseudoin-
structions. For example, the assembly-level architecture may support addressing modes that
are not really present at the ISA level. By extending the instruction set in this manner, the
assembly language makes it easier to program at the assembly level, without adding any
complexity to the hardware. During assembly, each pseudoinstruction is synthesized using
one or more ML instructions.

3.9.3 Macros

Macros go one step beyond pseudoinstructions by allowing the programmer to de(cid:12)ne (or use
prede(cid:12)ned) parameterized sequences of instructions that will be expanded during assembly.
Macros are very helpful in reducing the source code size as well as in making it more
readable, without incurring the overhead of subroutine calls and returns. It is often the case
that a sequence of instructions is repeated several times within a program. An example
of a sequence of instructions might be the operation that pushes data onto the stack, or

3.10. Concluding Remarks

147

the operation that pops data o(cid:11) the stack. A mechanism that lets the assembly language
programmer de(cid:12)ne sequences of instructions, and associate these instructions with a key
word or phrase is called a macro. Macros allow the assembly language programmer to de(cid:12)ne
a level of abstraction.

Simple text-substitution macros can be easily incorporated into an assembly language
by using the C language’s #define construct. The assembler can invoke the C preprocessor
cpp to do the required text substitution prior to carrying out the assembly process. An
example for such a macro de(cid:12)nition and use is given below.

#define LEAF(fname) n
.text; n
.globl
.ent
fname:

fname; n
fname; n

LEAF(foo)

3.10 Concluding Remarks

3.11 Exercises

1. Explain the ma jor di(cid:11)erences between high-level languages and assembly languages.

2. Explain why local variables (the ones declared inside subroutines) are typically as-
signed memory locations in the stack, and not in the .data section of memory.

3. Explain why the memory direct addressing mode cannot be used for accessing data
from the stack and heap sections of memory.

4. An assembly-level architecture de(cid:12)nes directives, non-branch instructions, branch in-
structions, subroutine calls and returns, registers, memory address space, macros,
operand stack, and AL-speci(cid:12)c instructions. Which of these are non-essential from a
strictly functional point of view? Explain.

5. Explain why, during compilation, dynamically allocated variables of a HLL program
are typically assigned memory locations in the heap section of memory and not in the
stack, even if the dynamic allocation takes place inside a subroutine?

6. Consider the following variable declaration and assignment involving pointers in C.
Translate this C code into MIPS-I assembly language code.

148

Chapter 3. Assembly-Level Architecture | User Mode

int **ppn, n;

*ppn = &n;

Example Assignment Statement Involving Pointers

7. Consider the following C code snippet:

i:
j:
k:

.data
.word 24
.word 22
.word 0

.text
__start:la
lw
jal
sw

$t0, i
$a0, 0($t0)
foo
$v0, k

foo:

loop:

$t1, j
lw
li
$v0, 0
addi $v0, $v0, 2
addi $t1, $t1, 1
$t1, $a0, loop
bne
jr
$ra

li
$v0, 10
syscall

# Code for exit system call
# Call OS to exit

(a) Trace the execution of this MIPS-I program for 12 instructions. Tracing of an in-
struction involves showing what value that instruction writes to a register or memory
location. The 12 instructions that you trace must be written in the order in which
they are executed.

(b) How many memory data references will be made during the execution of these 12
instructions? That is, how many values will be transferred from the memory to the
processor?

(c) (2 points) Will this program exit by calling the OS, or will it stay in an in(cid:12)nite
loop? Explain.

Chapter 4

Assembly-Level Architecture |
Kernel Mode

Give instruction to a wise man, and he wil l be stil l wiser;
Teach a just man, and he wil l increase in learning.

Proverbs 9: 9

The previous chapter discussed at length the assembly-level architecture machine seen
by application programmers. We also saw the basics of translating high-level language
application programs to equivalent assembly language programs. As discussed in the last
part of chapter 2, high-level languages provide application programmers with an application
programming interface (API) for specifying system-speci(cid:12)c functions such as input/output
and memory management. The API is implemented by the operating system kernel that
resides in computer systems. When an application program invokes one of the system
call functions speci(cid:12)ed in the API, the control of the computer system is transferred from
the application program to the operating system (OS), which performs the function, and
transfers control back to the application program.

In order to perform the system functions in an adequate and e(cid:14)cient manner, the
machine needs to include special resources to support the OS. Such resources are generally
restricted to the OS, and typically include a few registers, a notable portion of the memory
address space, a few instructions, and direct access to all of the IO device controllers.
Application programs are not allowed to access these restricted resources. In order to enforce
the distinction between application programs (which can only access limited resources) and
OS programs (which can access all resources), computers can operate in at least two di(cid:11)erent
execution modes|the User mode and the Kernel mode, which is also called Supervisor
mode or Privileged mode; in this book we use the term Kernel mode. The Kernel mode is

149

150

Chapter 4. Assembly-Level Architecture | Kernel Mode

intended to execute instructions belonging to the OS, and the User mode is intended to
execute instructions belonging to application programs.

\Every mode of life has its conveniences."
| Samuel Johnson. The Id ler

It is important to note that many of the routines in the OS kernel are executed on behalf
of user programs. For example, the shell program executes in the User mode and invokes
a system call (syscall) instruction to obtain the characters entered by the computer user
on the terminal keyboard. This syscall instruction is implemented by the OS kernel, which
executes in the Kernel mode on behalf of the shell program, reads the characters typed on
the keyboard, and returns the characters to the shell. The shell then executes in User mode,
interprets the character stream typed by the user, and performs the set of actions speci(cid:12)ed
by the user, which might involve invoking other syscall instructions.

Thus, Computer operating systems are another classic example of event-driven programs
on at least two levels. At the lowest level, interrupt handlers act as direct event handlers
for hardware events, with the CPU hardware performing the role of the dispatcher. Op-
erating systems also typically act as dispatchers for software processes, passing data and
software interrupts to user processes that in many cases are programmed as event handlers
themselves.

The discussion so far might suggest that systems spend very little time in the kernel
mode, and that most of the time is spent in the user mode. The truth is just the opposite!
Many embedded systems never leave the kernel mode. A signi(cid:12)cant amount of code is
therefore developed for the kernel mode.

4.1 Overview of Kernel Mode Assembly-Level Architecture

In this chapter, we will study the Kernel mode aspects of the assembly-level architecture of
modern computers. The Kernel mode part is similar in many ways to the User mode part
that we saw in detail in Chapter 3. In particular, the register model, the memory model,
the data types, and the instruction types available to the Kernel mode assembly language
programmer are all quite similar to those available to the User mode assembly language
programmer. Therefore, it is more instructive to consider the di(cid:11)erences between the two
modes, the details of which, unfortunately, vary somewhat from one machine to another.
The main di(cid:11)erences are given below:

(cid:15) In addition to the register set available in the User mode, an additional set of registers
called privileged registers is available in the Kernel mode. One such register is the
processor status register, for instance. The register set available in the Kernel
mode is a superset of what is available in the User mode.

(cid:15) Like the case with the register set, an extended memory address space is usually

4.1. Overview of Kernel Mode Assembly-Level Architecture

151

available in the Kernel mode. The overall Kernel mode memory address space may be
divided between addresses that are accessible only in the Kernel mode and addresses
that are accessible in both modes. User mode programs can access only User mode
addresses. Kernel mode programs can access both Kernel mode and User mode ad-
dresses. For instance, in the MIPS-I assembly-level architecture, memory addresses
from 0x80000000 to 0xffffffff can be accessed only by the OS.

(cid:15) In the Kernel mode, the assembly language programmer is provided a set of IO ports.
These ports are either provided as a set of IO registers or as part of the privileged
memory address space.

(cid:15) In the Kernel mode, the program has access to special hardware structures for perform-
ing resource management functions. One such hardware structure is TLB (Translation
Lookaside Bu(cid:11)er), which is used to implement the virtual memory concept.

(cid:15) In addition to the instruction set available in the User mode, an additional set of in-
structions called privileged instructions is available in the Kernel mode. The privileged
instructions cannot be executed while in User mode. For example, a machine may
have an instruction that manipulates the processor status register. If a User program
uses this instruction, the computer will not execute it, and instead will signal an error.

4.1.1 Privileged Registers

In addition to the register set available in the User mode, an additional set of registers
called privileged registers is available in the Kernel mode. Table 4.1 lists the privileged
registers de(cid:12)ned in the MIPS-I kernel mode architecture, along with their names and uses.
The (cid:12)rst eight registers in the table are used for implementing memory management, and
are explained in Chapter 7. The next 3 privileged registers|sr, cause, and epc|are used
for processor management. sr contains bits that specify the current operating mode and
conditions of the processor, such as the current interrupt priority level. epc is used to
store the memory address of the interrupted instruction, and is useful for returning to the
interrupted program after handling the interrupt/exception.
Its contents can be copied
to a general-purpose register rt by executing the privileged instruction \mfc0 rt, epc".
Finally, the PRId register is used for storing the processor’s generic type number.

Some architectures include a privileged register to point to the current process’ process
control block, the block of memory in the privileged address space where the OS stores
information about the process. Some architectures provide a page table pointer for speeding
up translations of virtual memory addresses to physical memory addresses. In machines
using vectored interrupts, an interrupt vector register may be provided.

152

Chapter 4. Assembly-Level Architecture | Kernel Mode

Register Register
Number Name

Use

0
1
2
4
5
6
8
10
12
13
14
15

Index
Random
EntryLo
Context
PageMask
Wired
BadVaddr
EntryHi
SR
Cause
EPC
PRId

Status register
Store the cause of the most recent exceptional event
Exception PC; store PC value of interrupted instruction
Processor ID register; store this processor’s generic type number

Table 4.1: Names and Uses of MIPS-I Privileged Registers

4.1.2 Privileged Memory Address Space

4.1.3

IO Addresses

The IO models supported by the User mode assembly-level architecture and the Kernel
mode assembly-level architecture are quite di(cid:11)erent. As discussed in Chapters 2 and 3,
the IO model presented to application program developers is at the level of (cid:12)les, and is
somewhat abstract. Any operation on a (cid:12)le is accomplished by calling the operating system.
The IO model presented to operating system developers is more concrete, and involves a
collection of IO addresses, which are accessed by IO instructions. In other words, the IO
primitives provided by the kernel mode machine consist of an IO address space and a set
of (privileged) IO instructions. The exact nature of the IO address space depends on the
type of IO addressing used, and is discussed in Section 4.3.

4.1.4 Privileged Instructions

The Kernel mode architecture includes additional address spaces and registers, as we just
saw.
In order to provide exclusive access to these, an additional set of instructions are
also included in the Kernel mode ISA. These instructions are called privileged instructions.
Examples include instructions to access IO addresses, instructions to manage memory, and
instructions to manage processes. The Kernel mode instruction set is thus a superset of the
User mode instruction set, as pictorially depicted in Figure 4.1. The User mode instruction
set contains a set of syscall instructions, as well as non-syscall instructions for performing

4.2. Switching from User Mode to Kernel Mode

153

data transfer operations between registers and memory, arithmetic/logic operations, and
control (cid:13)ow change operations. At the microarchitecture level (which is two levels below
the assembly level), a non-syscall instruction is directly interpreted for execution, whereas
a syscall instruction is interpreted by invoking a prede(cid:12)ned OS service. That is, a syscall
instruction is interpreted by executing a sequence of instructions in the Kernel mode (some
of which will be privileged instructions), which are then directly interpreted for execution
in the underlying kernel mode microarchitecture.

User Mode

System Call
Instructions

Non−system Call
Instructions

Privileged
Instructions

Kernel Mode

Figure 4.1: Relation between the User Mode and Kernel Mode Instruction Sets

The privileged instructions|which are not available in the User mode|consist of IO
instructions, inter-process synchronization instructions, memory management instructions,
and instructions to enable and disable interrupts. IO instructions include instructions that
read from or write to IO registers. The reason for keeping the IO instructions privileged is
straightforward: if an application program is permitted to execute a privileged instruction,
then it could read con(cid:12)dential data stored anywhere in the system, write on other users’
data, erase all of the information on a disk, and, in general, become a threat to the security
of the system itself. So, what will happen if a programmer includes a privileged instruction
in an application program? When the program is being executed, an attempt to execute
that instruction will generate an exception1 , which causes control to be transferred to the
OS. The OS will most likely terminate that application program.

4.2 Switching from User Mode to Kernel Mode

We can think of three events that cause the execution mode to switch from User mode to
Kernel mode, causing control to transfer from user code to kernel code. They are: syscall
instructions, device interrupts, and exceptions. These three events are illustrated in Figure
4.2, and are discussed in detail in this section. When any of these events happen, the kernel
gets the control and performs the required action. Among these three events, only the
action to be done for syscall instructions is de(cid:12)ned in the API (Application Programming
Interface).

The (cid:12)rst thing the kernel does after getting the control is to disable all interrupts (i.e.,
set the processor state to not accept any more interrupts), and save the essential state of

1Most assemblers will (cid:13)ag this as an error during the assembly process.

154

Chapter 4. Assembly-Level Architecture | Kernel Mode

the interrupted process on the kernel stack2 . This state includes the contents of the process’
general-purpose registers, program counter, and process status word. Afterwards, the kernel
determines the cause of the interrupt, and calls the appropriate low-level handler routine
by looking up a dispatch table containing the addresses of these routines. This low-level
routine performs the functions for which the OS was speci(cid:12)cally called at that time. When
this low-level routine completes, the kernel restores the state of the process, and sets the
execution mode back to the previous value.

Device Interrupts

System Calls

Exceptions

User
Mode

Kernel
Mode

RFE − Returns

Figure 4.2: Events Causing Switching Between User Mode and Kernel Mode

4.2.1 Syscall Instructions: Switching Initiated by User Programs

When an application program is being executed, the program can voluntarily hand over the
machine’s control to the OS by executing a syscall instruction (sometimes called software
interrupt or programmed interrupt). Execution of a syscall instruction is a synchronous
event, because it occurs at the same point of execution when a program is run multiple
times. To the application programmer, a syscall instruction seems very much like a function
call; however, this control (cid:13)ow change causes the processor to switch to the Kernel mode
and to begin executing kernel code. The exact semantics of each system call are de(cid:12)ned
in the API, and can be di(cid:11)erent, at least theoretically, in di(cid:11)erent APIs. For producing
portable code, however, the semantics of each system call are kept more or less the same
across APIs. To be on the safe side, it is prudent for application programs not to directly
call the OS, but instead call an appropriate library routine. When porting to a platform
with a di(cid:11)erent API, all that is required then is to use a di(cid:11)erent set of library routines
that suit the new API.

When a syscall instruction is executed, the computer temporarily stops execution of the
current program, switches to Kernel mode, and transfers control to a special OS routine
called system call layer. Figure 4.3 illustrates how transfer of control takes place when
a syscall instruction is executed. As shown, the syscall instruction is treated very similar

2Some operating systems save the state in the interrupted process’ user stack. Some others save the state
in a global interrupt stack that stores the frames for those interrupt handlers that are guaranteed to return
without switching context.

4.2. Switching from User Mode to Kernel Mode

155

to a jal S (jump and link) instruction, where S is the address of the (cid:12)rst instruction of
the system call layer routine. Thereafter the machine executes the instructions of this part
of the OS. After executing this routine, the eret instruction at the end of the routine
causes control to return to the instruction that immediately follows the syscall instruction
in the application program. The system call layer routine is very much like a subroutine;
an important di(cid:11)erence, however, is that it executes in Kernel mode.

User Mode

Application
Program

syscall

Kernel Mode

System Call
Interface
Routine

S:

#enable interrupts

eret

Similar to ‘jal S’ instruction

Figure 4.3: Transfer of Control while Executing a System Call Instruction

It is important to see the ma jor actions speci(cid:12)ed by a syscall instruction. These functions
are described below:

(cid:15) Switch to kernel mode

(cid:15) Disable interrupts: One of the (cid:12)rst actions to be performed by the system call layer
when control transfers to it is to save the current register state and perform other
book-keeping functions.
If another interrupt is accepted during this period, there
is a potential to loose useful data. Therefore, it is prudent to temporarily disable
interrupts. After performing the book-keeping functions, the handler may enable
interrupts of higher priority.

(cid:15) Save return address: The syscall instruction is similar to a subroutine call in many
ways. One of the striking similarities is in the manner of control (cid:13)ow return. When
control returns to the program that contains the syscall instruction, execution contin-
ues from the next instruction onwards. In order to e(cid:11)ect such a control (cid:13)ow transfer,
the return address (the address of the instruction immediately after the syscall in-
struction in the static program) needs to be recorded. The MIPS-I architecture, for
instance, speci(cid:12)es a privileged register called epc (exception program counter) for
storing this return address.

(cid:15) Record the cause for this exceptional event: Once the syscall instruction is executed
and control is transfered to a handler, the system has no way of remembering the

156

Chapter 4. Assembly-Level Architecture | Kernel Mode

reason for activating the handler. This is especially the case if multiple exceptional
events transfer control to the same entry point, i.e., the same handler. For instance,
the MIPS-I architecture speci(cid:12)es the same entry point (0x80000080) for all but two of
the exceptional events. To identify the reason for transferring control to this memory
location, the MIPS-I architecture provides a privileged register called cause.

(cid:15) Update pc to point to the entry point associated with syscall instructions. For the
MIPS-I architecture, this entry point is 0x80000080.

4.2.2 Device Interrupts: Switching Initiated by IO Interfaces

The syscall instruction is useful when an application program needs some service from the
OS. Sometimes, the currently executing application program may not need any service from
the OS, but an IO device needs attention, requiring the OS to be executed. This requirement
has led to the provision of device interrupts (also called hardware interrupts) by which an
IO device can notify the computer when it requires attention. Unlike a system call, a device
interrupt is an asynchronous event, because it may not occur at the same point of execution
when a program is run multiple times.

In order to run the OS, the currently running program has to be temporarily stopped.
Therefore, when an interrupt is received, the computer temporarily stops execution of the
current program and transfers control to a special OS routine called interrupt service
routine (ISR) or interrupt handler.
If the machine was in the User mode at the
time of the interrupt, then it is switched to the Kernel mode, giving the operating system
privileged access to the machine’s resources. An ISR is very much like the subroutines that
we saw earlier; an important di(cid:11)erence, however, is that a subroutine performs a function
required by the program from which it is called, whereas the ISR may have nothing in
common with the program being executed at the time the interrupt request is received.
The exact manner in which the interrupting device is identi(cid:12)ed and the appropriate ISR is
called varies from one machine to another.

Figure 4.4 illustrates one way of transfering control to the ISR when an interrupt is
raised. Assume that an interrupt request arrives during the execution of instruction i. The
computer (cid:12)rst completes the execution of i. Then it transfers control to an OS routine called
interrupt hander interface. This routine identi(cid:12)es the interrupting device, and determines
the starting address of the appropriate ISR, namely P in the (cid:12)gure.
It then transfers
control to the ISR by means of a CALL instruction. Thereafter the machine executes the
instructions of the ISR. After executing the ISR, control is returned to the interrupt hander
interface. The interface routine is terminated by an ERET instruction, which causes control
to return to instruction i + 1 in the interrupted program. Along with this, the computer
also switches back to the User mode.

Because an interrupt is an unscheduled event, the ISR must save and restore any registers
that it modi(cid:12)es. A convenient place to save the registers is the kernel stack.

interrupt
service
routine,
interrupt
handler

4.2. Switching from User Mode to Kernel Mode

157

Interrupted
Program

Kernel Mode

Interrupt
Handler
Interface

Interrupt
Service
Routine

S:

P:

Interrupt occurs

i
i +1

eret

jr $ra

Equivalent to ‘jal S’ instruction

Figure 4.4: Transfer of Control while Servicing an Interrupt

In a multi-tasking environment, whenever the machine switches to the Kernel mode,
handing control over to the OS, it also performs process scheduling. That is, the OS decides
which application process should run next. For the OS to do this process scheduling, it
must run periodically. However, extended periods of time may elapse with the computer
being in the User mode and no syscall instructions or device interrupts. In order to perform
process scheduling in an adequate manner, the OS needs to gain control of the machine on
a periodic basis. Multi-tasking computers typically implement this by including as an IO
device a hardware timer, which issues a hardware interrupt at regular intervals (cid:12)xed by the
OS.

4.2.3 Exceptions: Switching Initiated by Rare Events

\The young man knows the rules, but the old man knows the exceptions".
| Oliver Wendel l Holmes, Sr (American Physician, Poet, Writer, Humorist and
Professor at Harvard, 1809-1894) in The Young Practitioner

An exception is an unexpected event generated from the program being executed. Ex-
amples are attempt to execute an unde(cid:12)ned instruction, arithmetic over(cid:13)ow, and divide
by zero. When an exception occurs, the machine switches to Kernel mode and generates
an exception vector depending on the type of exception. The exception vector indicates
the memory address from which the machine should start execution (in Kernel mode) after
it detected the exceptional event. In other words, after an exceptional event occurs, the
machine starts executing in Kernel mode from the address speci(cid:12)ed in the exception vector.
The machine may also record the cause of the exception, usually in a privileged register.
The rest of exception handling is similar to that of interrupt handling. That is, the return
address is saved, and control is transfered to an exception handler routine in the OS.

The exception handler routine at the exception vector performs the appropriate actions.
If that vector is used for di(cid:11)erent exception types, then the routine inspects the recorded
cause of the exception, and other relevant state information, and branches to an appropri-

158

Chapter 4. Assembly-Level Architecture | Kernel Mode

ate exception handler routine to handle the exception. After taking the necessary steps,
control may be returned to the application program that caused the exception, switching
the mode back to the User mode. Sometimes, exception handling may involve terminating
the application program that generated the exception, in which case the OS gives control
to another application program.

4.3

IO Registers

We saw in Section 4.1 that the IO model supported in the kernel mode consists of a set of
IO registers and a set of instructions to access them. The nature of these IO registers vary
considerably, depending on the addressing method used. Two types of addressing methods
are used for IO registers: memory mapped IO and independent IO (or IO mapped IO). We
shall discuss these two schemes in detail.

4.3.1 Memory Mapped IO Address Space

In memory mapped IO, each IO address refers to an IO register, an entity similar to a
memory location that can be read/written. Because of this similarity, the IO registers are
assigned locations within the memory address space itself. Thus, there is a single address
space for both memory locations and IO registers. Some portions of the memory address
space are assigned to IO registers; loads and stores to those addresses are interpreted as reads
and writes to IO registers. The main advantage with this approach is that no extensions are
required to the instruction set to support IO operations 3 . Another advantage is that it allows
for wider compatibility among IO speci(cid:12)cations across di(cid:11)erent computer families. The
MIPS-I architecture uses this approach; memory addresses from 0xa0000000 to 0xbfffffff
are available to the OS for use as IO registers. An example IO read instruction for the MIPS-
I architecture is

lw

$t1, keyboard status

which copies the contents of IO register labeled keyboard status to general-purpose register
$t1. At execution time, this label will refer to an address in the range 0xa0000000 -
0xbfffffff. This mapping is done either at assembly time by the assembler or at IO port
con(cid:12)guration time by the OS.

4.3.2

Independent IO Address Space

In this type of IO addressing, a separate IO address space is provided independent of the
memory address space. Thus, like the register space and the memory address space, there
is an IO address space also. When accessing an address in the IO address space, an IO

3At the microarchitectural level, the hardware has to distinguish IO operations from memory operations,
and treat them accordingly.

4.3.

IO Registers

159

address can be speci(cid:12)ed either by a new addressing mode, or by a new set of opcodes. It is
customary to use the latter approach | providing a separate set of opcodes speci(cid:12)cally for
manipulating IO addresses. An example IO read instruction for a MIPS-I-like architecture
would be

in

$t1, keyboard status

What do we gain by providing a separate address space for the IO (and a separate set
of opcodes as well)? On (cid:12)rst glance, there seems to be no apparent gain. Now consider this
scenario. When we have a separate IO address space, instead of organizing it as a linear
address space, we have the (cid:13)exibility of organizing it as a set of IO ports or IO programming
interfaces, entities that are more complex than IO registers and memory locations. That
is, each IO address refers to an IO port. Several instruction opcodes can be provided to
perform complex operations on an IO port. An example IO instruction that tests the status
of a port used for connecting a keyboard is
keyboard port
test

Examples of machines with independent IO are the Intel x86 and the IBM 370 computers.
Figure 4.5 illustrates the two types of IO address mapping. It is important to note that
the two types of IO addressing are not mutually exclusive.
In a machine that supports
independent IO, some of the memory addresses can still be mapped to IO registers. For
instance, a graphics display port is usually memory-mapped, to permit device drivers to
easily modify bit patterns in memory, which are then displayed on the screen.

Memory
address

Hard disk
Terminal
Printer

Memory address
space

Memory address
space

Memory
address

IO address space

Hard disk
Terminal
Printer

IO
address

IO address
space

Data

Data

Data

(i)

(ii)

Figure 4.5: Di(cid:11)erent IO Address Mappings: (i) Memory Mapped IO; (ii) Independent IO

160

Chapter 4. Assembly-Level Architecture | Kernel Mode

4.3.3 Operating System’s Use of IO Addresses

Irrespective of the type of IO addressing used, the OS views the IO address space as a
collection of IO ports or IO programming interfaces. The speci(cid:12)cations of di(cid:11)erent IO
ports can be di(cid:11)erent, and are determined based on the type of IO devices that are meant
to be connected to them. Most of the ports adhere to one standard or other, as we will see
in Chapter 8.

In an ISA that speci(cid:12)es IO registers, each IO port then encompasses several consecutive
IO registers. Depending on the characteristics of the port, some of its registers are used to
record status information related to the port (IO status registers); individual bits of a status
register may correspond to a di(cid:11)erent attribute of the port for example, the least signi(cid:12)cant
bit may specify whether a new character has been typed on the keyboard, the next bit may
specify whether an error has occurred, etc.). Other registers may be used to store data
to be transferred between IO ports and general-purpose registers or main memory. These
registers are called IO data registers. Some of the registers in a port may be viewed as
read-only, some may be write-only, and the rest may be read-write. The OS decides which
IO port needs to be accessed to access a particular (cid:12)le.

Notice that all that the OS program \sees" of an IO device is its port, which includes a
set of IO registers and speci(cid:12)cations for the operation of these registers. When an IO write
instruction writes a value to a status/control register in an IO port, the device controller
interprets it as a command to a particular IO device. When an OS routine wants to know
the status of an IO device, it uses an IO read instruction to read the status register of its IO
port. Similarly, an IO write instruction can be executed by an OS routine to send data to
the data register or status register of the IO port pertaining to an IO device. An example
IO write instruction for the MIPS-I architecture is
$t1, keyboard status
sw

where keyboard status is the address assigned to the status register of the IO port per-
taining to the keyboard.

The astute reader would have realized by now that the IO model presented to OS
programmers, although more concrete than the one presented to applications programmers,
still does not provide direct access to the IO devices. That is, the instructions in the OS
program do not directly access the IO device hardware. There are a variety of reasons for
this:

It would be impractical for the OS device
(cid:15) The IO devices are incredibly diverse.
drivers to incorporate the necessary functionality to directly control a wide range of
IO devices. For example, if the OS directly accesses a hard disk, to get a data item
from the disk, the device driver would need to execute many instructions that deal
with the intricate details of how exactly that disk works. In the future, if this disk is
replaced by a slightly di(cid:11)erent hard disk, then the device driver (to access the disk)
may also need to be changed.

4.3.

IO Registers

161

Interconnect

Registers

Main
Memory

Status/Control Register

Status/Control Register

Status/Control Register

Data Register

Data Register

Data Register

IO Port

IO Port

IO Port

Figure 4.6: Abstraction of IO Devices Presented to the OS

(cid:15) IO devices are often electromechanical devices whose manner of operation is di(cid:11)erent
from that of the rest of the machine, which are implemented using electronic devices.
Therefore, a conversion of signal values may be required.

(cid:15) The data transfer rate of IO devices is often much lower than that of the rest of the
system, necessitating special synchronization operations for correct transfer of data.

(cid:15) IO devices often use data formats and word lengths that are di(cid:11)erent from those
speci(cid:12)ed in the kernel mode.

Because IO ports are modeling electromechanical IO devices, the behavior of IO registers
can be quite di(cid:11)erent from that of ordinary registers and memory locations. An assembly
language systems programmer needs to be aware of these di(cid:11)erences. The following di(cid:11)er-
ences may come as surprises:

(cid:15) IO registers may be active elements, and not just passive storage elements. A write
to an IO register often has side e(cid:11)ects, such as initiating an activity by the IO device
connected to the port. A read to an IO register may also have side e(cid:11)ects, such as
clearing an interrupt request or clearing an error condition.

(cid:15) IO registers may have timing characteristics that are di(cid:11)erent from ordinary memory.
If a write to an IO register is expected to produce a visible change in that or some other
register, the device driver programmer may need to introduce a pause, for example
by executing nops, to give the device time to respond.

162

Chapter 4. Assembly-Level Architecture | Kernel Mode

(cid:15) A read to an IO register may not necessarily return the value that was last written
to it. This is because some bits do not exist (always zero or meaningless), and some
others do not store a value, but are only sensitive to the value conveyed to them
during a write operation. Sometimes the contents or meaning of a bit varies, based
on the contents of other IO registers. In some cases, a single IO register may serve
the dual purpose of being a command register as well as a status register. The device
driver writes to this IO register to send commands to the interface, and reads from
the same register to obtain status information. Depending on whether the register is
being read or written, the contents associated with the register are di(cid:11)erent 4 ! Finally,
an IO register may be updated by the IO device connected to the port.

The behavior of an IO register depends on the speci(cid:12)cs of the IO port to which it belongs.
The speci(cid:12)cations of an IO port include the characteristics of its IO registers. Often, it
is possible to program the behavior of IO registers by writing speci(cid:12)c commands in the
control/status registers of the same IO port. All of this depends on the port speci(cid:12)cations.
It is therefore important to study the device’s manual, and learn how it functions, before
writing device drivers that control that IO device.

Although the above discussion seems to imply that the grouping of IO registers into IO
ports is done at the time of OS development, that is rarely the case. The behavior of a
port is very much dependent on the IO device it models. At the time of writing an OS,
it is di(cid:14)cult to know which devices will be eventually connected to a particular computer
system that uses that OS. Therefore, in practice, the OS is split into two parts|the kernel
and the device drivers (or device handlers, or IO handlers, or software drivers). The kernel
consists of the parts that do not change from system to system. The device drivers are
speci(cid:12)c to each computer system, and are usually added on when each IO device is hooked
to the system.

Standard IO Ports: We will see later how this IO model is used for writing assem-
bly language device driver routines that can do speci(cid:12)c IO operations, such as reading a
character from a keyboard.

4.4 Operating System Organization

In modern computers, the operating system is the only software component that runs in the
Kernel mode. It behooves us therefore to consider the structure and implementation of an
operating system. In particular, it is important to see how the system calls speci(cid:12)ed in the
application programming interface (API) (provided to user programs and library functions)

4At the microarchitectural level, the IO interface module typically implements such a register by two
separate registers. It is reasonable to ask why the assembly-level architecture and the ISA de(cid:12)ne a single IO
register for dual functions. The reason is to conserve the IO address space, which was once a scare resource.

4.4. Operating System Organization

163

are implemented by operating systems. The exact internal details of an operating system
vary considerably from one system to another.
It is beyond the scope of this book to
discuss di(cid:11)erent possibilities. Figure 4.7 gives a possible block diagram, which is somewhat
similar to that of a standard UNIX kernel.
In the (cid:12)gure, the kernel mode software blocks
are shown shaded. The main components of the OS software include a system call layer,
(cid:12)le system, process control system, and the device management system (device drivers).
This organization uses a layered approach, which makes it easier to develop the OS and to
introduce modi(cid:12)cations at a later time. This also makes it easier to debug the OS code,
because the e(cid:11)ect of bugs may be restricted to a single layer. The (cid:12)gure also shows the
relationship of the OS to user programs, library routines, and the hardware.

HCI

User Mode
Software

ABI

OS Interface

Application Programs and Shell

Dynamically Linked Libraries
System Calls

System Call Layer

Device−
Independent

File System

Kernel Mode
Software

IO Management
Character−
Network−
Oriented
Oriented

Block −
Oriented

Process Control System

Inter−process
Communication

Scheduler

Memory
Management

User Mode
Instructions

Device−
Dependent

Device Drivers

OS Kernel

s
t
n
e
v
E
 
l
a
n
o
i
t
p
e
c
x
E

ISA

Kernel Mode

User Mode

Device Controllers

Hardware

IO Registers

IO Control Logic

IO Devices

Privileged Registers and Memory User Mode Registers and Memory
Control Unit, ALU, and Memory Controllers

Device Interrupts

Program Control Transfers Initiated by Software

Program Control Transfers Initiated by Hardware

Hardware Accessed/Controlled

Hardware Buses

Hardware Connections

Figure 4.7: Block Diagram Showing the Structure of a UNIX-like Operating System

164

Chapter 4. Assembly-Level Architecture | Kernel Mode

4.4.1 System Call Layer

The system call layer provides one or more entry points for servicing syscall instructions and
exceptions, and in some cases device interrupts also. The user program conveys the system
call type by placing the system call number on the user stack or in a register; the MIPS-I
assembly language convention is to use register $2 for this purpose. The system call layer
copies the arguments of the system call from the user stack (or registers) and saves the user
process’ context, possibly on the kernel stack. It then uses the system call number to look up
a system cal l dispatch vector to determine the kernel function to be called to implement that
particular system call, interrupt, or exception. It then calls that kernel function, sometimes
mapping or converting the arguments. When this kernel function completes, the system
call layer sets the return values and error status in the user stack (or registers), restores
the user process’ context, and switches to User Mode, transferring control back to the user
process.

We can summarize the functions performed by the system call layer:

(cid:15) Determine type of syscall

(cid:15) Save registers

(cid:15) Call appropriate hander

(cid:15) Restore registers

(cid:15) Return to user program

4.4.2 File System

The API provided to application programs by the operating system, as we saw earlier,
includes device-independent IO. That is, the interface is the same,
irrespective of the
physical device that is involved in the IO operation. The (cid:12)le abstraction part of the API
is supposed to hide all device-speci(cid:12)c aspects of (cid:12)le manipulation from HLL application
programmers, and provide them with an abstraction of a simple, uniform space of named
(cid:12)les. Thus, HLL application programmers can rely on a single set of (cid:12)le-manipulation OS
routines for (cid:12)le management (and IO device management in an indirect manner). This is
device-
independent
sometimes referred to as device-independent IO.
IO

As we saw in Section 3.4.7, application programs access IO (i.e., (cid:12)les) through read and
write system calls. The read and write system calls (of the User mode) are implemented
in the Kernel mode by the (cid:12)le system part of the OS, possibly with the help of appropriate
device drivers.

Files of a computer installation may be stored on a number of physical devices, such as
disk drives, CD-ROM drives, and magnetic tapes, each of which can store many (cid:12)les. If the
IO device is a storage device, such as a disk, the (cid:12)le can be read back later; if the device is

4.4. Operating System Organization

165

a non-storage device such as a printer or monitor, the (cid:12)le cannot be read back. Di(cid:11)erent
(cid:12)les may store di(cid:11)erent kinds of data, for example, a picture, a spreadsheet, or the text of
a book chapter. As far as the OS is concerned, a (cid:12)le is simply a sequence of bytes written
to an IO device.

The OS partitions each (cid:12)le into blocks of (cid:12)xed size. Each block in a (cid:12)le has an address
that uniquely tells where within the physical device the block is located. Data is moved
between main memory and secondary storage in units of a single block, so as to take
advantage of the physical characteristics of storage devices such as magnetic disks and
optical disks.

File management related system calls invoked by application programs are interpreted
by the (cid:12)le system part of the OS, and transformed into device-speci(cid:12)c commands. The
process of implementing the open system call thus involves locating the (cid:12)le on disk, and
bringing into main memory all of the information necessary to access it. The OS also
reserves for the (cid:12)le a bu(cid:11)er space in its memory space, of size equal to that of a block.
When an application program invokes a system call to write some bytes to a (cid:12)le, the (cid:12)le
system part of the OS writes the bytes in the bu(cid:11)er allotted for the (cid:12)le. When the bu(cid:11)er
becomes full, the (cid:12)le system copies it into a block in a storage device (by invoking the
device’s device driver); this block becomes the next block of the (cid:12)le. When the application
process invokes the close system call for closing a (cid:12)le, the (cid:12)le system writes the (cid:12)le’s bu(cid:11)er
as the (cid:12)nal block of the (cid:12)le, irrespective of whether the bu(cid:11)er is full or not, prior to closing
the (cid:12)le. Closing a (cid:12)le involves freeing up the table space used to hold information about
the (cid:12)le, and reclaiming the bu(cid:11)er space allotted for the (cid:12)le.

4.4.3 Device Management: Device Drivers

The device management part of the OS is usually implemented separate from the kernel,
to facilitate easy adding or removal of devices. It is actually a collection of device drivers.
Most computers have input/output devices and storage devices such as disks, terminals,
and printers. Each of these devices requires speci(cid:12)c device driver software, which acts
as an interface between the device controller and the (cid:12)le system part of the OS kernel.
A speci(cid:12)c device driver is important, because each device has its own speci(cid:12)c commands
instead of generic commands. Each device driver itself is a collection of routines, and can
have multiple entry points. The device driver receives generic commands from the OS (cid:12)le
system and converts them into the specialized commands for the device, and vice versa. To
the extent possible, the driver software hides the unique characteristics of a device from OS
(cid:12)le system.

A device driver, or a software driver is a speci(cid:12)c type of computer software, developed
to interact with hardware devices. This usually constitutes an interface for communicating
with the device, through the speci(cid:12)c computer bus or communications subsystem that the
hardware is connected to, providing commands to and/or receiving data from the device, and
on the other end, the requisite interfaces to the operating system and software applications.

166

Chapter 4. Assembly-Level Architecture | Kernel Mode

Because of its interfacing nature, it is speci(cid:12)c to the hardware device as well as to the
operating system.

The key design goal of device drivers is abstraction. Every model of hardware (even
within the same class of device) is di(cid:11)erent. Newer models also are released by manufac-
turers that provide more reliable or better performance and these newer models are often
controlled di(cid:11)erently.

The operating system cannot be expected to know how to control every device, both now
and in the future. To solve this problem, operating systems essentially dictate how every
type of device should be controlled. The function of the device driver is then to translate
these OS mandated function calls into device speci(cid:12)c calls. In theory a new device, which
is controlled in a new manner, should function correctly if a suitable driver is available.
This new driver will ensure that the device appears to operate as usual from the operating
systems’ point of view.

Device drivers can be fairly complex. Many parameters may need to be set prior to
starting a device controller, and many status bits may need to be checked after the comple-
tion of each device operation. Many device drivers such as the keyboard driver are supplied
as part of the pre-installed system software. Device drivers for other devices need to be
installed as and when these devices are installed.

The routines in a device driver can be grouped into three kinds, based on functionality:

(cid:15) Autocon(cid:12)guration and initialization routines

(cid:15) IO initiation routines

(cid:15) IO continuation routines (interrupt service routinestem Autocon(cid:12)guration and initial-
ization routines

(cid:15) IO initiation routines

(cid:15) IO continuation routines (interrupt service routines)

The autocon(cid:12)guration routines are called at system reboot time, to check if the corre-
sponding device controller is present, and to perform the required initialization. The IO
initiation routines are called by the OS (cid:12)le system or process control system in response to
system call requests from application programs. These routines check the device status, and
initiate IO requests by sending commands to the device controller. If program-controlled
IO transfer is used for the device, then the IO initiation routines perform the IO transfers
also. By contrast, if interrupt-driven IO transfer is used for the device, then the actual IO
transfer is done by the interrupt service routines when the device becomes ready and issues
an interrupt.

4.4. Operating System Organization

167

4.4.4 Process Control System

4.4.4.1 Multi-Tasking

When a computer system supports multi-tasking, each process sees a separate virtual ma-
chine, although the concurrent processes are sharing the same physical resources. Therefore,
some means must be provided to separate the virtual machines from each other at the phys-
ical level. The physical resources that are typically shared by the virtual machines are the
processor (including the registers, ALU, etc), the physical memory, and the IO interfaces.
Of these, the processor and the IO interfaces are typically time-shared between the pro-
cesses (temporal separation), and the physical memory is partitioned between the processes
(spatial separation)5 . To perform a context switch of the virtual machines, the time-shared
resources must be switched from one virtual machine to the next. This switching must be
managed in such a way that the virtual machines do not interact through any state infor-
mation that may be present in the physically shared resources. For example, the ISA-visible
registers must be saved and restored during a context switch so that the new context cannot
access the old context’s register state.

Decisions regarding time-sharing and space-sharing are taken in the Kernel mode by the
operating system, which is responsible for allocating the physical resources to the virtual
machines. If a user process is allowed to make this decision, then it could possibly encroach
into another process’ resources, and tamper with its execution. The operating system’s
decisions, however, need to be enforced when the system is in the User mode. This enforce-
ment is done using special hardware (microarchitectural) support so that the enforcement
activity does not reduce performance.

4.4.4.2 Multi-Programming

Some applications can be most conveniently programmed for two or more cooperating pro-
cesses running in parallel rather than for a single process. In order for several processes to
work together in parallel, certain new Kernel mode instructions are needed. Most modern
operating systems allow processes to be created and terminated dynamically. To take full
advantage of this feature to achieve parallel processing, a system call to create a new pro-
cess is needed. This system call may just make a clone of the caller, or it may allow the
creating process to specify the initial state of the new process, including its program, data,
and starting address. In some cases, the creating (parent) process maintains partial or even
complete control over the created (child) processes. To this end, Kernel mode instructions
are added for a parent to stop, restart, examine, and terminate its children.

5Time-sharing the entire physical memory is not feasible, because it necessitates saving the physical
memory contents during each context switch.

168

Chapter 4. Assembly-Level Architecture | Kernel Mode

4.5 System Call Layer for a MIPS-I OS

We just saw a functional organization of an operating system, and the important functions
performed by each ma jor block. To get a better appreciation of what the OS code looks
like, let us get our feet wet with a detailed real-life example. In this section, we discuss the
barebones of the system call layer of an OS for a MIPS-I machine. The system call layer
implements the interface for system calls, device interrupts, and exceptions. We restrict
ourselves to the system call layer for two reasons: (i) It is perhaps the smallest ma jor block
in an OS (and therefore manageable to be discussed in a book of this scope), but is detailed
enough re(cid:13)ect many of the idiosyncrasies that make OS routines di(cid:11)erent from application
programs. (ii) It is the block that directly interacts with application programs, which is
what many of the programmers care about.

4.5.1 MIPS-I Machine Speci(cid:12)cations for Exceptions

Software is always written for a speci(cid:12)c (abstract) machine speci(cid:12)cation. Before writing
assembly-level systems software for the system call layer of a MIPS-I OS, we need to know
how the MIPS-I machine speci(cid:12)es information regarding the occurrence of system calls,
device interrupts, and exceptions.
Interestingly, MIPS-I does not make a big distiction
between the 3 categories|system calls, device interrupts, and exceptions|when reporting
their occurrence. It treats them all as exceptions! To be speci(cid:12)c, it does 3 things when an
exceptional event occurs:

(cid:15) It modi(cid:12)es privileged register sr (status register) to disable device interrupts and
to re(cid:13)ect Kernel mode of operation.

(cid:15) It stores the restart instruction’s address in privileged register epc.

(cid:15) Generates an exception vector depending on the type of exception.

The MIPS-I architecture provides only 3 exception vectors, in contrast to many others that
provide a much larger set of exception vectors. These 3 vectors are stated below:

(cid:15) 0xbfc00000: This exception vector is speci(cid:12)ed at computer reset. Thus, after a reset,
the computer starts executing in the Kernel mode the program starting at memory
address 0xbfc00000.

(cid:15) 0x80000000: This exception vector is speci(cid:12)ed when a User TLB miss exception
occurs; Chapter 7 discusses this case in detail.

(cid:15) 0x80000080: This exception vector is speci(cid:12)ed when any other exceptional event
occurs.

4.5. System Call Layer for a MIPS-I OS

169

ExcCode Mnemonic Expansion
Interrupt
Int
0
1
Mod
TLB Modi(cid:12)cation Exception

2

3
4

5
6

7
8
9
10
11

12
13-15

TLBL

TLB Load Exception

TLBS
AdEL

AdES
IBE

DBE
Sys
Bp
RI
CpU

Ovf
(cid:0)

TLB Store Exception
Address Error Load Exception

Address Error Store Exception
Instruction Bus Error Exception

Data Bus Error Exception
Syscall
Breakpoint
Reserved Instruction Exception
Co-processor Unusable Exception

Over(cid:13)ow Exception
Reserved

Description
Device interrupt
Attempt to write to an address
marked as read-only
TLB miss for load
or instruction fetch
TLB miss for store
Address error for load
or instruction fetch
Address error for store
Bus error for
instruction fetch
Bus error for data reference
syscall instruction
break instruction
Illegal instruction
Software can emulate the
o(cid:11)ending instruction
Arithmetic over(cid:13)ow

Table 4.2: Exceptions Corresponding to Di(cid:11)erent Values of ExcCode Field of Cause register
in a MIPS-I Architecture

The third exception vector (0x80000080) corresponds to di(cid:11)erent types of exceptional
events, and so there must be some provision for the OS to know the exact cause of the
exceptional event whenever this exception vector is generated. The MIPS-I architecture
de(cid:12)nes a privileged register called Cause for recording the cause of the most recent excep-
tional event. This register has a 4-bit ExcCode (cid:12)eld, which holds an encoded bit pattern
corresponding to the exception cause. Thus, a maximum of 16 di(cid:11)erent exception types
can be uniquely speci(cid:12)ed by the machine in this (cid:12)eld. Table 4.2 shows the ExcCode values
for di(cid:11)erent types of exceptional events. Interestingly, a single value (0) corresponds to all
types of device interrupts; however, there is an IP (interrupt pending) (cid:12)eld in Cause which
helps to do some di(cid:11)erentiation between the interrupt sources. The OS can thus perform
selective polling (of appropriate IO registers) to determine the exact cause for the interrupt.
Similarly, a single value (8) corresponds to all types of syscall instructions; the OS can
determine the type of syscall by inspecting the contents of register $2.

170

Chapter 4. Assembly-Level Architecture | Kernel Mode

4.5.2 OS Usage of MIPS-I Architecture Speci(cid:12)cations

With the above background on the features provided in the MIPS-I architecture to support
exceptional events, let us turn our attention to the barebones of a typical system call layer
used in MIPS-I OSes. This interface code provides 3 entry points, corresponding to the 3
exception vectors: 0xbfc00000, 0x80000000, and 0x80000080. Figure 4.8 shows these 3
entry points and the placement of the exception handler code in the MIPS-I kernel address
space. The third entry point is common for a number of exceptional events, and so the
routine at that entry point checks the Cause register to determine what caused the event.
Depending on the contents of the ExcCode (cid:12)eld of Cause, an appropriate handler is called.
This checking is similar to that of implementing the C switch statement, and the standard
way for making this selection is by means of a jump table that contains the starting addresses
of the handler routines.

Kernel Address Space

Addresses
0x0000 0000

2 GB

User programs,
data

2 GB

0x8000 0000

0xffff fffc

UTLB Miss Handler

0x8000 0000

General Handler

0x8000 0080

Boot Program

0xbfc0 0000

Figure 4.8: Placement of the Exception Handler Code in the MIPS-I Kernel Address Space

In the skeleton code given below, the jump table containing handler addresses is called
handler table. It is placed in the kernel’s data section, and is initialized statically (i.e., at
assembly time as opposed to run time).

Let us take a closer look at the handlers themseleves, which are placed in the kernel’s
text section. The (cid:12)rst handler given here|the one starting at address 0x80000000|deals

4.5. System Call Layer for a MIPS-I OS

171

with user TLB miss exceptions, and is discussed in detail in Chapter 8. It is included here
just for the sake of completeness. The second handler, which starts at address 0x80000080,
deals with general exceptional events. The (cid:12)rst thing it does is to save the register values
of the interrupted process. In particular, registers s0-s7, k0, k1, sp, and epc need to be
saved. A good place to save them is the kernel stack 6 .
At the end of the handler code, we have a pair of instructions, jr $k1 and rfe, which
merit further discussion. The jr $k1 instruction speci(cid:12)es the transfer of control back to
the interrupted program. The rfe (restore from exception) instruction tells the machine to
restore the system’s mode and interrupt status to what it was prior to taking this exception;
thus, it puts the system back in the condition in which the interrupted program was running.
What is a good place to include the rfe instruction in the exception handler? If the rfe
instruction is placed before the jr instruction, then the machine may try to execute in user
mode the jr instruction, which is in the kernel code space. If, on the other hand, the rfe
instruction is placed after the jr instruction, then control is transfered to the interrupted
program, preventing the execution of the rfe instruction. What we really require is that
these two instructions must be executed atomical ly, meaning this two-instruction sequence
should be done together, without any interruptions 7 .

##################################################################################
# Handler Table
##################################################################################
# Store subsequent items in kernel data section
.kdata
.align 2
handler table:
.word
IntHandler
ModHandler
.word
TLBLHandler
.word
TLBSHandler
.word
.word
AdELHandler
AdESHandler
.word
IBEHandler
.word
DBEHandler
.word
.word
SysHandler

# Initialize to interrupt handler address
# Initialize to modi(cid:12)cation exception handler address
# Initialize to TLB load miss exception handler address
# Initialize to TLB store miss exception handler address
# Initialize to load address error exception handler address
# Initialize to store address error exception handler address
# Initialize to instruction bus error exception handler address
# Initialize to data bus error exception handler address
# Initialize to syscall handler address

6As with the user stack, most of the RISC machines do not provide direct support for a kernel stack
either. The kernel stack is therefore implemented within part of the memory address space. Like the user
stack, it is accessed by using the register-displacement addressing along with one of the general-purpose
registers (which serves as the kernel stack pointer). Therefore, the kernel cannot save onto the kernel stack
the value stored in this register by the interrupted program.
If the interrupted program was using this
register for its purposes, this presents a problem. The MIPS-I assembly language convention to solve this
dilemma is to reserve registers $k0 and $k1 (i.e., $26 and $27) for use by the OS.
7At the ISA level, the MIPS-I jr instruction uses the concept of delayed branching; i.e., the transfer of
control induced by the jr instruction takes e(cid:11)ect only after executing the immediately following instruction,
which in this case is the rfe instruction. The later versions of MIPS have a kernel mode eret (exception
return) instruction, which performs both actions in a single instruction.

172

Chapter 4. Assembly-Level Architecture | Kernel Mode

.word
.word
.word
.word

BpHandler
RIHandler
CpUHandler
OvHandler

# Initialize to breakpoint handler address
# Initialize to reserved instruction exception handler address
# Initialize to co-processor unusable exception handler address
# Initialize to arithmetic over(cid:13)ow exception handler address

mfc0

$k1, $epc

##################################################################################
# User TLB Miss Handler
##################################################################################
# Store subsequent items in kernel text section
.ktext 0x80000000
# starting at address 0x80000000
UTLBMiss Handler:
# Copy context register contents (i.e., kseg2 virtual
mfc0
$k0, $context
# address of required user PTE) into GPR k0
# Copy epc contents (address of faulting instruction)
# into GPR k1
# Load user PTE from kseg2 addr to GPR k0
# This load can cause a TLBMISS exception!
# Copy the loaded PTE into EntryLo register
# Write the PTE in EntryLo register into TLB
# at slot number speci(cid:12)ed in Random register
# Jump to address of faulting instruction
# Switch to user mode

$k0, $EntryLo

mtc0
tlbwr

lw

$k0, 0($k0)

jr
rfe

$k1

##################################################################################
# General Handler
##################################################################################
# Store subsequent items in kernel text section
.ktext 0x80000080
# starting at address 0x80000080
General Handler:

##################################################################################
# Save interrupted process’ s0 - s7 registers, k0, k1, sp, and epc on kernel stack
...
...

##################################################################################
# Determine cause for coming here, and jump to appropriate handler
# Copy Cause register to R26
$k0, $cause
mfc0
# Take out the ExcCode value
$k0, $k0, 0x3c
andi
$k0, handler table($k0)# Get starting address of appropriate handler
lw
# Call appropriate handler
$k0
jalr

##################################################################################
# Returned from handler
# Restore interrupted process’ s0 - s7 registers, k0, k1, sp, and epc
from kernel stack
#
...
...

##################################################################################
# Return to interrupted program
# Copy epc register to R26
mfc0
$k0, $epc

4.6.

IO Schemes Employed by Device Management System

173

jr
rfe

$k0

# Return to interrupted program
# Restore from exception

4.6

IO Schemes Employed by Device Management System

This section gives an overview of the device management part of a typical OS. A complete
treatment of the internal operation of the device management system of even a single OS
is beyond the scope of this book.

As can be imagined, there is a wide disparity in the nature of the various abstract IO
devices. Di(cid:11)erent schemes are available to accommodate this disparaging di(cid:11)erences in
speed and behavior. They are:

(cid:15) sampling

(cid:15) program-control led IO

(cid:15) interrupt-driven IO

(cid:15) direct memory access (DMA)

(cid:15) IO co-processing

As we go from the (cid:12)rst scheme to the last, more functionality is shifted from the device
driver to the IO interface. The scheme used by a device driver to perform IO transfers is,
of course, transparent to the user program, because it is not speci(cid:12)ed in the API. We shall
discuss each of these schemes in detail.

Central to all these schemes is the need to do appropriate synchronization between the
device driver and the device. IO devices tend to be signi(cid:12)cantly slower than the processor.
Moreover, their response times tend to have a wide variance. Because of these reasons, no
assumptions can be made about the response times of IO devices. The di(cid:11)erent IO transfer
schemes also di(cid:11)er in how synchronization is done between the device driver and the device.

4.6.1 Sampling-Based IO

This is the simplest of the IO data transfer methods.
In this scheme, the IO device is
treated like main memory; that is, the device is always ready to accept or to provide data,
as appropriate. No checking of device status is required. The programming interface is
extremely simple, consisting of just one or more data registers. Example IO devices that
can be communicated in this manner are simple devices such as digital input port and motor
port.

174

Chapter 4. Assembly-Level Architecture | Kernel Mode

4.6.2 Program-Controlled IO

With program-controlled IO, the device driver being executed on the machine has direct
control of the IO operation, including sensing the device status, sending a read or write
command, and transferring the data. Sensing the device status may involve executing
an IO read instruction to read the device controller’s status register into a general purpose
register (GPR), and then checking the appropriate bits of the GPR. This process of checking
the status by continuous interrogation of the status register associated with the device is
performed until the IO device becomes ready8 . For example, the status register of the DEC
LP11 line printer interface contains a done bit, which is set by the printer controller when
it has printed a character, and an error bit, which indicates if the printer is jammed or out
of paper. The bytes to be printed are copied from a general-purpose register to the data
register of the printer interface (by means of IO write instructions), one at a time. Each
byte is copied only after ensuring that the done bit has been set. The error bit is also
checked every time to determine if a problem has occurred in the printer.

A (cid:13)owchart of the device driver that carries out program-controlled IO is shown in
Figure 4.9. The (cid:13)owchart assumes that a sequence of words has to be read from an IO
device and stored in main memory. The device driver continually examines the status of
the device interface until the appropriate (cid:13)ag becomes 1. A word is then brought into a
GPR, and transferred to main memory. The entire process is repeated until all of the data
have been transferred.

Example: To review the basic concepts of program-controlled IO, consider the IO operations
involved in reading a character from the keyboard and copying it to a memory location. For
an application program, the convenient way to read a character is by invoking the operating
system (by means of a syscall instruction). A MIPS-I assembly language application pro-
gram for doing this IO operation is given below. Notice that the execution of the syscall
instruction causes the system to switch to kernel mode, and execute many instructions in
the kernel mode before returning to the user program.

.text
li
la
li
li
syscall

# Place the (cid:12)le descriptor for keyboard (0) in $a0
$a0, 0
$a1, buffer # Place the starting address of buffer in $a1
# Place the number of bytes to be read (i.e., 1) in $a2
$a2, 1
$v0, read code # Place the code for read in $v0
# Call OS routine to perform the read

Now, assume that the keyboard’s interface sets the LSB of its 8-bit status register to 1
whenever a new character is typed by the user. The device driver resets it to 0 after reading
that character. The display’s interface resets the next-to-LSB of its 8-bit status register to

8Because IO devices are slow, it is important to check their status prior to issuing a new command. For
instance, when a new command is sent to a device, the device may not be ready to accept it because it is
still working on the previous command. This situation can be recognized by checking its status.

4.6.

IO Schemes Employed by Device Management System

175

Start

Read status register
into a GPR

Busy wait

Check relevant status bit
in GPR

No

Is device ready?

Yes

Read data register
into a GPR

Copy data from GPR
to Memory

Yes

More
transfer?

No

Stop

Figure 4.9: A Flowchart Depicting the Working of a Device Driver that Reads Data from
an IO Device using Program-Controlled IO Transfer

0 whenever the display is ready to accept a new character. The device driver sets it to 1
when it writes a character to the display interface’s data register. Figure 4.10 illustrates
the interface provided by the IO devices, along with the speci(cid:12)c addresses given to the IO
registers.

Let us write an assembly language device driver routine that will be called when the
above syscall instruction is executed. First of all, when the syscall instruction is exe-
cuted, the execution mode switches to Kernel mode, and control passes over to the system
call layer part of the operating system. This routine saves the User process’ registers
and other relevant state on the Kernel stack. It then calls the SysHandler routine, which
subsequently calls the keyboard device driver. Below, we present an example code for a
keyboard device driver that reads from the keyboard up to as many characters as speci-
(cid:12)ed in register $a2. If $a2 contains zero or a negative number, then no character is read.
Similarly, reading stops when the end of line is reached, which is detected by checking the
read character against 0n0 . The number of characters actually read is placed in register $v0.
The read characters are placed in consecutive memory locations, starting from the address

176

Chapter 4. Assembly-Level Architecture | Kernel Mode

Interconenct

Keyboard Status

Monitor Status

CPU

MM

0xA0000000
Status
Data
0xA0000001
Keyboard Interface

0xA0000002
Status
Data
0xA0000003
Graphics Interface

0xA0000002
Status
Data
0xA0000003
Disk Interface

Figure 4.10: A Flowchart for an Assembly Language Device Driver to Read Data from an
IO Device using Program-Controlled IO Transfer

originally speci(cid:12)ed in register $a1. Notice that this simple device driver does not check for
error conditions, one of the tasks a real device driver must do.

##########################################################################
# Keyboard Read Device Driver: Called by OS File System
# $a1 contains address of buffer; $a2 contains number of bytes to read
##########################################################################

.ktext
keybd read:
li
blez
read loop:
lb
andi
beqz
lb
sb
andi
sb
addu
beq
addu
subu
bnez
read done:
jr

# Store subsequent items in kernel text section

$v0, 0
$a2, read done

# Initialize number of bytes read ($v0) to 0
# Go to label read done if no byte is to be read

# Read keyboard status register into R9
$9, keybd status
# Isolate the status bit for keyboard input
$10, $9, 1
# Branch back to label read loop if status bit is not set
$10, read loop
# Read the byte from keyboard data register
$11, keybd data
# Store the byte in the bu(cid:11)er
$11, 0($a1)
# Reset keyboard input status bit (LSB) to 0
$12, $9, 0xFE
$12, keybd status # Update keyboard status register
# Increment number of bytes read
$v0, 1
$11, ’nn’, read done# Go to label read done if end of line
# Increment address of bu(cid:11)er to store next byte
$a1, 1
# Decrement number of bytes to be read
$a2, 1
# Go to read loop if there are more bytes to be read
$a2, read loop

$ra

# Return control to OS (cid:12)le system

4.6.

IO Schemes Employed by Device Management System

177

The read loop in the above device driver constantly checks the keyboard status until the
next character has been typed. This type of continuous checking of the device status bits
to see if it is ready for the next IO operation is a hallmark of program-controlled IO. The
device controller places the information in one of its status registers, and the device driver
gets this information by reading this register. The device driver is in complete control of
the IO transfer.

The disadvantage of program-controlled IO is that it can waste a lot of time executing
the driver because the IO devices are generally slower compared to the other parts of the
system. The driver code may read the device status register millions of times only to (cid:12)nd
that the IO device is still working on a previous command, or that no new character has
been typed on the keyboard since the last time it was polled. For output devices, it has to
do this status checking until the output operation is completed to ensure that the operation
was successful. Ideally, it is preferable to execute a device driver only when a device is ready
for a new transfer and a transfer is required. For instance, the keyboard driver needs to be
executed only when a character is available in the input bu(cid:11)er of the keyboard interface.
Similarly, the printer driver needs to be executed only when the printer has completed
a previous print command, or is ready to accept a new command and a print request is
pending.

4.6.3

Interrupt-Driven IO

The overhead in program-controlled IO transfer was recognized long ago, leading to the use
of interrupt-driven IO for at least some of the peripheral devices. An interrupt is generated
by an IO port to signal that a status change has occurred. With interrupt-driven IO,
device driver routines need not be continuously executed to check the status of IO ports;
instead other useful programs can be executed until the IO device becomes ready. Indeed,
by using interrupts, such waiting periods can almost be eliminated, provided the system
has a su(cid:14)cient degree of multiprogramming.

When an interrupt is generated, the system acknowledges it when it is ready to pro-
cess the interrupt. At that time, the system enters the Kernel execution mode, and the
appropriate interrupt service routine (ISR) or interrupt handler is called with one or more
parameters that uniquely identify the interrupting device. The parameters are important,
because a single ISR may handle multiple devices of the same type. If the identity of the in-
terrupting device is not supplied, the ISR may need to poll all potential IO ports to identify
the interrupting one.

Data transfer by means of interrupts is illustrated in Figure 4.11 as a time line. Time
is depicted along the horizontal direction. Process A executes a read system call at time
T1. Control is then transferred to the syscall interface routine of the OS, from where
control is transfered to the (cid:12)le system, and then to the appropriate device driver. After the
device driver updates the relevant status information, control is transferred to the process
control system, which then allows process B to run (at time T2). The device becomes

178

Chapter 4. Assembly-Level Architecture | Kernel Mode

ready at time T3, and generates an interrupt. Control is transfered to the OS, which then
calls the ISR for the device. After the ISR completes, the process control system allows
process A to continue. Notice that during the time taken by the device to become ready
(T2 to T3), the system is utilized by executing process B.

l
l
a
c
 
m
e
t
s
y
s
 
d
a
e
r

Process A

Process B

OS Kernel

Device Driver

t
p
u
r
r
e
t
n
i
 
e
c
i
v
e
d

(ISR)

T1

T2

T3

T4

Time

Figure 4.11: A Timeline Depicting an Interrupt-driven IO Transfer on behalf of Process A

Example: The keyboard interface raises an interrupt when a character is typed on the
keyboard. When the interrupt is accepted, control goes to the system call layer of the OS,
which saves the interrupted process’ state and registers. It then calls the interrupt handler
routine, which subsequently calls the ISR part of the keyboard device driver. Below, we
present the ISR part of the keyboard device driver to read in a character from the keyboard
interface to memory location buffer. Again, we assume that the status register address is
named keybd status and the data register address is named keybd data.

# Store subsequent items in kernel text section
.ktext
# Update stack pointer ($sp)
keybd isr: subu $sp, 12
# Save value of R8 in stack before overwriting it
sw
$8, 0($sp)
# Copy processor status register to R8
mfc0 $8, C12
$8, $8, 0xFFFFXXFF # Code to disable lower priority interrupts
and
# Copy R8 to processor status register
mtc0 C12, $8
# Save value of R9 in stack before overwriting it
$9, 4($sp)
sw
# Save value of R10 in stack
sw
$10, 8($sp)
# Read keyboard interface’s data register
$9, keybd data
lb
# Store the byte in the bu(cid:11)er
sb
$9, buffer

4.6.

IO Schemes Employed by Device Management System

179

$9, keybd status # Read keyboard interface’s status register
lb
# Reset keyboard’s status bit (LSB) to 0
$9, $9, 0xFE
and
$9, keybd status # Update keyboard interface’s status register
sb
# Restore value of R9 from stack
$9, 4($sp)
lw
# Restore value of R10 from stack
lw
$10, 8($sp)
# Copy processor status register to R8
mfc0 $8, C12
# Code to enable lower priority interrupts
or
$8, $8, 0xXX00
# Copy R8 to processor status register
mtc0 C12, $8
# Restore value of R8 from stack
lw
$8, 0($sp)
# Restore stack pointer ($sp)
addu $sp, 12
# Return control to calling routine
jr
$ra

A number of actions are involved in processing an interrupt. Figure 4.12 clearly depicts
these actions.

Hardware

Software

I/O interface module
issues interrupt

Finish execution of
current instruction

Disable lower priority
interrupts;
Save registers

Acknowledge
interrupt

Save
return address

Service I/O device

Restore registers;
Enable lower priority
interrupts

Execute a CALL
to appropriate ISR

RETURN

Figure 4.12: A Flowchart Depicting the Steps Involved in Processing an Interrupt

Handling Multiple Interrupts

We now consider a computer system in which several interfaces are capable of raising
interrupts. Because these interfaces operate independently, there may not be a speci(cid:12)c
order in which they generate interrupts. For example, the keyboard interface may generate
an interrupt while an interrupt caused by a printer interface is being serviced, or all device
interfaces may generate interrupts at exactly the same time, in a pathological situation!

180

Chapter 4. Assembly-Level Architecture | Kernel Mode

The presence of multiple interrupt-raising device interfaces raises several questions:

1. How can the system identify the IO port that generated an interrupt?

2. Because di(cid:11)erent devices probably require di(cid:11)erent ISRs, how to obtain the starting
address of the appropriate ISR?

3. Should an ISR for one device be allowed to be interrupted (by another IO port)?

4. What should be done if multiple device interfaces generate interrupts at the same
time?

The ways in which these problems are resolved vary considerably from one machine to
another. The approach taken in any machine is an important consideration in determining
the machine’s suitability for speci(cid:12)c real-time applications. We shall discuss some of the
common techniques here.

Device Identi(cid:12)cation: Consider the case in which a device interface requests an in-
terrupt by activating an Interrupt Request line that is common to all device interfaces.
When a request is received over the common Interrupt Request line, additional infor-
mation is needed to identify the particular device that needs attention. One option is to
set appropriate bits in the status register of the interrupting device interface, so that the
interrupting device can be identi(cid:12)ed by pol ling. A better option is to use a scheme called
vectored interrupts, in which the interrupting device’s identi(cid:12)cation is sent as a special code
along with the interrupt.

Prioritizing Interrupts: Now let us consider the situation in which several interrupts
occur simultaneously. In this case, the system must decide which device to service (cid:12)rst.
A priority interrupt system establishes a priority over the various interrupt sources to de-
termine which interrupt request to service (cid:12)rst when two or more arrive simultaneously.
The system may also determine which interrupts are permitted to interrupt an ISR. Higher
levels of priority are assigned to requests that, if delayed or interrupted, could have serious
consequences. Devices with high-speed transfers such as magnetic disks are given high pri-
ority, and slow devices such as keyboards receive the lowest priority. When two devices raise
interrupt at the same time, the computer services the device with the higher priority (cid:12)rst.
Establishing the priority of simultaneous interrupts can be done by software or hardware.
Software uses a polling procedure to identify the interrupt source of higher priority. In this
method, there is one common routine to which the control is transferred upon receiving
an interrupt. This routine performs the polling of the interrupt sources in the order of
their priority. In the hardware priority scheme, the priority is implemented by some kind
of hardware.

4.6.

IO Schemes Employed by Device Management System

181

4.6.4 Direct Memory Access (DMA)

Both program-controlled IO and interrupt-driven IO are particularly suited for interacting
with low-bandwidth IO devices. Both schemes place the burden of moving data and manag-
ing the transfer on a device driver or OS ISR, which does so with the help of IO read/write
instructions such as

R1, data register
lw
Of course, an instruction to transfer input or output data is executed only after executing
other instructions that con(cid:12)rm that the IO interface is indeed ready for the transfer. In
either case, considerable overhead is incurred, because several instructions must be exe-
cuted to transfer each data word. The reason is that instructions are needed for performing
tasks such as incrementing the memory address and keeping track of the word count. This
overhead becomes intolerable for performing high-bandwidth transfers such as disk IO. For
such high-bandwidth devices, most of the transfers involve large blocks of data (hundreds to
thousands of bytes). So computer architects invented a mechanism for o(cid:11)-loading the device
driver, and letting the IO interface directly transfer data to or from main memory with-
out involving the device driver. This kind of transfer is called direct memory access (DMA).

Steps Involved in Setting Up a DMA Transfer:

Although the DMA mode of data transfer happens without interrupting the program
being executed, its operation is still initiated by a device driver routine. To initiate the
transfer of a block of words, instructions are executed by the device driver routine to convey
the following information to the DMA interface (IO interface capable of performing DMA
transfers):

(cid:15) the address or identity of the IO device

(cid:15) the starting address in the main memory

(cid:15) the number of words to be transferred

(cid:15) the direction of transfer (read/write)

(cid:15) the command to initiate the DMA operation

Once this information is conveyed to the DMA interface, the device driver stops execution,
and another program is executed. When the entire block has been transferred, the DMA
interface raises an interrupt. An interrupt service routine is then executed to interrogate
the DMA interface to verify that the entire operation indeed completed successfully. Figure
4.13 depicts the actions involved in a DMA transfer as a (cid:13)owchart.

182

Chapter 4. Assembly-Level Architecture | Kernel Mode

Send information to
DMA Interface

Device
Driver

Is IO device ready?

No

Yes

Transfer next word
to/from memory

Increment memory address

No

Are all words
transferred?

Yes

Raise interrupt to indicate
end of transfer

DMA
Interface

Execute ISR to check
if transfer was successful

Device
Driver

Figure 4.13: A Flowchart Depicting the Steps Involved in a DMA Transfer

4.6.5

IO Co-processing

The DMA scheme o(cid:11)ers signi(cid:12)cant performance improvements over interrupt-driven IO
whenever large blocks of data are to be transferred. Is it possible to do even better? To
answer this question, we must (cid:12)rst look at the limitations of the DMA scheme. First, each
DMA operation has to be initiated by a device driver, by executing several instructions.
Second, a DMA interface can handle only a single DMA operation at a time. To overcome
these limitations, we can use a scheme called IO co-processing, in which (complex) IO func-
tions are implemented by IO routines, which are executed (by IO processors or IO channels
or peripheral processing units (PPUs)9 ) in parallel with normal execution of programs (by
the main processor). The IO routines are also typically placed in the memory address space.
The execution of an IO routine is triggered by the device driver as in the case of a DMA

9An IO channel is itself a computer, albeit simple, which is capable of executing a small set of general-
purpose instructions along with many special-purpose IO instructions such as \read a track from the disk"
or \skip forward two blocks on the tape." More sophisticated IO processors such as a peripheral processing
unit (PPU) can do even more sophisticated IO functions. Similarly, a modern graphics processing unit
(GPU) does complex graphics functions such as rendering, and hidden surface calculation.

4.7. Concluding Remarks

183

operation.

4.6.5.1 Graphics Co-Processing

4.6.6 Wrap Up

As we can see, there is a trade-o(cid:11) between IO interface complexity and performance. When
we use a simple, inexpensive device interface, most of the functionality is implemented
in software (its device driver), and hence its performance will be low. A more expensive
interface, on the other hand, performs more functions in hardware, and is therefore faster.

4.7 Concluding Remarks

\Anybody who real ly knows computers spends a certain amount of time helping out
friends who have loaded up their computers with trash, viruses, and spyware."
| Howard Gilbert in Introduction to PC Hardware

4.8 Exercises

1. Consider a situation in which an OS routine calls a user mode library routine. What
mode | user mode or kernel mode | will the library routine execute?

2. Explain the role played by the system cal l layer in a MIPS-I operating system.

3. Explain why interrupts are disabled when handling an exception.

4. Consider a simple embedded computer with two IO devices|a transmitter and a
receiver. Describe the protocol Write a MIPS-I assembly language program (kernel
mode) to accept data from the receiver and send them to the transmitter.

5. Consider a computer system that supports only a single mode of operation, i.e., it does
not provide a separate user mode and a kernel mode. What would be the shortcomings
of such a computer system?

6. For the computer system mentioned in the previous problem, explain how the hard-
ware can provide some minimal functions traditionally carried out by the operating
system, such as resource allocation among the processes.

7. What does a computer system do when two IO devices simultaneously raise interrupts?
How does the computer recognize the identity of the devices?

184

Chapter 4. Assembly-Level Architecture | Kernel Mode

8. Explain the di(cid:11)erences between hardware interrupts, software interrupts, and excep-
tions. What do they have in common? Give an example of each.

9. A device driver programmer uses the following (cid:13)ow-chart for performing program-
controlled reads from a keyboard. Explain what could go wrong with this scheme.
Give the correct (cid:13)ow-chart.

Start

Read status register

Read data register

Transfer data to memory

Yes

More
transfer?

No

Stop

Chapter 5

Instruction Set Architecture (ISA)

Whoever loves instruction loves know ledge.

Proverbs 12: 1

The central theme of this book is to view a modern computer as a series of architectural
abstractions, each one implementing the one above it. We have already seen the high-level
architecture and the two distinct modes of the assembly-level architecture. This chapter
discusses the architectural abstraction immediately below the assembly-level architecture,
called the instruction set architecture (ISA). In principle, this architecture is de(cid:12)ned by how
the computer appears to a machine language programmer. We can also view it as a set of
rules that describe the logical function of a computer as observable by a (machine language)
program running on that machine. Historically, this architecture came into existence before
any of the other architectures we saw so far. The ISA has a special signi(cid:12)cance that makes it
important for system architects: it is the interface between the software and the hardware.
This architecture serves as the boundary between the hardware and the software in modern
computer systems, and thus provides a functional speci(cid:12)cation for the hardware part of a
computer system.

In the early days of computers, all of the programming used to be done in machine lan-
guage! This was found to be so tedious that computer scientists quickly invented assembly
languages, which used symbols and notations that were closer to the way people think and
communicate. Once assembly languages were introduced as a symbolic representation for
machine languages, programmers virtually stopped using machine languages to program.
And, in theory, we can build hardware units that directly interpret assembly language pro-
grams. It is reasonable to ask why we still maintain a separate virtual machine level at the
ISA level. The reason is that hardware that directly interprets assembly language programs
will be signi(cid:12)cantly more complex than one that directly interprets machine language pro-
grams. The main di(cid:14)culty arises from the use of labels and macros in assembly languages.

185

186

Chapter 5.

Instruction Set Architecture (ISA)

By de(cid:12)ning a lower-level abstraction at the ISA level, software in the form of assemblers
can be used to translate assembly language programs to machine language programs, which
can be more easily interpreted by hardware circuits.

The ISA does not specify the details of exactly how the hardware performs its functions;
it only speci(cid:12)es the hardware’s functionality. Thus, an ISA speci(cid:12)es general-purpose regis-
ters, special registers, a memory address space, and an instruction set 1 . The ISA provides a
level of abstraction that allows the same (machine language) program to be run on a family
of computers having di(cid:11)erent implementations (i.e., microarchitectures). An example is the
x86 ISA which is implemented in the 80386, 80486, Pentium, Pentium II, Pentium III, and
Pentium 4 processors by Intel Corporation, and in the K6 and Athlon processors by AMD.

5.1 Overview of Instruction Set Architecture

We shall begin this chapter with an overview of the basic traits of an instruction set archi-
tecture. The assembly-level architecture that we covered in the last two chapters is nothing
but a symbolic representation of the ISA. Many aspects of the ISA are therefore very similar
to that of the assembly-level architecture. In fact, both have very similar memory models,
register models, and IO models. The main di(cid:11)erences between the two abstraction levels
are in the two areas of language and instruction set. Let us take a detailed look into both
of these issues.

5.1.1 Machine Language

The language used to specify programs that are targeted to an ISA has been historically
called the machine language. The most notable feature of machine languages is their alpha-
bet, which is restricted to just two symbols (0 and 1). Apart from allowing only an extremely
frugal set of symbols for the language (as opposed to the richer set of alphanumeric charac-
ters available for an assembly language), the ISA also bans many of the luxuries permitted
by the assembly-level architecture, such as labels, macros, AL-speci(cid:12)c instructions, and
AL-speci(cid:12)c addressing modes.

5.1.1.1 Language Alphabet

\‘There are 10 types of people in this world |
those who understand binary, and those who don’t."
| J. Kincaid and P. Lewis

1 Issues related to implementation of the ISA (such as cache memory, ALUs, and the type of internal
connections) do not form part of the ISA. The entire point of de(cid:12)ning an ISA is to insulate the machine-
level programmer (and the assembler) from those details.

5.1. Overview of Instruction Set Architecture

187

A machine language alphabet has only two symbols: f0, 1g. This means that all of
the information in a machine language program|instructions as well as data|have to be
expressed using sequences of just 0s and 1s! The earliest programmers, who programmed
computers at the ISA level, did precisely this. Thus, opcodes (such as add and and) and
operands (such as $t0 and $sp) are all speci(cid:12)ed in bit patterns rather than in a natural
language-oriented symbolic form. A machine language program is therefore far less read-
able than an equivalent assembly language program. In addition, assembly languages permit
programmers to use labels to identify speci(cid:12)c memory addresses that hold data or form the
target of branch instructions. Assembly languages also provide a variety of other conve-
nience features that make programs at this level shorter and easier to write. For example,
data layout directives allow a programmer to describe data in a more concise and natural
manner than its binary representation. Similarly, macros and pseudoinstructions enable a
sequence of instructions to be represented concisely by a single macro or pseudoinstruction.

Under these circumstances, why would anyone other than ISA designers want to study
machine language? Certainly, not for writing application programs! The compelling reason
is that besides application programs, there are a lot of support programs such as compilers,
assemblers, disassemblers, and debuggers. The writers of these programs need to know the
ISA details. Microarchitecture designers also need to know the machine language to design
the hardware for executing machine language programs.

Information Representation with Bit Patterns: A machine language needs to rep-
resent two kinds of information: instructions and data. An assembly language had a large
set of symbols|alphanumeric characters and punctuation marks|to represent information.
By contrast, the alphabet used by a machine language for representing information has only
2 symbols|0 and 1. Thus, any kind of information can be represented at this abstraction
level only by bit patterns obtained by concatenating bits. The same sequence of bits can
have di(cid:11)erent meanings; the correct meaning depends on:

(cid:15) the instruction being executed and

(cid:15) the number systems and coding schemes adopted by the ISA designers

For example, consider the bit pattern 0100 1110 0111 0010 0000 0000 0000 0000 (i.e.,
4E320000H). If this is interpreted as an integer, the value is 2:106 (cid:2) 10 10 . If it is interpreted
as an ANSI/IEEE 754 (cid:13)oating-point number, the value is 1:015 (cid:2) 10 9 . If it is interpreted
as part of an ASCII character string, then it indicates the 4 characters N, r, Null, Null. If
it is interpreted as a Motorola 68000 instruction, then it indicates the STOP instruction if
the system is in the Kernel mode, and a syscall instruction, otherwise. What this shows
is that a bit pattern does not have an inherent meaning; we must know the context as
well as the rules concerning the interpretation of those bits. The rules for interpretating a
bit pattern are established at ISA design time, and will be e(cid:11)ective throughout the life of
the system. The assumptions about bit meaning will impact the design of the lower-level

188

Chapter 5.

Instruction Set Architecture (ISA)

hardware that manipulates the bits. The choice of a number system is based on available
hardware technology, desired range of values, and other system goals.

The ISA speci(cid:12)es exactly how di(cid:11)erent instruction types and data types are encoded as
bit patterns2 .

\A word aptly spoken is like apples of gold in settings of silver"
| Proverbs 25: 11

5.1.1.2 Syntax

A machine language program starts with a header, which provides a description of the
various sections that are present in the program and are mapped to various portions of
the memory address space. This description includes the name of the section, its size, its
location within the program (cid:12)le, the portion of memory address space it maps to, etc.
Table 5.1 lists the commonly found sections in machine language programs. Some of these
sections|.rdata, .text, .sdata, and .data|are de(cid:12)ned in the assembly language also,
as we already saw in the previous two chapters. The rest|.bss, .sbss, .lit8, .lit4, and
.comments|are limited to machine languages, and are created by the assemblers.

Section Name Explanation
Read-only data
.rdata
Code
.text
Uninitialized writable data
.bss
Uninitialized writable small data
.sbss
64-bit (cid:13)oating-point constants
.lit8
32-bit (cid:13)oating-point constants
.lit4
Writable small data
.sdata
Writable data
.data
Comments
.comment

Table 5.1: Typical Sections in a Machine Language Program

Unlike the case with assembly languages, each of the sections that are present appears
exactly once. Within each section, the bit patterns corresponding to contiguous memory
locations are stored contiguously. This is pictorially depicted in Figure 5.1. The order of
the sections within the machine language program is not critical.

2Some of this encoding information|in particular, the encoding of data types|is known to the assembly
language programmer too.

5.1. Overview of Instruction Set Architecture

189

Header .rdata

.text

.data

.lit8 .lit4 .sdata .sbss .comment

Figure 5.1: Organization of Header and Various Sections in a Machine Language Program

5.1.1.3 ELF Format

5.1.1.4 Portable Executable File Format

5.1.2 Register, Memory, and IO Models

To produce machine language code, machine language programmers and the developers of
assemblers and disassemblers need to know the speci(cid:12)cations of the instruction set archi-
tecture. These attributes include the memory model, the register model, the IO model, the
data types, and the instruction types. The collection of all this information is what de(cid:12)nes
the instruction set architecture.

The register model supported by an ISA is the same as that supported by the assembly-
level architecture for that computer. On a similar note, the ISA’s memory model is also
virtually the same as that supported by the assembly-level architecture. The only di(cid:11)erence
is that the ISA does not permit labels to be used to specify memory locations.

5.1.3 Data Types and Formats

A picture is worth 1,000 words, especially in memory requirements. And a video is worth
a million words!

5.1.4

Instruction Types and Formats

Perhaps the biggest di(cid:11)erence in the two architectures is in the area of the instruction set.
The instruction set supported by the ISA is generally a subset of that supported by the
assembly-level architecture. While the assembly-level instruction set is designed to facilitate
assembly language programming, the machine-level instruction set is designed to facilitate
lower-level implementations. Thus, there is a noticeable semantic gap between an assembly-
level architecture and its corresponding ISA. This is especially the case for RISC (reduced
instruction set computer) ISAs, as we will see later in this chapter.

In the case of the MIPS-I, the assembly-level instruction set has several opcodes that
permit 32-bit immediate operands. Examples are li and lw.
In order to encode these
instructions using bit patterns, we need more than 32 bits. However, the MIPS-I ISA
designers wanted (cid:12)xed length instruction formats in which all instructions are encoded
with 32 bits. Therefore, li is not included in the MIPS-I ISA and lw is allowed to have

190

Chapter 5.

Instruction Set Architecture (ISA)

only a 16-bit immediate operand. This begs the question of how we get a 32-bit data value
into a register, at the ISA level. The standard method is to split the 32-bit data value into
2 parts|a 16-bit upper half and a 16-bit lower half|and use a lui-ori sequence to load
the two halves.

Like the load instructions, the branch instructions are also limited to specifying a 16-bit
immediate value at the ISA level. As 16 bits are not su(cid:14)cient by themselves to specify
the branch target address, the solution adopted is to consider the 16-bit immediate value
as the instruction o(cid:11)set from the branch instruction. Thus, when executing the ISA-level
instruction represented symbolically as
beq $1, $2, 8
8 instructions will be skipped if the contents of registers $1 and $2 are equal.

5.2 Example Instruction Set Architecture: MIPS-I

5.2.1 Register, Memory, and IO Models

5.2.2 Data Types and Formats

5.2.3

Instruction Types and Formats

The designers of the MIPS-I ISA decided to use a (cid:12)xed length instruction format, so as
to make it easier for the lower level machine (microarchitecture) to quickly determine in-
struction boundaries to facilitate the overlapped execution of multiple instructions. Every
instruction is encoded in exactly 32 bits. In the MIPS-I ISA, the opcode implicitly speci(cid:12)es
the addressing modes for each of the operands, and therefore no addressing mode infor-
mation is explicitly recorded in the instruction bits. Considering all of the instructions
in the MIPS-I ISA, we have the following (cid:12)elds: opcode, rs, rt, rd, offset/immediate,
and target. Based on the (cid:12)elds used, we can classify the MIPS-I ISA instructions into 3
categories:

(cid:15) Instructions that specify an immediate/offset: Examples are: (i) addi rt, rs,
immediate, (ii) lw rt, offset(rs), and (iii) beq rs, rt, offset. These instruc-
tions are called I format instructions.

(cid:15) Instructions that specify 3 registers: An example is add rd, rs, rt. These instruc-
tions are called R format instructions.

(cid:15) Instructions that specify a target. An example is jal target. These instructions
are called J format instructions.

Among the di(cid:11)erent (cid:12)elds, the rs, rt, and rd (cid:12)elds require 5 bits each, in order to specify
one of 32 general-purpose registers. Based on an analysis of existing programs, the MIPS-I

5.3. Translating Assembly Language Programs to Machine Language Programs

191

ISA designers decided to use 16 bits to specify the offset/immediate (cid:12)eld. For the (cid:12)rst
category of instructions, this leaves 6 bits for the opcode. With 6-bit opcodes, the maximum
number of opcodes that can be supported by the ISA is 64. Incidentally, the MIPS-I ISA has
more than 64 opcodes. To accommodate more than 64 opcodes with a (cid:12)xed opcode (cid:12)eld,
we need to either reduce the number of bits for the offset/immediate (cid:12)eld, or use longer
instruction formats, both of which have undesired consequences. The solution adopted by
the MIPS-I ISA designers was to use variable length opcodes or expanding opcodes within a
(cid:12)xed-length instruction format. Instructions that cannot a(cid:11)ord to support a longer opcode
(cid:12)eld|the ones in the (cid:12)rst and third categories|are given a 6-bit opcode (cid:12)eld. Instructions
in the second category have to specify only 3 register addresses, requiring a total of 15 bits,
besides the opcode. Therefore, these instructions are given a longer opcode (cid:12)eld. For these
instructions, the standard opcode (cid:12)eld is set to the bit pattern 000000, and a separate 6-bit
func (cid:12)eld is used to specify the desired operation. The three main formats of the MIPS-I
ISA are given in Figure 5.2.

I format

opcode

6

R format

000000

rs

5

rs

5

rt

5

rt

5

J format

opcode

6

offset

16

addi
lw
beq

rt, rs, immediate
rt, offset(rs)
rs, rt, offset

rd

5

target

26

opcode

6

add

rd, rs, rt

j
jal

target
target

Figure 5.2: Instruction Formats Used in the MIPS-I ISA

5.2.4 An Example MIPS-I ML Program

5.3 Translating Assembly Language Programs to Machine
Language Programs

Programs written in an assembly language are translated into machine language programs
by a special program called an assembler. An assembly language program is usually
entered into the computer through a terminal and stored either in the main memory or
on a magnetic disk. At this point, the program is simply a set of lines of alphanumeric
characters, which are stored as patterns of 0s and 1s, most likely as per the ASCII chart.
When the assembler program is executed, it reads this assembly language program, and
generates the desired machine language program (object code (cid:12)le), which also consists of
patterns of 0s and 1s (but in a tightly encoded form). The ob ject code is a combination

192

Chapter 5.

Instruction Set Architecture (ISA)

of machine language instructions, data, and information related to placing the instructions
properly in memory. It cannot be executed directly. Prior to execution, it undergoes the
steps of linking and relocation, which transform it into a binary executable (cid:12)le.

Because the assembly language program is just a symbolic representation of a machine
language program, on (cid:12)rst appearance it might seem that an assembler is a simple software
program that reads one statement at a time, and translates it to machine language. In fact,
for the most part, there is a one-to-one correspondence between instructions in an assem-
bly language program and the corresponding machine language program. The assembler’s
complexity arises from the additional features that are supported in the assembly-level ar-
chitecture. For instance, most assembly languages support AL-speci(cid:12)c instructions and
macros. In addition, all assembly languages support the use of labels.

5.3.1 MIPS-I Assembler Conventions

Like the MIPS-I compiler, the MIPS-I assembler also follows some conventions. These
conventions deal with the addresses allocated for di(cid:11)erent sections in memory, for instance.
Again, these conventions are not part of the machine language speci(cid:12)cations. Some of the
important conventions are given below:

(cid:15) The .text section starts at memory address 0x400000, and grows in the direction of
increasing memory addresses.

(cid:15) The .data section starts at memory address 0x10000000; it also grows in the direction
of increasing memory addresses.

(cid:15) The stack section starts at memory address 0x7fffffff, but grows in the direction
of decreasing memory addresses.

5.3.2 Translating Decimal Numbers

5.3.3 Translating AL-speci(cid:12)c Instructions and Macros

Many assembly languages provide AL-speci(cid:12)c instructions, which are not present in the
corresponding ISA level. The availability of these additional instructions makes it eas-

5.3. Translating Assembly Language Programs to Machine Language Programs

193

ier to program for the assembly-level architecture. The assembler translates AL-speci(cid:12)c
instructions by using short sequences of instructions that are present in the ISA. For in-
stance, the MIPS-I assembly-level architecture provides the opcode li, which is not present
in the MIPS-I ISA. A MIPS-I sequence for translating the AL-speci(cid:12)c instruction li R1,
0x11001010 is given below.

lui
ori

# Enter upper 16 bits of address in R1; clear lower 16 bits
R1, 0x1100
R1, R1, 0x1010 # Bitwise OR the lower 16 bits of address to R1

A few other examples are given in Table 5.2. In the table, the AL-speci(cid:12)c instruction
li is synthesized with an addiu instruction or lui-ori instruction sequence, depending on
whether the immediate value lies between (cid:6)32K or not. The AL-speci(cid:12)c instruction la is
also synthesized with a lui-ori pair. The AL-speci(cid:12)c instruction lw has a slightly di(cid:11)erent
format than the ISA-level lw instruction in that it uses memory direct addressing whereas
the latter uses register-relative addressing. This conversion is also done by using the lui
instruction to load the upper 16 bits of the memory address to a register, and then using
the ML lw instruction with the lower 16 bits as the displacement. Notice that when the
displacement value is negative, a 1 needs to be added to the upper 16 bits.

li

R2, 0x55ffff

MIPS-I AL Pseudoinstruction Equivalent MIPS-I AL Sequence
R2, R0, -10
addiu
R2, -10
li
R2, R0, 0x8000
ori
li
R2, 0x8000
R2, 0x55
lui
ori
R2, R2, 0xffff
R2, 0x1000
lui
R2, R2, 0xffff
ori
R4, 0x1000
lui
lw
R4, 0x1010(R4)
R5, 0x1001
lui
lw
R5, 0xffff(R5)

R4, 0x10001010

R2, 0x1000ffff

lw

R5, 0x1000ffff

la

lw

Table 5.2: Some MIPS-I AL-speci(cid:12)c Instructions and Their Equivalent Sequence

Translation of the macros can be done using text substitution in a similar manner.
For example, the assembler can invoke the C preprocessor cpp to do the required text
substitution prior to doing the assembly process.

5.3.4 Translating Labels

Another feature that assembly languages have incorporated for improving programmabil-
ity is the use of labels. A label is a symbolic representation for a memory location that

194

Chapter 5.

Instruction Set Architecture (ISA)

corresponds to an instruction or data. For example, in the following code, the label var1
corresponds to a memory address allocated for data, and the label L1 corresponds to a
memory address allocated for an instruction.

.data
var1: .word
.text
beq
ori
lw

L1:

6

# Allocate a 4-byte item at memory location named var1

R1, R2, L1
R1, R1, 0x1010
R2, var1

# Branch to label L1 if the contents of R1 and R2 are same

# Load from memory location named var1 into R2

Translating a label involves substituting all occurrences of the label with the correspond-
ing memory address. Although this is a straightforward substitution process, the situation
is complicated by the fact that most assembly languages permit a label to be used prior to
declaring it. This is illustrated in the above example code, where label L1 is used by the
beq instruction before its declaration two instructions later. Thus, the assembler cannot
perform the substitution for var1 when it (cid:12)rst encounters the beq instruction. In order to
translate such labels, the assembler needs to go through a program in two passes. The (cid:12)rst
pass scans the assembly language program for symbol declarations, and builds a symbol
table, which contains the addresses associated with each symbol. The second pass scans
the program, and substitutes each occurrence of a symbol with the corresponding address.
This pass also translates each instruction to machine language, and generates the machine
language program, possibly along with the assembly listing.

5.3.4.1 First Pass: Creating the Symbol Table

The purpose of the symbol table is to store the mapping between the symbolic names
(labels) and their memory addresses. As noted earlier, the ob jective of the (cid:12)rst pass is
to scan the assembly language program, and expand the AL-speci(cid:12)c instructions as well as
build the symbol table. For each symbol de(cid:12)ned in a module, a memory address is assigned,
and an entry is created in the symbol table. The symbol table is a data structure, on which
two operations can be performed:
insertion and search. The (cid:12)rst pass of the assembler
performs only insertions. An e(cid:14)cient organization of the symbol table is important for fast
assembly, as the symbol table may be referenced a large number of times during the second
pass.

5.3.4.2 Second Pass: Substituting Symbols with Addresses

In the second pass through the program, the assembler substitutes symbols with the re-
spective addresses stored in the symbol table created in the (cid:12)rst pass. For each symbol
encountered, it looks up in the symbol table to obtain the address. Unde(cid:12)ned symbols, if
any, are marked as external references. A list of unresolved references is made available at
the end of this pass.

5.3. Translating Assembly Language Programs to Machine Language Programs

195

5.3.5 Code Generation

Code generation involves translating each instruction to its corresponding bit pattern as
per the ISA de(cid:12)nition. THis step can be done in the second pass, and mostly involves table
lookups.

5.3.5.1 Example Ob ject File Format

To clarify the assembly process, we shall use a detailed example. For convenience, we shall
use the assembly language code that we saw earlier in pages 112 and 123. This code is
reproduced below. For the global variables, we shall use the same memory allocation given
in Figure 3.5. In the code given below, the left hand side shows the assembly language code.
The right hand side shows the memory map of the translated machine language code, along
with deassembled code (in assembly language format). This example shows how AL-speci(cid:12)c
instructions (such as la and bge) are translated using 2-instruction sequences. The example
also shows how various symbols (variable names as well as labels) are translated. After the
translation, references to variables are speci(cid:12)ed using the appropriate memory addresses.
Notice that in the MIPS-I ISA, a branch instruction speci(cid:12)es a branch o(cid:11)set instead of an
absolute target address.

# Assign memory locations for the global variables
# Initialize memory locations if necessary
.data
.word
a:
b:
.word
record: .word

0
12
0:3

f:
cptr:

.word
.word

loop:

.text
.align 2
la
R1, record

li
li
lw
bge

lw
add
addi
addi
b

R2, 0
R3, 0
R4, 8(R1)
R3, R4, done

R5, 0(R1)
R2, R2, R5
R1, R1, 4
R3, R3, 1
loop

0x10001000: 0x0
0x10001004: 0xc
0x10001008: 0x0
0x1000100c: 0x0
0x10001010: 0x0
0x10001014:
0x10001018:

0x400000:
0x400004:
0x400008:
0x40000c:
0x400010:
0x400014:
0x400018:
0x40001c:
0x400020:
0x400024:
0x400028:
0x40002c:

# lui R1, 0x1000
# ori R1, R1, 0x1008
# add R2, R0, R0
# add R3, R0, R0
# lw
R4, 8(R1)
# slt R6, R3, R4
# beq R6, R0, 6
# lw
R5, 0(R1)
# add R2, R2, R5
# addi R1, R1, 4
# addi R3, R3, 1
# beq R0, R0, (cid:0)6

196

Chapter 5.

Instruction Set Architecture (ISA)

done:

sw

R3, a

lw
sw

R4, cptr
R2, 0(R4)

0x400030:
0x400034:
0x400038:
0x40003c:

# lui R6, 0x1000
# sw
R3, 0x1000(R6)
# lw
R4, 0x1018(R6)
# sw
R2, 0(R4)

One important aspect to note is that the target of the assembled code is the machine
language seen by the linker.

5.3.6 Overview of an Assembler

Summary

AL Program

ML Program

Expanded (by a pre-processor)
Macro
Do not appear explicitly
Directive
Comment
Ignored, or placed in the .comments section
Label declaration Assigned memory address(es) in the appropriate section
Label reference
Substituted by the address assigned to the label
Synthesized by an instruction or a sequence of instructions
Instruction

Table 5.3: Translation of AL features to ML

5.3.7 Cross Assemblers

We just saw how an assembler translates assembly language programs to machine language.
As the assembler is implemented in software, it has to be executed on a host computer
for it to perform the translation. Normally when we execute an assembler on a particular
host computer, the assembler translates the program to the machine language of the host
computer itself. We can say that the assembler’s target ML is the same as that of the host
ML. Sometimes we may want to translate a program to an ML that is di(cid:11)erent from the
host ML. A cross assembler is used to do such a translation. There are several reasons why
such a cross assembly is required: (i) The target computer may not have enough memory
to run the assembler. This is often the case in embedded systems. (ii) When a new ML is
being developed, no host computer exists for that ML. To analyze di(cid:11)erent trade-o(cid:11)s, the
designer often builds a software simulator for the target computer. In order to \execute"
programs on the simulator, programs need to be translated to the new ML on a di(cid:11)erent
host computer.

5.4. Linking

5.4 Linking

197

Many computer programs are very large, and may even have millions of lines of instructions
and data. In order to make it easier to develop such large programs, a single program is
often developed as a collection of modules (i.e., a logically related collection of subroutines).
Di(cid:11)erent modules may even be developed by di(cid:11)erent programmers, possibly at di(cid:11)erent
times. For instance, the library routines are written as a common set of routines before
the development of application programs, and rarely undergo changes after development.
Several programs may even share modules other than the libraries.

When an assembly language program is developed as a collection of several modules,
how should it be assembled? If all of the source modules are combined into a single AL
program, and then the assembly is performed, that entails re-assembling the entire AL
program every time a change is made to one of the modules. This might require a long
time to perform the assembly, besides wasting computing resources. An alternative is to
assemble each module independently of other modules, and store the translated output as
an ob ject module (cid:12)le (on the disk). A change in a single source module would then require
re-assembling only that module. On Microsoft systems such as DOS, Windows 95/98, and
Windows NT, an ob ject module is named with a .obj extension, and an executable ML
program is given the extension .exe. On UNIX systems, ob ject (cid:12)les have extension .o,
whereas executable ML programs have no extension.

An ob ject module cannot be directly executed as a ML program, because it is likely
to contain unresolved references. An ob ject module contains an unresolved reference to a
label, if its source module does not contain a de(cid:12)nition for that label. For instance, the
module may have a call instruction to label foo, and foo may be de(cid:12)ned in another module.
In order to generate an executable ML program, we need to combine all of the component
ob ject modules into a single program. This process of combining | called linking | is is
customarily done using a program called link editor or linker. The linking process has to
be repeated whenever one or more of the component modules is re-assembled.

The linker performs four important tasks:

(cid:15) Resolve references among multiple ob ject modules, i.e., patch the external references.

(cid:15) Search the program libraries to (cid:12)nd library routines used by the program.

(cid:15) Determine the memory locations that will be occupied by the instructions and data
belonging to each module, and relocate its instructions by adjusting absolute refer-
ences.

(cid:15) Append a start-up routine at the beginning of the program.

198

Chapter 5.

Instruction Set Architecture (ISA)

5.4.1 Resolving External References

A linker’s (cid:12)rst task is to resolve external references in the ob ject modules. The linker
does this by matching the unresolved references of each ob ject module and the external
symbols of every other ob ject module. Information about external symbols and unresolved
references is generated in the second pass of the assembly process, and stored as part of the
ob ject module. An external symbol in one ob ject module resolves a reference from another
ob ject module if both have the same name. An unmatched reference means that a symbol
was used, but not de(cid:12)ned in any of the ob ject modules. Unresolved references at this stage
in the linking process do not necessarily mean that there is a bug in the program. The
program could have referenced a library routine (such as printf) whose code was not in
the ob ject modules passed to the linker.

After attempting to resolve external references in the ob ject modules, the linker searches
the system library archives such as /lib/libc.a to (cid:12)nd prede(cid:12)ned subroutines referenced
by the ob ject modules. When the program uses a library routine, the linker extracts the
routine’s code from the library and incorporates it into the program text section. This new
routine, in turn, may call other library routines, so the linker continues to fetch other library
routines until no external references remain to be resolved, or a routine cannot be found.
A program that references an unresolved symbol that is not in any library is erroneous and
cannot be linked.

5.4.2 Relocating the Memory Addresses

If all external references are resolved, then the linker determines the memory locations that
each module will occupy. Because the modules were assembled in isolation, the assembler did
not know where a module’s instructions and data will be placed relative to other modules.
When the linker places a module in memory, all absolute references must be relocated to
re(cid:13)ect its true location. Because the linker is provided with relocation information that
identi(cid:12)es all relocatable references, it can e(cid:14)ciently (cid:12)nd and backpatch these references.

5.4.3 Program Start-Up Routine

Finally, the linker appends a start-up routine at the beginning of the program. The start-
up routine performs several book-keeping functions. One of the book-keeping functions
performed by MIPS-I start-up routines involves copying from the stack to the argument
registers ($a0-$a3) the (cid:12)rst four parameters (to be passed to the function main). After
performing the required initializing jobs, the start-up code calls the main routine of the
program. When the main routine completes execution, control returns to the start-up
routine, which then terminates the program with an exit system call.

.text

5.5.

Instruction Formats: Design Choices

199

.globl
start:
lw
addiu
addiu
mul
addu
jal
ori
syscall

start

# Program execution begins here
# Copy argc from stack to R4
$a0, 0($sp)
# Place stack address corresponding to argv in R5
$a1, $sp, 4
# Start calculating stack address of envp
$a2, $a1, 4
# Stack addr of envp = stack addr of argv + argc (cid:2) 4 + 4
$v0, $a0, 2
# Place stack address corresponding to envp in R6
$a2, $a2, $v0
# Call subroutine main
main
$v0, $0, exit code# Place code for exit system call in R2
# Call OS to terminate the program

main: add
li
loop: li
li
syscall

# Place the address of c in $a1
$a1, $sp, 4
# Place the number of bytes to be read (i.e., 1) in $a2
$a2, 1
# Place the (cid:12)le descriptor (0) in $a0
$a0, 0
$v0, read code # Place the code for read in $v0
# Call OS routine to perform the read

blez
li
li
syscall
b

# break out of while loop if syscall returned zero
$v0, done
# Place the (cid:12)le descriptor (1) in R4
$a0, 1
$v0, write code # Place the code for write in $v0
# Call OS routine to perform the write
# go back to while loop

loop

done: jr

$ra

# return from subroutine main to start-up code

The linker produces an executable (cid:12)le that can directly run on a computer system (that
has an appropriate loader). Typically, this (cid:12)le has the same format as an ob ject (cid:12)le, except
that it contains no unresolved references.
Its target is the instruction set architecture
seen by the loader. Notice that, unlike the assembly process, the linking process does not
represent a change of level in the virtual machine abstraction tower, because the linker’s
input and output are programs for the same virtual machine, and use the same language.
Some linkers also produce an ASCII representation of the symbol table. This can be used
during debugging for displaying the addresses of symbols used in both ob ject and executable
(cid:12)les.

5.5

Instruction Formats: Design Choices

Like an assembly language program, a machine language program also consists of a sequence
of instructions, each one instructing the machine to do a speci(cid:12)c operation. However, the
machine language alphabet has only two symbols, f0, 1g. Therefore bit patterns are the
sole means of specifying each instruction. For instance, the MIPS-I AL instruction
add $v0, $a1, $a2
is represented in machine language as:
00000000100001010001000000100000

200

Chapter 5.

Instruction Set Architecture (ISA)

Never mind how we get this bit pattern|we will show that a little later. This bit pattern
encodes all of the information needed to correctly interpret the instruction, including the
opcode and all 3 explicit operands. In the general case, the encoded information includes:

(cid:15) opcode | operation to be performed

(cid:15) address/value of each explicit operand

(cid:15) addressing mode for each operand (optional)

The addressing mode part is treated as optional because the opcode often implies the
addressing mode; i.e., the addressing mode of the operand is unambiguous, and need not
be explicitly speci(cid:12)ed. How about labels and comments? Are they also encoded in the
machine language? Consider the same add instruction, with a label and comment added as
shown below:
# calculate return value
add $v0, $a1, $a2
done:
It turns out that the encoding for this AL instruction is same as that given above for the
instruction without the label and the comment! The label (cid:12)eld is indeed translated to
a memory address, but this address is not endoded along with the instruction; instead,
arrangements are made to ensure that the encoded ML instruction will be placed in that
memory addrress when the program is loaded. The comment (cid:12)eld is ignored altogether,
and is not represented in machine language programs.

A key issue to consider in deciding an encoding for the instructions is the number of
bits to be used for encoding each instruction type. If too many bits are used to encode
an instruction, then the code size increases unnecessarily, resulting in increased memory
storage requirement3 as well as increased instruction fetch bandwidth requirements. If too
few bits are used, it might become di(cid:14)cult to add new instructions to the ISA in the future.
Some ISAs use a (cid:12)xed number of bits to encode all of the instructions in the ISA, and the
rest use a variable number of bits to encode all of the instructions. Newer ISAs such as
MIPS-I, SPARC, Alpha, and PowerPC use (cid:12)xed length instruction encoding, whereas older
ISAs such as IA-32 and VAX use variable length instruction encoding. There are good
reasons why the older ISAs went for variable length instructions. When computers were
(cid:12)rst introduced, memory address space was at a premium, and it was important to reduce
the size of programs. The use of variable length instructions, coupled with an encoding
technique called Hu(cid:11)man encoding4 , enabled these ISAs to promote the development of
machine language programs that required the minimum amount of memory.

When an ISA design team has to choose instruction formats for a new ISA, a number
of factors must be considered. If a particular computer becomes commercially successful,

3Memory storage requirement was a ma jor concern when memory was expensive. With memory prices
falling steadily, this has become less of a concern, especially for general-purpose computers. In the embedded
computing world, it is still a concern.
4 In this encoding, the most frequent instructions are encoded with the fewest bits and the least frequent
ones are encoded with the maximum number of bits. The words in the English language follow such an
encoding to a large extent.

5.5.

Instruction Formats: Design Choices

201

its ISA may survive for 20 years or more, like the Intel IA-32. The ability to add new
instructions and exploit other opportunities that arise over an extended period is of great
importance, if the ISA survives long enough for it to be a success.

Field-based Encoding: Apart from the number of bits used for instruction encoding,
there is another ma jor di(cid:11)erence in how information in the di(cid:11)erent (cid:12)elds are encoded. One
approach starts by listing all of the unique combinations of opcodes, operand speci(cid:12)ers, and
addressing modes. Once this listing is done, a unique binary value is assigned to each
unique combination. Such an encoding is called total encoding. An alternate approach
is to encode each (cid:12)eld|the opcodes, the operand speci(cid:12)ers, and the addressing modes|
separately. This approach is called (cid:12)eld-based encoding. Such an encoding makes it easier
for the lower level machine to decode instructions during execution. A related feature that
makes it even easier to decode instructions is (cid:12)xed (cid:12)eld encoding. In such an encoding,
a particular set of bits are devoted for specifying a particular part of an instruction, such
as its opcode or operands. This type of encoding helps to determine the operands or their
addresses even before (cid:12)guring out the opcode of the instruction.

5.5.1 Fixed Length Instruction Encoding

In this type of encoding, all instructions are encoded with the same number of bits, and
therefore have the same length. Using a (cid:12)xed number of bits for instruction encoding makes
it easier to determine where each instruction starts and ends. Although this might waste
some space because all instructions are as long as the longest one, it makes instruction
decoding easier for the lower-level microarchitecture, which interprets ML instructions.
The customary method is to divide the bits in an instruction word into groups called (cid:12)elds.
Consider the instruction add rd, rs, rt. When the assembler converts this assembly-
language instruction into a machine-language instruction, it produces a bit pattern that
encodes the information contained in add rd, rs, rt. Consider the format given in Figure
5.3.

Opcode

Operand 1 Operand 2

Opcode specifiers

Addr Mode specifiers

Register specifiers

0 0 0

1

0 0

0

0 0

1

0 0

0

0

1

0

Addr
Mode

Addr/
Value

Addr
Mode

Addr/
Value

0000 : COPY
0001 : ADD
0010 : SUB
0011 : AND

000 : Register direct
001 : Memory direct
010 : Register indirect
011 : Memory indirect
100 : Immediate

000 : AX
001 : BX
010 : CX
011 : DX

Figure 5.3: A Sample 16-bit Format to Represent the Instruction add rd, rs, rt

In this format, 4 bits are allocated to specify the opcode, thereby allowing up to 16
unique opcodes in the instruction set. The opcode ADD is encoded by the bit pattern 0001.
The register names are also given speci(cid:12)c 3-bit patterns. For instance, the bit pattern 001

202

Chapter 5.

Instruction Set Architecture (ISA)

has been used to represent BX, and the bit pattern 010 has been used to represent CX. Three
bits are used to specify an addressing mode. For instance, the bit pattern 000 is used to
denote register direct addressing.

Example: What is the bit pattern for ADD AX, BX in the sample encoding of Figure 5.3?

The bit pattern for the opcode, ADD, is 0001. The bit patterns for registers AX and BX are
000 and 001, respectively. The bit pattern for register direct addressing is 000. Therefore,
the bit pattern corresponding to the instruction ADD AX, BX is 0001000000000001.

What would be the bit pattern for COPY AX, M[32]? We can see that it is not possible
to represent the bit pattern for memory address 32 in the 3 bits allotted for specifying an
address. To accommodate long addresses in a (cid:12)xed length format, instruction sets use a
technique called expanding opcodes or variable length opcodes.

Let us illustrate the idea of expanding opcodes using the instruction encoding done in
PDP-8, a popular single-address machine of the 1960s. It was a 12-bit machine made by
DEC. Of the 12 bits allotted to an instruction, 3 bits were used for specifying the opcode.
The PDP-8 used 3 di(cid:11)erent instruction formats, as shown in Figure 5.4. Let us calculate
the maximum number of unique opcodes that PDP-8 can have, and the maximum number
of memory locations that it can specify directly.

Single address instructions

Opcode

3

Addr
Mode
2

Address

7

I/O instructions

1

1

0

I/O Address

I/O Opcode

6

3

Zero address instructions

1

1

1

Opcode

9

Figure 5.4: Instruction Formats Used in the PDP-8 ISA

Consider the (cid:12)rst format, namely the one for single-address instructions. 3 bits have been
allocated for encoding the opcode, and therefore, 2 3 = 8 unique bit patterns are possible.
Out of the 8, two are used for identifying IO instructions and zero-address instructions.
Thus, a maximum of 6 single-address instruction opcodes are possible. In the IO instruction
format, 3 bits are allocated for encoding the opcode, and therefore, a maximum of 8 IO
instruction opcodes are possible. In the zero-address format, 9 bits are allocated for the
opcode, allowing a maximum of 29 = 512 unique opcodes. Thus, a total of 6 + 8 + 512 =
526 unique opcodes are possible for the PDP-8.

The maximum number of bits allocated for directly encoding memory addresses in any
of the three formats is 7. Thus, a maximum of 27 = 128 memory locations can be speci(cid:12)ed.

5.6. Data Formats: Design Choices and Standards

203

5.5.2 Variable Length Instruction Encoding

Some instruction sets have a large number of instructions, with many di(cid:11)erent addressing
modes. With such instruction sets, it may be impractical to encode all of the instructions
within a (cid:12)xed length of reasonable size. To encode such an instruction set, many instruction
sets use variable length instructions.

In the IA-32 ISA, instruction lengths may vary from 1 byte up to 17 bytes. Figure 5.*
shows the instruction formats for some of the commonly used IA-32 instructions. The 1
byte format is typically used for instructions that have no opcodes and those that involve
specifying a single register operand. The opcode byte usually contains a bit indicating
if the operands are 8 bits or 32 bits. For some opcodes, the addressing mode and the
register are (cid:12)xed, and therefore need not be explicitly speci(cid:12)ed. This is especially the
case for instructions of the form opcode register, immediate. Other instructions use
a post-byte or extra opcode byte, labeled mod, reg, r/m, which contains the addressing
mode information. This is the format for many of the instructions that specify a memory
operand. The base plus scaled index mode uses a second post-byte labeled sc, index,
base.

5.6 Data Formats: Design Choices and Standards

We have seen how instructions are represented in a machine language using bit patterns.
Next, we will see how data values are represented in an ISA. The encoding used to repre-
sent data values has a signi(cid:12)cant e(cid:11)ect on the complexity of the lower-level hardware that
performs various arithmetic/logical operations on the values. The hardware for performing
an arithmetic operation on two numbers can be simple or complex, depending on the repre-
sentation chosen for the numbers! Therefore, it is important to choose representations that
enable commonly used arithmetic/logical operations to be performed in a speedy manner.
When using N bits to represent numbers, 2N distinct bit patterns are possible. The
following table summarizes the number of representable values for popular word sizes.

Machine language programs often need to represent di(cid:11)erent kinds of information|
instructions, integers, (cid:13)oating-point numbers, and characters. All of these are represented
by bit patterns. An important aspect regarding the typical usage of bit patterns is that:

Bit patterns have no inherent meaning

Stating this di(cid:11)erently, a particular bit pattern may represent di(cid:11)erent information in
di(cid:11)erent contexts. Thus, the same bit pattern may be the representation for an integer as
well as a (cid:13)oating-point number. We shall illustrate this concept with an example from the
English language. The word pen has several meanings in English: (i) a small enclosure for
animals, (ii) an instrument used for writing, or (iii) to write. Depending on the context,

204

Chapter 5.

Instruction Set Architecture (ISA)

Number of
Word Size
(in bits) Representable Values
16
4
8
256
65,536
16
4.29 (cid:2)109
32
1.41 (cid:2)1014
48
1.84 (cid:2)1019
64

ISAs/Machines

Intel 4004
Intel 8080, Motorola 6800
DEC PDP 11, Intel 8086, Motorola?? 32020
IBM 370, Motorola 68020, VAX 11/780
Unisys
Cray, DEC Alpha

Table 5.4: Number of Representable Values for Di(cid:11)erent Word Sizes

we are able to (cid:12)gure out the intended meaning of the word pen. Similarly, based on the
context, the computer correctly (cid:12)gures out the information type and the intended meaning
of a bit pattern. It is important to note that the information type could also be encoded in
the bit pattern using additional bits, but that is rarely done.

\Time (cid:13)ies like an arrow, but fruit (cid:13)ies like an orange."

5.6.1 Unsigned Integers: Binary Number System

First, consider the encoding of unsigned integers, which only have a magnitude and no
sign. The standard encoding used to represent unsigned integers at the ISA level is to use
the binary number system. This is a positional number system, and the value of an N -bit
pattern, bN (cid:0)1 bN (cid:0)2 :::b1 b0 , if interpreted as an unsigned integer, is given by

Vunsigned = bN (cid:0)1 (cid:2) 2N (cid:0)1 + bN (cid:0)2 (cid:2) 2N (cid:0)2 + bN (cid:0)3 (cid:2) 2N (cid:0)3 + ::::: + b0 (cid:2) 20

Example: The value of bit pattern 10001101, if interpreted as an unsigned binary integer,
is given by
27 + 23 + 22 + 20 = 128 + 8 + 4 + 1 = 141.
To convert the representation of an N -bit unsigned integer to a 2N -bit number system,
all that needs to be done is to append N zeroes to the left of the most signi(cid:12)cant bit (MSB).

Range of an N -bit Unsigned Number System:

0 ! 2N (cid:0) 1
Figure 5.5 depicts this range for a 32-bit number system.

5.6. Data Formats: Design Choices and Standards

205

0

0

1

1

.
0

. . . . .
1 2 3 4

. . . . . . . . .

. . . . . . . . . . . .

..

. .

32
2 −1

Figure 5.5: Range of Unsigned Integers Represented in a 32-bit Binary Number System

5.6.2 Signed Integers: 2’s Complement Number System

The most widely used number system for representing signed integers at the ISA level is the
2’s complement number system. In the 2’s complement number system, positive numbers
are represented in their binary form as before, but negative numbers are represented in 2’s
complement form. The 2’s complement number system is also a positional number system,
and the value of a bit pattern, bN (cid:0)1 bN (cid:0)2 :::b1 b0 , if interpreted as a signed integer, is given
by

Vsigned = (cid:0)bN (cid:0)1 (cid:2) 2N (cid:0)1 + bN (cid:0)2 (cid:2) 2N (cid:0)2 + bN (cid:0)3 (cid:2) 2N (cid:0)3 + ::::: + b0 (cid:2) 20
This expression is similar to the one for usigned integers, except for the negative sign in
front of the bN (cid:0)1 term.
To convert the representation of an N -bit signed integer to a 2N -bit number system, all
that needs to be done is to append N copies of the sign bit to the left of the MSB.

Example: The integer (cid:0)12 is represented in the 8-bit 2’s complement number system as
11110100. Its representationj in the 16-bit 2’s complement number system is 1111111111110100,
obtained by appending 8 copies of the sign bit (1) to the left of the most signi(cid:12)cant bit.

Range of an N -bit 2’s Complement Number System:

(cid:0)2N (cid:0)1 ! 2N (cid:0)1 (cid:0) 1
Figure 5.6 depicts this range for a 32-bit number system. Roughly half of the range is
on the positive side and the other half is on the negative side. On comparing this range
with the one given earlier for unsigned integers, we can see that the upper half of the range
of unsigned integers have been replaced by negative integers.

Table 5.5 gives the bit pattern for some positive and negative numbers in the 8-bit 2’s
complement number system.

The main advantages of using the 2’s complement number system are:

(cid:15) Only one representation for zero|allows an extra (negative) number to be represented.

206

Chapter 5.

Instruction Set Architecture (ISA)

Negative Numbers

Positive Numbers

1

Any bit pattern

0

Any bit pattern

.

. . . . .

. . . . . . . . .
−2
−4
−3
−1

. . . . . . . .
1 2
3 4
0

. . . .

..

. .

31

−2

31
+2  −1

1 1

1

0

0

10

1

Figure 5.6: Range of Signed Integers Represented in a 32-bit 2’s Complement Number
System

Bit pattern Decimal value Remarks
(cid:0)128
Most negative representable integer
10000000
(cid:0)127
10000001

11111111
00000000
00000001
00000010

01111110
01111111

(cid:0)1
0
1
2

126
127

Unique representation for zero

Largest representable integer

Table 5.5: Decimal Equivalents of 8-bit Patterns in 2’s Complement Number System

(cid:15) The same adder can be used for adding unsigned integers as well as signed integers,
thereby simplifying the design of the ALU at the digital logic level.

For instance, if two signed integers (e.g., 45 and (cid:0)62), expressed in the Sign-Magnitude
number system, are added as if they are unsigned integers, then the result will be incorrect
((cid:0)107), as shown below. On the other hand, if the same two numbers are expressed in the
2’s complement number system and added as unsigned integers, the result ((cid:0)17) will be
correct.

5.6.3 Floating Point Numbers: ANSI/IEEE Floating Point Standard

Our next ob jective is to come up with suitable representations for (cid:13)oating-point numbers.
We already saw in Section 3.1.4 that a (cid:13)oating-point number has 4 parts: the sign, the
signi(cid:12)cand, the base, and the exponent. Among these, the base is (cid:12)xed for a given ISA,

5.6. Data Formats: Design Choices and Standards

207

Sign-Magnitude System

2’s Complement System

0

1

0

0

1 0 1 1 0 1

(45)

1

1

1 1 1 0

(-62)

0

0

1 0 1 1 0 1

(45)

1 1 0 0 0 0 1 0

(-62)

11010111

(-107)

11110111

(-17)

Incorrect

Correct

Figure 5.7: Adding Two Signed Integers in Sign-Magnitude Number System and 2’s Com-
plement Number System

and need not be explicitly encoded or represented in the bit pattern for a (cid:13)oating-point
number. The remaining parts (the sign, the signi(cid:12)cand, and the exponent) need to be
explicitly stored in the bit pattern. In designing a format for (cid:13)oating-point numbers, the
obvious choice is to use (cid:12)eld-based encoding, i.e., partition the available bit positions into
(cid:12)elds, and pack the di(cid:11)erent parts of the FP number into di(cid:11)erent (cid:12)elds. Thus the items
to be decided at ISA design time are:

(cid:15) the base

(cid:15) the signing convention for the signi(cid:12)cand and the exponent

(cid:15) the number of bits for specifying the signi(cid:12)cand and the exponent

(cid:15) the ordering of the sign, signi(cid:12)cand, and exponent (cid:12)elds

Until about 1980, almost every ISA used a unique FP format. There was no consensus
for even the base; di(cid:11)erent powers of 2 such as 2, 4, 8, or 16 have been used as the base.
The lack of a standard made it di(cid:14)cult to exchange FP data among di(cid:11)erent computers.
Worse yet, some computers occasionally did (cid:13)oating-point arithmetic incorrectly because
of some subtleties in (cid:13)oating-point arithmetic. To rectify this situation, the IEEE set up
a committee in the late 1970s to standardize FP representation and arithmetic. The goal
was not only to permit FP data to be exchanged among di(cid:11)erent computers but also to
provide hardware designers with a model known to be correct. The resulting work led
to IEEE Standard 754. Almost all of the present day ISAs (including the IA-32, Alpha,
SPARC, and JVM) use the IEEE FP standard and have FP instructions that conform to
this standard.

The IEEE standard de(cid:12)nes three formats: single precision (32 bits), double precision
(64 bits), and extended precision (80 bits). The extended precision format is intended to
reduce roundo(cid:11) errors while performing arithmetic operations. It is primarily used inside
FP arithmetic units, and so we will not discuss it further. Both the single- and double-
precision formats use base 2, and excess code for exponents. These two formats are shown
below.

208

Chapter 5.

Instruction Set Architecture (ISA)

\The most valuable of all talents is that of never using two words when one will do"
Thomas Je(cid:11)erson

1

8

23

1

11

52

Sign

Exponent

Significand

Sign

Exponent

Significand

(i)

(ii)

Figure 5.8: IEEE Floating-Point Formats: (i) Single Precision (ii) Double Precision

Single-Precision:

(cid:15) Base of the FP number system is 2
(cid:15) Number of bits allotted for representing the signi(cid:12)cand is 23
(cid:15) Signi(cid:12)cand’s precision, m is 24 (because the MSB is not explicitly stored)
(cid:15) Number of bits allotted for representing the exponent, e is 8
(cid:15) Format of the exponent: excess 127 code

Double-Precision:

(cid:15) Base of the FP number system is 2
(cid:15) Number of bits allotted for representing the signi(cid:12)cand is 52
(cid:15) Signi(cid:12)cand’s precision, m is 53 (because the MSB is not explicitly stored)
(cid:15) Number of bits allotted for representing the exponent, e is 11
(cid:15) Format of the exponent: excess 1023 code

Notice that the value used for the base is not encoded in the bit pattern, but forms part
of the de(cid:12)nition of the number system.
In the single-precision format, the exponent is
represented in 8 bits. With 8 bits, the exponent can take values ranging from (cid:0)128 to 127.
Of these, the most negative value ((cid:0)128) is treated as special; when the exponent is (cid:0)128,
the signi(cid:12)cand part is used to represent special numbers such as +1, (cid:0)1, and NaN (Not
a Number). Because of this arrangement, the most negative exponent allowed in this FP
format is (cid:0)127.

Normalization of Signi(cid:12)cand

When using the (cid:13)oating-point notation, a number can be written in a myriad ways. For
example, 0:001012 (cid:2) 23 , 1:012 (cid:2) 20 , and 101002 (cid:2) 2(cid:0)4 all denote 1:012 . To reduce this
profusion of equivalent forms, a speci(cid:12)c form is de(cid:12)ned to give every FP number a unique
representation. This process is called normalization. Normalization (cid:12)xes the position of
the radix point such that it immediately follows the most signi(cid:12)cant non-zero bit of the
signi(cid:12)cand. For example, the normalized form of 0:00101 2 (cid:2) 23 is 1:012 (cid:2) 20 . The normalized
signi(cid:12)cand is a fraction with value between 1 and almost 2.

5.6. Data Formats: Design Choices and Standards

209

Speci(cid:12)cally, Signi(cid:12)candMax = 1:111111:::::12 = 210 (approx)
Signi(cid:12)candMin = 1:000000:::::02 = 110

With non-zero binary numbers, the most signi(cid:12)cant non-zero bit of the signi(cid:12)cand is always
a 1, and therefore need not be explicitly stored. By not explicitly storing this bit, an extra bit
of precision can be provided for the signi(cid:12)cand. Thus, the single-precision format e(cid:11)ectively
represents 24-bit signi(cid:12)cands, which provides approximately the same precision as a 7-digit
decimal value. This technique of not explicitly representing the most signi(cid:12)cant bit is often
called the hidden bit technique. With this technique, the signi(cid:12)cand 1.011, for instance,
will be represented as follows, with the leading 1 and the radix point omitted.

0 1 1

0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

00

What about the number zero? Can it be expressed in normalized form? It turns out that
the number zero cannot be represented in the normalized form, because its signi(cid:12)cand does
not contain any 1s! The closest representable number is 1:0 2 (cid:2) 2(cid:0)127 , which has the smallest
normalized signi(cid:12)cand and the smallest exponent. A similar representation problem arises
when attempting to represent FP numbers that have magnitudes smaller than 1:0 2 (cid:2) 2(cid:0)127 ,
the smallest normalized FP number that can be represented with the allowed precision.
To provide good precision for these small numbers and to represent zero, IEEE 754 allows
these numbers to be represented in unnormalized form. Thus, when the exponent value is
(cid:0)127, the signi(cid:12)cand part is considered to have a hidden 0 (instead of a hidden 1) along
with a radix point. This makes it possible to represent numbers with very small magnitudes
and the number zero, at the expense of not representing some numbers such as 2 (cid:0)127 . For
example, the number zero will be represented as follows:

0

Bit pattern
for −127

0 0 0

0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

00

Biasing of Exponent

The designers of the IEEE 754 standard also considered several hardware issues when com-
ing up with this standard. In particular, they wanted an integer comparator to be able to
compare two (cid:13)oating-point numbers also. This is one of the motivating reasons for placing
the sign bit in the most signi(cid:12)cant position, and the exponent before the signi(cid:12)cand. Plac-
ing the sign bit at the most signi(cid:12)cant position ensures that the bit pattern of a negative
FP number will seem smaller than that of a positive FP number, if both patterns are inter-
preted as signed integers (in the 2’s complement number system). When using normalized
signi(cid:12)cands, for FP numbers with positive exponents, the bit patterns of numbers with
smaller exponents are guaranteed to appear smaller than numbers with bigger exponents.

A problem occurs, however, with negative exponents when exponents are represented in
the 2’s complement number system or any other number system in which negative integers

210

Chapter 5.

Instruction Set Architecture (ISA)

have a 1 in the MSB. This is because an FP number with a negative exponent will look
like a very big binary number. For example, the number 1:01 2 (cid:2) 2(cid:0)1 , which has a negative
exponent, would be represented as
0 11111111 01000000000000000000000
The number 1:012 (cid:2) 21 , which has a positive exponent, would be represented as
0 00000001 01000000000000000000000
If these two bit patterns are compared using an integer comparator, the latter pattern’s
value will be considered to be smaller than that of the former!

The desired notation for FP numbers must therefore represent the most negative expo-
nent as 000000002 and the most positive exponent as 111111112 . This can be achieved by
using the excess 127 code for representing the exponents so that the most negative exponent
((cid:0)127) can be represented as the bit pattern 00000000. In an excess E code system, before
representing an integer I , the excess value of E is purposely added to I such that the integer
value of the bit pattern stored is given by
S = I + E

With the use of the excess 127 code for the exponents, the representations for the above
two numbers (1:012 (cid:2) 2(cid:0)1 and 1:012 (cid:2) 21 ) become
0 01111110 01000000000000000000000

and

0 10000000 01000000000000000000000
respectively, in which case an integer comparator is su(cid:14)cient to compare them. Expressing
the exponent in excess code thus allows simpler hardware to compare two (cid:13)oating-point
numbers. At the same time, the excess code system retains the bene(cid:12)ts of the 2’s com-
plement number system in that it can handle negative exponents as gracefully as positive
exponents. Notice that the range of representable exponents is still (cid:0)127 ! 127. Thus,
the excess code system neither allows more exponents to be represented nor changes the
range of representable exponents. It only changes the bit patterns of the exponents that
were representable before the excess was applied.

In order to place the di(cid:11)erent features of the IEEE 754 standard in the proper context
and to see how they all (cid:12)t together, let us look at an example.

Example: What is the decimal equivalent of the following bit pattern if interpreted as an
IEEE single precision (cid:13)oating point number?
0 10000110 00110000000000000000000

(cid:15) Step 1: Extract the 8-bit exponent (cid:12)eld (10000110) and subtract 127 from it. One
way to do is to add the bit pattern 10000001, which is the 2’s complement of the bit
pattern for 127. The resulting bit pattern gives the exponent in 8-bit 2’s complement
number system. Find the decimal equivalent of this exponent.

(cid:15) Step 2: Extract the 23-bit signi(cid:12)cand (00110000000000000000000). If the exponent’s
actual value is (cid:0)127, include a ‘0.’ before the signi(cid:12)cand; otherwise include a ‘1.’.

5.6. Data Formats: Design Choices and Standards

211

(cid:15) Step 3: Extract the sign bit.
signi(cid:12)cand.

If it is a 1, then attach a negative sign before the

(cid:15) Step 4: Find the decimal equivalent of the signi(cid:12)cand.

Interestingly, when representing the exponents with the excess 127 code, the represen-
tation of (cid:13)oating-point number 0.0 becomes the all-zero pattern
00000000000000000000000000000000. Because zero is a frequently encountered (cid:13)oating-
point number, the ability to quickly identify zero from its bit pattern is a clear advantage.

Figure 5.6 succinctly shows the range of numbers representable by the IEEE 754 single-
precision format, and how it represents di(cid:11)erent regions within this range. The (cid:12)gure shows
the real number line; the dots in the line denote the real numbers that are representable
in the IEEE format. One ma jor di(cid:11)erence between the set of real numbers and the set
of representable (cid:13)oating-point numbers is their density. Real numbers form a continuum,
whereas the representable (cid:13)oating-point numbers do not form one. If a real number cannot
be represented in (cid:13)oating-point format, then the obvious thing to do is to round that number
to the nearest expressible (cid:13)oating-point number.

Zero

0

0

0

0

0

Negative Numbers
in Normalized Form

Positive Numbers
in Normalized Form

1

Any bit pattern

0

Any bit pattern

.

.

. . . .

.

.

. .

. .
...............
.
.
.
... ... .. .. . . . . .
.

..

.

...

..

.

.

-Infinity

128
-2

-126
-2

0

-126
+2

128
+2

+Infinity

1 1

1 0

0

0

1

1 0

0

1

0

0

Any non-zero pattern

0

0

0

Any non-zero pattern

Negative Numbers
in Unnormalized Form

Positive Numbers
in Unnormalized Form

Any bit pattern with at least one 0 and at least one 1

1

1 Any non-zero pattern

Not a Number

Figure 5.9: Range of Numbers Representable by IEEE 754 Single Precision Format

As we can see from the (cid:12)gure, the spacing between adjacent expressible numbers is
not constant throughout. However, when the spacing is expressed as a percentage of
the expressed numbers, there is no systematic variation throughout the expressible range.
Therefore, the relative error introduced by rounding is approximately the same for small
numbers and large numbers.

212

Chapter 5.

Instruction Set Architecture (ISA)

Rounding: As discussed earlier, because of using a (cid:12)nite number of bits, only a (cid:12)nite
number of distinct real numbers can be represented in the computer. This means that
most of the in(cid:12)nitely many real numbers must be rounded o(cid:11) to the nearest representable
(cid:13)oating-point number, producing roundo(cid:11) errors. In order to reduce roundo(cid:11) errors during
computation, IEEE provides the 80-bit extended precision format. Expand.

5.6.4 Characters: ASCII and Unicode

The last type of data type we look at is the character. At the ISA level, characters, like
numeric information, can only be represented by bit patterns. And in a manner similar to
numeric data, the assumptions made about the format of characters is made at ISA design
time.

The characters that are usually encountered in computer applications are upper-case
alphabet (A-Z), lower-case alphabet (a-z), decimal digits (0-9), punctuation and formatting
symbols (full stop, comma, quotation marks, space, tab, parenthesis, etc), arithmetic sym-
bols (+, (cid:0), etc), and control characters (CR, LF, etc). Altogether, we have more than 64
characters, but fewer than 128. Therefore, we need a minimum of 7 bits for encoding these
characters.

A standard code has been developed for representing characters. This code has received
almost universal acceptance, and is called ASCII (American Standard Code for Information
Interchange). ASCII uses a 7-bit representation for each character, allowing up to 128
di(cid:11)erent characters to be represented. The normal method for storing a character is to place
the 7-bit pattern in an 8-bit (cid:12)eld called a byte. The 8th bit can be used as the parity bit if
required. Table 5.6 gives a sample of the bit patterns assigned to characters in the ASCII
code. The (cid:12)rst 32 codes (0x0 - 0x1F) are assigned to control characters: codes originally
intended not to carry printable information, but rather to control devices (such as printers)
that make use of ASCII, or to provide meta-information about data streams such as those
stored on magnetic tape. For example, the bit pattern 0x08 represents \backspace", and
the bit pattern 0x0A represents the \line feed" function (which causes a printer to advance
its paper).

The character digits \0"-\9" are represented with their values in binary pre(cid:12)xed with
0x3 (this means that converting a BCD number to ASCII code involves just taking each
BCD nibble separately and pre(cid:12)xing it with 0x3. The uppercase alphabets A-Z are assigned
consecutive patterns from 0x41 - 0x5A. The lowercase alphabets a-z are also assigned con-
secutive patterns. The bit patterns for the lowercase and uppercase letters di(cid:11)er only in
the most signi(cid:12)cant bit. This trivializes case conversion to a range test (to avoid converting
characters that are not letters) followed by a single bitwise operation.

A character string is represented by a concatenation of the ASCII codes for its component
characters. For example, the 11-character string \God is good", is encoded in ASCII by
the following bit pattern sequence:
G
o
d

d

o

g

o

i

s

5.7. Designing ISAs for Better Performance

213

1000111 1101111 1100100 0100000 1101001 1110011 0100000 1100111 1101111 1101111 1100100

ASCII Code
00H
08H
0AH
0DH
20H
21H
30H
31H
.
.
39H
41H
42H
.
.
5AH

Character
NULL
BACKSPACE
Line Feed
Carriage Return
SPACE
!
0
1
.
.
9
A
B
.
.
Z

ASCII Code Character

61H
62H
.
.
7AH

a
b
.
.
z

Table 5.6: ASCII Codes for a Sample of Characters

In terms of mere adoption, the ASCII standard is perhaps one of the most successful
software standards ever introduced. Clearly, the 7-bit ASCII representation is not su(cid:14)cient
for representing the characters of languages such as Chinese and Japanese, which have a
very large number of characters. For representing the characters of such languages, more
bits are required. Recently, a 16-bit character code known as Unicode has been developed
as an international standard. The characters of most of the natural languages are encoded
in Unicode. In fact, the ASCII standard has become embedded in Unicode, as the (cid:12)rst 128
characters.

5.7 Designing ISAs for Better Performance

\We stil l have judgment here, that we but teach bloody instructions,
which, being taught, return to plague th inventor"
| Act I, scene 7 of Macbeth, Wil liam Shakespeare

The last ma jor topic we discuss in this chapter is ISA design, that is, coming up with
the speci(cid:12)cations for a new ISA. Unlike the case with microarchitectures and other lower-
level architectures, ISAs are not changed frequently, because of the need to maintain binary
compatibility for existing machine language programs.

214

Chapter 5.

Instruction Set Architecture (ISA)

5.7.1 Technological Improvements and Their E(cid:11)ects

Instruction set architectures have evolved greatly since the late 1940s, when the (cid:12)rst ISAs
were implemented. Much of this evolution has been in(cid:13)uenced by continued improvements
in hardware technology and compiler technology, and also by changes in the application
domain. For instance, the physical dimensions of the devices used to implement switching
functions at the lower hardware levels have been shrinking at a rapid pace, ever since
semiconductor transistors were invented. Today’s technology allows millions of transistors
to be integrated in a single chip. How does this a(cid:11)ect ISA design? First of all, more
functionality can be incorporated in the processor, thereby facilitating complex instructions
in the ISA. Secondly, large memories can be built more easily, thereby facilitating large
memory address spaces in the ISA.

Along with improvements in hardware technology came advances in language translation
techniques.

There are many aspects of an ISA that are common to all ISAs. Common arithmetic
and logical instructions, control-changing instructions, .... An ISA designer must consider
several aspects of a potential feature to decide if it supports or con(cid:13)icts with the design
goals.

During these (cid:12)fty years of ISA development, a consensus has emerged about the im-
portance of some ISA features, for example, support of arithmetic and logical instructions,
conditional branch instructions, subroutine call and return instructions, indirect addressing
mode(s), constant addressing mode, etc. Almost all of the current ISAs include these; de-
cisions to exclude any of these features must be made very carefully. On other issues, there
has been and remains fundamental disagreement, for instance over the question of whether
the support or lack of complex instructions is better. No single set of value judgments has
yet emerged, because di(cid:11)erent ISAs have di(cid:11)erent goals and intended uses. DSP processors,
for instance, include features such as .....

Competing Design Goals:
In the previous two chapters we had identi(cid:12)ed di(cid:11)erent types
of instruction opcodes and addressing mechanisms in the assembly-level architecture. One
of the questions that must be addressed by a computer architect concerns the number and
complexity of instructions to be included at the ISA level. One option is to include a large
number of opcodes and addressing modes so as to reduce the total number of instructions
in a program. An ISA that uses this method is called a complex instruction set computer
(CISC). An alternative method is to reduce the complexity of the instruction set, and
thereby reduce the complexity of the hardware required for interpreting the instruction set.
An ISA of this type is called a reduced instruction set computer (RISC). In this section, we
will examine some of the issues involved in this decision process.

The earliest computers were very simple in their ISA and implementation, both be-
cause of lack of experience with computers and because the implementation technology of
those days mandated a simple machine. However, computer users wanted to solve rela-

5.7. Designing ISAs for Better Performance

215

tively complex problems, and high-level languages were developed that treated variables
and arithmetic at a level higher than that of the machine language. Compilers were used
to translate programs written in high-level languages to machine language programs. This
resulted in what has become known as the semantic gap, which is the gap between the
language of the programmer and the language of the hardware.

5.7.2 CISC Design Philosophy

One of the ob jectives of CISC designs is to reduce this semantic gap between the high-level
language statements and the machine language instructions.
It was felt that narrowing
the gap by means of complex instructions and addressing modes would lead to better per-
formance. Thus, computer architects began to make the ISAs more complex, with cor-
respondingly complex hardware implementations, made possible by advances in hardware
technology. CISC ISAs generally seek to reduce the compiler’s complexity by making the
ML instructions more closely conform to the operations of the high-level languages. Some
computer systems (e.g. SYMBOL) have carried this to the extreme by completely abol-
ishing the semantic gap between the high-level language and the machine language. That
is, the high-level language itself is used as the native machine language. However, this is a
rare practice.

Another reason for the popularity of complex instructions was that it reduced the size of
machine language programs. Program and data storage were at a premium in those days.
For instance, when Motorola introduced the M6800, 16 KB RAM chips cost $500, and 40
MB hard disk drives cost $55,000. When the MC68000 was introduced, 64 KB RAM chips
still cost several hundred dollars, and 10 MB hard drives cost $5,000.

If there is an overriding characteristic of the CISC concept, it is an approach to ISA
design that emphasizes doing more with each instruction. As a result, CISC machines have
a wide variety of addressing modes, 14 in the case of the MC68000, and 25 in its more
complex successor, the MC68020. Furthermore, CISC machines allow an instruction to
have variable number of operands, which can be present in registers and/or memory. For
instance, the VAX ADD instruction can have two or three operands, and any one of them
can be in a register or in memory. Another example of a CISC ISA is Intel’s IA-32 ISA
(more commonly known as the x86 ISA).

5.7.3 RISC Design Philosophy

By the early 1980s, compilers had advanced to such a level that almost all of the program-
ming began to be done in high-level languages. At that time, studies were conducted to
analyze the instruction set usage in contemporary CISC machines. These studies indicated
that compiler-generated ML programs, for the most part, were utilizing only a small subset
of all available instructions. An indication of this can be seen when inspecting the assembly
language programs we wrote in the last two chapters; instructions such as li, lw, sw, add,

216

Chapter 5.

Instruction Set Architecture (ISA)

sub, beq, and bne were used most of the time. Carrying this observation to the next logical
step, computer architects (cid:12)gured that computer performance could be enhanced by includ-
ing in the ISA only the frequently used instructions, and by making them execute as fast
as possible. The RISC approach, therefore, is to use a simpler ISA so as to permit a faster
clock than that possible in a CISC processor. With many commercial implementations this
premise is ful(cid:12)lled. Examples are the MIPS-I, Sparc, PowerPC, and Alpha ISAs, which
were implemented in several commercial processors. In practice, RISC-type ISAs permit a
number of strategies to be employed by the microarchitects so as to make use of a variety of
implementation features, such as pipelining and multiple instruction issue. In addition, they
help to free up space within the processor chip that can be usefully employed to incorporate
on-chip cache memory.

The complex statements of a high-level language would be translated to longer instruc-
tion sequences than corresponding CISC instruction sequences. The result is that a machine
language program for a RISC ISA is likely to have more instructions, each of which may
execute faster. The name RISC fsigni(cid:12)es the focus on reducing the number and complexity
of the instructions in the ISA. There are several other tenets common to RISC ISAs, and
these are described in the following paragraphs. You should be aware, however, that a
particular RISC ISA may not have all of these tenets.

Minimal Number of Opcodes and Addressing Modes: RISC ISAs usually limit
themselves to about three addressing modes: register addressing, register indexed address-
ing, and constant addressing. Other, more-complex addressing modes are synthesized in
software from the simple ones. only the instructions that are frequently executed, RISC
machines exclude infrequently used instructions. Simpler instructions permit shorter clock
cycles, because less work has to be done in a clock. The result is a smaller, faster proces-
sor, that is capable of executing more instructions in a given amount of time than a CISC
processor. Complicated addressing modes mean longer clock periods, because there is more
address calculation to perform.

Fixed Instruction Length and Simple Instruction Formats: Another common char-
acteristic of RISC ISAs is the use of a few, simple instruction formats. Instructions have
(cid:12)xed length and are aligned on word boundaries. Instructions are encoded in such a way
that (cid:12)eld locations, especially the opcode, are (cid:12)xed. This type of encoding has a number of
bene(cid:12)ts. First, with (cid:12)xed (cid:12)elds, register operand accesses can proceed prior to the comple-
tion of instruction decoding. Second, simpli(cid:12)ed instruction formats simplify the instruction
decoder circuitry, making it faster.

Only Load and Store Instructions Access Main Memory: Another important
characteristic of RISC ISAs is that arithmetic/logic instructions deal only with register
operands. Thus, only data transfer instructions, such as LOAD and STORE instructions, deal
with the memory address space. Restricting the operands of arithmetic/logic instructions

5.8. Concluding Remarks

217

to be in registers makes it easier to meet the above stated ob jective of single cycle execution.
The RISC approach relies on the observation that values stored in registers are likely to be
used several times before they are written to main memory.

The proponents of the RISC philosophy cite several reasons why the RISC approach to
ISA design can potentially lead to faster processors. Some of these reasons are given below.

1. Instruction decoding can be faster because of simpler instruction decoder (due to fewer
instructions in the ISA).

2. Memory access is faster because of the use of on-chip cache memory (cf. chapter
7), which is possible because of less hardware in the chip to do instruction decoding,
complex addressing modes, and complex instructions.

3. The use of simpler instructions makes it easier to use a hardwired control unit, which
is faster (cf. chapter 9).

4. Clock cycle time is potentially smaller because of less hardware in time-critical path(s).

5.7.4 Recent Trends

\The amount of money you make is directly proportional to your vocabulary."

Multimedia Extensions

Vector Instructions

VLIW and EPIC Instructions

5.8 Concluding Remarks

From a functional point of view, the ISA implements the assembly-level architecture. Al-
though our discussion was based primarily on this view, historically, the ISA is de(cid:12)ned
before de(cid:12)ning the assembly-level architecture. In other words, the assembly-level architec-
ture being a symbolic form of the ISA, is built on top of the ISA. Considering the historical
development of computers, the ISA came into existence before the assembly-level architec-
ture as well as the high-level architecture; in the early computers, programs were directly
written in machine language, in 0s and 1s!

218

Chapter 5.

Instruction Set Architecture (ISA)

5.9 Exercises

1. What is the decimal equivalent of 0xDD580000 if interpreted as an IEEE single preci-
sion (cid:13)oating-point number?

2. What is the decimal equivalent of the following bit pattern if interpreted as an IEEE
single precision (cid:13)oating-point number?
0 00000000 000000 .... 0

3. What are the decimal equivalents of the largest and smallest positive numbers that
can be represented in the IEEE single precision (cid:13)oating-point format?

4. Consider a (cid:13)oating-point number format that is similar to the IEEE format in all
respects, except that it does not use the hidden bit technique. That is, it explicitly
stores the most signi(cid:12)cant bit of the signi(cid:12)cand. Show that this number system can
represent only fewer (cid:13)oating-point numbers than the IEEE number system.

5. Explain why denormalization is done in the IEEE (cid:13)oating-point format for the smallest
exponent?

6. Both the assembly language program and the machine language program are stored
as \bit patterns" inside the main memory. Explain how the bit patterns of these two
programs are di(cid:11)erent.

7. An ISA de(cid:12)nes 2 di(cid:11)erent instruction formats. In the (cid:12)rst format, 6 bits are used
for the opcode, Among these opcode bit patterns, three are used as special patterns.
When any of these three patterns appear in the opcode (cid:12)eld, the instruction is encoded
using the second format, and the actual opcode is present in another 6-bit (cid:12)eld. How
many unique opcodes can this ISA support?

8. Design a variable-length opcode (but (cid:12)xed-length instruction) to allow all of the fol-
lowing to be encoded in 36-bit formats:
7 instructions with two 15-bit addresses and one 3-bit register number
500 instructions with one 15-bit addresses and one 3-bit register number
50 instructions with no addresses or registers.

Part II

PROGRAM EXECUTION |
HARDWARE LEVELS

By wisdom a house is built, and through understanding it is established; through
know ledge its rooms are (cid:12)l led with rare and beautiful treasures.

Proverbs 24: 3-4

220

The theme of this book is that a modern computer can be viewed as a series of archi-
tectural abstractions, each one implementing the one above it. Part II of the book gave
a perspective of the computer from the high-level language, assembly language, and ma-
chine language programmers’ points of view. These views relate to the abstract machine
levels that deal with program development, typically called the software levels.
We now turn our attention to the design of hardware for carrying out program execution|
moving electrons through wires and semiconductors to (cid:12)nd the results of executing a
program with a speci(cid:12)c set of inputs. Leaping from the \lofty" programs to the \lowly"
electrons is quite a dive, as you can imagine! Naturally, computer hardware design, like
computer programming, is carried out at multiple abstraction levels. Although such a
multi-layer implementation may incur some performance costs, it does have important
engineering advantages. The main advantage of adding extra implementation levels is
that it permits the complexity of the hardware to be tackled in a step-by-step manner.
Once an appropriate interface has been agreed upon between two adjacent levels, the
development of these levels can proceed more or less independently.

We will study three of these levels|the microarchitecture, the RTL architecture, and the
logic-level architecture|in this part of the book. Although these machine levels have
traditionally been implemented in hardware (for a variety of reasons), their real distiction
from the previously seen machine levels is that they relate to program execution, and
implement the machine levels immediately above them by interpretation rather than by
translation. One point will become apparent when you study the hardware abstraction
levels presented in this last part of the book: many of the traditional digital circuit design
techniques are of limited use while designing the computer hardware! For instance, in
theory, the digital computer can be viewed as a (cid:12)nite state machine. However, it is not
practical to design a digital computer as a single (cid:12)nite state machine, because of the
combinatorial explosion in the number of states. In practice, computer hardware design
is carried out by partitioning the hardware into di(cid:11)erent parts, based on functionality.

Program execution begins with copying the executable binary from a storage medium
into the computer’s memory. This process, called loading the program, is typically done
by the operating system. Loading, along with related topics such as dynamic linking, are
discussed in Chapter 6.
The microarchitecture|the (cid:12)rst virtual machine level we study in this part|implements
the instruction set architecture (ISA) by interpreting ML programs, as illustrated in Fig-
ure 1.10 on page 42. The details of the microarchitecture depend on the ISA being
implemented, the hardware technology available, and the cost-performance-power con-
sumption goals for the computer system. Microarchitecture-related issues are covered in
two chapters. In Chapter 7, we discuss microarchitectural aspects related to implementing
the user mode ISA. The initial ob jective is to present a microarchitecture that correctly
implements the user mode ISA and correctly executes user mode ML programs. Then we
move on to more advanced microarchitectures for the processor system and the memory
system. In this part of the chapter, we give special importance to achieving high perfor-
mance and low power. One of the important microarchitectural techniques discussed for
improving processor performance is pipelining. For memory systems, we discuss cache
memories in detail. Chapter 8 discusses microarchitectural aspects that are speci(cid:12)c to
implementing the kernel mode ISA. Topics covered include exceptions and interrupts,
virtual memory, and the IO system.
Chapter 9 discusses register transfer level architecture.

The logic-level architecture implements the microarchitecture using logic gates, (cid:13)ip-(cid:13)ops,
and other building blocks. This virtual machine level is covered in detail in Chapter 10.

Chapter 6

Program Execution Basics

Apply your heart to instruction and your ears to words of know ledge.

Proverbs 23: 12

A successfully developed program becomes useful only when it is executed, most likely
with a set of inputs. Program execution involves executing in the proper sequence the
instructions speci(cid:12)ed in the program. A typical computer system has ....

\The execution of the laws is more important than the making of them."
| Thomas Je(cid:11)erson

\I know no method to secure the repeal of bad or obnoxious laws
so e(cid:11)ective as their stringent execution."
| Ulysses S. Grant

6.1 Overview of Program Execution

In a small embedded computer system or a microcontroller that executes only a single
program, the program to be executed may be permanently stored in some kind of ROM
(read-only memory) that implements the memory address space of the computer. This
stand-alone program utilizes and manages all of the system resources by itself. Upon reset,
the program counter points to the beginning of the program. Thereafter, the system con-
tinues to execute the program until the next reset or power down. \Loading" the program
into the ROM | a very rare event | is usually done using a ROM programmer device that
has direct access to the ROM.

221

222

Chapter 6. Program Execution Basics

The situation is more complex for general-purpose systems such as desktops and laptops,
which are controlled by an OS and can execute multiple programs. The ma jority of the
memory address space in such systems is implemented by RAM (random access memory) to
make it easy for end-users to change the program to be executed. Several steps are involved
in the execution of a program in such a system:

(cid:15) Select the program

(cid:15) Create the process

(cid:15) Load the program

(cid:15) Dynamically link libraries (optional)

(cid:15) Execute the program

(cid:15) Stop and restart the program (optional)

(cid:15) Halt the program

This chapter discusses each of these steps in detail. All of the steps, except the interpretation
step, are typically done by the OS. The interpretation step, which forms the bulk of program
execution, is usually done directly by the hardware, for e(cid:14)ciency reasons. We can, however,
do this step also in software, and for pedagogic reasons, we include a discussion on software
interprters at the end of this chapter.

6.2 Selecting the Program: User Interface

In computer systems with an installed operating system, the operating system decides which
program should be executed and when. The end-user can specify the program to be executed
by giving the appropriate command to the operating system (OS). The part of the OS that
receives commands from the end-user is called a shel l. From the end-user’s perspective, the
shell is thus an important part of a computer system; it is the interface between the user
and the system.

Functionally, the shell behaves like an interpreter, and operates in a simple loop: accept
a command, interpret the command, execute the command, and then wait for another
command. Executing the command often involves requesting the kernel to create a new
process (called a child process) that performs the command. The child process is overlaid
with the program to be executed; once the program is completed, the child process is
terminated. The program to be executed is typically stored on a (cid:12)le system maintained by
the OS.

Over the years, many shells have been used and improved. Historically, they have a
(cid:13)avor heavily dependent on the OS they are attached to. We can classify shells into one of
three categories based on the primary input device used to enter commands:

6.2. Selecting the Program: User Interface

223

(cid:15) Text based | command line interface (CLI)

(cid:15) Graphics based | graphical user interface (GUI)

(cid:15) Voice based | voice user interface (VUI)

In a CLI environment, a command can be given, for instance, by typing the command in
a terminal via a keyboard. In a GUI environment, a command can be given, for instance,
by pointing the mouse at the program’s icon and clicking it.
In a VUI environment, a
command can be given, for instance, by speaking into a microphone.

6.2.1 CLI Shells

In a CLI environment, the user speci(cid:12)es a program to be executed by typing the command
in a terminal via a keyboard. A CLI shell displays a prompt whenever it is ready to
accept a new command. A command consists of a command name, followed by command
options (optional) and command arguments (optional). The command name, options, and
arguments, are separated by blank space. The basic form of a CLI command is:
commandname [-options] [arguments]

commandname is the name of the program the end-user wants the computer to execute.
options, usually indicated by a dash, alter the behavior of the command. arguments are
the names of (cid:12)les, directories, or programs that the program needs to access. The square
brackets ([ and ]) signify optional parts of the command, which may be omitted.

Example: The Unix command ls -l /tmp gives a long listing of the contents of the /tmp
directory. In this example, ls is the command name, -l is an option that tells ls to create
a long, detailed output, and /tmp is an argument naming the directory that ls should list.
A lot of typing on a keyboard can lead to repetitive strain injury (RSI) 1 , Newer CLI
shells reduce the amount of typing, with features such as auto-completion, showing the
(cid:12)les it would complete, and showing the current directory in the prompt. In addition, they
provide many facilities such as command aliasing and job control. Furthermore, a collection
of commands may be stored in a (cid:12)le, and the shell can be invoked to execute the commands
in that (cid:12)le. Such a (cid:12)le is known as a shel l script (cid:12)le. The language used in that (cid:12)le is called
shel l script language. Like other programming languages, it has variables and (cid:13)ow control
statements (e.g. if-then-else, while, for, goto). Most shells also permit the user to abort a
command being executed, by typing Control-C on the terminal.

1Repetitive Stress Injury, sometimes referred to as overuse syndrome, is pain and swelling that results from
performing a repetitive task, like text messaging and typing. The traditional input devices that computer
professionals use|the keyboard and the mouse|force the user to adapt their posture in order to adequately
operate the input devices. The static tension of the muscles that occurs while operating a standard keyboard
and mouse is thought to be one of the main causes of RSI for computer professionals.

224

Chapter 6. Program Execution Basics

Examples of CLI shells for Unix/Linux operating systems are Bourne shell (sh), Bourne-
Again shell (bash), C shell (csh), TENEX C shell (tcsh), and Korn shell (ksh). Examples
of non-Unix CLI shells are command.com (for DOS), cmd.exe (for OS/2 in text mode and
for Windows NT).

6.2.2 GUI Shells

In a GUI environment, the end-user speci(cid:12)es a program to be executed by pointing the
mouse at the program’s icon and clicking it. The system then responds by loading the
program into the physical memory. It also creates a new window on the display for the user
to interact with the program. The end user can also change the size, shape, and position
of the window.

Figure 6.1: Photo/Screen Shot of a GUI Desktop

The visual approach and the ease of use o(cid:11)ered by GUIs have made them much more
popular, particularly among the masses. The relative merits of CLI- and GUI-based shells
are often debated. From the performance perspective, a GUI would probably be more
appropriate for a computer used for image or video editing. On the other hand, for certain
other operations such as moving (cid:12)les, a CLI tends to be more e(cid:14)cient than a GUI, making it
a better choice for servers used for data transfers and processing with expert administration.
The best choice is really dependent on the way in which a computer will be used.

Graphical user interfaces do have some disadvantages. They are harder to develop and
require a lot more memory to run. The system may also need to have powerful graphic
video capability. Moreover, they do not free the end user from repetitive strain injury (RSI).
Although the amount of muscle movement required is small with a mouse (compared to a
keyboard), all of the work done with a mouse is done with one hand and mostly with the

6.2. Selecting the Program: User Interface

225

index (cid:12)nger, whereas the keyboard lets the user distribute the work between two hands
and multiple (cid:12)ngers. In laptops, the mouse is usually located as a pointer in the middle of
the keyboard or as a touch pad. These require excellent coordination and precision of the
(cid:12)ngers, making the muscles extra tense. It is therefore important to use an external mouse
whenever possible.

\In baiting a mousetrap with cheese, always leave room for the mouse."
| Saki, in The Infernal Parliament

Examples of GUI shells for Unix/Linux operating systems (X Windows based) are KDE,
GNOME, Blackbox, and CDE. Examples of GUI shells for Microsoft Windows environments
are Windows Explorer, Litestep, Geoshell, BB4Win, and Emerge Desktop. Modern versions
of Microsoft’s Windows operating system o(cid:14)cially support only Windows Explorer as their
shell. Explorer provides the familiar desktop environment, start menu, and task bar, as
well as the (cid:12)le management functions of the operating system. Older versions also include
Program Manager, which was the Shell for the 3.x series of Microsoft Windows. Macintosh
Finder is a GUI for Apple. The newer versions of GUI shells improve end-user performance
by providing advanced menu options.

6.2.3 VUI Shells

The VUI environment provides a new dimension for the end-user to select programs to run
and to interact with the computer. It permits the user to speak command(s) directly into
a noise-cancelling microphone, with no intermediate keying or mouse clicks. The voice is
converted to digital form by the sound card, and is then processed by speech recognition
software. Voice recognition can be viewed as a problem de(cid:12)ned by three axes: vocabulary
size, degree of speaker independence, and continuous speech capability. Some of the impor-
tant issues in a VUI environment are speaker independence, continuous speech capability,
and (cid:13)exible vocabulary.

(cid:15) Vocabulary size: A large vocabulary size is bene(cid:12)cial, as it lets the end-user specify
a large number of unique commands. Some systems also permit customization of the
vocabulary.

(cid:15) Degree of speaker independence: Speaker independence allows a VUI to accept
and recognize (with high accuracy) commands spoken by many users, including voices
that were not part of its training set. Speaker-independent VUIs require no prior
training for an individual user.
In contrast, a speaker-dependent system requires
samples of speech for each individual user prior to system use (i.e., the user has to
train the system). Speaker-independence is desirable, as it permits the same computer
to be used by multiple end-users, without training sessions in between.

226

Chapter 6. Program Execution Basics

(cid:15) Degree of connectedness: This parameter deals with the extent to which words can
be slurred together. If a good deal of connectedness is allowed, the user can talk nat-
urally without pauses between words. The other extreme would sound like a teacher
in a locution class, mouthing each word. Depending on the degree of connectedness,
VUIs can be categorized as isolated word, connected word, and continuous speech. Iso-
lated word recognisers are the simplest, but require a short pause of approximately
1/5 second between each word. Connected word recognizers recognize words spoken
continuously, so long as the words do not vary as they run together, i.e., they require
clear pronunciation. Continuous speech allows the user to speak words as normally
spoken in (cid:13)uent speech, and is the most desirable one to have.

The simplest of the VUIs support a small vocabulary of speaker-dependent words, that
must be uttered with distinct pauses between each. On the other extreme would be a VUI
that supports a large vocabulary, and caters to a large number of speakers, who tend to
have slurred speech.

A typical o(cid:14)ce environment, with a high amplitude of background speech, turns out to
be one of the most adverse environments for current speech recognition technologies. Large-
vocabulary systems with speaker-independence that are designed to operate within these
adverse environments have been observed to have signi(cid:12)cantly lower recognition accuracy.
The typical recognition rate achievable as of 2005 for large-vocabulary speaker-independent
systems is about 80%-90% for a clear environment, and about 50% for a noisy environment
such as with a cellular phone.

Examples of voice recognition software are Apple Speech Recognition, DragonDictate (a
large vocabulary, speaker-adaptive voice recognition dictation system capable of recognizing
up to 120,000 words. It allows free-form dictation into most text-based applications.
It
allows full mouse movement as well as text and numerical dictation and complete formatting,
all by voice, completely hands free. DragonDictate uses industry standard third-party sound
cards.), NaturallySpeaking software, Voice Xpress package, and ViaVoice Pro.

In the past decade, tremendous advances in automatic speech recognition have taken
place. A reduction in the word error rate by more than a factor of 5 and an increase
in recognition speeds by several orders of magnitude (brought about by a combination of
faster recognition search algorithms and more powerful computers), have combined to make
high-accuracy, speaker-independent, continuous speech recognition for large vocabularies
possible in real time, on o(cid:11)-the-shelf workstations. These advances promise to make speech
recognition technology readily available to the general public.

Although VUI was introduced to combat the RSI problem inherent with keyboard and
mouse, it is not a magic wand either. Intensive use of VUI can sub ject the vocal cords to
overuse, and associated injury (vocal loading).

6.3. Creating the Process

227

\When I tel l you in few years it wil l be possible for you to sketch in the air and have
the thing you sketch in the air come to your eye, solid and real, so that you can walk
around it, so that you can scrutinize it from any direction and any view point you
please. I am tel ling you the truth."
| Steven A. Coons.
in a panel discussion entitled: The past and future of design
by computer, 1968

6.3 Creating the Process

6.4 Loading the Program

When the OS receives a command to execute a program, it transfers control to a loader (a
part of the OS), which reads the executable program into memory and initiates its execution.
When the program is loaded into memory, it is not just copied into memory and executed;
there are a number of steps the loader takes to load the image correctly and set up things
in that memory image before it jumps to the code in that image. The UNIX OS uses the
following steps to do loading; steps 2 and 3 may make sense only after studying virtual
memory, described in Chapter 9.

1. Read the executable (cid:12)le’s header and determine the size of the text and data sections.

2. Create a new virtual address space for the program. Allocate enough physical memory
to hold the text, data, and stack sections.

3. Copy the instructions and the data from the executable (cid:12)le onto the allotted physical
memory. If there is not enough physical memory, then copy at least the portion of
the text containing the entry point of the program, i.e., the (cid:12)rst instruction to be
executed.

4. Link and load libraries if load-time dynamic linking is to be done.

5. Copy onto the stack any parameters that are to be passed to the main function of the
program. Such parameters include command line arguments supplied by the user to
the OS shell.

6. Initialize the general-purpose registers. In general, most registers are cleared, but the
SP register is assigned the address of the top of stack.

7. Jump to the start-up routine at the beginning of the program.

228

Chapter 6. Program Execution Basics

6.4.1 Dynamic Linking of Libraries

In Chapter 6, we saw that libraries are often statically linked to the application program
while forming the executable binary. While this produces a \self-contained" executable
program, static linking of libraries does have some drawbacks. First and foremost, libraries
are generally quite large, and linking them to the static executable can result in very large
executables. Another drawback is that the executable cannot take advantage of newer
versions of the library when they become available, without re-doing the static linking.
Examples of libraries that are traditionally designed to be statically linked include the
ANSI C standard library and the ALIB assembler library.
Dynamic linking is a solution adopted to deal with these drawbacks 2 . With this scheme,
the library code is not stitched into the program executable by the static linker. Instead,
the static linker only records which libraries the program needs and their index names
or numbers. The libraries themselves remain in a separate (cid:12)le on disk. The ma jority of
the work of linking is done at program execution time. Such library routines are called
dynamical ly linked library or dynamic link library (DLL) routines. In Microsoft Windows
environments, dynamic libraries use the (cid:12)lename extension .dll. Two di(cid:11)erent options
exist for the time at which dynamic linking is done: load-time dynamic linking and run-time
dynamic linking.

6.4.1.1 Load-time dynamic linking

In this case, dynamic linking is done by the dynamic linker at the time the application
is loaded. The dynamic linker (cid:12)nds the relevant libraries on disk and links them to the
executable; these libraries are loaded at the same time to the process’ memory space. The
dynamic linker itself may be part of the library (as in Linux), or may be part of the OS
kernel (as in Windows ..). In the former case, the OS maps the dynamic linker to a part
of the process’ address space, and execution begins with the bootstrap code in the linker.
Some operating systems support dynamic linking only at load time, before the process starts
executing.

Lazy Procedure Linkage: Executables that call dynamic libraries generally contain
calls to a lot of functions in the library. During a single execution, many of these functions
may never be called. Furthermore, each dynamic library may also contain calls to functions
in other libraries, even fewer of which will be called during a given execution. In order to
reduce the overhead during program loading, dynamically linked ELF programs use lazy
binding of procedure addresses. That is, the address of a procedure is not bound until the
(cid:12)rst time the procedure is called.

2Although dynamic linking libraries have gained popularity only recently, they date back to at least the
MTS (Michigan Terminal System), built in the late 1960s [8].

6.4. Loading the Program

229

6.4.1.2 Run-time dynamic linking

In this case, the linking of a library is done just when it is actually referenced during the
execution of the process. This type of dynamic linking is often called delay loading.
Figure 6.2 illustrates how this type of dynamic linking occurs. When the user program
wants to call library routine S.dll (from address 0x401000), it uses a syscall to convey
the request to the dynamic loader part of the OS, as shown in part (a) of the (cid:12)gure. It
will also pass the name of the routine (S) to the OS. The dynamic loader loads S.dll (at
address 0x402000), as it was not previously loaded. This is shown in part (b) of the (cid:12)gure.
The loader then transfers control to S.dll. After the execution of S.dll is over, control
passes back to the dynamic loader, which then transfers control back to the user program
(at address 0x401004).

0x        1000
0x        1200

syscall

S.dll

0x        1000
0x        1200

syscall

S.dll

0x    400000

0x    400000

0x    400000

0x    401000

syscall

.text

0x    401000

syscall

.text

0x    401000

syscall

.text

0x80000080

0x80000080

0x80000080

0x80001000

Dynamic
Loader

OS space

0x80001000

Dynamic
Loader

OS space

0x80001000

Dynamic
Loader

OS space

$pc

0x    401000

$pc

0x80001100

$pc

0x        1000

(a) User program requests OS to load S.dll

(b) S.dll has been loaded

(c) S.dll is being executed

Figure 6.2: Memory Map During Di(cid:11)erent Stages of Dynamic Linking

One wrinkle that the loader must handle is that the memory location of the actual
library code is not known until after the executable and all dynamically linked libraries
have been loaded into memory, because the memory locations assigned will depend on
which speci(cid:12)c DLLs have been loaded. In theory, it is possible to examine the program
and replace all references to data in the libraries with pointers to the appropriate memory
locations once all DLLs have been loaded. However, this would require a large amount of

230

Chapter 6. Program Execution Basics

time and memory. Therefore, most dynamic library systems take a di(cid:11)erent approach. At
compile time, a symbol table with blank addresses called the import directory is linked into
the program. At load time, this table is updated with the location of the library code/data
by the loader/linker. At run time, all references to library code/data pass through the
import directory.

The library itself contains a table of all the methods within it, known as entry points.
Calls into the library "jump through" this table, looking up the location of the code in
memory, then calling it. This introduces overhead in calling into the library, but the delay
is usually so small as to be negligible.

Specifying the Library Routine: A dynamic link library routine can be speci(cid:12)ed in a
program executable in two di(cid:11)erent ways. The (cid:12)rst option is to specify the path that locates
the library within the OS (cid:12)le system. Any change to the library naming or layout of the (cid:12)le
system will cause these systems to fail. In the second option | the more common one | only
the name of the library routine (and not the path) is speci(cid:12)ed in the executable, with the
operating system incorporating a mechanism to locate the library in the (cid:12)le system. Unix-
based systems have a list of \places to look" in a con(cid:12)guration (cid:12)le, and dynamic library
routines are placed in these places. On the downside this can make installation of new
libraries problematic, and these "known" locations quickly become home to an increasing
number of library (cid:12)les, making management more complex. Microsoft Windows will check
the Registry to determine the proper place to (cid:12)nd an ActiveX DLL, but for standard DLLs
it will check the current working directory; the directory set by SetDllDirectory(); the
System32, System, and Windows directories; and (cid:12)nally the PATH environment variable.

6.5 Executing the Program

After the loader loads the selected program into memory and completes all steps of dynamic
linking, it transfers control to the program. That is, the program counter is set to the entry
point of the program. Thereafter, the processor executes the loaded program by executing
the instructions (in the .text portion) of the program. Execution of these instructions
involves repeating two actions: fetch and execute. Fetch concerns obtaining the instruction
bit pattern from memory and decoding the bit pattern. Execution involves carrying out
the action speci(cid:12)ed in the instruction, including determining the next instruction to be
executed. The fetch and execute phases can be subdivided into the following sequence of
steps (some of which are optional):

6.6. Halting the Program

231

Executing the ML Program

(cid:15) Fetch Phase

1. Fetch instruction: Read the next instruction from the memory.

2. Decode instruction: Decode the instruction bit pattern so as to determine
the action speci(cid:12)ed by it.

(cid:15) Execution Phase

1. Fetch source operand values: Fetch source operands (if any) from regis-
ters, memory locations, or IO registers.

2. Process data: Perform arithmetic or logical operation on source operand
values, if required.

3. Write result operand value: Write the result of an arithmetic or logical
operation to a register, memory location, or IO register, if required.

4. Update program counter: Normally, the program counter needs to be in-
cremented so as to point to the immediately following instruction. When
a control (cid:13)ow change is required because of a branch instruction or the
like, the program counter is updated with the new target address deter-
mined in the execution phase.

6.6 Halting the Program

6.7

Instruction Set Simulator

We have seen the fundamental steps involved in executing a (machine language) program.
Although the operating system plays a ma jor role in setting up the stage, the bulk of
the work | interpreting the program | is usually carried out directly in hardware. A
hardware interpreter is more popular because of its superiority in performance and power
consumption. As pointed out in Chapter 1, however, the distinction between hardware and
software is somewhat blur. We can in fact implement an emulator in software; such an
emulator is generally called a software simulator. The SPIM simulator that we discussed
in Chapter 3 is a good example. It loaded MIPS assembly language programs, translated
them to MIPS machine language (ML) programs, and then executed them by interpreting
the ML instructions, one by one. Partly to prove this point, and more importantly, to
start with a simple emulator, we shall (cid:12)rst study a software emulator, and then move on to
hardware interpreters in the next chapter.

It is important to note that a software simulator program can perform its emulation

232

Chapter 6. Program Execution Basics

of a target ISA only when the simulator is executed on another microarchitecture (called
host machine), entailing an interpretation by the host machine’s control unit. Thus, imple-
menting a microarchitecture in software introduces an extra interpretation step for the ML
program. Why in any case, would anyone want to introduce extra interpretation steps, other
than for pedagogic reasons? Although software simulation may not make much sense for
ordinary users, software simulation is the de facto tool of the trade for computer architects
who are in the business of developing new ISAs and microarchitectures.

Another common situation that warrants the use of a software simulator is in executing
Java applets that are supplied over the web. Java applets are compiled and assembled into
bytecode, with the Java Virtual Machine (JVM) as the target ISA. Java bytecode
can be directly executed on hardware only if the hardware implements the JVM ISA. When
a web browser running on a di(cid:11)erent platform downloads a Java applet, it interprets the
bytecode using a software interpreter, as illustrated in Figure 6.3. This permits the web
server to supply the same bytecode irrespective of the platform from which web browsing
is done.

JVM
Bytecode

Java Class
Libraries

JVM

JVM Interpreter

Host OS

Host ISA

Host Hardware

Figure 6.3: Execution of a Java Application by Interpretation

It is important to note that this simulator uses a behavioral approach, and models only
the functional aspects of emulation, without doing a hardware microarchitecture design. A
functional simulator thus correctly executes machine language programs, without modeling
any hardware devices. Such a simulator has no notion of hardware-speci(cid:12)c features such as
clock cycles and buses. It just takes in as input the program to be executed (along with
the executed program’s inputs), and outputs (among other things) the outputs generated
by the executed program. Examples are the SPIM simulator and the JVM interpreter.

We shall develop a simple functional simulator here. For simplicity and ease of under-
standing, we restrict ourselves to a subset of the MIPS-I ML instruction set which we call

6.7.

Instruction Set Simulator

233

MIPS-0. The encodings of the MIPS-0 instructions and the semantics are same as that in
the MIPS-I ISA. The MIPS-0 instruction set, along with the instruction encodings, is given
in Figure 6.4.

LUI

rt, immed

001111

LW

rt, offset(rs)

100011

SW rt, offset(rs)

101011

ADDI

rt, rs, immed

001000

ADDU rd, rs, rt

000000

SUBU

rd, rs, rt

000000

ANDI

rt, rs, immed

001100

AND

rd, rs, rt

000000

ORI

rt, rs, immed

001101

OR

rd, rs, rt

000000

NOR

rd, rs, rt

000000

SLLV

rd, rs, rt

000000

BEQ rs, rt, offset

000100

BNE rs, rt, offset

000101

JALR

rd, rs

JR

rs

SYSCALL

000000

000000

000000

rs

rs

rs

rs

rs

rs

rs

rs

rs

rs

rs

rs

rs

rs

rs

rt

rt

rt

rt

rt

rt

rt

rt

rt

rt

rt

rt

rt

rt

immed

offset

offset

immed

immed

immed

offset

offset

100001

100011

100100

100101

100111

000100

001001

001000

001100

rd

rd

rd

rd

rd

rd

rd

Figure 6.4: The MIPS-0 Instruction Set, along with Encoding

6.7.1

Implementing the Register Space

Let us take a closer look at designing functional simulators. First, we will see how the register
model speci(cid:12)ed in the ISA is implemented in the simulator. This can be accomplished by
declaring an appropriate data structure (usually an array) in the simulator.

/* Allocate space for modeling the ISA-visible registers */
long R[32];
long PC;

234

Chapter 6. Program Execution Basics

Apart from the ISA-visible registers, many new \registers" may need to be de(cid:12)ned, for
storing temporary values.

6.7.2

Implementing the Memory Address Space

Conceptually, the memory address space of the target ISA can be implemented in a manner
similar to what we did for the register space. Notice that with this simple approach, either
the target’s memory space should be smaller than that of the host, or the entire memory
space of the target is not simulated.

Host’s User Memory Space

Text

Data

Registers

Target’s Simulated
Register Space

Target’s Simulated
Memory Space

Text

Data

Stack

Stack

Figure 6.5: Implementing the Register Space and Memory Space in a Functional Simulator

The simulator maintains the mapping between the target addresses and the host ad-
dresses. When the simulated program accesses a memory location, the simulator translates
that address to a host memory address.

/* Allocate space for modeling the 3 sections of the memory address space */
long text[0x800000];
long data[0x800000];

6.7.

Instruction Set Simulator

235

long stack[0x800000];

6.7.3 Program Loading

The binary program to be executed (i.e., the ML program whose execution needs to be
simulated) is typically stored as a (cid:12)le in the host machine’s (cid:12)le system. Prior to execution
by the simulator, this program needs to be loaded into the target’s instruction memory
which is modeled in the simulator. This loading job is quite similar to the job done by the
loader part of the OS when a user requests a program to be executed directly in a machine.
The simulator needs to have the functionality to perform this \loading".

6.7.4

Instruction Fetch Phase

/* Fetch the instruction from the text section of memory */
IR = text[PC - 0x400000];

/* Separate the different fields of the instruction */
opcode = [IR & 0xfc000000] >> 26;
rs
= [IR & 0x3e00000] >> 21;
= [IR & 0x1f0000] >> 16;
rt
= [IR & 0xf800] >> 11;
rd
func
= IR & 0x3f;
offset = IR & 0xffff;

/* Decode the instruction */
switch (opcode)
{

case 000000b:

case 000100b: /* beq */

case 000101b: /* bne */

case 001000b: /* addi */

}

6.7.5 Executing the ML Instructions

Once the program to be executed is loaded into the simulator’s memory, execution of the
program (i.e., simulation of the execution) can begin. The PC variable is initialized to the

236

Chapter 6. Program Execution Basics

program starting address. The simulator then goes through a loop, with each iteration of
the loop implementing the fetching and execution of an ML instruction.

The simulation of the machine is done by just a big function in the simulator. This
function understands the format of MIPS instructions and the expected behavior of those
instructions as de(cid:12)ned by the MIPS architecture. When the MIPS simulator is executing
a \user program", it simulates the behavior of a real MIPS CPU by executing a tight
loop, fetching MIPS instructions from the simulated machine memory and \executing"
them by transforming the state of the simulated memory and simulated machine registers
according to the de(cid:12)ned meaning of the instructions in the MIPS architecture speci(cid:12)cation.
Remember that the simulated machine’s physical memory and registers are data structures
in the simulator program.

6.7.6 Executing the Syscall Instruction

Execution of syscall instructions in the simulator is somewhat complex due to a variety of
reasons. First of all, the binary program being \executed" does not contain the Kernel code
that needs to be executed upon encountering a syscall instruction; this code is a part of the
operating system of the target machine. Therefore, instead of simulating the execution of
Kernel code on an instruction-by-instruction basis, the simulator directly implements the
functionality speci(cid:12)ed by the syscall instruction.

Secondly, many of the syscall instructions require intervention of the host machine’s
operating system. For instance, if a syscall instruction speci(cid:12)es a (cid:12)le to be opened (open()
system call), only the host machine’s operating system can open the (cid:12)le; remember that
the simulator is just an Application program (User mode program) that runs on the host
machine. Similarly, if a syscall instruction speci(cid:12)es another process to be created (fork()
system call), only the host machine’s operating system can create another process. For
such system calls, the simulator should act as an interface to the host machine’s operating
system. Some other system calls, such as brk() (which requests the operating system to
increase the size of the heap memory segment), on the other hand, do not require any
intervention from the host operating system. Instead, the simulator itself will act as the
operating system for the executed program.

The simulator’s \kernel" controls the simulated machine in the same way that a real
OS kernel controls a real machine. Like a real kernel on a real machine, the simulator’s
kernel can direct the simulated machine to begin executing code in user mode at a speci(cid:12)c
memory address. The machine will return control to the kernel if the simulated user program
executes a syscall or trap instruction, or if an interrupt or other machine exception occurs.

Like a real kernel, the simulator’s kernel must examine and modify the machine registers
and other machine state in order to service exceptions and run user programs. For example,
system call arguments and results are passed between a user program and the kernel through
the machine’s registers. The kernel will also modify page table data structures that are used
by the simulated machine for translating virtual addresses to physical addresses. From the

6.8. Hardware Design

237

perspective of the simulator’s kernel, all of the machine state { registers, memory, and page
tables { are simply data structures (most likely arrays) in the kernel address space??

Figure needed here.

6.7.7 Comparison with Hardware Microarchitecture

In the next chapter, we will study hardware microarchitectures, which interpret machine
language programs using hardware circuitry. It is informative to compare and contrast the
functionality performed in traditional microarchitectures and simulators.

HW Microarchitecture
Attribute
Hardware registers
Register space
Memory address space Hardware memory structures
Loader part of OS
Loading
Interpretation
Control unit hardware

SW Microarchitecture
Simulator program variables
Simulator program variables
Loader part of simulator

Table 6.1: A Succinct Comparison of Hardware and Software Microarchitectures

6.8 Hardware Design

6.8.1 Clock

Computer hardware, like other synchronous digital circuits, use a clock signal to coordinate
the actions of two or more circuit blocks. A clock signal oscillates between a high and a low
state, normally with a 50% duty cycle. In other words, the signal is a square wave. The
circuits using the clock signal for synchronization may become active at either the rising or
falling edge, or both, of the clock signal. Processor clock speed or clock rate is an indicator of
the speed at which the processor executes instructions. Every computer hardware contains
an internal clock that regulates the rate at which instructions are executed and synchronizes
all the various computer components. The processor requires several clock ticks (or clock
cycles) to execute each instruction. The faster the clock, the more instructions the processor
can generally execute per second. Clock speeds are expressed in megahertz (MHz) or
gigahertz (GHz).

6.8.2 Hardware Description Language (HDL)

The two ma jor HDLs are Verilog and VHDL (Very high speed integrated circuits HDL).
Both of these are equally popular and have roughly equal market presence.

238

Chapter 6. Program Execution Basics

Machine−Level Architecture
(Instruction Set Architecture)

User Mode

Kernel Mode Designed by Instruction Set Architect

User Program (ML)

Interpretation
by OS code

OS Program (ML)

Control Unit (Interpretor)

Microarchitecture

PC     MAR

Logic−Level Architecture
PC_out, MAR_in

Microarchitectural       Data Path
(RFs, Caches,      ALUs, Buses)

Designed by Microarchitect

Microinstruction

Microsequencer (Interpretor)

Logic−Level         Data Path
(Gates, Flip−flops, MUXes, ROMs)

Designed by Logic Designer

Control Signals

Device Control Inputs (Implementor)

Device−Level Architecture

Device−Level            Data Path
(Transistors, Wires, Layouts)

Designed by VLSI Designer

Figure 6.6: Machine Abstractions relevant to Program Execution, along with the Ma jor
Components of each Abstract Machine.

6.8.3 Design Speci(cid:12)cation in HDL

6.8.4 Design Veri(cid:12)cation using Simulation

6.8.5 Hardware Design Metrics

6.8.5.1 Technology Trends

Moore’s Law:

6.8.5.2 Performance

\A man with a watch knows what time it is.
A man with two watches is never sure."
| Segal’s Law

The performance of a computer depends on a number of factors, many of which are re-
lated to the design of the processor data path. Three of the most important factors are the
strength of the machine language instructions, the number of clock cycles required to fetch

6.9. Concluding Remarks

239

and execute a machine language instruction, and the clock speed. A powerful instruction
performs a complex operation and accomplishes more than what a simple instruction ac-
complishes, although its execution might take several additional clock cycles. The strength
of instructions is an issue that is dealt with at the ISA level (taking into consideration
microarchitectural issues), and is not under the control of the microarchitect.

Clock speed has a ma jor in(cid:13)uence on performance, and depends on the technology used
to implement the electronic circuits. The use of densely packed, small transistors to fabricate
the digital circuits leads to high clock speeds. Thus, implementing the entire processor on
a single VLSI chip allows much higher clock speeds than would be possible if several chips
were used. The use of simple logic circuits also makes it easier to clock the circuit faster.
Clock speed is an issue that is primarily dealt with at the design of logic-level architectures,
although microarchitectural decisions have a strong bearing on the maximum clock speeds
attainable.

The third factor in performance, namely the number of clock cycles required to fetch
and execute an instruction, is certainly a microarchitectural issue. In the last several years,
speed improvements due to better microarchitectures, while less amazing than that due to
faster circuits, have nevertheless been impressive. There are two ways to reduce the average
number of cycles required to fetch and execute an instruction: (i) reduce the number of
cycles needed to fetch and execute each instruction, and (ii) overlap the interpretation
of multiple instructions so that the average number of cycles per instruction is reduced.
Both involve signi(cid:12)cant modi(cid:12)cations to the processor data path, primarily to add more
connectivity and latches.

6.8.5.3 Power Consumption

6.8.5.4 Price

6.9 Concluding Remarks

6.10 Exercises

1. Explain the di(cid:11)erence between static, load-time, and lazy (fully dynamic) linking.
What are the advantages and disadvantages of each type of linking?

2. Write a software simulator program (in any high-level language) that can interpret
machine language programs written for the MIPS-0 ISA.

240

Chapter 6. Program Execution Basics

Chapter 7

Microarchitecture | User Mode

Listen to counsel and receive instruction, That you may be wise in your latter days.

Proverbs 19: 20

Executing a program involves several steps, as highlighted in the previous chapter. These
steps are carried out by di(cid:11)erent entities: program selection is typically done by the end user;
process creation, program loading, and process halting are typically done by the operating
system. Program execution | the bulk of the e(cid:11)ort | generally falls on the processor’s
shoulders1 . The processor is a hardware entity.
Our ob jective in this chapter is to implement the machine speci(cid:12)cation given in the
instruction set architecture (ISA). Because of the complexity of this machine, it is imprac-
tical to directly design a gate-level circuitry that implements the ISA. Therefore, computer
architects have taken a more structured approach by introducing one or more hardware
abstraction levels in between2 . The abstraction level directly below the instruction set ar-
chitecture is called the microarchitecture. This level is responsible for executing machine
language programs.

In this chapter, we study the organization and operation of the di(cid:11)erent components
that constitute the user mode microarchitecture, the machine that is responsible for imple-
menting the user mode instruction set architecture (ISA)|i.e., carrying out the execution
of user mode machine language programs. A microarchitectural view of a computer is re-
stricted to seeing the ma jor building blocks of the computer, such as register (cid:12)les, memory
units, ALUs and other functional units, and di(cid:11)erent buses.

1Even the steps carried out by the operating system involve work by the processor, as the operating
system itself is a process that is executed by the processor, in kernel mode.
2Even the design of simple digital circuits involves a somewhat structured approach. When designing a
circuit as a (cid:12)nite state machine, we (cid:12)rst construct the state transition diagram, and then implement the
state transition diagram.

241

242

Chapter 7. Microarchitecture | User Mode

A particular ISA may be implemented in di(cid:11)erent ways, with di(cid:11)erent microarchitec-
tures. An executable program developed for this ISA can be executed on any of these
microarchitectures without any change, regardless of their di(cid:11)erences. For example, the
Intel IA-32 ISA has been implemented in di(cid:11)erent processors such as Pentium III, Pentium
4, and Athlon. Some of these processors are built by Intel, and the others by competitors
such as AMD and Cyrix. Even when using the same processor type, computer vendors build
many di(cid:11)erent system microarchitectures by putting together processors, memory modules,
and IO interface modules in di(cid:11)erent ways. One microarchitecture might focus on high
performance, whereas another might focus on reducing the cost or the power consumption.
The ability to develop di(cid:11)erent microarchitectures for the same ISA allows processor ven-
dors and memory vendors to take advantage of new IC process technology, while providing
upward compatibility to the users for their past investments in software. Although our
discussion may seem to hint that the microarchitecture is always implemented in hardware,
strictly speaking, the microarchitecture only speci(cid:12)es an abstract model. This abstract ma-
chine can be implemented in software if needed. Examples are the SPIM simulator we saw
in Chapter 4 (which is a functional simulator) and the SimpleScalar simulator [?] (which is
a cycle-accurate simulator).

Like the design of the higher level architectures that we already saw, microarchitecture
design is also replete with trade-o(cid:11)s. The trade-o(cid:11)s at this level involve characteristics such
as speed, cost, power consumption, die size, and reliability. For general-purpose computers
such as desktops, one trade-o(cid:11) drives the most important choices the microarchitect must
make: speed versus cost. For laptops and embedded systems, the important considerations
are size and power consumption. For space exploration and other critical applications,
reliability is of primary concern.

Present day computer microarchitecture is incredibly complex. Fortunately, the two
principles that enabled computer scientists to develop high-level architectures that support
million-line programs|modularization and partitioning|are also available to computer en-
gineers to design complex microarchitectures that meet various requirements. By breaking
up a microarchitecture into multiple blocks, the design becomes much more tractable. For
simplifying the discussion, this chapter presents only microarchitectural features that are
speci(cid:12)c to implementing the user mode features of the ISA. Chapter 8 deals with microar-
chitectural aspects that are speci(cid:12)c to implementing the kernel mode features of the ISA.
The current chapter addresses some of the fundamental questions concerning user mode
microarchitecture, such as:

(cid:15) What are the building blocks in a computer microarchitecture, and how are they
connected together?

(cid:15) What steps should the microarchitecture perform to sequence through a machine
language program, and to execute (i.e., accomplish the work speci(cid:12)ed in) each machine
language instruction?

(cid:15) What are some simple organizational techniques that can reduce the number of time-

7.1. Overview of User Mode Microarchitecture

243

steps needed to execute each machine language instruction?

(cid:15) What are some techniques commonly used to reduce the latency of memory accesses?

Go to the ant, you sluggard; consider its ways and be wise! It has no commander, no
overseer or ruler, yet it stores its provisions in summer and gathers its food at harvest.
Proverbs 6: 6-8

7.1 Overview of User Mode Microarchitecture

A commonly used approach for designing hardware | especially small to medium size
circuits | is the (cid:12)nite state machine approach. Finite state machine-based design involves
identifying a suitable state variable, identifying all possible states, and then developing a
state transition diagram. Theoretically, the computer hardware can also be built in this
manner as a single (cid:12)nite state machine; it is, after all, a digital circuit. However, such a
(cid:12)nite state machine would have far too many states (a single bit change in a single memory
location changes the system state), making it extremely di(cid:14)cult to comprehend the complex
functionality, let alone design one in an e(cid:14)cient manner.

7.1.1 Dichotomy: Data Path and Control Unit

In order to tackle the above state explosion, microarchitects partition the computer hard-
ware into a data path and a control unit3 . The data path serves as the platform for executing
machine language (ML) programs, whereas the control unit serves as the interpreter of ML
programs, for execution on the data path. This point needs further clari(cid:12)cation. In order to
execute a machine language program on the data path, the program has to be interpreted.
This interpretation is done by the control unit. The relation between the interpreter and
the microarchitecture is pictorially depicted in Figure 7.1.

ML Program

Control Unit

(Interpreter)

Status

Control

Data Path

Figure 7.1: Relation between the Data Path and the Control Unit (Interpreter)

3The data path itself is partitioned into several smaller data paths (with their own miniature control
units) to keep the design even simpler. Thus, the memory unit is designed as a separate data path, with its
own memory controller.

244

Chapter 7. Microarchitecture | User Mode

The data path incorporates circuitry and interconnections that are required for executing
each of the instructions de(cid:12)ned in the instruction set architecture, and serves as a platform
for executing ML programs. It is a collection of storage components such as registers and
memories, functional units such as ALUs, multipliers, and shifters, as well as interconnects
that connect these components. The movement of data through the data path is controlled
by the control unit. Microarchitects use a language called micro-assembly language
(MAL) to indicate the control actions speci(cid:12)ed by the control unit. In order to execute
ML programs on the data path, ML programs are interpreted in terms of MAL commands.
This interpretation is performed by the control unit, which supplies the MAL commands
to the data path. Because a signi(cid:12)cant portion of the functionality has been shifted to
the data path, the control unit can be conveniently built as a (cid:12)nite state machine with a
manageable number of states. A change in a main memory value does not cause a state
transition in the control unit, for instance.

The sequence of operations performed in the data path is determined by commands
generated by the control unit. The control unit thus functions as the interpreter of machine
language programs.

The computer data path can be easily divided into ma jor building blocks and subsystems
based on how data (cid:13)ow is speci(cid:12)ed in the ISA. That is, we break the functionality speci(cid:12)ed
in the machine language into di(cid:11)erent parts, and use separate building blocks to implement
each of the parts in the data path. Thus at the system level, we see the computer hardware
in terms of ma jor building blocks and their interconnections.

In order to design a data path for implementing a user mode ISA we need to consider
what the user mode ISA requires for data to (cid:13)ow through the system. There are two aspects
to consider here:

(cid:15) The storage locations (register name space and memory address space) de(cid:12)ned in the
user mode ISA

(cid:15) The operations or functions de(cid:12)ned in the user mode ISA.

The data path of a computer is built from building blocks such as registers, memory
elements, arithmetic-logic units, other functional units, and interconnections. Assumptions
about the behavior of these building blocks become speci(cid:12)cations of the data path, which
are then conveyed to the logic-level designer of the data path.

7.1.2 Register File and Individual Registers

A microarchitecture de(cid:12)nes one or more register (cid:12)les, each consisting of several physical
registers. The register (cid:12)les are used to implement the integer and (cid:13)oating-point register
name spaces de(cid:12)ned in the ISA. Figure 7.2 depicts a register (cid:12)le. It has an address input
for specifying the register to be read from or written into. Any speci(cid:12)c register in the
register (cid:12)le can be read or written by placing the corresponding register number or address

7.1. Overview of User Mode Microarchitecture

245

in the address input lines. The data that is read or written is transmitted through the
data lines. The data lines can be bidirectional as shown in the (cid:12)gure, or can be two sets of
unidirectional lines.

Register Number

R

Register File

R
2   Registers

N

Data

Figure 7.2: An N -bit wide Register File containing 2R Registers

Apart from register (cid:12)les, several individual hardware registers are also included in the
microarchitecture. Some of these registers can be used to implement other ISA-de(cid:12)ned
registers such as pc, sp, and flags. Individual registers and register (cid:12)les that implement
registers de(cid:12)ned in the ISA are often called ISA-visible registers.

Register (cid:12)les as well as individual registers that are not ISA-visible are invisible to the
machine language programmer, and are called microarchitectural registers. These serve as
temporary storage for values generated or used in the midst of instruction execution in
the microarchitecture. The data path may also need to keep track of special properties of
arithmetic and logical operations such as condition codes; most data paths use a register
called flags4 . Notice that the total number of registers provided in the microarchitecture
is higher than the number of registers de(cid:12)ned in the ISA.

7.1.3 Memory Structures

A microarchitecture de(cid:12)nes one or more memory structures, each containing very large
numbers of memory locations. These memory structures are useful for implementing the
memory address space and sections de(cid:12)ned in the ISA. Each memory structure resembles a
very large register (cid:12)le. Like a register (cid:12)le, each memory structure has an address input for
specifying the location to be read from or written into. Thus, any speci(cid:12)c memory location
can be read or written by placing the corresponding memory address in the address input
lines. The di(cid:11)erent storage elements | memory structure(s) as well as register (cid:12)le(s) |
are pictorially shown in Figure 7.3.

4 In some architectures such as the IA-32, the flags register is visible at the ISA level (usually as a set
of condition codes). In MIPS-I, it is not visible at the ISA level.

246

Chapter 7. Microarchitecture | User Mode

PC

Register File

Registers

Memory

Memory Locations

Figure 7.3: Implementing the Storage Locations De(cid:12)ned in the User Mode ISA

7.1.4 ALUs and Other Functional Units

We have just looked at the storage elements present in a microarchitecture. Next, we shall
consider the elements that serve to perform the operations and functionalities speci(cid:12)ed in
the ISA. In order to do this, let us review the functions the microarchitecture must perform
so as to execute instructions. Some of the important functionalities are listed below.

(cid:15) Data transfer between memory locations and registers.

(cid:15) Data transfer between registers

(cid:15) Arithmetic/logical operations on data present in registers

Next, let us consider equipping the data path to perform arithmetic and logical oper-
ations, such as addition, subtraction, AND, OR, and shift. For carrying out operations,
the microarchitecture de(cid:12)nes hardware circuitry that can perform di(cid:11)erent operations on
bit patterns, and produce result bit patterns in the speci(cid:12)c data encodings used. Many
microarchitectures consolidate the hardware circuitry for performing arithmetic and logical
operations on integer data into a single multi-function block called arithmetic and logic unit
(ALU). An ALU takes data inputs as well as control inputs, as depicted in Figure 7.4. The
two main sets of data inputs are marked as X and Y . The control inputs indicate the spe-
ci(cid:12)c arithmetic/logic function to be performed, and the data type of the inputs. An ALU

7.1. Overview of User Mode Microarchitecture

247

typically performs functions such as addition and subtraction (for signed integers as well as
unsigned integers), AND, OR, NAND, NOR, left shift, logical right shift, and arithmetic
right shift.

Hardware circuitry for performing complex operations such as integer multiplication,
integer division, and all types of (cid:13)oating-point operations are usually not integrated into
the ALU; instead, these are typically built as separate functional units. This is because
these operations typically require much longer times than that required for integer addition,
integer subtraction, and all logical operations.
Integrating all of these operations into a
single ALU forces every operation to be as slow as the slowest one.

Carry out
Overflow

X

N

Y

N

N−bit
ALU

N

A

k

F
Data Type

Figure 7.4: An N -bit ALU Capable of Performing 2k Functions

7.1.5

Interconnects

It is not su(cid:14)cient to implement the storage locations and the functional units in a microar-
chitecture. For meaningful operations to be carried out in a data path, it is important
that the storage elements and the functional units be connected properly. To achieve a
reasonable speed of operation, most of these connections transfer a full word in parallel
over multiple wires. The connections can be either buses or direct paths.

A bus connects together a large number of blocks using a single set of wires, and is a
shared communication link. Multiple blocks can communicate over a single bus, at di(cid:11)erent
times. At any particular instant, data can be transferred from a single block to one or more
blocks. Thus a bus permits broadcasting of data to multiple destination blocks. In addition
to the wires that carry the data, additional wires may be included in a bus for addressing
and control purposes. One point to note here is that a bus allows only a single transfer at
a time, and so, if a data path has only a small number of buses, then only very few data
transfers can happen in parallel in that data path.

A direct path, unlike a bus, is a point-to-point connection that connects exactly two
blocks. A direct path-based interconnect therefore consists of a collection of point-to-point
connections between di(cid:11)erent blocks. Direct paths tend to be faster than buses, due to two
reasons. First, their wires have only two connections, and this results in less capacitance

248

Chapter 7. Microarchitecture | User Mode

than that in buses, which have many connections. Secondly, direct paths tend to be much
shorter than buses, and so have lower transmission delays. Direct paths are particularly
suited for connecting hardware blocks that are physically close and communicate frequently.
The downside is that a direct path based data path tends to have too many paths, requiring
a large chip area for the wires.

The di(cid:11)erent blocks in a microarchitecture can be interconnected in a variety of ways.
The type of interconnects and the connectivity they provide determine, to a large extent,
the time it takes to execute a machine language instruction. A typical data path has too
many hardware blocks for every block to be connected to every other block by point-to-
point connections. What would be a good interconnect to use here? The simplest type of
connection that we can think of is to use a single bus to connect all of them, as illustrated
in Figure 7.5. In this (cid:12)gure, all of the storage elements (including the microarchitectural
registers) and the ALU are connected together by a single bus called system bus.

PC

Register File

ISA−visible Registers

s
u
B
 
m
e
t
s
y
S

Memory

Memory Addresses

IR

flags

Microarchitectural Registers

ALU

Functional Units

Figure 7.5: Interconnecting the ISA-visible Registers, Microarchitectural Registers, and the
ALU using a Single Bus to form a Computer Data Path

7.1. Overview of User Mode Microarchitecture

249

7.1.6 Processor and Memory Subsystems

In the discussion so far, the registers, the memory structures, and the functional units are
all interconnected with a single bus, called the system bus. The use of a single bus has
several limitations:

(cid:15) It permits only a single data transfer between the di(cid:11)erent units at any given instant.

(cid:15) Some of the transfers may be very fast (those involving only registers), whereas some
others (those involving memory) are very slow. The di(cid:11)erence in transfer rates neces-
sitates an asynchronous protocol for the system bus, which introduces an overhead to
the register-only transfers, which are more frequent.

(cid:15) The bus may have high capacitance due to two reasons: (i) it is very long so as to
connect all devices in the system, and (ii) each device that is hooked up to the bus
adds to the capacitance. To reduce the capacitance, each device must have expensive
low-impedance drivers to drive the bus.

In order to improve performance, we typically organize the user mode microarchitecture
as two subsystems: the processor subsystem and the memory subsystem.
This
is depicted in Figure 7.6. In such an organization, the registers and functional units are
integrated within the processor subsystem, and the memory structures are integrated within
the memory subsystem. Because the functional units work with values stored in registers,
they are typically included in the processor subsystem where the registers are present. The
memory subsystem is traditionally placed outside the processor chip 5 .
Another notable change is that we now have two buses|a processor bus and a system
bus. The processor bus is used to interconnect the blocks within the processor subsystem,
and the system bus is used to connect the two subsystems. The processor subsystem also
includes a memory interface to act as a tie between the two buses. The use of two buses
enables multiple transfers to happen in parallel. For instance, data could be transferred
from the register (cid:12)le to the ALU, while data transfer is taking place between the memory
subsystem and the memory interface.

7.1.7 Micro-Assembly Language (MAL)

As mentioned earlier, the data path does not do anything out of its own volition; it merely
carries out the elementary instruction that it receives from the control unit. Thus, it is
the control unit that decides what function should be carried out by the processor data
path in a particular clock cycle. Likewise, the function to be performed by the execution

5As we will see later in this chapter, the memory subsystem of a typical computer includes multiple
levels of cache memories, of which the top one or two levels are often integrated into the processor chip.
With continued advancements in VLSI technology, a number of researchers are even working towards the
integration of the processor and memory subsystems into a single chip.

250

Chapter 7. Microarchitecture | User Mode

System Bus

Memory
Interface

PC

IR

s
u
B
 
r
o
s
s
e
c
o
r
P

Register File

Memory

ALU

flags

Processor Subsystem

Memory
Subsystem

Figure 7.6: Organizing the User Mode Microarchitecture as a Combination of Processor
and Memory Subsystems

unit in a clock cycle is also decided by the control unit. For ease of understanding, we
shall use an assembly-like language to express these elementary instructions sent by the
control unit to the data path. This language uses mnemonics similar to those used in the
assembly languages studied in Chapters 3 and 4, and is called a micro-assembly language
or MAL. Accordingly, an elementary instruction represented in this language is called a
micro-assembly language command or a MAL command for short. A MAL command may
be composed of one or more elementary operations called MAL operations, performed on
data stored in registers or in memory. A MAL operation can be as simple as copying data
from one physical register to another, or more complex, such as adding the contents of two
physical registers and storing the result in a third physical register.

We shall deal with the speci(cid:12)cs of the data transfer when we look at RTL architectures
in Chapter ??.

7.2. Example Microarchitecture for Executing MIPS-0 Programs

251

7.2 Example Microarchitecture for Executing MIPS-0 Pro-
grams

Designing a computer microarchitecture is better caught than taught. Accordingly, in this
chapter we will design several example microarchitectures, and illustrate the principles
involved. Although a microarchitecture can be designed in a generic manner to support
a variety of ISAs, such an approach is rarely taken.
In practice, each microarchitecture
is designed with a speci(cid:12)c ISA in mind6 . The primary reason for this is performance. If
we design a generic microarchitecture to support the idiosyncrasies of a number of ISAs
(like the XXL size T-shirts distributed during class reunion ceremonies), then that data
path is likely to be signi(cid:12)cantly slower than one that caters to a speci(cid:12)c ISA. In accordance
with this practice, our microarchitecture designs are also for a speci(cid:12)c ISA. For the sake of
continuity with the preceding chapters, these microarchitectures are designed for executing
MIPS ISA instructions7 . For simplicity and ease of understanding, we restrict ourselves to
a representative subset of the MIPS-I instruction set which we call MIPS-0. We will (cid:12)rst
discuss a simple example microarchitecture in detail, and later move on to more complex
microarchitectures. This simple microarchitecture will be used in the discussions of the
control unit as well.

In order to design a microarchitecture for the MIPS-0 ISA, we (cid:12)rst consider the ISA-
de(cid:12)ned storage locations that are typically implemented within the processor subsystem.
These include the general-purpose registers and the special registers. The MIPS ISA de(cid:12)nes
32 general-purpose registers, R0 - R31, and we use a 32-entry register (cid:12)le to implement
them. In each clock cycle, this register (cid:12)le can accept a single address input for specifying
the register to be read from or written into. A special register called PC is used to store the
memory address of the next instruction to be interpreted and executed. Apart from these
ISA-visible registers, we shall include the following microarchitectural register: flags to
store important properties of the result produced by the ALU. The contents of the flags
register can be used to perform selective updates to PC.

We shall use a single multi-function ALU (Arithmetic and Logic Unit) to perform the
arithmetic and logical operations speci(cid:12)ed in the MIPS-0 ISA. More advanced designs may
use a collection of specialized functional units.

We shall design the data path as a combination of a processor subsystem and a memory
subsystem, as discussed in Section 7.1.6. The processor subsystem includes the registers,
the ALU, and the interface to the memory subsystem. We shall use a single 32-bit processor
bus to interconnect the hardware structures inside the processor subsystem, and another

6This is in contrast to the practice followed in designing an assembly-level architecture, where the design
is not tailored for any particular high-level language.
7The microarchitecture presented in this section is somewhat generic, and is not necessarily close to the
ones present in any of the commercial MIPS processors.
In Sections 7.6.2 and 7.7, we present microar-
chitectures that are tailored for the MIPS ISA, and are therefore closer to commercial MIPS processor
microarchitectures.

252

Chapter 7. Microarchitecture | User Mode

bus | the system bus | to connect the processor subsystem to the memory subsystem.

Figure 7.7 pictorially shows one possible way of interconnecting the main building blocks
so as to perform the required functions. This (cid:12)gure suppresses information on how the
control unit controls the functioning of the blocks; these details will be discussed later. The
use of a bus permits a full range of paths to be established between the blocks connected
by the bus.
It is quite possible that some of these paths may never have to be used.
The direct path based data path that we will see later in this chapter eliminates all such
unnecessary paths by providing independent connections only between those blocks that
need to communicate.

System Address Bus

PC

ID

To Control Unit

Register Address

32

Register File
(RF)

Register Data

s
u
B
 
r
o
s
s
e
c
o
r
P

Memory
Interface

Memory
Structure

Memory Subsystem

System Data Bus

ALU

Flags

Processor Subsystem

Figure 7.7: A Microarchitecture for Implementing the MIPS-0 User Mode ISA

Finally, the data path includes the memory subsystem, which implements the mem-
ory address space de(cid:12)ned in the user mode ISA. There is a memory interface unit for
interfacing the processor bus to the memory subsystem.

7.2. Example Microarchitecture for Executing MIPS-0 Programs

253

7.2.1 MAL Commands

The process of executing a machine language (ML) instruction on the data path can be
broken down into a sequence of more elementary steps. We saw in Section 7.1 how we can
use a micro-assembly language (MAL) to express these elementary steps. We can de(cid:12)ne a
set of MAL operations for the MIPS-0 data path de(cid:12)ned in Figure 7.7.

If the interconnections of the data path are rich enough to allow multiple MAL opera-
tions in the same time period, these can be denoted by writing them in the same line, as
follows:

Fetch instruction;

Increment PC

speci(cid:12)es that in the same step, the next instruction is fetched from memory, and the contents
of PC are incremented by 4.

Finally, for MAL operations that are conditional in nature, an \if " construct patterned
after the C language’s \if " construct is de(cid:12)ned.
if (src1 == src2)

Update PC

indicates that if the zero (cid:13)ag is equal to 1, the contents of AOR is copied to PC; otherwise no
action is taken.

7.2.2 MAL Operation Set

Table 7.1 gives a list of useful MAL operations for the microarchitecture of Figure 7.7. The
MAL operations done (in parallel) in a single step form a MAL command.

7.2.3 An Example MAL Routine

Now that we are familiar with the syntax of MAL as well as the useful MAL operations for
the microarchitecture of Figure 7.7, we shall look at a simple sequence of MAL operations
called a MAL routine. We shall write a MAL routine that adds the contents of registers
speci(cid:12)ed in the rs and rt (cid:12)elds of the instruction, and writes the result into the register
speci(cid:12)ed in the rd (cid:12)eld of the instruction. The astute reader may have noticed that executing
this MAL routine amounts to executing the MIPS-I machine language instruction and rd,
rs, rt in the microarchitecture of Figure 7.7, provided the binary pattern of this instruction
has been fetched and decoded.

In theory, we can write entire programs in MAL that can perform tasks such as ‘printing
\hello, world!" ’ and more; we will, of course, need a special storage for storing the MAL
programs and a mechanism for sequencing through the MAL program. Having seen the
di(cid:14)culty of writing programs in assembly language and machine language, one can easily
imagine the nightmare of writing entire programs in MAL! However, developing a MAL
program may not be as bad as it sounds, given that we can develop translator software such
as assemblers that take machine language programs and translate them to MAL programs.

254

Chapter 7. Microarchitecture | User Mode

No.
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14

Comments
MAL Command
Instruction = Memory[PC]
Instruction
Fetch
Determine opcode, src1, src2, dest, offset
Instruction
Decode
Result = R[rs] + R[rt]
src1, src2
Add
src1, offset Result = R[rs] + sign-extended offset
Add
Result = R[rs] AND R[rt]
And
src1, src2
src1, offset Result = R[rs] AND sign-extended offset
And
Address = R[rs] + sign-extended offset
MemAddress
Compute
BranchTarget Target = PC + 4 + 4 (cid:2) sign-extended offset
Compute
Target = PC + 4 + 4 (cid:2) sign-extended offset
Compute
JumpTarget
R[rt]   Memory[Address]
Load
Memory[Address]   R[rt]
Store
Write result to register dest
Write
R[31]   PC
Save
PC   PC + 4
Increment
PC   Target
Update

PC
PC
PC

Table 7.1: A List of Useful MAL Operations for the Microarchitecture of Figure 7.7

Step MAL Command
Comments
registers Read registers rs and rt
0
Read
AND the operand values
1
Compute
result
2
Write result into destination register (rd) if it is non-zero
Write

Table 7.2: An Example MAL Routine for Executing an Instruction in the Microarchitecture
of Figure 7.7

The real di(cid:14)culty is that MAL programs will be substantially bigger than the corresponding
ML programs, thereby requiring very large amounts of storage. This is where interpretation
comes in. By generating the required MAL routines on-the-(cid:13)y at run-time, the storage
requirements are drastically reduced, as we will see next.

7.3

Interpreting ML Programs by MAL Routines

Having discussed the basics of de(cid:12)ning MAL routines, our next step is to investigate how
machine language programs can be interpreted by a sequence of MAL commands that are
de(cid:12)ned for a particular data path. To that end, we will take individual MIPS-0 instructions
and how MAL commands can be put together to carry out its execution in the data path.
Before developing the MAL command routines, let us brie(cid:13)y review the functions the data
path must perform so as to execute instructions one by one. As seen in Section 6.5, the

7.3.

Interpreting ML Programs by MAL Routines

255

interpretation process involves two ma jor functions:

(cid:15) Fetching the instruction: This concerns fetching the next instruction in the pro-
gram.

(cid:15) Executing the instruction: This concerns executing the fetched instruction.

These actions are typically done as a fetch-execute sequence. We also saw that these two
functions can be further divided into a sequence of steps (some of which are optional), as
indicated below.

Executing the next instruction

(cid:15) Fetch Phase

1. Fetch instruction: Read the next instruction from the main memory into
the processor data path.

2. Decode instruction: Decode the instruction bit pattern so as to determine
the action speci(cid:12)ed by it.

(cid:15) Execute Phase

1. Fetch source operand values: Fetch source operands (if any) from regis-
ters, main memory, or IO registers.

2. Process data: Perform arithmetic or logical operation on source operand
values, if required.

3. Write result operand value: Write the result of an arithmetic or logical
operation to a register, memory location, or IO register, if required.

4. Update instruction address: Determine the memory address of the next
instruction to be interpreted.

These 6 steps serve as directives for the data path in its attempt to carry out the
execution of each instruction. In order to execute an entire ML program, which is just a
sequence of instructions, the interpretation should also implement the sequencing among
instructions. That is, after the completion of the above 6 steps for a single instruction,
it should go back to the fetch phase to begin executing the next instruction in the ML
program. Thus, the fetch-execute sequence becomes a fetch-execute cycle.

All of these steps can be accomplished by a sequence of MAL commands called a MAL
routine. A MAL routine is allowed to freely use and modify any of the ISA-invisible
microarchitectural registers. The ISA-visible registers, however, can be modi(cid:12)ed only as
per the semantics of the instruction being interpreted.

256

Chapter 7. Microarchitecture | User Mode

7.3.1

Interpreting an Instruction | the Fetch Phase

When implementing ISAs with (cid:12)xed length instructions, the actions required to perform
the fetch phase of the interpretation are the same for all instructions. The MIPS-0 ISA is no
exception. We shall consider this phase of instruction interpretation (cid:12)rst. In the data path
of Figure 7.7, register PC keeps the memory address of the next instruction to be fetched.
To fetch this instruction from memory, we have to utilize the memory interface provided in
the data path. Table 7.3 gives a MAL routine that implements the fetch phase of executing
the instruction. This routine also includes the MAL command for incrementing PC after the
fetch phase so as to point to the sequentially following instruction, which is normally the
instruction executed after the execution of the current instruction. If the current instruction
is a branch instruction, then the PC value may be modi(cid:12)ed in the execute phase.

Step

0
1

2

Comments
MAL Command
Fetch and Decode
instruction
instruction Determine opcode, src1, src2, dest
PC increment
Increment PC by 4

PC

Fetch
Decode

Increment

Table 7.3: A MAL Routine for the Fetching a MIPS-0 ISA Instruction in the Microarchi-
tecture of Figure 7.7

The (cid:12)rst step involves getting the binary pattern corresponding to the instruction from
the memory8 . In this MAL routine, we consider the time taken to perform this MAL com-
mand as one unit; the exact number of cycles taken depends on the speci(cid:12)cs of the memory
system used and the RTL sequence used.
In step 1, the fetched instruction bit pattern
is decoded by the instruction decoding circuitry. The exact manner in which instruction
decoding is performed is not speci(cid:12)ed here; this will be relevant only to the lower level
of design. The opcode and funct (cid:12)elds of the fetched instruction uniquely identify the
instruction. Steps 0-1 thus constitute the MAL routine for the fetch phase of instruction
interpretation. Naturally, this portion is the same for every instruction in the MIPS ISA
because all instructions have the same size. Had the MIPS ISA used variable length in-
structions, this fetch process would have to be repeated as many times as the number of
words in the instruction.

After interpreting an instruction, the interpreter needs to go back to step 0 to interpret
the next instruction in the ML program. However, prior to that, it has to increment PC to
point to the next instruction; otherwise the same instruction gets interpreted repeatedly.
In this MAL routine, PC update is done immediately after completing the instruction fetch.

8We have used a single MAL command, namely Fetch instruction, to tell the microarchitecture to
fetch an instruction. At the register transfer level (RTL), this MAL command may be implemented by a
sequence of RTL instructions.

7.3.

Interpreting ML Programs by MAL Routines

257

Thus, in step 2, PC is incremented by 4 to point to the next instruction in the executed
machine language program. After step 2, the interpreter goes to the execute phase.

This MAL routine has 3 steps. We can, in fact, perform the Increment PC function in
parallel to the instruction fetch or decode functions, and reduce the total number of steps
required for the interpretation. Table 7.4 provides the modi(cid:12)ed MAL routine, which requires
only 2 steps. In this routine, in step 0, PC is incremented at the same time the instruction
is fetched from memory. It is important to note that to do multiple MAL operations in
parallel, the microarchitecture needs to have appropriate connectivity. In general, the more
the connectivity provided in a data path, the more the opportunities for performing multiple
MAL operations in parallel.

Step

0
1

Fetch
Decode

MAL Command
Fetch, Decode, and PC increment
instruction;
PC
Increment
instruction

Comments

Table 7.4: An Optimized MAL Routine for Fetching a MIPS-0 ISA Instruction in the
Microarchitecture of Figure 7.7

7.3.2

Interpreting Arithmetic/Logical Instructions

We just saw a MAL routine for the fetch phase of instruction interpretation. Next, let
us consider the execute phase of instructions. Unlike the actions in the fetch phase, the
actions in the execute phase are not identical for di(cid:11)erent instructions. Therefore, the MAL
routines for the execute phase are di(cid:11)erent for the di(cid:11)erent instructions. We shall con-
sider one instruction each from the four types of instructions: (i) data transfer instruction,
(ii) arithmetic/logical instruction, (iii) control (cid:13)ow changing instruction, and (iv) syscall
instruction.

Let us start by considering an arithmetic instruction. We shall put together a sequence
of MAL commands to interpret an arithmetic/logical instruction.

Example: Consider the MIPS ADDU instruction whose symbolic representation is addu
rd, rs, rt. Its encoding is given below.

000000

rs

rt

rd

ADD

The (cid:12)elds rs, rt, and rd specify register numbers; the (cid:12)rst two of these contain the data
values to be added together as unsigned intergers. The rd (cid:12)eld indicates the destination
register, i.e., the register to which the result should be written to, as long as it is not register
$0. In this data path, the addition of the two register values can be done in the ALU.

258

Chapter 7. Microarchitecture | User Mode

Table 7.5 speci(cid:12)es a sequence of MAL commands for carrying out the execute phase of
this instruction in the data path of Figure 7.7. Let us go through the working of this MAL
routine. We name the (cid:12)rst MAL command of the routine as step 2, as the execute phase is
a continuation of the fetch phase, which ended at step 2.

Step MAL Command Comments
Execute phase
src1, src2 Read operands R[rs] and R[rt]
Perform arithmetic operation
Write result to register rd, if rd is non-zero

Read
Add
Write

2
2
3

Table 7.5: A MAL Routine for Executing the MIPS-0 Instruction Represented Symbolically
as addu rd, rs, rt. This MAL Routine is for executing the instruction in the Data Path
of Figure 7.7

Step 2 begins the execution phase. First, the register operands must be fetched from the
register (cid:12)le. In step 2, the operand values present in general-purpose registers rs and rt
are read, and are added together in the ALU. In step 3, the result of the addition is written
to the destination register (rd), if rd is non-zero. By performing this sequence of MAL
commands in the correct order, the instruction addu rd, rs, rt is correctly interpreted.

7.3.3

Interpreting Memory-Referencing Instructions

Let us next put together a sequence of MAL commands to fetch and execute a memory-
referencing instruction. Because all MIPS instructions are of the same length, the MAL
routine for the fetch part of the instruction is the same as before; the di(cid:11)erences are only in
the execution part. Consider the MIPS load instruction whose symbolic representation is
lw rt, offset(rs). The semantics of this instruction are to copy to GPR rt the contents
of memory location whose address is given by the sum of the contents of GPR rs and sign-
extended offset. We need to come up with a sequence of MAL commands that e(cid:11)ectively
fetch and execute this instruction in the data path of Figure 7.7.

100011

rs

rt

offset

The interpretation of a memory-referencing instruction requires the computation of an
address. For the MIPS ISA, address calculation involves sign-extending the offset (cid:12)eld of
the instruction to form a 32-bit signed o(cid:11)set, and adding it to the contents of the register
speci(cid:12)ed in the rs (cid:12)eld of the instruction. In this data path, the address calculation is done
using the same ALU, as no separate adder has been provided. With this introduction, let
us look at the MAL routine given in Table 7.6 to interpret this lw instruction.

In step 2, the memory address is computed by reading the contents of register speci(cid:12)ed

7.3.

Interpreting ML Programs by MAL Routines

259

Step

MAL Command

Comments
Execute phase
MemAddress Memory address = R[rs] + sign-extended offset
Load from memory into register rt

2
3

Compute
Load

Table 7.6: A MAL Routine for the Execute Phase of the Interpretation of the MIPS-0 ISA
Instruction Represented Symbolically as lw rt, offset(rs). This MAL Routine is for
executing the instruction in the Data Path of Figure 7.7

in the rs (cid:12)eld and adding the sign-extended offset value to it. In step 3, the contents of
the memory location at the computed address is loaded into register rt. Recall that the
memory transfer takes place through the memory interface.

7.3.4

Interpreting Control-Changing Instructions

The instructions that we interpreted so far | addu and lw | do not involve control (cid:13)ow
changes that cause deviations from straightline sequencing in the ML program. Next let us
see how we can interpret control-changing instructions, which involve modifying PC, usually
based on a condition.

Example: Consider the MIPS-0 conditional branch instruction whose symbolic representa-
tion is beq rs, rt, offset. The semantics of this instruction state that if the contents
of GPRs rs and rt are equal, then the value offset (cid:2) 4 + 4 should be added to PC so as
to cause a control (cid:13)ow change9 ; otherwise, PC is incremented by 4 as usual. The encoding
of this instruction is given below:

000100

rs

rt

offset

Step

2
3
4

MAL Instruction
Comments
Execute phase
BranchTarget
src1, src2
IfEqual

Compute
Compare
Update

Compare operands R[rs] and R[rt]
Update PC if operands are equal

Table 7.7: A MAL Routine for Executing the MIPS-0 ISA Instruction Represented Sym-
bolically as beq rs, rt, offset. This MAL Routine is for executing the instruction in
the Data Path of Figure 7.7

9The actual MIPS ISA uses a delayed branch scheme; i.e., the control (cid:13)ow change happens only after
executing the instruction that follows the branch instruction in the program. We avoid delayed branches to
keep the discussion simple.

260

Chapter 7. Microarchitecture | User Mode

Table 7.7 presents a MAL routine to execute this instruction in the data path given in
Figure 7.7. The execute routine has 3 steps numbered 2-4. The (cid:12)rst step of the routine
(step 2) calculates the target address of the branch instruction by adding 4 times the sign-
extended offset value to the incremented PC value. In step 3, the contents of registers rs
and rt are compared, and the result of the comparison is stored in the flag register. In
the last step, PC is updated with the calculated target address, if the two operands were
equal. Thus, if the operands turned out to be not equal, then PC retains the incremented
value it obtained in step 1, which is the address of the instruction following the branch in
the machine language program being interpreted.

7.3.5

Interpreting Trap Instructions

We have seen the execute phase for all of the instruction types other than the syscall
instructions. Let us next look at how the data path performs syscall execution, which
is somewhat di(cid:11)erent from the previously seen ones. Like control-changing instructions,
syscall instructions also involve modifying the contents of pc. For the MIPS-I ISA, the
pc is updated to 0x80000080. In addition, the machine is placed in the kernel mode. We
shall take a detailed look at the corresponding MAL sequence in Section 8.1.1, along with
other kernel mode implementation issues.

7.4 Memory System Organization

The memory system is an integral component of the microarchitecture of a stored program
computer, as it stores the instructions and data of the program being executed. In this
section we look at the microarchitecture of this subsystem more closely. This section begins
by illustrating the need to organize the physical memory system as a hierarchy, in order to
achieve high speed without incurring high cost. Then it describes how di(cid:11)erent components
of the memory hierarchy work. The next section provides an in-depth treatment of cache
memories, and demonstrates how they help to increase the apparent speed of the memory
system.

\In a hierarchy every employee tends to rise to his level of incompetence."
| Dr. Laurence Peter, 1919-90, in The Peter Principle, 1969

7.4.1 Memory Hierarchy: Achieving Low Latency and Cost

The discussion we had so far may seem to indicate that the main memory is implemented
as a single structure in a computer microarchitecture. That is, a single memory structure
implements the entire address space, provides fast access, and is inexpensive. Unfortunately,
this ideal situation is feasible only in some small systems that have a small address space
and work with a slow processor. For general-purpose computers, it is impossible to meet

7.4. Memory System Organization

261

all three of these requirements simultaneously with today’s semiconductor technology. As
might be expected, there is a trade-o(cid:11) among these three key characteristics, because of the
following relationships:

(cid:15) larger a memory structure, slower its operation.

(cid:15) larger a memory structure, greater its cost

(cid:15) faster a memory structure, greater its cost

\Everyone who comes in here wants three things:
(1) They want it quick.
(2) They want it good.
(3) They want it cheap.
I tel l ’em to pick two and cal l me back."
| A sign on the back wal l of a smal l printing company

When we implement the entire address space using a single memory structure, a memory
access cannot be completed in 1 cycle, contrary to what we had in mind when writing
the MAL routines.
Instead, a memory access will take anywhere from 50-100 processor
clock cycles, because the memory structure is quite large and also requires o(cid:11)-chip access.
Furthermore, because the memory address space de(cid:12)ned in ISAs is often quite large, it may
not be cost-e(cid:11)ective to implement the entire address space using semiconductor memory.

Unlike the predicament of the printing company in the above quote, we do have a solution
for the memory subsystem to deal with the problems introduced by the three relationships
given above. To motivate this solution, let us consider a simple analogy that illustrates the
key principles and mechanisms well. Many of you may be using an Address Book that stores
information about frequently used telephone numbers or email addresses, in order to reduce
the time spent in going through a large telephone directory. Once this Address Book has
been initialized, chances are high that most of the time you will get the required telephone
number or address directly from this list, without going through the telephone directory.
Keeping the frequently accessed information as a separate list results in signi(cid:12)cant time
savings, compared to going through the telephone directory each time.

The same principle can be used to create the illusion of a large memory that can be
accessed as quickly as a small memory. Just like we do not access every number in the
telephone directory with equal probability at any given time, a program does not access all
of its program or data locations with equal probability at any given time. The principle of
locality underlies both the way in which we deal with telephone numbers and the way that
programs operate. This principle indicates that programs access a relatively small portion
of their address space at any instant of time. There are two di(cid:11)erent types of locality:

(cid:15) Temporal locality (locality in time): If an item is referenced now, it is likely to be
accessed again in the near future.

262

Chapter 7. Microarchitecture | User Mode

(cid:15) Spatial locality (locality in space): If an item is referenced now, items whose addresses
are close by are likely to be accessed in the near future.

Just as accesses to telephone numbers exhibit locality, locality in programs arises from sim-
ple and natural program structures. For example, most programs contain loops. Therefore,
instructions and data are likely to be accessed repeatedly, resulting in high amounts of tem-
poral locality. Analyses of program executions show that about 90% of the execution time
is generally spent on 10% of the code. This may be due to simple loops, nested loops, or
a few subroutines that repeatedly call each other. Also, because instructions are normally
accessed sequentially, instruction accesses show high spatial locality. Accesses to data also
exhibit a natural spatial locality. For example, accesses to elements of an array or a record
will naturally have high degrees of spatial locality.

Much work has gone into developing clever structures that improve the apparent speed
of the memory, without terribly increasing the cost, by taking advantage of the principle
of locality. The solution that has been widely adopted is not to rely on a single memory
component or technology, but to use a memory hierarchy. A memory hierarchy consists of
multiple levels of memory with di(cid:11)erent speeds and sizes, as illustrated in Figure 7.8. The
fastest memories are more expensive per bit than the slower ones, and are usually smaller.
The goal is to achieve a performance close to that of the fastest memory, and a cost per bit
close to that of the least expensive memory.

Speed

Cost per Bit

CPU

Memory

Memory

Memory

Memory

Figure 7.8: The Basic Structure of a Typical Memory Hierarchy

Size

A memory hierarchy can consist of multiple levels, but data is copied only between two
adjacent levels at a time. The fastest elements are the ones closest to the processor. The top
part of the hierarchy will supply data most of the time because of the principle of locality.
Memory hierarchies take advantage of temporal locality by keeping recently accessed data

7.4. Memory System Organization

263

items closer to the processor. They take advantage of spatial locality by moving blocks
consisting of multiple contiguous words to upper levels of the hierarchy, even though only
a single word was requested by the processor. If the fraction of times a memory request is
satis(cid:12)ed at the upper level is high, the memory hierarchy has an e(cid:11)ective access time close
to that of the highest (and fastest) level and an e(cid:11)ective size equal to that of the lowest
(and largest) level.

7.4.2 Cache Memory: Basic Organization

Over the years, processor speeds have been increasing at an astronomical rate. Main mem-
ory speeds, however, have not been increasing at a comparable rate. Today, it takes any-
where from 50-200 processor cycles to access a word from main memory. A common solution
adopted to bridge this gap in speeds is to insert a relatively small, high-speed memory, called
cache memory, between the processor and the main memory. The cache memory keeps a
copy of the frequently accessed memory locations. When the processor issues a request to
access a memory location, the cache memory responds if it contains the requested word. If
it does not contain the requested word, the request is passed on to main memory.

Conceptually, the operation of cache memory is very simple. The temporal aspect of
the locality of reference suggests that whenever the contents of a location is needed for the
(cid:12)rst time, this item should be brought into the cache so that the cache can supply the value
the next time the same location is accessed. The spatial locality suggests that instead of
bringing just one item from the main memory to the cache memory, it is wise to bring
several nearby items as well. We will use the term block to refer to a set of contiguous
addresses. Another term that is often used to refer to a block is line.

Consider the simple arrangement shown in the previous (cid:12)gure. When the processor
issues a Read request to a memory location for the (cid:12)rst time, the contents of the speci(cid:12)ed
location and the surrounding block of locations are transferred to the cache, one word after
another. Subsequently, when the program references any of the words in this block, the
requested word is read directly from the cache, and the request is not forwarded to main
memory.

Notice that cache memory is a microarchitectural feature to enhance performance, and
is not part of most instruction set architectures (ISAs). Thus, the machine language pro-
grammer does not need to know the existence of the cache. The processor simply issues
Read and Write requests using main memory addresses. The cache control circuitry deter-
mines if the requested word currently exists in the cache. If the cache contains the word, a
cache hit is said to have occurred. If it does not, then a cache miss is said to have occurred.

264

Chapter 7. Microarchitecture | User Mode

7.4.3 MIPS-0 Data Path with Cache Memories

7.4.4 Cache Performance

A basic question concerning a cache memory system is how to quantify its performance.
The (cid:12)gure of merit of interest to us is the average memory access time, considering the
cache memory and the main memory together as a single system. A simple formula for this
metric is given by:

TEF F = TCA + m (cid:2) TM iss
where TCA is the access time of the cache, TM iss is the time taken to service a cache miss,
and m is the miss ratio (i.e., the fraction of accesses that resulted in a cache miss).

Example: In order to improve the memory access time from 50 cycles, a system designer
decided to incorporate a cache memory, which has an access time of 4 cycles. The cache
was able to supply data for 80% of the memory references. Has the cache memory improved
the memory system’s performance or decreased it?

When the system did not use cache memory, each memory access took 50 cycles. When
a cache memory is introduced, the cache miss ratio is 20%. Therefore, the average memory
access time becomes 4 + 0.2 (cid:2) 50 = 14 cycles. Thus, in this case, the introduction of the
cache memory improves the average access time from 50 cycles to 14 cycles. If the cache
memory has a higher hit ratio and/or a lower access time, then the improvement will be
even more marked.

7.4.5 Address Mapping Functions

The cache memory is obviously much smaller than the main memory, which means that the
maximum number of blocks it can store at any time is less than the total number of blocks
present in the main memory. This has two ma jor implications:

(cid:15) We cannot use the straightforward linear addressing methodology for accessing the
cache memory. For instance, if we want to access memory address 1 billion, we cannot
just go to the 1 billionth location in the cache. Thus, there has to be some mapping
between the main memory blocks and the physical frames of the cache. In an Address
Book, for instance, this mapping is typically based on the (cid:12)rst letter of the person’s
name, as illustrated in Figure 7.9; the information pertaining to a person whose name
starts with \A" is entered in the page corresponding to letter \A". Below, we will
look at address mapping schemes that are typically used in cache memories.

(cid:15) Multiple main memory blocks can map to the same cache block frame, at di(cid:11)erent
times. This implies that there must be some mechanism to identify the main memory
block that is currently mapped to each cache clock frame. Again, in Figure 7.9, we
can see that two di(cid:11)erent names are mapped to the page corresponding to letter \A".

7.4. Memory System Organization

265

The name currently mapped to the page is identi(cid:12)ed by explicitly storing the name
| which serves as a tag | along with the address and phone number | which form
the data.

Tag

Tag

Adam First

1234 Garden Pkwy
Eden Gardens
123−456−6789

A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
P
Q
R
S
T
U
V
W
X
Y
Z

Data

A l i c e   L i d d e l l

2 2 1 B   R a b b i t ’ s   H o l e
W o n d e r l a n d
9 8 7 − 6 5 4 − 4 2 3 1

A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
P
Q
R
S
T
U
V
W
X
Y
Z

Data

Figure 7.9: An Alphabetically Organized Address Book Containing Two Di(cid:11)erent Name
Entries

Identi(cid:12)cation of the memory block that is currently mapped to a cache block frame is
typically achieved by including a Tag (cid:12)eld in each cache block frame. When a new memory
block is copied to a cache block frame, the Tag (cid:12)eld is updated appropriately. A Valid bit
is also included for indicating if the cache block frame currently has a valid main memory
block.

The correspondence between the main memory blocks and those present in the cache
is speci(cid:12)ed by a mapping function. When the cache is full and the processor accesses a
word that is not present in the cache, the cache control hardware must decide which block
should be removed to create space for the new block that contains the referenced word. The
collection of rules for making this decision constitutes the replacement algorithm.

To discuss possible methods for specifying where main memory blocks are placed in the
cache, we use a speci(cid:12)c small example. Consider a cache consisting of 8 blocks of 32 words
each, for a total of 256 words, and assume that the main memory is addressable by a 10-bit
address. The main memory has 1M words, which can be viewed as 32K blocks of 32 words
each.

The simplest way to map main memory blocks to cache block frames is the fully asso-
ciative mapping technique. In this mapping, a main memory block can be placed in any

266

Chapter 7. Microarchitecture | User Mode

cache block frame, as shown in the (cid:12)rst part of Figure 7.10. Therefore, a new memory block
will replace an existing block only if the cache is full. However, the cost of an associative
cache is high because of the need to search all cache block frames to determine if a given
main memory block is in the cache. A search of this kind is called an associative search.
For performance reasons, the tags must be searched in parallel.

Cache Block Frame No.

Tag

Cache Memory
Valid
Data

0
1
2
3
4
5
6
7
fully associative: MM block 12 can go anywhere in cache

0
1
2
3
4
5
6
7
direct mapped: MM block 12 can go only to one cache block frame

0
1
2
3
4
5
6
7

Set 0

Set 1

Set 2

Set 3

2−way set associative: MM block 12 can go anywhere in one set

Main Memory

MM Block No.
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

32K − 3
32K − 2
32K − 1

Figure 7.10: Possible Mappings between Main Memory and Cache Memory

An alternative approach is the direct mapping technique, depicted in the second part
of Figure 7.10.
In this technique, a particular main memory block maps to exactly one
cache block frame. The commonly used hash function is such that main memory block i
maps onto cache block frame i mod Num Cache Blocks. Thus, it is easy to determine if
a particular main memory block is present in the cache or not. However, because a main
memory block can be placed only in one particular cache block frame, contention may arise
for that position even if the cache is not full or if another cache block frame has an unused
main memory block.

\He who always puts things in place, is too lazy to look for them."
| Anonymous

7.4. Memory System Organization

267

Set associative mapping is a compromise that captures the advantages of both the
fully associative and the direct mapping approaches. Cache blocks are grouped into sets,
and the mapping allows a main memory block to reside in any block of a speci(cid:12)c cache set.
Hence, the contention problem of the direct mapping method is reduced by having some
choices for block replacement. At the same time, the hardware cost of fully associative
mapping is reduced by decreasing the size of the associative search.

7.4.6 Finding a Word in the Cache

How does the cache memory (cid:12)nd a memory word if it is contained in the cache? Given
a memory address, the cache (cid:12)rst (cid:12)nds the memory block number to which the address
belongs. This can be determined by dividing the memory address by the block size. Because
it is time-consuming to do an integer division operation, cache designers only use block sizes
that are powers of 2. The division, then, becomes taking out the upper bits of the memory
address. Figure 7.11 shows how a memory address bit pattern is split. The (cid:12)rst split
is between the Main Memory Block Number and the Offset within Block. The Main
Memory Block Number (cid:12)eld is further split into the Tag (cid:12)eld and the Cache Set Number
(cid:12)eld. The Offset within Block (cid:12)eld selects the desired word once the block is found, the
Cache Set Number (cid:12)eld selects the cache set, and the Tag (cid:12)eld is compared against the tag
values in the selected set to check if a hit occurs.

Main Memory Address

log

2

(no. Addrs in Main Memory)

Main Memory Block Number

Offset within Block

log

2

(no. Addrs in Block)

Tag

Cache Set Number

Offset within Block

log   (no. sets)
2

Figure 7.11: Splitting a Memory Address Bit Pattern for Accessing the Cache Memory

268

Chapter 7. Microarchitecture | User Mode

For a given total cache size, increasing the associativity reduces the number of cache
sets, thereby decreasing the size of the Cache Set Number (cid:12)eld and increasing the size of
the Tag (cid:12)eld. That is, the boundary between the Tag and Cache Set Number (cid:12)elds moves
to the right with increasing associativity, with the limit case of fully associative caches
having no Cache Set Number (cid:12)eld (because the entire cache is just one set).

Example: Consider a computer system that has a 4-way set-associative cache. The cache
receives 16-bit addresses, and splits it as follows: 5 bits for (cid:12)nding the Offset within
Block; 8 bits for (cid:12)nding the Cache Set Number.

1. How many sets are present in the cache memory?
Because 8 bits are decoded to determine the Cache Set Number, the cache has 2 8 =
256 sets.

2. How many words are present in a block?
Because 5 bits are decoded to determine the Offset within Block, each block has
25 = 32 words.

3. How many block frames are present in the cache memory?
The number of block frames in a cache memory is given by the product of the number
of sets it has and the number of blocks in each set. Therefore, this cache has 256 (cid:2) 4
= 1024 blocks.

4. How many words are present in the cache memory?
The number of words in a cache memory is given by the product of the number of
block frames it has and the number of words in each block. Therefore, this cache has
1024 (cid:2) 32 = 32K words.

5. What is the cache set number corresponding to hexadecimal address FBFC?
The hexadecimal address FBFC corresponds to bit pattern 1111101111111100. The
Cache Set Number is given by bits 12-5, which is the 8-bit pattern 11011111.
In
hexadecimal number system, this bit pattern is represented as DF.

7.4.7 Block Replacement Policy

When a new memory block is to be brought into a cache set, and all the block frames that
it may occupy are occupied by other memory blocks, the cache controller must decide which
of the old blocks should be replaced. In a direct mapped cache, there is exactly one cache
block frame where a particular main memory block can be placed; hence there is no need for
a replacement strategy. In fully associative and set-associative caches, on the other hand,
there is a choice in determining the block to be replaced. A good replacement strategy is
important for the cache memory to perform well. In general, the ob jective is to keep in the
cache those blocks that are likely to be re-referenced in the immediate future. However, it
is not easy to determine which blocks will be re-referenced in the future.

7.5. Processor-Memory Bus

269

\The best thing about the future is that it comes to us only one day at a time."
| Abraham Lincoln

The property of locality of references suggests that blocks that have been referenced
recently are very likely to be re-referenced in the near future. This is illustrated in the
following pattern of accesses to three memory addresses fX , Y , Z g which belong to three
di(cid:11)erent blocks that map to the same cache block frame.
XXX Y Y Y Y Y Y XXXX Z Z Z Z

In this pattern, except for the few times a transition happens from one address to
another, the memory block being referenced is the same as the one referenced during the
last access. Therefore, when a block is to be replaced in a set, it is sensible to replace the one
that has not been referenced for the longest time in that set. This block is called the least
recently used block (LRU), and this replacement policy is called the LRU replacement
policy.

The LRU replacement policy works well for most scenarios. It works poorly, however,
for some scenarios. Consider, for instance, a 2-way set associative cache in which accesses
to addresses fX , Y , Z g follow a di(cid:11)erent pattern:
X Y ZX Y ZX Y ZX Y ZX Y ZX Y

In this case, whenever an access to Z causes a miss, X ’s block is the LRU block. Inter-
estingly, this is the block to be retained in the cache, for the next access to be a hit. The
LRU replacement policy is a poor choice for this kind of access pattern, although it is a
repeating pattern.

7.4.8 Multi-Level Cache Memories

7.5 Processor-Memory Bus

The next topic that we look at is the interconnection between the processor and the memory
system, which includes the cache memories. For a variety of reasons, this connection is often
done via a bus called the processor-memory bus. (In Intel x86-based computers, this bus is
called the Front Side Bus or FSB.) A bus, as we saw earlier, is a shared communication
link, connecting multiple devices using a single set of wires. In addition to wires, a bus also
includes hardware circuitry for controlling access to the bus and the transfer of information
over the wires, as per speci(cid:12)c protocols. The processor-memory bus consists of 3 sets of
wires: address, data, and control. The address lines carry the address of the memory location
to be accessed. The data lines carry data information between the source and destination.
The control lines are used for transmitting signals that control data transfer as per speci(cid:12)c
protocols. They specify the times at which the processor and the memory interfaces may
place data on the bus or receive data from the bus.

In a desktop environment, the processor-memory bus is almost always contained within
the motherboard; several sockets are provided along the bus for inserting the memory

270

Chapter 7. Microarchitecture | User Mode

modules. The bus speed is generally matched to the memory system so as to maximize
processor-memory bandwidth and minimize memory latency. Processor-memory buses are
often ISA-speci(cid:12)c, because their structure is closely tied to the ISA.

Bus design and operation are su(cid:14)ciently complex sub jects that entire books have been
written about them. The ma jor issues in bus design are bus width, bus clocking, bus
arbitration, and bus operations. Each of these issues has a substantial impact on the speed
and bandwidth of the bus. We will brie(cid:13)y examine each of these.

7.5.1 Bus Width

Bus width is an important microarchitectural parameter. The wider the bus, the more
its potential bandwidth. The problem, however, is that wide buses require more wires
than narrow ones. They also take up more physical space (e.g., on the motherboard)
and need bigger connectors. All of these factors make the wider bus more expensive. To
cut costs many system designers tend to be shortsighted, with unfortunate consequences
later. There are two ways to increase the data bandwidth of a bus: decrease the bus cycle
time (more transfers/sec) or increase the data bus width (more bits/transfer). Speeding
up the bus is possible, but di(cid:14)cult because the signals on di(cid:11)erent lines travel at slightly
di(cid:11)erent speeds, a problem known as bus skew. The faster the bus, the more serious bus
skew becomes. Another problem with speeding up the bus is that doing this will not be
backward-compatible. Old boards designed for the slower bus will not work with the new
one. Therefore the usual approach to improving bus performance is to add more data lines.

To get around the problem of very wide buses, sometimes bus designers opt for a mul-
tiplexed bus. In this design, instead of having separate address and data lines, a common
set of lines is used for both address and data. At the start of a bus operation, the lines
are used for the address. Later on, they are used for data. Multiplexing the lines in this
manner reduces bus width (and cost), but results in a slightly slower system. Bus designers
have to carefully weigh all these options when making choices.

7.5.2 Bus Operations

The simplest bus operation involves a master and a slave. The master, which in this case
can be the processor or the cache controller, initiates a read/write request by placing the
address of the location in the address lines of the bus. For a write operation, in addition the
data to be written is placed on the data lines. The master also activates the control lines
to indicate if it is a read or write operation. When the slave becomes ready, it performs the
read/write to the location speci(cid:12)ed in the address lines.

Block Transfers or Burst Transfers: Normally, one word is transferred in one trans-
fer. The throughput of such a bus is limited by the protocol overhead associated with
each transaction. Each transfer requires arbitration for bus mastership, transmission of an

7.5. Processor-Memory Bus

271

address, and slave acknowledgement, in addition to the transfer of the actual data. This
type of transfer is particularly wasteful for transfering contiguous blocks of data, such as
what happens during cache re(cid:12)ll for servicing cache misses. In such situations, only the
starting address and word count need be speci(cid:12)ed by the master.
In order to improve
the throughput for such situations, processor-memory buses generally support block trans-
fer transactions that e(cid:11)ectively amortize the transaction overhead over larger data blocks.
When a block read operation is initiated, the bus master tells the slave how many words are
to be transferred, for example by initially placing the word count on the data lines. Instead
of returning a single word, the slave returns one word at a time until the count has been
exhausted.

Split Transaction:
In the bus operations discussed so far, the bus is tied up for the entire
duration of a data transfer, that is, from the time the request is initiated until the time the
transfer is accomplished. No other transfers can be done or even initiated during this period.
If a memory module has a 50-cycle access time, for example, a read transaction might take
a total of about 55 cycles|the ma jority of which constitute idle time. To improve the bus
e(cid:14)ciency under such circumstances, certain buses split conventional transfer operations into
two parts: a request transaction and a response transaction. The intent is to relinquish the
bus in between these two transactions, allowing it to be used for other transactions in the
interim. Such a bus is called a split-transaction bus. A variety of issues must be addressed to
make a split-transaction bus work. First, in general, the bus protocols must work even when
several pending operations are directed at a single slave module. This typically involves
pipelined operation of the slave itself.
In addition, if the bus is to support out-of-order
responses from slave modules, there must be some provision to tag each response with the
identity of the request it corresponds to.

Read-Modify-Write Operation: Other kinds of bus cycles also exist. For example,
to support synchronization in a multiprocessor system, many ISAs provide an instruction
like TEST AND SET (X). The semantics of this instruction is: read the value at memory
location X, and set it to 1, i(cid:11) the read value is 0. The read and write operations together
form an atomic operation. Executing this instruction requires a bus read cycle and a bus
write cycle. It is important that after the read cycle is performed, and before the write
cycle is performed, no other device reads or modi(cid:12)es memory location X. To implement this
instruction correctly, multiprocessor bus systems often have a special read-modify-write bus
cycle that allows a processor to read a word from memory, inspect and modify it, and write
it back to memory, all without releasing the bus. This type of bus cycle prevents competing
processors from being able to use the bus and thus interfere with the (cid:12)rst processor’s
operation.

272

Chapter 7. Microarchitecture | User Mode

7.6 Processor Data Path Interconnects: Design Choices

The previous sections discussed the basics of designing a data path for interpreting machine
language instructions. We purposely used a very simple data path for ease of understanding.
Although such a data path may not provide high performance, it may be quite su(cid:14)cient
for certain embedded applications. The data path of modern general-purpose computers,
on the other hand, is far more complex. In this section and the one following it, we will
introduce high-performance versions of the processor data path. The processor plays a
central role in a computer microarchitecture’s function. It communicates with and controls
the operation of other subsystems within the computer. The performance of the processor
therefore has a ma jor impact on the performance of the computer system as a whole.

7.6.1 Multiple-Bus based Data Paths

As discussed in Chapter 6, three important factors that determine the performance of a
computer are the strength of the machine language instructions, the number of clock cycles
required to fetch and execute a machine language instruction, and the clock speed. Among
these three, the number of cycles required to fetch and execute an instruction can be reduced
by redesigning the data path so that several micro-operations can be performed in parallel.
The more interconnections there are between the di(cid:11)erent blocks of a data path, the more
the data transfers that can happen in parallel in a clock cycle. The number of micro-
operations that can be done in parallel in a data path does depend on the connectivity it
provides. In a bus-based processor data path, the number of buses provided is probably the
most important limiting factor governing the number of cycles required to interpret machine
language instructions, as most of the microinstructions would need to use the bus(es) in
one way or other.

A processor data path with a single internal bus, such as the one given in Figure 7.7,
will take several cycles to fetch the register operands and to write the ALU result to the
destination register. This is because it takes one cycle to transfer each register operand to
the ALU, and one cycle to transfer the result from the ALU to a register. An obvious way
to reduce the number of cycles required to interpret instructions is to include multiple buses
in the data path, which makes it possible to parallelly transfer multiple register values to
the ALU. Before going into speci(cid:12)c multiple bus-based data paths, let us provide a word of
caution: buses can consume signi(cid:12)cant chip area and power. Furthermore, they may need
to cross at various points in the chip, which can make it di(cid:14)cult to do the layout at the
device level.

A 2-Bus Processor Data Path

The single-bus processor data path given in Figure 7.7 can be enhanced by adding one more
internal bus. The additional bus can be connected to the blocks in di(cid:11)erent ways. Figure

7.6. Processor Data Path Interconnects: Design Choices

273

7.12 pictorially shows one possible way of connecting the second bus. In the (cid:12)gure, the
new bus is shown in darker shade, and is used primarily for routing the ALU result back
to di(cid:11)erent registers. A unidirectional path is provided from the (cid:12)rst bus to the second.
Notice that to perform a register read and a register write in the same clock cycle, the

PC

MAR

System Address Bus

4

IR

)
s
u
B
 
d
n
a
r
e
p
O
(
 
1
 
s
u
B
 
r
o
s
s
e
c
o
r
P

)
s
u
B
 
t
l
u
s
e
R
(
 
2
 
s
u
B
 
r
o
s
s
e
c
o
r
P

Sign
Extend

rs

rt

rd

31

MUX

MUX

Register Write Addr

To Control Unit

Register File
(RF)

Register Read Addr

Register Data

Memory
Subsystem

System Data Bus

MUX

MDR

AIR

ALU

Flags

Processor Data Path

Figure 7.12: A 2-Bus Based Processor Data Path for Implementing the MIPS-0 ISA

register (cid:12)le requires two ports (a read port and a write port).

7.6.2 Direct Path-based Data Path

As we keep adding more buses to the processor data path, we eventually get a data path
that has many point-to-point connections or direct paths. If we are willing to use a large
number of such direct path connections, it is better to redesign the processor data path.

274

Chapter 7. Microarchitecture | User Mode

Figure 7.13 presents a direct path-based data path for the MIPS-0 ISA. In this data path,
instead of using one or more buses, point-to-point connections or direct paths are provided
between each pair of components that transfer data. This approach allows multiple data
transfers to take place simultanesously, thereby speeding up the execution of instructions,
as we will see in Chapter 9. For instance, the MAL command ComputeAddress requires
several data transfers, which can only be done sequentially in a single-bus based data path.
The use of direct paths probably makes the data path’s functioning easier to visualize,
because each interconnection is now used for a speci(cid:12)c transfer as opposed to the situation
in a bus-based data path where a bus transaction changes from clock cycle to clock cycle.
Notice, however, that a direct path based data path is even more tailored to a particular
ISA.

Notice that there are two paths emanating from the register (cid:12)le. If both paths are to
be active at the same time, then the register (cid:12)le needs to have two read ports.

7.7 Pipelined Data Path: Overlapping the Execution of Mul-
tiple Instructions

The use of multiple buses and direct paths in the processor data path enables us to reduce
the number of steps required to fetch and execute instructions. Can we further reduce the
number of steps by increasing the connectivity? It may be possible; however, it is very
di(cid:14)cult, because many of the steps are somewhat sequential in nature. Instead of reducing
the number of steps required to fetch and execute each instruction, we can, however, re-
duce the total number of steps required to fetch and execute a sequence of instructions by
overlapping the interpretation of multiple instructions. In order to overlap the execution
of multiple instructions, a commonly used technique is pipelining, similar in spirit to the
assembly-line processing used in factories.

In the data paths that we studied so far, after an instruction is fetched and passed onto
the instruction decoder, the fetch part of the processor sits idle until the execution of the
instruction is completed. By contrast, in a pipelined data path, the fetch part of the data
path utilizes this time to fetch the next several instructions. Similarly, after the instruction
decoder has decoded an instruction, it starts decoding the next instruction, without waiting
for the previous instruction’s execution to be completed.

It is important to understand that pipelining does not result in individual instructions
being executed faster; rather, it increases the rate at which instructions are interpreted.
With a four-stage pipeline, for instance, instruction execution can be completed up to four
times the rate of an unpipelined processor. Based on this discussion, it may appear that the
performance obtained with pipelining increases linearly with the number of pipeline stages.
Unfortunately, this is not the case. For a variety of reasons, a pipeline stage may not be
able to perform useful work during every clock cycle. This causes the pipelined data path’s
performance to be less than the maximum possible.

7.7. Pipelined Data Path: Overlapping the Execution of Multiple Instructions

275

PC

Instruction Address

+

Instruction
Memory
System

Instruction

Instruction
Decoder

31

Register File
(RF)

Return Address

ALU

+

ALU Result

Memory Address

Store Data

Data
Memory
System
Load Data

Processor Microarchitecture

Figure 7.13: A Direct Path-Based Processor Microarchitecture for Implementing the MIPS-
0 ISA

7.7.1 De(cid:12)ning a Pipelined Data Path

To begin with, we need to determine which hardware blocks in the data path are used
during every clock cycle, and make sure that the same hardware block is not used for two
di(cid:11)erent instructions during the same clock cycle. Ensuring this becomes di(cid:14)cult if the
same hardware resource is used in multiple cycles during the execution of an instruction.
This makes it di(cid:14)cult to use a bus-based data path, as a bus is generally used multiple times
during the execution of an instruction. Therefore, our starting point is the direct path-based
data path that we saw in Figure 7.13. What would be a natural way to partition this data
path into di(cid:11)erent stages? While we can think of many di(cid:11)erent ways to partition this data

276

Chapter 7. Microarchitecture | User Mode

path, a straightforward way to partition is as follows: place the hardware resources that
perform each of the MAL commands of the MAL routines (in Table ??) in a di(cid:11)erent stage.
Thus, we can think of a 5-stage pipeline as depicted in Figure 7.14. The 5 stages of the
pipeline are named fetch (F), read source registers (R), ALU operation (A), memory access
(M), and end (E).

F

R

A

M

E

F
R
A
M
E

− Fetch ML Instruction
− Read Source Registers
− ALU Operation
− Memory Access
− End

Figure 7.14: Illustration of a 5-Stage Pipelined Data Path

Next, let us partition the direct path-based data path of Figure 7.13 as per this 5-
stage framework. Figure 7.15 shows such a partitioning. In this pipelined data path, the
instruction fetch portion of the data path|which includes PC and its update logic|has
been demarcated as the F stage. IR, which stores the fetched instruction, forms a bu(cid:11)er
between the F stage and the R stage. The instruction decoder and the register (cid:12)le have
been placed in the R stage. Similarly, the ALU is placed in the A stage. AOR, which stores
the output of the ALU, forms a bu(cid:11)er between the A stage and the M stage, which houses
the logic for accessing the data memory. Finally, the E stage just consists of the logic for
updating the register (cid:12)le. Table 7.8 shows the primary resources used in each stage of the
pipeline.

Primary Resources
Stage
PC, Instruction memory
Fetch
Register (cid:12)le, Sign extension unit
Read register
ALU
ALU
Memory access Data memory, Register (cid:12)le

Table 7.8: Primary Resources Used in Each Stage of the Pipelined Data Path of Figure
7.15

In the pipelined data path being discussed, an instruction uses the ma jor hardware blocks
in di(cid:11)erent clock cycles, and hence overlapping the interpretation of multiple instructions
introduces relatively fewer con(cid:13)icts. Some of the hardware blocks in the direct path-based

7.7. Pipelined Data Path: Overlapping the Execution of Multiple Instructions

277

Instruction Address

PC

Incrementer

PC_R

L1
Instruction
Cache

Instruction
rs
IR

rt

rd

Instruction
Decoder
rs
rt
Register File
(RF)

Opcode

Fetch (F)

Register Read (R)

dest_A

Rrs_A

Rrt_A

Offset_A

PC_A

)
W
_
r
(
 
r
e
t
s
i
g
e
R
 
n
o
i
t
a
n
i
t
s
e
D

Jump Target

+

Branch Target

ALU (A)

ALU

dest_M

Flags

AOR

Rrt_M

PC_M

Target

L1
Data
Cache

Memory (M)

ALU Result

Return Address

MUX

Processor Data Path

Figure 7.15: A Pipelined Processor Microarchitecture for Implementing the MIPS-0 ISA

data path are used multiple times while interpreting some instructions. In order to avoid
con(cid:13)icts for these hardware blocks, they have been replicated in the pipelined data path.
For example, a single PC cannot store the addresses of two di(cid:11)erent instructions at the same
time. Therefore, the pipelined data path must provide multiple copies of PC, one for each
instruction that is being executed in the processor.

Pipelining has been used since the 1960s to increase instruction throughput. Since then,

278

Chapter 7. Microarchitecture | User Mode

a number of hardware and software features have been developed to enhance the e(cid:14)ciency
and e(cid:11)ectiveness of pipelining. Not all pipelined data paths have all of these characteristics;
the ones we discuss below are quite prevalent.

Multi-ported Register File: When multiple instructions are being executed in a pipelined
data path, the same hardware block cannot be used for two instructions in the same clock
cycle. This requirement is straightforward to meet if each hardware block is accessed at most
in one piepline stage. A prime example of this is the ALU. It is required for an instruction
only in the A stage. An inspection of Table 7.8 reveals that the register (cid:12)le, on the other
hand, may be used by instructions present in two di(cid:11)erent stages of the pipeline|stages
R and M. Thus, in any given clock cycle, when the instruction in the R stage is trying
to read from the register (cid:12)le, a previous instruction in the M stage may be attempting to
write to the register (cid:12)le. The solution adopted in our data path is to make the register (cid:12)le
multi-ported. That is, the register (cid:12)le permits multiple accesses to be made in the same
clock cycle10 .

Separate Instruction Cache and Data Cache: Just like the case with register (cid:12)le
accesses, memory accesses may also be performed from multiple pipeline stages | instruc-
tion fetch from the F stage and data access from the M stage. If a single memory structure
is used for storing both the instructions and the data, then the data access done by a
memory-referencing instruction (load or store) will happen at the same time as the instruc-
tion fetch for a succeeding instruction. The data path of Figure 7.15 used two di(cid:11)erent
memory structures|an L1 instruction cache and an L1 data cache|to avoid this problem.
Another option is to use a single dual-ported memory structure.

Interconnect: Pipelined data paths typically use the direct path-based approach dis-
cussed earlier, which uses direct connections between the various hardware blocks. A bus-
based data path is not suitable for pipelining, because only one data transfer can take place
in a bus in a clock cycle. The direct path approach allows data transfers to take place
simultanesously in each pipeline stage, which indeed they must if the goal of allowing many
activities to proceed simultanesously is to be achieved. In fact, we have to add even more
interconnections that in a non-pipelined data path.

Pipeline Registers or Latches: When the processing of an instruction shifts from one
pipeline stage to the next, the vacated stage is utilized for processing the next instruc-
tion. Thus, consecutive stages of the pipeline contain information pertaining to di(cid:11)erent
instructions.
If adequate bu(cid:11)ering is not provided between consecutive stages, informa-
tion pertaining to di(cid:11)erent instructions can mix together, leading to incorrect execution.

10Making a register (cid:12)le multi-ported increases its size as well as the time it takes to read from or write to
the register (cid:12)le.

7.7. Pipelined Data Path: Overlapping the Execution of Multiple Instructions

279

Therefore, it is customary to separate consecutive stages by latches or registers. Because an
instruction can \occupy" only one pipeline stage at any given time, whatever information
is needed to execute that instruction at a subsequent stage must be carried along until that
stage is reached. This is the case even if the information is not required at every stage
through which it passes. Each instruction must \carry its own load," so to speak. Such
information might include opcode, data values, etc. A convenient way of propagating this
information is to insert extra pipeline latches between the pipeline stages. For example, the
incremented PC value is carried along, with the help of pipeline registers PC R, PC A, and
PC M.

Additional Hardware Resouces: Pipelined data paths usually need additional hard-
ware resources to avoid resource con(cid:13)icts between simultaneously executed instructions. A
likely candidate is a separate incrementer to increment the PC, freeing the ALU for doing
arithmetic/logical operations in every clock cycle.

7.7.2

Interpreting ML Instructions in a Pipelined Data Path

The ob jective of designing a pipelined data path is to permit the interpretation of multiple
instructions at the same time, in a pipelined manner. The MAL routines used for inter-
preting each instruction is similar to those for the direct path-based data path. Table 9.13
illustrates how the MAL routines for multiple instructions are executed in parallel.

Step
No.
0
1
2
3
4

5

lw rt, offset(rs)

Fetch instruction
Read registers
Rrs E + Off E ! AOR
DM[AOR] ! R[r W]

MAL Command
addu rd, rs, rt

beq rs, rt, offset

Fetch instruction
Read
registers
Rrs E + Rrt E ! AOR
AOR ! R[r W]

instruction
Fetch
registers
Read
Rrs E == Rrt E ! Z
PC E + Off E (cid:2) 4 ! Target
if (Z) Target ! PC

Table 7.9: Overlapped Execution of MAL Routines for Fetching and Executing Three MIPS-
0 Instructions in the Pipelined Data Path of Figure 7.15

7.7.3 Control Unit for a Pipelined Data Path

Converting an unpipelined data path into a pipelined one involves several modi(cid:12)cations, as
we just saw. The control unit also needs to be modi(cid:12)ed accordingly. We shall brie(cid:13)y look
at how the control unit of a pipelined data path works. With a pipelined data path, in each
clock cycle, the control unit has to generate appropriate microinstructions for interpreting

280

Chapter 7. Microarchitecture | User Mode

the instructions that are present in each pipeline stage. This calls for some ma jor changes
in the control unit, perhaps even more than what was required for the data path. A simple
approach is to generate at instruction decode time all of the microinstructions required
to interpret that instruction in the subsequent clock cycles (in di(cid:11)erent pipeline stages).
These microinstructions are passed on to the pipelined data path, which carries them along
with the corresponding instruction by means of extended pipeline registers. Then, in each
pipeline stage, the appropriate microinstruction is executed by pulling it out of its pipeline
register.

An alternate approach is for each pipeline stage to have its own control unit, so to
speak. Depending on the opcode of the instruction that is corrently occupying pipeline
stage i, the ith control unit generates the appropriate microinstruction and passes it to
stage i. Designing a control unit for a pipelined data path requires sophisticated techniques
which are beyond the scope of this book.

7.7.4 Dealing with Control Flow

The pipelined data path discussed above will not work correctly when the executed program
deviates from straightline sequencing. Such deviations happen whenever a branch instruc-
tion is taken (i.e., its condition is satis(cid:12)ed and control is transferred to its target address),
or a jump instruction is encountered. System call instructions also cause a change in control
(cid:13)ow, in addition to switching the system to kernel mode. The pipelined data path that we
have been studying does have the ability to perform the control (cid:13)ow deviation dictated by
branches, jumps, and syscalls. The Target register is updated with the appropriate target
address, and this value can be selectively loaded into the PC register, so that instructions
can be fetched from the target address thereafter. The problem, however, is that prior to
fetching instructions from the target address, the data path would have already fetched
the three instructions that immediately follow the branch/jump/syscall instruction. These
instructions, if left unhampered, will wreak havoc in the data path by causing unintended
changes to the ISA-visible state | the register (cid:12)le, PC, and the memory.

Consider the following MIPS program snippet. It starts with a conditional branch in-
struction at address 0x400500. If the branch condition is not satis(cid:12)ed, control (cid:13)ow proceeds
in a straightline fashion to fall-through instructions 1, 2, 3, .... By contrast, if the condition
is satis(cid:12)ed, control (cid:13)ow is transferred to the sub instruction, skipping the 3 add instructions.

0x400500:

beq $1, $2, 3

0x400504:
0x400508:
0x40050c:
0x400510:

add $3, $1, $2
add $5, $1, $4
add $6, $2, $4
sub $7, $3, $4

# skip next 3 instructions
# if values in $1 and $2 are equal
# fall-through instruction 1
# fall-through instruction 2
# fall-through instruction 3
# target address instruction

Now, let us track the execution of this program snippet in our pipelined data path. When

7.7. Pipelined Data Path: Overlapping the Execution of Multiple Instructions

281

this snippet is about to be executed, PC contains address 0x400500. Once this address is
in place in PC, fetching of the branch instruction automatically happens in the F stage. In
addition, the PC is also updated to 0x400504 in the F stage. In the next clock cycle, the
fetched branch instruction moves to the R stage of the pipeline, where reading of its source
registers $1 and $2 occurrs. At the same time, the F stage continues its action by fetching
fall-through instruction 1 from address 0x400504. In the third clock cycle, evaluation of
the branch condition takes place in the A stage. Calculation of the branch target address
(0x400510) also happens in the A stage. The outcomes of these actions are recorded in
the flags register and Target register, which sit at the boundary between the A and M
stages.
If the branch condition is satis(cid:12)ed, updation of the PC register with the target
address happens in the 4th clock cycle, when the branch instruction moves into the M
stage of the pipeline. By this time, stages A, R, and F of the pipeline would be populated
by fall-through instructions 1, 2, and 3, respectively. The pipeline latches present at the
boundaries of these stages would accordingly be updated by information pertaining to these
instructions. All of this is perfectly (cid:12)ne if the branch condition is not satis(cid:12)ed and program
control should continue to these instructions as per straightline sequencing. If the branch
condition is satis(cid:12)ed, on the other hand, then allowing these three instructions to proceed
would result in incorrect execution of the program. Speci(cid:12)cally, the value of register $3 used
by the target address instruction would be the one produced by fall-through instruction 1.

There are several ways to deal with the above control hazard problem. The most
straightforward among these is to detect the presence of a control hazard, and then squash
the unwanted instructions from the pipeline, if there is a change in control (cid:13)ow. Squashing
an instruction amounts to converting the instruction into a nop instruction within the
pipeline. Notice that the squashed instructions would not have made any changes to our
data path’s ISA-visible state such as the register (cid:12)le and the memory locations. Figure
***** illustrates this dynamic squashing of instructions within a pipeline.

The squashing method discussed above results in a loss of 3 clock cycles, as three in-
structions were converted to nop instructions. We call this 3-cycle loss as the branch
penalty. In most of the real-life programs, a control (cid:13)ow change is not an infrequent event
| roughly one in 6 of the executed instructions can be a control-changing instruction.
Loosing 3 clock cycles every 6 cycles is not very appealing. Microarchitects reduce this
penalty by incorporating one or more of the following techniques:

(cid:15) Branch latency reduction

(cid:15) Branch outcome prediction

(cid:15) Branch e(cid:11)ect delaying

7.7.4.1 Branch Latency Reduction

In our pipelined data path, the outcome of a conditional branch instruction is determined
in the A stage; calculation of the target address of control-changing instructions is also done

282

Chapter 7. Microarchitecture | User Mode

in the A stage. We can in fact determine the branch outcome in the R stage itself, as the
branch conditions in the MIPS-I ISA are quite simple (testing the equality of two register
values, or determining if the (cid:12)rst value is greater than or less than the second value). An
early computation such as this would, of course, require an extra comparator to be included
in the R stage.

In a similar manner, calculation of the target address can also be performed in the R
stage itself, as the ingredients for this calculation, such as the PC value and the Offset
value, are available in the R stage itself. An important point is in order here: to complete
the target address calculation in a timely manner in the R stage, the calculation may have
to be started before knowing for sure if target address calculation is indeed required, i.e.,
before instruction decoding has been completed. Moreover, in the MIPS-I ISA, the target
address calculation is di(cid:11)erent for the branch instructions and the jump instructions. This
means that both types of calculation have to be done in parallel, such that the correct one
among them can be selected after instruction decoding is completed. Notice also that such
speculative actions increase the power consumption within the data path.

Performing both branch condition evaluation and target address calculation in the R
stage reduces the branch penalty to just one cycle! Being able to determine the branch
outcome as well as the target address in the R stage may seem to be quite an achievement;
not so for modern pipelined processor data paths, which tend to have long, multi-stage,
fetch engines.
In these machines, several pipeline stages may precede the R stage; even
if branch resolution is done in the R stage, several clock cycles are lost for every branch
instruction. Can branch resolution be done earlier in such pipelines? Yes and no. While
complete resolution of a branch instruction can be done only after the source registers have
been read, a speculative resolution can be done before knowing the source register values.
Next, let us look at this widely used scheme for reducing branch penalties in deeply pipelined
machines.

7.7.4.2 Branch Outcome Prediction

When a conditional branch instruction is executed, one of two possible outcomes can hap-
pen: either the branch is taken or it is not taken. Once this outcome is known, the data path
can take appropriate actions for dealing with the instructions that were fetched following
the branch. Can the data path somehow predict the outcome of a branch instruction well
before its outcome becomes known? If it can do so, then it can take appropriate actions
based on the predicted outcome. If the prediction turns out to be correct, then the data
path does not su(cid:11)er from an inordinate branch penalty. If the prediction turns out to be
incorrect, then the data path su(cid:11)ers a branch misprediction penalty, which may even
be higher than the normal branch penalty. If the branch predictions turn out to be correct
the ma jority of the time, then it is worth the gamble!

Are branch instruction outcomes very predictable? Branch behavior is an artifact of
the way programmers write high-level programs. Extensive studies by computer architects

7.7. Pipelined Data Path: Overlapping the Execution of Multiple Instructions

283

with real-life programs have conclusively shown that branch outcomes are very predictable.
There are several reasons for this predictability. First, branches that are used to implement
loops tend to be taken many times in a row before they end up being not taken. These
branches are highly biased, in that they are taken many more times than they are not
taken. Second, many branches serve the function of testing for error conditions, which
come into existence very rarely. Such branches are also highly biased. Even if a branch is
not highly biased one way or the other, it may still be very predictable. Some of them depict
alternating behavior: taken, not taken, taken, not taken, .... Some others have outcomes
that have more complex, yet predictable, patterns. Yet others show behavior that is strongly
correlated to the outcomes of branches that immediately precede it. A detailed treatment
of branch behavior and branch prediction techniques is beyond the scope of this textbook.
We shall, however, outline some of the branch behavior and branch prediction schemes that
exploit such behavior. Before that, we need to see how the data path decides to make a
prediction before knowing that the instruction being fetched is a branch instruction. The
key to determining this is, again, temporal locality|branches that are executed now are
most likely to have been encountered before.

Branch Target Bu(cid:11)er (BTB): The branch target bu(cid:11)er is a microarchitectural struc-
ture for storing that target address of previously encountered branch instructions.
It is
usually indexed by hashing the pc value of the instruction. Because multiple pc values can
hash to the same entry in the BTB, a pc (cid:12)eld is kept in each BTB entry to identify the
address of the branch instruction that is currently mapped to that entry. Figure 7.16 shows
the organization of a BTB. This BTB has two entries occupied with information about the
branches at addresses 0x400800 and 0x400020. Their target addresses are 0x400880 and
0x400008. While fetching an instruction from the instruction cache, the BTB is consulted
in parallel to determine if the instruction being fetched is a branch instruction (that has
been seen before), and if so, to obtain its target address. A miss in the BTB indicates that
the instruction is either not a branch instruction or a branch instruction that is unlikely
to have been encountered in the recent past. In either case, if there is a miss in the BTB,
pipelined execution proceeds as normal, as in straightline sequencing.

Loop-terminating branches: As mentioned earlier, branch instructions that terminate
a loop tend to be taken many more times than they are not taken. A simple way to
exploit this behavior is to speculatively start executing from the target address whenever
a loop-terminating branch is encountered. How can the microarchitecture identify a loop-
terminating branch? A simple heuristic to use is to consider all branches with negative
o(cid:11)sets to be loop-terminating branches.

Dynamic branch prediction: Branches that are not loop-terminating may not be heav-
ily biased in one direction; however, they may still have predictable behavior. The best way
to capture such behavior is to use a microarchitectural structure that keeps track of the

284

Chapter 7. Microarchitecture | User Mode

pc

target

0x400800

0x400880

0x400020

0x400008

Hash

Figure 7.16: Organization of a Branch Target Bu(cid:11)er (BTB)

recent history of branches, and predicts the outcome every time a branch instruction is
fetched. The scheme used for prediction could range from the simple Last Outcome based
prediction to the most complex Perceptron based prediction. The Last Outcome predictor
.........

7.7.4.3 Branch E(cid:11)ect Delaying

Branch prediction is a popular technique for reducing branch penalty in high-performance
processors. However, it calls for higher power consumption as well as hardware complexity.
The hardware structures used to store the past behavior of branches can be quite large,
taking up space in the processor chip. They can also consume a signi(cid:12)cant amount of elec-
trical power. Both of these factors can make branch prediction less attractive in embedded
systems where size and power consumption may be constrained. Moreover, real-time system
designers like to have very little uncertainities in the system; with branch prediction, it is
di(cid:14)cult to accurately estimate the running time of a program. In such a scenario, we can
employ yet another scheme for reducing branch penalty: rede(cid:12)ne the semantics of a branch
instruction to mean that a (cid:12)xed number of instructions after the branch instruction should
be executed irrespective of the outcome of the branch.

7.7.5 Dealing with Data Flow

The pipelined data path discussed above will work correctly as long as the instructions in
the pipeline do not need to read registers that are updated by earlier instructions that are
still present in the pipeline. There is indeed a problem when an instruction has to read a

7.7. Pipelined Data Path: Overlapping the Execution of Multiple Instructions

285

register that will be updated by an earlier instruction. Consider the following instruction
sequence:

add
$3, $1, $2
addi $4, $3, 16

The (cid:12)rst instruction updates register $3, which is the source register for the second instruc-
tion. When these two instructions are executed in a non-pipelined data path, fetching and
execution of the second instruction begins only after the execution of the (cid:12)rst instruction
is completed and its result is written in $3. By contrast, in a pipelined data path, the
execution of the two instructions overlap in time. Let us trace this overlapped execution
and see what happens. Figure 7.17 shows a timeline depicting the ma jor actions happening
in the pipeline.

0

1

2

3

Clock Cycles

add  $3, $1, $2

Fetch

Read $1,$2 Add

Update $3

addi $4, $3, 16

Fetch

Read $3

Add

Update $4

Reads incorrect value

Figure 7.17: An Example Illustrating Data Hazards in a Pipelined Processor

The (cid:12)rst instruction starts execution in clock cycle 0, and completes in clock cycle 3
when it writes its result to register $3. The second instruction starts execution in clock cycle
1, and reads register $3 in clock cycle 2. As this read operation happens before the write
operation of clock cycle 3, it obtains the value that was in register $3 prior to the execution
of the (cid:12)rst instruction. This clearly violates the semantic meaning of the program.

There are di(cid:11)erent ways to prevent such a data hazard from causing incorrect execu-
tion. The simplest among them is to detect the presence of data hazards, and temporarily
switch to unpipelined operation if and when a data hazard is detected. In the above exam-
ple, for instance, if the second instruction is delayed by 2 cycles as shown in Figure 7.18,
then its register read operation happens only after the (cid:12)rst instruction performs its register
write operation. Of course, we need to introduce additional hardware to detect the pres-
ence of data hazards. Whenever a register-reading instruction reaches the R stage of the
pipeline, this hardware should check if its source registers match the destination registers
of the instructions present in the A and M stages of the pipeline.

The additional hardware for detecting data hazards turns out to be straightforward to
design, once we list all data hazard scenarios. For the pipelined data path that is on our
plate, we can group the data hazards into the following scenarios:

286

Chapter 7. Microarchitecture | User Mode

Clock Cycles

0

1

2

3

4

5

6

I0:

add  $3, $1, $2

Fetch

Read $1,$2 Add

Update $3

I1:

addi $4, $3, 16

Fetch

Stall

Stall

Read $3

Add

Update $4

Pipeline Occupancy

I0

I1

I0

I1

I0

I1

I0

I1

I1

I1

Reads correct value

Figure 7.18: An Example Illustrating the use of Pipeline Stalls to deal with Data Hazards
in a Pipelined Processor

1. lw
rt, offset(rs) # any load instruction (register rt is destination)
# any R-format instruction or branch-type instruction
add rd, rs, rt
# (registers rs and rt are sources)

A data hazard exists if the rt value of lw is equal to the rs or rt values of add.

2. lw
rt, offset(rs) # any load instruction (register rt is destination)
addi rt, rs, immed
# any I-format arithmetc/logic instruction
# (register rs is source)

A data hazard exists if the rt value of lw is equal to the rs value of addi.

3. lw
...
add rd, rs, rt

rt, offset(rs) # any load instruction (register rt is destination)
# any instruction that doesn’t have register rt as source

4. lw
...
addi rt, rs, immed

rt, offset(rs) # any load instruction (register rt is destination)
# any instruction that does not have register rt as source
# any I-format arithmetc/logic instruction
# (register rs is source)

7.7.6 Pipelines in Commercial Processors

Pipelining has been in use for several decades. One of the earliest machines to use pipelining
was the CDC 6600. The MIPS R2000 and R3000 processors, which came out in 198* and
199*, respectively, used the 5-stage pipeline that we just looked at. Intel’s Pentium processor
also used a relatively short 5-stage pipeline. The Pentium Pro/II/III series of processors
used a deeper pipeline having 12 stages, whereas the recent Pentium IV uses a very deep
pipeline having 20 stages.

7.8. Wide Data Paths: Superscalar and VLIW Processing

287

jump
branch

Instruction Address

L1
Instr.
Cache

Instruction
rs

IR

rt

rd

Opcode
Func

MUX

PC

4

+

PC_D

Fetch (F)

31 0

MUX

rs
rt
Register File
(RF)

Sign
Extend

Register Read (R)

MUX

MUX

=
=
=
=

rd_E

Rrs_E

Rrt_E

Offset_E

PC_E

)
W
_
r
(
 
r
e
t
s
i
g
e
R
 
n
o
i
t
a
n
i
t
s
e
D

rd_M

MUX
Jump Target

ALU

+

Branch Target

MUX

Flags

AOR

Rrt_M

PC_M

Target

L1
Data
Cache

ALU Result

Return Address

MUX

Processor Data Path

ALU (A)

Memory (M)

Figure 7.19: A Pipelined Processor Data Path with Data Forwarding

7.8 Wide Data Paths: Superscalar and VLIW Processing

We have seen how pipelining the datapath overlaps the execution of multiple instructions,
thereby executing more instructions in unit time. The maximum instruction execution
rate that can be achieved with pipelining, however, is only one instruction per clock cycle;
notice that during every clock cycle, at most one new instruction is entering the pipeline (at

288

Chapter 7. Microarchitecture | User Mode

the fetch stage). Modern processors surpass this execution rate by widening the pipelined
data path. This means that every clock cycle, they fetch multiple instructions in parallel,
decode them in parallel, and execute them in parallel. This type of multiple-issue is done
in addition to the pipelining technique described above. For multiple-issue to bear fruit,
the processor data path should have multiple ALUs, or functional units.

Wide-issue datapaths have multiple instructions in each stage of the pipeline, and there-
fore overlap the execution of a large number of instructions. This exacerbates the control
hazards and data hazards problem.

Compared to a single-wide pipeline, wider pipelines overlap the execution of a larger
number of instructions, exacerbating the problem of control hazards and data hazards. For
instance, when a control hazard is detected, more instructions are (cid:13)ushed from the pipeline,
in general. The complexity of hazard detection and corrective action due to the hazards
increases in two ways:

(cid:15) More instructions are present in the pipelined datapath

(cid:15) Hazards may be present among the multiple instructions present in the same stage of
the pipeline

The (cid:12)rst one among these is similar to what happens in a deep pipeline, and calls for rigorous
application of the techniques discussed in Sections 7.7.4 and ?? for dealing with control
hazards and data hazards | techniques such as branch prediction and data forwarding.
Thus, we need branch predictors that are highly accurate, for instance. We also need many
more paths for forwarding data.

Hazards introduced due to the presence of multiple instructions in the same pipeline
stage ....

[This section needs to be expanded.]

7.9 Co-Processors

7.10 Processor Data Paths for Low Power

\The ultimate \computer," our own brain, uses only ten watts of power { one-tenth
the energy consumed by a hundred-watt bulb."
| Paul Valery, Poet and Essayist

All of this chapter’s discussion so far focussed primarily on performance. While perfor-
mance is one of the primary considerations in microarchitecture design, power consumption
is becoming an important design consideration. Power consumption in a hardware circuit
is the sum of the power consumed by its individual low-level components (primarily the
transistors). The dynamic power consumed by a transistor is directly proportional to the

7.10. Processor Data Paths for Low Power

289

activity in the transistor in unit time, which can be loosely quanti(cid:12)ed as the product of
number of times it switched in unit time, its capacitance, and the square of the supply
voltage. The static power consumed by a transistor is directly proportional to its supply
voltage and leakage current.

From the above discussion, it is clear that power consumption can be reduced by reducing
the supply voltage and the number of transistors. Reducing the supply voltage, although
feasible up to a point, does have the drawback of reducing the noise immunity of the circuit.
Reducing the number of transistors directly translates to reducing the hardware circuitry
in the design. In many situations, this is likely to hurt performance. Thus, there is oftan a
trade-o(cid:11) between performance and power consumption. Intelligent designs attempt to use
as little hardware as possible without reducing the performance.

Recent Processors Intel ***

Recent Processors

290

Chapter 7. Microarchitecture | User Mode

7.11 Concluding Remarks

7.12. Exercises

7.12 Exercises

291

1. Consider a non-MIPS ISA that has variable length instructions. One of the instruc-
tions in this ISA is ADD (ADDR), which means add the contents of memory location
ADDR to ACC register. This instruction has the following encoding; notice that it oc-
cupies two memory locations.

Bit Pattern
for ADD

Bit Pattern
for mem dir
addressing

Bit Pattern for ADDR

Specify a MAL routine to carry out the interpretation of this instruction in the data
path given in Figure 9.12.

Main

Memory

RD

WR

Addr

Data_out

Data_in

1

+/-

SP

IR

ID

IR<Addr2>

IR<Addr1>

ADD

AND

CU

F0
F1

Len

PC

MAR

MDR

ALU

ACC

WR

Internal Bus

CPU

Figure 7.20: A Single-Bus based Processor Data Path for Interpreting an ACC based ISA

2. Consider the non-MIPS instruction ADD (ADDR1), (ADDR2), where ADDR1 and ADDR2
are memory locations whose contents needed to be added. The result is to be stored
in memory location ADDR1. This instruction has the following encoding; notice that

292

Chapter 7. Microarchitecture | User Mode

addresses ADDR1 and ADDR2 are speci(cid:12)ed in the words subsequent to the word that
stores the ADD opcode.

Bit Pattern
for ADD

Bit Pattern
for mem dir
addressing

Bit Pattern
for mem dir
addressing

Bit Pattern for ADDR1

Bit Pattern for ADDR2

Word 0

Word 1

Word 2

Specify the data transfer operations required to fetch and execute this instruction in
the data path given in Figure 9.12. Notice that register ACC is a part of the ISA,
and therefore needs to retain the value that it had before interpreting the current
instruction. You are allowed to grow the stack in the direction of lower memory
addresses.

3. Explain how adding multiple CPU internal buses can help improve performance.

4. Explain with the help of diagram(s) how pipelining the processor data path helps
improve performance.

5. Consider a very small direct mapped cache with a total of 4 block frames and a block
size of 256 bytes. Assume that the cache is initially empty. The CPU accesses the fol-
lowing memory locations, in that order: c881H, 7742H, 79c3H, c003H, 7842H, c803H,
7181H, 7381H, 7703H, 7745H. All addresses are byte addresses. To get partial credit,
show clearly what happens on each access.

(a) For each memory reference, indicate the outcome of the reference, either \hit" or
\miss".

(b) What are the (cid:12)nal contents of the cache? That is, for each cache set and each
block frame within a set, indicate if the block frame is empty or occupied, and if
occupied, indicate the tag of the memory block that is currently present.

Chapter 8

Microarchitecture | Kernel Mode

A cheerful look brings joy to the heart, and good news gives health to the bones

Proverbs 15: 30

Look at Proverbs 23: 23

The previous chapter discussed aspects of the microarchitecture that deal with the imple-
mentation of the user mode ISA. The kernel mode ISA includes several additional features,
such as privileged registers, privileged memory address space, IO registers, and privileged
instructions. This chapter focuses on microarchitecture aspects that deal speci(cid:12)cally with
the implementation of these additional features present in the kernel mode ISA. The func-
tionality served by these features can be classi(cid:12)ed under 3 broad categories: processor
management, memory management (virtual memory system), and IO system. This chapter
is organized precisely along this classi(cid:12)cation.

The (cid:12)rst part of the chapter discusses microarchitectural aspects that are essential for
performing context switches at times of system calls, exceptions, interrupts, and return
from exception. The second part provides an in-depth treatment of virtual memory. After
presenting the ma jor concepts of virtual memory, we take a close look at the virtual memory
scheme in a MIPS-I system. Finally, the last part of the chapter address IO subsystem design
and other system architecture issues.

8.1 Processor Management

Processor management involves allocating the processor to a particular process. As we saw
in Chapter 4, the processor is generally time-multiplexed among the active processes in the

293

294

Chapter 8. Microarchitecture | Kernel Mode

system1 . We can break down the task of processor management into three aspects: recog-
nizing exceptions and interrupts; enabling and disabling of interrupts; and switching back
and forth between the user and kernel modes. We shall look at how the microarchitecture
recognizes exceptions, and performs switching between the user and kernel modes.

8.1.1

Interpreting a System Call Instruction

In Chapter 7, we brie(cid:13)y discussed the execution of syscall instructions. However, we did not
speci(cid:12)cally say what happens after the system is placed in the kernel mode. We shall see
here how the data path implements this part of the syscall instruction execution. Again, our
discussion is based on the MIPS-I ISA, although the underlying principles are applicable
to other ISAs as well. It is important to recall from Section 4.2.1 the ma jor functionality
speci(cid:12)ed by an instruction like syscall. We highlight these functions below:

(cid:15) Switch to kernel mode: In the MIPS-I ISA, this is done by modifying the state
register.

(cid:15) Disable interrupts:
register.

In the MIPS-I ISA, this is also done by modifying the state

(cid:15) Save return address: In the MIPS-I ISA, the return address of a syscall instruction is
saved in the privileged register called epc (exception program counter).

(cid:15) Record the cause for this exceptional event: In MIPS-I ISA, the same entry point
(0x80000080) is speci(cid:12)ed for all but two of the exceptional events. Therefore, to
identify the reason for transferring control to this memory location, the syscall code
is entered into the ExcCode (cid:12)eld of cause register.

(cid:15) Update PC to point to the entry point associated with syscall (0x80000080).

In order to perform these functions, the data paths presented in Chapter 7 need to be
enhanced to include the privileged registers state, cause, and epc. Figure 8.1 shows a
data path obtained by including these privileged registers in the dat path of Figure 7.7.

Table 8.1 presents a MAL routine for executing the MIPS-I syscall instruction in the
data path of Figure 8.1. The (cid:12)rst step of this routine updates the state register to place
the system in the Kernel mode and to disable device interrupts. The next step saves the
address of the syscall instruction in the EPC register. PC is then updated with the entry
point address 0x80000080. Because this entry point is common for many exceptional events,
the ExcCode (cid:12)eld of the cause register is set to 8 to indicate that the exceptional event in
this case is a syscall instruction.

1This is in contrast to the memory system and the IO system, which are generally space-multiplexed
among the active processes.

8.1. Processor Management

295

To Control Unit

Cause

EPC

SR

Shift

PRE_PC

PC

IR

Instr.
Decoder

32

Register Address

5

Register File
(RF)

s
u
B
 
r
o
s
s
e
c
o
r
P

Memory Interface

Kernel Mode
Registers

MAR

System Address Bus

Memory
Subsystem

System Data Bus

Register Data

MDR

ALU

Flags

Processor Data Path

Figure 8.1: A Data Path for Implementing the MIPS-0 Kernel Mode ISA

8.1.2 Recognizing Exceptions and Hardware Interrupts

A realistic processor must do more than fetch and execute instructions.
It must handle
exceptions and also respond to device interrupts, irrespective of whether it is in the user
mode or the kernel mode. The processor must explicitly check for exceptions whenever an
exception can occur. Similarly, at the end of executing an ML instruction, it must check if
there are any pending interrupts from IO devices. The actions taken in both these cases are
somewhat similar to those taken when executing a syscall instruction. After all, a syscall
instruction is a software interrupt.

Consider the add instruction of the MIPS-I ISA. This instruction is similar to the addu
instruction whose execution we saw in detail in Chapter 7.
It speci(cid:12)es the contents of
registers rs and rt to be added and the result to be placed in register rd. The only di(cid:11)erence

296

Step
No.

4

5
6
7

Chapter 8. Microarchitecture | Kernel Mode

Next Step No. MAL Instruction to be Generated
for Control Unit
for Data Path
Execute phase
SR<3:0> << 2 ! SR<5:0>

Comments

Set system in kernel mode
and disable interrupts
Pre PC ! EPC
Save instruction address
0x80000080 ! PC
Set PC to 0x80000080
8 ! cause<ExcCode> Update cause register

Goto step 0

Table 8.1: A MAL Routine for carrying out the Execute Phase of the MIPS-0 Instruction
syscall

between add and addu is that add speci(cid:12)es checking for arithmetic over(cid:13)ow (assuming the
integers to be in the 2’s complement number system).
In the event of an over(cid:13)ow, an
exception should be generated. Table 8.2 presents a MAL routine for interpreting the add
instruction.

Step
No.

Next Step No.
for Control Unit

4
5
6

7

8
9
10

if (cid:22)Ov goto step 0

Goto step 0

MAL Instruction to be Generated
for Data Path
Execute phase
R[rs] ! AIR
R[rt] + AIR ! AOR, Ov
if ( (cid:22)Ov && rd) AOR ! R[rd]
Take Over(cid:13)ow Exception
SR<3:0> << 2 ! SR<5:0>

Comments

Update Ov (cid:13)ag also

Set system in kernel mode
and disable interrupts
Save instruction address
Pre PC ! EPC
0x80000080 ! PC
Set PC to 0x80000080
12 ! cause<ExcCode> Update cause register

Table 8.2: A MAL Routine for the Execute Phase of the Interpretation of the MIPS-0
ISA Instruction Represented Symbolically as add rd, rs, rt. This MAL Routine is for
executing the ML instruction in the Data Path of Figure 8.18

The (cid:12)rst step of this MAL routine is same as before. In the ALU operation step, in
addition to writing the addition result in AOR, the Ov (over(cid:13)ow) (cid:13)ag is updated to indicate
the occurrence of over(cid:13)ow, if any. The next step explicitly checks for the value of Ov. If
this (cid:13)ag is not set, then the addition result is written to register rd and control goes back
to step 0. Otherwise, the microarchitecture proceeds to step 7 which begins the routine for
taking the exception. This part is the same as that of the syscall instruction except that
the code written in the ExcCode (cid:12)eld of the cause register is 12.

8.2. Memory Management: Implementing Virtual Memory

297

8.1.3

Interpreting an RFE Instruction

The kernel mode includes many privileged instructions, especially for manipulating di(cid:11)erent
privileged registers. One of the kernel mode instructions that warrants special treatment is
the rfe (restore from exception) instruction. This instruction tells the machine to restore
the system’s mode and interrupt status to what it was prior to taking this exception.
Ironically, it does not tell the machine to transfer control to the interrupted program; a
standard jr instruction is used to achieve this transfer of control, as we saw in Chapter 4.
Table 8.3 presents the MAL routine for the execute phase of the rfe instruction.

Step
No.

4

Next Step No. MAL Instruction to be Generated
for Data Path
for Control Unit
Execute phase
SR<5:0> >> 2 ! SR<3:0>

Goto step 0

Comments

Restore system mode
and interrupt status

Table 8.3: A MAL Routine for the Execute Phase of the Interpretation of the MIPS-0 kernel
mode Instruction rfe

8.2 Memory Management: Implementing Virtual Memory

8.2.1 Virtual Memory: Implementing a Large Address Space

Next, let us turn our attention to the implementation of the memory address space de(cid:12)ned in
the ISA. The most straightforward approach to is to implement the entire memory address
as system memory (i.e., physical memory). Modern ISAs, however, specify quite a large
address space, spanning 232 (i.e., 4 G) or even 264 locations. Implementing such a large
address space as physical memory is not a worthwhile proposition from the economic point
of view, despite today’s falling memory prices.

The above problem becomes more acute in a multitasked system. Recall from Chapter
5 that modern computer systems incorporate multitasking, permitting multiple processes
to be simultaneously active in the system. The processor is time-multiplexed among the
active processes by the operating system. At the time of a context switch, the current
state of the processor (including the register values) is saved in the kernel address space,
and the processor state is updated with the previously stored state of the process that is
scheduled to run next. It is important to note, however, that a process’ state is not limited
to its processor state; the state includes the memory values too. Therefore, besides saving
and restoring the register values, the memory values also need to be saved and restored.
However, time-multiplexing the system memory is quite impractical, as it requires writing
and reading huge amounts of data to a slow secondary storage device such as a hard disk.
Each context switch will then take seconds or even minutes! Again, a brute-force solution

298

Chapter 8. Microarchitecture | Kernel Mode

would be to provide a separate physical memory for each active process. But such a naive
approach stretches the physical memory requirement even further, especially considering
today’s high degrees of multitasking (64 processes and more). Imagine a computer system
with 64 (cid:2) 4 GB = 256 GB of physical memory!

The above discussion highlights the di(cid:14)culty of implementing the entire memory address
space of one or more processes by physical memory. Most of today’s computer systems
overcome this di(cid:14)culty by using a scheme called virtual memory. In this scheme, the ISA-
de(cid:12)ned memory address space is implemented at the microarchitectural level by a two-tier
memory system, consisting of a small amount of physical memory at the upper tier and
a large amount of swap space at the lower tier, managed by the operating system 2 . The
memory addresses generated by the processor, called virtual addresses, are translated into
physical addresses on the (cid:13)y by a Memory Management Unit (MMU). If a virtual address
refers to a part of the program or data space that is mapped to the physical memory, then
the contents of the appropriate location in the physical memory are accessed immediately.
On the other hand, if the referenced location is mapped to the swap space, then an exception
is generated to turn control over to the operating system, which gets the requested word
after a much longer time. Finally, if the referenced location is not mapped to either the
physical memory or the swap space, then special action needs to be taken by the operating
system. The low-level application programmer, as we saw in Chapter 3, is not aware of the
limitations imposed by the smaller amount of physical memory available.

While the physical memory can be accessed in a few processor cycles, accessing the swap
space takes millions of processor cycles. Because of this huge discrepancy, unless the number
of accesses to the swap space is limited to a very tiny fraction of the total references, the
performance of the system will be very poor. It is imperative that the operating system
does a good job here, and we will see how it does this in the next chapter.

Protection and Sharing: Another important issue in memory management in a multi-
tasking environment is one of protection. When the physical memory is partitioned among
multiple processes, the operating system has to protect itself and the others from accessing
each other’s memory.

Next, let us turn our attention to the implementing the virtual memory concept that
we have been discussing. Recall that the primary purpose of a virtual memory system is to
implement the large address space de(cid:12)ned in modern ISAs in a cost-e(cid:11)ective manner. Let
us (cid:12)rst highlight the main features of this memory system. In a virtual memory system,
the ISA-de(cid:12)ned memory address space is implemented at the microarchitectural level by a
two-tier memory system. This two-tier consists of a small amount of physical memory at
the upper tier and a large amount of swap space at the lower tier. The management of the
system is done by a combination of hardware (the memory management unit (MMU)) and
software (the memory management system of OS). The memory addresses generated by the

2The operating system generally stores the swap space in a hard disk.

8.2. Memory Management: Implementing Virtual Memory

299

Microarchitecture Level: Kernel Mode

ISA Level

Virtual Memory

Microarchitecture Level: User Mode
Updated only in Kernel Mode
Address
Translator

Physical Memory

CPU

Virtual
Address

Swap Space

Memory Mapping Exception

Addresses available in physical memory

Addresses available in swap space

Addresses available nowhere

Figure 8.2: Illustrating the Concept of Virtual Memory

processor, called virtual addresses, are translated into physical addresses on the (cid:13)y by the
MMU. If a virtual address refers to a part of the program or data space that is mapped to
the physical memory, then the contents of the appropriate location in the physical memory
are accessed immediately. On the other hand, if the referenced location is mapped to the
swap space, then the access time will be much longer. Finally, if the referenced location is
not mapped to either the primary memory or the swap space, then special action needs to
be taken by the system. Thus, the low-level application programmer sees a single memory
address space (virtual address space), and is not aware of the limitations imposed by the
smaller amount of physical memory available.

Today’s general-purpose computer systems have physical memory ranging from about 32
MB to about 512 MB, which is less than the memory address space de(cid:12)ned by the ISA for a
single process. The operating system partitions the available physical memory among itself
and the other active processes in the system, although not necessarily in equal portions.
Some portions are common to multiple processes so that they can share common data.
\What belongs to everybody belongs to nobody."
| Spanish Proverb

fbox

300

Chapter 8. Microarchitecture | Kernel Mode

Microarchitecture Level: Kernel Mode

ISA Level

Virtual Memory

Microarchitecture Level: User Mode
Updated only in Kernel Mode
Address
Translator

Physical Memory

CPU

Virtual
Address

Swap Space

Disk

Addresses available in physical memory

Addresses available in swap space

Addresses available nowhere

Figure 8.3: Illustrating the Implementation of Virtual Memory

Whereas the access time of the physical memory equals a few processor cycles, the access
time of the swap space equals millions of processor cycles. Because of this huge discrepancy,
unless the number of accesses to the swap space is a very tiny fraction of the total references,
the performance of the system will be really bad. In order to obtain good performance, the
mapping (from virtual addresses to physical addresses) is dynamically adapted in such a
manner that the locations that were frequently accessed in the recent past (and are therefore
expected to be accessed again in the near future) are mapped to the physical memory. The
infrequently accessed locations are mapped to the swap space.

With the above arrangement, whenever a memory access gets translated to the swap
space, the contents of that location are brought into a suitable location in the physical
memory prior to using them. At that time, if there is no empty space in the physical
memory, then the displaced contents are automatically swapped out to the swap space.
Thus, programs and their data are automatically moved between the physical memory and

8.2. Memory Management: Implementing Virtual Memory

301

secondary storage in such as way as to capture temporal locality among memory accesses.

8.2.2 Paging and Address Translation

To facilitate the implementation of virtual memory and to e(cid:11)ectively capture the spatial
locality present among the memory references (which is very important for obtaining good
performance), the translation from virtual addresses to physical addresses is done at a
granularity much larger than that of individual addresses. Two distinct schemes and their
combinations are widely used|paging and segmentation.

The paging scheme is similar to the cache memory scheme that we saw in the previous
chapter. In this scheme, the translation is done at the granularity of equal (cid:12)xed-size chunks
called pages. Each page consists of a block of words that occupy contiguous addresses in
the address space. It constitutes the basic unit of information that is transferred between
the physical memory and the swap space in the disk whenever the translation mechanism
determines that a transfer is required. (Pages are similar to blocks used in cache memory,
but are much bigger.) The virtual address space is partitioned into virtual pages, and the
physical memory is partitioned into page frames. Page sizes commonly range from 2 KB to
16 KB in size. Page size is an important parameter in the performance of a virtual memory
system.
If the pages are too small, then not much of spatial locality will be exploited,
resulting in too many page faults (i.e., accesses to the secondary storage). Given that the
access time of a magnetic disk is much longer (10 to 20 milliseconds) than the access time
of physical memory, it is very important to keep the number of page faults at an extremely
small value. On the other hand, if pages are too large, it is possible that a substantial
portion of a page may not be used, and temporal locality cannot be exploited to the desired
extent.

Information about the mapping from virtual page numbers to page frame numbers is kept
in tabular form in a structure called page table. Figure 8.4 shows the basic organization
of a page table, along with how it is used in address translation. The page table has
one entry for each virtual page. Each page table entry (PTE) has at least three (cid:12)elds|
Protection (cid:12)eld, Valid bit, and Page Frame Number (PFN). The Protection (P) (cid:12)eld
stores the access rights for the virtual page. The Valid (V) bit indicates if the virtual
page is currently mapped to physical memory, in which case page frame number (PFN) is
available the PFN (cid:12)eld.

The actions involved in translating a virtual address to a physical address are best
expressed using a (cid:13)ow chart. Figure 8.5 presents such a (cid:13)owchart. This (cid:13)owchart starts at
the top left corner. We can conceptually divide this (cid:13)owchart into two parts. The actions
on the left side of the (cid:12)gure are the frequently encountered ones and are best done by the
hardware (MMU). Those on the right are encountered infrequently and are done by the
software (MMS). In this (cid:13)owchart, the page table is assumed to be a hardware structure
that can be read in the Kernel mode as well as in the User mode; of course, writes to the page
table can only be done in the Kernel mode. Later, we will see more modern organizations in

302

Chapter 8. Microarchitecture | Kernel Mode

Virtual Address (Generated by Processor)

Virtual Page Number (VPN)

Offset within Page

0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

01

1

0

0 0 0

0

0

0

0 0 0

11

P

V

Page Frame Number (PFN)

Physical Memory

VP 5

Current Process’
Page Table

0 0

0 0 0 1

1

00

0 0

0
...

VPN 0
VPN 1
VPN 2
VPN 3
VPN 4
VPN 5
VPN 6

VPN 1M−1

Physical Address

0

0

0 0

0

0 0

0 0 0 1

0

0 0 0

0 0

0

0 0 0

11

Page Frame Number (PFN)

Offset within Page

PF 1, offset 3

..
.

...

...

...

location 0
location 1
location 2
location 3

location 4095
location 0
location 1
location 2
location 3

location 4095
location 0
location 1
location 2
location 3

location 4095

PF 0

PF 1

PF 2

location 0
location 1
location 2
location 3

location 4095

PF 2047

Figure 8.4: Virtual Address to Physical Address Translation Using a Page Table

which the page table is stored within the memory address space itself ! While going through
the sequence of actions of the (cid:13)owchart, it is useful to consult Figure 8.4 for a structural
understanding of what goes on.

Given a virtual address, the MMU splits it into two parts|a Virtual Page Number
(VPN) followed by an Offset within Page that speci(cid:12)es the location within the page.
For instance, in Figure 8.4, the VPN corresponding to virtual address 0x5003 is 5. The
MMU uses the VPN value as an index into the page table, and reads the corresponding
page table entry (PTE). If the Valid (V) bit of the PTE is set, then the virtual page is
currently present in the physical memory, and translation continues. This is the frequent
case. If the protection bits of the PTE indicate that the access is not permitted, then the
MMU generates a memory access violation exception to transfer control to the OS, which
generates an error message and terminates the o(cid:11)ending process. If the access is a permitted
one, the MMU calculates the physical address by concatenating PFN and Offset within
Page. The physical memory access is then performed using the physical address obtained.

8.2. Memory Management: Implementing Virtual Memory

303

Determine VPN & Offset

Lookup in PT with VPN

Is VP present
in PM?

Yes

Update PT Access Info

Is this type of 
access permitted?

Yes

Determine PA

Perform Memory Access

Hardware
(MMU)

Is VP present
in SS?

Yes

Copy Page from SS

Allocate new VP

Page Fault Exception

Update PT

Memory Access Violation Exception

Print Error Message

Terminate Process

Software
(MMS)

Figure 8.5: A Flowchart of the Steps Involved in Address Translation with a Page Table

In Figure 8.4, the PFN is 1, and the physical address is 0x1003. This location is shaded in
the (cid:12)gure.

If the Valid bit of the relevant PTE is not set, the requested VPN is not currently
mapped to physical memory, and the MMU generates a page fault exception. The genera-
tion of the exception automatically switches control to the OS, and eventually to the MMS
part of the OS. The MMS performs one of two things: (i) copy the required page from the
swap space to a page frame (if this involves replacing a dirty page, the old page is copied
to the swap space), or (ii) \create" a new page if the virtual page does not yet \exist" (for
example, when a process allocates a new stack frame in an unchartered territory). It then
updates the corresponding PTE to re(cid:13)ect the new translation, and transfers control back
to the interrupted process, which results in re-execution of the instruction that caused the

304

Chapter 8. Microarchitecture | Kernel Mode

page fault exception. Thus, page faults and page table updates are typically handled by
the OS software.

Example: In order to clarify these ideas further, let us go through an example based on
Figure 8.4. The example also gives an idea about the exorbitant space requirement of page
tables. Assume that the ISA (instruction set architecture) speci(cid:12)es a 32-bit address space
in which each address corresponds to a byte. The virtual memory system uses a page size
of 4 KB. The computer has 8 MB of physical memory.

1. How many bits of the 32-bit address are used to determine the Offset within Page?
Because the page size is 4 KB, the number of bits needed to determine the Offset
within Page is log2 4K = 12 bits.

2. How many virtual pages exist?
The virtual memory address space consists of 2 32 bytes. The number of virtual pages
is given by dividing the virtual address space size by the page size, and is therefore
equal to 232 bytes (cid:4) 4 KB = 220 = 1 M.

3. How many page frames exist?
The physical memory consists of 8 MB. The number of page frames is obtained by
dividing the physical memory size by the page size, and is therefore equal to 8 MB (cid:4)
4 KB = 2K.

4. If each PTE occupies 4 bytes, how many bytes will the page table occupy?
The page table has as many entries as the number of virtual pages, namely 1 M. If
each PTE occupies 4 bytes, the page table occupies 1 M (cid:2) 4 bytes = 4 MB.

5. What is the physical address corresponding to virtual address 0x00005003?
The least signi(cid:12)cant 12 bits of the virtual address (0x003) give the O(cid:11)set within Page,
and the remaining bits (0x00005) give the VPN. The MMU then identi(cid:12)es the PTE
corresponding to VPN 5. The V bit of this PTE is set, indicating that the PFN (cid:12)eld is
valid. Therefore, the PFN value is 0x001. On concatenating the PFN with the O(cid:11)set
within Page, we get the physical address of 0x001003.

8.2.3 Page Table Organization

An issue that we have skirted so far is the physical location of the page table. A few
decades ago, when address spaces were much smaller, the page table was small enough to
be stored in the MMU hardware structure, permitting fast address translations. As memory
address spaces grew, page tables grew along with them, eventually forcing system designers
to migrate them to the memory address space itself. Thus, the current practice is to store
the page tables of all active processes in the memory address space. The starting address
of each page table can be either calculated from the ASID (address space ID) or is recorded

8.2. Memory Management: Implementing Virtual Memory

305

in a separate table. But, this approach of storing the page tables in the memory address
space raises three important problems.

First, if the page table is allocated in the virtual address space, accessing the page
table requires an address translation, which in turn requires another translation, and so on,
resulting in an endless cycle of address translations. This problem is dealt with by specifying
a portion of the memory address space to be either untranslated (also called unmapped) or
direct mapped.
In the former case, if a virtual address is within an untranslated region,
then the physical address is the same as the virtual address. In the latter case, if a virtual
address is within a direct-mapped region, then the physical address is obtained by applying
some trivial hashing function to it. By placing the page table in an untranslated/direct-
mapped region, the page table’s physical location can be determined without performing a
page table-based address translation.

The second problem is the size of the page table. For a 32-bit byte-addressable user
address space partitioned into 4 KB pages, the page table has a million (2 32 B (cid:4) 4 KB =
220 ) entries, which requires at least 4 MB of storage. Considering the amount of physical
memory required to store the page tables of several active processes, storing the entire page
tables in the untranslated/direct-mapped address space seems to be a di(cid:14)cult proposition.
Ironically, large portions of the page tables may not be required by an application, and
only a small subset of the virtual pages may be currently mapped to physical pages. One
solution to deal with the size problem mentioned above is to store the user page tables in
the translated portion of the kernel address space, and to store only the kernel page table
(which is then required to access the user page tables) in the untranslated/direct-mapped
address space. Figure 8.6 shows such an organization.

Figure 8.6: Root Page Table

If the root page table is found to be too large, it can be organized in an hierarchical
manner, with only the topmost level of the hierarchy stored in the untranslated/direct-

306

Chapter 8. Microarchitecture | Kernel Mode

mapped address space. Figure 8.7 shows the organization of an hierarchical page table.
The page tables in all of the levels except the bottom-most one serve as directories for the
level immediately below it. The space occupied by each small page table (at each level) is
typically limited to one page. Many recent computer systems implement hierarchical page
tables. For instance, computer systems based on the DEC Alpha 21*64 processor, which
supports a 64-bit address space, typically use a 4-tiered hierarchical page table.

Figure 8.7: An Hierarchical Page Table

8.2.4 Translation Lookaside Bu(cid:11)er (TLB)

The third problem is that every memory reference speci(cid:12)ed in the program requires two
or more memory accesses, all of which except the last one are page table accesses done for
translating the address; the last access is to the translated memory location. The solution
commonly adopted to reduce the number of accesses for translation purposes is to cache the
active portion of the page table in a small, special hardware structure called Translation
Lookaside Bu(cid:11)er (TLB), which is kept within the MMU, as shown in Figure 8.8. Because
each TLB entry covers one page of memory address space, it is possible to get a very high
hit rate from a reasonably small TLB.

8.2. Memory Management: Implementing Virtual Memory

307

Kernel Address Space

e ’ ’

h

c

‘ ‘ C a

Page
Tables

TLB

MMU

Figure 8.8: Relationship between TLB and Page Tables

With the introduction of a TLB, the address translation procedure involves a small
change, as depicted in Figure 8.9. The main change is that the TLB has replaced the page
table as the primary structure used for obtaining address translation. The page table is
still used, but only if the TLB fails to provide a translation.

The TLB can be managed either by the hardware or by the software. We shall (cid:12)rst
discuss the operation when using a hardware-managed TLB. Figure 8.10 presents a (cid:13)owchart
of the actions in such a system. This (cid:13)owchart is a modi(cid:12)cation of the one shown earlier
without a TLB. The main di(cid:11)erence is that after determining the VPN, the MMU looks
in the TLB for the referenced VPN (instead of directly accessing the page table). If the
page table entry for this VPN is found in the TLB, the PFN and protection information are TLB hit
obtained immediately. If the protection bits indicate that the access is not permitted, then
scenario
a memory access violation exception is generated as before. If the access is a permitted one,
the physical address is determined by concatenating PFN and O(cid:11)set within Page, and the
main memory access is performed.

If there is no entry in the TLB for the speci(cid:12)ed VPN, a TLB miss is said to have TLB
occurred. The MMU handles the TLB miss by reading the required PTE from the page
miss sce-
table stored in the kernel address space. If the PTE is obtained, then the MMU updates
nario
the TLB with the new entry. If the page table does not contain a valid PTE for the virtual
page, then a page fault exception is generated as before. The MMS part of the OS handles Page
the page fault exception and then transfers control back to the program that caused the
fault sce-
nario
exception.

308

Chapter 8. Microarchitecture | Kernel Mode

CPU−generated Virtual Address

Virtual Page Number

Offset within Page

Physical Memory

0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

01

1

0

0 0 0

0

0

0

0 0 0

11

Virtual Page Number

VP

Page Frame Number

0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

01

1

1

00

0 0

0 0

0 0 0 1

0
...

VP 5

TLB Miss

Page
Table
Lookup

TLB

Physical Address

0

0

0 0

0

0 0

0 0 0 1

0

0 0 0

0 0

0

0 0 0

11

Page Frame Number

Offset within Page

PF 1, Offset 3

..
.

...

...

...

location 0
location 1
location 2
location 3

location 4095
location 0
location 1
location 2
location 3

location 4095
location 0
location 1
location 2
location 3

location 4095

PF 0

PF 1

PF 2

location 0
location 1
location 2
location 3

location 4095

PF 2047

Figure 8.9: Virtual Address to Physical Address Translation using a TLB

Among the actions given in this (cid:13)owchart, the actions in the leftmost column are per-
formed very frequently, and are critical for performance. These are always implemented in
hardware (MMU). The actions in the rightmost column are done very rarely, and so are
done by software (OS). Thus, actions that are generally implemented in hardware include
VPN determination, TLB hit determination, TLB LRU information update, memory access
restriction enforcement, physical address determination (concatenation of PFN and Offset
within Page, and physical memory access. In other words, the MMU hardware performs
the address translation, as long as the TLB lookups result in hits. Actions that are generally
implemented by the OS include page table creation and page fault handling functions such
as swap area (hard disk) access and page table update.

The actions in the middle column of the (cid:13)owchart deal with TLB miss handling and
TLB replacement, and are done on an occasional basis. With a hardware-managed TLB,
these actions are also done in hardware by the MMU. Some systems, however, implement
these steps in software, resulting in a software-managed TLB. We next take a more detailed

8.2. Memory Management: Implementing Virtual Memory

309

Determine VPN & Offset

Lookup in PT with VPN

Is VP present
in SS?

Yes

Lookup in TLB with VPN

TLB Hit?

TLB Miss

TLB Hit

Is VP present
in PM?

Yes

Page Fault
Exception

Copy Page from SS

Allocate new VP

Update PT

Update TLB with PTE

Memory Access Violation Exception

Update TLB LRU Info

Is this type of 
access permitted?

Yes

Determine PA

Perform Memory Access

Very Frequent

Occasional

Hardware
(MMU)

Print Error Message

Terminate Process

Very Infrequent

Software
(MMS)

Figure 8.10: A Flowchart of the Address Translation Steps with a Hardware-Managed TLB

look at software-managed TLBs.

8.2.5 Software-Managed TLB and the Role of the Operating System in
Virtual Memory

One aspect that must be clear by now is that unlike the implementation of cache memories,
the implementation of the virtual memory concept in modern computers is rarely done
entirely in hardware. Instead, a combination of hardware and software schemes is used,
with the software part being implemented as a collection of OS routines called the memory
management system (MMS). The page tables are usually placed in the kernel address space.

In a virtual memory system implementing a hardware-managed TLB, TLB misses are

310

Chapter 8. Microarchitecture | Kernel Mode

handled in hardware by the MMU. Examples of processors that include a hardware-managed
TLB are the IA-32 family and the PowerPC. The organization of such a system is illustrated
in Figure 8.11(i). In such a system, the TLB is not de(cid:12)ned in the (kernel mode) ISA, and
so the OS does not know about this hardware unit. The TLB manager hardware, which
handles TLB misses, is also unknown to the OS. The page table (PT) is de(cid:12)ned in the
kernel mode ISA so that when handling TLB misses, the TLB manager can perform a PT
lookup | hardware page table walking.

Operating System

Kernel Mode ISA

Hardware

PT Manager

Page Fault Exception

PT

PT

TLB Manager
TLB Miss

TLB

Operating System

PT Manager

Page Fault

PT

TLB Manager

TLB Miss Exception

TLB

Kernel Mode ISA

TLB

Hardware

(i) Hardware−Managed TLB

(ii) Software−Managed TLB

Figure 8.11: Conceptual Organization of a Computer System with (i) a Hardware-Managed
TLB; (ii) a Software-Managed TLB

8.2.5.1 Software-Managed TLB

In a system implementing a software-managed TLB, TLB misses are handled in software
by the MMS part of the OS. Examples of processors that require a software-managed TLB
are MIPS, Sparc, Alpha, and PA-RISC. The organization of such a system is illustrated
in Figure 8.11(ii). In such systems, the TLB speci(cid:12)cation is included in the kernel mode
ISA. A TLB miss in such a system would cause a TLBMiss Exception, which automatically
transfers control to the operating system. The operating system then locates and accesses
the appropriate PTE, updates the TLB with information from this PTE, and transfers

8.2. Memory Management: Implementing Virtual Memory

311

control back to the program that caused the TLB miss.

Figure 8.12 shows a (cid:13)owchart that re(cid:13)ects the working of a virtual memory system using
a software-managed TLB. On comparing this with the (cid:13)owchart of Figure 8.10, we can see
that there are no di(cid:11)erences in the steps themselves, but only in who performs some of the
steps. The steps in the middle column are performed here in software by the MMS part of
the OS, rather than by the MMU hardware.

Determine VPN & Offset

Lookup in PT with VPN

Is VP present
in SS?

Yes

Lookup in TLB with VPN

Is VP present
in PM?

Yes

Page Fault

Copy Page from SS

Allocate new VP

TLB Miss Exception

Update PT

Update TLB with PTE

Memory Access Violation Exception

TLB Hit?

TLB Hit

Update TLB LRU Info

Is this type of 
access permitted?

Yes

Determine PA

Perform Memory Access

Very Frequent

Occasional

Hardware
(MMU)

Software
(MMS)

Print Error Message

Terminate Process

Very Infrequent

Figure 8.12: A Flowchart of the Address Translation Steps with a Software-Managed TLB

The advantages of using a software-managed TLB are two-fold:

(cid:15) The OS can implement virtually any TLB replacement policy.

(cid:15) The algorithm, structure, and format of the page table are not (cid:12)xed, and the OS can
implement any page table organization.

312

Chapter 8. Microarchitecture | Kernel Mode

The disadvantage, of course, is that it takes more time to service a TLB miss. If the OS can
implement TLB replacement policies that reduce the number of TLB misses signi(cid:12)cantly,
then this strategy has an overall advantage.

8.2.6 Sharing in a Paging System

8.2.7 A Real-Life Example: a MIPS-I Virtual Memory System

Computer systems have implemented a wide variety of virtual memory organizations. They
di(cid:11)er primarily in terms of how the page table is organized, and who manages the TLB|
hardware or software. Instead of describing all possible organizations, we have attempted
to cover the fundamental principles behind them. For completeness, we will also look
at a concrete example|the memory management system in a MIPS-I based system|to
see how all of the di(cid:11)erent aspects work together 3 . The MIPS-I ISA supports one of the
simplest memory management organizations among recent microprocessors. It was one of
the earliest commercial ISAs to support a software-managed TLB. This feature implies
that the page table is not de(cid:12)ned in the kernel mode ISA, and that the OS is free to choose
a PT organization. Some support is provided, nevertheless, for implementing a simple
linear PT organization in the kernel’s virtual address space. Similarly, although the OS
is free to implement any replacement policy for the TLB, the ISA provides some support
for a random replacement policy. In the ensuing discussion, we (cid:12)rst describe the MIPS-I
kernel address space, and then the support provided in the kernel ISA to implement virtual
memory. Finally, we show how these ISA features can be used in a possible virtual memory
system organization for MIPS-I based systems.

8.2.7.1 Kernel Mode ISA Support for Virtual Memory

We shall start with the kernel address space. The MIPS-I ISA speci(cid:12)es a 32-bit kernel
address space. Part of the address space is unmapped so that accesses to this part can
proceed without doing a table lookup. The 4 GB address space is divided into 4 segments,
as illustrated in Figure 8.13. These segments have di(cid:11)erent mapping characteristics and
serve di(cid:11)erent functions:

(cid:15) kuseg: This 2 GB segment is cacheable and mapped to physical addresses. In the
kernel mode, accesses to this segment are treated just like user mode accesses. This
serves as a means for the kernel routines to access the code and data of the user
process on whose behalf they are running. Thus, translations for this segment are
available in the corresponding user page table (stored in kseg2).

3Our discussion of the MIPS-I virtual memory system is detailed enough to get a good appreciation of
the fundamental issues involved. However, it does not cover every nuance that a MIPS OS developer needs
to be aware of to develop the MMS part of the OS. Additional details can be obtained from sources such as
[?].

8.2. Memory Management: Implementing Virtual Memory

313

Kernel Address Space

Addresses
0x0000 0000

kuseg
2 GB

Mapped
Cacheable

kseg0
512 MB

kseg1
512 MB

Unmapped
Cached

Unmapped
Uncached

kseg2
1 GB

Mapped
Cacheable

0x8000 0000

0xa000 0000

0xc000 0000

0xffff ffff

Figure 8.13: Functionality-based Partitioning of the MIPS Kernel Address Space into Seg-
ments

(cid:15) kseg0: This 512 MB segment is cached and unmapped. Addresses within this segment
are direct-mapped onto the (cid:12)rst 512 MB of physical address space without using the
TLB. This segment is typically used for storing frequently executed parts of the OS
code|such as the exception handlers|and some kernel data such as the kernel page
table (which stores the mapping for the kseg2 segment). The TLB miss handler is
stored in this segment.

(cid:15) kseg1: This 512 MB segment is uncached and unmapped. It is also direct-mapped
to the (cid:12)rst 512 MB of physical address space without using the TLB. Unlike kseg0,
however, this segment is uncacheable. It is used for the boot-up code, the IO registers,
and disk bu(cid:11)ers, all of which must be unmapped and uncached.

(cid:15) kseg2: This 1 GB segment is cacheable and mapped to arbitrary physical addresses,
like kuseg. Translations for this segment are stored in the kernel’s page table (root
page table. This segment is typically used to store the kernel stack, \U-area", user
page tables, and some dynamically allocated data areas.

The MIPS-I kernel mode ISA supports a paged virtual memory with 4 KB sized pages.
A 32-bit address can thus be split into a 20-bit VPN part and a 12-bit page o(cid:11)set part. The

314

Chapter 8. Microarchitecture | Kernel Mode

ISA also speci(cid:12)es a uni(cid:12)ed 64-entry, fully-associative TLB. The OS loads page table entries
(PTEs) into the TLB, using either random replacement or speci(cid:12)ed placement. Among the
64 TLB entries, eight are wired entries; these do not get replaced when doing a random
replacement. This wiring feature allows the OS to store a few important PTEs, such as the
frequently required root PTEs. Figure 8.14 shows the format of a TLB entry. It has the
following (cid:12)elds/bits:

VPN

20

ASID

6

PFN

20

N D V G

1 1 1 1

VPN
ASID
PFN

: Virtual Page Number
: Address Space ID
: Physical Frame Number

N
D
V
G

: Noncacheable
: Dirty
: Valid
: Global

Figure 8.14: A MIPS-I TLB Entry

(cid:15) VPN (Virtual Page Number):

(cid:15) ASID (Address Space ID):

(cid:15) PFN (Physical Frame Number):

(cid:15) N (Noncacheable): This bit indicates if the mapped page is noncacheable. If it is set,
the processor sends the address to main memory, bypassing the cache.

(cid:15) D (Dirty): This bit indicates if the mapped page has been updated by the process.

(cid:15) V (Valid): This bit indicates if the entry contains a valid mapping.

(cid:15) G (Global): This bit indicates if the mapped page is shared. If it is set, the MMU does
not perform an ASID match while doing a TLB lookup.

TLB-Related Exceptions: TLB misses in a MIPS-I system are conveyed to the OS as
an exception. We can group the TLB-related exceptions into 3 categories:

(cid:15) UTLBMISS (User mode TLB Miss): This exception category includes cases where no
matching entry has been found in the TLB for an address in the kuseg portion of the
address space.
Such exceptions can occur for instruction fetch, data read, or data
write. This is the exception that is likely to occur the most frequently.

(cid:15) TLBMISS (Kernel mode TLB Miss): This category cases where either a TLB miss
has occurred to the kseg2 portion of the address space, or a matching TLB entry was
found with the Valid bit not set.

8.2. Memory Management: Implementing Virtual Memory

315

(cid:15) TLBMOD: This exception indicates that a TLB hit has occurred for a store instruction
to an entry whose Dirty bit is not set. The purpose is to let the OS set the Dirty bit
of the TLB entry.

Among these 3 categories, the (cid:12)rst one has its own exception vector (0x80000000) because
it is the one that occurs most frequently among the TLB-related exceptions. The last 2
categories are grouped into a single exception vector (0x80000080).

Privileged Registers: When a TLB miss exception occurs, information about the ex-
ception event (such as the faulting virtual address) needs to be conveyed to the OS. It is cus-
tomary to use a privileged register to store this information. Similarly, the OS might require
additional special registers for manipulating the TLB. The MIPS kernel mode ISA provides
6 privileged registers|EntryHi, EntryLo, Index, Random, BadVAddr, and Context|for
conveying additional information regarding TLB miss exceptions to the OS and for the OS
to modify the TLB. These additional registers are depicted in Figure 8.15, and are described
below:

(cid:15) EntryHi: This register is used to store a VPN and an ASID (Address Space IDenti(cid:12)er).

(cid:15) EntryLo: This register is used to store a PFN and status information. The information
in a TLB entry is equivalent to the concatenation of EntryHi and EntryLo values.

(cid:15) Index: This register is used by the OS to store the index value to be used for accessing
a speci(cid:12)c TLB entry. The 6-bit Index (cid:12)eld of this register can store values between
0-63.

(cid:15) Random: This register holds a pseudo-random value, which is used as the TLB index
when the OS performs random replacement of TLB entries. The MMU hardware
automatically increments it every clock cycle so that it serves as a pseudo-random
generator.

(cid:15) BadVAddr: The MMU hardware uses this register to save the bad virtual address that
caused the latest addressing exception.

(cid:15) Context: This register is used to store some information that facilitates the handling
of certain kinds of exceptions such as TLB misses. The PTEBase (cid:12)eld is set by the
OS, normally to the high-order address bits of the current user ASID’s page table,
located in the kseg2 segment of the kernel address space. The BadVPN (cid:12)eld is set by
the MMU hardware when the addressing exception occurs.

Privileged Instructions: The kernel mode ISA also provides the following privileged
instructions for the OS to re(cid:12)ll the TLB:

316

Chapter 8. Microarchitecture | Kernel Mode

P

1

VPN

20

PFN

20

0

17

0

18

ASID

6

0

6

N D V G

1 1 1 1

Index

6

Random

6

0

8

0

8

0

8

EntryHi

EntryLo

Index

Random

BadVAddr

BadVAddr

32

PTEBase

11

BadVPN

19

Context

0

2

Figure 8.15: Privileged Registers Present in the MIPS-I Kernel Mode ISA for Supporting
Virtual Memory

(cid:15) tlbwr: Write the PTE present in EntryHi-EntryLo pair onto the TLB at index
provided in register Random. This instruction is used by the OS to insert a PTE
randomly into the TLB.

(cid:15) tlbwi: Write the PTE present in EntryHi-EntryLo pair onto the TLB entry indexed
by register Index. This instruction is used by the OS to insert a PTE at a speci(cid:12)ed
index in the TLB.

(cid:15) tlbr: Read TLB entry indexed by register Index, and place the information into
EntryHi and EntryLo registers.

TLB Operation: Let us put in a nutshell the operation of the TLB hardware. If the
system is in the Kernel mode, all accesses to the kseg0 or kseg1 segments bypass the TLB,
and are direct-mapped as described earlier. If the access is to kuseg (in either mode) or
to kseg2, then the MMU determines the VPN and associatively compares it against the
VPN (cid:12)eld of all TLB entries. If there is a match, and if either the ASID (cid:12)eld of EntryHi
register matches that of the matching TLB entry or if the TLB entry’s Global bit is set,
then translation can continue. If no such entry is found in the TLB, a UTLBMISS exception

8.2. Memory Management: Implementing Virtual Memory

317

is generated if the address is to kuseg, and a TLBMISS exception is generated if the address
is to kseg2.
If a matching TLB entry was found, but its Valid bit was not set, then a
TLBMISS exception is generated, irrespective of whether the address is to kuseg or kseg2.
In all of these cases, the faulting VPN is placed in the Context register.

8.2.7.2 Operating System Usage of Machine Features

With the above background on the features provided in the MIPS-I kernel ISA to support
virtual memory, let us turn our attention to the barebones of a typical MMS used in MIPS-I
based systems. This MMS organizes each user page table as a simple linear array of PTE
entries. Each PTE is 4 bytes wide, and its format matches the bit(cid:12)elds of the EntryLo
register. Each user ASID’s PT requires 2 MB of space. All of the user ASID page tables
are placed in the kseg2 part of the kernel address space. The kernel PT stores the mapping
for the kseg2 segment only. The entire kernel PT (part of which contains the root PTEs) is
stored in kseg0. The placement of the page tables is pictorially shown in Figure 8.16. Each
kernel PTE stores the current mapping for 4 KB of kseg2. If the kernel PTE happens to
be a root PTE, then this 4 KB stores 1 K entries of a user page table, and that root PTE
e(cid:11)ectively covers 1 K user PTEs, or 1 K (cid:2) 4 KB = 4 MB of the user address space.

The OS makes sure that whenever a user process is being run, the ASID (cid:12)eld of the
EntryHi register re(cid:13)ects the user ASID and the PTEBase (cid:12)eld of the Context register re(cid:13)ects
the base virtual address of the user page table.

A UNIX-like OS typically uses the 8 wired entries of the TLB for storing kernel PTEs as
follows: one for a kernel PTE that maps 4 KB of the kernel stack, one for a kernel PTE that
maps 4 KB of the U-area, one each for a root PTE that covers 4 MB each of the currently
active user ftext, data, stackg, and up to 3 more for root PTEs that cover 4 MB each of
user data.

We shall focus our discussion on UTLBMISS, the most frequent TLB-related exception.
When a UTLBMISS exception occurs, the Context register gives the kseg2 virtual address of
the user PTE to be read to service the TLB miss. When the UTLBMISS handler attempts
to read the PTE present at this kseg2 virtual address, an address translation is required
because kseg2 is a mapped segment.
If a TLB entry is available for this kseg2 virtual
address (most likely, this should be one of the wired entries in the TLB), then the OS can
successfully access the required user PTE with just one memory access. In the worst case,
this TLB lookup for a kseg2 virtual address can cause a TLBMISS exception. The TLBMISS
handler will read the root PTE stored in the unmapped kseg0 segment, enter this root
PTE in the TLB, and return control back to the UTLBMISS handler.

After obtaining the required user PTE from kseg2, the UTLBMISS handler updates the
TLB with this PTE. Assembly language code for an UTLBMISS handler is given below.
Notice that the rfe instruction (at the end), which puts the system back in the user mode
is in the delay slot of the jr $k1 instruction, which speci(cid:12)es the transfer of control back to
the user program that caused the TLB miss.

318

Chapter 8. Microarchitecture | Kernel Mode

Addresses
0x8000 0000

0x8000 0080

kseg0

UTLBMISS Handler

Other
Exception
Handlers

0x9ff0 0000

0x9ff2 0000

0x9fff ffff

Root PTEs
Kernel PTEs
1 MB

512 MB
Unmapped, Cached

kseg2

PT for ASID 0
2 MB

PT for ASID 1
2 MB

Addresses
0xc000 0000

0xc020 0000

0xc040 0000

PT for ASID 63
2 MB

0xc7e0 0000

0xc800 0000

The PTE in kseg0 stores the mapping

for the virtual page in kseg2

Kernel Stack

1 GB
Mapped, Cacheable

0xffff ffff

Figure 8.16: Placement of Kernel Page Table and User Page Tables in the kseg0 and kseg2
Segments of the MIPS Kernel Address Space

0x80000000: mfc0

$k0, $context

mfc0

$k1, $epc

lw

$k0, 0($k0)

mtc0
tlbwr

jr
rfe

$k0, $EntryLo

$k1

# Copy context register contents (i.e., kseg2 virtual
# address of required user PTE) into GPR k0
# Copy epc contents (address of faulting instruction)
# into GPR k1
# Load user PTE from kseg2 addr to GPR k0
# This load can cause a TLBMISS exception!
# Copy the loaded PTE into EntryLo register
# Write the PTE in EntryLo register into TLB
# at slot number speci(cid:12)ed in Random register
# Jump to address of faulting instruction
# Switch to user mode

8.2. Memory Management: Implementing Virtual Memory

319

An Assembly Language UTLBMISS Handler for a MIPS-I System

User Mode

ML Program

Microroutine

Kernel Mode

ML Program
(UTLB Miss Handler)

lw

$2, 4($3)

Interpretation

Fetch instruction
Read rs and rt registers
Calculate address
Lookup in TLB
Access physical memory
End

M I S

B

L

T

U

0x8000 0000: mfc0
mfc0
lw
mtc0
tlbwr
rfe
jr

k0, context
k1, epc
k0, 0(k0)
k0, EntryLo

k1

Figure 8.17: A Program Execution View of a UTLBMISS Exception

8.2.8

Interpreting a MIPS-I Memory-Referencing Instruction

In the previous chapter, we had looked at the interpretation of a memory-referencing in-
struction, without considering the address translation process. The calculated (virtual)
address was directly copied to MAR, from where it was supplied to the memory subsystem.
In this section, we shall brie(cid:13)y revisit the interpretation of memory-referencing instructions,
by considering the address translation process also. Again, we tailor the discussion to the
MIPS-I ISA. For ease of understanding, we modify the familiar data path of Figure 8.1
to include a memory management unit. The modi(cid:12)ed data path is given in Figure 8.18.
It includes a microarchitectural register called VAR for holding the virtual address to be
translated. It also includes a software-managed TLB. The microarchitectural register MAR
now consists of 2 (cid:12)elds. The Offset (cid:12)eld is directly updated from the Offset (cid:12)eld of VAR.
The PFN (cid:12)eld can be updated either from the TLB or directly from the VPN (cid:12)eld of VAR.

Table 8.4 gives a modi(cid:12)ed MAL routine for interpreting a MIPS lw instruction in this
data path. In step 6, the calculated virtual address is copied from AOR to VAR (instead of
MAR). In step 7, the VPN (cid:12)eld of this address is supplied to the TLB. If it results in a TLB
miss, then a TLB miss exception is activated by transfering control to MAL step 10. On
the other hand, if there is a TLB hit|which will be the frequent case|the PFN (cid:12)eld of the
concerned TLB entry is copied to MAR’s PFN (cid:12)eld. The Offset (cid:12)eld of VAR is also copied
to the corresponding (cid:12)eld of MAR.

The TLB miss exception is initiated in steps 10-15. This part of the routine is similar to
the initiation of the over(cid:13)ow exception that we saw earlier. The main points to note are: (i)

320

Chapter 8. Microarchitecture | Kernel Mode

To Control Unit

Memory Interface

Cause

VAR

VPN

Offset

VPN

PFN

SR

EPC

Shift

PRE_PC

PC

IR

Instr.
Decoder

32

Register Address

5

Register File
(RF)

TLB

PFN
Offset
MAR

s
u
B
 
r
o
s
s
e
c
o
r
P

Register Data

MDR

ALU

Flags

Processor Data Path

Kernel Mode
Registers

System Address Bus

Memory
Subsystem

System Data Bus

Figure 8.18: A Data Path for Implementing the MIPS-0 Kernel Mode ISA Including Virtual
Memory

the BadVPN (cid:12)eld of Context register is updated with the virtual page number that missed
in the TLB; (ii) PC is updated with 0x80000000 or 0x80000080 depending on whether the
TLB miss occurred in the user mode or the kernel mode, respectively.

8.2.9 Combining Cache Memory and Virtual Memory

The discussion of virtual memory clearly parallels the cache memory concepts discussed in
the previous chapter. There are many similarities and some di(cid:11)erences. The cache memory
bridges the speed gap between the processor and the main memory and is implemented
entirely in hardware. The virtual memory mechanism bridges the size gap between the
main memory and the virtual memory and is usually implemented by a combination of

8.3.

IO System Organization

321

Step
No.

4
5
6
7

8
9

10

11
12
13
14

15

MAL Instruction
for Data Path
Execute phase
R[rs] ! AIR
SE(offset) + AIR ! AOR
AOR ! VAR
if (VAR.VPN misses in TLB)
goto step 10
else TLB[hit index].PFN ! MAR.PFN;
VAR.Offset ! MAR.Offset
M[MAR] ! MDR
if (rt) MDR ! R[rt]
Take TLB Miss Exception
SR<3:0> << 2 ! SR<5:0>

Comments

Goto step 0

Set system in kernel mode
and disable interrupts
Save instruction’s address
Prev PC ! EPC
VAR ! BadVAddr
Save bad virtual address
VAR.VPN ! Context.BadVPN Save bad VPN
Set PC to 0x80000000
0x80000000 | (SR<3> << 6) ! PC
or 0x80000080
Place TLBL code in cause
Goto step 0

2 ! Cause.ExcCode

Table 8.4: A MAL Routine for the Execute Phase of the Interpretation of the MIPS-0 ISA
Instruction Represented Symbolically as lw rt, offset(rs). This MAL Routine is for
executing the ML instruction in the Data Path of Figure 8.18

hardware and software techniques. Table 8.5 provides a succinct comparison of the two
concepts and their typical implementations.

The descriptions of cache memory and virtual memory were given in isolation.

8.3

IO System Organization

Another ma jor component of the kernel mode microarchitecture is the IO system, which
houses the IO devices, their interfaces, and their interconnections.
In terms of size, the
IO system is usually the largest part of any computer. This is because the IO devices are
electromechanical devices, which cannot be easily miniaturized. The IO interfaces also tend
to be bulky, as they are built out of several integrated circuits (IC chips). Moreover, most
of the IO interconenctions are made out of cables that are external to the IC chips.

The IO system performs the following functions:

322

Chapter 8. Microarchitecture | Kernel Mode

Cache Memory
No.
1. Cache hit time is small (1-2 cycles)

Virtual Memory
Physical memory hit time is large
(10-100 cycles)
2. Cache miss time is large (10-100 cycles) Page fault handling time is very large
3. Block size is small (4-32 words)
Page size is large (1K-1M words)
Usually fully-associative mapping or
4. Usually direct mapping or
high level of associativity
low level of associativity
On a page fault, the OS loads
5. On a cache miss, the cache hardware
loads the block from main memory
the page from disk to main memory
to cache memory
6. The cache memory is usually not
visible in the ISA

The virtual memory is
visible in the kernel mode ISA

Table 8.5: A Succinct Comparison of Cache Memory and Virtual Memory

(cid:15) Implement the IO address space de(cid:12)ned in the kernel mode ISA

(cid:15) Implement the IO interface protocols of the IO devices

(cid:15) Implement the IO devices themselves

The (cid:12)rst two functions are de(cid:12)nitely related, and we shall explain them in more detail.
Implementation of various IO devices is discussed in Appendix **.

8.3.1

Implementing the IO Address Space: IO Data Path

We shall begin our discussion of the IO system with the topic of implementing the IO address
space de(cid:12)ned in the kernel mode ISA, as we saw in Section 4.3. This address space can be
part of the memory address space (as in memory-mapped IO) or a separate address space
(as in independent IO). Irrespective of whether the IO register space is memory-mapped
or independently mapped, the hardware registers that implement this name space are built
separately from the memory system. When the processor executes an IO instruction|an
instruction that accesses the IO address space|it issues a signal indicating that the address
on the system bus is an IO address.

Unlike monolithic structures such as the register (cid:12)le used to implement the general-
purpose registers, the structures used to implement the IO address space are distributed.
That is, the set of hardware registers implementing the IO address space is distributed over
di(cid:11)erent hardware structures, depending on their function. For instance, the IO registers
that relate to the keyboard device will be physically located within the keyboard interface
module, and those that relate to the mouse will be physically located within the mouse

8.3.

IO System Organization

323

interface module. This type of distribution is required because the behavior of IO registers
vary, depending on the IO device they relate to, as discussed in Section 4.3 The number of
IO addresses allotted for an IO device is generally small, of the order of a few tens or less;
an exception is video memory.

Design of the IO data path is a very complex sub ject, perhaps even more complex than
that of the processor data path and the memory data path. Therefore, we shall present
this topic in a step by step manner, beginning with very simple data paths and steadily
increasing the complexity. The simplest IO data path we present consists of a single system
bus that connects several IO registers as shown in Figure ****.

GIVE MAL ROUTINE FOR an lw instruction that is an IO instruction Let this MAL
routine be related to the one given in virtual memory

GIVE MAL ROUTINE for a MIPS-like in instruction

When comparing the register (cid:12)le and the IO hardware registers, besides physical dis-
tribution, there is another ma jor di(cid:11)erence: a register in a register (cid:12)le has a (cid:12)xed address
whereas an IO hardware register may not have a (cid:12)xed address. Like a physical memory
location in a virtual memory system, an IO hardware register also can map to di(cid:11)erent
logical IO register addresses at di(cid:11)erent times.

In this section, we give an overview of selected aspects of computer IO and communica-
tion between the processor and IO devices. Because of the wide variety of IO devices and
the quest for faster handling of programs and data, IO is one of the most complex areas of
computer design. As a result, we present only selected pieces of the IO puzzle. We start
with a discussion of IO interface modules, which are pivotal in implementing the IO regis-
ters speci(cid:12)ed in the kernel mode ISA. These interface modules come in various kinds, and
provide di(cid:11)erent types of ports for connecting various IO devices. We consider the interface
module for connecting a keyboard as an illustration. We then introduce the system bus and
other types of buses to connect the IO interface modules to the processor-memory system.

8.3.1.1 System Bus

The IO hardware registers must be connected to the rest of the data path | the processor
and memory system | for them to be accessible by IO instructions. The most convenient
way to carry out this connection is to extend the processor-memory bus that we used earlier
to connect the processor and memory system. When a single bus is used in this manner
to connect all of the devices|the processor, the memory, and the IO registers|the bus is
usually called a system bus. Such a connection was illustrated in Figure 8.19.

8.3.2

Implementing the IO Interface Protocols: IO Controllers

Unlike the general-purpose registers and the memory locations, the IO registers are generally
implemented in a distributed manner, using special register elements inside the di(cid:11)erent IO

324

Chapter 8. Microarchitecture | Kernel Mode

interface modules. To perform the functionality speci(cid:12)ed in an IO port speci(cid:12)cation, an
IO interface module is used. The interface module that implements the port interacts with
the appropriate IO devices. From an IO device’s viewpoint, its interface module serves as
a conduit for connecting it to the rest of the computer system. An IO interface module
performs one or more of the following functions:

(cid:15) It interacts with IO devices to carry out the functionality speci(cid:12)ed for the IO port
that it implements. For example, a keyboard port may specify that whenever its
status bit is set, its data register will have a code value that corresponds to the last
key depressed or released in a keyboard.

(cid:15) It performs conversion of signal values and data formats between electromechanical
IO devices and the processor-memory system.

(cid:15) It performs necessary synchronization operations for correct transfer of data from slow
IO devices.

(cid:15) It performs necessary bu(cid:11)ering of data and commands for interfacing with slow IO
devices.

(cid:15) It performs direct transfer of data between Io devices and memory.

One side of an IO interface module connects to a system bus, and the other side connects
to one or more IO devices via tailored data links, as shown in Figure 8.19. The IO interface
module can be thought of as a small processor that has its own register set and logic
circuitry.

It is worthwhile to point out that most of the IO devices are electromechanical devices,
with some electronic circuitry associated with it. This circuitry carries out functions that
are very speci(cid:12)c to that device. For example, in a printer, this circuitry controls the motion
of the paper, the print timing, and the selection of the characters to be printed. The printer
interface module does not carry out these functions. Thus, an interface module directly
interacts only with the electronic circuitry, and not with the mechanical parts of the device.

When an IO write instruction writes a value to an interface module’s status/control
register, it is interpreted by the module as a command to a particular IO device attached
to it; and the module sends the appropriate command to the IO device.

8.3.3 Example IO Controllers

The IO controllers consist of the circuitry required to transfer data between the processor-
memory system and an IO device. Therefore, on the processor side of the module we have
the IO registers that are part of the kernel mode ISA, along with circuitry to interact
with the bus to which it is connected on the processor side. On the device side we have a
data path with its associated controls, which enables transfer of data between the module

8.3.

IO System Organization

System Bus

325

Address
Data
Control

CPU

Main
Memory

Status/Control Register

Status/Control Register

Data Register

Data Register

I/O Interface

I/O Interface (Disk Controller)

Links to Peripheral Devices

Figure 8.19: Use of IO Interface Modules to Implement IO Ports to Connect IO Devices

and an IO device. This side may be either designed or programmed at run-time to be
device-dependent.

8.3.4 Frame Bu(cid:11)er:

aka Display memory and Video memory.

A framebu(cid:11)er is a video output device that drives a video display from a memory bu(cid:11)er
containing a complete frame of data. The information in the bu(cid:11)er typically consists of
color values for every pixel (point that can be displayed) on the screen. Color values are
commonly stored in 1-bit monochrome, 4-bit palettized, 8-bit palettized, 16-bit highcolor
and 24-bit truecolor formats. An additional alpha channel is sometimes used to retain
information about pixel transparency. The total amount of the memory required to drive
the framebu(cid:11)er depends on the resolution of the output signal, and on the color depth and
palette size.

Before an image can be sent to a display monitor, it is (cid:12)rst represented as a bit map in
an area of video memory called the frame bu(cid:11)er. The amount of video memory, therefore,
dictates the maximum resolution and color depth available.

With a conventional video adapter, the bit map to be displayed is (cid:12)rst generated by
the computer’s microprocessor and then sent to the frame bu(cid:11)er. Most modern video
adapters, however, are actually graphics accelerators. This means that they have their own
microprocessor that is capable of manipulating bit maps and graphics ob jects. A small

326

Chapter 8. Microarchitecture | Kernel Mode

amount of memory is reserved for these operations as well.

Because of the demands of video systems, video memory needs to be faster than main
memory. For this reason, most video memory is dual-ported, which means that one set of
data can be transferred between video memory and the video processor at the same time
that another set of data is being transferred to the monitor. There are many di(cid:11)erent types
of video memory, including VRAM, WRAM, RDRAM, and SGRAM. The standard VGA
hardware contains up to 256K of onboard display memory. While it would seem logical that
this memory would be directly available to the processor, this is not the case. The host CPU
accesses the display memory through a window of up to 128K located in the high memory
area.
(Note that many SVGA chipsets provide an alternate method of accessing video
memory directly, called a Linear Frame Bu(cid:11)er.) Thus in order to be able to access display
memory you must deal with registers that control the mapping into host address space. To
further complicate things, the VGA hardware provides support for memory models similar
to that used by the monochrome, CGA, EGA, and MCGA adapters. In addition, due to
the way the VGA handles 16 color modes, additional hardware is included that can speed
access immensely. Also, hardware is present that allows the programer to rapidly copy data
from one area of display memory to another. While it is quite complicated to understand,
learning to utilize the VGA’s hardware at a low level can vastly improve performance.
Many game programmers utilize the BIOS mode 13h, simply because it o(cid:11)ers the simplest
memory model and doesn’t require having to deal with the VGA’s registers to draw pixels.
However, this same decision limits them from being able to use the infamous X modes, or
higher resolution modes.

Host Address to Display Address Translation The most complicated part of accessing
display memory involves the translation between a host address and a display memory
address. Internally, the VGA has a 64K 32-bit memory locations. These are divided into
four 64K bit planes. Because the VGA was designed for 8 and 16 bit bus systems, and due
to the way the Intel chips handle memory accesses, it is impossible for the host CPU to
access the bit planes directly, instead relying on I/O registers to make part of the memory
accessible. The most straightforward display translation is where a host access translates
directly to a display memory address. What part of the particular 32-bit memory location
is dependent on certain registers.

8.3.4.1 Universal Asynchronous Receiver/Transmitter (UART)

The transfer of data between two blocks may be performed in parallel or serial. In parallel
data transfer, each bit of the word has its own wire, and all the bits of an entire word are
transmitted at the same time. This means that an N -bit word is transmitted in parallel
through N separate conductor wires. In serial data transmission, bits in a word are sent in
sequence, one at a time. This type of transmission requires only one or two signal lines. The
key feature of a serial interface module is a circuit capable of communicating in bit-serial
fashion on the device side (providing a serial port) and in bit-parallel fashion on the bus side

8.3.

IO System Organization

327

(processor side). Transformation between parallel and serial formats is achieved with shift
registers that have parallel access capability. Parallel transmission is faster, but requires
many wires. It is used for short distances and when speed is important. Serial transmission
is slower, but less expensive, because it requires only one conductor wire.

The UART is a transceiver (transmistter/receiver) that translates data between par-
allel and serial interfaces.

The UART sends a start bit, (cid:12)ve to eight data bits with the least-signi(cid:12)cant-bit (cid:12)rst,
an optional parity bit, and then one, one and a half, or two stop bits. The start bit is the
opposite polarity of the data-line’s idle state. The stop bit is the data-line’s idle state, and
provides a delay before the next character can start. (This is called asynchronous start-stop
transmission).

8.3.4.2 DMA Controller

DMA transfers are performed by a special device controller called a DMA controller.
The DMA controller performs the functions that would normally be performed by the
processor when accessing the main memory. For each word transferred, it provides the
memory address and all the bus signals that control data transfer. The DMA controller
also increments the memory address after transferring each word, and keeps track of the
number of words transferred. A DMA controller may handle DMA operations for a number
of IO devices, or may be dedicated to a single IO device. The latter controller is called a
bus-mastering DMA controller.

Some computer systems permit DMA operations between two IO devices without in-
volving main memory. For example, a block transfer can be performed directly between
two hard disks, or from a video capture device to a display adapter. Such a DMA opera-
tion can improve system performance, especially if the system provides multiple buses for
simultaneous transfers to happen in di(cid:11)erent parts of the system.

8.3.4.3 Keyboard Controller

We shall look at a speci(cid:12)c example|an interface module for a keyboard|to get a better
appreciation of what IO interface modules are and how they work. An interface module
that connects a keyboard is one of the simplest and most commonly used interface modules
in a general-purpose computer. To study its design and working, we need to know a little
bit about how the keyboard device and the associated controller circuitry work.

Figure 8.20 shows a keyboard unit, along with its connections to the interface module.
Whenever a key is depressed or released, a corresponding K-scan code is generated and
sent to the encoder. The encoder converts the K-scan code to a more standard scan code,
and sends it to the interface module|housed in the system unit|usually through a serial
keyboard cable. Notice that if a key is depressed continuously, after a brief period the

328

Chapter 8. Microarchitecture | Kernel Mode

keyboard unit repeatedly sends the scan code for the key’s make code until the key is
released.

Data
Address
Control

IO Interface Module

Interrupt
Interrupt Acknowledge

Read/Write

D
e
c
o
d
e
r

Data Register

Status Register

Control
Logic

Serial-Parallel
Converter

Keyboard Cable

Keyboard Unit

Encoder

Figure 8.20: Role of IO Interface Module for Connecting a Keyboard Unit

With this background on the working of a keyboard unit, let us summarize the ma jor
functions the interface module needs to perform:

(cid:15) Incorporate the IO registers (a small portion of the IO address space) that are part
of the keyboard port.

(cid:15) Convert the scan code arriving serially from the keyboard unit into parallel form, and
place it in an IO register to be read by the processor.

(cid:15) Maintain the status of the keyboard circuitry|interface module and the keyboard
device|in an IO register to be read by the processor.

(cid:15) Generate an interrupt at the occurrence of keyboard-related events, such as updation
of the data register with a scan code.

(cid:15) Process processor commands, such as varying the keyboard’s typematic rate and delay.
This provides a means for the keyboard user to specify to the OS the desired repetition
rate.

8.3.

IO System Organization

329

Figure 8.20 also shows the internals of an interface module that can perform the above
functions4 . It incorporates a data register, a status register 5, an address decoder, a
serial-parallel converter, and control logic. The module works as follows. Upon receiving
a scan code from the keyboard unit, the serial-parallel converter converts it to parallel
form, and places the code in the data register. The control logic makes appropriate
modi(cid:12)cations to the status register, and raises an interrupt to inform the presence of a
valid value in the data register.

The interrupt causes the execution of the ISR part of the keyboard device driver, which
reads the input data register, and copies it to the corresponding process’ keystroke
bu(cid:11)er. The scan code is then converted into ASCII code or other meaningful data by the
systems software, which usually also displays the character on the monitor.

When the keyboard device driver wants to send a command, such as varying the repe-
tition rate, it executes IO write instructions to update the data register and the status
register. The control logic, upon sensing the update, conveys the information to the
serial-parallel converter, which converts it to serial form and sends it to the keyboard unit.

8.3.5

IO Con(cid:12)guration: Assigning IO Addresses to IO Controllers

Within a computer system, from the point of view of the processor (and the device drivers),
an IO controller is uniquely identi(cid:12)ed by the set of IO addresses assigned to its hardware reg-
isters. An IO instruction would specify an IO address, and the corresponding IO controller
would respond.

An issue that we have skirted so far is: how are IO addresses assigned to IO controllers?
And, how does an IO controller know the addresses assigned to its IO registers? A simple-
minded approach is for the kernel mode ISA to specify the addresses assigned to each
IO register. For example, kernel mode addresses 0x a0000000 - 0x a00000007 can be
earmarked for the mouse controller, addresses 0xa00000008 - 0xa000000ff for the keyboard,
and so on. The device drivers would then be written with these addresses hardcoded into
them. The designer of an IO controller can also hardcode addresses into the design so that
its address decoder recognizes only the IO addresses assigned to it. However, this approach
precludes standardization of IO controllers across di(cid:11)erent computer families, driving up
controller costs.

A slightly better option is not to hardcode the addresses in the IO controller, but provide
an ability to manually enter the addresses by setting switches or jumper connections on the
controller, before installing the controller in a computer system. Once the controller knows
the addresses assigned to it, its decoder can speci(cid:12)cally look for those addresses in the

4 In XT systems, for example, this function is implemented by an Intel 8255A-5 or compatible keyboard
interface chip.
5The number of registers incorporated in an IO interface module, in general, is higher than the number of
IO registers visible for the port at the kernel mode ISA level. This is similar to the processor microarchitecture
which incorporates a number of registers in addition to the ones de(cid:12)ned in the ISA.

330

Chapter 8. Microarchitecture | Kernel Mode

future. Although this solves the standardization problem, manual entering of addresses is,
nevertheless, clumsy.

8.3.5.1 Autocon(cid:12)guration

Autocon(cid:12)guration shifts the burden of deciding the IO addresses to the OS. In the basic
autocon(cid:12)guration scheme, called plug-and-play, the Plug and Play Manager part of the
OS enters the addresses into the controller at boot-up time. This requires some extra
instructions in the boot-up code, but removes the burden of manually entering the addresses.
The astute student might wonder how the Plug and Play Manager will communicate to an
IO controller that does not know the addresses it should respond to! What address will the
Plug and Play Manager use for getting the controller’s attention? Consider the following
classroom scenario.
In the (cid:12)rst day of class, the professor wants to assign his students
unique ID numbers that should be used throughout the semester. He wants to convey the
ID numbers to them without calling out their names. One way he could do this is by calling
out the individual seat numbers and telling the corresponding ID number. Once all students
know their ID numbers, individual students can be called out using their ID numbers, and
the students are free to sit wherever they like.

In a similar manner, before an IO controller in the system gets its addresses, we need a
di(cid:11)erent attribute to identify the controller. In the plug-and-play scheme, a unique num-
ber is associated with each IO connector position (analogous to class seat number). These
unique connector numbers help to create yet another address space, called the con(cid:12)guration
address space. Depending on the connector into which an interface module is plugged, the
module has a unique range of con(cid:12)guration addresses. Each module also has a con(cid:12)gura-
tion memory, which stores information about the type and characteristics of the module.
During the interface initialization part of boot-up, the Plug and Play Manager reads the
con(cid:12)guration memory of each IO controller that is hooked up to the system; it uses the
module’s con(cid:12)guration addresses to do this access. The con(cid:12)guration information uniquely
identi(cid:12)es the controller, and provides information concerning the device drivers and re-
sources (such as DMA channels) it requires. After obtaining the information present in the
con(cid:12)guration memory, the Plug and Play Manager assigns IO addresses to the IO registers
present in the controller. Thereafter, the OS uses these IO addresses to communicate with
the controller.

In the above scheme, address assignment is done only during boot-up time. This scheme
mandates that the computer be reset every time a new IO card is connected to the system.
While this scheme may be acceptable for IO controllers that connect to disk drives and CD-
ROM drives, which are rarely connected when the system is working (especially in a desktop
environment), it is somewhat annoying for IO controllers that connect to keyboards, mice,
speakers, etc, which may be connected or disconnected when the system is in operation.
Furthermore, in mission-critical computers, downtimes can be extremely costly. In order
to avoid resetting the computer every time a new IO controller is plugged in, modern

8.3.

IO System Organization

331

computer systems enhance the plug-and-play feature with a feature called hot plug or hot
swap. With this feature, if a new controller is connected when the computer system is
in operation, the system recognizes the change, and con(cid:12)gures the new controller with the
help of systems software. This requires more sophisticated software and hardware (such as
protection against electrostatic discharge) than does plug-and-play. Recent IO buses such
as USB and PCIe are particulary geared for the hot swap feature, as they incorporate a
tree topology. In older standards such as the PCI, the hot swap feature can be incorporated
by using a dedicated secondary PCI bus at each primary PCI slot, as illustrated in Figure
8.21. Because each secondary PCI bus has a single PCI slot, an IO controller can be added
or removed without a(cid:11)ecting other controllers.

Processor−Memory Bus

CPU

Cache
Memory

Cache
Interface &
Controller

Main
Memory

Memory
Interface &
Controller

PM−PCI Bridge

Primary PCI Bus

Primary PCI Bus

PCI−PCI Bridge

PCI−PCI Bridge

PCI−PCI Bridge

SCSI
Controller

Secondary PCI Bus

Secondary PCI Bus

Secondary PCI Bus

SCSI Bus (IO Bus)

Hot Swappable
Slots

USB Controller

Graphics
Interface &
Controller

Ethernet
Interface &
Controller

Disk
Interface &
Controller

CD−ROM
Interface &
Controller

Tape
Interface &
Controller

Input Output
Audio

Serial
Ports

Desktop Bus

Graphics

Ethernet

Disk Drive

CD−ROM Drive

Tape Drive

Figure 8.21: Hot Pluggable PCI Slots

332

Chapter 8. Microarchitecture | Kernel Mode

8.4 System Architecture

As with the processor system and the memory system, many of the characteristics of the
IO system are driven by technology. For example, the properties of disk drives a(cid:11)ect how
the disks are connected to the processor, as well as how the operating system interacts with
them. IO systems di(cid:11)er from the processor and memory systems in several important ways.
Although processor-memory designers often focus primarily on performance, designers of
IO systems must consider issues such as expandability and resilience in the face of failure
as much as they consider performance. Second, for an IO system, characterizing the per-
formance is a more complex characteristic than for a processor. For example, with some
devices we may care primarily about access latency, whereas with others bandwidth|the
number of bits that can be transferred per second|is more important. Furthermore, per-
formance depends on many aspects of the system: the device characteristics, the connection
between the device and the rest of the system, the memory hierarchy, and the operating
system.

Until recently, computer performance was almost exclusively judged by the performance
of the processor-memory subsystem. In the past few years, the situation has begun to change
with users giving more importance to overall system performance. This calls for balanced
systems with IO subsystems that perform as good as the processor-memory subsystem.

The IO controllers need to be connected to the processor-memory system in order for
them to serve as IO ports for hooking up IO devices. There are several ways in which
this connection can be made. The manner in which the controllers are connected a(cid:11)ect
the system performance in a ma jor way. The earliest computers connected each controller
directly to the processor using a direct connection. The resulting connection topology is a
star. as shown in Figure 8.22. In addition to the direct connections to the processor, DMA
controllers, if present, are connected to the main memory as well. The disadvantage of a star
topology is that it requires several cables, at least one for each controller. Moreover, each
controller has to be tailored for a particular processor, and cannot be used in computers
having a di(cid:11)erent processor.

8.4.1 Single System Bus

In order to reduce the number of cables required, the simplest approach that we can think
of is to hook up all of the controllers to the processor-memory bus, which connects the
processor and the main memory. When a single bus is used in this manner to connect
all of the ma jor blocks|the processor, the main memory, and the controllers|the bus is
usually called a system bus (as opposed to a processor-memory bus). Such a connection
was illustrated in Figure 8.19. The two ma jor advantages of hooking up the controllers to
a bus instead of using tailored connections are versatility and low cost due to fewer cables.
Adding new devices and their controllers to the system is very straightforward.

A bus has its own data transfer protocol, and so in a single-bus system, the data transfer

8.4. System Architecture

333

Processor-Memory Bus

CPU

Memory
Interface &
Controller

Memory

Keyboard
Interface &
Controller

Mouse
Interface &
Controller

Graphics
Interface &
Controller

Ethernet
Interface &
Controller

Disk
Interface &
Controller

CD-ROM
Interface &
Controller

Tape
Interface &
Controller

Graphics

Ethernet

CD-ROM Drive

Tape Drive

Figure 8.22: Connecting IO Controllers to the Processor via a Star Topology

protocol between any two devices in the system is identical. The operation of the system
bus is, in fact, quite straightforward. For each data transfer, the master device (usually
the processor or a DMA controller) places the slave address (a memory address or an IO
register address) on the address lines of the bus. The data lines are used to transfer the
data value to be written or the value that is read from a location. The control lines are
used to specify the nature of the transfer, and also to coordinate the actions of the sending
and receiving units.

A computer that used a single system bus, and became very popular during its time in
the 1970s was PDP-11, a minicomputer manufactured by Digital Equipment Corporation
(DEC). Its system bus was called Unibus. The earliest PCs also had only a single system
bus, as the (cid:12)rst and second generation CPUs ran at relatively low clock frequencies, which
permitted all system components to keep up with those speeds.

8.4.2 Hierarchical Bus Systems

A single bus con(cid:12)guration that interconnects the processor, the memory system, and the
IO controllers has to balance the demands of processor-memory communication with those
of IO interface-memory communication. Such a setup has several drawbacks:

(cid:15) The use of a single system bus permits only a single data transfer between the units
at any given instant, potentially becoming a communication bottleneck.

(cid:15) The use of a single system bus may impose a severe practical restriction on the number

334

Chapter 8. Microarchitecture | Kernel Mode

of controllers that can be connected to the bus without increasing the bus capacitance
and skew e(cid:11)ects. A single bus may have high electrical capacitance due to two reasons:
(i) the bus ends up being long as it has to connect all devices in the system, and (ii)
each device that is hooked up to the bus adds to the capacitance. To reduce the
capacitance, each device must have expensive low-impedance drivers to drive the bus.

(cid:15) Because of the nature of most of the IO devices, there is a signi(cid:12)cant mismatch in speed
between transfers involving their controllers and the high-speed transfers between the
processor and memory. If we use a single high-speed system bus, then we are forced to
use expensive, high clock speed, controllers 6 , but without signi(cid:12)cant improvements in
overall performance. By contrast, if we use a low-speed system bus, then the system
will have poor performance.

(cid:15) If we use a single system bus, many of its speci(cid:12)cations (such as word size) will be tied
to the ISA (instruction set architecture), and therefore tend to di(cid:11)er widely across
di(cid:11)erent computer families, and even between family members. This forces us to
design speci(cid:12)c controllers for each computer family, which increases the cost of IO
controllers.

These issues are a concern, except for small computers in which the processor, the main
memory, and the IO controllers are placed on a single printed-circuit board (PCB) or a single
chip (SoC), allowing the system bus to be fully contained within the board. Often, such
systems do not require high performance either. Larger computers such as main-frames and
modern desktops, on the other hand, incorporate a large number of IO controllers, making
it di(cid:14)cult to place all components on a single board. These IO devices tend to have widely
di(cid:11)erent speeds, and the system as a whole tends to require high IO throughputs. All of
these make it impossible to use a single system bus. Therefore, larger computer systems
use multiple buses to connect various devices. These buses are typically arranged in a
hierarchical manner, with the fastest bus at the top of the hierarchy and the slower buses
towards the bottom of the hierarchy7 . Devices are connected to the bus hierarchy at the
level that matches their speed and bandwidth requirements. Activity on one bus does not
necessarily obstruct activity on another.

Figure 8.23 illustrates this concept by depicting a 2-level hierarchical arrangement of
buses. The bus at the top of the hierarchy is typically used to connect the processor and
the memory, and is called a processor-memory bus as before. Apart from the processor-
memory bus, there is a (slower) IO bus that connects all of the controllers together, and
forms the backbone of the IO system. The IO system is connected to the processor-memory

6 If the IO controllers are made to operate at the same clock speeds as the processor, they will become very
expensive, without substantially enhancing the IO throughput, which depend primarily on the IO devices
themselves.
7A faster bus does not imply a higher speed of electrical transmission through the wires, but rather a
shorter time between meaningful bus events (the \bus clock period"). The controllers connected to a faster
bus have to react more quickly, which calls for more rigorous engineering.

8.4. System Architecture

335

system by means of an IO bus bridge, which acts as an interface between the IO bus and
the processor-memory bus. The interfacing function involves translating the signals and
protocols of one bus to those of the other, akin to the function served by the exit ramps
that are used to connect the high-speed highway road systems to slow-speed local road
systems.

Processor-Memory Subsystem

Processor-Memory Bus

CPU

Cache
Memory

Cache
Interface &
Controller

Main
Memory

Memory
Interface &
Controller

PM-IO Bridge

Graphics
Interface &
Controller

Keyboard
Interface & 
Controller

Mouse
Interface &
Controller

Graphics

IO Bus

Ethernet
Interface &
Controller

Ethernet

Disk
Interface &
Controller

CD-ROM
Interface &
Controller

Tape
Interface & 
Controller

IO Subsystem

Disk Drive

CD-ROM Drive

Tape Drive

Figure 8.23: Use of an IO Bus to Interconnect IO Interfaces in the IO System

Large computers and modern desktops extend the bus hierarchy further by using mul-
tiple buses within the IO subsystem, with a bus bridge serving as an interface whenever
two buses are connected together. These buses di(cid:11)er primarily in speed. Depending on the
speed requirements of an IO device, it is connected to the appropriate bus via a proper IO
controller. Based on speed, the di(cid:11)erent buses can be loosely classi(cid:12)ed into the following
categories:

(cid:15) processor-memory buses

(cid:15) high-speed IO bus or backplane buses

(cid:15) low-speed IO bus or peripheral buses.

Processor-memory buses are used within the processor-memory system, and are the ones
closest to the processor. They are generally short, high speed, and matched to the memory
system so as to maximize processor-memory bandwidth. We have already seen processor-
memory buses in detail in the previous chapter. We shall look into the other two types of
buses in this chapter.

336

Chapter 8. Microarchitecture | Kernel Mode

Despite the prevalence of standardization among computer buses, no standardization
exists for the terminology used to characterize computer buses! What some mean by a
\local bus" is entirely di(cid:11)erent from what others mean by it. The classi(cid:12)cation given above
is loosely related to other types of classi(cid:12)cation, such as:

(cid:15) Transparent versus non-transparent

(cid:15) Serial versus parallel

(cid:15) Local (internal) versus expansion (external)

8.4.2.1 High-Speed and Low-Speed IO Buses

The buses used within the IO system can be high-speed or low-speed. Unlike processor-
memory buses, these buses can be lengthy, and can be used to hook up controllers having
di(cid:11)erent data bandwidth requirements. Among the two categories of IO buses, the back-
plane buses are closer (physically as well as logically) to the processor, and serve as the
main expressway that handles a large volume of high-speed IO tra(cid:14)c. Aside from the
graphics interfaces of game PCs, the processor talks to most of the IO interfaces through
the backplane bus, albeit the transfer may eventually go through other buses also. High-
speed device interfaces such as video cards and network controllers are most often connected
directly to the backplane bus. Backplane buses are generally slower than processor-memory
buses, although the di(cid:11)erence in speed has been reducing lately. The name backplane bus
originated in the days when IO interface circuit boards used to be housed in a card cage
having a plane of connectors at the back. Corresponding pins of the backplane connectors
were wired together to form a bus, and each IO interface board was plugged into a connec-
tor. Examples of standard backplane buses are AGP bus, PCI bus, PCIe, and In(cid:12)niBand
bus.

Peripheral buses are usually slower than backplane buses, and are therefore not directly
connected to the processor-memory system. Instead, they are connected to a backplane bus,
and are therefore farther from the processor than the backplane bus. They are typically
used to connect controllers that connect to secondary storage devices such as disk drives,
CD-ROM drives, and tape drives.

8.4.2.2 Transparent and Non-transparent Buses

When a separate IO bus is used as in Figure 8.23, the new bus can be logically con(cid:12)gured
in one of two ways: (i) as a logical extension of the processor-memory bus (transparent
bus), or (ii) logically disjoint to the processor-memory bus (non-transparent). In the former
case, the IO controllers connected to the IO bus logically appear to the processor as if
they are connected directly to the processor-memory bus. The processor can read their
IO registers or write to them by executing IO read/write instructions. The bus bridge is

8.4. System Architecture

337

logically transparent to the processor; all it does is to translate the signals and protocols of
one bus to those of the other. Examples of expansion buses are ISA, PCI, AGP, and PCI
Express. Thus, the PCI bridge is designed in such a manner that IO interfaces connected
to the PCI bus logically appear to the processor as if they are connected directly to the
processor-memory bus.

In the latter case, called non-transparent bus, the processor cannot directly access the
registers in the IO controllers connected to the IO bus.
Instead, a bus adapter (also known
as bus control ler) acts as an IO controller whose registers can be directly accessed by the
processor. Logically speaking, the address space seen by the IO controllers connected to
such a bus is di(cid:11)erent from the IO address space seen by the processor (address domain
isolation). When the processor wants to communicate with an IO controller connected to
such a bus, it sends the appropriate command (along with proper IO controller identi(cid:12)ca-
tion) to the bus adapter, which then performs the required address domain translation and
interacts with the IO controller by accessing its registers. A non-transparent bus is rarely
used as a backplane bus.

Examples of standard non-transparent buses are SCSI, USB, and Firewire. For device
drivers to access these buses, they need to be connected to the computer through a bus
controller/adapter such as SCSI controller, USB host, and FireWire host. A PCI bus can
also be set up to operate as a non-transparent bus by connecting it to the backplane PCI
bus using a non-transparent PCI-to-PCI bridge. Non-transparent bus bridges (or embedded
bridges) are quite useful when an IO device requires a large address space. They are also
useful when con(cid:12)guring intelligent IO subsystems such as RAID controllers, because the
entire subsystem appears to the processor as a single virtual PCI device (only a single
device driver is needed for the subsystem). They also facilitate the interconnection of
multiple processors in communications and embedded PCI applications.

8.4.2.3 Serial and Parallel Buses

One of the important factors a(cid:11)ecting the cost of a bus, especially those that are not short,
is the number of wires in the bus cable. A serial bus transfers data one bit a time. Although
this impacts the bandwidth, it permits thin cables to be used. Serial buses were initially
used within the IO system for hooking up the controllers of low-bandwidth devices such
as keyboard, mouse, and audio. Those serial buses were somewhat slow, and served as
peripheral buses that connected to a backplane bus. A few standards were also developed
for such type of serial buses. Examples are Serial Peripheral Interface Bus (SPI) and I2C.

As bus speeds continued to increase, it became possible to design serial buses that o(cid:11)er
higher bandwidth. Serial buses such as USB, Firewire, and SATA have become very popular
now. Because a serial bus requires only a single signal wire to transmit the data, it is able to
save space in the connector as well as in the cable. This means that many more ports can be
physically located at the computer system unit’s periphery, and that topology can be more
sophisticated than the simple linear topology. USB was designed to allow peripherals to

338

Chapter 8. Microarchitecture | Kernel Mode

be connected without the need to plug expansion cards into the computer’s ISA, EISA, or
PCI bus, and to improve plug-and-play capabilities by allowing devices to be hot-swapped
(connected or disconnected without powering down or rebooting the computer). Moreover,
unlike parallel buses, serial buses are not con(cid:12)ned to a small physical area. Also, it is often
the case that serial buses can be clocked considerably faster than parallel buses, because
designers do not need to worry about clock skew as well as crosstalk between multiple wires,
and better isolation from surroundings acievable due to thinner cables.

Parallel buses provide multiple wires for transmitting addresses and data. Therefore,
bits of address or data move through the bus simultaneously rather than one at a time.
Expansion slots and ports of parallel buses have a large number of pins and are therefore
big. This means that only a small number of expansion slots and ports can be incorporated
on a motherboard or backplane.

8.4.2.4 Local and Expansion Buses (Internal and External Buses)

A bus is generally called a local bus (also called internal bus or host bus) if it is physically
\close" to the processor, and has no expansion slots. Its electrical and functional properties
are suited for relatively short wires and relatively simple device controllers with a fast
response time. As such, it is usually on the motherboard 8 , and is used for connecting the
processor to the memory, o(cid:11)-chip cache memory, or other high-speed structures. These
buses tend to be proprietary. Some standard buses such as VLB and PCI may also be used
as local buses, especially in small systems.

Larger computer systems often have the need to connect to more \distant" devices such
as peripheral devices and other computers. An expansion bus (or external bus) connects
the motherboard to such devices. To this end, it provides expansion slots, thereby allowing
users to \expand" the computer.
The expansion slots permit plug-in expansion cards
to be directly connected to the expansion bus. Standard buses that are commonly used as
expansion buses are ISA, PCI, AGP, USB, and PCIe. An expansion bus can be used as a
backplane bus or a peripheral bus. Historically, the ISA bus was the predominant expansion
bus. Then, the PCI bus became very popular, with the ISA bus being provided for legacy
interfaces.

8.4.2.5 Data Transfer Types

A bus may support multiple types of data tranfer.

Control Transfer: Control transfers are typically used by the processor for sending short,
simple commands to IO devices. They are also used by the processor for receiving status

8 In the embedded systems arena, an entire system is often integrated into a single chip (System on
Chip { SoC) or a single package (System in Package { SiP), in which case the local bus is contained
within a chip package.

8.4. System Architecture

responses from the devices.

339

Interrupt Transfer:
Interrupt transfers are used by devices that transfer very little data
and need guaranteed quick responses (bounded latency). Examples are keyboards and
pointing devices such as mice.

Bulk Transfer: Bulk transfers involve large sporadic transfers such as (cid:12)le transfers. De-
vices such as printers that receive data in one big packet use the bulk transfer mode. These
transfers generally do not have guarantees on bandwidth or latency.

Isochronous Transfer:
Isochronous transfers are guaranteed to occur at a predetermined
speed. This speed may be less than the required speed, in which case data loss can occur.
Streaming devices such as speakers that deal with realtime audio and video generally use
the isochronous transfer mode.

8.4.2.6 Bus Characteristics

We have seen some of the features of computer buses. Some other characteristics relevant
to buses are:

(cid:15) Clock speed

(cid:15) Bus width

(cid:15) Bandwidth

(cid:15) Number of slots

(cid:15) Address spaces supported (memory, IO, con(cid:12)guration, etc.)

(cid:15) Arbitration

(cid:15) Communication medium and distance (copper cable, (cid:12)ber cable, backplane, IC, wire-
less, etc.)

(cid:15) Physical topology (backbone, ring, tree, star, etc.)

(cid:15) Signalling method (single-ended, di(cid:11)erential, etc.)

(cid:15) Plug and Play (PnP) and hot plug features: Plug and play is the ability to automati-
cally discover the con(cid:12)guration of devices attached to a bus at system boot time, with-
out requiring recon(cid:12)guration or manual installation of device drivers. This automatic
assignment of I/O addresses and interrupts to prevent con(cid:13)icts and identi(cid:12)cation of
drivers makes it easy to add new peripheral devices, because the user does not need to

340

Chapter 8. Microarchitecture | Kernel Mode

\tell" the computer that a device has been added. The system automatically detects
the device at reboot time.

Hot plug or hot swap is the ability to remove and replace devices, while the system
is operating. Once the appropriate software is installed on the computer, a user can
plug and unplug the component without rebooting. A well-known example of this
functionality is the Universal Serial Bus (USB) that allows users to add or remove
peripheral components such as a mouse, keyboard, or printer.

8.4.2.7 Expansion Slots and Hardware Ports

An IO bus is a medium for devices to communicate. For an IO bus to be useful, it must
provide the ability to hook up IO controllers. These controllers are hooked up only at speci(cid:12)c
places in the bus where a hardware socket is located. We can classify these hardware sockets
into two kinds, depending on where they are physically located: internal (expansion slots)
and external (hardware ports).

Expansion slots are located inside the system unit, and serve as receptacles for directly
plugging in IO interface cards (or expansion cards). They are usually situated on the
motherboard, as illustrated in Figure 8.24. The (cid:12)gure also shows a sound card being inserted
into one of the expansion slots. Expansion slots are named after the bus to which they are
connected; their size and shape are also based on the bus. Some of the common expansion
slots are SATA slot, IDE or PATA slot, AGP (for graphics cards), PCI slot, and PCIe slot.

Hardware ports are specialized outlets situated on the front or back panel of the computer
system unit. These serve as receptacles for plugs or cables that connect to external devices.
The ports themselves are mounted on the motherboard or the expansion cards in such a way
they show through openings in the system unit panels. Looking at the sound card in Figure
8.24, we can see several hardware ports. When this card is inserted into an expansion slot,
these ports will become visible at the back side of the system unit. Hardware ports are
generally not made of male connectors, since the protruding pins of a male connector can
break easily. Table 8.6 lists some of the commonly found hardware ports, the typical cards
that house them, and their typical uses.

The connectors for the above ports cover a wide variety of shapes such as round (e.g.,
PS/2), rectangular (e.g., FireWire), square (e.g., RJ11), and trapezoidal (e.g., D-Sub).
They also cover a variety of colors such as orange, purple, or gray (e.g., PS/2 port for
keyboard), green (e.g., PS/2 port for mouse, TSR port for speaker), pink (e.g., TSR port
for microphone), amber (serial DB-25 or DB-9), and blue or magenta (e.g., parallel DB-25).
While many of the above ports have their own speci(cid:12)c connectors, others may use di(cid:11)erent
types of connectors. The RS232 port, for instance, may use a wide variety of connectors
such as DB-25, DE-9, RJ45, and RJ50.

8.4. System Architecture

341

Housing Controller

Typical Use
Port Name
Keyboard, Mouse
PS/2
Terminal, Printer
Serial (tty, COM1:, COM2:) RS-232
Printer
LPT1:, LPT2:
Parallel
VGA monitor
VGA
Video card
LCD monitor
Video card
DVI
LAN, ISDN
Network interface card
Ethernet
Flash drive, Mouse, Camera
USB controller
USB
computer network, Camcorder
FireWire
FireWire controller
Sound card
MIDI or \game"
Joystick, other game controllers
Motherboard, video card Uncompressed HDTV
HDMI
Speaker
Mini audio
RCA
Microphone, speaker, musical keyboard, video composite

Table 8.6: Commonly Found Hardware Ports

8.4.3 Standard Buses and Interconnects

We saw that computers use di(cid:11)erent types of buses, having di(cid:11)erent characteristics. An IO
interface that is tailored to connect to a particular bus cannot, in general, be connected to
a di(cid:11)erent bus, because of di(cid:11)erences in bus characteristics. Two important considerations
in designing an IO system, besides performance, are low cost and the ability to interchange
IO interfaces across di(cid:11)erent computer platforms. In order to satisfy these two criteria, it is
important to standardize the buses used in the IO systems of di(cid:11)erent computers, and make
them independent of the ISA (instruction set architecture) 9 . If the computer community
de(cid:12)nes a standard for IO buses, then di(cid:11)erent vendors can make IO interfaces suitable
for that standard bus, increasing the availability of IO interfaces and reducing their cost.
These motivations have propelled the development and use of standard buses in computers
designed in the last 2-3 decades, even if the computers implement di(cid:11)erent ISAs and are
manufactured by di(cid:11)erent vendors. This has led to an enormous degree of commonality
among the IO systems of di(cid:11)erent computers.

Although a uniform bus standard may be desirable, in practice, it is di(cid:14)cult to develop
a standard bus that covers all peripherals (because of the extremely wide range of transfer
speeds and other requirements). Therefore, over the years, several standard buses have
emerged. For each standard, several revisions|supporting higher data rates, lower supply
voltages, etc.|have also appeared. Some of the notable bus standards are ISA (Industry
Standard Architecture), IDE (Integrated Device Electronics), PCI (Peripheral Component

9The processor-memory bus tends to be proprietary.
It is di(cid:14)cult to de(cid:12)ne a standard for this bus,
because its signals and protocols are closely tied to the ISA implemented by the computer, and its speed to
the processor speed.

342

Chapter 8. Microarchitecture | Kernel Mode

Motherboard

Expansion Card

Hardware Ports

Midi

MIC

line in

line out

SPKR

Expansion Slots

Figure 8.24: Expansion Slots and Hardware Ports

Interconnect), SCSI (Small Computer System Interface), and USB (Universal Serial Bus).
Some of these were speci(cid:12)cally developed by the industry as standards whereas the others
became standards through their popularity. By the year 2000, two dominant standards
had emerged in the desktop personal computer market. These are the PCI and SCSI
standards. The IBM-compatible and Macintosh platforms being developed today have
invariably adopted PCI as the backplane bus and SCSI as the peripheral bus. A larger
fraction of workstation vendors are also adhering to these standards. Although systems
with older buses (ISA or IDE) continue to ship, such systems have rapidly been replaced on
all but the lowest-performance computers. We shall take a detailed look at the PCI, SCSI,
and USB standards, which are very popular today in the desktop computing world.

8.4.3.1 Peripheral Component Interconnect (PCI) Bus

This bus standard was developed by Intel in the early 1990s, and became a widely adopted
standard for PCs, thanks to Intel’s decision to put all of the related patents into the pub-
lic domain (which allows other companies to build PCI-based peripherals without paying
royalties). The PCI bus is typically used as a computer’s backplane bus for connecting
many high-bandwidth devices and other buses. Structurally, it is almost always set up as

8.4. System Architecture

343

Internal/
External
Internal
External
External

Clock
speed
8 MHz
66.6 MHz
5 MHz

Data
width
16 bits

8 bits

Data
Max Topology Application
Rate devices

Backplane

Specialized industrial use

133 MB/s
5 MB/s

7

50 pin bus Mass storage devices

External

10 MHz

16 bits

20 MB/s

15

68 pin bus Mass storage devices

External

16 bits

40 MB/s

External

160 MHz

16 bits

640 MB/s

15

15

Mass storage devices

Mass storage devices

Bus

ISA
ATA-7
SCSI-1
Fast
Wide
SCSI
Wide
Ultra
SCSI
Ultra640
SCSI
SATA/300
SAS-2
PCI 2.2
PCIe 2.0

AGP 8x
USB 2.0
Firewire 800

Internal
External
External

Internal

Internal
Internal

3 GHz
GHz
66 MHz

533 MHz

Serial
Serial
64 bits
Serial
32 lanes
32 bits
Serial
Serial

300 MB/s
6 Gb/s
533 MB/s
16 GB/s

2.1 GB/s
60 MB/s
800 Mb/s

4 pin cable Hard drives, CD/DVD drives
Tree
Hard drives, CD/DVD drives, scanners, printers
5 Backplane
1 Point-to
-point

Video card

1
127 Tree
63 Tree

Video card
Ubiquitous
Video devices

Table 8.7: Characteristics of Standard Computer Buses

an expansion bus; that is, expansion slots are attached to the bus. Logically, it is typically
set up as a transparent bus. That is, the PCI bridge is designed in such a manner that IO
interfaces connected to the PCI bus logically appear to the processor as if they are con-
nected directly to the processor-memory bus. The PCI bus also supports autocon(cid:12)guration;
thus, it supports three independent address spaces: memory, IO, and con(cid:12)guration. The
PCI bus provides good support for multimedia applications by providing high bandwidth,
long burst transfers, good support for multiple bus masters, and guaranteed low-latency
access. A 32-bit PCI bus operating at 33 MHz supports a peak transfer rate of 33 MHz (cid:2)
4 bytes = 132 MB per second. The PCI bus is multiplexed, which means that some signal
lines are used for multiple purposes, depending on the operation performed or the step of a
particular operation. Although this makes it slightly di(cid:14)cult to design PCI-compatible IO
interfaces, these interfaces require only fewer pins, which make them less costly.

The PCI bus is common in today’s PCs and other computer types. It is being succeeded
by PCI Express (PCI-E), which o(cid:11)ers much higher bandwidth. As of 2007 the PCI standard
is still used by many legacy and new devices that do not require the higher bandwidth of
PCI-E. PCI is expected to be the primary expansion bus in desktops for a few more years.

344

Chapter 8. Microarchitecture | Kernel Mode

8.4.3.2 Small Computer System Interface (SCSI) Bus

The original SCSI standard (now known as SCSI-1) was de(cid:12)ned by the American National
Standards Institute (ANSI), under the designation X3.131. The SCSI bus is usually used
as a peripheral bus for connecting storage devices such as disk drives, CD-ROM drives,
and tape drives.
It is also used to connect input-output devices such as scanners and
printers. As a peripheral bus, it is slower than the backplane bus, and is connected to the
computer through the backplane bus. Unlike the PCI bus, the SCSI bus is not usually set
up as a transparent bus. IO interfaces connected to the SCSI bus are therefore not directly
manipulated by the device driver’s IO instructions. Each SCSI-1 bus can support up to 8
controllers, allowing up to 8 peripherals to be connected.

IO controllers are connected to a SCSI bus via cables. The
Controllers and Devices:
SCSI bus itself is connected to the backplane bus via a SCSI control ler (also called a host
adapter), coordinates between all of the device interfaces on the SCSI bus and the computer.
The SCSI controller can be a card that is plugged into an available backplane bus slot or
it can be built into the motherboard. The controller itself is like a microcontroller in that
it has a small ROM or Flash memory chip that stores the SCSI BIOS | the software
needed to access and control the devices on the bus. It has the ability to perform DMA
operations. The controller can be directly accessed by the device driver running on the CPU.
On getting commands from the device driver, the SCSI controller accesses the controller of
the appropriate IO device, and performs the required IO operations. The controller can also
deliver electrical power to SCSI-enabled devices. The SCSI bus expects its device interfaces
to have functionality such as bu(cid:11)ering of data and ensuring of data integrity. Each device
connected to a SCSI bus, including the SCSI controller, must have a unique identi(cid:12)er (ID)
in order for it to work properly. This ID is speci(cid:12)ed through a hardware or software setting.
If the bus can support sixteen devices, for instance, the IDs range from zero to 15, with the
SCSI controller typically having the highest ID.

Cables and Connectors:
Internal devices are connected to a SCSI controller by a ribbon
cable. External devices are attached to the controller in a daisy chain using a thick, round
cable.
In a daisy chain, each device connects to the next one in line. For this reason,
external SCSI devices typically have two SCSI connectors | one to connect to the previous
device in the chain, and the other to connect to the next device. Di(cid:11)erent SCSI standards
use di(cid:11)erent connectors, which are often incompatible with one another. These connectors
usually use 50, 68 or 80 pins. Serial Attached SCSI (SAS) devices use (smaller) SATA
cables and SATA-compatible connectors.

SCSI Types: Since the introduction of the (cid:12)rst SCSI standard, many upgrades have been
introduced, resulting in a proliferation of SCSI standards. They di(cid:11)er in terms of speed, bus
width, connectors, and maximum number of devices that can be attached. Examples for

8.4. System Architecture

345

these improved SCSI standards are SCSI-2, Fast SCSI, Ultra SCSI, and Wide SCSI. All of
these SCSI buses are parallel. The newest type of SCSI, called Serial Attached SCSI (SAS),
however, uses SCSI commands but transmits data serially. SAS uses a point-to-point serial
connection to transfer data at 3 gigabits per second, and each SAS port can support up to
128 devices or expanders.

The SCSI bus has several bene(cid:12)ts. It is reasonably fast, with transfer rates up to 640
megabytes per second. It has been around for more than 20 years and has been thoroughly
tested; so it has a reputation for being reliable. However, the SCSI standards also have some
potential problems. They have limited system BIOS support, and have to be con(cid:12)gured
for each computer. There is also no common SCSI software interface. Finally, the di(cid:11)erent
SCSI types are incompatible, di(cid:11)ering in speed, bus width, and connector. The SCSI bus
is slowly being replaced by other buses such as serial-ATA (SATA).

8.4.3.3 Universal Serial Bus (USB)

Overview: The USB is a popular interconnect for connecting external devices to a com-
puter. Just about any computer that you buy today comes with one or more USB ports,
identi(cid:12)ed by the USB icon shown in Figure 8.25. These USB ports let you attach everything
from mice to printers to your computer quickly and easily. This standard was developed by
a collaborative e(cid:11)ort of several companies.

Figure 8.25: USB Icon

Universal Serial Bus (USB) provides a serial bus standard for connecting IO devices
to computers (although it is also becoming commonplace on video game consoles such as
Sony’s PlayStation 2, Microsoft’s Xbox 360, Nintendo’s Revolution, and PDAs, and even
devices like televisions and home stereo equipment). The USB’s low cost led to its adoption
as the standard for connecting most peripherals that do not require a high-speed bus.

Because the USB port is serial, it is small. The USB standard allows a thin (serial)
cable to connect an IO controller module to a USB port, and this makes it possible to move
the IO controller from the systems unit to the the IO device itself. In fact, the USB was
designed to allow peripherals to be connected without the need to plug expansion cards
into the computer’s ISA, EISA, or PCI bus, and to improve plug-and-play capabilities by
allowing devices to be hot swapped.

USB can connect peripherals such as mice, keyboards, gamepads and joysticks, scanners,
digital cameras, printers, external storage, networking components, etc. For many devices
such as scanners and digital cameras, USB has become the standard connection method.

346

Chapter 8. Microarchitecture | Kernel Mode

USB is also used extensively to connect non-networked printers, replacing the parallel ports
which were widely used; USB simpli(cid:12)es connecting several printers to one computer. As of
2005, the only large classes of peripherals that cannot use USB are displays and high-quality
digital video components (because they need a higher bandwidth than that supported by
USB).

Cables, Connectors, and Topology: The USB’s connectivity is di(cid:11)erent from that of
a regular bus. It uses a tree-type topology, with a hub at each node of the tree. Each hub
has a few connectors for connecting USB-compatible IO interfaces and other USB hubs,
sub ject to a limit of 5 levels of branching. The hub at the root of the tree is called the
root hub. It contains a USB host controller, whereas the remaining hubs contain a USB
repeater. The root hub is hooked to the backplane bus of the computer. Physically, it
may be implemented on a USB adapter card that is plugged onto an expansion slot, or it
may be directly implemented on the motherboard. Device drivers can directly access only
the host controller, which contains the hardware registers that map to the computer’s IO
address space. Not more than 127 devices, including the bus devices, may be connected to
a single host controller. Modern computers often have several host controllers, allowing a
very large number of USB devices to be connected. USB endpoints actually reside on the
connected device: the channels to the host is referred to as a pipe. The devices (and hubs)
have associated pipes (logical channels) which are connections from the host controller to
a logical entity on the device named an endpoint. Individual USB cables can run as long
as 5m, and have 4 wires: two are for the power lines (4.5 V and Ground) and the other
two form a twisted pair for data. USB cables do not need to be terminated. USB 2.0 uses
bursts unlike (cid:12)rewire.

Power supply: The USB also has wires for supplying electrical power from the computer
to the connected devices. Devices such as mice that require only a small amount of power
can thus obtain power from the bus itself, without requiring an external power source. The
USB connector provides a single nominally 5 volt wire from which connected USB devices
may power themselves. A given segment of the bus is speci(cid:12)ed to deliver up to 500 mA.
Devices that need more than 500 mA must provide their own power. Many hubs include
external power supplies which will power devices connected through them without taking
power from up-stream.

Enumeration: When the host controller powers up, it queries all of the devices connected
down-stream. For each device, it assigns a unique 7-bit address and loads the device driver
it needs. This process is called enumeration. Devices are also enumerated when they
are connected to the bus while the computer is working; recall that USB devices are hot-
swappable. At the time of enumeration, the host also (cid:12)nds out from each device the type of
data transfer it will perform in the future. The host also keeps track of the total bandwidth
requested by all of the isochronous and interrupt devices. They can consume up to 90

8.4. System Architecture

347

percent of the available bandwidth. After 90 percent is used up, the host denies access to
any other isochronous or interrupt devices. Control packets and bulk transfer packets use
any bandwidth left over (at least 10 percent).

IO Transaction: An IO request reaching the root hub from the backplane bus is prop-
agated to all nodes in the tree. All IO interfaces connected to the tree nodes will see the
request, just like in a regular bus, although the request may travel through one or more
hubs. However, a transaction from an IO interface to the processor-memory system is prop-
agated only towards the root hub, and is not seen by other IO interfaces connected to the
tree. This function is logically di(cid:11)erent from that of a bus.

USB 2.0 (High-speed USB): The standard for USB version 2.0 was released in April
2000 and serves as an upgrade for USB 1.1. USB 2.0 provides additional bandwidth for
multimedia and storage applications and has a data transmission speed 40 times faster than
USB 1.1. To allow a smooth transition for both consumers and manufacturers, USB 2.0 has
full forward and backward compatibility with original USB devices and can work with cables
and connectors made for the original USB. By supporting the following three speed transfer
speeds, USB 2.0 supports low-bandwidth devices such as keyboards and mice, as well as
high-bandwidth ones like high-resolution Webcams, scanners, printers and high-capacity
storage systems.

(cid:15) A Low Speed rate of 1.5 Mbit/s that is mostly used for Human Interface Devices
(HID) such as keyboards, mice and joysticks.

(cid:15) A Full Speed rate of 12 Mbit/s that was the fastest rate before the USB 2.0 speci(cid:12)-
cation. All USB Hubs support Full Speed.

(cid:15) A Hi-Speed rate of 480 Mbit/s.

Storage Devices: USB implements connections to storage devices using a set of stan-
dards called the USB mass-storage device class. This was initially intended for traditional
magnetic and optical drives, but has been extended to support a wide variety of devices.
As USB makes it possible to install and remove devices without opening the computer case,
it is especially useful for external drives. Today, a number of manufacturers o(cid:11)er exter-
nal, portable USB hard drives that o(cid:11)er performance comparable to internal drives. These
external drives usually contain a translating device that interfaces a drive of conventional
technology (IDE, ATA, SATA, ATAPI, or even SCSI) to a USB port. Functionally, the
drive appears to the user just like another internal drive.

348

Chapter 8. Microarchitecture | Kernel Mode

8.4.3.4 FireWire (IEEE 1394)

IEEE 1394 is a serial bus interface standard for high-speed communications and isochronous
real-time data transfer. It is more commonly known as FireWire, the name given by Apple
Inc., its creator (Sony Corp. calls it i.LINK).

While the USB was designed for low-to-medium speed peripherals, the FireWire was
designed for interfacing with high-speed devices such as digital camcorders and disk drives.
It has many similarities with the USB, such as plug-and-play, hot swap, provision of power
through the cable, and ability to connect many devices. Apart from speed di(cid:11)erences, the
big di(cid:11)erence between FireWire and USB 2.0 is that USB 2.0 is host-based, meaning that
devices must connect to a computer in order to communicate. FireWire, on the other hand,
is peer-to-peer, meaning that two FireWire devices can talk to each other without going
through the processor and memory subsystems. Implementing FireWire costs a little more
than USB.

The main features of FireWire are:

* Fast transfer of data * Ability to put lots of devices on the bus * Provision of power
through the cable * Plug-and-play: Unlike USB devices, each FireWire node participates
in the con(cid:12)guration process without intervention from the host system. * Hot-pluggable
ability * Low cabling cost

FireWire is a method of transferring information between digital devices, especially
audio and video equipment. FireWire is fast { the latest version achieves speeds up to 800
Mbps. At some time in the future, that number is expected to jump to 3.2 Gbps when
optical (cid:12)ber is introduced.

Up to 63 devices can be connected to a FireWire bus.

When the computer powers up, it queries all of the devices connected to the FireWire
bus and assigns each one an address, a process called enumeration. Each time a new device
is added to or removed from the bus, the FireWire bus is re-enumerated. FireWire is plug-
and-play as well as hot pluggable; so if you connect a new FireWire device to your computer,
the operating system auto-detects it and starts talking to it.

FireWire 800 is capable of transfer rates up to 800 Mbps, and permits the cable length
to be up to 100 meters. The faster 1394b standard is backward-compatible with 1394a.

Cables and Connectors: FireWire devices can be powered or unpowered. FireWire
allows devices to draw their power from their connection. Two power conductors in the
cable can supply power (8 to 30 volts, 1.5 amps maximum) from the computer to an
unpowered device. Two twisted pair sets carry the data in a FireWire 400 cable using
a 6-pin con(cid:12)guration. Some smaller FireWire-enabled devices use 4-pin connectors to save
space, omitting the two pins used to supply power. The maximum cable length is 4.5m.
The length between nodes can be increased by adding repeaters.

8.4. System Architecture

349

Sending Data via FireWire: FireWire uses 64-bit (cid:12)xed addressing, based on the IEEE
1212 standard. There are three parts to each packet of information sent by a device over
FireWire:

* A 10-bit bus ID that is used to determine which FireWire bus the data came from *
A 6-bit physical ID that identi(cid:12)es which device on the bus sent the data * A 48-bit storage
area that is capable of addressing 256 terabytes of information for each node

The bus ID and physical ID together comprise the 16-bit node ID, which allows for
64,000 nodes on a system. Data can be sent through up to 16 hops (device to device). Hops
occur when devices are daisy-chained together.

Digital Video: Now that we’ve seen how FireWire works, let’s take a closer look at one
of its most popular applications: streaming digital video. FireWire really shines when it
comes to digital video applications. Most digital video cameras or camcorders now have a
FireWire plug. When you attach a camcorder to a computer using FireWire, the connection
is amazing.

An important element of FireWire is the support of isochronous devices. In isochronous
mode, data is streamed between the device and the host in real-time with guaranteed
bandwidth and no error correction. Essentially, this means that a device like a digital
camcorder can request that the host computer allocate enough bandwidth for the camcorder
to send uncompressed video in real-time to the computer. When the computer-to-camera
FireWire connection enters isochronous mode, the camera can send the video in a steady
(cid:13)ow to the computer without anything disrupting the process.

8.4.3.5 Ethernet

Topology: At the most fundamental level, all Ethernet networks are laid out as a bus
topology, with the devices tapping into the bus.

Packet Assembly: To transmit a message across an Ethernet, a node constructs an
Ethernet frame, a package of data and control information that travels as a unit across the
network. Large messages are split across multiple frames.

The arbitration method followed for the Ethernet bus network is di(cid:11)erent from those
used for the buses inside the computer. Instead of using a centralized bus arbitration unit,
it takes a distributed approach. Before a device places a frame in the bus, it has to make
sure that the bus is not in use, i.e., no other frames are already present on the bus. The
device hardware follows the following protocol: it monitors the bus and waits until no frame
is present in the bus. Once it (cid:12)nds no frame to be present, it waits further for a short period
of time and checks again. If the bus is found to be idle, then the device begins transmitting
the frame.

350

Chapter 8. Microarchitecture | Kernel Mode

With the above arrangement, it is possible that two or more devices may simultaneously
(cid:12)nd the bus to be idle and start transmitting their frames at exactly the same time. In
Ethernet parlance, this is termed a col lision. When the Ethernet NIC detects a collision, it
waits for a random amount of time, re-checks to see if the bus is idle and then attempts a
re-transmission.

The original Ethernet described communication over a single cable shared by all devices
on the network. Once a device attached to this cable, it had the ability to communicate
with any other attached device. This allows the network to expand to accommodate new
devices without requiring any modi(cid:12)cation to those devices already on the network.

One interesting thing about Ethernet addressing is the implementation of a broadcast
address. A frame with a destination address equal to the broadcast address (simply called
a broadcast, for short) is intended for every node on the network, and every node will both
receive and process this type of frame.

8.4.3.6 Bluetooth

Bluetooth is an industrial speci(cid:12)cation for wireless personal area networks (PANs). The
standard also includes support for more powerful, longer-range devices suitable for con-
structing wireless LANs. Bluetooth provides a way to connect and exchange information
between devices like personal digital assistants (PDAs), mobile phones, laptops, PCs, print-
ers and digital cameras via a secure, low-cost, globally available short range radio fre-
quency. It is a radio standard primarily designed for low power consumption, with a short
range (power class dependent: 10 centimeters, 10 meters, 100 meters) and with a low-cost
transceiver microchip in each device.

Bluetooth lets these devices talk to each other when they come in range, even if they are
not in the same room, as long as they are within up to 100 meters of each other, depending
on the power class of the product.

Communication and Connection

A Bluetooth device playing the role of the master can communicate with up to 7 devices
playing the role of the slave. This network of up to 8 devices is called a piconet. At any
given time, data can be transferred between the master and one or more slaves; but the role
of the master switches rapidly among the devices in a round-robin fashion. (Simultaneous
transmission from the master to multiple slaves, although possible, is not very common).
Either device may switch the master/slave role at any time.

Bluetooth speci(cid:12)cation also allows connecting multiple piconets together to form a scat-
ternet, with some devices acting as a bridge by simultaneously playing the master role in
one piconet and the slave role in another piconet.

Any Bluetooth device will transmit the following sets of information on demand

* Device Name * Device Class * List of services * Technical information eg: device
features, manufacturer, Bluetooth speci(cid:12)cation, clock o(cid:11)set

8.4. System Architecture

351

Any device may perform an "inquiry" to (cid:12)nd other devices to which to connect, and
any device can be con(cid:12)gured to respond to such inquiries. However, if the device trying to
connect knows the address of the device it will always respond to direct connection requests
and will transmit the information shown in the list above if requested for it. Use of the
device’s services however may require pairing or its owner to accept but the connection
itself can be started by any device and be held until it goes out of range. Some devices
can only be connected to one device at a time and connecting to them will prevent them
from connecting to other devices and showing up in inquiries until they disconnect the other
device.

Every device has a unique 48-bit address. However these addresses are generally not
shown in inquiries and instead friendly "Bluetooth names" are used which can be set by
the user, and will appear when another user scans for devices and in lists of paired devices.
Most phones have the Bluetooth name set to the manufacturer and model of the phone by
default. Most phones and laptops will only show the Bluetooth names and special programs
are required to get additional information about remote devices. This can get confusing
with activities such as Bluejacking as there could be several phones in range named "T610"
for example. On Nokia phones the Bluetooth address may be found by entering "#2820#".
On computers running Linux the address and class of a USB Bluetooth dongle may be
found by entering "hcicon(cid:12)g hci0 class" as root ("hci0" may need to be replaced by another
device name).

Every device also has a 24-bit class identi(cid:12)er. This provides information on what kind of
a device it is (Phone, Smartphone, Computer, Headset, etc), which will also be transmitted
when other devices perform an inquiry. On some phones this information is translated into
a little icon displayed beside the device’s name.

Bluetooth devices will also transmit a list of services if requested by another device;
this also includes some extra information such as the name of the service and what channel
it is on. These channels are virtual and have nothing to do with the frequency of the
transmission, much like TCP ports. A device can therefore have multiple identical services.

Pairing

Pairs of devices may establish a trusted relationship by learning (by user input) a shared
secret known as a passkey. A device that wants to communicate only with a trusted device
can cryptographically authenticate the identity of the other device. Trusted devices may
also encrypt the data that they exchange over the air so that no one can listen in. The
encryption can however be turned o(cid:11) and passkeys are stored on the device’s (cid:12)le system
and not the Bluetooth chip itself. Since the Bluetooth address is permanent a pairing will
be preserved even if the Bluetooth name is changed. Pairs can be deleted at any time
by either device. Devices will generally require pairing or will prompt the owner before
it allows a remote device to use any or most of its services. Some devices such as Sony
Ericsson phones will usually accept OBEX business cards and notes without any pairing
or prompts. Certain printers and access points will allow any device to use its services by
default much like unsecured Wi-Fi networks.

352

Chapter 8. Microarchitecture | Kernel Mode

Air interface

The Bluetooth protocol operates in the license-free ISM band at 2.45 GHz. In order to
avoid interfering with other protocols that use this band, the Bluetooth protocol divides
the band into 79 channels (each 1 MHz wide) and changes channels up to 1600 times per
second. Implementations with versions 1.1 and 1.2 reach speeds of 723.1 kbit/s. Version
2.0 implementations feature Bluetooth Enhanced Data Rate (EDR), and thus reach 2.1
Mbit/s. Technically version 2.0 devices have a higher power consumption, but the three
times faster rate reduces the transmission times, e(cid:11)ectively reducing consumption to half
that of 1.x devices (assuming equal tra(cid:14)c load).

Bluetooth di(cid:11)ers from Wi-Fi in that the latter provides higher throughput and covers
greater distances but requires more expensive hardware and higher power consumption.
They use the same frequency range, but employ di(cid:11)erent multiplexing schemes. While
Bluetooth is a cable replacement for a variety of applications, Wi-Fi is a cable replacement
only for local area network access. A glib summary is that Bluetooth is wireless USB
whereas Wi-Fi is wireless Ethernet, both operating at much lower bandwidth than the
cable systems they are trying to replace.

Applications:

(cid:15) Wireless networking between desktops and laptops, or desktops in a con(cid:12)ned space
and where little bandwidth is required

(cid:15) Bluetooth peripherals such as printers, mice and keyboards

(cid:15) Bluetooth cell phones, which are able to connect to other cell phones, computers,
personal digital assistants (PDAs), and automobile handsfree systems.

(cid:15) Bluetooth mp3 players and digital cameras to transfer (cid:12)les to and from computers
Bluetooth headsets for mobile phones and smartphones * Medical applications Ad-
vanced Medical Electronics Corporation is working on several devices * For remote
controls where infrared was traditionally used. * Hearing aids Starkey Laboratories
have created a device to plug into some hearing aids [2] * Newer model Zoll De(cid:12)brila-
tors for the purpose of transmitting De(cid:12)brilation Data and Patient Monitoring/ECG
data between the unit and a reporting PC using Zoll Rescue Net software.

Speci(cid:12)cations and Features

The Bluetooth speci(cid:12)cation was (cid:12)rst developed by Ericsson

8.4.4 Expansion Bus and Expansion Slots

A bus is a set of electronic signal pathways that allows information and signals to travel
between components inside or outside of a computer.

8.4. System Architecture

353

An expansion slot (connector): Remember that the expansion bus, or external bus,
is made up of the electronic pathways that connect the di(cid:11)erent external devices to the rest
of your computer. These external devices (monitor, telephone line, printer, etc.) connect to
ports on the back of the computer. Those ports are actually part of a small circuit board
or ’card’ that (cid:12)ts into a connector on your motherboard inside the case. The connector is
called an expansion slot.

Note: Communication ports (com ports), printer ports, hard drive and (cid:13)oppy connec-
tors, etc., are all devices which used to be installed via adapter cards. These connectors
are now integrated onto the motherboard, but they are still accessed via the expansion
(external) bus and are allocated the same type of resources as required by expansion cards.
As a matter of fact (and unfortunately, in my opinion), other devices like modems, video
technology, network and sound cards are now being integrated, or embedded, right onto the
motherboard.

Expansion slots are easy to recognize on the motherboard. They make up a row of
long plastic connectors at the back of your computer with tiny copper ’(cid:12)nger slots’ in a
narrow channel that grab the metal (cid:12)ngers or connectors on the expansion cards. In other
words, the expansion cards plug into them. The slots attach to tiny copper pathways on the
motherboard (the expansion bus), which allows the device to communicate with the rest of
the computer. Each pathway has a speci(cid:12)c function. Some may provide voltages needed
by the new device (+5, +12 and ground), and some will transmit data. Other pathways
allow the device to be addressed through a set of I/O (input/output) addresses, so that the
rest of the computer knows where to send and retrieve information. Still more pathways
are needed to provide clock signals for synchronization and other functions like interrupt
requests, DMA channels and bus mastering capability.

As with any other part of the computer, technology has evolved in an e(cid:11)ort to increase
the speed, capability and performance of expansion slots. Now you’ll hear about more
busses - PCI bus, ISA bus, VESA bus, etc. Not to worry! These are all just types of
expansion (external) busses. They just describe the type of connector and the particular
technology or architecture being used. Thus, the adapter card being installed must match
the architecture or type of slot that it’s going into. An ISA card (cid:12)ts into an ISA slot, a PCI
adapter card must be installed into a PCI expansion slot, etc.

Traditionally, PCs have utilized an expansion bus called the ISA bus. In recent years,
however, the ISA bus has become a bottleneck, so nearly all new PCs have a PCI bus for
performance as well as an ISA bus for backward compatibility.

The current popular expansion bus is the PCI (Peripheral Component Interconnect) bus
for all cards except the graphics cards. For graphics cards, the bus of choice is AGP. Most
motherboards today have one AGP slot and several PCI slots. Your expansion cards will
plug into these card slots. Be sure you get cards that match the available type of slots on
your motherboard. Other popular computer buses include NuBus for the Apple Macintosh,
VESA local bus, and PCMCIA (PC Card).

354

Chapter 8. Microarchitecture | Kernel Mode

8.4.5

IO System in Modern Desktops

The IO system is one area where there is a signi(cid:12)cant di(cid:11)erence across di(cid:11)erent computers.
This is despite the use of standard buses and standard ports, which isolate any e(cid:11)ects of
the kernel mode ISA as well as processor characteristics. The signi(cid:12)cant di(cid:11)erences in IO
system organization arise from 3 factors:

(cid:15) Di(cid:11)erences in the number and types of buses used: First, systems often di(cid:11)er in the
number of backplane buses, serial buses, and peripheral buses used. Second, although
standard buses have been proposed, there are just far too many standards available
for each type of bus. For instance, for the backplane bus, the widely used standard
is the PCI. However, because the ISA bus was popular until recently as a backplane
bus, many systems still support an ISA bus that is connected to the PCI bus using a
ISA bus controller. On a di(cid:11)erent note, even within a standard, there are variations
such as SCSI, SCSI-2, and SCSI-3.

(cid:15) The manner in which the buses are interconencted: Even if two computer systems
used exactly the same buses for their IO system, they can di(cid:11)er in the manner in
which these buses are interconnected. Depending on the system requirements, system
architects connect the buses in di(cid:11)erent manners.

(cid:15) Di(cid:11)erences in the number and types of IO interfaces connected: Finally, even if two
computers have the same bus con(cid:12)guration, they may not have the same set of IO
devices hooked to them. Although they may have been shipped with identical system
con(cid:12)guration, the end user can add or remove IO devices later. Moreover, one user
may connect a particular IO interface, such as a keyboard interface, to one port while
another may connect it to a di(cid:11)erent port.

Figure 8.26 shows one possible organization of the IO system in a mid-range to high-
end desktop machine in 2002.
In this con(cid:12)guration, PCI is used as the backplane bus,
with slower devices sharing the lower-performance SCSI bus. The PCI backplane bus is
used to connect all interfaces to the processor-memory system. Several of the slow IO
devices (audio IO, serial ports, and the desktop bus) share a single port onto the PCI bus.
Serial ports provide for connections such as low-speed Appletalk network. The desktop bus
provides support for keyboards and mice. The second interface in the (cid:12)gure is used for
graphics output. The third interface is used for connecting to the ethernet network. The
fourth interface is the SCSI controller, which is used extensively for connecting bulk storage
devices such as disks, tapes, and CD-ROMs. In the (cid:12)gure, a disk drive, a tape drive, and
a CD-ROM reader are connected to the SCSI bus by means of their respective controllers
(interfaces).

8.4. System Architecture

355

CPU

Processor −Memory Bus

Memory

Memory Controller

PCI Bridge

PCI Bus (Backplane Bus)

I/O Controller

Graphics
Controller

Ethernet
Controller

SCSI
Controller

Input Output
Audio

Serial
Ports

Desktop Bus

Graphics

Ethernet

SCSI Bus (I/O Bus)

Disk
Controller

CD −ROM
Controller

Tape
Controller

CD −ROM Drive

Tape Drive

Figure 8.26: A Possible Organization of the IO System in a Modern Desktop Computer

8.4.6 Circa 2006

A circa 2006 PC incorporates many specialized buses of di(cid:11)erent protocols and bandwidth
capabilities, such as the Serial ATA, USB, and Firewire. The PCI bus is too slow to
be the "backbone" for hooking up all of these buses. A solution adopted since the late
1990s?? is to expand the functionality of the PMI-PCI bridge and connect some devices
directly to this bridge. Examples of such devices include the AGP. This central bridge
itself is usually organized as two parts|the northbridge and the southbridge or IO
bridge. The northbridge is used to connect the 3 fastest parts of the system|the CPU,
the memory, and the video card.
In a modern computer system, the video card’s GPU
(graphics processing unit) is functionally a second (or third) CPU.

The northbridge is connected to the southbridge, which routes the IO tra(cid:14)c between the
northbridge and the rest of the IO subsystem. The southbridge provides ports for di(cid:11)erent
types of buses, such as the PCI. With continued advances in bus standards and speeds,
the southbridge is steadily evolving to accommodate more complex bus controllers such as
Serial ATA and Firewire. So today’s southbridge is sort of the Swiss Army Knife of IO

356

Chapter 8. Microarchitecture | Kernel Mode

switches, and thanks to Moore’s Law it has been able to keep adding functionality in the
form of new interfaces that keep bandwidth-hungry devices from starving on the PCI bus.

In an ideal world, there would be one primary type of bus and one bus protocol that
connects the di(cid:11)erent IO devices
including the video card/GPU to the CPU and main
memory. Although we are unlikely to return to this utopian ideal, PCI Express (PCIe)
promises to bring some order to the current chaos. In any case, it is expected to dominate
the personal computer in the coming decade. With Intel’s recent launch of its 900-series
chipsets and NVIDIA and ATI’s announcements of PCI Express-compatible cards, PCIe
will shortly begin cropping up in consumer systems. In(cid:12)niband shows even more technical
promise towards returning to the ideal single bus.

8.4.7 RAID

Future Directions in IO Organization

What does the future hold for IO systems? The rapidly increasing performance of processors
strains IO systems, whose mechanical components cannot o(cid:11)er similar improvements in
performance. To reduce the growing gap between the speed of processors and the access
time to secondary storage (mainly disks), operating systems often cache active parts of the
(cid:12)le system in the physical memory. These caches are called (cid:12)le caches, and like the cache
memories we saw earlier, they attempt to make use of temporal and spatial localities in
access to secondary storage. The use of (cid:12)le caches allows many (cid:12)le accesses to be satis(cid:12)ed
by physical memory rather than by disk.

Magnetic disks are increasing in capacity quickly, but access time is improving only
slowly. In addition to increases in density, transfer rates have grown rapidly as disks in-
creased in rotational speed and disk controllers improved.
In addition, virtually every
high-performance disk manufactured today includes a track or sector bu(cid:11)er that caches
sectors as the read head passes over them.

Many companies are working feverishly to develop next-generation bus standards, as
standards such as PCI are fast approaching the upper limits of what they can do. All of
these new standards have one thing in common. They do away with the shared-bus topology
used in PCI and instead use point-to-point switching connections 10 . By providing multiple
direct links, such a bus can allow multiple data transfers to occur simultaneously.

HyperTransport, one of the recently proposed standards, is beginning to replace the
front side bus. For each session between nodes, it provides two point-to-point links. Each
link can be anywhere from 2 bits to 32 bits wide, supporting a maximum transfer rate of

10This trend is similar to the direct path based processor data paths we saw in Chapter 7. However, there
are some di(cid:11)erences between the direct paths used in modern processor data paths and the direct links used
in modern IO buses. In the case of IO buses, a direct connection between two nodes is established only
while they are communicating with each other. While these two nodes are communicating, no other node
can access that path.

8.5. Network Architecture

357

6.4 GB per second, operating at 2.6 GHz. By using a HyperTransport-PCI bridge, PCI
devices can be connected to the HyperTransport bus.

PCI-Express is another point-to-point system, allowing for better performance. It is
also scalable. A basic PCI-Express slot will be a 1x connection. This will provide enough
bandwidth for high-speed Internet connections and other peripherals. The 1x means that
there is one lane to carry data.
If a component requires more bandwidth, PCI-Express
2x, 4x, 8x, and 16x slots can be built into motherboards, adding more lanes and allowing
the system to carry more data through the connection. In fact, PCI-Express 16x slots are
already available in place of the AGP graphics card slot on some motherboards. As prices
come down and motherboards built to handle the newer cards become more common, AGP
could fade into history.

Motherboards can incorporate PCI-Express connectors that attach to special cables.
This could allow for completely modular computer system, much like home stereo systems.
The basic unit would be a small box with the motherboard having several PCI-Express
connection jacks. An external hard drive could be connected via USB 2.0 or PCI-Express.
Small modules containing sound cards, video cards, and modems could also be connected
as needed. Thus, instead of one large unit, the computer would be only as large as the
devices connected.

8.5 Network Architecture

Till now we were mostly discussing stand-alone computers, which are not connected to any
computer network. Computer networks are a natural extension of a computer’s IO orga-
nization, enabling it to communicate with other computers and remote peripherals such
as network printers. Unlike the IO buses which typically use electrical wires, the connec-
tions in a computer network could be via some forms of telecommunication media such
as telephone wires, ethernet cables, (cid:12)ber optics, or even microwaves (wireless). Based on
transmission technology, we can categorize computer networks into two: broadcast networks
and point-to-point networks.

Broadcast networks have a single communication channel shared by all the devices on the
network. Any of these devices can transmit short messages called packets on the network,
which are then received by all the other devices. An address (cid:12)eld within the packet speci(cid:12)es
the device for whom it is intended. Upon receiving a packet, a device checks this address
(cid:12)eld to see if it is intended for it.

By contrast, point-to-point networks consist of connections between individual pairs of
machines.

Physical distance is an important metric for classifying computer networks, as di(cid:11)erent
techniques are used at di(cid:11)erent size scales.

Most of today’s desktop computers are instead connected to a network, and therefore it

358

Chapter 8. Microarchitecture | Kernel Mode

is useful for us to have a brief introduction to this topic. A computer network is a collection
of computers and other devices that communicate to share data, hardware, and software.
Each device on a network is called a node. A network that is located within a relatively
limited area such as a building or campus is called a local area network or LAN, and a
network that covers a large geographical area is called a wide area network or WAN. The
former is typically found in medium-sized and large businesses, educational institutions,
and government o(cid:14)ces. Di(cid:11)erent types of networks provide di(cid:11)erent services, use di(cid:11)erent
technology, and require users to follow di(cid:11)erent procedures. Popular network types include
Ethernet, Token Ring, ARCnet, FDDI, and ATM.

Give a (cid:12)gure here

A computer connected to a network can still use all of its local resources, such as hard
drive, software, data (cid:12)les, and printer.
In addition, it has access to network resources,
which typically include network servers and network printers. Network servers can serve as
a (cid:12)le server, application server, or both. A (cid:12)le server serves as a common repository for
storing program (cid:12)les and data (cid:12)les that need to be accessible from multiple workstations|
client nodes|on the network. When an individual client node sends a request to the (cid:12)le
server, it supplies the stored information to the client node. Thus, when the user of a client
workstation attempts to execute a program, the client’s OS sends a request to the (cid:12)le server
to get a copy of the executable program. Once the server sends the program, it is copied
into the memory of the client workstation, and the program is executed in the client. The
(cid:12)le server can also supply data (cid:12)les to clients in a similar manner. An application server, on
the other hand, runs application software on request from other computers, and forwards
the results to the requesting client.

8.5.1 Network Interface Card (NIC)

In order to connect a computer to a network, a network interface card (NIC) is required.
This interface card sends data from the computer out over the network and collects incoming
data for the computer. The NIC for a desktop computer can be plugged into one of the
expansion slots in the motherboard. The NIC for a laptop computer is usually a PCMCIA
card. Di(cid:11)erent types of networks require di(cid:11)erent types of NICs.

The NIC performs the tasks that are at the lowest levels in the communication protocol.
The transmitter in the NIC places the actual bit stream on the communications channel.
Most often, it also performs the task of appending additional (cid:12)elds to the bit stream, such
as a preamble for timing purposes, or error check information. The receiver in the NIC
receives packets addressed to the computer, and most likely strips o(cid:11) preamble bits and
performs error checking.

Modems

8.6.

Interpreting an IO Instruction

359

8.5.2 Protocol Stacks

Computer networks may be implemented using a variety of protocol stack architectures,
computer buses or combinations of media and protocol layers, incorporating one or more of
........... such as ATM, Bluetooth, Ethernet, and FDDI.

Computer networks are also making great strides. Both 100 Mbit Ethernet and switched
Ethernet solutions are being used in new networks and in upgrading networks that cannot
handle the tremendous explosion in bandwidth created by the use of multimedia and the
growing importance of the World Wide Web. ATM represents another potential technology
for expanding even further. To support the growth in tra(cid:14)c, the Internet backbones are
being switched to optical (cid:12)ber, which allows a signi(cid:12)cant increase in bandwidth for long-haul
networks.

8.6

Interpreting an IO Instruction

We have discussed at length the IO system and di(cid:11)erent ways of organizing it. It behoves
us to take a look at how an IO instruction is interpreted for a microarchitecture. As we
recall from Chapters 5 and 6, in both the assembly-level architecture and the ISA, all of the
IO instructions in a systems program involve reading or writing IO registers. The rest of
the IO functions are performed by the IO interfaces in conjunction with the IO controllers
and, of course, the IO devices.

As far as the processor is concerned, the interpretation of an IO instruction is quite
similar to that of a load/store instruction. In fact, for memory-mapped IO addresses, there
is hardly any di(cid:11)erence, except that IO addresses are generally unmapped (no address
translation is required), and that the IO access step (the one analogous to the memory
access step) may take a much longer time than a memory access step.

Let us trace through an IO access microinstruction in a MIPS microarchitecture. This
microinstruction is executed after the ALU operation step in which the IO address is calcu-
lated. The IO request is initially transmitted over the processor-memory bus. The backplane
bus bridge, which is hooked to the processor-memory bus, recognizes the address as an IO
address. It performs the necessary signal conversions, and transmits the request over the
backplane bus. All of the IO interfaces hooked to the backplane bus see the address. The
interface that has the matching address responds to the request.

8.7 System-Level Design

The exact boundaries of chips and printed circuit boards (PCBs) keep changing. In the
early days, the processor was built out of multiple chips. With the current very large scale
integration (VLSI) technologies, the processor is invariably built as a single chip. The
main memory has traditionally been built out of multiple chips. However, that is changing

360

Chapter 8. Microarchitecture | Kernel Mode

now. Currently, parts of the cache memory hierarchy have migrated into the processor chip.
Research e(cid:11)orts are on to completely integrate the processor and memory systems into a
single chip.

In the domain of embedded systems, the situation can be di(cid:11)erent. An entire system is
often built on a single chip (system-on-a-chip).

8.8 Concluding Remarks

8.9 Exercises

1. Explain why interrupts are disabled when interpreting a syscall instruction.

2. What are the fundamental storage components in a paging-based virtual memory
system? Explain with (cid:12)gure(s) the basic working of such a virtual memory system.

3. A virtual memory system uses 15-bit virtual addresses. The physical memory consists
of 8 Kbytes. The page size is 2 Kbytes. The TLB can hold up to 3 entries. Both the
TLB and the page table are replaced using the LRU (least recently used) policy.

(a) Indicate using a diagram how the MMU would split a 15-bit address to get the
virtual page number (VPN).

(b) Consider the following sequence of memory address accesses: 0x6(cid:11)c, 0x7(cid:11)c, 0x6000,
0x4000, 0x3000, 0x2000, 0x7(cid:11)c, 0x2008, 0x74fc, 0x64fc. For each of these accesses, in-
dicate if it would be a TLB hit or miss. Also indicate if it would be a page fault or not.

(c) Draw the (cid:12)nal state of the TLB and the page table.

4. Explain the advantages of USB devices over PCI devices.

5. Explain what is meant by a hierarchical bus organization in a computer system.
Explain why hierarchical bus organizations are used in computer systems.

Chapter 9

Register Tranfer Level
Architecture

Listen to counsel and receive instruction, That you may be wise in your latter days.

Proverbs 19: 20

Our ob jective in this chapter is to study ways of implementing various microarchitecture
speci(cid:12)cations we saw in Chapters 7 and 8. Because of the complexity of hardware design, it
is impractical to directly design a gate-level circuitry that implements the microarchitecture.
Therefore, computer architects have taken a more structured approach by introducing one
or more abstraction levels in between. In this chapter, we study the RTL implementation
of the microarchitectures discussed in the last two chapters. An RTL view of a computer
is restricted to seeing the ma jor storage elements in the computer, and the transfer of
information between them.

A particular microarchitecture may be implemented in di(cid:11)erent ways, with di(cid:11)erent
RTL architectures. Like the design of the higher level architectures that we already saw,
RTL architecture design is also replete with trade-o(cid:11)s. The trade-o(cid:11)s at this level also
involve characteristics such as speed, cost, power consumption, die size, and reliability. For
general-purpose computers such as desktops, the most important factors are speed and
cost. For laptops and embedded systems, the important considerations are size and power
consumption. For space exploration and other critical applications, reliability is of primary
concern.

This chapter addresses some of the fundamental questions concerning RTL architectures,
such as:

(cid:15) What are the building blocks in an RTL architecture, and how are they connected
together?

361

362

Chapter 9. Register Tranfer Level Architecture

(cid:15) What steps should the RTL architecture perform to sequence through a machine
language program, and to execute (i.e., accomplish the work speci(cid:12)ed in) each machine
language instruction?

(cid:15) What are some simple organizational techniques to reduce the number of clock cycles
taken to execute each machine language instruction?

9.1 Overview of RTL Architecture

The RTL architecture implements the (more abstract) microarchitecture. The building
blocks at this level are at a somewhat lower level, such as multiplexers and ........

The RTL architecture follows the data path - control unit dichotomy.
RTL designers often use a language called register transfer language 1 to indicate
the control actions speci(cid:12)ed by the control unit. Each ma jor function speci(cid:12)ed at the
microarchitecture level | as a MAL instruction | is expressed as a sequence of RTL
instructions.

The sequence of operations performed in the data path is determined by commands
generated by the control unit. The control unit thus functions as the data path’s interpreter
of machine language programs.

At the RTL, the computer data path consists of several building blocks and subsys-
tems connected together based on the microarchitecture-level data path speci(cid:12)cation. The
ma jor building blocks of the microarchitectures we saw were registers, memory elements,
arithmetic-logic units, other functional units, and interconnections. Assumptions about the
behavior of these building blocks are used by the RTL designer.

9.1.1 Register File and Individual Registers

An RTL architecture incorporates all of the register (cid:12)les and individual registers speci(cid:12)ed
in the microarchitecture. Apart from these registers, the RTL architecture may incorporate
several additional registers and/or latches. These serve as temporary storage for values gen-
erated or used in the midst of instruction execution. One such register/latch, for instance, is
generally used to store a copy of the ML instruction that is currently being executed. This
register is generally called instruction register (IR). Similarly, the result of an arithmetic
operation may need to be stored in a temporary hardware register before it can be routed
to the appropriate destination register in the register (cid:12)le. The data path may also need to
keep track of special properties of arithmetic and logical operations such as condition codes;

1Modern designs are more frequently speci(cid:12)ed using a hardware description language (HDL) such as
Verilog or VHDL.

9.1. Overview of RTL Architecture

363

most data paths use a register called flags2 .

9.1.2 ALUs and Other Functional Units

We have just looked at RTL implementations of the storage elements present in a microar-
chitecture. Next, we shall consider the elements that serve to perform the operations and
functionalities speci(cid:12)ed in the ISA.

9.1.3 Register Transfer Language

By now, it must be amply clear that the data path does not do anything out of its own
volition; it merely carries out the elementary instruction that it receives from the control
unit. Thus, it is the control unit that decides what register value should be made available
in the processor bus in a particular clock cycle. Likewise, the function to be performed
by the ALU on the input values that are present at its inputs in a clock cycle is also
decided by the control unit. For ease of understanding, we shall use the register transfer
notation to express these elementary instructions sent by the control unit to the data path.
This language is called a register transfer language. Accordingly, an elementary instruction
represented in this language is called an RTL instruction. An RTL instruction may be
composed of one or more elementary operations called RTL operations, performed on data
stored in registers or in memory. An RTL operation can be as simple as copying data
from one physical register to another, or more complex, such as adding the contents of two
physical registers and storing the result in a third physical register. In RTL, data transfer
operations and arithmetic/logical operations are speci(cid:12)ed using a notation called register
transfer notation (RTN). In this notation, a data transfer is designated in symbolic form
by means of the replacement operator (!). An example RTL operation is
PC ! MAR

The semantics of this RTL instruction is quite straightforward: copy the contents of
register PC to register MAR. By de(cid:12)nition, the contents of the source register do not change
as a result of the transfer. The RTL operation does not specify how the microarchitecture
should do this copying; it merely speci(cid:12)es what action the microarchitecture needs to do.
We shall deal with the speci(cid:12)cs of the data transfer when we look at logic-level architectures
in Chapter 9.

2 In some architectures such as the IA-32, the flags register is visible at the ISA level (usually as a set
of condition codes). In MIPS-I, it is not visible at the ISA level.

364

Chapter 9. Register Tranfer Level Architecture

9.2 Example RTL Data Path for Executing MIPS-0 ML Pro-
grams

Designing a computer data path is better caught than taught. Accordingly, in this chapter
we will design several example data paths and illustrate the principles involved. For simplic-
ity and ease of understanding, we restrict ourselves to implementing the microarchitecture-
level data paths that we studied in Chapters 7 and 8 for the MIPS-0 ISA. We will start
with the simple data paths, and later move on to the more complex data paths. This simple
data path will be used in the discussions of the control unit as well.

In order to design an RTL data path, we consider each block in the microarchitecture,
and design it as a collection of combinational logic subcircuits and registers/latches. Thus,
we may add some more non-architected registers: IR to store a copy of the current ML
instruction (bit pattern) being executed, and flags to store important properties of the
result produced by the ALU. The output of the flags register is connected to the control
unit (not shown in (cid:12)gure), which can access its bits individually.

We shall augment the multi-function ALU (Arithmetic and Logic Unit) with an ALU
Input Register (AIR) and an ALU Output Register (AOR) to temporarily store one of the
input values and the output value, respectively.

Figure 9.1 pictorially shows one possible way of interconnecting the combinational sub-
circuits and the registers/latches so as to perform the required functions. This (cid:12)gure sup-
presses information on how the control unit controls the functioning of the subcircuits; these
details will be discussed later. In addition to the processor bus, some dedicated paths have
been provided to perform data transfers that are ine(cid:14)cient to be mapped to the bus. One
such path connects the offset (cid:12)eld of IR to the sign extend unit.

The RTL register IR and its outputs have special properties that warrant further dis-
cussion. The data path uses this register to store the bit pattern of the instruction being
interpreted and executed. Unlike other registers, the outputs from IR are organized as
(cid:12)elds, in line with the di(cid:11)erent (cid:12)elds speci(cid:12)ed in the MIPS instruction formats. These
outputs are depicted in Figure 9.2. The 6-bit opcode and func (cid:12)eld outputs are supplied
to the Opcode Decoder (OD), which produces a binary pattern that uniquely identi(cid:12)es the
opcode of the fetched instruction. This binary pattern is supplied to the processor control
unit (not shown in Figure 9.1). The 5-bit rs, rt, and rd outputs are used to select the
register that is to be read or written into. The register read addresses can be rs or rt,
whereas the register write address can be rt, rd, or 31. (Recall that when a subroutine call
instruction is executed, the return address is written into R31.) The 16-bit offset output
includes the least signi(cid:12)cant 16 bits of IR, and is supplied to a sign extend unit to convert
it into a 32-bit signed integer. This 32-bit output of the sign extend unit is connected to
the processor bus directly. This connection is provided for the purpose of executing the
memory-referencing instructions, the register-immediate ALU type instructions, and the
branch instructions, all of which specify a 16-bit immediate operand.

9.2. Example RTL Data Path for Executing MIPS-0 ML Programs

365

Memory Interface

PC

MAR

System Address Bus

Constants

1

2

4

opcode

rs

rt

rd

func

IR

offset

Opcode

To Control Unit

OD

31

Sign
Extend

32

Memory
Structure

Memory Subsystem

System Data Bus

MUX

5
Register Address

Register File
(RF)

Register Data

AIR

ALU

AOR

Flags

Z N C O

MUX

MUX

MDR

s
u
B
 
r
o
s
s
e
c
o
r
P

Processor Subsystem

Figure 9.1: An RTL Data Path for Implementing the MIPS-0 User Mode ISA

The ALU has two 32-bit data inputs, one of which is always taken from microarchitec-
tural register AIR via a direct path. The other input is taken from the processor bus. The
output of the ALU is fed to microarchitectural register AOR through a direct path. We need
register AOR because the ALU output cannot be routed to a storage location in the same
step via the processor bus (though which one of the ALU data inputs arrives in the same
step). Apart from the normal ALU output, other outputs such as zero, negative, and carry
are routed to a flags register via direct paths.

Finally, the data path includes the memory subsystem, which implements the memory
address space de(cid:12)ned in the user mode ISA. For interfacing the processor bus to the mem-
ory subsystem, this data path uses two special microarchitectural registers, called memory

366

Chapter 9. Register Tranfer Level Architecture

6

opcode

5

rs

5

rt

5

rd

target

IR

6

func

5

offset

Figure 9.2: Outputs of Microarchitectural Register IR, which holds a copy of the ML
Instruction

address register (MAR) and memory data register (MDR). The former is used to store the ad-
dress of the memory location to be read from or written to, and the latter is used to store
the data that is read or to be written. The address stored in MAR is made available to the
address inputs of the memory subsystem through the system address bus during a mem-
ory read/write operation. When a memory read operation is performed, MDR is updated
from the system data bus. Similarly, when a memory write operation is performed, the
contents of MDR are transmitted to the appropriate memory through the system data bus.

9.2.1 RTL Instruction Set

The process of executing a machine language (ML) instruction on the data path can be
broken down into a sequence of more elementary steps. We saw in Section 7.1 how we can
use a register transfer language to express these elementary steps. We can de(cid:12)ne a set of
RTL operations for the MIPS-0 data path de(cid:12)ned in Figure 9.1. An RTL operation can
specify the (cid:13)ow of data between two or more storage locations only if there is a connection
between them. Thus

MAR ! AOR

cannot be a valid RTL operation in this data path, because there is no direct connection
between MAR and AOR. Similarly,

MDR ! ALU

can not be a valid RTL operation, because ALU is combinational logic, and not a storage
device.

Normally, all bits of a register are involved in a transfer. However, if a subset of the bits
is to be transferred, then the speci(cid:12)c bits are identi(cid:12)ed by the use of pointed brackets. The
RTL operation

IR<15:0> ! MDR

speci(cid:12)es that bits 15 to 0 of IR are copied to MDR. Similarly, memory locations or general-
purpose registers (GPRs) are speci(cid:12)ed with square brackets. The RTL operation

9.2. Example RTL Data Path for Executing MIPS-0 ML Programs

367

R[rs] ! MDR

indicates that the contents of the GPR whose address is present in the rs (cid:12)eld (of IR) are
transferred to MDR. The rs (cid:12)eld speci(cid:12)es a particular GPR. Similarly, the RTL operation
MDR ! M[MAR]

indicates that the contents of MDR are transferred to the memory location whose address is
present in MAR.

If the interconnections of the data path are rich enough to allow multiple RTL opera-
tions in the same time period, these can be denoted by writing them in the same line, as
follows:

AIR + 4 ! AOR;

M[MAR] ! MDR

speci(cid:12)es that in the same step, the value in AIR is incremented by 4 and written to AOR,
and the contents of the memory location addressed by MAR are copied to MDR.

Finally, for RTL operations that are conditional in nature, an \if " construct patterned
after the C language’s \if " construct is de(cid:12)ned.
AOR ! PC
if (Z)

indicates that if the zero (cid:13)ag is equal to 1, the contents of AOR is copied to PC; otherwise no
action is taken.

9.2.2 RTL Operation Types

The actions performed by RTL operations can be classi(cid:12)ed into three di(cid:11)erent types:

1. Data transfer: These RTL operations copy the contents of one register/memory
location to another. An example data transfer RTL operation is PC ! MAR.

2. Arithmetic/Logic: These RTL operations perform arithmetic or logical operations
on data stored in registers. Data transfers can only occur along the interconnections
provided in the data path, from one register/memory location to another. An example
arithmetic RTL operation is AIR + PC ! AOR.

3. Conditional: These RTL operations perform a function such as a data tranfer if and
only if the speci(cid:12)ed condition is satis(cid:12)ed. An example conditional RTL operation is
if (Z) AOR ! PC.

Table 9.1 gives a list of useful RTL operations for the microarchitecture of Figure 9.1.
A given RTL operation may specify actions of more than one type. For example, the RTL
operation if (Z) AOR ! PC is a conditional as well as a data transfer RTL operation. The
RTL operations done (in parallel) in a single step form an RTL instruction.

368

No.

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

18
19
20
21

22

Chapter 9. Register Tranfer Level Architecture

RTL Operation
for Data Path

Comments

Data Transfer Type
PC ! MAR
PC ! MDR
PC ! AIR
PC ! R[31]
M[MAR] ! MDR
Read from memory
MDR ! M[MAR] Write to memory
MDR ! IR
MDR ! AIR
MDR ! R[rt]
MDR ! PC
R[rs] ! AIR
R[rs] ! PC
R[rt] ! AIR
R[rt] ! MDR
SE(offset) ! AIR
AOR ! R[rd]
AOR ! R[rt]
AOR ! MAR
Arithmetic/Logic Type
AIR op R[rt] ! AOR
AIR op const ! AOR
AIR op SE(offset) ! AOR
AIR op R[rt] ! Z

Copy sign-extended Offset (cid:12)eld of IR to AIR

Conditional Type
if (Z) AOR ! PC

Table 9.1: A List of Useful RTL Operations for the Data Path of Figure 9.1

9.2.3 An Example RTL Routine

Now that we are familiar with the syntax of RTL as well as the useful RTL operations for
the data path of Figure 9.1, we shall look at a simple sequence of RTL operations called an
RTL routine. Let us write an RTL routine that adds the contents of registers speci(cid:12)ed in
the rs and rt (cid:12)elds of IR, and writes the result into the register speci(cid:12)ed in the rd (cid:12)eld
of IR. The astute reader may have noticed that executing this RTL routine amounts to
executing the MIPS-I machine language instruction and rd, rs, rt in the data path of
Figure 9.1, provided the binary pattern of this instruction is languishing in IR.

In theory, we can write entire programs in RTL that can perform tasks such as ‘printing

9.3.

Interpreting ML Programs by RTL Routines

369

Step
0
1
2

Comments
RTL Instruction
Copy value of register rs to AIR
R[rs] ! AIR
AIR AND R[rt] ! AOR
AND this value with value in register rt
AOR ! R[rd] Write result into register rd

Table 9.2: An Example RTL Routine for the Data Path of Figure 9.1

\hello, world!" ’ and more; we will, of course, need a special storage for storing the RTL
programs and a mechanism for sequencing through the RTL program. Having seen the
di(cid:14)culty of writing programs in assembly language and machine language, one can easily
imagine the nightmare of writing entire programs in RTL! However, developing an RTL
program may not be as bad as it sounds, given that we can develop translator software such
as assemblers that take machine language programs and translate them to RTL programs.
The real di(cid:14)culty is that RTL programs will be substantially bigger than the corresponding
ML programs, thereby requiring very large amounts of storage. This is where interpretation
comes in. By generating the required RTL routines on-the-(cid:13)y at run-time, the storage
requirements are drastically reduced, as we will see next.

9.3

Interpreting ML Programs by RTL Routines

Having discussed the basics of de(cid:12)ning RTL routines, our next step is to investigate how
machine language programs can be interpreted by a sequence of RTL instructions that are
de(cid:12)ned for a particular data path. This section discusses how RTL instructions can be put
together as an RTL routine to carry out this interpretation for each instruction de(cid:12)ned
in the ISA. An RTL routine is allowed to freely use and modify any of the ISA-invisible
microarchitectural registers. The ISA-visible registers, however, can be modi(cid:12)ed only as
per the semantics of the ML instruction being interpreted.

9.3.1

Interpreting the Fetch and PC Update Commands for Each In-
struction

When implementing ISAs supporting (cid:12)xed length instructions, the actions required to per-
form the fetch command are the same for all instructions. The MIPS-0 ISA is no exception.
We shall consider this phase of instruction execution (cid:12)rst. In the data path of Figure 9.1,
register PC keeps the memory address of the next ML instruction to be fetched. To fetch
this instruction from memory, we have to utilize the memory interface provided in the data
path. This interface consists of two registers, MAR and MDR, that store the memory address
and data, respectively. Table 9.3 gives an RTL routine that implements the fetch phase of
executing the instruction. This routine also includes the RTL instructions for updating PC
after the fetch phase so as to point to the next ML instruction to be executed.

370

Chapter 9. Register Tranfer Level Architecture

Step RTL Instruction Comments
Fetch and Decode
PC ! MAR
M[MAR] ! MDR
MDR ! IR

Decode instr
PC increment
PC ! AIR
AIR + 4 ! AOR
AOR ! PC

Goto execute phase

0
1
2
3

4
5
6

Table 9.3: An RTL Routine for Fetching a MIPS-0 ML Instruction in the Data Path of
Figure 9.1

The (cid:12)rst step involves copying the contents of PC to MAR so that this address can be
supplied through the system address bus to the memory system. Thus, at the end of
step 0, MAR has the address of the memory location that contains the instruction bit pat-
tern. Step 1 speci(cid:12)es a memory read operation, during which the contents of the memory
location speci(cid:12)ed through the system address bus by MAR are read from the memory. The
instruction bit pattern so read is placed on the system data bus, from where it is loaded
into memory interface register MDR in the processor data path. Thus, at the end of step
1, the instruction bit pattern is present in MDR. In this RTL routine, we consider the time
taken to perform this step as one clock cycle, although the exact number of cycles taken
depends on the speci(cid:12)cs of the memory system used. In step 2, the instruction bit pattern
is copied from MDR into microarchitectural register IR.

Once the instruction bit pattern is copied into IR, the instruction decoding circuitry (not
shown in (cid:12)gure) decodes the bit pattern. The exact manner in which instruction decoding
is performed is not speci(cid:12)ed here; we have earmarked a separate step for carrying out the
decode operation. The opcode and funct (cid:12)elds of IR uniquely identify the instruction.
Steps 0-3 thus constitute the RTL routine for the fetch phase of instruction interpreta-
tion. Naturally, this portion is the same for every instruction in the MIPS ISA because
all instructions have the same size. Had the MIPS ISA used variable length instructions,
this fetch process would have to be repeated as many times as the number of words in the
instruction.

After executing an ML instruction, the system needs to go back to step 0 so as to execute
the next instruction in the ML program. However, prior to that, it has to increment PC to
point to the next instruction; otherwise the same ML instruction gets executed repeatedly.
In this RTL routine, this update of PC is done immediately after completing the instruction
fetch. Thus, in steps 4-6, PC is incremented by 4 to point to the next instruction in the
executed machine language program. After step 6, the system goes to the execute phase.

9.3.

Interpreting ML Programs by RTL Routines

371

This RTL routine takes 7 clock cycles to complete. We can, in fact, perform the PC
increment function in parallel to the instruction fetch function, and reduce the total number
of clock cycles required for the two activities. Table 9.4 provides the modi(cid:12)ed RTL routine,
which requires only 4 clock cycles. In this routine, in steps 0 and 1, the updated value of
PC is calculated in parallel with the transfer of the instruction bit pattern from the main
memory to MDR. It is important to note that multiple RTL operations can be done in parallel,
only if they do not share the same bus or destination register. In general, the more the
connectivity provided in a data path, the more the opportunities for performing multiple
RTL operations in parallel.

Step

0
1
2
3

RTL Instruction
Comments
Fetch, Decode, and PC increment
PC ! MAR;
PC ! AIR
M[MAR] ! MDR; AIR + 4 ! AOR Read instr
MDR ! IR
AOR ! PC

Decode instr
Goto execute phase

Table 9.4: An Optimized RTL Routine for Fetching a MIPS-0 ML Instruction in the Data
Path of Figure 9.1

9.3.2

Interpreting Arithmetic/Logical Instructions

We just saw an RTL routine for fetching an instruction. Next, let us consider the execute
phase of ML instructions. Unlike the fetch phase, this phase is di(cid:11)erent for di(cid:11)erent ML
instructions. Therefore, the RTL routines for the execute phase will be di(cid:11)erent for the
di(cid:11)erent instructions. We shall consider one instruction each from the four types of ML
instructions: (i) data transfer instruction, (ii) arithmetic/logical instruction, (iii) control
(cid:13)ow changing instruction, and (iv) syscall instruction.

Let us start by considering an arithmetic instruction. We shall put together a sequence
of RTL instructions to interpret an arithmetic/logical instruction.

Example: Consider the MIPS ADDU instruction whose symbolic representation is addu
rd, rs, rt. Its encoding is given below.

000000

rs

rt

rd

ADD

The (cid:12)elds rs, rt, and rd specify register numbers; the (cid:12)rst two of these contain the data
values to be added together as unsigned intergers. The rd (cid:12)eld indicates the destination
register, i.e., the register to which the result should be written to, as long as it is not

372

Chapter 9. Register Tranfer Level Architecture

register $0. In this data path, the addition of the two register values can be done in the
ALU. However, there is only a single bus to route the two register values as well as the
result of the addition operation. Therefore, we will have to do the routing of data values in
multiple steps and use the temporary registers AIR and AOR.

Table 9.5 speci(cid:12)es a sequence of RTL instructions for carrying out the execute phase of
this instruction in the data path of Figure 9.1. Let us go through the working of this RTL
routine. We name the (cid:12)rst RTL instruction of the routine as step 4, as the execute phase
is a continuation of the fetch phase, which ended at step 3.

Step

4
5
6

RTL Instruction
Comments
Execute phase
R[rs] ! AIR
Perform arithmetic operation
AIR + R[rt] ! AOR
if (rd) AOR ! R[rd] Write result

Table 9.5: An RTL Routine for the Execute Phase of the Interpretation of the MIPS-0
ISA Instruction Represented Symbolically as addu rd, rs, rt. This RTL Routine is for
executing the ML instruction in the Data Path of Figure 9.1

Step 4 begins the execution phase. First, the register operands must be fetched from
the register (cid:12)le. In step 4, the value in the rs (cid:12)eld of IR is used as an address to read the
general-purpose register numbered rs into microarchitectural register AIR. In step 5, the
contents of general-purpose register numbered rt are read and supplied to the ALU, which
adds it to the contents of AIR, and stores the result in the microarchitectural register AOR.
In step 6, the contents of AOR are transfered to the general-purpose register numbered rd,
if rd is non-zero. By performing this sequence of RTL instructions in the correct order, the
ML instruction addu rd, rs, rt is correctly interpreted.

9.3.3

Interpreting Memory-Referencing Instructions

Let us next put together a sequence of RTL instructions to fetch and execute a memory-
referencing machine language instruction. Because all MIPS instructions are of the same
length, the RTL routine for the fetch part of the instruction is the same as before; the dif-
ferences are only in the execution part. Consider the MIPS load instruction whose symbolic
representation is lw rt, offset(rs). The semantics of this instruction are to copy to
GPR rt the contents of memory location whose address is given by the sum of the contents
of GPR rs and sign-extended offset. We need to come up with a sequence of RTL in-
structions that e(cid:11)ectively fetch and execute this instruction in the data path of Figure 9.1.

100011

rs

rt

offset

9.3.

Interpreting ML Programs by RTL Routines

373

The interpretation of a memory-referencing instruction requires the computation of an
address. For the MIPS ISA, address calculation involves sign-extending the offset (cid:12)eld of
the instruction to form a 32-bit signed o(cid:11)set, and adding it to the contents of the register
speci(cid:12)ed in the rs (cid:12)eld of the instruction. In this data path, the address calculation is done
using the same ALU, as no separate adder has been provided. With this introduction, let
us look at the RTL routine given in Table 9.6 to interpret this lw instruction.

Step

4
5
6
7
8

Comments

RTL Instruction
Execute phase
R[rs] ! AIR
AIR + SE(offset) ! AOR
AOR ! MAR
Read from memory
M[MAR] ! MDR
if (rt) MDR ! R[rt] Write loaded value

Compute memory address

Table 9.6: An RTL Routine for the Execute Phase of the Interpretation of the MIPS-0 ISA
Instruction Represented Symbolically as lw rt, offset(rs). This RTL Routine is for
executing the ML instruction in the Data Path of Figure 9.1

In step 4, the contents of register speci(cid:12)ed in the rs (cid:12)eld is copied to AIR. In step 5,
the sign-extended offset value is added to this value to obtain the memory address. This
memory address is stored in AOR, and is copied to MAR in the next step. Recall that for a
memory transfer to take place, the address of the memory location must be present in MAR.
In step 7, the actual memory read is performed and the value obtained is stored in MDR;
this step is the same as step 2 of the fetch routine. Finally, in step 8, the loaded value is
copied to the register speci(cid:12)ed in the rt (cid:12)eld of IR.

9.3.4

Interpreting Control-Changing Instructions

The instructions that we interpreted so far | addu and lw | do not involve control (cid:13)ow
changes that cause deviations from straightline sequencing in the ML program. Next let us
see how we can interpret control-changing instructions, which involve modifying PC, usually
based on a condition.

Example: Consider the MIPS-0 conditional branch instruction whose symbolic representa-
tion is beq rs, rt, offset. The semantics of this instruction state that if the contents
of GPRs rs and rt are equal, then the value offset (cid:2) 4 + 4 should be added to PC so as
to cause a control (cid:13)ow change3 ; otherwise, PC is incremented by 4 as usual. The encoding
of this instruction is given below:

3The actual MIPS ISA uses a delayed branch scheme; i.e., the control (cid:13)ow change happens only after
executing the ML instruction that follows the branch instruction in the program. We avoid delayed branches
to keep the discussion simple.

374

Chapter 9. Register Tranfer Level Architecture

000100

rs

rt

offset

Step

4
5
6
7
8
9

RTL Instruction
Comments
Execute phase
PC ! AIR
SE(offset) << 2 ! AOR Multiply offset by 4
AIR + AOR ! AOR Calculate branch target address
R[rs] ! AIR
R[rt] == AIR ! Z
if (Z) AOR ! PC

Evaluate branch condition
Update PC if condition is satis(cid:12)ed

Table 9.7: An RTL Routine for the Execute Phase of the Interpretation of the MIPS-0 ISA
Instruction Represented Symbolically as beq rs, rt, offset. This RTL Routine is for
executing the ML instruction in the Data Path of Figure 9.1

Table 9.7 presents an RTL routine to execute this instruction in the data path given in
Figure 9.1. The execute routine has 6 steps numbered 4-9. The (cid:12)rst part of the routine
(steps 4-6) calculates the target address of the branch instruction. In step 4, the incremented
PC value is copied to AIR. In the next step, the sign-extended offset value is multiplied by
4 by shifting it left by 2 bit positions. In the next step, it is added to the copy of PC value
present in AIR to obtain the target address of the beq instruction. In step 8, the contents
of register rt are compared against those of AIR (which were copied from register rs), and
the result of the comparison is stored in (cid:13)ag Z. That is, Z is set to 1 if they are the same,
and reset to 0 otherwise. In the last step, PC is updated with the calculated target value
present in AOR, if (cid:13)ag Z is 1. Thus, if (cid:13)ag Z is not set, then PC retains the incremented value
it obtained in step 3, which is the address of the instruction following the branch in the
machine language program being interpreted.

9.3.5

Interpreting Trap Instructions

We have seen the execute phase of the interpretation for all of the instruction types except
the syscall instructions. Let us next look at how the data path performs this interpreta-
tion, which is somewhat di(cid:11)erent from the previously seen ones. Like the control-changing
instructions, syscall instructions also involve modifying PC. For the MIPS-I ISA, the pc is
updated to 0x80000080. In addition, the machine is placed in the kernel mode. We shall
take a detailed look at the corresponding RTL sequence in Section 8.1.1, along with other
kernel mode implementation issues.

9.4. RTL Control Unit: An Interpreter for ML Programs

375

9.4 RTL Control Unit: An Interpreter for ML Programs

We have seen how individual machine language instructions can be interpreted using a
sequence of RTL instructions for execution in a data path. Stated di(cid:11)erently, execution
of an RTL routine in the data path is tantamount to executing the corresponding ML
instruction. And, the execution of many such RTL routines is tantamount to executing an
entire ML program. In other words, when the data path executes an RTL program | a
sequence of RTL routines | it indirectly executes an ML program.

Who generates the RTL routine for each ML instruction? Where is the routine stored
after it is generated? These are questions we attempt to answer in this section. Generation
of the RTL instructions, in the proper sequence, is the function of the control unit. The
control unit thus serves as the interpreter of machine language programs. In other words,
it is the unit that converts ML programs to equivalent RTL programs. The algorithm for
doing this conversion is hardwired into the control unit hardware.

We can think of two di(cid:11)erent granularities at which the control unit may supply RTL
routines to the data path4 . The (cid:12)rst option is to generate the entire RTL routine for the
execution of an instruction (about 3-5 RTL instructions) in one shot, and give it to the data
path. The data path will then need a storage structure to store the entire routine. It will
also have to do its own sequencing through this RTL routine. Because of shifting some of
the sequencing burden to the data path, the control unit will be somewhat simpler.

In the alternate option | the more prevalent one | RTL instructions are supplied one
at a time to the data path. The control unit is then responsible for sequencing through the
RTL sequence. The data path just needs to carry out the most recent RTL instruction that
it has received from the control unit. We will be dealing exclusively with this option in this
chapter.

Figure 9.3 shows the general relationship between the overall data path and the proces-
sor control unit. The (cid:12)gure explicitly shows only the processor’s data path. The processor
control unit periodically receives the status of the data path, and speci(cid:12)es the next mi-
croinstruction to be performed by the data path. Thus, the processor control unit controls
the processing of data within the processor data path, as well as the (cid:13)ow of data within,
to, and from the processor data path. In this section we show how to design a processor
control unit that generates microinstructions in a timely fashion.

9.4.1 Developing an Algorithm for RTL Instruction Generation

Before the design of the control unit can begin, it is imperative to de(cid:12)ne the processor data
path (including the interface to the processor-memory bus) as well as the RTL routines

4An important point is in order here. RTL instructions are not supplied to the data path in the English-
like RTN format that we have been using in this chapter. Rather, they are supplied in an encoded manner
called microinstructions. In the ensuing discussion, we often use the terms \microinstruction" and \RTL
instruction" interchangeably.

376

Chapter 9. Register Tranfer Level Architecture

Processor

Status

Control
Unit

(ML Instruction, Flags)
Control

Data
Path

(Microinstruction)

Address

Data

Control

Figure 9.3: A Microarchitectural View of the Interaction between the Processor Control
Unit, Processor Data Path, and the Rest of the System

for interpreting each of the machine language instructions. The control unit designer then
translates all of the RTL routines into an encoded form called microroutines either by hand
or by a micro-assembler (which is a program similar to an assembler).
In implementing
the control unit, there are two distinct aspects to deal with: (i) proper sequencing through
the steps, and (ii) generating the appropriate microinstruction for each step. We can achieve
both of these tasks by (cid:12)rst combining the RTL routines for all of the machine language
instructions into a single microprogram. Such an algorithm will contain not only intra-
routine sequencing, but also inter-routine sequencing.

We shall use the now familiar data path of Figure 9.1 (on page 365) for designing the
sample control unit. For this data path, we had already seen the RTL routines to be
generated to carry out the fetch phase of the MIPS-0 ISA instructions, and the execute
phase of the following three instructions: (i) addu rd, rs, rt, (ii) lw rt, offset(rs),
and (iii) beq rs, rt, offset. These routines are available in Tables 9.4-9.7.

The next step in the design is to put all of these information together to develop a
(cid:13)owchart or algorithm for the functioning of the control unit. Table 9.8 combines the
generation of all of these RTL routines into a single algorithm starting at step 0. The
(cid:12)rst column, step, indicates the current step performed by the control unit, and the next
column, next step, indicates the (cid:13)ow of control through this algorithm. The third (cid:12)eld
indicates the RTL instruction to be generated for the current step, and the last (cid:12)eld indicates
comments, if any. The next step (cid:12)eld facilitates the processor control unit in its seqencing
function. As can be seen from the next step entries, control (cid:13)ow through the control unit
algorithm is mostly straightline in nature. i.e., the next step is generally the immediately
following step.

In this algorithm, the generation of the instruction fetch routine appears just once (as it
is the same for all instructions), and includes steps 0 through 3. The decode process takes
place in step 3, and enables the control circuitry to choose the appropriate RTL instructions
for the execution phase. The next step (cid:12)eld for step 3 has an entry of n. This indicates
that a multi-way branch is required in the control unit’s algorithm, based on the opcode of
the decoded ML instruction. Notice also that the next step (cid:12)eld is set to 0 at the end of
each execute routine to indicate that control has to go back to step 0 after the execution of
that RTL instruction.

9.4. RTL Control Unit: An Interpreter for ML Programs

377

Step

Next
Step

0
1
2
3

4
5
6

7
8
9
10
11

12
13
14
15
16
17

1
2
3
n

5
6
0

8
9
10
11
0

13
14
15
16
17
0

RTL Instruction Generated
for Data Path
Fetch phase of every instruction
PC ! MAR;
PC ! AIR
M[MAR] ! MDR; AIR + 4 ! AOR
MDR ! IR
AOR ! PC

Comments

Decode
Branch based on opcode

Execute phase of addu
R[rs] ! AIR
R[rt] + AIR ! AOR
if (rd) AOR ! R[rd] End of routine
Go to step 0

Execute phase of lw
R[rs] ! AIR
AIR + SE(offset) ! AOR
AOR ! MAR
M[MAR] ! MDR
if (rt) MDR ! R[rt] End of routine
Go to step 0

Execute phase of beq
PC ! AIR
SE(offset) << 2 ! AOR
AIR + AOR ! AOR
R[rs] ! AIR
R[rt] == AIR ! Z
if (Z) AOR ! PC

Execute phase of next ML instruction
...

End of routine
Go to step 0

Table 9.8: The Algorithm followed by the Control Unit for Generating RTL Instructions so
9.4.2 Designing the Control Unit as a Finite State Machine
as to Interpret MIPS-0 ML Programs for Execution in the Single-Bus Data Path of Figure
9.1
On inspecting the algorithm given in Table 9.8, we can see that the processor control unit
essentially goes through an in(cid:12)nite loop starting from step 0. This control unit can be
designed as a (cid:12)nite state machine (FSM). In order to do this, we can develop a state
transition diagram for this algorithm. This state diagram corresponds to a Moore-type
FSM, where the output values are strictly a function of the current state. Figure 9.4
presents a state transition diagram for this FSM. In this diagram, each step of Table 9.8 is
implemented by a state, which decides the microinstruction to be generated when in that
state. The (cid:12)gure also indicates the conditions that cause the control unit FSM to go from
one state to another. Most of the transitions are not marked by a condition, which means
that those transitions occur unconditionally. The (cid:12)rst 4 states, S0 - S3, correspond to the
instruction fetch routine; the microinstruction to be generated in each of these states s

378

Chapter 9. Register Tranfer Level Architecture

corresponds to what is given in Table 9.8 for step s. At the end of the microroutine for
instruction fetch, the fetched ML instruction would have been decoded, and a multi-way
branch occurs from state S3 to the appropriate microroutine for executing the instruction.
By specifying the states and their transitions, we specify the microinstruction the processor
control unit must generate in order for the data path to fetch, decode, and execute every
instruction in the ISA.

S0

PC

MAR, AIR

S1

M[MAR]

MDR; AIR+4

AOR

S2

MDR

IR

S3

AOR

PC

addu

lw

beq

S4

S5

S6

S7

S8

S9

S10

S11

S12

S13

S14

S15

S16

S17

Fetch & Decode
ML Instruction

Multi−way
Branch

Execute
ML Instruction

addu rd,rs,rt

lw rt,offset(rs)

beq rs,rt,offset

Figure 9.4: A State Transition Diagram for a Processor Control Unit for the Single-Bus
Data Path of Figure 9.1

Having developed a state transition diagram to interpret ML instructions, the next step
is to design a hardware unit that implements this state transition diagram. Figure 9.5 gives
a high-level block diagram of a generic processor control unit FSM. This FSM performs two
functions:

9.4. RTL Control Unit: An Interpreter for ML Programs

379

(cid:15) Sequencing through the control unit states

(cid:15) Generating the microinstruction for the current state

The left side of the (cid:12)gure handles the sequencing part, and the right side of the (cid:12)gure
handles the microinstruction generation part.

opcode

func

I
R

addu
lw
beq

Instruction
Decoder

Next State
Generator

done

Control Unit

Clock

S
t
a
t
e

MIR

Microinstruction
Generator

Sequencing through Microprogram

Generating the Microinstruction

Micro−operations for Data Path

Figure 9.5: A Block Diagram of a Processor Control Unit

The FSM’s state is stored in a state register. The microinstruction generator block
takes the contents of state as input and generates the corresponding microinstruction,
which can be optionally latched onto a MIR (microinstruction register). Please be careful
not to confuse state with PC; MIR with IR; and microinstructions with ML instructions.

The next state value is calculated by a combinational block called next state generator,
based on the current state and some inputs from the data path. Normally, the state value
is incremented at each clock pulse, causing successive microinstructions to be generated.
The following 3 events cause a deviation from this straightline sequencing.

(cid:15) When the current state corresponds to the end of the fetch microroutine (state 3 in our
example), a new ML instruction has just been decoded. The next state generator
has to deviate from straightline sequencing so as to start generating the execute-
microroutine corresponding to the newly decoded ML instruction. It determines the

380

Chapter 9. Register Tranfer Level Architecture

next state based on the output of the instruction decoder. Thus, the control unit is
able to generate the microinstructions for executing the newly decoded ML instruction.

(cid:15) Similarly, when the current state corresponds to the end of an execute-microroutine,
the next state generator block deviates from straightline sequencing, and initial-
izes state to zero so as to start fetching the next ML instruction.

(cid:15) Finally, some microinstructions require multiple clock cycles to complete execution.
For instance, a memory read microinstruction (written symbolically in RTL as M[MAR]
! MDR) may not be completed in a single clock cycle. Worse still, the completion time
of this microinstruction may not even be deterministic due to a variety of reasons,
as we will see later in this chapter. The easiest approach to handle state updation
when executing a multi-cycle microinstruction is to maintain the same state value
until the microinstruction is completed. For microinstructions with non-deterministic
execution latencies, information regarding their completion can be conveyed by the
data path to the control unit using a signal called done.

Thus, the updation of the state register in the control unit is performed by the next state
generator block, based on:

1. the contents of the state register

2. the outputs of the instruction decoder

3. the done signal from the data path.

9.4.3

Incorporating Sequencing Information in the Microinstruction

The next state generator block can be simpli(cid:12)ed by including in each microinstruction
the necessary sequencing information. That is, the goto operations of Table 9.8 can also be
included in the microinstruction (in an encoded form). With such an arrangement, when
a microinstruction is generated, if it contains a goto operation, this information can be
fed back to the next state generator block, as shown in Figure 9.6. For instance, if
the current microinstruction includes the encoded form of goto Sn, this information (along
with the instruction decoder output) will be used by next state generator to deviate from
straightline sequencing. Similarly, while sequencing through an execute-microroutine, if the
current microinstruction includes the binary equivalent of goto S0, next state generator
resets the state register to zero, so that in the next cycle the processor can begin fetching
the next ML instruction. Notice that the microinstruction generator block becomes
more complex.

At the digital logic level, the sequencing part of the control unit FSM can be implemented
using a sequencer plus a decoder or one (cid:13)ip-(cid:13)op per state. The sequencer can be built in
many ways. One possibility is to use a counter or shift register with synchronous reset

9.4. RTL Control Unit: An Interpreter for ML Programs

381

Clock

Control Unit

Next State
Generator
(Simpler)

S
t
a
t
e

Microinstruction
Generator
(More Complex)

opcode

I
R

func

addu
lw
beq

Instruction
Decoder

done

goto S*

MIR

Micro−operations for Data Path

Figure 9.6: A Block Diagram of a Processor Control Unit that encodes Sequencing Infor-
mation in the Microcode to Simplify the Next State Generator Block

and parallel loading facility. The microinstruction generator can also be built in more
than one way. Two common methods involve the use of either discrete logic or ROM. The
control units so designed are called hardwired control and microprogrammed control,
respectively. We will discuss both of these approaches in Chapter 10.

9.4.4 State Reduction

The state diagram of the control unit we just designed incorporates a separate microroutine
for the execution part of each ML instruction. For an ISA that speci(cid:12)es hundreds of instruc-
tions, this approach is likely to produce an FSM with thousands of states. Many of the states
in such an FSM can actually be combined by considering the fact that the execution micro-
routines of similar instructions have many equivalent states. For instance, the (cid:12)rst 3 states
in the execution microroutine of a memory-referencing instruction deal with computing the
e(cid:11)ective address, and will be the same for all memory-referencing instructions. Just like we
used a common microroutine for the fetch phase of all ML instructions, we can use a com-
mon address calculation microroutine for all memory-referencing instructions in the MIPS
ISA. If we take this approach, we can get a reduced FSM as shown in Figure 9.7. This FSM
has far fewer states than the one given earlier, allowing a much smaller Microinstruction
Generator to be used. Notice, however, that we will have more goto operations, which

