Journal of Machine Learning Research 16 (2015) 1-46

Submitted 1/14; Revised 7/14; Published 1/15

Statistical Decision Making for Optimal Budget Allocation
in Crowd Labeling

Xi Chen
Stern School of Business
New York University
New York, New York, 10012, USA

Qihang Lin
Tippie Col lege of Business
University of Iowa
Iowa City, Iowa, 52242, USA

Dengyong Zhou
Microsoft Research
Redmond, Washington, 98052, USA

Editor: Yuan (Alan) Qi

xichen@nyu.edu

qihang-lin@uiowa.edu

dengyong.zhou@microsoft.com

Abstract

It has become increasingly popular to obtain machine learning labels through commercial
crowdsourcing services. The crowdsourcing workers or annotators are paid for each label
they provide, but the task requester usually has only a limited amount of the budget. Since
the data instances have diﬀerent levels of labeling diﬃculty and the workers have diﬀerent
reliability for the labeling task, it is desirable to wisely allocate the budget among all the
instances and workers such that the overall labeling quality is maximized. In this paper,
we formulate the budget allocation problem as a Bayesian Markov decision process (MDP),
which simultaneously conducts learning and decision making. The optimal allocation pol-
icy can be obtained by using the dynamic programming (DP) recurrence. However, DP
quickly becomes computationally intractable when the size of the problem increases. To
solve this challenge, we propose a computationally eﬃcient approximate policy which is
called optimistic knowledge gradient. Our method applies to both pull crowdsourcing mar-
ketplaces with homogeneous workers and push marketplaces with heterogeneous workers.
It can also incorporate the contextual information of instances when they are available.
The experiments on both simulated and real data show that our policy achieves a higher
labeling quality than other existing policies at the same budget level.
Keywords:
crowdsourcing, budget allocation, Markov decision process, dynamic pro-
gramming, optimistic knowledge gradient

1. Introduction

In many real applications, data are usually collected without any innate label. For example,
a digital camera will not automatically tag a picture as a portrait or a landscape. A
traditional approach for data labeling is to hire a small group of experts to provide labels
for the entire set of data. However, for large-scale data, such an approach becomes ineﬃcient
and very costly. Thanks to the advent of many online crowdsourcing services, e.g., Amazon

c(cid:13)2015 Xi Chen and Qihang Lin and Dengyong Zhou.

Chen, Lin and Zhou

Mechanical Turk, a much more eﬃcient way is to post unlabeled data to a crowdsourcing
marketplace, where a big crowd of low-paid workers can be hired instantaneously to perform
labeling tasks.
Despite its high eﬃciency and immediate availability, crowd labeling raises many new
challenges. Since labeling tasks are tedious and workers are usually non-experts, labels
generated by the crowd suﬀer from low quality. As a remedy, most crowdsourcing services
resort to labeling redundancy to reduce the labeling noise, which is achieved by collecting
multiple labels from diﬀerent workers for each data instance. In particular, a crowd labeling
process can be described as a two phase procedure:

1. In the ﬁrst phase, unlabeled data instances are assigned to a crowd of workers and
multiple raw labels are collected for each data instance.

2. In the second phase, for each data instance, one aggregates the collected raw labels
to infer its true label.

In principle, more raw labels will lead to a higher chance of recovering the true label.
However, each raw label comes with a cost: the requester has to pay workers pre-speciﬁed
monetary reward for each label they provide, usually, regardless of the label’s correctness.
For example, a worker typically earns 10 cents by categorizing a website as porn or not.
In practice, the requester has only a limited amount of budget which essentially restricts
the total number of raw labels that he/she can collect. This raises a challenging question
central in crowd labeling: What is the best way to al locate the budget among data instances
and workers so that the overal l accuracy of aggregated labels is maximized ?
The most important factors that decide how to allocate the budget are the intrinsic char-
acteristics of data instances and workers: labeling diﬃculty/ambiguity for each data instance
and reliability/quality of each worker. In particular, an instance is less ambiguous if its label
can be decided based on the common knowledge and a vast ma jority of reliable workers will
provide the same label for it. In principle, we should avoid spending too much budget on
those easy instances since excessive raw labels will not bring much additional information.
In contrast, for an ambiguous instance which falls near the boundary of categories, even
those reliable workers will still disagree with each other and generate inconsistent labels.
For those ambiguous instances, we are facing a challenging decision problem on how much
budget that we should spend on them. On one hand, it is worth to collect more labels to
boost the accuracy of the aggregate label. On the other hand, since our goal is to maximize
the overal l labeling accuracy, when the budget is limited, we should simply put those few
highly ambiguous instances aside to save budget for labeling less diﬃcult instances.
In
addition to the ambiguity of data instances, the other important factor is the reliability
of each worker and, undoubtedly, it is desirable to assign more instances to those reliable
workers. Despite their importance in deciding how to allocate the budget, both the data
ambiguity and workers’ reliability are unknown parameters at the beginning and need to
be updated based on the stream of collected raw labels in an online fashion. This further
suggests that the budget allocation policy should be dynamic and simultaneously conduct
parameter estimation and decision making.
To search for an optimal budget allocation policy, we model the data ambiguity and
workers’ reliability using two sets of random variables drawn from known prior distribu-
tions. Then, we formulate the problem into a ﬁnite-horizon Bayesian Markov Decision

2

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

Process (MDP) (Puterman, 2005), whose state variables are the posterior distributions of
these variables, which are updated by each new label. Here, the Bayesian setting is nec-
essary. We will show that an optimal policy only exists in the Bayesian setting. Using
the MDP formulation, the optimal budget allocation policy for any ﬁnite budget level can
be readily obtained via the dynamic programming (DP). However, DP is computationally
intractable for large-scale problems since the size of the state space grows exponentially in
budget level. The existing widely-used approximate policies, such as approximate Gittins
index rule (Gittins, 1989) or knowledge gradient (KG) (Gupta and Miescke, 1996; Frazier
et al., 2008), either has a high computational cost or poor performance in our problem. In
this paper, we propose a new policy, called optimistic know ledge gradient (Opt-KG). In par-
ticular, the Opt-KG policy dynamically chooses the next instance-worker pair based on the
optimistic outcome of the marginal improvement on the accuracy, which is a function of state
variables. We further propose a more general Opt-KG policy using the conditional value-
at-risk measure (Rockafellar and Uryasev, 2002). The Opt-KG is computationally eﬃcient,
achieves superior empirical performance and has some asymptotic theoretical guarantees.
To better present the main idea of our MDP formulation and the Opt-KG policy, we
start from the binary labeling task (i.e., providing the category, either positive or negative,
for each instance). We ﬁrst consider the pul l marketplace (e.g., Amazon Mechanical Turk
or Galaxy Zoo) , where the labeling requester can only post instances to the general worker
pool with either anonymous or transient workers, but cannot assign to an identiﬁed worker.
In a pull marketplace, workers are typically treated as homogeneous and one models the
entire worker pool instead of each individual worker. We further assume that workers are
fully reliable (or noiseless) such that the chance that they make an error only depend on
instances’ own ambiguity. At a ﬁrst glance, such an assumption may seem oversimpliﬁed.
In fact, it turns out that the budget-optimal crowd labeling under such an assumption has
been highly non-trivial. We formulate this problem into a Bayesian MDP and propose the
computational eﬃcient Opt-KG policy. We further prove that the Opt-KG policy in such a
setting is asymptotically consistent, that is, when the budget goes to inﬁnity, the accuracy
converges to 100% almost surely.
Then, we extend the MDP formulation to deal with push marketplaces with heteroge-
neous workers. In a push marketplace (e.g., data annotation team in Microsoft Bing group),
once an instance is allocated to an identiﬁed worker, the worker is required to ﬁnish the
instance in a short period of time. Based on the previous model for fully reliable workers,
we further introduce another set of parameters to characterize workers’ reliability. Then
our decision process simultaneously selects the next instance to label and the next worker
for labeling the instance according to the optimistic knowledge gradient policy.
In fact,
the proposed MDP framework is so ﬂexible that we can further extend it to incorporate
contextual information of instances whenever they are available (e.g., as in web search and
advertising applications discussed in Li et al., 2010) and to handle multi-class labeling.
In summary, the main contribution of the paper consists of the three folds: (1) we
formulate the budget allocation in crowd labeling into a MDP and characterize the optimal
policy using DP; (2) computationally, we propose an eﬃcient approximate policy, optimistic
knowledge gradient; (3) the proposed MDP framework can be used as a general framework
to address various budget allocation problems in crowdsourcing (e.g., rating and ranking
tasks).

3

Chen, Lin and Zhou

The rest of this paper is organized as follows. In Section 2, we ﬁrst present the modeling
of budget allocation process for binary labeling tasks with fully reliable workers and motivate
our Bayesian modeling. In Section 3, we present the Bayesian MDP and the optimal policy
via DP. In Section 4, we propose a computationally eﬃcient approximate policy, Opt-KG.
In Section 5, we extend our MDP to model heterogeneous workers with diﬀerent reliability.
In Section 6, we present other important extensions, including incorporating contextual
information and multi-class labeling. In Section 7, we discuss the related works. In Section
8, we present numerical results on both simulated and real data sets, followed by conclusions
in Section 9.

2. Binary Labeling with Homogeneous Noiseless Workers

We ﬁrst consider the budget allocation problem in a pull marketplace with homogeneous
noiseless workers for binary labeling tasks. We note that such a simpliﬁcation is important
for investigating this problem, since the incorporation of workers’ reliability and extensions
to multiple categories become rather straightforward once this problem is correctly modeled
(see Section 5 and 6).
Suppose that there are K instances and each one is associated with a latent true label
Zi ∈ {−1, 1} for 1 ≤ i ≤ K . Our goal is to infer the set of positive instances, denoted by
H ∗ = {i : Zi = 1}. Here, we assume that the homogeneous worker pool is ful ly reliable
or noiseless. We note that it does not mean that each worker knows the true label Zi .
Instead, it means that fully reliable workers will do their best to make judgments but their
labels may be still incorrect due to the instance’s ambiguity. Further, we model the labeling
diﬃculty/ambiguity of each instance by a latent soft-label θi , which can be interpreted as the
percentage of workers in the homogeneous noiseless crowd who will label the i-th instance
as positive. In other words, if we randomly choose a worker from a large crowd of fully
reliable workers, we will receive a positive label for the i-th instance with probability θi and
a negative label with probability 1 − θi . In general, we assume the crowd is large enough
so that the value of θi can be any value in [0, 1]. To see how θi characterizes the labeling
diﬃculty of the i-th instance, we consider a concrete example where a worker is asked to
label a person as adult (positive) or not (negative) based on the photo of that person. If
the person is more than 25 years old, most likely, the corresponding θi will be close to 1,
generating positive labels consistently. On the other hand, if the person is younger than
15, she may be labeled as negative by almost all the reliable workers since θi is close to 0.
In both of this cases, we regard the instance (person) easy to label since Zi can be inferred
with a high accuracy based on only a few raw labels. On the contrary, for a person is one
or two years below or above 18, the θi is near 0.5 and the numbers of positive and negative
labels become relatively comparable so that the corresponding labeling task is very diﬃcult.
Given the deﬁnition of soft labels, we further make the following assumption:

Assumption 1 We assume that the soft-label θi is consistent with the true label in the
sense that Zi = 1 if and only if θi ≥ 0.5, i.e., the majority of the crowd are correct, and
hence H ∗ = {i : θi ≥ 0.5}.

Given the total budget, denoted by T , we suppose that each label costs one unit of
budget. As discussed in the introduction, the crowd labeling has two phases. The ﬁrst

4

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

Instance
Instance 1 (θ1 )
Instance 2 (θ2 )
Instance 3 (θ3 )

1st round label

2nd round label

1

1

1

1
−1

Table 1: Toy example with 3 instances and 5 collected labels. Instance 1 has two positive
labels, instance 2 has one positive and one negative label, and instance 3 has only
one positive label. The question is that, given only one more labeling chance,
which instance should be chosen to label?

phase is the budget al location phase, which is a dynamic decision process with T stages.
In each stage 0 ≤ t ≤ T − 1, an instance it ∈ A = {1, . . . , K } is selected based on the
historical labeling results. Once it is selected, it will be labeled by a random worker from
the homogeneous noiseless worker pool. According to the deﬁnition of θit , the label received,
denoted by yit ∈ {−1, 1}, will follow the Bernoulli distribution with the parameter θit :
Pr (yit = −1) = 1 − θit .

Pr (yit = 1) = θit

and

(1)

We note that, at this moment, all workers are assumed to be homogeneous and noiseless
so that yit only depends on θit but not on which worker provides the label. Therefore, it is
suﬃce for the decision maker (e.g., requester or crowdsourcing service) to select the instance
in each stage instead of an instance-worker pair.
The second phase is the label aggregation phase. When the budget is exhausted, the
decision maker needs to infer true labels {Zi}n
i=1 by aggregating all the collected labels.
According to Assumption 1, it is equivalent to infer the set of positive instances whose
θi ≥ 0.5. Let HT be the estimated positive set. The ﬁnal overall accuracy is measured by
|HT ∩ H ∗ | + |(HT )c ∩ (H ∗ )c |, the size of the mutual overlap between H ∗ and HT .
Our goal is to determine the optimal al location policy, (i0 , . . . , iT −1 ), so that overall
accuracy is maximized. Here, a natural question to ask is whether the optimal allocation
policy exists and what assumptions do we need for the existence of the optimal policy.
To answer this question, we provide a concrete example, which motivates our Bayesian
modeling.

2.1 Why We Need a Bayesian Modeling

Let us check a toy example with 3 instances and 5 collected labels (see Table 1). We assume
that the workers are homogeneous noiseless and the label aggregation is performed by the
ma jority vote rule. Now if we only have the budget to get one more label, which instance
should be chosen to label? It is obvious that we should not put the remaining budget on
the ﬁrst instance since we are relatively more conﬁdent on what its true label should be.
Thus, the problem becomes how to choose between the second and third instances. In what
follows, we shall show that there is no optimal policy under the frequentist setting. To be
more explicit, the optimal policy leads to the expected accuracy which is at least as good
as that of all other policies for any values of {θi}n
i=1 .

5

Chen, Lin and Zhou

Current Accuracy

y = 1

θ1 > 0.5
θ1 < 0.5
θ2 > 0.5
θ2 < 0.5
θ3 > 0.5
θ3 < 0.5

1

0

0.5

0.5

1

0

1

0

1

0

1

0

y = −1 Expected Accuracy
1
1

0

0

1

0.5

0.5

0
θ2
1 − θ2
θ3 + 0.5(1 − θ3 )
0.5(1 − θ3 )

Improvement

0

0
θ2 − 0.5 > 0
0.5 − θ2 > 0
0.5(θ3 − 1) < 0
0.5(1 − θ3 ) > 0

Table 2: Expected improvements in accuracy for collecting an extra label, i.e., the expected
accuracy of obtaining one more label minus the current expected accuracy. The 3rd
and 4th columns contain the accuracies with the next label being 1 and −1. The
5th is the expected accuracy which is computed by taking θ times the 3rd column
plus (1 − θ) times the 4th. The last column contains the expected improvements
which is computed by taking the diﬀerence between the 5th and 2nd columns.

Figure 1: Decision Boundary.

Let us compute the expected improvement in accuracy in terms of the frequentist risk
in Table 2. We assume that θi (cid:54)= 0.5 and if the number of 1 and −1 labels are the same
for an instance, the accuracy is 0.5 based on a random guess. From Table 2, we should not
label the ﬁrst instance since the improvement is always 0. This coincides with our intuition.
When max(θ2 − 0.5, 0.5 − θ2 ) > 0.5(1 − θ3 ) or θ3 > 0.5, which corresponds to the blue region
in Figure 1, we should choose to label the second instance. Otherwise, we should ask the
label for the third one. Since the true value of θ2 and θ3 are unknown, a optimal policy
does not exist under the frequentist paradigm. Further, it will be diﬃcult to estimate θ2
and θ3 accurately when the budget is very limited.
In contrast, in a Bayesian setting with prior distribution on each θi , the optimal policy
is deﬁned as the policy which leads to the highest expected accuracy under the given prior

6

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

instead of for any possible values of {θi}n
i=1 . Therefore, we can optimally determine the next
instance to label by taking another expectation over the distribution of θi . In this paper, we
adopt the Bayesian modeling to formulate the budget allocation problem in crowd labeling.

3. Bayesian MDP and Optimal Policy

In this section, we ﬁrst introduce the Bayesian MDP for modeling the dynamic budget allo-
cation process and then provide the optimal allocation policy using dynamic programming.

3.1 Bayesian Modeling

We assume that each θi is drawn from a known Beta prior Beta(a0
i , b0
i ). Beta is a rich
family of distributions in the sense that it exhibits a fairly wide variety of shapes on the
domain of θi , i.e., the unit interval [0, 1]. For presentation simplicity, instead of considering
a full Bayesian model with hyper-priors on a0
i and b0
i , we ﬁx a0
i and b0
i at the beginning.
In practice, if the budget is suﬃcient, one can ﬁrst label each instance equally many times
to pre-estimate {a0
i }K
i , b0
i=1 before the dynamic labeling procedure is invoked. Otherwise,
when there is no prior knowledge, we can simply assume a0
i = b0
i = 1 so that the prior is
a uniform distribution. According to our simulated experimental results in Section 8.1.2,
uniform prior works reasonably well unless the data is highly skewed in terms of class
distribution. Other commonly used uninformative priors such as Jeﬀreys prior or reference
prior (Beta(1/2, 1/2)) or Haldane prior (Beta(0, 0)) can also be adopted (see Robert, 2007
for more on uninformative priors). Choices of prior distributions are discussed in more
details in Section 4.2.
i , bt
At each stage t with Beta(at
i ) as the current posterior distribution for θi , we make a de-
cision by choosing an instance it ∈ A = {1, . . . , K } and acquire its label yit ∼ Bernoulli(θit ).
Here A denotes the action set. By the fact that Beta is the conjugate prior of the Bernoulli,
(cid:40)
the posterior of θit in the stage t + 1 will be updated as:
+ 1, bt
Beta(at
yit = 1;
)
if
it
it
yit = −1.
, bt
Beta(at
+ 1)
if
it
it
i=1 into a K × 2 matrix S t , called a state matrix, and let S t
We put {at
i }K
i , bt
i = (at
i , bt
i ) be the
(cid:40)
i-th row of S t . The update of the state matrix can be written in a more compact form:
S t + (eit , 0)
if yit = 1;
if yit = −1,
S t + (0, eit )
where eit is a K × 1 vector with 1 at the it -th entry and 0 at all other entries. As we
can see, {S t} is a Markovian process because S t+1 is completely determined by the current
state S t , the action it and the obtained label yit . It is easy to calculate the state transition
probability Pr(yit |S t , it ), which is the posterior probability that we are in the next state
S t+1 if we choose it to be label in the current state S t :

Beta(at+1
it

S t+1 =

, bt+1
it

) =

(2)

Pr(yit = 1|S t , it ) = E(θit |S t ) =

at
it
+ bt
it

at
it

and Pr(yit = −1|S t , it ) =

bt
it
+ bt
it

.

at
it

(3)

7

Chen, Lin and Zhou

Given this labeling process, the budget allocation policy is deﬁned as a sequence of
decisions: π = (i0 , . . . , iT −1 ). Here, we require decisions depend only upon the previous
information. To make this more formal, we deﬁne a ﬁltration {Ft}T
t=0 , where Ft is the
information collected until the stage t − 1. More precisely, Ft is the the σ -algebra generated
by the sample path (i0 , yi0 , . . . , it−1 , yit−1 ). We require the action it is determined based on
the historical labeling results up to the stage t − 1, i.e., it is Ft -measurable.

3.2 Inference About True Labels

As described in Section 2, the budget allocation process has two phases: the dynamic budget
allocation phase and the label aggregation phase. Since the goal of the dynamic budget
allocation in the ﬁrst phase is to maximize the accuracy of aggregated labels in the second
phase, we ﬁrst present how to infer the true label via label aggregation in the second phase.
When the decision process terminates at the stage T , we need to determine a positive set
HT to maximize the conditional expected accuracy conditioning on FT , which corresponds
to minimizing the posterior risk:
(cid:0)1(i ∈ H ) · 1(i ∈ H ∗ ) + 1(i (cid:54)∈ H ) · 1(i (cid:54)∈ H ∗ )(cid:1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)FT
(cid:32) K(cid:88)
(cid:33)
i=1
where 1(A) is the indicator function, which takes the value 1 if the event A is true and 0
otherwise. The term inside expectation in (4) is the binary labeling accuracy which can
also be written as |H ∩ H ∗ | + |H c ∩ (H ∗ )c |.
We ﬁrst observe that, for 0 ≤ t ≤ T , the conditional distribution θi |Ft is exactly the
posterior distribution Beta(at
i , bt
i ), which depends on the historical sampling results only
i , bt
i = (at
through S t
i ). Hence, we deﬁne
= Pr(θ ≥ 0.5|θ ∼ Beta(a, b)),
.
I (a, b)
= Pr(i ∈ H ∗ |Ft ) = Pr(θi ≥ 0.5|Ft ) = Pr(θi ≥ 0.5|S t
.
P t
i , bt
i ) = I (at
i ).
i

HT = arg max
H⊂{1,...,K }

(4)

(5)

(6)

E

,

As shown in Xie and Frazier (2013), the optimal positive set HT can be determined by the
Bayes decision rule as follows.
Proposition 2 HT = {i : Pr(i ∈ H ∗ |FT ) ≥ 0.5} = {i : P T
i ≥ 0.5} solves (4).
The proof of Proposition 2 is given in the appendix for completeness.
With Proposition 2 in place, we plug the optimal positive set HT into the right hand
(cid:0)1(i ∈ HT ) · 1(i ∈ H ∗ ) + 1(i (cid:54)∈ HT ) · 1(i (cid:54)∈ H ∗ )(cid:1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)FT
(cid:33)
(cid:32) K(cid:88)
side of (4) and the conditional expected accuracy given FT can be simpliﬁed as:
K(cid:88)
i=1
i=1
= max(x, 1 − x). We also note that P T
.
where h(x)
i provides not only the estimated label
for the i-th instance but also how conﬁdent the estimated label is correct. According to the
next corollary with the proof in the appendix, we show that the optimal HT is constructed
based on a reﬁned majority vote rule which incorporates the prior information.

h(P T
i ),

(7)

E

=

8

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

Corollary 3 I (a, b) > 0.5 if and only if a > b and I (a, b) = 0.5 if and only if a = b.
Therefore, HT = {i : aT
i ≥ bT
i } solves (4).
i as pseudo-counts of 1s and −1s at the initial stage, the parameters
By viewing a0
i and b0
i ≥ bT
i are the total counts of 1s and −1s. The estimated positive set HT = {i : aT
i }
aT
i and bT
consists of instances with more (or equal) counts of 1s than that of −1s. When a0
i = b0
i ,
HT is constructed exactly according to the vanilla majority vote rule.
To ﬁnd the optimal allocation policy which maximizes the expected accuracy, we need
(cid:0)1(i ∈ HT ) · 1(i ∈ H ∗ ) + 1(i (cid:54)∈ HT ) · 1(i (cid:54)∈ H ∗ )(cid:1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)FT
(cid:34)
(cid:32) K(cid:88)
(cid:33)(cid:35)
to solve the following optimization problem:
E
(cid:32) K(cid:88)
(cid:33)
i=1
h(P T
,
i )
i=1
where Eπ represents the expectation taken over the sample paths (i0 , yi0 , . . . , iT −1 , yiT −1 )
generated by a policy π . The second equality is due to Proposition 2 and V (S 0 ) is called
value function at the initial state S 0 . The optimal policy π∗ is any policy π that attains
the supremum in (8).

.
= sup
π

= sup
π

V (S 0 )

Eπ

Eπ

(8)

3.3 Markov Decision Process

The optimization problem in (8) is essentially a Bayesian multi-armed bandit (MAB) prob-
lem, where each instance corresponds to an arm and the decision is which instance/arm
to be sampled next. However, it is diﬀerent from the classical MAB problem (Auer et al.,
2002; Bubeck and Cesa-Bianchi, 2012), which assumes that each sample of an arm yields
independent and identically distributed (i.i.d.) reward according to some unknown distri-
bution associated with that arm. Given the total budget T , the goal is to determine a
sequential allocation policy so that the collected rewards can be maximized. We contrast
this problem with our problem: instead of collecting intermediate independent rewards on
the ﬂy, our ob jective in (8) merely involves the ﬁnal “reward”, i.e., overall labeling accuracy,
which is only available at the ﬁnal stage when the budget runs out. Although there is no
intermediate reward in our problem, we can still decompose the ﬁnal expected accuracy into
sum of stage-wise rewards using the technique from Xie and Frazier (2013), which further
leads to our MDP formulation. Since these stage-wise rewards are artiﬁcially created, they
are no longer i.i.d.
for each instance. We also note that the problem in Xie and Frazier
(2013) is an inﬁnite-horizon one which optimizes the stopping time while our problem is
ﬁnite-horizon since the decision process must be stopped at the stage T .
(cid:33)
(cid:32) K(cid:88)
) − K(cid:88)
i )(cid:12)(cid:12)S t , it
Proposition 4 Deﬁne the stage-wise expected reward as:
= E (cid:0)h(P t+1
h(P t
it
i=1
i=1

) − h(P t
it )|S t , it

R(S t , it ) = E

h(P t+1
i

(cid:1) ,

(9)

then the value function (8) becomes:

9

Eπ

,

(10)

(cid:33)

R(S t , it )

Chen, Lin and Zhou
(cid:32)T −1(cid:88)
V (S 0 ) = G0 (S 0 ) + sup
where G0 (S 0 ) = (cid:80)K
π
t=0
i ) and the optimal policy π∗ is any policy π that attains the
i=1 h(P 0
supremum.
in (9) has a straightforward interpretation. According to (8), the term (cid:80)K
The proof of Proposition 4 is presented in the appendix. In fact, the stage-wise reward
i=1 h(P t
i ) is the
expected accuracy at the t-th stage. The stage-wise reward R(S t , it ) takes the form of the
diﬀerence between the expected accuracy at the (t + 1)-stage and the t-th stage, i.e., the
expected gain in accuracy for collecting another label for the it -th instance. The second
equality in (9) holds simply because: only the it -th instance receives the new label and the
changes while all other P t
corresponding P t
i remain the same. Since the expected reward
(cid:1) ,
(cid:1) = R (cid:0)at
R(S t , it ) = R (cid:0)S t
it
, bt
= (at
(9) only depends on S t
), we write
it
it
it
it , bt
it
it
and use them interchangeably. The function R(a, b) with two parameters a and b has an
analytical representation as follows. For any state (a, b) of a single instance, the reward of
getting a label 1 and a label −1 are:
R1 (a, b) = h(I (a + 1, b)) − h(I (a, b)),
R2 (a, b) = h(I (a, b + 1)) − h(I (a, b)).

(13)

(12)

(11)

The expected reward takes the following form:

R(a, b) = p1R1 + p2R2 ,

(14)

where p1 = a
a+b and p2 = b
a+b are the transition probabilities in (3).
With Proposition 4, the maximization problem (8) is formulated as a T -stage Markov
Decision Process (MDP) as in (10), which is associated with a tuple:
{T , {S t}, A, Pr(yit |S t , it ), R(S t , it )}.
Here, the state space at the stage t, S t , is all possible states that can be reached at t. Once
(cid:41)
(cid:40)
we collect a label yit , one element in S t (either at
or bt
) will add one. Therefore, we have
K(cid:88)
it
it
i − b0
i − a0
i ≥ b0
i ≥ a0
i }K
{at
i ) + (bt
(at
i , bt
i=1 : at
i , bt
i ) = t
i ,
i=1
The action space is the set of instances that could be labeled next: A = {1, . . . , K }. The
transition probability Pr(yit |S t , it ) is deﬁned in (3) and the expected reward at each stage
R(S t , it ) is deﬁned in (9).

S t =

(15)

.

Remark 5 We can also view Proposition 4 as a consequence of applying the reward shap-
ing technique (Ng et al., 1999) to the original problem (8). In fact, we can add an artiﬁcial

10

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

Instance i S T −1
i
1
(3,1)

2

3

(2,2)

(2,1)

p1
3
4
1
2
2
3

p2 R1 (S T −1
i
1
0.0625
4
1
2
1
3

0.1875

0.1250

) R2 (S T −1
)
i
−0.1875
0.1875
−0.2500

R(S T −1 , i) = R(S T −1
)
i
4 · (0.0625) + 1
4 · (−0.1875) = 0
3
2 · (0.1875) + 1
2 · (0.1875) = 0.1875
1
3 · (0.1250) + 1
3 · (−0.2500) = 0
2

Table 3: Calculation of the expected reward for the toy example in Table 1 according to
(12), (13) and (14).

absorbing state, named Sobs , to the original state space (15) and assume that, when the bud-
zero transition reward until the state enters Sobs where the transition reward is (cid:80)K
get al location process ﬁnishes, the state must transit one more time to reach Sobs regard less
of which action is taken. Hence, the original problem (8) becomes a MDP that generates a
space as Φ(S t ) = (cid:80)K
i=1 h(P T
i ).
Then, we deﬁne a potential-based shaping function (Ng et al., 1999) over this extended state
i ) for S t ∈ S t and Φ(Sobs ) = 0. After this, (4) can be viewed
i=1 h(P t
as a new MDP whose transition reward equals that of (8) plus the shaping-reward function
Φ(S (cid:48) ) − Φ(S ) when the state transits from S to S (cid:48) . According to Theorem 1 in Ng et al.
(1999), (4) and (8) have the same optimal policy. This provides an alternative justiﬁcation
for Proposition 4.

3.4 Optimal Policy via DP

With the MDP in place, we can apply the dynamic programming (DP) algorithm (a.k.a.
backward induction) (Puterman, 2005) to compute the optimal policy:
1. Set VT −1 (S T −1 ) = maxi∈{1,...,K } R(S T −1 , i) for al l possible states S T −1 ∈ S T −1 . The
optimal decision i∗
T −1 (S T −1 ) is the decision i that achieves the maximum when the
state is S T −1 .
2. Iterate for t = T − 2, . . . , 0, compute the Vt (S t ) for all possible S t ∈ S t using the
Bellman equation:
(cid:0)S t + (0, ei )(cid:1)(cid:17)
(cid:16)
(cid:0)S t + (ei , 0)(cid:1) + Pr(yi = −1|S t , i)Vt+1
Vt (S t )
R(S t , i) + Pr(yi = 1|S t , i)Vt+1
= max
i
and i∗
t (S t ) is the i that achieves the maximum.
0 , . . . , i∗
The optimal policy π∗ = (i∗
T ). For an illustration purpose, we use DP to calculate
the optimal instance to be labeled next in the toy example in Section 2.1 under the uniform
prior B (1, 1) for all θi . Since we assume that there is only one labeling chance remaining,
which corresponds to the last stage of DP, we should choose the instance i∗
T −1 (S T −1 ) =
arg maxi∈{1,...,K } R(S T −1 , i). According to the calculation in Table 3, there is a unique
optimal instance for labeling, which is the second instance.
Although DP ﬁnds the optimal policy, its computation is intractable since the size of the
state space |S t | grows exponentially in t according to (15). Therefore, we need to develop
a computationally eﬃcient approximate policy, which is the goal of the next section.

,

11

Chen, Lin and Zhou

4. Approximate Policies

Since DP is computationally intractable, approximate policies are needed for large-scale
applications. The simplest policy is the uniform sampling (a.k.a, pure exploration), i.e., we
choose the next instance uniformly and independently at random: it ∼ Uniform(1, . . . , K ).
However, this policy does not explore any structure of the problem.
With the decomposed reward function, our problem is essentially a ﬁnite-horizon Bayesian
MAB problem. Gittins (1989) showed that Gittins index policy is optimal for inﬁnite-
horizon MAB with the discounted reward. It has been applied to the inﬁnite-horizon ver-
sion of problem (10) in Xie and Frazier (2013). Since our problem is ﬁnite-horizon, Gittins
index is no longer optimal while it can still provide us a good heuristic index rule. However,
the computational cost of Gittins index is very high: the state-of-art-method proposed by
Nino-Mora (2011) requires O(T 6 ) time and space complexity.
A computationally more attractive policy is the knowledge gradient (KG) (Gupta and
Miescke, 1996; Frazier et al., 2008). It is essentially a single-step look-ahead policy, which
(cid:18)
(cid:19)
greedily selects the next instance with the largest expected reward:
i , bt
R(at
i )

i , bt
R1 (at
i ) +

(16)

i , bt
R2 (at
i )

.

it = arg max
i∈{1,...,K }

.
=

at
i
i + bt
at
i

bt
i
i + bt
at
i

As we can see, this policy corresponds to the last stage in DP and hence KG policy is
optimal if only one labeling chance is remaining.
When there is a tie, if we select the smallest index i, the policy is referred to deterministic
KG while if we randomly break the tie, the policy is referred to randomized KG. Although
KG has been successfully applied to many MDP problems (Powell, 2007), it will fail in our
problem as shown in the next proposition with the proof in the appendix.
i are positive integers and letting E = {i : a0
i },
Proposition 6 Assuming that a0
i = b0
i and b0
then the deterministic KG policy wil l acquire one label for each instance in E and then
consistently obtain the label for the ﬁrst instance even if the budget T goes to inﬁnity.

According to Proposition 6, the deterministic KG is not a consistent policy, where the
consistent policy refers to the policy that will provide correct labels for all instances (i.e.,
HT = H ∗ ) almost surely when T goes to inﬁnity. We note that randomized KG policy can
address this problem. However, from the proof of Proposition 6, randomized KG behaves
similarly to the uniform sampling policy in many cases and its empirical performance is
undesirable according to Section 8. In the next subsection, we will propose a new approx-
imate allocation policy based on KG which is a consistent policy with superior empirical
performance.

4.1 Optimistic Knowledge Gradient

The stage-wise reward can be viewed as a random variable with a two point distribution,
i.e., with the probability p1 = a
a+b of being R1 (a, b) and the probability p2 = b
a+b of being
R2 (a, b). The KG policy selects the instance with the largest expected reward. However, it
is not consistent.

12

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

Algorithm 1 Optimistic Knowledge Gradient
Input: Parameters of prior distributions for instances {a0
i }K
i , b0
i=1 and the budget T .
for t = 0, . . . , T − 1 do
(cid:0)R+ (at
i ))(cid:1) .
Select the next instance it to label according to:
.
i , bt
i ), R2 (at
i , bt
= max(R1 (at
i , bt
i )

(17)

it = arg max
i∈{1,...,K }
Acquire the label yit ∈ {−1, 1}.
if yit = 1 then
at+1
+ 1, bt+1
= at
it
it
it
else
at+1
it
end if
end for
i ≥ bT
Output: The positive set HT = {i : aT
i }.

, bt+1
it

= at
it

= bt
it

i for all i (cid:54)= it .
; at+1
i , bt+1
i = at
i = bt
i for all i (cid:54)= it .
+ 1; at+1
i , bt+1
i = at
i = bt

= bt
it

Figure 2: Illustration of R+ (a, b).

In this section, we introduce a new index policy called “optimistic knowledge gradient”
(Opt-KG) policy. The Opt-KG policy assumes that decision makers are optimistic in the
sense that they select the next instance based on the optimistic outcome of the reward. As
i , bt
a simplest version of the Opt-KG policy, for any state (at
i ), the optimistic outcome of the
i ) is deﬁned as maximum over the reward of obtaining the label 1, R1 (at
reward R+ (at
i , bt
i , bt
i ),
and the reward of obtaining the label −1, R2 (at
i , bt
i ). Then the optimistic decision maker
selects the next instance i with the largest R+ (at
i , bt
i ) as in (17) in Algorithm 1. The overall
decision process using the Opt-KG policy is highlighted in Algorithm 1.
In the next theorem, we prove that Opt-KG policy is consistent.

i and b0
Theorem 7 Assuming that a0
i are positive integers, the Opt-KG is a consistent
policy, i.e, as T goes to inﬁnity, the accuracy wil l be 100% (i.e., HT = H ∗ ) almost surely.

13

ab  12345678910109876543210.05 0.10.15 0.20.25Chen, Lin and Zhou

Figure 3: Illustration of Conditional Value-at-Risk.

The key of proving the consistency is to show that when T goes to inﬁnity, each instance
will be labeled inﬁnitely many times. We prove this by showing that for any pair of positive
integers (a, b), R+ (a, b) = max(R1 (a, b), R2 (a, b)) > 0 and R+ (a, b) → 0 when a + b → ∞.
As an illustration, the values of R+ (a, b) are plotted in Figure 2. Then, by strong law
of large number, we obtain the consistency of the Opt-KG as stated in Theorem 7. The
details are presented in the appendix. We have to note that asymptotic consistency is the
minimum guarantee for a good policy. However, it does not necessarily guarantee the good
empirical performance for the ﬁnite budget level. We will use experimental results to show
the superior performance of the proposed policy.
The proposed Opt-KG policy is a general framework for budget allocation in crowd
labeling. We can extend the allocation policy based on the maximum over the two possible
rewards (Algorithm 1) to a more general policy using the conditional value-at-risk (CVaR)
(Rockafellar and Uryasev, 2002). We note that here, instead of adopting the CVaR as a
risk measure, we apply it to the reward distribution. In particular, for a random variable
X with the support X (e.g., the random reward with the two point distribution), let α-
quantile function be denoted as Qα (X ) = inf {x ∈ X : α ≤ FX (x)}, where FX (·) is the CDF
of X . The value-at-risk VaRα (X ) is the smallest value such that the probability that X is
less than (or equal to) it is greater than (or equal to) 1 − α: VaRα (X ) = Q1−α (X ). The
conditional value-at-risk (CVaRα (X )) is deﬁned as the expected reward exceeding (or equal
to) VaRα (X ). An illustration of CVaR is shown in Figure 3.
For our problem, according to Rockafellar and Uryasev (2002), CVaRα (X ) can be ex-
pressed as a simple linear program:

{q1≥0,q2≥0} q1R1 + q2R2 ,
CVaRα (X ) = max
q1 ≤ 1
p1 , q2 ≤ 1
α
α

s.t.

p2 , q1 + q2 = 1.

As we can see, when α = 1, CVaRα (X ) = p1R1 + p2R2 , which is the expected reward;
when α → 0, CVaRα (X ) = max(R1 , R2 ), which is used as the selection criterion in (17) in
Algorithm 1. In fact, a more general Opt-KG policy could be selecting the next instance
with the largest CVaRα (X ) with a tuning parameter α ∈ [0, 1]. We can extend Theorem 7
to prove that the policy based on CVaRα (X ) is consistent for any α < 1. According to our

14

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

own experience, α → 0 usually has a better performance in our problem especially when
the budget is very limited. Therefore, for the sake of presentation simplicity, we introduce
the Opt-KG using max(R1 , R2 ) (i.e., α → 0 in CVaRα (X )) as the selection criterion.
Finally, we highlight that the Opt-KG policy is computationally very eﬃcient. For K
instances with T units of the budget, the overall time and space complexity are O(K T ) and
O(K ) respectively. It is much more eﬃcient that the Gittins index policy which requires
O(T 6 ) time and space complexity.

4.2 Discussions

It is interesting to see the connection between the idea of making the decision based on
the optimistic outcome of the reward and the UCB (upper conﬁdence bounds) policy (Auer
et al., 2002) for the classical multi-armed bandit problem as described in Section 3.3. In
particular, the UCB policy selects the next arm with the maximum upper conﬁdence index,
which is deﬁned as the current average reward plus the one-sided conﬁdence interval. As
we can see, the upper conﬁdence index can be viewed as an “optimistic” estimate of the
reward. However, we note that since we are in a Bayesian setting and our stage-wise rewards
are artiﬁcially created and thus not i.i.d. for each arm, the UCB policy (Auer et al., 2002)
cannot be directly applied to our problem.
In fact, our Opt-KG follows a more general principle of “optimism in the face uncer-
tainty” (Szita and L˝orincz, 2008). Essentially, the non-consistency of KG is due to its nature
of pure exploitation while a consistent policy should typically utilizes exploration. One of
the common techniques to handle the exploration-exploitation dilemma is to take an action
based on an optimistic estimation of the rewards (see Szita and L˝orincz, 2008; Even-Dar
and Mansour, 2001), which is the role R+ (a, b) plays in Opt-KG.
(cid:0)R− (at
i ))(cid:1) . However, as shown in the next
For our problem, it is also straightforward to design the “pessimistic knowledge gradient”
policy which selects the next instance it based on the pessimistic outcome of the reward,
.
i , bt
= min(R1 (at
i , bt
i ), R2 (at
i , bt
i.e., it = arg maxi
i )
proposition with the proof in the appendix, the pessimistic KG policy is inconsistent under
the uniform prior.

Proposition 8 When starting from the uniform prior (i.e., a0
i = b0
i = 1) for al l θi , the
pessimistic KG policy wil l acquire one label for each instance and then consistently acquire
the label for the ﬁrst instance even if the budget T goes to inﬁnity.

Finally, we discuss some other possible choices of prior distributions. For presentation
simplicity, we only consider the Beta prior for each θi with the ﬁxed parameters a0
i and b0
i .
In practice, more complicated priors can be easily incorporated into our framework. For
example, instead of using only one Beta prior, one can adopt a mixture of Beta distributions
as the prior and the posterior will also follow a mixture of Beta distributions, which allows
an easy inference about the posterior. As we show in the experiments (see Section 8.1.2),
the uniform prior does not work well when the data is highly skewed in terms of class
distribution. To address this problem, one possible choice is to adopt the prior p(θ) =
w1Beta(c, 1) + w2Beta(1, 1) + w3Beta(1, c) where w1 , w2 and w3 are the weights and c is
a constant larger than 1 (e.g., c = 5).
In such a prior, B (c, 1) corresponds to the data
with more positive labels while B (1, c) to the data with more negative labels. In addition

15

Chen, Lin and Zhou

to the mixture Beta prior, one can adopt the hierarchical Bayesian approach which puts
hyper-priors on the parameters in the Beta priors. The inference can be performed using
empirical Bayes approach (Gelman et al., 2013; Robert, 2007).
In particular, one can
periodically re-calculate the MAP estimate of the hyper-parameters based on the available
data and update the model, but otherwise proceed with the given hyper-parameters. For
common choices of hyper-priors of Beta, please refer to Section 5.3 in Gelman et al. (2013).
These approaches can also be applied to model the workers’ reliability as we introduced in
the next Section. For example, one can use a mixture of Beta distributions as the prior
for the workers’ reliability, where Beta(c, 1) corresponds to reliable workers, Beta(1, 1) to
random workers and Beta(1, c) to malicious or poorly informed workers.

5. Incorporate Reliability of Heterogeneous Workers

In push crowdsourcing marketplaces, it is important to model workers’ reliability so that
the decision maker could assign more instances to reliable workers. Assuming that there
are M workers in a push marketplace, we can capture the reliability of the j -th worker by
introducing an extra parameter ρj ∈ [0, 1] as in (Dawid and Skene, 1979; Raykar et al.,
2010; Karger et al., 2013b), which is deﬁned as the probability of getting the same label
as the one from a random fully reliable worker. Recall that the soft-label θi is the i-th
instance’s probability of being labeled as positive by a fully reliable worker and let zij be
the label provided by the j -th worker for the i-th instance. We model the distribution of
zij for given θi and ρj using the one-coin model (Dawid and Skene, 1979; Karger et al.,
2013b):
Pr(zij = 1|θi , ρj ) = Pr(zij = 1|yi = 1, ρj ) Pr(yi = 1|θi ) + Pr(zij = 1|yi = −1, ρj ) Pr(yi = −1|θi )
= ρj θi + (1 − ρj )(1 − θi );
(18)
Pr(zij = −1|θi , ρj ) = Pr(zij = −1|yi = −1, ρj ) Pr(yi = −1|θi ) + Pr(zij = −1|yi = 1, ρj ) Pr(yi = 1|θi )
= ρj (1 − θi ) + (1 − ρj )θi ,
(19)

where yi denotes the label provided a random fully reliable worker for the i-th instance.
We also note that it is straightforward to extend the current one-coin model to a more
complex two-coin model (Dawid and Skene, 1979; Raykar et al., 2010) by introducing a
pair of parameters (ρj 1 , ρj 2 ) to model the j -th worker’s reliability. In particular, ρj 1 and ρj 2
are the probabilities of getting the positive and negative labels when a fully reliable worker
provides the same label.
Here we make the following implicit assumption:

Assumption 9 We assume that diﬀerent workers make independent judgments and, for
each single worker, the labels provided by him/her to diﬀerent instances are also independent.

As the parameter ρj increases from 0 to 1, the j -th worker’s reliability also increases in
the sense that Pr(zij = 1|θi , ρj ) gets more and more close to θi , which is the probability
of getting a positive label from a random fully reliable worker. Diﬀerent types of workers
can be easily characterized by ρj . When all ρj = 1, it recovers the previous model with
fully reliable workers since Pr(zij = 1|θi , ρj ) = θi , i.e, each worker provides the label only
according to the underlying soft-label of the instance. When ρj = 0.5, we have Pr(zij =
1|θi , ρj ) = Pr(zij = −1|θi , ρj ) = 0.5, which indicates that the j -th worker is a spammer,
who randomly submits positive or negative labels. When ρj = 0, it indicates that the

16

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

p(θi , ρj |zij = z ) =

j -th worker is poorly informed or misunderstands the instruction such that he/she always
assigns wrong labels.
i=1 and workers’ reliability {ρj }M
We assume that instances’ soft-label {θi}K
j=1 are drawn
from known Beta prior distributions: θi ∼ Beta(a0
i ) and ρj ∼ Beta(c0
j , d0
i , b0
j ). At each
stage, we need to make the decision on both the next instance i to be labeled and the
next worker j to label the instance i (we omit t in i, j here for notational simplicity). In
other words, the action space A = {(i, j ) : (i, j ) ∈ {1, . . . , K } × {1, . . . , M }}. Once the
decision is made, the distribution of the outcome zij is given by (18) and (19). Given the
prior distributions and likelihood functions in (18) and (19), the Bayesian Markov Decision
process can be formally deﬁned as in Section 3. Similar to the homogeneous worker setting,
the optimal inferred positive set HT takes the form of HT = {i : P T
i ≥ 0.5} as in Proposition
i = Pr(i ∈ H ∗ |Ft ) = Pr (θi ≥ 0.5|Ft ). The value function V (S 0 ) still takes the
2 with P t
form of (8), which can be further decomposed into the sum of stage-wise rewards in (9)
using Proposition 4. Unfortunately, in the heterogeneous worker setting, the posterior
distributions of θi and ρj are highly correlated with a sophisticated joint distribution, which
makes the computation of stage-wise rewards in (9) much more challenging. In particular,
given the prior θi ∼ Beta(a0
i ) and ρj ∼ Beta(c0
i , b0
j , d0
j ), the posterior distribution of θi and
ρj given the label zij = z ∈ {−1, 1} takes the following form:
Pr(zij = z |θi , ρj )Beta(a0
j , d0
i )Beta(c0
i , b0
j )
Pr(zij = z )
where Pr(zij = z |θi , ρj ) is the likelihood function deﬁned in (18) and (19) and
Pr(zij = 1) = E(Pr(zij = 1|θi , ρj )) = E(θi )E(ρj ) + (1 − E(θi ))(1 − E(ρj ))
d0
c0
a0
b0
j
j
i
i
j + d0
c0
i + b0
a0
j + d0
c0
i + b0
a0
i
j
i
j
As we can see, the posterior distribution p(θi , ρj |zij = z ) no longer takes the form of the
product of the distributions of θi and ρj and the marginal posterior of θi is no longer a
Beta distribution. As a result, P t
i does not have a simple representation as in (5), which
makes the computation of the reward function much more diﬃcult as the number of stages
increases. Therefore, to apply our Opt-KG policy to large-scale applications, we need to
use some approximate posterior inference techniques.
When applying Opt-KG, we need to perform 2 ·K ·M ·T inferences of the posterior distri-
bution in total. Each approximate inference should be computed very eﬃciently, hopefully
in a closed-form. For large-scale problems, most traditional approximate inference tech-
niques such as Markov Chain Monte Carlo (MCMC) or variational Bayesian methods (e.g.,
Beal, 2003; Paisley et al., 2012) may lead to higher computational cost since each inference
is an iterative procedure. To address the computational challenge, we apply the variational
approximation with the moment matching technique so that each inference of the approxi-
mate posterior can be computed in a closed-form. In fact, any highly eﬃcient approximate
inference can be utilized to compute the reward function. Since the main focus of the paper
is on the MDP model and Opt-KG policy, we omit the discussion for other possible approx-
imate inference techniques. In particular, we ﬁrst adopt the variational approximation by

=

,

(20)

+

.

17

Chen, Lin and Zhou

Algorithm 2 Optimistic Knowledge Gradient for Heterogeneous Workers
Input: Parameters of prior distributions for instances {a0
i }K
i , b0
i=1 and for workers
{c0
j }M
j , d0
j=1 . The total budget T .
for t = 0, . . . , T − 1 do
(cid:0)R+ (at
j ))(cid:1) .
1. Select the next instance it to label and the next worker jt to label it according to:
.
j , dt
i , ct
i , bt
j ), R2 (at
j , dt
i , ct
i , bt
= max(R1 (at
j , dt
i , ct
i , bt
j )
arg max
(i,j )∈{1,...,K }×{1,...,M }
(21)
2. Acquire the label zit jt ∈ {−1, 1} of the i-th instance from the j -th worker.
3. Update the posterior by setting:

(it , jt ) =

= ˜bt
ct+1
bt+1
at+1
= ˜ct
= ˜at
jt (zit jt )
it (zit jt )
it (zit jt )
jt
it
it
and all parameters for i (cid:54)= it and j (cid:54)= jt remain the same.
end for
i ≥ bT
Output: The positive set HT = {i : aT
i }.

dt+1
jt

= ˜dt
jt (zit jt ),

assuming the conditional independence of θi and ρj :
p(θi , ρj |zij = z ) ≈ p(θi |zij = z )p(ρj |zij = z ).
We further approximate p(θi |zij = z ) and p(ρj |zij = z ) by two Beta distributions:
p(θi |zij = z ) ≈ Beta(˜ai (z ), ˜bi (z )),
p(ρj |zij = z ) ≈ Beta(˜cj (z ), ˜dj (z )),
where the parameters ˜ai (z ), ˜bi (z ), ˜cj (z ), ˜dj (z ) are computed using moment matching with
the analytical form presented in the appendix. After this approximation, the new posterior
distributions of θi and ρj still have the same structure as their prior distribution, i.e., the
product of two Beta distributions, which allows a repeatable use of this approximation every
time when a new label is collected. Moreover, due to the Beta distribution approximation
of p(θi |zij = z ), the reward function takes a similar form as in the previous setting.
In
particular, assuming at a certain stage, θi has the posterior distribution Beta(ai , bi ) and
ρj has the posterior distribution Beta(cj , dj ). The reward of getting positive and negative
labels for the i-th instance from the j -th worker are presented in (22) and (23):
R1 (ai , bi , cj , dj ) = h(I (˜ai (z = 1), ˜bi (z = 1))) − h(I (ai , bi )),
R2 (ai , bi , cj , dj ) = h(I (˜ai (z = −1), ˜bi (z = −1))) − h(I (ai , bi )).
With the reward in place, we present Opt-KG for budget allocation in the heterogeneous
worker setting in Algorithm 2. We also note that due to the variational approximation of
the posterior, establishing the consistency results of Opt-KG becomes very challenging in
the heterogeneous worker setting.

(22)

(23)

6. Extensions

Our MDP formulation is a general framework to address many complex settings of dy-
namic budget allocation problems in crowd labeling. In this section, we brieﬂy discuss two

18

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

important extensions, where for both extensions, Opt-KG can be directly applied as an
approximate policy. We note that for the sake of presentation simplicity, we only present
these extensions in the noiseless homogeneous worker setting. Further extensions to the
heterogeneous setting are rather straightforward using the technique from Section 5.

6.1 Utilizing Contextual Information

When the contextual information is available for instances, we could easily extend our model
to incorporate such an important information. In particular, let the contextual information
for the i-th instance be represented by a p-dimensional feature vector xi ∈ Rp . We could
utilize the feature information by assuming a logistic model for θi :
exp{(cid:104)w, xi (cid:105)}
1 + exp{(cid:104)w, xi (cid:105)} ,

.
=

θi

where w is assumed to be drawn from a Gaussian prior N (µ0 , Σ0 ). At the t-th stage with
the current state (µt , Σt ), the decision maker determines the instance it and acquire its label
yit ∈ {−1, 1}. Then we update the posterior µt+1 and Σt+1 using the Laplace method as in
Bayesian logistic regression (Bishop, 2007). Variational methods can be applied to further
accelerate the posterior update (Jaakkola and Jordan, 2000). The details are provided in
the appendix.

6.2 Multi-Class Categorization

Our MDP formulation can also be extended to deal with multi-class categorization problems,
where each instance is a multiple choice question with several possible options (i.e., classes).
More formally, in a multi-class setting with C diﬀerent classes, we assume that the i-th
(cid:80)C
instance is associated with a probability vector θ i = (θi1 , . . . θiC ), where θic is the probability
that the i-th instance will be labeled as the class c by a random fully reliable worker and
i=1 θic = 1. We assume that θ i has a Dirichlet prior θ i ∼ Dir(α0
i ) and the initial state
S 0 is a K × C matrix with α0
which follows the categorical distribution: p(yit ) = (cid:81)C
i as its i-th row. At each stage t with the current state
S t , we determine the next instance it to be labeled and collect its label yit ∈ {1, . . . , C },
I (yit =c)
c=1 θ
. Since the Dirichlet is
it c
the conjugate prior of the categorical distribution, the next state induced by the posterior
i for all i (cid:54)= it . Here δ c is a row vector with
distribution is: S t+1
and S t+1
i = S t
= S t
+ δyit
it
it
one at the c-th entry and zeros at all other entries. The transition probability is:
it c(cid:80)C
αt
c=1 αt
it c
(cid:54)= c}. By a
c = {i : θic ≥ θic(cid:48) , ∀c(cid:48)
We denote the true set of instances in class c by H ∗
similar argument as in Proposition 2, at the ﬁnal stage T , the estimated set of instances
belonging to class c is
c = {i : P T
ic ≥ P T
ic(cid:48) , ∀c(cid:48) (cid:54)= c},
H T
c |Ft ) = Pr(θic ≥ θic(cid:48) ,
ic = Pr(i ∈ H ∗
∀ c(cid:48)
(cid:54)= c|S t ). We note that if the i-th
where P t
instance belongs to more than one H T
c , we only assign it to the one with the smallest

Pr(yit = c|S t , it ) = E(θit c |S t ) =

.

19

Chen, Lin and Zhou

index c so that {H T
c=1 forms a partition of {1, . . . , K }. Let Pt
c }C
i = (P t
i1 , . . . , P t
iC ) and
R(S t , it ) = E (cid:0)h(Pt+1
(cid:1) .
i ) = max1≤c≤C P t
h(Pt
ic . The expected reward takes the form of:
it )|S t , it
) − h(Pt
it
With the reward function in place, we can formulate the problem into a MDP and use
DP to obtain the optimal policy and Opt-KG to compute an approximate policy. The
only computational challenge is how to calculate P t
ic eﬃciently so that the reward can be
evaluated. We present an eﬃcient method in the appendix. We can further use Dirichlet
distribution to model workers reliability as in Liu and Wang (2012). Using multi-class
Bayesian logistic regression, we can also incorporate contextual information into the multi-
class setting in a straightforward manner.

7. Related Works

Categorical crowd labeling is one of the most popular tasks in crowdsourcing since it requires
less eﬀort of the workers to provide categorical labels than other tasks such as language
translations. Most work in categorical crowd labeling are solving a static problem, i.e.,
inferring true labels and workers’ reliability based on a static labeled data set (Dawid and
Skene, 1979; Raykar et al., 2010; Liu and Wang, 2012; Welinder et al., 2010; Whitehill
et al., 2009; Zhou et al., 2012; Liu et al., 2012; Gao and Zhou, 2013). The ﬁrst work that
incorporates diversity of worker reliability is by Dawid and Skene (1979), which uses EM
to perform the point estimation on both worker reliability and true class labels. Based on
that, Raykar et al. (2010) extended (Dawid and Skene, 1979) by introducing Beta prior
for workers’ reliability and features of instances in the binary setting; and Liu and Wang
(2012) further introduced Dirichlet prior for modeling workers’ reliability in the multi-class
setting. Our work utilizes the modeling techniques in these two static models as basic
building blocks but extends to dynamic budget allocation settings.
In recent years, there are several works that have been devoted into online learning or
budget allocation in crowdsourcing (Karger et al., 2013a,b; Bachrach et al., 2012; Ho et al.,
2013; Ertekin et al., 2012; Yan et al., 2011; Kamar et al., 2012; Ipeirotis et al., 2013). The
method proposed in Karger et al. (2013b) is based on the one-coin model. In particular, it
assigns instances to workers according to a random regular bipartite graph. Although the
error rate is proved to achieve the minimax rate, its analysis is asymptotic and method is
not optimal when the budget is limited. Karger et al. (2013a) further extended the work
by Karger et al. (2013b) to the multi-class setting. The new labeling uncertainty method in
Ipeirotis et al. (2013) is one of the state-of-the-art methods for repeated labeling. However,
it does not model each worker’s reliability and incorporate it into the allocation process.
Ho et al. (2013) proposed an online primal dual method for adaptive task assignment and
investigated the sample complexity to guarantee that the probability of making an error for
each instance is less that a threshold. However, it requires gold samples to estimate workers’
reliability. Kamar et al. (2012) used MDP to address a diﬀerent decision problem in crowd
labeling, where the decision maker collects labels for each instance one after another and
only decides whether to hire an additional worker or not. Basically, it is an optimal stopping
problem since there is no pre-ﬁxed amount of budget and one needs to balance the accuracy
v.s. the amount of budget. Since the accuracy and the amount of budget are in diﬀerent

20

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

metrics, such a balance could be very sub jective. Furthermore, the MDP framework in
Kamar et al. (2012) cannot distinguish diﬀerent workers. To the best of our knowledge,
there is no existing method that characterizes the optimal allocation policy for ﬁnite T . In
this work, with the MDP formulation and DP algorithm, we characterize the optimal policy
for budget allocation in crowd labeling under any budget level.

We also note that the budget allocation in crowd labeling is fundamentally diﬀerent
from noisy active learning (Settles, 2009; Nowak, 2009). Active learning usually does not
model the variability of labeling diﬃculties among instances and assumes a single (noisy)
oracle; while in crowd labeling, we need to model both instances’ labeling diﬃculty and
diﬀerent workers’ reliability. Secondly, active learning requires the feature information of
instances for the decision, which could be unavailable in crowd labeling. Finally, the goal
of the active learning is to label as few instances as possible to learn a good classiﬁer. In
contrast, for budget allocation in crowd labeling, the goal is to infer the true labels for as
many instances as possible.

In fact, our MDP formulation is essentially a ﬁnite-horizon Bayesian multi-armed bandit
(MAB) problem. While the inﬁnite-horizon Bayesian MAB has been well-studied and the
optimal policy can be computed via Gittins index (Gittins, 1989), for ﬁnite-horizon Bayesian
MAB, the Gittins index rule is only an approximate policy with high computational cost.
The proposed Opt-KG and a more general conditional value-at-risk based KG could be gen-
eral policies for Bayesian MAB. Recently, a Bayesian UCB policy was proposed to address
a diﬀerent Bayesian MAB problem (Kaufmann et al., 2012). However, it is not clear how
to directly apply the policy to our problem since we are not updating the posterior of the
mean of rewards as in Kaufmann et al. (2012). We note that our problem is also related
to optimal stopping problem. The main diﬀerence is that the optimal stopping problem is
inﬁnite-horizon while our problem is ﬁnite-horizon and the decision process must stop when
the budget is exhausted.

8. Experiments

In this section, we conduct empirical study to show some interesting properties of the
proposed Opt-KG policy and compare its performance to other methods. We observe
that several commonly used priors such as the uniform prior (Beta(1, 1)), Jeﬀery prior
(Beta(1/2, 1/2)) and Haldane prior (Beta(0, 0)) for instances’ soft-label {θi}K
i=1 lead to very
similar performance. Therefore, we adopt the uniform prior (Beta(1, 1)) unless otherwise
|HT ∩H ∗ |+|(HT )c∩(H ∗ )c |
speciﬁed. In addition, for the ease of comparison, the accuracy is deﬁned as
K
which is normalized between [0, 1].

,

8.1 Simulation Study

In this section, we conduct simulated study. For each simulated experiment, we randomly
generate 20 diﬀerent sets of data and report the averaged accuracy. The deviations for
diﬀerent methods are similar and quite small and thus omitted for the purpose of better
visualization and space-saving.

21

Chen, Lin and Zhou

(a) T = 5K = 105

(b) T = 15K = 315

(c) T = 50K = 1050

Figure 4: Labeling counts for instances with diﬀerent levels of ambiguity.

(a) T = 5K = 105

(b) T = 15K = 315

(c) T = 50K = 1050

Figure 5: Labeling counts for workers with diﬀerent levels of reliability.

8.1.1 Study on Labeling Frequency

We ﬁrst investigate that, in the homogeneous noiseless worker setting (i.e., workers are
fully reliable), how the total budget is allocated among instances with diﬀerent levels of
ambiguity.
In particular, we assume there are K = 21 instances with soft-labels θ =
(θ1 , θ2 , θ3 , . . . , θK ) = (0, 0.05, 0.1, . . . , 1). We vary the total budget T = 5K, 15K, 50K and
report the number of times that each instance is labeled on average over 20 independent
runs. The results are presented in Figure 4.
It can be seen from Figure 4 that, more
ambiguous instances with θ close to 0.5 in general receive more labels than those simple
instances with θ close to 0 or 1. A more interesting observation is that when the budget
level is low (e.g., T = 5K in Figure 4(a)), the policy spends less budget on those very
ambiguous instances (e.g., θ = 0.45 or 0.5 ), but more budget on exploring less ambiguous
instances (e.g., θ = 0.35, 0.4 or 0.6). When the budget goes higher (e.g., T = 15K in Figure
4(b)), those very ambiguous instances receive more labels but the most ambiguous instance
(θ = 0.5) not necessarily receives the most labels. In fact, the instances with θ = 0.45 and
θ = 0.55 receive more labels than that of the most ambiguous instance. When the total
budget is suﬃciently large (e.g., T = 50K in Figure 4(c)), the most ambiguous instance
receives the most labels since all the other instances have received enough labels to infer
their true labels.

22

00.20.40.60.8124681012Soft label (θ)Labeling frequency00.20.40.60.810102030405060Soft label (θ)Labeling frequency00.20.40.60.81050100150200250300Soft label (θ)Labeling frequency00.20.40.60.810.511.522.533.5Worker reliability (ρ)Labeling frequency00.20.40.60.81246810Worker reliability (ρ)Labeling frequency00.20.40.60.8115161718192021Worker reliability (ρ)Labeling frequencyStatistical Decision Making for Optimal Budget Allocation in Crowd Labeling

(a) Beta(0.5, 0.5)

(b) Beta(2, 2)

(c) Beta(2, 1)

(d) Beta(4, 1)

Figure 6: Density plot for diﬀerent Beta distributions for generating each θi . Here, (a)
represents there are more easier instances; (b) more ambiguous instances; (c) &
(d) imbalanced class distributions with more positive instances.

Next, we investigate that, in the heterogeneous worker setting, how many instances each
worker is assigned. We simulate K = 21 instances’ soft-labels as before and further simu-
late workers’ reliability ρ = (ρ1 , ρ2 , . . . , ρM ) = (0.1, 0.15, . . . , 0.5, 0.505, 0.515, . . . , 0.995) for
M = 59 workers. Such a simulation ensures that there are more reliable workers, which is
in line with actual situation. We vary the total budget T = 5K, 15K, 50K and report the
number of instances that each worker is assigned on average over 20 independent runs in
Figure 5. As one can see, when the budget level goes up, there is clear trend that more
reliable workers receive more instances.

8.1.2 Prior for Instances

We investigate how robust Opt-KG is when using the uniform prior for each θi . We ﬁrst
simulate K = 50 instances with each θi ∼ Beta(0.5, 0.5), θi ∼ Beta(2, 2), θi ∼ Beta(2, 1) or
θi ∼ Beta(4, 1). The density functions of these four diﬀerent Beta distributions are plotted
in Figure 6. For each generating distribution of θi , we compare Opt-KG using the uniform
prior (Beta(1, 1)) (in red line) to Opt-KG with the true generating distribution as the prior
(in blue line). The comparison in accuracy with diﬀerent levels of budget (T = 2K, . . . , 20K )
is shown in Figure 7. As we can see, the performance of Opt-KG using two diﬀerent priors
are quite similar for most generating distributions except for θi ∼ Beta(4, 1) (i.e., the highly
imbalanced class distribution). When θi ∼ Beta(4, 1), the Opt-KG with uniform prior needs
at least T = 16K units of budget to match the performance of Opt-KG with true generating
distribution as the prior. This result indicates that for balanced class distributions, the
uniform prior is a good choice and robust to the underlying distribution of θi . For highly
imbalanced class distributions, if a uniform prior is adopted, one needs more budget to
recover from the inaccurate prior belief.

8.1.3 Prior on Workers

We investigate how sensitive the prior for the workers’ reliability ρj is. In particular, we sim-
ulate K = 50 instances with each θi ∼ Beta(1, 1) and M = 100 workers with ρj ∼ Beta(3, 1),
ρj ∼ Beta(8, 1) or ρj ∼ Beta(5, 2). We ensure that there are more reliable workers than
spammers or poorly informed workers, which is in line with the actual situation. We use the

23

00.20.40.60.81012345x00.20.40.60.8100.511.5x00.20.40.60.8100.511.52x00.20.40.60.8101234xChen, Lin and Zhou

(a) θi ∼ Beta(0.5, 0.5)

(b) θi ∼ Beta(2, 2)

(c) θi ∼ Beta(2, 1)

(d) θi ∼ Beta(4, 1)

Figure 7: Comparison between Opt-KG using the uniform distribution and true generating
distribution as the prior.

(a) Beta(3, 1)

(b) Beta(8, 1)

(c) Beta(5, 2)

(d) Beta(4, 1)

Figure 8: Density plot for diﬀerent Beta distributions for generating ρj . The plot in (d) is
the one that we use as the prior.

prior Beta(4, 1), which indicates that we have the prior belief that most workers preform
reasonably well and the averaged accuracy is 4/5 = 80%. In Figure 8, we show diﬀerent
density functions for generating ρj and the prior that we use (in Figure 8 (d)). For each

24

020040060080010000.860.880.90.920.940.960.98BudgetAccuracy  Uniform Prior: Beta(1,1)True Prior: Beta(0.5,0.5)020040060080010000.70.750.80.850.90.95BudgetAccuracy  Uniform Prior: Beta(1,1)True Prior: Beta(2,2)020040060080010000.750.80.850.90.95BudgetAccuracy  Uniform Prior: Beta(1,1)True Prior: Beta(2,1)020040060080010000.850.90.951BudgetAccuracy  Uniform Prior: Beta(1,1)True Prior: Beta(4,1)00.20.40.60.8100.511.522.53x00.20.40.60.8102468x00.20.40.60.8100.511.522.5x00.20.40.60.8101234xStatistical Decision Making for Optimal Budget Allocation in Crowd Labeling

(a) θi ∼ Beta(3, 1)

(b) θi ∼ Beta(8, 1)

(c) θi ∼ Beta(5, 2)

Figure 9: Comparison between Opt-KG using Beta(4, 1) and true generating distribution
prior as the prior.

generating distribution of θi , we compare the Opt-KG policy using the prior (Beta(4, 1))
(in red line) to the Opt-KG with the true generating distribution as the prior (in blue line).
The comparison in accuracy with diﬀerent levels of budget (T = 2K, . . . , 20K ) is shown in
Figure 9. From Figure 9, we observe that the performance of Opt-KG using two diﬀerent
priors are quite similar in all diﬀerent settings. Hence, we will use Beta(4, 1) as the prior
when the true prior of workers is unavailable.

8.1.4 Performance Comparison Under the Homogeneous Noiseless Worker
Setting

We compare the performance of Opt-KG under the homogeneous noiseless worker setting
to several other competitors, including

1. Uniform: Uniform sampling.

2. KG(Random): Randomized knowledge gradient (Frazier et al., 2008).

3. Gittins-Inf: A Gittins-indexed based policy proposed by Xie and Frazier (2013) for
solving an inﬁnite-horizon Bayesian MAB problem where the reward is discounted by
δ . Although it solves a diﬀerent problem, we apply it as a heuristic by choosing the
discount factor δ such that T = 1/(1 − δ).

4. NLU: The “new labeling uncertainty” method proposed by Ipeirotis et al. (2013).

We note that we do not compare to the ﬁnite-horizon Gittins index rule (Nino-Mora, 2011)
since its computation is very expensive. On some small-scale problems, we observe that the
ﬁnite-horizon Gittins index rule (Nino-Mora, 2011) has the similar performance as Gittins-Inf
in Xie and Frazier (2013).
We simulate K = 50 instances with each θi ∼ Beta(1, 1), θi ∼ Beta(0.5, 0.5), θi ∼
Beta(2, 2), θi ∼ Beta(2, 1) or θi ∼ Beta(4, 1) (see Figure 6). For each of the ﬁve settings,
we vary the total budget T = 2K, 3K, . . . , 20K and report the mean of accuracy for 20
independently generated sets of {θi}K
i=1 . For the last four settings, we report the comparison
among diﬀerent methods when either using the uniform prior (“uni prior” for short) or the

25

020040060080010000.650.70.750.80.850.90.95BudgetAccuracy  Prior: Beta(4,1)True Prior: Beta(3,1)020040060080010000.70.750.80.850.90.95BudgetAccuracy  Prior: Beta(4,1)True Prior: Beta(8,1)020040060080010000.650.70.750.80.850.90.95BudgetAccuracy  Prior: Beta(4,1)True Prior: Beta(5,2)Chen, Lin and Zhou

true generating distribution as the prior. From Figure 10, the proposed Opt-KG outperforms
all the other competitors in most settings regardless the choice of the prior. For θi ∼
Beta(0.5, 0.5), NLU matches the performance of Opt-KG; and for θi ∼ Beta(2, 2), Gittins-inf
matches the performance of Opt-KG. We also observe that the performance of randomized
KG only slightly improves that of uniform sampling.

8.1.5 Performance Comparison Under the Heterogeneous Worker Setting

We compare the proposed Opt-KG under the heterogeneous worker setting to several other
competitors:

1. Uniform: Uniform sampling.

2. KG(Random): Randomized knowledge gradient (Frazier et al., 2008).

3. KOS: The randomized budget allocation algorithm in Karger et al. (2013b).

We note that several competitors for the homogeneous worker setting (e.g., Gittins-inf and
NLU) cannot be directly applied to the heterogeneous worker setting since they fail to model
each worker’s reliability.
We simulate K = 50 instances with each θi ∼ Beta(1, 1) and M = 100 workers with
ρj ∼ Beta(4, 1), ρj ∼ Beta(3, 1), ρj ∼ Beta(8, 1) or ρj ∼ Beta(5, 2) (see Figure 8). For each
of the four settings, we vary the total budget T = 2K, 3K, . . . , 20K and report the mean
of accuracy for 20 independently generated sets of parameters. For the last three settings,
we report the comparison among diﬀerent methods when either using Beta(4, 1) prior or
the true generating distribution for ρj as the prior. From Figure 11, the proposed Opt-KG
outperforms all the other competitors regardless the choice of the prior.

8.2 Real Data

We compare diﬀerent policies on a standard real data set for recognizing textual entailment
(RTE) (Section 4.3 in Snow et al., 2008). There are 800 instances and each instance is a
sentence pair. Each sentence pair is presented to 10 diﬀerent workers to acquire binary
choices of whether the second hypothesis sentence can be inferred from the ﬁrst one. There
are in total 164 diﬀerent workers. We ﬁrst consider the homogeneous noiseless setting
without incorporating the diversity of workers and use the uniform prior (Beta(1, 1)) for
each θi .
In such a setting, once we decide to label an instance, we randomly choose a
worker (who provides the label in the full data set) to acquire the label. Due to this
randomness, we run each policy 20 times and report the mean of the accuracy in Figure
12(a). As we can see, Opt-KG, Gittins-inf and NLU all perform quite well. We also note
that although Gittins-inf performs slightly better than our method on this data, it requires
solving a linear system with O(T 2 ) variables at each stage, which could be too expensive for
large-scale applications. While our Opt-KG policy has a time complexity linear in K T and
space complexity linear in K , which is much more eﬃcient when a quick online decision is
required. In particular, we present the comparison between Opt-KG and Gittins-inf on the
averaged CPU time under diﬀerent budget levels in Table 4. As one can see, Gittins-inf is
computationally more expensive than Opt-KG.

26

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

(a) θi ∼ Beta(1, 1) (True Prior) (b) θi ∼ Beta(0.5, 0.5) (True Prior) (c) θi ∼ Beta(0.5, 0.5) (Uni Prior)

(d) θi ∼ Beta(2, 2) (True Prior)

(e) θi ∼ Beta(2, 2) (Uni Prior)

(f ) θi ∼ Beta(2, 1) (True Prior)

(g) θi ∼ Beta(2, 1) (Uni Prior)

(h) θi ∼ Beta(4, 1) (True Prior)

(i) θi ∼ Beta(4, 1) (Uni Prior)

Figure 10: Performance comparison under the homogeneous noiseless worker setting.

When the worker reliability is incorporated, we compare diﬀerent policies in Figure
12(b). We put a Beta(4, 1) prior distribution for each ρj which indicates that we have the

27

020040060080010000.750.80.850.90.95BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020040060080010000.80.850.90.951BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020040060080010000.80.850.90.951BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020040060080010000.650.70.750.80.850.90.95BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020040060080010000.650.70.750.80.850.90.95BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020040060080010000.750.80.850.90.95BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020040060080010000.750.80.850.90.95BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020040060080010000.850.90.951BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020040060080010000.80.850.90.951BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KGChen, Lin and Zhou

(a) ρj ∼ Beta(4, 1) (True Prior)

(b) ρj ∼ Beta(3, 1) (True Prior) (c) ρj ∼ Beta(3, 1)
Prior)

(Beta(4, 1)

(d) ρj ∼ Beta(8, 1) (True Prior)

(e) ρj ∼ Beta(8, 1) (Beta(4, 1) Prior)

(f ) ρj ∼ Beta(5, 2) (True Prior)

(g) ρj ∼ Beta(5, 2) (Beta(4, 1) Prior)

Figure 11: Performance comparison under the heterogeneous worker setting.

Budget T

2K = 1, 600

4K = 3, 200

6K = 4, 800

10K = 8, 000

Opt-KG

Gittins-inf

1.09

25.87

2.19

35.70

3.29

45.59

5.48

130.68

Table 4: Comparison in CPU time (seconds)

prior belief that most workers perform reasonably well. Other priors in Figure 8 lead to
similar results and thus omitted here. As one can see, the accuracy of Opt-KG is much
higher than that of other policies when T is small. It achieves the highest accuracy of 92.05%
only using 40% of the total budget (i.e., on average, each instance is labeled 4 times). One
may also observe that when T > 4K = 3, 200, the performance of Opt-KG does not improve

28

020040060080010000.40.50.60.70.80.9BudgetAccuracy  UniformKG (Random)KOSOpt−KG020040060080010000.40.50.60.70.80.9BudgetAccuracy  UniformKG (Random)KOSOpt−KG020040060080010000.40.50.60.70.80.9BudgetAccuracy  UniformKG (Random)KOSOpt−KG020040060080010000.40.50.60.70.80.91BudgetAccuracy  UniformKG (Random)KOSOpt−KG020040060080010000.40.50.60.70.80.91BudgetAccuracy  UniformKG (Random)KOSOpt−KG020040060080010000.40.50.60.70.80.9BudgetAccuracy  UniformKG (Random)KOSOpt−KG020040060080010000.40.50.60.70.80.9BudgetAccuracy  UniformKG (Random)KOSOpt−KGStatistical Decision Making for Optimal Budget Allocation in Crowd Labeling

(a) RTE: Homogeneous Noiseless Worker

(b) RTE: Heterogeneous Worker

Figure 12: Performance comparison on the real data set.

and in fact, slightly downgrades a little bit. This is mainly due to the restrictiveness of
the experimental setting. In particular, since the experiment is conducted on a ﬁxed data
set with partially observed labels, the Opt-KG cannot freely choose instance-worker pairs
especially when the budget goes up (i.e., the action set is greatly restricted). According
to our experience, such a phenomenon will not happen on experiments when labels can be
obtained from any instance-worker pair. Comparing Figure 12(b) to 12(a), we also observe
that Opt-KG under the heterogeneous worker setting performs much better than Opt-KG
under the homogeneous worker setting, which indicates that it is beneﬁcial to incorporate
workers’ reliability.

9. Conclusions and Future Work

In this paper, we propose to address the problem of budget allocation in crowd labeling.
We model the problem using the Bayesian Markov decision process and characterize the
optimal policy using the dynamic programming. We further propose a computationally
more attractive approximate policy: optimistic knowledge gradient. Our MDP formulation
is a general framework, which can be applied to binary or multi-class, contextual or non-
contextual crowd labeling problems in either pull or push crowdsourcing marketplaces.

There are several possible future directions for this work. First, it is of great interest
to show the consistency of Opt-KG in heterogeneous worker setting and further provide
the theoretical results on the performance of Opt-KG under ﬁnite budget. Second, in
this work, we assume that both instances and workers are equally priced. Although this
assumption is standard in many crowd labeling applications, a dynamic pricing strategy as
the allocation process proceeds will better motivate those more reliable workers to label more
challenge instances. A recent work in Wang et al. (2013) provides some quality-based pricing
algorithms for crowd workers and it will be interesting to incorporate their strategies into
our dynamic allocation framework. Third, we assume that the labels provided by the same
worker to diﬀerent instances are independent. It is more interesting to consider that the
workers’ reliability will be improved during the labeling process when some useful feedback

29

020004000600080000.650.70.750.80.850.90.95BudgetAccuracy  UniformKG (Random)Gittins−InfNLUOpt−KG020004000600080000.650.70.750.80.850.90.95BudgetAccuracy  UniformKG (Random)KOSOpt−KGChen, Lin and Zhou

can be provided. Further, since the proposed Opt-KG is a fairly general approximate policy
for MDP, it is also interesting to apply it to other statistical decision problems.

Acknowledgments

We would like to thank Qiang Liu for sharing the code for KOS method; Jing Xie and Peter
Frazier for sharing their code for computing inﬁnite-horizon Gittins index; John Platt,
Chris Burges and Kevin Murphy for helpful discussions; and anonymous reviewers and the
associate editor for their constructive comments on improving the quality of the paper.

Appendix A. Proof of Results

.

(24)

=

(25)

HT = arg max
H

E

(1(i ∈ H )1(i ∈ H ∗ ) + 1(i (cid:54)∈ H )1(i (cid:54)∈ H ∗ ))

In this section, we provide the proofs of the main results of our paper.
A.1 Proof of Proposition 2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)FT
(cid:32) K(cid:88)
(cid:33)
The ﬁnal positive set HT is chosen to maximize the expected accuracy conditioned on FT :
i=1
According to the deﬁnition (6) of P T
i , we can re-write (24) using the linearity of the
expectation:
K(cid:88)
(1(i ∈ H ) Pr(i ∈ H ∗ |FT ) + 1(i (cid:54)∈ H ) Pr(i (cid:54)∈ H ∗ |FT ))
K(cid:88)
(cid:0)1(i ∈ H )P T
i )(cid:1) .
i=1
i + 1(i (cid:54)∈ H )(1 − P T
i=1
To maximize (25) over H , it easy to see that we should set i ∈ H if and only if P T
i ≥ 0.5.
Therefore, we have the positive set
i ≥ 0.5}.
HT = {i : P T
(cid:90) 1
0.5
where B (a, b) is the beta function.
It is easy to see that I (a, b) > 0.5 ⇐⇒ I (a, b) > 1 − I (a, b). We re-write 1 − I (a, b) as
(cid:90) 0.5
(cid:90) 1
follows
1 − I (a, b) =
0.5
0

Recall that
I (a, b) = Pr(θ ≥ 0.5|θ ∼ Beta(a, b)) =

ta−1 (1 − t)b−1dt =

ta−1 (1 − t)b−1dt,

(26)

A.2 Proof of Corollary 3

1
B (a, b)

1
B (a, b)

tb−1 (1 − t)a−1dt,

1
B (a, b)

30

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

I (a, b) − (1 − I (a, b)) =

(cid:90) 1
where the second equality is obtained by setting t := 1 − t. Then we have:
(cid:33)
(cid:32)(cid:18) t
(ta−1 (1 − t)b−1 − tb−1 (1 − t)a−1 )dt
(cid:19)a−b − 1
1
(cid:90) 1
B (a, b)
0.5
ta−1 (1 − t)b−1
1
1 − t
dt.
=
(cid:17)a−b
(cid:16) t
B (a, b)
0.5
(cid:16) t
(cid:16) t
(cid:17)a−b ≡ 1 and I (a, b) = 0.5. When a < b,
(cid:17)a−b
> 1 and hence I (a, b) − (1 − I (a, b)) > 0, i.e,
t
1−t > 1. When a > b,
1−t
1−t
1−t

Since t > 0.5,

< 1 and

I (a, b) > 0.5. When a = b,
I (a, b) < 0.5.

A.3 Proof of Proposition 4

(27)

V (S 0 ) = G0 (S 0 ) + sup
π

We use the proof technique in Xie and Frazier (2013) to prove Proposition 4. According to
(cid:32) K(cid:88)
(cid:33)
(8), the value function takes the following form,
Eπ
V (S 0 ) = sup
h(P T
i )
.
To decompose the ﬁnal accuracy (cid:80)K
π
i=1
i ). Then, (cid:80)K
) − (cid:80)K
i ) and Gt+1 = (cid:80)K
we deﬁne G0 = (cid:80)K
can be decomposed as: (cid:80)K
i ) ≡ G0 + (cid:80)T −1
i=1 h(P T
i ) into the incremental reward at each stage,
i=1 h(P t+1
i=1 h(P T
i=1 h(P t
i=1 h(P 0
i )
i
i=1 h(P T
t=0 Gt+1 . The value function can now be
re-written as follows:
T −1(cid:88)
T −1(cid:88)
t=0
T −1(cid:88)
t=0
t=0

Eπ (E(Gt+1 |Ft ))
Eπ (cid:0)E(Gt+1 |S t , it )(cid:1) .
Here, the ﬁrst inequality is true because G0 is determinant and independent of π ; the
second inequality is due to the tower property of conditional expectation and the third one
i , depends on Ft only through S t
holds because Gt+1 , which is a function of P t+1
and P t
i
and it . We deﬁne incremental expected reward gained by labeling the it -th instance at the
state S t as follows:
(cid:33)
(cid:32) K(cid:88)
) − K(cid:88)
R(S t , it ) = E(Gt+1 |S t , it ) = E
(cid:1) .
= E (cid:0)h(P t+1
h(P t+1
i
i=1
i=1
) − h(P t
it )|S t , it
it

= G0 (S 0 ) + sup
π

= G0 (S 0 ) + sup
π

Eπ (Gt+1 )

i )|S t , it
h(P t

(28)

31

Chen, Lin and Zhou

The last equation is due to the fact that only P t
will be changed if the it -th instance is
it
labeled next. With the expected reward function in place, the value function in (8) can be
(cid:33)
(cid:32)T −1(cid:88)
(cid:12)(cid:12)(cid:12)S 0
re-formulated as:
R(S t , it )
t=0

V (S 0 ) = G0 (s) + sup
π

(29)

Eπ

.

A.4 Proof of Proposition 6

To prove the failure of deterministic KG, we ﬁrst show a key property for the expected
reward function:

R(a, b) =

a
a + b

(h(I (a + 1, b)) − h(I (a, b))) +

(h(I (a, b + 1)) − h(I (a, b))) .
(30)
aB (a,a) and if a (cid:54)= b,
Lemma 10 When a, b are positive integers, if a = b, R(a, b) = 0.52a
R(a, b) = 0.

b
a + b

To prove lemma 10, we ﬁrst present several basic properties for B (a, b) and I (a, b), which
will be used in all the following theorems and proofs.

1. Properties for B (a, b):

B (a + 1, b) =

B (a, b) = B (b, a),
a
a + b
b
a + b

B (a, b + 1) =

B (a, b),

B (a, b).

2. Properties for B (a, b):

I (a, b) = 1 − I (b, a),

I (a + 1, b) = I (a, b) +

0.5a+b
aB (a, b)
I (a, b + 1) = I (a, b) − 0.5a+b
bB (a, b)

,

.

(31)

(32)

(33)

(34)

(35)

(36)

The properties for I (a, b) are derived from the basic property of regularized incomplete
beta function. 1

Proof [Proof of Lemma 10]
When a = b, by Corollary 3, we have I (a + 1, b) > 0.5, I (a, b) = 0.5 and I (a, b + 1) < 0.5.
Therefore, the expected reward (30) takes the following form:
R(a, b) = 0.5(I (a + 1, a) − I (a, a)) + 0.5((1 − I (a, a + 1)) − I (a, a))
0.52a
= I (a + 1, a) − I (a, a) =
aB (a, a)

.

1. http://dlmf.nist.gov/8.17

32

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

When a > b, since a, b are integers, we have a ≥ b+1 and hence I (a+1, b) > 0.5, I (a, b) >
0.5, I (a, b + 1) ≥ 0.5 according to Corollary 3. The expected reward (30) now becomes:
(cid:90) 1
b
a
I (a + 1, b) +
a + b
a + b
(cid:90) 1
t · ta−1 (1 − t)b−1dt
1
a
B (a + 1, b)
a + b
0.5
(cid:90) 1
b
1
a + b
B (a, b + 1)
0.5
(t + (1 − t)) · ta−1 (1 − t)b−1dt − I (a, b)
1
=
B (a, b)
0.5
=I (a, b) − I (a, b) = 0.

ta−1 (1 − t)(1 − t)b−1dt − I (a, b)

I (a, b + 1) − I (a, b)

R(a, b) =

=

+

a
1
1
B (a,b+1) = 1
B (a+1,b) = b
Here we use (32) and (33) to show that
B (a,b) .
a+b
a+b
When a ≤ b − 1, we can prove R(a, b) = 0 in a similar way.

With Lemma 10 in place, the proof for Proposition 6 is straightforward. Recall that the
deterministic KG policy chooses the next instance according to

it = arg max
i

R(S t , i) = arg max
i

i , bt
R(at
i ),

and breaks the tie by selecting the one with the smallest index. Since R(a, b) > 0 if and only
i ) > 0 for those instances i ∈ E = {i : a0
i }.
if a = b, at the initial stage t = 0, R(a0
i , b0
i = b0
The policy will ﬁrst select i0 ∈ E with the largest R(a0
i , b0
i ). After obtaining the label yi0 ,
(cid:54)= b1
either a0
or b0
will add one and hence a1
and R(a1
, b1
) = 0. The policy will
i0
i0
i0
i0
i0
i0
select another instance i1 ∈ E with the “current” largest expected reward and the expected
reward for i1 after obtaining the label yi1 will then become zero. As a consequence, the
|E |
|E |
KG policy will label each instance in E for the ﬁrst |E | stages and R(a
, b
i ) = 0 for all
i ∈ {1, . . . , K }. Then the deterministic policy will break the tie selecting the ﬁrst instance
i
1 (cid:54)= bt
to label. From now on, for any t ≥ |E |, if at
1 , then the expected reward R(at
1 , bt
1 ) = 0.
Since the expected reward for other instances are all zero, the policy will still label the ﬁrst
instance. On the other hand, if at
1 = bt
1 , and the ﬁrst instance is the only one with the
positive expected reward and the policy will label it. Thus Proposition 6 is proved.
Remark 11 For randomized KG, after getting one label for each instance in E for the ﬁrst
|E | stages, the expected reward for each instance has become zero. Then randomized KG wil l
uniformly select one instance to label. At any stage t ≥ |E |, if there exists one instance i (at
most one instance) with at
i = bt
i , the KG policy wil l provide the next label for i; otherwise,
it wil l randomly select an instance to label.

A.5 Proof of Theorem 7

To prove the consistency of the Opt-KG policy, we ﬁrst show the exact values for R+
α (a, b) =
max(R1 (a, b), R2 (a, b)).

33

Chen, Lin and Zhou

1. When a ≥ b + 1:

Therefore,

2. When a = b:

0.5a+b
R1 (a, b) = I (a + 1, b) − I (a, b) =
aB (a, b)
R2 (a, b) = I (a, b + 1) − I (a, b) = − 0.5a+b
bB (a, b)

> 0;

< 0.

R+ (a, b) = R1 (a, b) =

0.5a+b
aB (a, b)

> 0.

R1 (a, b) = I (a + 1, a) − I (a, a) =

0.52a
aB (a, a)
0.52a
R2 (a, b) = 1 − I (a, a + 1) − I (a, a) =
aB (a, a)

;

.

Therefore, we have R1 = R2 and

R+ (a, b) = R1 (a, b) = R2 (a, b) =

0.52a
aB (a, a)

> 0.

3. When b − 1 ≥ a:

R1 (a, b) = I (a, b) − I (a + 1, b) = − 0.5a+b
aB (a, b)
0.5a+b
bB (a, b)

R2 (a, b) = I (a, b) − I (a, b + 1) =

> 0.

< 0;

Therefore

R+ (a, b) = R2 (a, b) =

0.5a+b
bB (a, b)

> 0.

We note that the values of R+ (a, b) for diﬀerent a, b are plotted in Figure 2 in main text.
As we can see R+ (a, b) > 0, for any positive integers (a, b), we ﬁrst prove in the following
lemma that

a+b→∞ R+ (a, b) = 0.
lim

(37)

Lemma 12 Properties for R+ (a, b):

1. R(a, b) is symmetric, i.e., R+ (a, b) = R+ (b, a).

2. lima→∞ R+ (a, a) = 0.
3. For any ﬁxed a ≥ 1, R+ (a + k , a − k) = R+ (a − k , a + k) is monotonical ly decreasing
in k for k = 0, . . . , a − 1.

34

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

4. When a ≥ b, for any ﬁxed b, R+ (a, b) is monotonical ly decreasing in a. By the sym-
metry of R+ (a, b), when b ≥ a, for any ﬁxed a, R+ (a, b) is monotonical ly decreasing
in b.

By the above four properties, we have lim(a+b)→∞ R+ (a, b) = 0.

Proof [Proof of Lemma 12]
We ﬁrst prove these four properties.
• Property 1: By the fact that B (a, b) = B (b, a), the symmetry of R+ (a, b) is straight-
forward.
• Property 2: For a > 1,
R+ (a−1,a−1) = 2a−1
R+ (a,a)
2a < 1 and hence R+ (a, a) is monotonically
a(cid:89)
a(cid:89)
decreasing in a. Moreover,
) ≤ R+ (1, 1)e− (cid:80)a
2i − 1
(1 − 1
R+ (a, a) = R+ (1, 1)
Since lima→∞ (cid:80)a
i=2
2i
2i
i=2
i=2
2i = ∞ and R+ (a, a) ≥ 0, lima→∞ R+ (a, a) = 0.
1
i=2
• Property 3: For any k ≥ 0,
R+ (a + (k + 1), a − (k + 1))
R+ (a + k , a − k)

(a + k)B (a + k , a − k)
(a + k + 1)B (a + (k + 1), a − (k + 1))
a − (k + 1)
a + (k + 1)
• Property 4: When a ≥ b, for any ﬁxed b:

= R+ (1, 1)

< 1.

1
2i .

=

=

R+ (a + 1, b)
R+ (a, b)

=

aB (a, b)
2(a + 1)B (a + 1, b)

=

a(a + b)
2a(a + 1)

< 1.

According to the third property, when a + b is an even number, we have R+ (a, b) <
2 ). According to the fourth property, when a + b is an odd number and a ≥ b + 1,
R+ ( a+b
2 , a+b
we have R+ (a, b) < R+ (a − 1, b) < R+ ( a+b−1
, a+b−1
); while when a + b is an odd number
(cid:19)
(cid:18)
2
2
and a ≤ b − 1, we have R+ (a, b) < R+ (a, b − 1) < R+ ( a+b−1
, a+b−1
). Therefore,
2
2
(cid:99)
(cid:99), (cid:98) a + b
2

R+ (a, b) < R+

(cid:98) a + b
2

.

According to the second property such that lima→∞ R+ (a, a) = 0, we obtain (37).

Using Lemma 12, we ﬁrst show that, in any sample path, the Opt-KG will label each
instance inﬁnitely many times as T goes to inﬁnity. Let ηi (T ) be a random variable rep-
resenting the number of times that the i-th instance has been labeled until the stage T
using Opt-KG. Given a sample path ω , let I (ω) = {i : limT →∞ ηi (T )(ω) < ∞} be the
set of instances that has been labeled only ﬁnite number of times as T goes to inﬁnity in

35

Chen, Lin and Zhou

contradiction. Assuming that I (ω) is not empty, then after a certain stage (cid:98)T , instances in
this sample path. We need to prove that I (ω) is an empty set for any ω . We prove it by
Therefore, there will exist ¯T > (cid:98)T such that:
I (ω) will never be labeled. By Lemma 12, for any j ∈ I c , limT →∞ R+ (aT
j (ω), bT
j (ω)) = 0.
i (ω), b (cid:98)T
i∈I R+ (a (cid:98)T
¯T
¯T
¯T
¯T
i∈I R+ (a
R+ (a
i (ω), b
i (ω)) = max
j (ω), b
max
j (ω)) < max
i (ω)).
j∈I c
Then according to the Opt-KG policy, the next instance to be labeled must be in I (ω),
which leads to the contradiction. Therefore, I (ω) will be an empty set for any ω .
Let Y s
i be the random variable which takes the value 1 if the s-th label of the i-th instance
is 1 and the value −1 if the s-th label is 0. It is easy to see that E(Y s
i |θi ) = Pr(Y s
i = 1|θi ) =
θi . Hence, Y s
i , s = 1, 2, . . . are independent and identically distributed random variables.
By the fact that limT →∞ ηT (i) = ∞ in all sample paths and using the strong law of large
(cid:80)ηi (T )
number, we conclude that, conditioning on θi , i = 1, . . . , K , the conditional probability of
i − bT
aT
s=1 Y s
i |θi ) = 2θi − 1
= E(Y s
i
i
lim
= lim
T →∞
T →∞
ηi (T )
ηi (T )
i } and
for all i = 1, . . . , K , is one. According to Proposition 2, we have HT = {i : aT
i ≥ bT
H ∗ = {i : θi ≥ 0.5}. The accuracy is Acc(T ) = 1
K (|HT ∩ H ∗ | + |H c
T ∩ (H ∗ )c |) . We have:
(cid:18)
(cid:19)
(cid:18)
(cid:19)
T →∞(|HT ∩ H ∗ | + |H c
T →∞ Acc(T ) = 1|{θi}K
T ∩ (H ∗ )c |) = K |{θi}K
Pr( lim
lim
i=1 ) = Pr
i=1
i − bT
aT
= 2θi − 1, ∀i = 1, . . . , K |{θi}K
i
lim
T →∞
i=1
ηi (T )
whenever θi (cid:54)= 0.5 for all i. The last inequality is due to the fact that, as long as θi is
i −bT
ηi (T ) = 2θi − 1, ∀i =
not 0.5 in any i, any sample path that gives the event limT →∞ aT
i
i ) = sgn(2θi − 1)(+∞), which further implies
i − bT
1, . . . , K also gives the event limT →∞ (aT
limT →∞ (|HT ∩ H ∗ | + |H c
T ∩ (H ∗ )c |) = K .
(cid:18)
(cid:19)
(cid:20)
(cid:18)
(cid:19)(cid:21)
Finally, we have:
(cid:20)
(cid:18)
T →∞ Acc(T ) = 1|{θi}K
= E{θi }K
lim
T →∞ Acc(T ) = 1
lim
i=1
i=1
T →∞ Acc(T ) = 1|{θi}K
= E{θi :θi (cid:54)=0.5}K
lim
Pr
i=1
i=1
= E{θi :θi (cid:54)=0.5}K
[1] = 1,
i=1
where the second equality is because {θi : ∃i, θi = 0.5} is a zero measure set.

(cid:19)(cid:21)

≥ Pr

= 1,

Pr

Pr

A.6 Proof of Proposition 8
Recall that our random reward is a two-point distribution with the probability p1 = a
a+b
of being R1 (a, b) = h(I (a + 1, b)) − h(I (a, b)) and p2 = b
a+b of being R2 (a, b) = h(I (a, b +
1)) − h(I (a, b)). The pessimistic KG selects the next instance which maximizes R− (a, b) =

36

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

min(R1 (a, b), R2 (a, b)). To show that the policy is inconsistent, we ﬁrst compute the exact
values for R− (a, b) for positive integers (a, b).
Utilizing Corollary 3 and the basic properties of I (a, b) in (34), (35), (36), we have:
1. When a ≥ b + 1:

0.5a+b
R1 (a, b) = I (a + 1, b) − I (a, b) =
aB (a, b)
R2 (a, b) = I (a, b + 1) − I (a, b) = − 0.5a+b
bB (a, b)

> 0;

< 0.

Therefore,

2. When a = b:

R− (a, b) = R2 (a, b) = − 0.5a+b
bB (a, b)

< 0.

R1 (a, b) = I (a + 1, a) − I (a, a) =

0.52a
aB (a, a)
0.52a
R2 (a, b) = 1 − I (a, a + 1) − I (a, a) =
aB (a, a)

;

.

Therefore, we have x1 = x2 and

R− (a, b) = R1 (a, b) = R2 (a, b) =

0.52a
aB (a, a)

> 0.

3. When b − 1 ≥ a:

R1 (a, b) = I (a, b) − I (a + 1, b) = − 0.5a+b
aB (a, b)
0.5a+b
bB (a, b)

R2 (a, b) = I (a, b) − I (a, b + 1) =

> 0.

< 0;

Therefore

< 0.

R− (a, b) = R1 (a, b) = − 0.5a+b
aB (a, b)
We summarize the properties of R− (a, b) in the next Lemma.
Lemma 13 Properties for R− (a, b):
1. R− (a, b) > 0 if and only if a = b.
2. R− (a, b) is symmetric, i.e., R− (a, b) = R− (b, a)
3. When a = b + 1, then R− (a, b) = R− (b + 1, b) is monotonical ly increasing in b. By
the symmetry of R− (a, b), when b = a + 1, R− (a, b) = R− (a, a + 1) is monotonical ly
increasing in a.

37

Chen, Lin and Zhou

Figure 13: Illustration of R− (a, b).

4. When a ≥ b + 1, for any ﬁxed b, R− (a, b) is monotonical ly increasing in a. By the
symmetry of R− (a, b), when b ≥ a + 1, for any ﬁxed a, R− (a, b) is monotonical ly
increasing in b.

For better visualization, we plot values of R− (a, b) for diﬀerent a, b in Figure 13. All the
properties in Lemma 13 can be seen clearly from Figure 13. The proof of these properties
are based on simple algebra and thus omitted here.
From Lemma 13, we can conclude that for any positive integers a, b with a + b (cid:54)= 3:
R− (1, 2) = R− (2, 1) < R− (a, b).

(38)

Recall that the pessimistic KG selects:

it = arg max
i∈{1,...,K }

R− (at
i , bt
i ).

i = 1 for all i ∈ {1 . . . , K }, the corre-
When starting from the uniform prior with a0
i = b0
sponding R− (a0
i ) = R− (1, 1) > 0. After obtaining a label for any instance i, the Beta
i , b0
parameters for θi will become either (2, 1) or (1, 2) with R− (1, 2) = R− (2, 1) < 0. There-
fore, for the ﬁrst K stages, the pessimistic KG policy will acquire the label for each instance
once. For any instance i, we have either aK
i = 2, bK
i = 1 or aK
i = 1, bK
i = 2 at the stage K .
Then the pessimistic KG policy will select the ﬁrst instance to label. According to (38),
for any t ≥ K , R− (at
1 ) > R− (1, 2) = R− (2, 1). Therefore, the pessimistic KG policy will
1 , bt
consistently acquire the label for the ﬁrst instance. Since the tie will only appear at the
stage K , the randomized pessimistic KG will also consistently select a single instance to
label after K stages.

Appendix B. Incorporate Reliability of Heterogeneous Workers

As we discussed in Section 5 in main text, we approximate the posterior so that at any
stage for all i, j , θi and ρj will follow Beta distributions. In particular, assuming at the

38

ab  1234567891010987654321−0.25 −0.2−0.15 −0.1−0.05    0 0.05  0.1 0.15  0.2 0.25Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

current state θi ∼ Beta(ai , bi ) and ρj ∼ Beta(cj , dj ), the posterior distribution conditioned
on zij takes the following form:
Pr(zij = 1|θi , ρj )Beta(ai , bi )Beta(cj , dj )
p(θi , ρj |zij = 1) =
,
Pr(zij = 1)
Pr(zij = −1|θi , ρj )Beta(ai , bi )Beta(cj , dj )
p(θi , ρj |zij = −1) =
Pr(zij = −1)
,
where the likelihood Pr(zij = z |θi , ρj ) for z = 1, −1 is deﬁned in (18) and (19), i.e.,
Pr(zij = 1|θi , ρj ) = θiρj + (1 − θi )(1 − ρj ),
Pr(zij = −1|θi , ρj ) = (1 − θi )ρj + θi (1 − ρj ).

Also,

Pr(zij = 1) = E(Pr(zij = 1|θi , ρj )) = E(θi )E(ρj ) + (1 − E(θi ))(1 − E(ρj ))
dj
cj
bi
ai
ai + bi
cj + dj
ai + bi
cj + dj

=

+

,

Pr(zij = −1) = E(Pr(zij = −1|θi , ρj )) = (1 − E(θi ))E(ρj ) + E(θi )(1 − E(ρj ))
dj
cj
ai
bi
+
.
=
ai + bi
cj + dj
ai + bi
cj + dj
The posterior distributions p(θi , pj |zij = z ) no longer takes the form of the product
of Beta distributions on θi and pj . Therefore, we use variational approximation by ﬁrst
assuming the conditional independence of θi and ρj :
p(θi , ρj |zij = z ) ≈ p(θi |zij = z )p(ρj |zij = z ).

p(ρj |zij = 1) =

p(θi |zij = −1) =

In fact, the exact form of marginal distributions can be calculated as follows:
θiE(ρj ) + (1 − θi )(1 − E(ρj ))
p(θi |zij = 1) =
Pr(zij = 1)
E(θi )ρj + (1 − E(θi ))(1 − ρj )
Pr(zij = 1)
(1 − θi )E(ρj ) + θi (1 − E(ρj ))
Pr(zij = −1)
(1 − E(θi ))ρj + E(θi )(1 − ρj )
Pr(zij = −1)
To approximate the marginal distribution as Beta distribution, we use the moment matching
technique. In particular, we approximate p (θi |zij = z ) ≈ Beta(˜ai (z ), ˜bi (z )) such that
(cid:101)Ez (θi )
˜ai (z )
(cid:101)Ez (θ2
˜ai (z ) + ˜bi (z )
˜ai (z )(˜ai (z ) + 1)
i )
(˜ai (z ) + ˜bi (z ))(˜ai (z ) + ˜bi (z ) + 1)

.
= Ep(θi |zij =z ) (θ2
i ) =

.
= Ep(θi |zij =z ) (θi ) =

p(ρj |zij = −1) =

(39)

(40)

,

Beta(ai , bi ),

Beta(cj , dj ),

Beta(ai , bi ),

Beta(cj , dj ).

,

39

Chen, Lin and Zhou

,

,

(43)

(41)

(42)

(44)

.
= Ep(ρj |zij =z ) (ρ2
j ) =

˜ai (z )(˜ai (z )+1)
˜ai (z )
where
are the ﬁrst and second order moment of
and
(˜ai (z )+˜bi (z ))(˜ai (z )+˜bi (z )+1)
˜ai (z )+˜bi (z )
(cid:101)Ez (θi ) − (cid:101)Ez (θ2
Beta(˜ai (z ), ˜bi (z )). To make (39) and (40) hold, we have:
˜ai (z ) = (cid:101)Ez (θi )
i ) − (cid:16)(cid:101)Ez (θi )
(cid:17)2 ,
(cid:101)Ez (θ2
i )
(cid:101)Ez (θi ) − (cid:101)Ez (θ2
˜bi (z ) = (1 − (cid:101)Ez (θi ))
i ) − (cid:16)(cid:101)Ez (θi )
(cid:17)2 .
(cid:101)Ez (θ2
i )
Similarly, we approximate p (ρj |zij = z ) ≈ Beta(˜cj (z ), ˜dj (z )), such that
(cid:101)Ez (ρj )
˜cj (z )
.
= Ep(ρj |zij =z ) (ρj ) =
(cid:101)Ez (ρ2
˜cj (z ) + ˜dj (z )
˜cj (z )(˜cj (z ) + 1)
j )
(˜cj (z ) + ˜dj (z ))(˜cj (z ) + ˜dj (z ) + 1)
˜cj (z )(˜cj (z )+1)
˜cj (z )
are the ﬁrst and second order moment of
and
where
(˜cj (z )+ ˜dj (z ))(˜cj (z )+ ˜dj (z )+1)
˜cj (z )+ ˜dj (z )
(cid:101)Ez (ρj ) − (cid:101)Ez (ρ2
Beta(˜cj (z ), ˜dj (z )). To make (39) and (40) hold, we have:
˜cj (z ) = (cid:101)Ez (ρj )
j ) − (cid:16) (cid:101)Ez (ρj )
(cid:17)2 ,
(cid:101)Ez (ρ2
j )
(cid:101)Ez (ρj ) − (cid:101)Ez (ρ2
˜dj (z ) = (1 − (cid:101)Ez (ρj ))
j ) − (cid:16)(cid:101)Ez (ρj )
(cid:17)2 .
(cid:101)Ez (ρ2
j )
(46)
i ), (cid:101)Ez (ρj ) and (cid:101)Ez (ρ2
Furthermore, we can compute the exact values for (cid:101)Ez (θi ), (cid:101)Ez (θ2
j ) as
follows.(cid:101)E1 (θi ) =
i )E(ρj ) + (E(θi ) − E(θ2
i ))(1 − E(ρj ))
E(θ2
ai ((ai + 1)cj + bidj )
(cid:101)E1 (θ2
(ai + bi + 1)(ai cj + bidj )
p(zij = 1)
i ) − E(θ3
i ))(1 − E(ρj ))
i )E(ρj ) + (E(θ2
E(θ3
ai (ai + 1)((ai + 2)cj + bidj )
i ) =
(cid:101)E−1 (θi ) =
(ai + bi + 1)(ai + bi + 2)(ai cj + bidj )
p(zij = 1)
(E(θi ) − E(θ2
i )(1 − E(ρj ))
i ))E(ρj ) + E(θ2
ai (bi cj + (ai + 1)dj )
p(zij = −1)
(cid:101)E−1 (θ2
(ai + bi + 1)(bi cj + aidj )
i ) − E(θ3
i )(1 − E(ρj ))
(E(θ2
i ))E(ρj ) + E(θ3
ai (ai + 1)(bi cj + (ai + 2)dj )
p(zij = −1)
i ) =
(cid:101)E1 (ρj ) =
(ai + bi + 1)(ai + bi + 2)(bi cj + aidj )
j ) + (1 − E(θi ))(E(ρj ) − E(ρ2
E(θi )E(ρ2
j ))
cj (ai (cj + 1) + bidj )
(cid:101)E1 (ρ2
(cj + dj + 1)(ai cj + bidj )
p(zij = 1)
j ) − E(ρ3
j ) + (1 − E(θi ))(E(ρ2
E(θi )E(ρ3
j ))
cj (cj + 1)(ai (cj + 2) + bidj )
j ) =
(cid:101)E−1 (ρj ) =
(cj + dj + 1)(cj + dj + 2)(ai cj + bidj )
p(zij = 1)
(1 − E(θi ))E(ρ2
j ) + E(θi )(E(ρj ) − E(ρ2
j ))
cj (bi (cj + 1) + aidj )
p(zij = −1)
(cid:101)E−1 (ρ2
(cj + dj + 1)(bi cj + aidj )
j ) − E(ρ3
(1 − E(θi ))E(ρ3
j ) + E(θi )(E(ρ2
j ))
cj (cj + 1)(bi (cj + 2) + aidj )
p(zij = −1)
j ) =
(cj + dj + 1)(cj + dj + 2)(bi cj + aidj )

(45)

,

.

,

,

=

=

=

=

,

,

,

,

=

=

=

=

40

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

Assuming that at a certain stage, θi follows a Beta posterior Beta(ai , bi ) and ρj follows
a Beta posterior Beta(cj , dj ), the reward of getting positive and negative labels for the i-th
instance from the j -th worker are:
R1 (ai , bi , cj , dj ) = h(I (˜ai (z = 1), ˜bi (z = 1))) − h(I (ai , bi )),
R2 (ai , bi , cj , dj ) = h(I (˜ai (z = −1), ˜bi (z = −1))) − h(I (ai , bi )),
(48)
cj and dj through (cid:101)Ez (θi ) and (cid:101)Ez (θ2
where ˜ai (z = ±1) and ˜bi (z = ±1) are deﬁned in (41) and (42), which further depend on
i ). With the reward in place, we can directly apply the
Opt-KG policy in the heterogeneous worker setting.

(47)

Appendix C. Extensions

In this section, we provide detailed Opt-KG algorithms for extensions in Section 6.

C.1 Utilizing Contextual Information
When each instance is associated with a p-dimensional feature vector xi ∈ Rp , we incorpo-
rate the feature information in our budget allocation problem by assuming:
θi = σ((cid:104)w, xi (cid:105))

1
1 + exp{−(cid:104)w, xi (cid:105)} ,
1
where σ(x) =
1+exp{−x} is the sigmoid function and w is assumed to be drawn from a
Gaussian prior N (µ0 , Σ0 ). At the t-th stage with the state S t = (µt , Σt ) and w ∼ (µt , Σt ),
the decision maker chooses the it -th instance to be labeled and observes the label yit ∈
{−1, 1}. The posterior distribution p(w|yit , S t ) ∝ p(yit |w)p(w|S t ) has the following log-
likelihood:

(49)

.
=

ln p(w|yit , S t ) = ln p(yit |w) + ln p(w|S t ) + const
=1(yit = 1) ln σ((cid:104)w, xit (cid:105)) + 1(yit = −1) ln (1 − σ((cid:104)w, xit (cid:105)))
(w − µt )(cid:48)Ωt (w − µt ) + const,
− 1
2
where Ωt = (Σt )−1 is the precision matrix. To approximate p(w|yit , µt , Σt ) by a Gaussian
distribution N (µt+1 , Σt+1 ), we use the Laplace method (see Chapter 4.4 in Bishop, 2007).
In particular, the mean of the posterior Gaussian is the MAP (maximum a posteriori)
estimator of w:
ln p(w|yit , S t ),

µt+1 = arg max
w
which can be computed by any numerical optimization method (e.g., Newton’s method).
Ωt+1 = −∇2 ln p(w|yit , S t )(cid:12)(cid:12)w=µt+1
The precision matrix takes the following form,
t+1xit+1 )(1 − σ(µ(cid:48)
t+1xit+1 ))xit+1 x(cid:48)
= Ωt + σ(µ(cid:48)
it+1 .
By Sherman-Morrison formula, the covariance matrix can be computed as,
t+1xit )(1 − σ(µt+1xit ))
σ(µ(cid:48)
Σt+1 = (Ωt+1 )−1 = Σt −
Σtxit+1 x(cid:48)
t+1xit )(1 − σ(µ(cid:48)
it Σt .
t+1xit ))x(cid:48)
1 + σ(µ(cid:48)
it

Σtxit

(50)

41

Chen, Lin and Zhou

We also calculate the transition probability of yit = 1 and yit = −1 using the technique
(cid:90)
(cid:90)
from Bayesian logistic regression (see Chapter 4.5 in Bishop, 2007):

Pr(yit = 1|S t , it ) =

p(yit = 1|w)p(w|S t )dw =

σ(w(cid:48)xi )p(w|S t )dw ≈ σ(µiκ(s2
i )),

i /8)−1/2 and µi = (cid:104)µt , xi (cid:105) and s2
i = x(cid:48)
where κ(s2
i ) = (1 + πs2
iΣtxi .
To calculate the reward function, in addition to the transition probability, we also need
to compute:
(cid:19)
(cid:18)
(cid:12)(cid:12)(cid:12)wt ∼ N (µt , Σt )
i = Pr(θi ≥ 0.5|Ft )
P t
txi} ≥ 0.5
1
1 + exp{−w(cid:48)
= Pr
(cid:19)
(cid:18)(cid:90)
(cid:90) ∞
txi ≥ 0|wt ∼ N (µt , Σt ))
= Pr(w(cid:48)
δ(c − (cid:104)w, xi (cid:105))N (w|µt , Σt )dw
w
0
(cid:90)
where δ(·) is the Dirac delta function. Let
w

δ(c − (cid:104)w, xi (cid:105))N (w|µt , Σt )dw.

p(c) =

dc,

=

P t
i =

Therefore, we have:

p(c)dc = 1 − Φ

Since the marginal of a Gaussian distribution is still a Gaussian, p(c) is a univariate-Gaussian
distribution with the mean and variance:
µi = E(c) = (cid:104)E(w), xi (cid:105) = (cid:104)µt , xi (cid:105),
i = Var(c) = (xi )(cid:48)Cov(w, w)xi = (xi )(cid:48)Σtxi .
s2
(cid:19)
(cid:18)
(cid:90) ∞
− µi
si
0
where Φ(·) is the CDF of the standard Gaussian distribution.
With P t
i and transition probability in place, the expected reward in value function takes
(cid:32) K(cid:88)
(cid:33)
(cid:12)(cid:12)(cid:12)S t , it
) − K(cid:88)
the following form :
R(S t , it ) = E
h(P t+1
h(P t
(52)
.
i )
i
and hence (52) cannot be written as E (cid:0)h(P t+1
(cid:1) in (28). In this problem,
i=1
i=1
We note that since w will aﬀect all P t
i , the summation from 1 to K in (52) can not be omitted
)|S t , it
) − h(P t
it
it
KG or Opt-KG need to solve O(2T K ) optimization problems to compute the mean of the
posterior as in (50), which could be computationally quite expensive. One possibility to
address this problem is to use the variational Bayesian logistic regression (Jaakkola and
Jordan, 2000), which could lead to a faster optimization procedure.

(51)

,

42

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

C.2 Multi-Class Categorization

c }C
{H T
c=1 =

Given the model and notations introduced in Section 6.2, at the ﬁnal stage T when all
budget is used up, we construct the set H T
for each class c to maximize the conditional
c
expected classiﬁcation accuracy:
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)FT
(cid:33)
(cid:32) K(cid:88)
C(cid:88)
I (i ∈ Hc )I (i ∈ H ∗
E
c )
K(cid:88)
C(cid:88)
c=1
i=1
c |FT ) .
I (i ∈ Hc ) Pr (i ∈ H ∗
arg max
Hc⊆{1,...,C },Hc∩H˜c=∅
c=1
i=1
c = {i : θic ≥ θic(cid:48) , ∀c(cid:48) (cid:54)= c} is the true set of instances in the class c. The set H T
Here, H ∗
c
consists of instances that belong to class c. Therefore, {H T
c }C
c=1 should form a partition of
all instances {1, . . . , K }. Let

arg max
Hc⊆{1,...,C },Hc∩H˜c=∅

(53)

=

ic = Pr(i ∈ H ∗
c |FT ) = Pr(θic ≥ θi˜c , ∀ ˜c (cid:54)= c|FT ).
P T

(54)

V (S 0 )

Eπ

= sup
π

.
= sup
π

To maximize the right hand side of (53), we have
c = {i : P T
ic ≥ P T
i˜c , ∀˜c (cid:54)= c}.
H T
(55)
(cid:0)maxc∈{1...,C } P T
(cid:1) .
c. The maximum conditional expected accuracy takes the form: (cid:80)K
If there is i belongs to more than one H T
c , we only assign it to the one with the smallest index
(cid:33)
(cid:32) K(cid:88)
(cid:33)(cid:33)
(cid:32) K(cid:88)
(cid:32)
ic
i=1
(cid:12)(cid:12)(cid:12)FT
C(cid:88)
Then the value function can be deﬁned as:
c )I (i ∈ H ∗
I (i ∈ H T
E
c )
c=1
i=1
i=1
.
= maxc∈{1...,C } P T
iC ) and h(PT
i = (P T
where PT
i1 , . . . , P T
i )
ic . Following Proposition 4, let
ic = Pr(i ∈ H ∗
c |Ft ) and Pt
R(S t , it ) = E (cid:0)h(Pt+1
(cid:1) .
P t
i = (P t
i1 , . . . , P t
iC ), we deﬁne incremental reward function at each
stage:
) − h(Pt
it )|S t , it
it
(cid:32)T −1(cid:88)
(cid:12)(cid:12)(cid:12)S 0
The value function can be re-written as:
R(S t , it )
where G0 (S 0 ) = (cid:80)K
t=0
i ). Since the reward function only depends on S t
i=1 h(P0
it
C(cid:88)
we can deﬁne the reward function in a more explicit way by deﬁning:
αc(cid:80)C
˜c=1 α˜c
c=1

h(I (α + δ c )) − h(I (α)).

V (S 0 ) = G0 (S 0 ) + sup
π

∈ RC
+ ,

h(PT
i )

,

(cid:33)

,

R(α) =

Eπ

= αt
it

Eπ

43

Chen, Lin and Zhou

Here δ c be a row vector of length C with one at the c-th entry and zeros at all other entries;
and I (α) = (I1 (α), . . . , IC (α)) where
Ic (α) = Pr(θc ≥ θ˜c , ∀˜c (cid:54)= c|θ ∼ Dir(α)).

(56)

Therefore, we have R(S t , it ) = R(αt
).
it
To evaluate the reward R(α), the ma jor bottleneck is how to compute Ic (α) eﬃciently.
Directly taking the C -dimensional integration on the region {θc ≥ θ˜c , ∀˜c (cid:54)= c} ∩ ∆C will be
computationally very expensive, where ∆C denotes the C -dimensional simplex. Therefore,
we propose a method to convert the computation of Ic (α) into a one-dimensional integration.
It is known that to generate θ ∼ Dir(α), it is equivalent to generate {Xc}C
c=1 with Xc ∼
Gamma(αc , 1) and let θc ≡ Xc(cid:80)C
. Then θ = (θ1 , . . . , θC ) will follow Dir(α). Therefore,
c=1 Xc
we have:

Ic (α) = Pr(Xc ≥ X˜c , ∀˜c (cid:54)= c|Xc ∼ Gamma(αc , 1)).
(cid:90)
(cid:90)
(cid:90)
C(cid:89)
It is easy to see that
(cid:90)
(cid:89)
0≤xC ≤xc
0≤x1≤xc
xc≥0
c=1
xc≥0
˜c(cid:54)=c

FGamma (xc ; α˜c , 1)dxc ,

Ic (α) =

=

fGamma (xc ; αc , 1)

· · ·

· · ·

fGamma (xc ; αc , 1)dx1 . . . dxC

(57)

(58)

where fGamma (x; αc , 1) is the density function of Gamma distribution with the parameter
(αc , 1) and FGamma (xc ; α˜c , 1) is the CDF of Gamma distribution at xc with the parameter
(α˜c , 1). In many softwares, FGamma (xc ; α˜c , 1) can be calculated very eﬃciently without an
explicit integration. Therefore, we can evaluate Ic (α) by performing only a one-dimensional
numerical integration as in (58). We could also use Monte-Carlo approximation to further
accelerate the computation in (58).

References

P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine Learning, 47:235–256, 2002.

Y. Bachrach, T. Minka, J. Guiver, and T. Graepel. How to grade a test without knowing the
answers - a Bayesian graphical model for adaptive crowdsourcing and aptitude testing.
In ICML, 2012.

M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, Gatsby
Computational Neuroscience Unit, University College London, 2003.

C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2007.

S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed
bandit problems. Foundations and Trends in Machine Learning, 5(1):1–122, 2012.

44

Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling

A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using
the EM algorithm. Journal of the Royal Statistical Society Series C, 28:20–28, 1979.

S. Ertekin, H. Hirsh, and C. Rudin. Wisely using a budget for crowdsourcing. Technical
report, MIT, 2012.

Eyal Even-Dar and Yishay Mansour. Convergence of optimistic and incremental Q-learning.
In NIPS, 2001.

P. Frazier, W. B. Powell, and S. Dayanik. A knowledge-gradient policy for sequential
information collection. SIAM J. Control Optim., 47(5):2410–2439, 2008.

C. Gao and D. Zhou. Minimax optimal convergence rates for estimating ground truth from
crowdsourced labels. arXiv:1310.5764, 2013.

Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B.
Rubin. Bayesian Data Analysis. Chapman and Hall, 3rd edition, 2013.

J. C. Gittins. Multi-armed Bandit Al location Indices. John Wiley & Sons, 1989.

S. S. Gupta and K. J. Miescke. Bayesian look ahead one stage sampling allocations for
selection the largest normal mean. J. of Stat. Planning and Inference, 54(2):229–244,
1996.

C. Ho, S. Jabbari, and J. W. Vaughan. Adaptive task assignment for crowdsourced classi-
ﬁcation. In ICML, 2013.

P. G. Ipeirotis, F. Provost, V. S. Sheng, and J. Wang. Repeated labeling using multiple
noisy label. Data Mining and Know ledge Discovery, 2013.

T. Jaakkola and M. I. Jordan. Bayesian parameter estimation via variational methods.
Statistics and Computing, 10:25–37, 2000.

E. Kamar, S. Hacker, and E. Horvitz. Combing human and machine intelligence in large-
scale crowdsourcing. In AAMAS, 2012.

D. Karger, S. Oh, and D. Shah. Eﬃcient crowdsourcing for multi-class labeling. In ACM
Sigmetrics, 2013a.

D. R. Karger, S. Oh, and D. Shah. Budget-optimal task allocation for reliable crowdsourcing
systems. Operations Research, 62(1):1–24, 2013b.

E. Kaufmann, O. Cappe, and A. Garivier. On Bayesian upper conﬁdence bounds for bandit
problems. In AISTATS, 2012.

L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to person-
alized news article recommendation. In Proceedings of International World Wide Web
Conference, 2010.

C. Liu and Y. M. Wang. Truelabel + confusions: A spectrum of probabilistic models in
analyzing multiple ratings. In ICML, 2012.

45

Chen, Lin and Zhou

Q. Liu, J. Peng, and A. Ihler. Variational inference for crowdsourcing. In NIPS, 2012.

Andrew Y. Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward trans-
formations: Theory and application to reward shaping. In ICML, 1999.

J. Nino-Mora. Computing a classic index for ﬁnite-horizon bandits. INFORMS Journal on
Computing, 23(2):254–267, 2011.

R. D. Nowak. Noisy generalized binary search. In NIPS, 2009.

J. Paisley, D. Blei, and M. Jordan. Variational bayesian inference with stochastic search.
In ICML, 2012.

W. B. Powell. Approximate Dynamic Programming: solving the curses of dimensionality.
John Wiley & Sons, 2007.

M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.
Wiley, 2005.

V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning
from crowds. Journal of Machine Learning Research, 11:1297–1322, 2010.

C. P. Robert. The Bayesian Choice: From Decision-Theoretic Foundations to Computa-
tional Implementation. Springer, 2007.

R. T. Rockafellar and S. Uryasev. Conditional value-at-risk for general loss distributions.
J. of Banking and Finance, 26:1443–1471, 2002.

B. Settles. Active learning literature survey. Technical report, University of Wisconsin–
Madison, 2009.

R. Snow, B. O. Connor, D. Jurafsky, and A. Y. Ng. Cheap and fast - but is it good?
evaluating non-expert annotations for natural language tasks. In EMNLP, 2008.

Istv´an Szita and Andr´as L˝orincz. The many faces of optimism: a unifying approach. In
ICML, 2008.

J. Wang, P. G. Ipeirotis, and F. Provost. Quality-based pricing for crowdsourced workers.
Technical report, New York University, 2013.

P. Welinder, S. Branson, S. Belongie, and P. Perona. The multidimensional wisdom of
crowds. In NIPS, 2010.

J. Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and J. R. Movellan. Whose vote should count
more: Optimal integration of labels from labelers of unknown expertise. In NIPS, 2009.

J. Xie and P. I. Frazier. Sequential bayes-optimal policies for multiple comparisons with a
known standard. Operations Research, 61(5):1174–1189, 2013.

Y. Yan, R. Rosales, G. Fung, and J. Dy. Active learning from crowds. In ICML, 2011.

D. Zhou, S. Basu, Y. Mao, and J. Platt. Learning from the wisdom of crowds by minimax
conditional entropy. In NIPS, 2012.

46

