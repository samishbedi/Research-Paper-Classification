Journal of Machine Learning Research 16 (2015) 455-490

Submitted 3/14; Revised 8/14; Published 3/15

Iterative and Active Graph Clustering Using Trace Norm
Minimization Without Cluster Size Constraints∗

Nir Ailon
Department of Computer Science
Technion IIT Haifa, Israel

Yudong Chen
Department of Electrical Engineering and Computer Sciences
University of California, Berkeley
Berkeley, CA 94720, USA

nailon@cs.technion.ac.il

yudong.chen@eecs.berkeley.edu

Huan Xu
Department of Mechanical Engineering
National University of Singapore
Singapore 117575

Editor: Tong Zhang

mpexuh@nus.edu.sg

Abstract

This paper investigates graph clustering under the planted partition model in the presence
of smal l clusters. Traditional results dictate that for an algorithm to provably correctly
√
recover the underlying clusters, all clusters must be suﬃciently large—in particular, the
cluster sizes need to be ˜Ω(
n), where n is the number of nodes of the graph. We show
that this is not really a restriction: by a reﬁned analysis of a convex-optimization-based
recovery approach, we prove that small clusters, under certain mild assumptions, do not
hinder recovery of large ones. Based on this result, we further devise an iterative algorithm
to provably recover almost al l clusters via a “peeling strategy”: we recover large clusters
ﬁrst, leading to a reduced problem, and repeat this procedure. These results are extended
to the partial observation setting, in which only a (chosen) part of the graph is observed.
The peeling strategy gives rise to an active learning algorithm, in which edges adjacent
to smaller clusters are queried more often after large clusters are learned (and removed).
We expect that the idea of iterative peeling—that is, sequentially identifying a subset of
the clusters and reducing the problem to a smaller one—is useful more broadly beyond the
speciﬁc implementations (based on convex optimization) used in this paper.
Keywords:
graph clustering, community detection, active clustering, convex optimiza-
tion, planted partition model, stochastic block model

1. Introduction

This paper considers the following classic graph clustering problem: given an undirected
unweighted graph, partition the nodes into disjoint clusters so that the density of edges
within each cluster is higher than those across clusters. Graph clustering arises naturally in
many applications across science and engineering; prominent examples include community

∗. This work extends and improves a preliminary conference version Ailon et al. (2013).

c(cid:13)2015 Nir Ailon and Yudong Chen and Huan Xu.

Ailon, Chen and Xu

detection in social networks (Mishra et al., 2007; Zhao et al., 2011), submarket identiﬁcation
in E-commerce and sponsored search (Yahoo!-Inc, 2009), and co-authorship analysis in
document database (Ester et al., 1995), among others. From a purely binary classiﬁcation
theoretical point of view, the edges of the graph are (noisy) labels of “similarity” or “aﬃnity”
between pairs of ob jects, and the concept class consists of clusterings of the ob jects (encoded
graphically by identifying clusters with cliques).
Many theoretical results in graph clustering consider the Planted Partition Model (Con-
don and Karp, 2001), in which the edges are generated randomly based on an unknown
set of underlying clusters; see Section 1.1 for more details. While numerous diﬀerent meth-
ods have been proposed, their performance guarantees under the planted partition model
generally have the following form: under certain conditions of the density of edges (within
√
clusters and across clusters), the method succeeds to recover the correct clusters exactly if
al l clusters are larger than a threshold size, typically ˜Ω(
n);1 see e.g., McSherry (2001);
Bollob´as and Scott (2004); Ames and Vavasis (2011); Chen et al. (2012); Chaudhuri et al.
(2012); Anandkumar et al. (2014).
In this paper, we aim to relax this cluster size constraint of graph clustering under the
planted partition model. Identifying extremely small clusters is inherently hard as they are
easily confused with “fake” clusters generated by noisy edges,2 and is not the focus of this
paper. Instead, in this paper we investigate a question that has not been addressed before:
Can we still recover large clusters in the presence of small clusters? Intuitively, this should
be doable. To illustrate, consider an extreme example where the given graph G consists of
two subgraphs G1 and G2 with disjoint node sets. Suppose G1 , if presented alone, can be
correctly clustered using some existing methods, G2 is a very small clique, and there are
relatively few edges connecting G1 and G2 . The graph G certainly violates the minimum
cluster size requirement of previous results, but why should G2 spoil our ability to correctly
cluster G1?
Our main result conﬁrms this intuition. We show that the cluster size barrier arising in
previous work is not really a restriction, but rather an artifact of the attempt to solve the
problem in a single shot and recover large and small clusters simultaneously. Using a more
√
careful analysis, we prove that a mixed trace-norm and (cid:96)1 -norm based convex formulation
can recover clusters of size ˜Ω(
n) even in the presence of smaller clusters. That is, small
clusters do not interfere with recovery of the large clusters.
The main implication of this result is that one can apply an iterative “peeling” strategy
to recover smaller and smaller clusters. The intuition is simple: suppose the number of
clusters is limited, then either all clusters are large, or the sizes of the clusters vary signif-
icantly. The ﬁrst case is obviously easy. But the second is also tractable, for a diﬀerent
reason: using the aforementioned convex formulation, the larger clusters can be correctly
identiﬁed; if we remove all nodes from these larger clusters, the remaining subgraph contains
signiﬁcantly fewer nodes than the original graph, which leads to a much lower threshold on
the size of the cluster for correct recovery, making it possible for correctly identify some

1. The notations ˜Ω(·) and ˜O(·) ignore logarithmic factors.
2. Indeed, even in a more lenient setup where one clique (i.e., a perfect cluster) of size K is embedded in an
√
Erdos-Renyi graph of n nodes and 0.5 probability of forming an edge, the best known polynomial-time
n) in order to recover the hidden clique, and it has been a long standing open
method requires K = Ω(
problem to relax this requirement.

456

Iterative and Active Clustering Without Size Constraints

smaller clusters. By repeating this procedure, indeed, we can recover the cluster structure
for almost all nodes with no lower bound on the minimal cluster size. Below we summarize
our main contributions and techniques:

1. We provide a reﬁned analysis (Theorem 2) of the mixed trace-norm and (cid:96)1 -norm
convex relaxation approach for exact cluster recovery proposed in Chen et al. (2014a,
partition setting, if each cluster is either large (more precisely, of size at least σ ≈ √
2012), focusing on the case where small clusters exist. We show that in the planted
n)
or small (of size at most σ/C for some global constant C > 1), then with high
probability, this convex relaxation approach correctly identiﬁes all large clusters while
“ignoring” the small ones.
In fact, it is possible to arbitrarily increase the tuning
parameter σ in quest of an interval (σ/C, σ) that is disjoint from the set of cluster
sizes. The analysis is done by identifying a certain feasible solution to the convex
program and proving its almost sure optimality. This solution easily identiﬁes the
√
large clusters. Previous analysis is performed only in the case where all clusters are
of size greater than
n.

2. We provide a converse (Theorem 5) of the result just described. More precisely, we
show that if for some value of the tuning parameter σ , an optimal solution to the
convex relaxation program is an exact representation of a collection of large clusters
(a partial clustering), then these clusters are actual ground truth clusters, even if the
particular interval corresponding to σ isn’t really free of cluster sizes. This allows
the practitioner to be certain that the optimal solution is useful. Moreover, this
has important algorithmic implications for an iterative recovery procedure which we
describe below.

3. The last two points imply that if some interval of the form (σ/C, σ) is free of cluster
sizes, then an exhaustive search of this interval will constructively ﬁnd large clusters,
though not necessarily for that particular interval (Theorem 6). Removing the re-
covered large clusters leads to a reduced problem with a smaller graph. Repeating
this procedure gives rise to an iterative algorithm (Algorithm 2), using a “peeling
strategy”, to recover smaller and smaller clusters that are otherwise impossible to re-
cover. Using this iterative algorithm, we prove that as long as the number of clusters
is bounded by O(log n), regardless of the cluster sizes, we can correctly recover the
cluster structure for an overwhelming fraction of nodes (Theorem 7). To the best of
our knowledge, this is the ﬁrst result of provably correct graph clustering assuming
only an upper bound on the number of clusters, but otherwise no assumption on the
cluster sizes.

4. We extend the result to the partial observation setting, where only a fraction of
similarity labels (i.e., edge/no edge) are queried. As expected, large clusters can be
identiﬁed using small observation rates, and a higher rate is needed to ﬁnd smaller
clusters. Hence, the observation rate serves as the tuning parameter. This gives rise
to an active learning algorithm (Algorithm 4) based on adaptively increasing the rate
of sampling in order to hit an interval free of cluster sizes, and spending more queries
on smaller subgraphs after we identify large clusters and peel them oﬀ. Performance

457

Ailon, Chen and Xu

guarantees are given for this algorithm (Corollary 8–Theorem 11). This active learning
scheme requires signiﬁcantly fewer samples than uniform sampling .

Beside these technical contributions, this paper suggests a new strategy that is poten-
tially useful for general low-rank matrix recovery and other high-dimensional statistical
problems, where the data are typically assumed to have certain low-dimensional structures.
Many methods have been developed to exploit this a priori structural information so that
consistent estimation is possible even when the dimensionality of the problem is larger than
the number of samples. Our result shows that one may combine these methods with a
“peeling strategy” to further push the envelope of learning structured data: by iteratively
recovering the easier structural components and reducing the problem complexity, it may be
possible to learn complicated structures that are otherwise diﬃcult to recover using existing
one-shot approaches.

1.1 Related Work

The literature of graph clustering is too vast for a detailed survey here; we concentrate on
the most related work, and in particular those provide provable guarantees on exact cluster
recovery.

1.1.1 Planted Partition Model

Also known as the stochastic block model (Holland et al., 1983; Condon and Karp, 2001),
this classical model assumes that n nodes are partitioned into subsets, referred to as the
“true clusters”, and a graph is randomly generated as follows: for each pair of nodes, de-
pending on whether or not they belong to the same subset, an edge connecting them is
generated with a probability p or q respectively. The goal is to correctly recover the clus-
ters given the random graph. The planted partition model has a large body of literature.
Earlier work focused on the setting where the minimal cluster size is Θ(n) (Boppana, 1987;
Condon and Karp, 2001; Carson and Impagliazzo, 2001; Bollob´as and Scott, 2004). Sub-
sequently, a number of methods have been proposed methods to handle sublinear cluster
sizes, including randomized algorithms (Shamir and Tsur, 2007), spectral clustering (Mc-
Sherry, 2001; Chaudhuri et al., 2012; Rohe et al., 2011; Kumar and Kannan, 2010), convex
optimization based approaches (Jalali et al., 2011; Chen et al., 2014a, 2012; Ames and
Vavasis, 2011; Oymak and Hassibi, 2011) and tensor decomposition methods (Anandkumar
et al., 2014). See Chen et al. (2014b) for a survey of existing theoretical guarantees for
the planted partition model. While the methodology diﬀers, all the work above requires,
√
sometimes implicitly, a constraint on the minimum size of the true clusters; in particular,
the size must be Ω(
n). Our analysis is carried under the planted partition model, and our
approach requires no constraint on the cluster sizes. We also mention the work of Zhao et al.
(2011) for community detection in social networks, which works under a type of planted
partition model. Like ours, their algorithm extracts clusters in an iterative manner and
is also amenable to outliers. However, their theoretical guarantees are only shown to hold
when n → ∞ and the cluster sizes grow linearly with n.

458

Iterative and Active Clustering Without Size Constraints

1.1.2 Low-rank and Sparse Matrix Decomposition via Trace Norm

Motivated by robustifying principal component analysis (PCA), several authors (Chan-
drasekaran et al., 2011; Cand`es et al., 2011) show that it is possible to recover a low-rank
matrix from sparse errors of arbitrary magnitude, where the key ingredient is using the trace
norm (also known as the nuclear norm) as a convex surrogate of the rank. Similar results
are obtained when the low rank matrix is corrupted by other types of noise (Xu et al., 2012).
Of particular relevance to this paper is the work by Jalali et al. (2011), Oymak and Hassibi
√
(2011) and Chen et al. (2012, 2014a), where they apply this approach to graph clustering,
and speciﬁcally to the planted partition model. These works require the ˜Ω(
n) bound on
the minimal cluster size. Our approach uses the trace norm relaxation, combined with a
more reﬁned analysis and an iterative/active peeling strategy.

1.1.3 Active Learning/Active Clustering

Another line of work that motivates this paper is the study of active learning (a setting in
which labeled instances are chosen by the learner, rather than by nature), and in particular
active learning algorithms for clustering. The most related work is Ailon et al. (2014), who
investigated active learning for the correlation clustering problem (Bansal et al., 2004),
where the goal is to ﬁnd a set of clusters whose Hamming distance from the graph is
minimized. Ailon et al. (2014) obtain a (1 + ε)-approximate solution with respect to the
optimum, while (actively) querying no more than O(n poly(log n, k , ε−1 )) edges, where k
is the number of clusters. Their result imposed no restriction on cluster sizes and hence
inspired this work, but diﬀers in at least two ma jor ways. First, Ailon et al. (2014) did
not consider exact cluster recovery as we do. Second, their guarantees fall in the Empirical
Risk Minimization (ERM) framework, with no running time guarantees. Our work uses a
convex relaxation algorithm, and is hence computationally eﬃcient. The problem of active
learning has also been investigated in other setups including clustering based on distance
matrix (Voevodski et al., 2012; Shamir and Tishby, 2011), hierarchical clustering (Eriksson
et al., 2011; Krishnamurthy et al., 2012) and low-rank matrix/tensor recovery (Krishna-
murthy and Singh, 2013). These setups diﬀer signiﬁcantly from ours..

Remark 1 (A note on a preliminary version of this paper) The authors published
a weaker version of the results in this paper in a preliminary conference paper (Ailon et al.,
2013). An exact comparison is stated after each theorem in the text.

2. Notation and Setup

In this paper the following notations are used. We use X (i, j ) to denote the (i, j )-the
entry of a matrix X . For a matrix X ∈ Rn×n and a subset S ⊆ [n] of size m, the matrix
X [S ] ∈ Rm×m is the principal minor of X corresponding to the set of indexes S . For a
matrix M , s(M ) denotes the support of M , namely, the set of index pairs (i, j ) such that
(cid:40)
M (i, j ) (cid:54)= 0. For any subset Φ of [n] × [n], PΦM is the matrix that satisﬁes
(i, j ) ∈ Φ
M (i, j ),
otherwise.
0,

(PΦM )(i, j ) =

459

Ailon, Chen and Xu

We now describe the problem setup. Throughout the paper, V denotes a ground set
of elements, which we identify with the set [n] = {1, . . . , n}. We assume a ground truth
clustering of V given by a pairwise disjoint covering V1 , . . . , Vk , where k is the number of
clusters. We say i ∼ j if i, j ∈ Va for some a ∈ [k ], otherwise i (cid:54)∼ j . We let na := |Va | be
the size of the a-th cluster for each a ∈ [k ]. For each i ∈ [n], (cid:104)i(cid:105) is index of the cluster that
contains i, the unique index satisfying i ∈ V(cid:104)i(cid:105) .
The ground truth clustering matrix, denoted as K ∗ , is deﬁned as the n × n matrix so
that K ∗ (i, j ) = 1 if i ∼ j , otherwise 0. This is a block diagonal matrix, each block consisting
of 1’s only, and its rank is k . The input is a symmetric n × n matrix A, which is a noisy
version of K ∗ . It is generated according to the planted partition model with parameters p
and q as follows.

We think of A as the adjacency matrix of an undirected random graph, where
the edge (i, j ) is in the graph for i > j with probability pij if i ∼ j , otherwise
with probability qij , independent of other choices, where we only assume the
edge probabilities satisfy (min pij ) =: p > q := (max qij ).

We use the convention that the diagonal entries of A are all 1. The matrix B ∗ := A − K ∗
can be viewed as the noise matrix. Given A, the task is to ﬁnd the ground truth clusters.
We remark that the setup above is more ﬂexible than the standard planted partition
model: we allow the clusters to have diﬀerent sizes, and the edges probabilities (pij and qij )
need not be uniform across node pairs (i, j ). One consequence is that the node degrees may
not be uniform or correlated with the sizes of the associated clusters. Non-uniformity makes
some simple heuristics, such as degree counting and single linkage clustering, vulnerable.
For example, we cannot distinguish between large and small clusters simply by looking at
the node degrees, since nodes in a small cluster may also have high expected degrees. The
√
single linkage clustering approach also fails in the presence of non-uniformity. We illustrate
this with an example. Suppose there are
n clusters of equal size, p = 1 and q = 0.1. We
use the number of common neighbors as the distance function in single linkage clustering.
If all qij are equal to q , then it is easy to see that single linkage clustering will succeed, since
with high probability node pairs in the same cluster will have more common neighbors than
those in diﬀerent clusters. Yet, this is not true for non-uniform qij ’s. Consider three nodes
1, 2 and 3, where nodes 1 and 2 are in the same cluster, and node 3 belongs to a diﬀerent
√
cluster. Suppose for all i > 3, q1i = 0, q2i = q3i = 0.1. The expected number of common
√
√
√
neighbors between nodes 1 and 2 is
n, whereas the expected number of common neighbors
n + 0.01(n − 2
between nodes 2 and 3 is 0.2
n for large n and
n), which is larger than
hence single linkage clustering fails. In contrast, the proposed convex-optimization based
method can handle such non-uniform settings, as we show in what follows.

3. Main Results
the (entry-wise) (cid:96)1 norm of a matrix M is (cid:107)M (cid:107)1 := (cid:80)
We remind the reader that the trace norm of a matrix is the sum of its singular values, and
i,j |M (i, j )|. Consider the following
convex program, combining the trace norm of a matrix variable K with the (cid:96)1 norm of

460

Iterative and Active Clustering Without Size Constraints

(cid:13)(cid:13)Ps(A)c B(cid:13)(cid:13)1
(cid:13)(cid:13)Ps(A)B(cid:13)(cid:13)1
another matrix variable B using two parameters c1 , c2 that will be determined later:
(cid:107)K (cid:107)∗ + c1
min
K,B∈Rn×n
s.t. K + B = A,
0 ≤ Kij ≤ 1, ∀(i, j ).

(CP)

+ c2

Here the trace norm term in the ob jective promotes low-rank solutions and thus encourages
the matrix K to have the zero-one block-diagonal structure of a clustering matrix. The
matrix Ps(A)B = Ps(A) (A − K ) is non-zero only on the pairs (i, j ) between which there is
an edge in the graph (Aij = 1) but the candidate solution has Kij = 0, and thus Ps(A)B
corresponds to the “cross-cluster disagreements” between A and K . Similarly, the matrix
Ps(A)c B corresponds to the “in-cluster disagreements”. Hence, the last two terms in the
ob jective is the weighted sum of these two types of disagreements. The formulation (CP) can
therefore be considered as a convex relaxation of the so-called weighted correlation clustering
approach (Bansal et al., 2004), whose ob jective is to ﬁnd a clustering that minimizes the
weighted disagreements. See Oymak and Hassibi (2011); Mathieu and Schudy (2010); Chen
et al. (2014a) for related formulations.
Important to subsequent development is the following new theoretical guarantee for the
formulation (CP). We show that (CP) identiﬁes the large clusters whose sizes are above a
threshold (chosen by the user) even when small clusters are present. The proof is given in
Section 5.1.

max

,

and set

(cid:96)(cid:93) := b3

Theorem 2 There exist universal constants b3 > 1 > b4 > 0 such that the fol lowing is
(cid:41)
(cid:40)
true. For any (user-speciﬁed) parameters κ ≥ 1 and t ∈ [ 1
κ(cid:112)p(1 − q)n
(cid:112)p(1 − q) log4 n
κ(cid:112)p(1 − q)n
4 p + 3
4 q , 3
4 p + 1
4 q ], deﬁne
√
p − q
p − q
κ(p − q)
1,
(cid:96)(cid:91) := b4
n
(cid:114) 1 − t
(cid:114) t
√
√
1
1
1 − t
,
c2 :=
.
(2)
c1 :=
t
n
100κ
n
100κ
If (i) n ≥ (cid:96)(cid:93) and n ≥ 700, and (ii) for each a ∈ [k ], either na ≥ (cid:96)(cid:93) or na ≤ (cid:96)(cid:91) , then with
probability at least 1 − n−3 , the optimal solution to (CP) with c1 , c2 given above is unique
(cid:40)
and equal to ( ˆK , ˆB ) = (P(cid:93)K ∗ , A − ˆK ), where for a matrix M , P(cid:93)M is the matrix deﬁned by
M (i, j ), max{n(cid:104)i(cid:105) , n(cid:104)j (cid:105)} ≥ (cid:96)(cid:93)
0,
otherwise.

(P(cid:93)M )(i, j ) =

,

(1)

The theorem improves on a weaker version in Ailon et al. 2013, where the ratio (cid:96)(cid:93)/(cid:96)(cid:91) was
√
larger by a factor of log2 n than here. The theorem says that the solution to (CP) identiﬁes
√
clusters of size larger than (cid:96)(cid:93) = Ω(κ
n) and ignores other clusters smaller than (cid:96)(cid:91) . Setting
κ = 1 we recover the usual
n scaling in previous theoretical results. The main novelty
here is the treatment of small clusters, whereas in previous work only large clusters were
allowed, and there was no guarantee for recovery when small clusters are present.

461

Ailon, Chen and Xu

Black represents 1, white represents 0. Here
σmin (K ) is the side length of the smallest
black square.

Figure 1: Illustration of a partial clustering matrix K .

Note that by the theorem’s premise, ˆK is the matrix obtained from K ∗ after zeroing out
p − q ≥ (cid:112)p(1 − q) log4 n/
blocks corresponding to clusters of size at most (cid:96)(cid:91) . Also note that under the assumption
√
κ(cid:112)p(1 − q)n
we get the following simpler expression for (cid:96)(cid:93) in the theorem, replacing its deﬁnition in (1):
p − q

(cid:96)(cid:93) = b3

n ,

(3)

(4)

.

In this case, (cid:96)(cid:93) and (cid:96)(cid:91) diﬀer by only a multiplicative absolute constant b3/b4 . We will make
the assumption (3) in what follows for simplicity, although it is not generally necessary.

Remark 3 The requirement of having a multiplicative constant gap b3/b4 between the sizes
(cid:96)(cid:93) and (cid:96)(cid:91) of the large and smal l clusters, is not an artifact of our analysis; cf. the discussion
at the end of Section 4.

For the convenience of subsequent discussion, we use the following deﬁnition.
Deﬁnition 4 (Partial Clustering Matrix) An n × n matrix K is said to be a partial
clustering matrix if there exists a col lection of pairwise disjoint sets U1 , . . . , Ur ⊆ V (cal led
the induced clusters) such that K (i, j ) = 1 if and only if i, j ∈ Ua for some a ∈ [r], otherwise
0. If K is a partial clustering matrix then σmin (K ) is deﬁned as mina∈[r ] |Ua |.

The deﬁnition is depicted in Figure 1. The key message in Theorem 2 is that by choosing κ
properly such that no cluster size falls in the interval ((cid:96)(cid:91) , (cid:96)(cid:93) ), the unique optimal solution
( ˆK , ˆB ) to the convex program (CP) is such that ˆK is a partial clustering corresponding to
large ground truth clusters.
But how can we choose a proper κ? Moreover, given that we chose a κ (say, by exhaustive
search), how can we certify that it was indeed chosen properly? In order to develop an
algorithm, we would need a type of converse of Theorem 2: There exists an event with high
probability (in the random process generating the input graph), such that conditioned on
this event, for all values of κ, if an optimal solution to the corresponding (CP) is a partial
clustering matrix with the structure illustrated in Figure 1, then the blocks of ˆK correspond
to ground truth clusters.

Theorem 5 There exist absolute constants C1 , C2 > 0 such that with probability at least
1 − n−3 , the fol lowing holds. For al l κ ≥ 1 and t ∈ [ 3
4 q + 1
4 p, 1
4 q + 3
4 p], if (K, B ) is

462

Iterative and Active Clustering Without Size Constraints

(cid:40)
(cid:41)
C2κ(cid:112)p(1 − q)n log n
an optimal solution to (CP) with c1 , c2 as deﬁned in Theorem 2, and additional ly K is a
partial clustering corresponding to U1 , . . . , Ur ⊆ V , with
σmin (K ) ≥ max
C1k log n
p − q
(p − q)2 ,
,
(5)
then U1 , . . . , Ur are actual ground truth clusters, namely, there exists an injection φ : [r] (cid:55)→
[k ] such that Ua = Vφ(a) for al l a ∈ [r].

Algorithm 1 RecoverBigFullObs(V , A, p, q)
require: ground set V , graph A ∈ RV ×V , probabilities p, q
n ← |V |
t ← 1
4 p + 1
4 q , 3
4 p + 3
4 q (or anything in [ 1
4 p + 3
4 q ])
(cid:96)(cid:93) ← n, g ← b3
(cid:27)
(cid:26)
// (If have prior bound k0 on the number of clusters, take (cid:96)(cid:93) ← n/k0 )
b4
√
p(1−q)n log n
while (cid:96)(cid:93) ≥ max
C2
C1 k log n
do
(p−q)2 ,
p−q
solve for κ using (1), set c1 , c2 as in (2)
(K, B ) ← optimal solution to (CP) with c1 , c2
if K is a partial clustering matrix with σmin (K ) ≥ (cid:96)(cid:93) then
return induced clusters {U1 , . . . , Ur } of K
end if
(cid:96)(cid:93) ← (cid:96)(cid:93)/g
end while
return ∅

The proof is given in Section 5.2. The combination of Theorems 2 and 5 implies the
following, which we state in rough terms for simplicity. Let g := b3/b4 . Assume that
√
we iteratively solve (CP) for κ taking values in some decreasing geometric progression of
common ratio g (starting at roughly κ =
n), and halt if the optimal solution is a partial
clustering with clusters of size at least (cid:96)(cid:93) = (cid:96)(cid:93) (κ) (see Algorithm 1). Then these clusters are
(extremely likely to be) ground truth clusters. Moreover, if for some κ in the sequence, (i)
the interval ((cid:96)(cid:91) = (cid:96)(cid:91) (κ), (cid:96)(cid:93) = (cid:96)(cid:93) (κ)) intersects no cluster size, and (ii) there is at least one
cluster at least of size (cid:96)(cid:93) , then such a halt will (be extremely likely to) occur.
The next question is, when are (i) and (ii) guaranteed? If the number of clusters k is a
priori bounded by some k0 , then there is at least one cluster of size at least n/k0 (alluding
to (ii)), and by the pigeonhole principle, any set of k0 + 1 pairwise disjoint intervals of the
form (α, gα) contains at least one interval that intersects no clusters size (alluding to (i)).
For simplicity, we make an exact quantiﬁcation of this principle for the case in which p, q
are assumed to be ﬁxed and independent of n.3 As the following theorem shows, it turns
out that in this regime, k0 can be assumed to be asymptotically logarithmic in n to ensure
recovery of at least one cluster.4 In what follows, notation such as C (p, q), C3 (p, q) denotes
positive functions that depend on p, q only.
3. In fact, we need only ﬁx (p − q), but we wish to keep this exposition simple.
4. In comparison, Ailon et al. (2014) require k0 to be constant for their guarantees, as do the Correlation
Clustering PTAS in Giotis and Guruswami (2006).

463

Ailon, Chen and Xu

Algorithm 2 RecoverFullObs(V , A, p, q)
require: ground set V , matrix A ∈ RV ×V , probabilities p, q
{U1 , . . . , Ur } ← RecoverBigFullObs(V , A, p, q)
V (cid:48) ← [n] \ (U1 ∪ · · · ∪ Ur )
if r = 0 then
return ∅
else
return RecoverFullObs(V (cid:48) , A[V (cid:48) ], p, q) ∪ {U1 , . . . , Ur }
end if

Theorem 6 There exist C3 (p, q), C4 (p, q), C5 > 0 such that the fol lowing holds. Assume
that n > C4 (p, q), and that we are guaranteed that k ≤ k0 , where k0 = C3 (p, q) log n. Then
with probability at least 1 − 2n−3 , Algorithm 1 wil l recover at least one cluster in at most
C5k0 iterations.

The theorem improves on a counterpart in the preliminary paper (Ailon et al., 2013), where
k0 was smaller by a factor of log log n than here.
(cid:17)
(cid:16)
(cid:17)
(cid:16)
(cid:17)
(cid:16)
Proof Consider the set of intervals
n/(g2k0 ), n/(gk0 )

n/(gk0+1k0 ), n/(gk0 k0 )

, . . . ,

n/(gk0 ), n/k0

,

.

By the pigeonhole principle, one of these intervals must not intersect the set of cluster
sizes. Assume this interval is (n/(g i0+1k0 ), n/(g i0 k0 )), for some 0 ≤ i0 ≤ k0 . By setting
√
√
C3 (p, q) small enough so that n/k0 is at least Ω(
n log n), and C4 (p, q) large enough so that
n/gk0+1k0 is at least Ω(
n log n), one easily checks that both the requirements of Theo-
rems 2 and 5 are fulﬁlled.

Theorem 6 ensures that by trying at most a logarithmic number of values of κ, we
can recover at least one large cluster, assuming the number of clusters is logarithmic in n.
After recovering and removing such a cluster, we are left with an input of size n(cid:48) < n,
0 < k0 on the number of clusters. As long as k (cid:48)
together with an updated upper bound k (cid:48)
0
is logarithmic in n(cid:48) , we can continue identifying another large cluster (with respect to the
smaller problem) using the same procedure. Clearly, as long as the input size is of size
at most exp{C3 (p, q)k0}, we can iteratively continue this process. The following has been
proved:

Theorem 7 Assume an upper bound k0 on the number k of clusters, and also that n, k0
satisfy the requirements of Theorem 6. Then with probability at least 1 − 2n−2 , Algorithm 2
recovers clusters covering al l but at most max {exp{C3 (p, q)k0}, C4 (p, q)} elements, without
any restriction on the minimal cluster size.

The theorem improves on a counterpart in the preliminary paper (Ailon et al., 2013). The
consequence is, for example, that if k0 ≤
1
2C3 (p,q) log n, then the algorithm recovers with
high probability clusters covering all but at most O(n1/2 ) elements, without any restriction
on the minimal cluster size.

464

Iterative and Active Clustering Without Size Constraints

3.1 Partial Observations and Active Sampling

We now consider the case where the input matrix A is not given to us in entirety, but rather
that we have oracle access to A(i, j ) for (i, j ) of our choice. Unobserved values are formally
marked as A(i, j ) = ∗.
Consider a more particular setting in which the edge probabilities are p(cid:48) and q (cid:48) , and the
probability of sampling an observation is ρ. More precisely: For i ∼ j we have A(i, j ) = 1
with probability ρp(cid:48) , 0 with probability ρ(1 − p(cid:48) ) and ∗ with remaining probability, indepen-
dently of other pairs. For i (cid:54)∼ j we have A(i, j ) = 1 with probability ρq (cid:48) , 0 with probability
ρ(1 − q (cid:48) ) and ∗ with remaining probability, independently of other pairs. Clearly, by pre-
tending that the values ∗ in A are 0, we emulate the full observation case of the planted
partition model with parameters p = ρp(cid:48) , q = ρq (cid:48) .
Of particular interest is the case in which p(cid:48) , q (cid:48) are held ﬁxed and ρ tends to zero as n
grows. In this regime, by varying ρ and ﬁxing κ = 1, Theorem 2 implies the following:
Corollary 8 There exist constants b3 (p(cid:48) , q (cid:48) ) > b4 (p(cid:48) , q (cid:48) ) > 0 and b5 (p(cid:48) , q (cid:48) ) > 0 such that the
(cid:26)
(cid:27)
fol lowing is true. For any sampling probability parameter 0 < ρ ≤ 1, deﬁne
√
√
log4 n√
(cid:96)(cid:93) = b3 (p(cid:48) , q (cid:48) )
(cid:96)(cid:91) = b4 (p(cid:48) , q (cid:48) )
n√
n√
,
ρ
ρn
ρ
If for each a ∈ [k ], either na ≥ (cid:96)(cid:93) or na ≤ (cid:96)(cid:91) , then, with probability at least 1 − n−3 , the
(cid:115)
program (CP) (after setting ∗ in A to 0) with
(cid:115)

1 − b5 (p(cid:48) , q (cid:48) )ρ
b5 (p(cid:48) , q (cid:48) )ρ
b5 (p(cid:48) , q (cid:48) )
√
1
1 − b5 (p(cid:48) , q (cid:48) )ρ
100
n
has a unique optimal solution equal to ( ˆK , ˆB ) = (P(cid:93)K ∗ , A − ˆK ), where P(cid:93) is as deﬁned in
Theorem 2.

c2 = c2 (p(cid:48) , q (cid:48) ) =

c1 = c1 (p(cid:48) , q (cid:48) ) =

max

√
1
100

n

,

1,

.

(6)

Note that we have slightly abused notation by reusing previously deﬁned global constants
(e.g., b1 ) with global functions of p(cid:48) , q (cid:48) (e.g., b1 (p(cid:48) , q (cid:48) )). Notice now that the sampling
probability ρ can be used as a tuning parameter for controlling the sizes of the clusters we
try to recover, instead of κ. In what follows, we will always assume the following bound on
the observation rate:

ρ ≥ log8 n
n
so that the deﬁnition of (cid:96)(cid:93) in (6) can be replaced by the simpler:
√
(cid:96)(cid:93) = b3 (p(cid:48) , q (cid:48) )
n√
ρ

,

.

(7)

(8)

This assumption is made for simplicity of the exposition, and a more elaborate (though
tedious) derivation can be done without it.
We now present an analogue of the converse result in Theorem 5 for the partial obser-
vation setting. Our main focus is to understand the asymptotics as ρ → 0.

465

Ailon, Chen and Xu

Theorem 9 There exist constants C1 (p(cid:48) , q (cid:48) ), C2 (p(cid:48) , q (cid:48) ) > 0 such that the fol lowing holds
with probability at least 1 − n−3 . For al l observation rate parameters ρ ≤ 1, if (K, B ) is
an optimal solution to (CP) with c1 , c2 as deﬁned in Corol lary 8, and additional ly K is a
(cid:27)
(cid:26) C1 (p(cid:48) , q (cid:48) )k log n
partial clustering corresponding to U1 , . . . , Ur ⊆ V , and also
√
C2 (p(cid:48) , q (cid:48) )
σmin (K ) ≥ max
√
,
(9)
,
ρ
ρ
then U1 , . . . , Ur are actual ground truth clusters, namely, there exists an injection φ : [r] (cid:55)→
[k ] such that Ua = Vφ(a) for each a ∈ [r].

n log n

The proof is similar to that of Theorem 5. The necessary changes are outlined in
Section 5.3. Using the same reasoning as before, we derive the following:
Theorem 10 Let g = (b3 (p(cid:48) , q (cid:48) )/b4 (p(cid:48) , q (cid:48) ))2 (with b3 (p(cid:48) , q (cid:48) ), b4 (p(cid:48) , q (cid:48) ) deﬁned in Corol lary 8).
There exist constants C3 (p(cid:48) , q (cid:48) ) and C4 (p(cid:48) , q (cid:48) ) such that the fol lowing holds. Assume n ≥
C3 (p(cid:48) , q (cid:48) ) and the number of clusters k is bounded by some known number k0 ≤ C4 (p(cid:48) , q (cid:48) ) log n.
Let ρ0 = b3 (p(cid:48) ,q (cid:48) )2 k2
. Then there exists ρ in the set {ρ0 , ρ0g , . . . , ρ0gk0 } for which, if A
0 log n
is obtained with sampling rate ρ (zeroing ∗’s), then with probability at least 1 − 2n−3 , any
n
optimal solution (K, B ) to (CP) with c1 (p(cid:48) , q (cid:48) ), c2 (p(cid:48) , q (cid:48) ) from Corol lary 8 satisﬁes that K is
a partial clustering with the property in (9).

Note that the upper bound on k0 ensures that ρgk0 is a probability. The theorem improves
on a counterpart in the preliminary paper (Ailon et al., 2013), where k0 was smaller by a
factor of log log n compared to here. The theorem is proven, again, using a simple pigeonhole
principle, noting that one of the intervals ((cid:96)(cid:91) (ρ), (cid:96)(cid:93) (ρ)) must be disjoint from the set of cluster
sizes, and there is at least one cluster of size at least n/k0 . The value of ρ0 is chosen so
that n/k0 is larger than the RHS of (9). This theorem motivates the iterative procedure
in Algorithm 3: we start with a low sampling rate ρ, which is then increased geometrically
until the program (CP) returns a partial clustering.
Theorem 10 together with Corollary 8 and Theorem 9 ensures the following. On one
end of the spectrum, if k0 is a constant (and n is large enough), then with high probability
(cid:32)
(cid:19)2k0 (cid:33)
(cid:18) b3 (p(cid:48) , q (cid:48) )
Algorithm 3 recovers at least one large cluster (of size at least n/k0 ) after querying no more
than
nk2
O
0 (log n)
(10)
b4 (p(cid:48) , q (cid:48) )
values of A(i, j ). On the other end of the spectrum, if k0 ≤ δ log n and n is large enough
(exponential in 1/δ), then Algorithm 3 recovers at least one large cluster after querying no
more than n1+O(δ) values of A(i, j ). Iteratively recovering and removing large clusters leads
to Algorithm 4 with the following guarantees.

Theorem 11 Assume an upper bound k0 on the number of clusters k . As long as n is
larger than some function of k0 , p(cid:48) , q (cid:48) , Algorithm 4 wil l recover, with probability at least
1 − n−2 , at least one cluster of size at least n/k0 , regard less of the size of other (smal l)
clusters. Moreover, if k0 is a constant, then clusters covering al l but a constant number
of elements wil l be recovered with probability at least 1 − 2n−2 , and the total number of
observation queries is given by (10), hence almost linear.

466

Iterative and Active Clustering Without Size Constraints

Algorithm 3 RecoverBigPartialObs(V , k0 ) (Assume p(cid:48) , q (cid:48) known, ﬁxed)
require: ground set V , oracle access to A ∈ RV ×V , upper bound k0 on number of clusters
n ← |V |
ρ0 ← b3 (p(cid:48) ,q (cid:48) )2 k2
0 log n
g ← b3 (p(cid:48) , q (cid:48) )2/b4 (p(cid:48) , q (cid:48) )2
n
for s ∈ {0, . . . , k0} do
ρ ← ρ0gs
obtain matrix A ∈ {0, 1, ∗}V ×V by sampling oracle at rate ρ, then zero ∗ values in A
// (can reuse observations from previous iterations)
c1 (p(cid:48) , q (cid:48) ), c2 (p(cid:48) , q (cid:48) ) ← as in Corollary 8
(K, B ) ← an optimal solution to (CP)
if K is a partial clustering matrix satisfying (9) then
return induced clusters {U1 , . . . , Ur }
end if
end for
return ∅

Algorithm 4 RecoverPartialObs(V , k0 ) (Assume p(cid:48) , q (cid:48) known, ﬁxed)
require: ground set V , oracle access to A ∈ RV ×V , upper bound k0 on number of clusters
{U1 , . . . , Ur } ← RecoverBigPartialObs(V , k0 )
V (cid:48) ← [n] \ (U1 ∪ · · · ∪ Ur )
if r = 0 then
return ∅
else
return RecoverFullObs(V (cid:48) , k0 − r) ∪ {U1 , . . . , Ur }
end if

The theorem improves on a counterpart in the preliminary paper (Ailon et al., 2013), where
the recovery covers all but a super-constant (in n) number of elements. Unlike previous
√
convex relaxation based approaches for this problem, which require all cluster sizes to be
n to succeed, there is no constraint on the cluster sizes for our
of size at least roughly
algorithm.

Also note that our algorithm is an active learning one, because more observations fall
√
in smaller clusters which survive deeper in the recursion of Algorithm 4. This feature can
lead to a signiﬁcant saving in the number of queries. When small clusters of size ˜Θ(
n) are
present, previous one-shot algorithms for graph clustering with partial observations (e.g.,
Jalali et al., 2011; Oymak and Hassibi, 2011; Chen et al., 2014a) only guarantee recovery
using O(n2 ) queries, which is much larger than the almost linear requirement ˜O(n) of our
active algorithm.

467

Ailon, Chen and Xu

4. Experiments

We test our main Algorithms 2 and 4 (with subroutines Algorithms 1 and 3) on synthetic
data. In all experiment reports below, we use a variant of the Alternating Direction Method
of Multipliers (ADMM) to solve the semideﬁnite program (CP); see Lin et al. (2011); Chen
et al. (2012). The main cost of ADMM is the computation of the Singular Value Decom-
position (SVD) of an n × n matrix in each round. Note that one can take advantage of the
sparsity of the observations to speed up the SVD (cf. Lin et al. 2011). As is discussed in
previous work, and also observed empirically by us, ADMM converges linearly, so the num-
ber of SVD needed is usually small. See the references above for further discussion of the
optimization issues. The overall computation time also depends on the number of recursive
calls in Algorithm 2 and 4, as well as the number of iterations used in Algorithm 1 and 3
in search for suitable values for κ and ρ (using a multiplicative update rule). These two
numbers are at most O(max(k , log n)) (k is the number of clusters) under the conditions of
the theorems, and in our experiments they are both quite small.

In the experiments we consider simpliﬁed versions of the algorithms: we did not make
an eﬀort to compute the constants (cid:96)(cid:93)/(cid:96)(cid:91) deﬁning the algorithms, creating a diﬃculty in
exact implementation. Instead, for Algorithm 1, we start with κ = 1 and increase κ by
a multiplicative factor of 1.1 in each iteration until a partial clustering matrix is found.
Similarly, in Algorithm 3, the sampling rate ρ has an initial value of 0 and is increased by
an additive factor of 0.025. Still, it is obvious that our experiments support our theoretical
ﬁndings. A more practical “user’s guide” for this method with actual constants is sub ject
to future work.
Whenever we say that “clusters {Vi1 , Vi2 , . . . } were recovered”, we mean that a corre-
sponding instantiation of (CP) resulted in an optimal solution (K, B ) for which K was a
partial clustering matrix induced by {Vi1 , Vi2 , . . . }.

4.1 Experiment 1 (Full Observation)

Consider n = 1100 nodes partitioned into 4 clusters V1 , . . . , V4 , of sizes 800, 200, 80, 20,
respectively. The graph is generated according to the planted partition model with p = 0.5
and q = 0.2, and we assume the full observation setting. We apply the simpliﬁed version
of Algorithm 2 described previously, which terminates in 4 iterations using 44 seconds.
The recovered clusters at each iteration are detailed in Table 1. The table also shows the
values of κ adaptively chosen by the algorithm at each iteration (which happens to equal 1
throughout). We note that the ﬁrst iteration of the algorithm is similar to existing convex
optimization based approaches to graph clustering (Jalali et al., 2011; Oymak and Hassibi,
2011; Chen et al., 2012); the experiment shows that these approaches by itself fail to recover
all the clusters in one shot, thus necessitating the iterative procedure proposed in this paper.

4.2 Experiment 2 (Partial Observation, Fixed Sample Rate)

We have n = 1100 with clusters V1 , . . . , V4 of sizes 800, 200, 50, 50. The observed graph
is generated with p(cid:48) = 0.7, q (cid:48) = 0.1, and observation rate ρ = 0.3. We repeatedly solve
(CP) with c1 , c2 given in Corollary 8. At each iteration, we see that at least one large

468

Iterative and Active Clustering Without Size Constraints

Iteration κ # nodes left Clusters recovered
1100
1
1
V1
300
1
2
V2
3
1
100
V3
20
1
4
V4

Table 1: Results for experiment 1: n = 1100, {|Va |} = {800, 200, 80, 20}, p = 0.5, q = 0.2,
ﬁxed ρ = 1.

Iteration κ # nodes left Clusters recovered
1100
1
1
V1
300
1
2
V2
3
1
100
V3 , V4

Table 2: Results for experiment 2: n = 1100, {|Va |} = {800, 200, 50, 50}, p(cid:48) = 0.7, q (cid:48) = 0.1,
ﬁxed ρ = 0.3.

cluster (compared to the input size at that iteration) is recovered exactly and removed.
The experiment terminates in 3 iterations using 18 seconds. Results are shown in Table 2.

4.3 Experiment 3 (Partial Observation, Adaptive Sampling Rate)

We use the simpliﬁed version of Algorithm 4 described previously. We have n = 1100
with clusters V1 , . . . , V4 of sizes 800, 200, 50, 50. The graph is generated with p(cid:48) = 0.8 and
q (cid:48) = 0.2, and then adaptively sampled by the algorithm. The algorithm terminates in 3
iterations using 148 seconds. Table 3 shows the recovery result and the sampling rates used
in each iteration. From the table we can see that the expected total number of observed
entries used by the algorithm is
11002 · 0.125 + 3002 · 0.25 + 1002 · 0.55 = 179250,

which is 14.8% of all possible node pairs (the actual number of observations is very close to
this expected value). In comparison, we perform another experiment using a non-adaptive
sampling rate, for which we need ρ = 97.5% in order to recover all the clusters in one shot.
Therefore, our adaptive algorithm achieves a signiﬁcant saving in the number of queries.

4.4 Experiment 3A

We repeat the above experiment with a larger instance: n = 4500 with clusters V1 , . . . , V6
of sizes 3200, 800, 200, 200, 50, 50, and p(cid:48) = 0.8, q (cid:48) = 0.2. The algorithm terminates in 182
√
seconds, with results shown in Table 4. Note that we recover the smallest clusters, whose
sizes are below
n. The expected total number of observations used by the algorithm is
3388000, which is 16.7% of all possible node pairs. Using a non-adaptive sampling rate

469

Ailon, Chen and Xu

Iteration
1
2
3

ρ
0.125
0.25
0.55

# nodes left Clusters recovered
1100
V1
300
V2
100
V3 , V4

Table 3: Results for experiment 3: n = 1100, {|Va |} = {800, 200, 50, 50}, p(cid:48) = 0.8, q (cid:48) = 0.2.

Iteration
1
2
3
4

ρ
0.15
0.175
0.2
0.475

# nodes left Clusters recovered
4500
V1
1300
V2
V3 , V4
500
100
V5 , V6

Table 4: Results for experiment 3A: n = 4500, {|Va |} = {3200, 800, 200, 200, 50, 50}, p(cid:48) =
0.8, q (cid:48) = 0.2.

ρ = 35.0% only recovers the 4 largest clusters, and we are unable to recover all 6 clusters
in one shot even with ρ = 1 .

4.5 Experiment 4 (Mid-Size Clusters)

Our current theoretical results do not say anything about the mid-size clusters—those with
sizes between (cid:96)(cid:91) and (cid:96)(cid:93) . It is interesting to investigate the behavior of (CP) in the presence
of mid-size clusters. We generate an instance with n = 750 nodes partitioned into four
clusters of sizes {500, 150, 70, 30}, edge probabilities p = 0.8, q = 0.2 and a sampling rate
ρ = 0.12. We then solve (CP) with a ﬁxed κ = 1. The low-rank part K of the solution
is shown in Figure 2. The large cluster of size 500 is completely recovered in K , while the
two small clusters of sizes 70 and 30 are entirely ignored. The medium cluster of size 150,
however, exhibits a pattern we ﬁnd diﬃcult to characterize. This shows that the constant
gap between (cid:96)(cid:93) and (cid:96)(cid:91) in our theorems is a real phenomenon and not an artifact of our proof
techniques. Nevertheless, the mid-size cluster appears clean, and might allow recovery using
a simple combinatorial procedure. If this is true in general, it might not be necessary to
search for a gap free of cluster sizes. In particular, perhaps for any κ, (CP) identiﬁes all large
clusters above (cid:96)(cid:93) after a simple mid-size cleanup procedure, and ignores all other clusters.
Understanding this phenomenon and its algorithmic implications is of much interest.

5. Proofs

We use the following notation and conventions throughout the proofs. With high probability
or w.h.p. means with probability at least 1 − n−6 . The expressions a ∨ b and a ∧ b mean
max{a, b} and min{a, b}, respectively. For a real n × n matrix M , we use the unadorned
norm (cid:107)M (cid:107) to denote its spectral norm. The notation (cid:107)M (cid:107)F refers to the Frobenius norm,

470

Iterative and Active Clustering Without Size Constraints

(cid:107)M (cid:107)1 is (cid:80)
Figure 2: The solution to (CP) with mid-size clusters.
product (cid:104)X, Y (cid:105) := (cid:80)n
i,j |M (i, j )|, and (cid:107)M (cid:107)∞ is maxij |M (i, j )|. We shall use the standard inner
i,j=1 X (i, j )Y (i, j ).
We will also study operators on the space of matrices, and denote them using a calli-
graphic font, e.g., P . The norm (cid:107)P (cid:107) of an operator is deﬁned as
(cid:107)P (cid:107) :=
(cid:107)PM (cid:107)F .
sup
M ∈Rn×n :(cid:107)M (cid:107)F =1
For a ﬁxed real n × n matrix M , we deﬁne the matrix linear subspace T (M ) as follows:
T (M ) := {Y M + M X : X, Y ∈ Rn×n} .

In words, this subspace is the set of matrices spanned by matrices each row of which is in
the row space of M , and matrices each column of which is in the column space of M . We
let T (M )⊥ denote the orthogonal subspace to T (M ) with respect to (cid:104)·, ·(cid:105), which is given by
T (M )⊥ := {X ∈ Rn×n : (cid:104)X, Y (cid:105) = 0, ∀Y ∈ T (M )} .
It is a well known fact that the pro jection PT (X ) onto T (X ) w.r.t. (cid:104)·, ·(cid:105) is given by
PT (X )M := PC (X )M + PR(X )M − PC (X )PR(X )M ,
where PC (X ) is pro jection (of each column of a matrix) onto the column space of X , and
PR(X ) is pro jection onto the row space of X . The pro jection onto T (M )⊥ is PT (X )⊥M =
M − PT (X )M .
Finally, we recall that s(M ) is the support of M , Ps(M )X is the matrix obtained from
X by setting its entries outside s(M ) to zero, and Ps(M )c X := X − Ps(M )X .

5.1 Proof of Theorem 2

The proof builds on the analysis in Chen et al. (2012). We need some additional notation:
1. We let V(cid:91) ⊆ V denote the set of of elements i such that n(cid:104)i(cid:105) ≤ (cid:96)(cid:91) . (We remind the
reader that n(cid:104)i(cid:105) = |V(cid:104)i(cid:105) |.)
(cid:40)
2. We remind the reader that the pro jection P(cid:93) is deﬁned as follows:
M (i, j ), max{n(cid:104)i(cid:105) , n(cid:104)j (cid:105)} ≥ (cid:96)(cid:93)
otherwise.
0,

(P(cid:93)M )(i, j ) =

471

100200300400500600700100200300400500600700Ailon, Chen and Xu

(P(cid:91)M )(i, j ) =

(cid:40)
3. The pro jection P(cid:91) is deﬁned as follows:
M (i, j ), max{n(cid:104)i(cid:105) , n(cid:104)j (cid:105)} ≤ (cid:96)(cid:91)
0,
otherwise.
In words, P(cid:91) pro jects onto the set of matrices supported on V(cid:91) × V(cid:91) . Note that by the
theorem assumption, P(cid:93) + P(cid:91) = I d (equivalently, P(cid:93) pro jects onto the set of matrices
supported on (V × V ) \ (V(cid:91) × V(cid:91) )).
4. We use U ΣU (cid:62) to denote the rank-k (cid:48) Singular Value Decomposition (SVD) of the
symmetric matrix ˆK , where k (cid:48) = rank( ˆK ) and equals the number of clusters with size
at least (cid:96)(cid:93) .
(cid:9) ,
D := (cid:8)∆ ∈ Rn×n |∆ij ≤ 0, ∀i ∼ j, (i, j ) /∈ V(cid:91) × V(cid:91) ; 0 ≤ ∆ij , ∀i (cid:54)∼ j, (i, j ) /∈ V(cid:91) × V(cid:91)
5. Deﬁne the set
which strictly contains all feasible deviation from ˆK .

6. For simplicity we write T := T ( ˆK ).

We will make use of the following facts:
1. I d = Ps( ˆB ) + Ps( ˆB )c = Ps(A) + Ps(A)c .
2. P(cid:93) , P(cid:91) , Ps( ˆB ) , Ps( ˆB )c , Ps(A) , and Ps(A)c commute with each other.
5.1.1 Approximate Dual Certificate Condition

We begin by giving a deterministic suﬃcient condition for ( ˆK , ˆB ) to be the unique optimal
solution to the program (CP).

Proposition 12 ( ˆK , ˆB ) is the unique optimal solution to (CP) if there exists a matrix
Q ∈ Rn×n and a number 0 <  < 1 satisfying:
1. (cid:107)Q(cid:107) < 1;
2. (cid:107)PT (Q)(cid:107)∞ ≤ 
2 min {c1 , c2};
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )P(cid:93)∆
(cid:68)
(cid:69)
3. ∀∆ ∈ D:
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )P(cid:93)∆
(cid:69)
(cid:68)
U U (cid:62) + Q, Ps(A)Ps( ˆB )P(cid:93)∆
,
= (1 + )c1
U U (cid:62) + Q, Ps(A)c Ps( ˆB )P(cid:93)∆
;
= (1 + )c2
(b)
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )c P(cid:93)∆
(cid:69) ≥ −(1 − )c1
(cid:68)
4. ∀∆ ∈ D:
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )c P(cid:93)∆
(cid:69) ≥ −(1 − )c2
(cid:68)
U U (cid:62) + Q, Ps(A)Ps( ˆB )c P(cid:93)∆
,
U U (cid:62) + Q, Ps(A)c Ps( ˆB )c P(cid:93)∆

(a)

;

(a)

(b)

472

Iterative and Active Clustering Without Size Constraints

(cid:13)(cid:13)(cid:13)Ps( ˆB )c P(cid:91) (U U (cid:62) + Q)
(cid:13)(cid:13)(cid:13)∞
5. Ps( ˆB )P(cid:91) (U U (cid:62) + Q) = c1P(cid:91)
ˆB ;
≤ c2 .
6.
Proof Consider any feasible solution ( ˆK + ∆, ˆB − ∆) to (CP); we know ∆ ∈ D due to
the inequality constraints in (CP). We will show that this solution will have strictly higher
ob jective value than ( ˆK , ˆB ) if ∆ (cid:54)= 0.
For this ∆, let G∆ be a matrix in T ⊥ ∩ Range(P(cid:91) ) satisfying (cid:107)G∆(cid:107) = 1 and (cid:104)G∆ , ∆(cid:105) =
(cid:107)PT ⊥ P(cid:91)∆(cid:107)∗ ; such a matrix always exists because RangeP(cid:91) ⊆ T ⊥ . Suppose (cid:107)Q(cid:107) = b.
Clearly, PT ⊥ Q + (1 − b)G∆ ∈ T ⊥ and, due to Property 1 in the proposition, we have
b < 1 and (cid:107)PT ⊥ Q + (1 − b)G∆(cid:107) ≤ (cid:107)Q(cid:107) + (1 − b) (cid:107)G∆(cid:107) = b + (1 − b) = 1. Therefore,
U U (cid:62) + PT ⊥ Q + (1 − b)G∆ is a subgradient of f (K ) = (cid:107)K (cid:107)∗ at K = ˆK . On the other hand,
Ps(A) ( ˆB + F∆ ) is a subgradient of g1 (B ) = (cid:13)(cid:13)Ps(A)B(cid:13)(cid:13)1
deﬁne the matrix F∆ = −Ps( ˆB )c sgn(∆). We have F∆ ∈ s( ˆB )c and (cid:107)F∆(cid:107)∞ ≤ 1. Therefore,
is a subgradient of g2 (B ) = (cid:13)(cid:13)Ps(A)c B(cid:13)(cid:13)1
at B = ˆB , and Ps(A)c ( ˆB + F∆ )
at B = ˆB . Using these three subgradients, the
diﬀerence in the ob jective value can be bounded as follows:
(cid:44) (cid:13)(cid:13)(cid:13) ˆK + ∆
(cid:13)(cid:13)(cid:13)Ps(A)
(cid:13)(cid:13)(cid:13)∗
− (cid:13)(cid:13)(cid:13) ˆK
(cid:13)(cid:13)(cid:13)Ps(A)c ( ˆB − ∆)
(cid:13)(cid:13)(cid:13)Ps(A) ( ˆB − ∆)
(cid:13)(cid:13)(cid:13)∗ + c1
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)1
d(∆)
(cid:13)(cid:13)(cid:13)Ps(A)c ˆB
(cid:13)(cid:13)(cid:13)1
− c1
ˆB
≥ (cid:68)
(cid:69)
(cid:69)
(cid:68)Ps(A) ( ˆB + F∆ ), −∆
(cid:68)Ps(A)c ( ˆB + F∆ ), −∆
− c2
(cid:68)
(cid:69)
(cid:68)Ps(A)
(cid:69)
(cid:68)Ps(A)c ˆB , −∆
(cid:69)
U U (cid:62) + PT ⊥ Q + (1 − b)G∆ , ∆
+ c1
+ c2
(cid:10)Ps(A)c F∆ , −∆(cid:11)
(cid:10)Ps(A)F∆ , −∆(cid:11) + c2
U U (cid:62) + PT ⊥ Q, ∆
=(1 − b) (cid:107)PT ⊥ P(cid:91)∆(cid:107)∗ +
ˆB , −∆
+ c1
+ c2
(cid:68)
(cid:69)
(cid:69)
(cid:68)P(cid:91)Ps(A)c ˆB , −∆
(cid:68)P(cid:91)Ps(A)
(cid:69)
+ c1
(cid:68)P(cid:93)Ps(A)
(cid:69)
(cid:68)P(cid:93)Ps(A)c ˆB , −∆
(cid:69)
(cid:10)Ps(A)c F∆ , −∆(cid:11) .
(cid:10)Ps(A)F∆ , −∆(cid:11) + c2
=(1 − b) (cid:107)PT ⊥ P(cid:91)∆(cid:107)∗ +
U U (cid:62) + PT ⊥ Q, ∆
ˆB , −∆
+ c2
+ c1
ˆB , −∆
+ c1
+ c2
+ c1
(cid:68)P(cid:91)Ps(A)c ˆB , −∆
(cid:69)
(cid:68)P(cid:91)Ps(A)
(cid:68)P(cid:91)
(cid:69)
(cid:69)
The last six terms of the last RHS satisfy:
(cid:13)(cid:13)(cid:13)1
(cid:69) ≥ − (cid:13)(cid:13)(cid:13)P(cid:93)Ps(A)c Ps( ˆB )∆
(cid:69) ≥ − (cid:13)(cid:13)(cid:13)P(cid:93)Ps(A)Ps( ˆB )∆
(cid:13)(cid:13)(cid:13)1
(cid:68)P(cid:93)Ps(A)
(cid:68)P(cid:93)Ps(A)c ˆB , ∆
ˆB , −∆
ˆB , −∆
, because P(cid:91)
ˆB ∈ s(A).
= c1
1. c1
+ c2
(cid:13)(cid:13)(cid:13)∞
(cid:13)(cid:13)(cid:13) ˆB
ˆB , −∆
2.
and
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )c ∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )c ∆
(cid:13)(cid:13)(cid:13)1
because ˆB ∈ s( ˆB ) and
≤ 1.
3. (cid:10)Ps(A)F∆ , −∆(cid:11) =
and (cid:10)Ps(A)c F∆ , −∆(cid:11) =
, due to
the deﬁnition of F .
(cid:13)(cid:13)(cid:13)P(cid:93)Ps(A)Ps( ˆB )∆
(cid:13)(cid:13)(cid:13)1
(cid:69) − c1
(cid:68)P(cid:91)
(cid:69)
(cid:68)
It follows that
(cid:13)(cid:13)(cid:13)P(cid:93)Ps(A)c Ps( ˆB )∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)Psc ( ˆB )∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)Ps(A)c Psc ∆(cid:13)(cid:13)1
ˆB , −∆
U U (cid:62) + PT ⊥ Q, ∆
d(∆) ≥(1 − b) (cid:107)PT ⊥ P(cid:91)∆(cid:107)∗ +
+ c1
− c2
(11)
.
+ c2
473

(cid:69)

+ c2

+ c1

,

Ailon, Chen and Xu

.

=

+

(cid:69)

(cid:68)
(cid:69)
(cid:68)
(cid:69)
(cid:68)
(cid:69) − (cid:104)PT Q, ∆(cid:105) .
Consider the second term in the last RHS, which equals
U U (cid:62) + PT ⊥ Q, ∆
U U (cid:62) + Q, P(cid:93)∆
U U (cid:62) + Q, P(cid:91)∆
(cid:68)
(cid:69)
We bound these three terms separately. For the ﬁrst term, we have
(cid:16)Ps(A)Ps( ˆB )P(cid:93) + Ps(A)c Ps( ˆB )P(cid:93) + Ps(A)Ps( ˆB )c P(cid:93) + Ps(A)c Psc P(cid:93)
(cid:68)
(cid:69)
(cid:17)
U U (cid:62) + Q, P(cid:93)∆
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )c P(cid:93)∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )P(cid:93)∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )P(cid:93)∆
(cid:13)(cid:13)(cid:13)1
U U (cid:62) + Q,
=
∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )c P(cid:93)∆
− (1 − )c1
≥(1 + )c1
+ (1 + )c2
− (1 − )c2
(Using Properties 3 and 4.)
(cid:69)
(cid:68)
For the second term, we have
(cid:69)
(cid:68)Ps( ˆB )P(cid:91) (U U (cid:62) + Q), ∆
(cid:68)Ps( ˆB )c P(cid:91) (U U (cid:62) + Q), ∆
U U (cid:62) + Q, P(cid:91)∆
(cid:13)(cid:13)(cid:13)Ps( ˆB )c P(cid:91)∆
(cid:13)(cid:13)(cid:13)1
(cid:68)P(cid:91)
(cid:69) − c2
+
=
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )c P(cid:91)∆
(cid:68)P(cid:91)
(cid:69) − c2
≥c1
ˆB , ∆
(using Properties 5 and 6)
(Because Ps(A)c Ps( ˆB )c P(cid:91) = Ps( ˆB )c P(cid:91) .)
Finally, for the third term, Due to the block diagonal structure of the elements of T , we
have PT = P(cid:93)PT and therefore
(cid:104)−PT Q, ∆(cid:105) = − (cid:104)PT Q, P(cid:93)∆(cid:105) ≥ − (cid:107)PT Q(cid:107)∞ (cid:107)P(cid:93)∆(cid:107)1 ≥ − 
min {c1 , c2} (cid:107)P(cid:93)∆(cid:107)1 .
2
Combining the above three bounds with Eq. (11), we obtain
(cid:13)(cid:13)(cid:13)P(cid:93)Ps(A)c Ps( ˆB )∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)P(cid:93)Ps(A)Ps( ˆB )∆
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )c P(cid:93)∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)1
d(∆)
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )c P(cid:91)∆
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )c P(cid:93)∆
≥(1− b) (cid:107)PT ⊥ P(cid:91)∆(cid:107)∗ + c1
+ c2
+ c1
(cid:13)(cid:13)P(cid:93)Ps(A)c ∆(cid:13)(cid:13)1
(cid:13)(cid:13)P(cid:93)Ps(A)∆(cid:13)(cid:13)1
− 
min {c1 , c2} (cid:107)P(cid:93)∆(cid:107)1
+ c1
+ c2
2
− 
min {c1 , c2} (cid:107)P(cid:93)∆(cid:107)1
=(1 − b) (cid:107)PT ⊥ P(cid:91)∆(cid:107)∗ + c1
+ c2
2
(note that Ps(A)Ps( ˆB )c P(cid:91)∆=0)
≥(1 − b) (cid:107)P(cid:91)∆(cid:107)∗ +
min {c1 , c2} (cid:107)P(cid:93)∆(cid:107)1 ,

2
which is strictly greater than zero for ∆ (cid:54)= 0.

ˆB , ∆

=c1

.

5.1.2 Constructing Q
To prove the theorem, it suﬃces to show that with probability at least 1 − n−3 , there
exists a matrix Q with the properties required by Proposition 12. We do this by explicitly

474

Iterative and Active Clustering Without Size Constraints

(cid:115)

(cid:40)

√
κ

n

,

(cid:41)

,

 :=

max

P(cid:93)Q2 (i, j ) =

P(cid:93)Q1 (i, j ) =

constructing Q. Suppose we take
100(cid:112)t(1 − t)
log4 n
(cid:96)(cid:93)
(cid:96)(cid:93)
and use the weights c1 and c2 given in Theorem 2. We specify P(cid:93)Q and P(cid:91)Q separately.

The matrix P(cid:93)Q is given by P(cid:93)Q = P(cid:93)Q1 + P(cid:93)Q2 + P(cid:93)Q3 , where for (i, j ) /∈ V(cid:91) × V(cid:91) ,
− 1
i ∼ j, (i, j ) ∈ s( ˆB )
n(cid:104)i(cid:105) ,
n(cid:104)i(cid:105) · 1−pij
i ∼ j, (i, j ) ∈ s( ˆB )c
1
,
−(1 + )c2 ,
pij
i (cid:54)∼ j
0,
i ∼ j, (i, j ) ∈ s( ˆB )
1−pij
i ∼ j, (i, j ) ∈ s( ˆB )c
(1 + )c2
(1 + )c1 ,
pij
i (cid:54)∼ j
0,
i (cid:54)∼ j, (i, j ) ∈ s( ˆB )
i (cid:54)∼ j, (i, j ) ∈ s( ˆB )c
−(1 + )c1
qij
1−qij
i ∼ j.
0,
Note that these matrices have zero-mean entries. (Recall that s( ˆB ) = s(A − ˆK ) is a random
set since the graph A is random.)

P(cid:91)Q is given as follows. For (i, j ) ∈ V(cid:91) × V(cid:91) ,
c1 ,
−c2 ,
c1 ,
c2W (i, j ),
(cid:40)
where W is a symmetric matrix whose upper-triangle entries are independent and obey
t−q
2t(1−q) ,
+1, with probability
−1, with remaining probability.

i ∼ j, (i, j ) ∈ s(A)
i ∼ j, (i, j ) ∈ s(A)c
i (cid:54)∼ j, (i, j ) ∈ s(A)
i (cid:54)∼ j, (i, j ) ∈ s(A)c ,

P(cid:93)Q3 (i, j ) =

P(cid:91)Q(i, j ) =

,

,

W (i, j ) =

Note that we introduced additional randomness in W .

5.1.3 Validating Q
4 p ≤ t ≤ p and 1
4 (1 − q) ≤ 1 − t ≤ 1 − q .
Under the choice of t in Theorem 2, we have 1
Also under the assumption (1) in the theorem and since p − q ≤ p(1 − q), (cid:96)(cid:93) ≤ n, we have
≥ b3 log4 n
∨ b3 log4 n
p(1 − q) ≥ b2
3 κ2n
. Using these inequalities, it is easy to check that  < 1
(cid:96)2
(cid:96)(cid:93)
n
2
(cid:93)
provided that the constant b3 is suﬃciently large. We will make use of these facts frequently
in the proof.
We now verify that the Q constructed above satisfy the six properties in Proposition 12
with probability at least 1 − n−3 .

475

Ailon, Chen and Xu

√

1
100κ

n

≤ max

≤ 1
32

,

(cid:107)E[P(cid:91)Q∼ ](cid:107) ≤ (cid:96)(cid:91)

Property 1:
Suppose the matrix Q∼ is obtained from Q by setting all Q(i, j ) with i (cid:54)∼ j to zero,
and Q(cid:54)∼ = Q − Q∼ . Note that (cid:107)Q(cid:107) ≤ (cid:107)P(cid:93)Q∼(cid:107) + (cid:107)P(cid:93)Q(cid:54)∼(cid:107) + (cid:107)P(cid:91)Q∼(cid:107) + (cid:107)P(cid:91)Q(cid:54)∼(cid:107). Below we
show that with high probability, the ﬁrst term is upper-bounded by 7
32 and the other threes
4 , which establishes that (cid:107)Q(cid:107) ≤ 31
terms are upper-bounded by 1
32 .
(a) P(cid:91)Q∼ is a block diagonal matrix support on V(cid:91) × V(cid:91) , where the size of each block is at
most (cid:96)(cid:91) . Note that P(cid:91)Q∼ = E[P(cid:91)Q∼ ] + (P(cid:91)Q∼ − E[P(cid:91)Q∼ ]). Here E[P(cid:91)Q∼ ] is a deterministic
p−t√
√
1
. We thus have
matrix with all non-zero entries equal to
t(1−t)
n
100κ
p − t(cid:112)t(1 − t)
where the last inequality holds under the deﬁnition of (cid:96)(cid:91) in Theorem 2. On the other
hand, the matrix P(cid:91)Q∼ − E[P(cid:91)Q∼ ] is a random matrix whose entries are independent,
bounded almost surely by B := max{c1 , c2} and have zero mean with variance bounded by
1002 κ2n · p(1−p)
t(1−t) . If (cid:96)(cid:91) ≤ n2/3 , we apply part 1 of Lemma 17 to obtain
(cid:115)
(cid:41)
(cid:40)
1
p(1 − p)
√
(cid:107)P(cid:91)Q∼ − E[P(cid:91)Q∼ ](cid:107) ≤ 10 max
(cid:96)(cid:91) log n, (c1 ∨ c2 ) log n
1
(cid:115)
(cid:40)
(cid:32)(cid:114) 1 − t
(cid:114) t
t(1 − t)
100κ
n
p(1 − p)
√
≤ 3
1
1
log n
t(1 − t)
1 − t
n1/3
t
10κ
16
10κ
n
(cid:16)
(cid:17)
where the last inequality follows from t(1 − t) ≥ p(1−q)
n . If (cid:96)(cid:91) ≥ n2/3 ≥ 76, then the
(cid:38) log4 n
16
∨ (1−t)2 log4 n
p(1 − p) ∨ t2 log4 n
1
variance of the entries is bounded by σ2 :=
,
1002 κ2nt(1−t)
(cid:96)(cid:91)
(cid:96)(cid:91)
and σ (cid:38) B log2 n√
(cid:107)P(cid:91)Q∼ − E[P(cid:91)Q∼ ](cid:107) ≤ 10σ(cid:112)(cid:96)(cid:91) ≤ 3
. Hence we can apply part 2 of Lemma 17 to get
(cid:96)(cid:91)
, w.h.p.,
16
16 p(1 − q) (cid:38) log4 n
where in the last inequality we use n ≥ (cid:96)(cid:91) and t(1 − t) ≥ 1
n . We conclude
that (cid:107)P(cid:91)Q∼(cid:107) ≤ (cid:107)E[P(cid:91)Q∼ ](cid:107) + (cid:107)P(cid:91)Q∼ − E[P(cid:91)Q∼ ](cid:107) ≤ 1
32 + 3
16 = 7
32 w.h.p.
(b) P(cid:91)Q(cid:54)∼ is a random matrix supported on V(cid:91) × V(cid:91) , whose entries are independent, zero
1002 κ2n · t2+q−2tq
mean, bounded almost surely by B (cid:48) := max{c1 , c2}, and have variance
1
. If
(1−t)t
(cid:115)
(cid:41)
(cid:40)
(cid:96)(cid:91) ≤ n2/3 , we apply part 1 of Lemma 17 to obtain
t2 + q − 2tq
√
(cid:107)P(cid:91)Q(cid:54)∼(cid:107) ≤ 10 max
(cid:96)(cid:91) log n, (c1 ∨ c2 ) log n
1
(cid:115)
(cid:32)(cid:114) 1 − t
(cid:40)
(cid:114) t
t(1 − t)
n
100κ
t2 + q − 2tq
√
∨
≤ 1
1
log n
1
t(1 − t)
1 − t
n1/3
t
10κ
4
10κ
n
1002 κ2n ·(cid:16) t2+q−2tq
where the last inequality follows from t(1−t) ≥ p(1−q)
n . If (cid:96)(cid:91) ≥ n2/3 ≥ 76, one veriﬁes
(cid:38) log4 n
16
∨ (1−t) log4 n
(1−t)t ∨ t log4 n
that the variance of the entries is bounded by (σ (cid:48) )2 :=
1
(1−t)(cid:96)(cid:91)
t(cid:96)(cid:91)

(cid:33)
log n

log n

,

(cid:33)

(cid:41)

≤ max

,

∨

,

476

(cid:41)

,

(cid:17)

,

Iterative and Active Clustering Without Size Constraints

and σ (cid:48) (cid:38) B (cid:48) log2 n√
(cid:96)(cid:91)

, w.h.p.,

. Hence we can apply part 2 of Lemma 17 to obtain
(cid:107)P(cid:91)Q(cid:54)∼(cid:107) ≤ 10σ (cid:48)(cid:112)(cid:96)(cid:91) ≤ 1
4
16 p(1 − q) (cid:38) log4 n
where in the last inequality we use n ≥ (cid:96)(cid:91) and t(1 − t) ≥ 1
n .
(c) Note that P(cid:93)Q∼ = P(cid:93)Q1 + P(cid:93)Q2 . By construction these two matrices are both
block-diagonal, have independent zero-mean entries which are bounded almost surely by
(cid:96)(cid:93) p and B∼,2 := 2c2
p respectively, and and have variance bounded by σ2∼1 := 1
B∼,1 := 1
and
p(cid:96)2
(cid:93)
σ2∼2 := 4(1−t)
2 respectively. One veriﬁes that σ∼,i (cid:38) B∼,i log2 n
√
c2
for i = 1, 2. We can then
√
p
n
apply part 2 of Lemma 17 to obtain (cid:107)P(cid:93)Q∼(cid:107) ≤ 10(σ∼,1 + σ∼,2 )
n ≤ 1
4 w.h.p.
(d) Note that P(cid:93)Q (cid:54)∼ = P(cid:93)Q3 is a random matrix with independent zero-mean entries
which are bounded almost surely by B(cid:54)∼ := 2c1
1−q and have variance bounded by σ2(cid:54)∼ :=
1 . One veriﬁes that σ(cid:54)∼ ≥ B(cid:54)∼ log2 n√
4t
1−q c2
. We can then apply part 2 of Lemma 17 to obtain
√
n
n ≤ 1
(cid:107)P(cid:93)Q(cid:54)∼(cid:107) ≤ 4σ (cid:54)∼
4 w.h.p.
Property 2:
(cid:13)(cid:13)(cid:13)U U (cid:62) (P(cid:93)Q) + (P(cid:93)Q)U U (cid:62) + U U (cid:62) (P(cid:93)Q)U U (cid:62)(cid:13)(cid:13)(cid:13)∞
Due to the structure of T , we have
(cid:107)PT Q(cid:107)∞ = (cid:107)PT P(cid:93)Q(cid:107)∞ =
(cid:13)(cid:13)(cid:13)∞
(cid:13)(cid:13)(cid:13)U U (cid:62)P(cid:93)Q
(cid:13)(cid:13)(cid:13)U U (cid:62)P(cid:93)Qm
(cid:13)(cid:13)(cid:13)∞ .
3(cid:88)
≤ 3
≤ 3
Now observe that (U U (cid:62)P(cid:93)Qm )(i, j ) = (cid:80)
m=1
n(cid:104)i(cid:105) P(cid:93)Qm (l, j ) is the sum of independent
1
l∈V(cid:104)i(cid:105)
zero-mean random variables with bounded magnitude and variance. Using the Bernstein
inequality in Lemma 19, we obtain that for each (i, j ) and with probability at least 1 − n−8 ,
(cid:115)
(cid:18)(cid:114) 1 − p
(cid:19)
(cid:12)(cid:12)(cid:12) ≤ 10
(cid:12)(cid:12)(cid:12)(U U (cid:62)P(cid:93)Q1 )(i, j )
· (cid:113)
≤ 1
log n
n(cid:104)i(cid:105) log n +
n(cid:104)i(cid:105) (cid:96)(cid:93)
p
p
24κ
(cid:113) log2 n
(cid:13)(cid:13)∞ ≤ 1
By union bound we conclude that (cid:13)(cid:13)U U (cid:62)P(cid:93)Q1
. For i ∈ V(cid:91) , clearly (U U (cid:62)P(cid:93)Q1 )(i, j ) = 0.
where in the last inequality we use p (cid:38) κ2n
(cid:13)(cid:13)U U (cid:62)P(cid:93)Q2
(cid:13)(cid:13)∞ in a similar fashion: for each (i, j ) and with probability
(cid:13)(cid:13)∞ and (cid:13)(cid:13)U U (cid:62)P(cid:93)Q3
(cid:96)2
(cid:93)
w.h.p. We can bound
n(cid:96)(cid:93)
24κ
at least 1 − n−8 :
(cid:18)(cid:114) 1 − p
(cid:19)
(cid:12)(cid:12)(cid:12)(U U (cid:62)P(cid:93)Q2 )(i, j )
(cid:12)(cid:12)(cid:12) ≤ 10
· (cid:113)
(cid:115)
(cid:32)(cid:115)
(cid:33)
n(cid:104)i(cid:105) log n +
p
(1 − p) log n
≤ 15
p(cid:96)(cid:93)
100κ

(cid:115)
(1 + )c2
n(cid:104)i(cid:105)

t
(1 − t)n

log2 n
n(cid:96)(cid:93)

,

log2 n
n(cid:96)(cid:93)

+

log n
(cid:96)(cid:93)p

≤ 1
6κ

, w.h.p.,

log n
p

·

477

Ailon, Chen and Xu

where the last inequality follows from p(1 − t) (cid:38) log n
(cid:18)(cid:114) q
(cid:12)(cid:12)(cid:12)(U U (cid:62)P(cid:93)Q3 )(i, j )
(cid:12)(cid:12)(cid:12) ≤ 10
· (cid:113)
(cid:96)(cid:93)
(cid:32)(cid:115)
(1 + )c1
(cid:114) 1 − t
n(cid:104)i(cid:105) log n +
1 − q
n(cid:104)i(cid:105)
≤ 15
q log n
(1 − q)(cid:96)(cid:93)
100κ
tn

, and

+

·

log n
(cid:96)(cid:93) (1 − q)

(cid:33)
log n
1 − q

(cid:19)

≤ 1
6κ

(cid:115)

log2 n
n(cid:96)(cid:93)

,

=

·

1
κt

log2 n
n(cid:96)(cid:93)

,

c1  ≥ 1
100κ

U U (cid:62) + Q, Ps(A)Ps( ˆB )P(cid:93)∆

log4 n
t(1 − t)(cid:96)(cid:93)
(cid:115)

log4 n
≥ 3
κ
n(cid:96)(cid:93)
(cid:115)

where the last inequality follows from t(1 − q) (cid:38) log n
. On the other hand, under the
(cid:96)(cid:93)
(cid:115)
(cid:115)
(cid:115)
(cid:114) 1 − t
deﬁnition of c1 , c2 and , we have
· 100
tn
(cid:115)
and similarly
log4 n
log2 n
· 100
≥ 3
c2  ≥ 1
t
t(1 − t)(cid:96)(cid:93)
(1 − t)n
It follows that (cid:107)PT Q(cid:107)∞ ≤ 3 ·(cid:0) 1
(cid:1) · 
100κ
n(cid:96)(cid:93)
κ
3 (c1 ∧c2 ) ≤ 
2 (c1 ∧c2 ) w.h.p., proving Property 2).
6 + 1
24 + 1
6
Properties 3(a) and 3(b):
(cid:69)
(cid:68)Ps(A)Ps( ˆB )P(cid:93)Q3 , Ps(A)Ps( ˆB )P(cid:93)∆
(cid:68)
(cid:69)
For 3(a), by construction of Q we have
= (1 + )c1 · (cid:88)
=
P(cid:93)∆(i, j )
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )P(cid:93)∆
(i,j )∈s( ˆB )∩s(A)
= (1 + )c1
where the last equality follows from ∆ ∈ D. Similarly, since
Ps(A)c Ps( ˆB )P(cid:93)Q1 = Ps(A)c Ps( ˆB )P(cid:93) (−U U (cid:62) ),
(cid:68)Ps(A)c Ps( ˆB )P(cid:93)Q2 , Ps(A)c Ps( ˆB )P(cid:93)∆
(cid:69)
(cid:69)
= −(1 + )c2 · (cid:88)
=
P(cid:93)∆(i, j )
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )P(cid:93)∆
(i,j )∈s( ˆB )∩s(A)c
= (1 + )c2
where the last equality again follows from ∆ ∈ D; this proves Property 3(b).
Properties 4(a) and 4(b):

(cid:68)
U U (cid:62) + Q, Ps(A)c Ps( ˆB )P(cid:93)∆

we have

,

.

,

478

Iterative and Active Clustering Without Size Constraints

=

=

(12)

·

≤ c1 .

+

+ (1 + )c2

,

·

1
p(cid:96)(cid:93)

=

100κ
(cid:96)(cid:93)

p − t ≥ p − q
4

(cid:68)
(cid:69)
For 4(a), we have
(cid:68)Ps(A)Ps( ˆB )c P(cid:93)
(cid:16)
(cid:69)
(cid:17)
U U (cid:62) + Q, Ps(A)Ps( ˆB )c P(cid:93)∆
(cid:18) 1
(cid:19)
(cid:88)
, Ps(A)Ps( ˆB )c P(cid:93)∆
U U (cid:62) + P(cid:93)Q1 + P(cid:93)Q2
1 − pij
1 − pij
P(cid:93)∆(i, j )
1
(cid:18) 1
(cid:19) (cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )c P(cid:93)∆
+ (1 + )c2
(cid:13)(cid:13)(cid:13)1
n(cid:104)i(cid:105)
n(cid:104)i(cid:105)
pij
pij
(i,j )∈s( ˆB )c∩s(A)
1 − p
≥ −
p(cid:96)(cid:93)
p
where the last inequality follows from ∆ ∈ D, pij ≥ p and n(cid:104)i(cid:105) ≥ (cid:96)(cid:93) , ∀i ∈ V(cid:93) . Consider the
(cid:115)
(cid:114) 1 − t
(cid:114) n
(cid:114) n
two terms in the parenthesis in (12). For the ﬁrst term, we have
t(1 − t)
t(1 − t)
tn

t(1 − t)
≤ 100κ
1
1002κ2p2n
100κ
(cid:96)(cid:93)
(cid:115)
(cid:41)
(cid:40)
κ(cid:112)b3p(1 − q)n
For the second term in (12), we have the following:
b3p(1 − q) log4 n
(cid:115)
(cid:40)
(cid:112)t(1 − q)
,
(cid:96)(cid:93)
(cid:96)(cid:93)
(cid:112)t(1 − t)
(cid:112)p(1 − t)
√
· max
n
κ
(cid:115)
(cid:40)
(cid:41)
,
(cid:96)(cid:93)
(cid:112)t(1 − t)
√
log4 n
≥8p(1 − t) · 100 max
= 8p(1 − t).
κ
n
t(1 − t)(cid:96)(cid:93)
(cid:113) 1−t
(cid:113) t
1−p
p ≤ (1 − )
A little algebra shows that this implies (1 + )
, or equivalently
1−t
t
1−p
p ≤ (1 − 2)c1 . Substituting back to (12), we conclude that
(cid:13)(cid:13)(cid:13)Ps(A)Ps( ˆB )c P(cid:93)∆
(cid:13)(cid:13)(cid:13)1
(cid:69) ≥ − (c1 + (1 − 2)c1 )
(cid:68)
(cid:68)Ps(A)c Ps( ˆB )c P(cid:93)Q3 , Ps(A)c Ps( ˆB )c P(cid:93)∆
(cid:69)
(cid:88)
−(1 + )
P(cid:93)∆(i, j )
c1 qij
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )c P(cid:93)∆
1 − qij
(i,j )∈s(A)c∩s( ˆB )c
≥ −(1 + )
c1 q
1 − q

proving Property 4(a).
(cid:68)
For 4(b), we have
U U (cid:62) + Q, Ps(A)c Ps( ˆB )c P(cid:93)∆

U U (cid:62) + Q, Ps(A)Ps( ˆB )c P(cid:93)∆

≥ 1
max
4
√

log4 n
t(1 − t)(cid:96)(cid:93)

,

(13)

=

b3
4

· p(1 − t) ·

(cid:69)

=

=

,

(cid:96)(cid:93)

(cid:41)

,

(1 + )c2

479

Ailon, Chen and Xu

max

≥ 1
4

where the last inequality follows from qij ≤ q . Consider the factor before the norm in (13).
(cid:115)
(cid:40)
(cid:41)
κ(cid:112)b3p(1 − q)n
Similarly as before, we have
b3p(1 − q) log4 n
t − q ≥ p − q
(cid:115)
(cid:41)
(cid:40)
,
4
(cid:96)(cid:93)
(cid:96)(cid:93)
(cid:112)t(1 − t)
√
log4 n
= 2t(1 − q).
n
κ
t(1 − t)(cid:96)(cid:93)
(cid:113) 1−t
(cid:113) t
1−q ≤ (1 − )
q
A little algebra shows that this implies (1 + )
1−t , or equivalently
t
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13)Ps(A)c Ps( ˆB )c P(cid:93)∆
(cid:68)
(cid:69) ≥ −(1 − )c2
1−q ≤ (1 − )c2 . Substituting back to (13), we conclude that
q
(1 + )c1
U U (cid:62) + Q, Ps(A)c Ps( ˆB )c P(cid:93)∆

≥ 2t(1 − q) · 100 max

(cid:96)(cid:93)

,

,

proving Property 4(b).
Properties 5 and 6:
Note that P(cid:91)U U (cid:62) = 0 and Ps( ˆB )P(cid:91) = Ps(A)P(cid:91) . These two properties hold by construc-
tion of Q.
We note that Properties (3)-(6) hold deterministically.
Combining the above results and applying the union bound, we conclude that with
probability at least 1 − n−3 , there exists a matrix Q (which is the one constructed and
veriﬁed above) that satisﬁes the properties in Proposition 12, where the probability is with
respect to the randomness in the graph A and the matrix W . Since W is independent of
A, integrating out the randomness in W proves the theorem.

5.2 Proof of Theorem 5

To ease notation, throughout the proof, C denotes a general universal positive constant
that can take diﬀerent values at diﬀerent locations. We let Ω := s(B ∗ ) denote the noise
locations.
Fix κ ≥ 1 and t in the allowed range, let (K, B ) be an optimal solution to (CP), and
assume K is a partial clustering induced by U1 , . . . , Ur for some integer r, and also assume
σmin (K ) = mini∈[r ] |Ui | satisﬁes (5). Let M = σmin (K ). We need a few helpful facts. Note
that from the deﬁnition of t, c1 , c2 ,
(p − q) ≤ c2
= t ≤ p − 1
1
4
c1 + c2
4
We say that a pair of sets Y ⊆ V , Z ⊆ V is cluster separated if there is no pair (y , z ) ∈
Y × Z satisfying y ∼ z .

(p − q) .

(14)

q +

Assumption 13 For al l pairs of cluster-separated sets Y , Z of size at least m := C log n
(p−q)2
each,

| ˆdY ,Z − q | <

(p − q) ,

1
4

(15)

where ˆdY ,Z :=

|(Y ×Z )∩Ω|
|Y |·|Z |

.

480

Iterative and Active Clustering Without Size Constraints

(17)

This is proven by a Hoeﬀding tail bound and a union bound to hold with probability at
least 1 − n−4 . To see why, ﬁx the sizes mY , mZ of |Y |, |Z |, assume mY ≤ mZ w.l.o.g. For
each such choice, there are at most exp{C (mY +mZ ) log n} ≤ exp{2CmZ log n} possibilities
for the choice of sets Y , Z . For each such choice, the probability that (15) does not hold is
exp{−CmY mZ (p − q)2}
(16)
using Hoeﬀding inequality. Hence, as long as mY ≥ m as deﬁned above, using union bound
(over all possibilities of mY , mZ and of Y , Z ) we obtain (15) uniformly. If we also assume
that
M ≥ 3m ,
the implication of Assumption 13 is that it cannot be the case that some Ui contains a
i of size in the range [m, |Ui | − m] such that U (cid:48)
i = Vg ∩ Ui for some g . Otherwise,
subset U (cid:48)
if such a set existed, then we would ﬁnd a strictly better solution to (CP), call it (K (cid:48) , B (cid:48) ),
which is deﬁned so that K (cid:48) is obtained from K by splitting the block corresponding to Ui
i and the other to Ui \ U (cid:48)
into two blocks, one corresponding to U (cid:48)
i . The diﬀerence ∆ between
i and Z := U \ U (cid:48)
the cost of (K, B ) and (K (cid:48) , B (cid:48) ) is (renaming Y := U (cid:48)
i )
∆ = c1 |(Y × Z ) ∩ Ω| − c2 |(Y × Z ) ∩ Ωc | = (c1 + c2 ) ˆdY ,Z |Y | |Z | − c2 |Y | |Z | .
But the sign of ∆ is exactly the sign of ˆdY ,Z − c2
which is strictly negative by (15) and
c1+c2
(14). (We also used the fact that the trace norm part of the utility function is equal for
both solutions: (cid:107)K (cid:48)(cid:107)∗ = (cid:107)K (cid:107)∗ ).
The conclusion is that for each i, the sets (Ui ∩ V1 ), . . . , (Ui ∩ Vk ) must all be of size at
most m, except maybe for at most one set of size at least |Ui | − m. But note that by the
theorem’s assumption,
M > km = (kC log n)/(p − q)2 ,
(18)
so we conclude that not all the sets (Ui ∩ V1 ), . . . , (Ui ∩ Vk ) can be of size at most m. Hence
exactly one of these sets must have size at least |Ui | − m. From this we conclude that there
is a function φ : [r] (cid:55)→ [k ] such that for all i ∈ [r],
|Ui ∩ Vφ(i) | ≥ |Ui | − m .
We now claim that this function is an injection. We will need the following assumption:
Assumption 14 For any 4 pairwise disjoint subsets (Y , Y (cid:48) , Z, Z (cid:48) ) such that (Y ∪ Y (cid:48) ) ⊆ Vi
for some i, (Z ∪ Z (cid:48) ) ⊆ [n] \ Vi , max{|Z |, |Z (cid:48) |} ≤ m, min{|Y |, |Y (cid:48) |} ≥ M − m:
|Y | · |Y (cid:48) | ˆdY ,Y (cid:48) − |Y | · |Z | ˆdY ,Z − |Y (cid:48) | · |Z (cid:48) | ˆdY (cid:48) ,Z (cid:48) >
(|Y | · |Y (cid:48) | − |Y | · |Z | − |Y (cid:48) | · |Z (cid:48) |)
c2
(19)
c1 + c2
The assumption holds with probability at least 1 − n−4 by using Hoeﬀding inequality,
union bounding over all possible sets Y , Y (cid:48) , Z, Z (cid:48) as above. Indeed, notice that for ﬁxed
mY , mY (cid:48) , mZ , mZ (cid:48) (with, say, mY ≥ mY (cid:48) ), and for each tuple Y , Y (cid:48) , Z, Z (cid:48) such that |Y | =
mY , |Y (cid:48) | = mY (cid:48) , |Z | = mZ , |Z (cid:48) | = mZ (cid:48) , the probability that (19) is violated is at most
exp{−C (p − q)2 (mY mY (cid:48) + mY mZ + mY (cid:48) mZ (cid:48) )} .

(20)

481

Ailon, Chen and Xu

Using (17), this is at most

exp{−C (p − q)2 (mY mY (cid:48) )} .
Now notice that the number of possibilities to choose such a 4 tuple of sets is bounded
above by exp{CmY log n}. Assuming

(21)

M ≥ C log n
(p − q)2 ,
and applying a union bound over all possible combinations Y , Y (cid:48) , Z, Z (cid:48) of sizes mY , mY (cid:48) , mZ , mZ (cid:48)
respectively, of which there are at most exp{CmY log n}, we conclude that (19) is violated
for some combination with probability at most
exp{−C (p − q)2mY mY (cid:48) /2}
which is at most exp{−C log n} if

(23)

(22)

M ≥ C log n
(p − q)2 .

(24)

Apply a union bound now over the possible combinations of the tuple (mY , mY (cid:48) , mZ , mZ (cid:48) ),
of which there are at most exp{C log n} to conclude that (19) holds uniformly for all pos-
sibilities of Y , Y (cid:48) , Z, Z (cid:48) with probability at least 1 − n−4 .
Now assume by contradiction that φ is not an injection, so φ(i) = φ(i(cid:48) ) =: j for some
distinct i, i(cid:48) ∈ [r]. Set Y = Ui ∩ Vj , Y (cid:48) = Ui(cid:48) ∩ Vj , Z = Ui \ Y , Z (cid:48) = Ui(cid:48) \ Y (cid:48) . Note that
max{|Z |, |Z (cid:48) |} ≤ m and min{|Y |, |Y (cid:48) |} ≥ M − m by the derivations to this point. Consider
the solution (K (cid:48) , B (cid:48) ) where K (cid:48) is obtained from K by replacing the two blocks corresponding
to Ui , Ui(cid:48) with four blocks: Y , Y (cid:48) , Z, Z (cid:48) . Inequality (19) guarantees that the cost of (K (cid:48) , B (cid:48) )
is strictly lower than that of (K, B ), contradicting optimality of the latter. (Note that we
used the fact that the corresponding contributions (cid:107)K (cid:107)∗ and (cid:107)K (cid:48)(cid:107)∗ to the trace-norm part
of the utility function are equal.)
We can now also conclude that r ≤ k . Fix i ∈ [r]. We show that not too many elements
of Vφ(i) can be contained in V \ {U1 ∪ · · · ∪ Ur }. We need the following assumption.
Assumption 15 For al l pairwise disjoint sets Y , X, Z ⊆ V such that |Y | ≥ M − m, |X | ≥
(cid:19)
(cid:18)|X |
m, (Y ∪ X ) ⊆ Vj for some j ∈ [k ], |Z | ≤ m, Z ∩ Vj = ∅:
(cid:19)
(cid:18)|X |
ˆdx,x − |Y | · |Z | ˆdY ,Z >
2
|X |
(|X | · |Y | +
c2
c1 + c2
2
c1 + c2
The assumption holds with probability at least 1 − n−4 . To see why, ﬁrst notice that
8 (p − q)|X | · |Y | by (5), as long as C2 is large enough. This implies that
|X |/(c1 + c2 ) ≤ 1
(cid:19)
(cid:18)
(cid:18)(cid:18)|X |
(cid:19)
(cid:19)
the RHS of (25) is upper bounded by
(p − q)
p − 1
8
2

|X | · |Y | ˆdX,Y +

− |Y | · |Z |) +

|X | · |Y | +

− |Y | · |Z |

.

(25)

c2
c1 + c

482

(26)

Iterative and Active Clustering Without Size Constraints

Proving that the LHS of (25) (denoted f (X, Y , Z )) is larger than (26) (denoted g(X, Y , Z ))
uniformly w.h.p. can now be easily done as follows. By ﬁxing mY = |Y |, mX = |X |, the
number of combinations for Y , X, Z is at most exp{C (mY + mX ) log n} for some global
C > 0. On the other hand, the probability that f (X, Y , Z ) ≤ g(X, Y , Z ) for any such option
is at most
exp{−C (p − q)2mY mX } .
Hence, by union bounding, the probability that some tuple Y , X, Z of sizes mY , mX , mZ
respectively satisﬁes f (X, Y , Z ) ≤ g(X, Y , Z ) is at most
exp{−C (p − q)2mY /2} ,
which is at most exp{−C log n} assuming
M ≥ C (log n)/(p − q)2 .

(27)

(28)

(29)

Another union bound over the possible choices of mY , mX , mZ proves that (25) holds uni-
formly with probability at least 1 − n−4 .
Now assume, by way of contradiction, that for some i ∈ [r], the set X := Vφ(i) ∩ ([n] \
{U1 ∪ · · · ∪ Ur }) is of size greater than m. Set Y := Vφ(i) ∩ Ui and Z = Ui \ Vφ(i) . Deﬁne
the solution (K (cid:48) , B (cid:48) ) where K (cid:48) is obtained from K by replacing the block corresponding to
Ui = Y ∪ Z in K with two blocks: Y ∪ X and Z . Assumption 15 tells us that the cost of
|X |
(K (cid:48) , B (cid:48) ) is strictly lower than that of (K, B ). Note that the expression
in the RHS of
(25) accounts for the trace norm diﬀerence (cid:107)K (cid:48)(cid:107)∗ − (cid:107)K (cid:107)∗ = |X |.
c1+c2
We are prepared to perform the ﬁnal “cleanup” step. At this point we know that for
each i ∈ [r], the set Ti = Ui ∩ Vφ(i) satisﬁes
ti := |Ti | ≥ max{|Ui | − m, |Vφ(i) | − rm} .
(30)
(To see why ti ≥ |Vφ(i) | − rm, note that at most m elements of Vφ(i) may be contained in
Ui(cid:48) for i(cid:48) (cid:54)= i, and another at most m elements in V \ (U1 ∪ · · · ∪ Ur ).) We are now going to
conclude from this that Ui = Vφ(i) for all i. To that end, let (K (cid:48) , B (cid:48) ) be the feasible solution
to (CP) deﬁned so that K (cid:48) is a partial clustering induced by Vφ(1) , . . . , Vφ(r) . We would like
to argue that if K (cid:54)= K (cid:48) then the cost of (K (cid:48) , B (cid:48) ) is strictly smaller than that of (K, B ).
Fix the value of the collection
(cid:0)mij := |Vφ(i) ∩ Uj |)(cid:1)
Y := ((r, φ(1), . . . , φ(r),
i := |Vφ(i) ∩ (V \ (U1 ∪ · · · ∪ Ur ))|(cid:1)
(cid:0)m(cid:48)
,
i,j∈[r ],i (cid:54)=j
).
i∈[r]
Let β (Y ) denote the number of i (cid:54)= j such that mij > 0 plus the number of i ∈ [r] such
possibilities for K giving rise to Y is exp{C ((cid:80)
i (cid:54)=j mij +(cid:80)
i > 0. We can assume β (Y ) > 0, otherwise Ui = Vφ(i) for all i ∈ [r]. The number of
that m(cid:48)
i mi ) log n}. Fix such a possibility,
and let
Dij = Vφ(i) ∩ Uj , D (cid:48)
i = Vφ(i) ∩ (V \ (U1 ∪ · · · ∪ Ur )) .

483

Ailon, Chen and Xu

δ =c1

m(cid:48)
i ,

+ c1

− c2

|(Dij × Ui ) ∩ Ω| + c1

The diﬀerence δ(K, K (cid:48) ) between the (CP) costs of solutions K and K (cid:48) is given by the
(cid:88)
(cid:88)
(cid:88)
(cid:88)
following expression:
|(Dij1 × Dij2 ) ∩ Ω|
(cid:88)
(cid:88)
(cid:88)
j (cid:54)=i
i
i
j1<j2
j1 ,j2 (cid:54)=i
|((Vφ(i) \ D (cid:48)
i ) ∩ Ω| + c2
i ) × D (cid:48)
|(Dij × Uj ) ∩ Ωc |
(cid:88)
(cid:88)
(cid:88)
(cid:88)
j (cid:54)=i
i
i
|(Dij1 × Dij2 ) ∩ Ωc |
|(Dij × Ui ) ∩ Ωc | − c2
|(Dij × Uj ) ∩ Ω| − (cid:88)
(cid:88)
(cid:88)
(cid:88)
j (cid:54)=i
i
i
j1<j2
j1 ,j2 (cid:54)=i
i ) × D (cid:48)
|((Vφ(i) \ D (cid:48)
i ) ∩ Ωc | − c1
− c2
where the expression (cid:80) m(cid:48)
j (cid:54)=i
i
i
i comes from the trace norm contribution. If the quantity δ(K, K (cid:48) )
(cid:80)
(i) (cid:80)
(cid:80)
j (cid:54)=i |(Dij × Ui ) ∩ Ω| + (cid:80)
is non-positive, then at least one of the following must be true:
(cid:80)
(cid:80)
(cid:80)
(cid:80)
|(Dij1 × Dij2 ) ∩ Ω|
j1<j2
i
i
j1 ,j2 (cid:54)=i
|(Dij1 × Dij2 )|
j (cid:54)=i |(Dij × Ui )| + c2
(cid:80)
(ii) (cid:80)
< c2
j1<j2
i
i
j1 ,j2 (cid:54)=i
c1+c2
c1+c2
(iii) (cid:80)
(cid:80)
(cid:80)
(cid:80)
i )| + 1
i |((Vφ(i) \ D (cid:48)
i ) × D (cid:48)
i ) ∩ Ω| < c1
i |((Vφ(i) \ D (cid:48)
i ) × D (cid:48)
c1+c2
c1+c2
j (cid:54)=i |(Dij × Uj )|.
j (cid:54)=i |(Dij × Uj ) ∩ Ω| > c2
i
i
c1+c2
−C (p − q)2 (cid:88)

Inequality (i) occurs with probability at most
(cid:88)
M
mij
j (cid:54)=i
i
(cid:41)
(cid:40)
−C (p − q)2 (cid:88)
using Hoeﬀding bound; we also used (30). Inequality (ii) occurs with probability at most
4 (p − q) (cid:80)
(cid:80)
i
using Hoeﬀding inequalities. (We also used the fact that the rightmost expression of (ii),
i |((Vφ(i) \ D (cid:48)
i ) × D (cid:48)
i )| due to the theorem
i m(cid:48)
i , is bounded above by 1
1
c1+c2
assumptions.) Inequality (iii) occurs occurs with probability at most (31), using Hoeﬀding
gross estimation, at most exp{((cid:80)
j (cid:54)=i mij + (cid:80)
bounds again.
Now notice that the number of choices of K (cid:48) giving rise to our ﬁxed Y and β (Y ) is, by a
i ) log n}. The assumptions of the theorem
i m(cid:48)
ensure that, using a union bound over all such possibilities K , and then over all options
for β (Y ) and Y , with probability at least 1 − n−4 the diﬀerence δ(K, K (cid:48) ) is positive. This
means that the (CP) cost of K (cid:48) is simultaneously strictly lower than that of K for all K we
have enumerated over.
Taking the theorem’s C1 , C2 large enough to satisfy the requirements above concludes
the proof.

(cid:80)
i m(cid:48)
i .

(31)

(32)

exp

exp

M m(cid:48)
i

484

| ˆdY ,Z − q | <

(ρp(cid:48) − ρq (cid:48) ) ,

1
4

(33)

Iterative and Active Clustering Without Size Constraints

5.3 Proof of Theorem 9

The proof of Theorem 5 in the previous section made repeated use of Hoeﬀding tail inequal-
ities, for uniformly bounding the size of the intersection of the noise support Ω with various
submatrices (with high probability). This is tight for p, q which are bounded away from 0
and 1. However, if p = ρp(cid:48) , q = ρq (cid:48) , the noise probabilities p(cid:48) , q (cid:48) are ﬁxed and ρ tends to 0,
a sharper bound is obtained using Bernstein tail bound (Lemma 18 in see Appendix A.2).
Using Bernstein inequality instead of Hoeﬀding inequality, gives the required result. To see
how this is done, the counterpart of Assumption 13 above is as follows:

Assumption 16 For al l pairs of cluster-separated sets Y , Z of size at least m := C log n
ρ
each,

|(Y ×Z )∩Ω|
where ˆdY ,Z :=
.
|Y |·|Z |
Note: In this section, C (and hence also m) depends on p(cid:48) , q (cid:48) only, which are assumed ﬁxed.
Deﬁning henceforth m as in Assumption 9, Assumption 14 holds with probability at least
1 − n−4 . This can be seen by replacing the Hoeﬀding bound in (20) with a Chernoﬀ bound:
exp{−C (p(cid:48) , q (cid:48) )ρ(mY mY (cid:48) + mY mZ + mY (cid:48) mZ (cid:48) )} .

(34)

The rest of the proof is obtained by a similar step by step technical alteration of the
proof in Section 5.2.

6. Discussion

An immediate future research is to better understand the “mid-size crisis”. Our current
results say nothing about clusters that are neither large nor small, falling in the interval
((cid:96)(cid:91) , (cid:96)(cid:93) ). Our numerical experiments conﬁrm that the mid-size phenomenon is real: they
are neither completely recovered nor entirely ignored by the optimal ˆK . The part of ˆK
restricted to these clusters does not seem to have an obvious pattern. Proving whether we
can still eﬃciently recover large clusters in the presence of mid-size clusters is an interesting
open problem.
Our study was mainly theoretical, focusing on the planted partition model. As such,
our experiments focused on conﬁrming the theoretical ﬁndings with data generated exactly
according to the distribution we could provide provable guarantees for. It would be inter-
esting to apply the presented methodology to real applications, particularly large data sets
merged from web application and social networks.
Another interesting direction is extending the “peeling strategy” to other settings. Our
algorithms use the convex program (CP) as a subroutine, taking advantage of the fact that
the recovery of large clusters via (CP) is not hindered by the presence of small clusters,
and that (CP) has a tunable parameter that controls the sizes of the clusters that are
considered large. It is possible that other clustering routines also have these properties and
thus can be used as a subroutine in our iterative and active algorithms. More generally,
our problem concerns the inference of an unknown structure, and our high-level strategy
is to sequentially infer and remove the “easy” (or low-resolution) part of the problem and

485

Ailon, Chen and Xu

zoom into the “hard” (or high-resolution) part. It is interesting to explore this strategy in
a broader context, and to understand for what problems and under what conditions this
strategy may work.

Acknowledgments

The authors are grateful to the anonymous reviewers for their thorough reviews of this
work and valuable suggestions on improving the manuscript. N. Ailon acknowledges the
support of a Marie Curie International Reintegration Grant PIRG07-GA-2010-268403, and
a grant from Technion-Cornell Innovation Institute (TCII). Y. Chen was supported by NSF
grant CIF-31712-23800 and ONR MURI grant N00014-11-1-0688. The work of H. Xu was
partially supported by the Ministry of Education of Singapore through AcRF Tier Two
grant R265-000-443-112.

Appendix A. Technical Lemmas

In this section we state several lemmas needed in the proofs of our main results.

.

A.1 The Spectral Norm of Random Matrices
Lemma 17 Suppose A ∈ RN ×N is a symmetric matrix, where Aij , 1 ≤ i ≤ j ≤ m are
independent random variables, each of which has mean 0 and variance at most σ2 and is
bounded in absolute value by B a.s.
(cid:110)
(cid:111)
σ(cid:112)N log n, B log n
1. If n ≥ N , then with probability at least 1 − n−6 , the ﬁrst singular value A satisﬁes
λ1 (A) ≤ 10 max
2. If further n ≥ N ≥ 76, N ≥ n2/3 and σ ≥ c1
B log2 n√
N
then with probability at least 1 − n−6 , we have
√
λ1 (A) ≤ 10σ
N .
are zero-mean random matrices independent of each other, and A = (cid:80)
Proof We ﬁrst prove part 1 of the lemma. Let ei be the i-th standard basis in RN . Deﬁne
for i ∈ [N ]. Then the Zij ’s
for 1 ≤ i < j ≤ N , and Zii = Aiieie(cid:62)
j + Aj iej e(cid:62)
Zij = Aij eie(cid:62)
i
i
1≤i≤j≤N Zij . We
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ N σ2 .
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) =
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
have (cid:107)Zij (cid:107) ≤ B almost surely. We also have
(cid:88)
(cid:88)
(cid:88)
(cid:88)
eie(cid:62)
ii )eie(cid:62)
E(Zij Z (cid:62)
E(A2
E(A2
i +
ij )
ij )
i
Similarly, we have (cid:107) (cid:80)
1≤i≤N
1≤i≤N
1≤i≤j≤N
j :j (cid:54)=i
ity (Theorem 1.6 in Tropp 2012) with t = 10 max (cid:8)σ
N log n, B log n(cid:9) yields the desired
ij Zij )(cid:107) ≤ N σ2 . Applying the Matrix Bernstein Inequal-
E(Z (cid:62)
√
1≤i≤j≤N
bound.

for some absolute constant c1 > 0,

486

Iterative and Active Clustering Without Size Constraints

(cid:20) 0 A
(cid:21)
We turn to part 2 of the lemma. Let A(cid:48) be an independent copy of A, and deﬁne
A(cid:48)
0
Note that ¯A is an 2N × 2N random matrix with i.i.d. entries. If σ ≥ c1
B log2 n√
for some
N
√
suﬃciently large absolute constant c1 > 0, then by Theorem 3.1 in Achlioptas and Mcsherry
(2007) we know that with probability at least 1 − n−6 , λ1 ( ¯A) ≤ 10σ
N . The lemma follows
from noting that λ1 (A) ≤ λ1 ( ¯A).

¯A :=

.

Pr

Yi

.

Yi − E

A.2 Standard Bernstein Inequality for the Sum of Independent Variables
Lemma 18 ( Bernstein inequality) Let Y1 , . . . , YN be independent random variables, each
of which has variance bounded by σ2 and is bounded in absolute value by B a.s.. Then we
(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) N(cid:88)
(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > t
(cid:34) N(cid:88)
(cid:35)
(cid:27)
(cid:26)
have that
i=1
i=1
The following lemma is an immediate consequence of Lemma 18.
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) N(cid:88)
(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 10
Lemma 19 Let Y1 , . . . , YN be independent random variables, each of which has variance
(cid:34) N(cid:88)
(cid:17)
(cid:16)
σ(cid:112)N log n + B log n
bounded by σ2 and is bounded in absolute value by B a.s. Then we have
i=1
i=1
with probability at least 1 − 2n−8 .

t2/2
N σ2 + B t/3

≤ 2 exp

Yi − E

Yi

References

Dimitris Achlioptas and Frank Mcsherry. Fast computation of low-rank matrix approxima-
tions. Journal of the ACM, 54(2):9, 2007.

Nir Ailon, Yudong Chen, and Huan Xu. Breaking the small cluster barrier of graph clus-
tering. In Proceedings of International Conference on Machine Learning (ICML), pages
995–1003, 2013.

Nir Ailon, Ron Begleiter, and Esther Ezra. Active learning using smooth relative regret
approximations with applications. Journal of Machine Learning Research, 15:885–920,
2014.

Brendan P. W. Ames and Stephen A. Vavasis. Nuclear norm minimization for the planted
clique and biclique problems. Mathematical Programming, 129(1):69–89, 2011.

Anima Anandkumar, Rong Ge, Daniel Hsu, and Sham M. Kakade. A tensor spectral
approach to learning mixed membership community models. Journal of Machine Learning
Research, 15:2239–2312, June 2014.

487

Ailon, Chen and Xu

Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine Learning,
56:89–113, 2004.

B´ela Bollob´as and Alex D. Scott. Max cut for random graphs with a planted partition.
Combinatorics, Probability and Computing, 13(4-5):451–474, 2004.

Ravi B. Boppana. Eigenvalues and graph bisection: an average-case analysis. In Proceedings
of Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 280–
285, 1987.

Emmanuel J. Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component
analysis? Journal of the ACM, 58:1–37, 2011.

Ted Carson and Russell Impagliazzo. Hill-climbing ﬁnds random planted bisections.
In
Proceedings of the 12th Annual Symposium on Discrete Algorithms, pages 903–909, 2001.

Venkat Chandrasekaran, Sujay Sanghavi, Pablo Parrilo, and Alan Willsky. Rank-sparsity
incoherence for matrix decomposition. SIAM Journal on Optimization, 21(2):572–596,
2011.

Kamalika Chaudhuri, Fan Chung, and Alexander Tsiatas. Spectral clustering of graphs
with general degrees in the extended planted partition model. In Proceedings of the 25th
Annual Conference on Learning Theory (COLT), pages 35.1–35.23, 2012.

Yudong Chen, Sujay Sanghavi, and Huan Xu. Clustering sparse graphs. In Advances in
Neural Information Processing Systems 25, pages 2204–2212. Curran Associates, Inc.,
2012.

Yudong Chen, Ali Jalali, Sujay Sanghavi, and Huan Xu. Clustering partially observed
graphs via convex optimization. Journal of Machine Learning Research, 15:2213–2238,
June 2014a.

Yudong Chen, Sujay Sanghavi, and Huan Xu. Improved graph clustering. IEEE Transac-
tions on Information Theory, 60(10):6440–6455, 2014b.

Anne Condon and Richard M. Karp. Algorithms for graph partitioning on the planted
partition model. Random Structures and Algorithms, 18(2):116–140, 2001.

Brian Eriksson, Gautam Dasarathy, Aarti Singh, and Robert Nowak. Active clustering: ro-
bust and eﬃcient hierarchical clustering using adaptively selected similarities. In Proceed-
ings of International Conference on Artiﬁcial Intel ligence and Statistics, pages 260–268,
2011.

Martin Ester, Hans-Peter Kriegel, and Xiaowei Xu. A database interface for clustering in
large spatial databases.
In Proceedings of 1st International Conference on Know ledge
Discovery and Data Mining (KDD), 1995.

Ioannis Giotis and Venkatesan Guruswami. Correlation clustering with a ﬁxed number of
clusters. Theory of Computing, 2(1):249–266, 2006.

488

Iterative and Active Clustering Without Size Constraints

Paul W. Holland, Kathryn B. Laskey, and Samuel Leinhardt. Stochastic blockmodels: some
ﬁrst steps. Social networks, 5(2):109–137, 1983.

Ali Jalali, Yudong Chen, Sujay Sanghavi, and Huan Xu. Clustering partially observed
graphs via convex optimization. In Proceedigns of the 28th International Conference on
Machine Learning, pages 1001–1008, 2011.

Akshay Krishnamurthy and Aarti Singh. Low-rank matrix and tensor completion via adap-
tive sampling. In Advances in Neural Information Processing Systems 26, pages 836–844,
2013.

Akshay Krishnamurthy, Sivaraman Balakrishnan, Min Xu, and Aarti Singh. Eﬃcient active
algorithms for hierarchical clustering. In Proceedings of the 29th International Conference
on Machine Learning (ICML), pages 887–894, 2012.

Amit Kumar and Ravindran Kannan. Clustering with spectral norm and the k-means
algorithm. In Proceedings of 51st Annual IEEE Symposium on Foundations of Computer
Science (FOCS), pages 299–308, 2010.

Zhouchen Lin, Risheng Liu, and Zhixun Su. Linearized alternating direction method with
adaptive penalty for low-rank representation. In Advances in Neural Information Pro-
cessing Systems 24, pages 612–620, 2011.

Claire Mathieu and Warren Schudy. Correlation clustering with noisy input. In Proceedings
of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, pages 712–
728. SIAM, 2010.

Frank McSherry. Spectral partitioning of random graphs. In Proceedings of 42nd IEEE
Symposium on Foundations of Computer Science, pages 529–537, 2001.

Nina Mishra, Robert Schreiber, Isabelle Stanton, and Robert E. Tarjan. Clustering social
networks. In Algorithms and Models for the Web-Graph, pages 56–67. Springer, 2007.

Samet Oymak and Babak Hassibi. Finding dense clusters via low rank + sparse decompo-
sition. arXiv:1104.5186v1, 2011.

Karl Rohe, Sourav Chatterjee, and Bin Yu. Spectral clustering and the high-dimensional
stochastic block model. Annals of Statistics, 39:1878–1915, 2011.

Ohad Shamir and Naftali Tishby. Spectral clustering on a budget. In Proceedings of the 14th
International Conference on Artiﬁcial Intel ligence and Statistics, pages 661–669, 2011.

Ron Shamir and Dekel Tsur. Improved algorithms for the random cluster graph model.
Random Structure and Algorithm, 31(4):418–449, 2007.

Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of
Computational Mathematics, 12(4):389–434, 2012.

Konstantin Voevodski, Maria-Florina Balcan, Heiko R¨oglin, Shang-Hua Teng, and Yu Xia.
Active clustering of biological sequences. Journal of Machine Learning Research, 13:
203–225, 2012.

489

Ailon, Chen and Xu

Huan Xu, Constantine Caramanis, and Sujay Sanghavi. Robust PCA via outlier pursuit.
IEEE Transactions on Information Theory, 58(5):3047–3064, 2012.

Yahoo!-Inc. Graph partitioning. http://research.yahoo.com/project/2368, 2009.

Yunpeng Zhao, Elizaveta Levina, and Ji Zhu. Community extraction for social networks.
Proceedings of the National Academy of Sciences, 108(18):7321–7326, 2011.

490

